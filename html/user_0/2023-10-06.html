<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time Measurements. (arXiv:2310.03140v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03140">http://arxiv.org/abs/2310.03140</a></li>
<li>Code URL: https://github.com/bryanbocao/vifit</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03140]] ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time Measurements(http://arxiv.org/abs/2310.03140)</code></li>
<li>Summary: <p>Tracking subjects in videos is one of the most widely used functions in
camera-based IoT applications such as security surveillance, smart city traffic
safety enhancement, vehicle to pedestrian communication and so on. In the
computer vision domain, tracking is usually achieved by first detecting
subjects with bounding boxes, then associating detected bounding boxes across
video frames. For many IoT systems, images captured by cameras are usually sent
over the network to be processed at a different site that has more powerful
computing resources than edge devices. However, sending entire frames through
the network causes significant bandwidth consumption that may exceed the system
bandwidth constraints. To tackle this problem, we propose ViFiT, a
transformer-based model that reconstructs vision bounding box trajectories from
phone data (IMU and Fine Time Measurements). It leverages a transformer ability
of better modeling long-term time series data. ViFiT is evaluated on Vi-Fi
Dataset, a large-scale multimodal dataset in 5 diverse real world scenes,
including indoor and outdoor environments. To fill the gap of proper metrics of
jointly capturing the system characteristics of both tracking quality and video
bandwidth reduction, we propose a novel evaluation framework dubbed Minimum
Required Frames (MRF) and Minimum Required Frames Ratio (MRFR). ViFiT achieves
an MRFR of 0.65 that outperforms the state-of-the-art approach for cross-modal
reconstruction in LSTM Encoder-Decoder architecture X-Translator of 0.98,
resulting in a high frame reduction rate as 97.76%.
</p></li>
</ul>

<h3>Title: Visual inspection for illicit items in X-ray images using Deep Learning. (arXiv:2310.03658v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03658">http://arxiv.org/abs/2310.03658</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03658]] Visual inspection for illicit items in X-ray images using Deep Learning(http://arxiv.org/abs/2310.03658)</code></li>
<li>Summary: <p>Automated detection of contraband items in X-ray images can significantly
increase public safety, by enhancing the productivity and alleviating the
mental load of security officers in airports, subways, customs/post offices,
etc. The large volume and high throughput of passengers, mailed parcels, etc.,
during rush hours practically make it a Big Data problem. Modern computer
vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of
undertaking this task even under resource-constrained and embedded execution
scenarios, e.g., as is the case with fast, single-stage object detectors.
However, no comparative experimental assessment of the various relevant DNN
components/methods has been performed under a common evaluation protocol, which
means that reliable cross-method comparisons are missing. This paper presents
exactly such a comparative assessment, utilizing a public relevant dataset and
a well-defined methodology for selecting the specific DNN components/modules
that are being evaluated. The results indicate the superiority of Transformer
detectors, the obsolete nature of auxiliary neural modules that have been
developed in the past few years for security applications and the efficiency of
the CSP-DarkNet backbone CNN.
</p></li>
</ul>

<h3>Title: Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset. (arXiv:2310.03119v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03119">http://arxiv.org/abs/2310.03119</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03119]] Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset(http://arxiv.org/abs/2310.03119)</code></li>
<li>Summary: <p>IoT (Internet of Things) refers to the network of interconnected physical
devices, vehicles, home appliances, and other items embedded with sensors,
software, and connectivity, enabling them to collect and exchange data. IoT
Forensics is collecting and analyzing digital evidence from IoT devices to
investigate cybercrimes, security breaches, and other malicious activities that
may have taken place on these connected devices. In particular, EM-SCA has
become an essential tool for IoT forensics due to its ability to reveal
confidential information about the internal workings of IoT devices without
interfering these devices or wiretapping their networks. However, the accuracy
and reliability of EM-SCA results can be limited by device variability,
environmental factors, and data collection and processing methods. Besides,
there is very few research on these limitations that affects significantly the
accuracy of EM-SCA approaches for the crossed-IoT device portability as well as
limited research on the possible solutions to address such challenge.
Therefore, this empirical study examines the impact of device variability on
the accuracy and reliability of EM-SCA approaches, in particular
machine-learning (ML) based approaches for EM-SCA. We firstly presents the
background, basic concepts and techniques used to evaluate the limitations of
current EM-SCA approaches and datasets. Our study then addresses one of the
most important limitation, which is caused by the multi-core architecture of
the processors (SoC). We present an approach to collect the EM-SCA datasets and
demonstrate the feasibility of using transfer learning to obtain more
meaningful and reliable results from EM-SCA in IoT forensics of crossed-IoT
devices. Our study moreover contributes a new dataset for using deep learning
models in analysing Electromagnetic Side-Channel data with regards to the
cross-device portability matter.
</p></li>
</ul>

<h3>Title: Impedance Leakage Vulnerability and its Utilization in Reverse-engineering Embedded Software. (arXiv:2310.03175v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03175">http://arxiv.org/abs/2310.03175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03175]] Impedance Leakage Vulnerability and its Utilization in Reverse-engineering Embedded Software(http://arxiv.org/abs/2310.03175)</code></li>
<li>Summary: <p>Discovering new vulnerabilities and implementing security and privacy
measures are important to protect systems and data against physical attacks.
One such vulnerability is impedance, an inherent property of a device that can
be exploited to leak information through an unintended side channel, thereby
posing significant security and privacy risks. Unlike traditional
vulnerabilities, impedance is often overlooked or narrowly explored, as it is
typically treated as a fixed value at a specific frequency in research and
design endeavors. Moreover, impedance has never been explored as a source of
information leakage. This paper demonstrates that the impedance of an embedded
device is not constant and directly relates to the programs executed on the
device. We define this phenomenon as impedance leakage and use this as a side
channel to extract software instructions from protected memory. Our experiment
on the ATmega328P microcontroller and the Artix 7 FPGA indicates that the
impedance side channel can detect software instructions with 96.1% and 92.6%
accuracy, respectively. Furthermore, we explore the dual nature of the
impedance side channel, highlighting the potential for beneficial purposes and
the associated risk of intellectual property theft. Finally, potential
countermeasures that specifically address impedance leakage are discussed.
</p></li>
</ul>

<h3>Title: Ask for Alice: Online Covert Distress Signal in the Presence of a Strong Adversary. (arXiv:2310.03237v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03237">http://arxiv.org/abs/2310.03237</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03237]] Ask for Alice: Online Covert Distress Signal in the Presence of a Strong Adversary(http://arxiv.org/abs/2310.03237)</code></li>
<li>Summary: <p>In this paper we propose a protocol that can be used to covertly send a
distress signal through a seemingly normal webserver, even if the adversary is
monitoring both the network and the user's device. This allows a user to call
for help even when they are in the same physical space as their adversaries. We
model such a scenario by introducing a strong adversary model that captures a
high degree of access to the user's device and full control over the network.
</p>
<p>Our model fits into scenarios where a user is under surveillance and wishes
to inform a trusted party of the situation. To do this, our method uses
existing websites to act as intermediaries between the user and a trusted
backend; this enables the user to initiate the distress signal without arousing
suspicion, even while being actively monitored. We accomplish this by utilising
the TLS handshake to convey additional information; this means that any website
wishing to participate can do so with minimal effort and anyone monitoring the
traffic will just see common TLS connections. In order for websites to be
willing to host such a functionality the protocol must coexist gracefully with
users who use normal TLS and the computational overhead must be minimal. We
provide a full security analysis of the architecture and prove that the
adversary cannot distinguish between a set of communications which contains a
distress call and a normal communication.
</p></li>
</ul>

<h3>Title: CyMed: A Framework for Testing Cybersecurity of Connected Medical Devices. (arXiv:2310.03583v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03583">http://arxiv.org/abs/2310.03583</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03583]] CyMed: A Framework for Testing Cybersecurity of Connected Medical Devices(http://arxiv.org/abs/2310.03583)</code></li>
<li>Summary: <p>Connected Medical Devices (CMDs) have a large impact on patients as they
allow them to lead a more normal life. Any malfunction could not only remove
the health benefits the CMDs provide, they could also cause further harm to the
patient. Due to this, there are many safety regulations which must be adhered
to prior to a CMD entering the market. However, while many detailed safety
regulations exist, there are a fundamental lack of cybersecurity frameworks
applicable to CMDs. While there are recent regulations which aim to enforce
cybersecurity practices, they are vague and do not contain the concrete steps
necessary to implement cybersecurity. This paper aims to fill that gap by
describing a framework, CyMed, to be used by vendors and ens-users, which
contains concrete measures to improve the resilience of CMDs against cyber
attack. The CyMed framework is subsequently evaluated based on practical tests
as well as expert interviews.
</p></li>
</ul>

<h3>Title: Solving Degree Bounds For Iterated Polynomial Systems. (arXiv:2310.03637v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03637">http://arxiv.org/abs/2310.03637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03637]] Solving Degree Bounds For Iterated Polynomial Systems(http://arxiv.org/abs/2310.03637)</code></li>
<li>Summary: <p>For Arithmetization-Oriented ciphers and hash functions Gr\"obner basis
attacks are generally considered as the most competitive attack vector.
Unfortunately, the complexity of Gr\"obner basis algorithms is only understood
for special cases, and it is needless to say that these cases do not apply to
most cryptographic polynomial systems. Therefore, cryptographers have to resort
to experiments, extrapolations and hypotheses to assess the security of their
designs. One established measure to quantify the complexity of linear
algebra-based Gr\"obner basis algorithms is the so-called solving degree.
Caminata \&amp; Gorla revealed that under a certain genericity condition on a
polynomial system the solving degree is always upper bounded by the
Castelnuovo-Mumford regularity and henceforth by the Macaulay bound, which only
takes the degrees and number of variables of the input polynomials into
account. In this paper we extend their framework to iterated polynomial
systems, the standard polynomial model for symmetric ciphers and hash
functions. In particular, we prove solving degree bounds for various attacks on
MiMC, Feistel-MiMC, Feistel-MiMC-Hash, Hades and GMiMC. Our bounds fall in line
with the hypothesized complexity of Gr\"obner basis attacks on these designs,
and to the best of our knowledge this is the first time that a mathematical
proof for these complexities is provided.
</p>
<p>Moreover, by studying polynomials with degree falls we can prove lower bounds
on the Castelnuovo-Mumford regularity for attacks on MiMC, Feistel-MiMC and
Feistel-MiMC-Hash provided that only a few solutions of the corresponding
iterated polynomial system originate from the base field. Hence,
regularity-based solving degree estimations can never surpass a certain
threshold, a desirable property for cryptographic polynomial systems.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy-preserving Multi-biometric Indexing based on Frequent Binary Patterns. (arXiv:2310.03091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03091">http://arxiv.org/abs/2310.03091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03091]] Privacy-preserving Multi-biometric Indexing based on Frequent Binary Patterns(http://arxiv.org/abs/2310.03091)</code></li>
<li>Summary: <p>The development of large-scale identification systems that ensure the privacy
protection of enrolled subjects represents a major challenge. Biometric
deployments that provide interoperability and usability by including efficient
multi-biometric solutions are a recent requirement. In the context of privacy
protection, several template protection schemes have been proposed in the past.
However, these schemes seem inadequate for indexing (workload reduction) in
biometric identification systems. More specifically, they have been used in
identification systems that perform exhaustive searches, leading to a
degradation of computational efficiency. To overcome these limitations, we
propose an efficient privacy-preserving multi-biometric identification system
that retrieves protected deep cancelable templates and is agnostic with respect
to biometric characteristics and biometric template protection schemes. To this
end, a multi-biometric binning scheme is designed to exploit the low
intra-class variation properties contained in the frequent binary patterns
extracted from different types of biometric characteristics. Experimental
results reported on publicly available databases using state-of-the-art Deep
Neural Network (DNN)-based embedding extractors show that the protected
multi-biometric identification system can reduce the computational workload to
approximately 57\% (indexing up to three types of biometric characteristics)
and 53% (indexing up to two types of biometric characteristics), while
simultaneously improving the biometric performance of the baseline biometric
system at the high-security thresholds. The source code of the proposed
multi-biometric indexing approach together with the composed multi-biometric
dataset, will be made available to the research community once the article is
accepted.
</p></li>
</ul>

<h3>Title: Shielding the Unseen: Privacy Protection through Poisoning NeRF with Spatial Deformation. (arXiv:2310.03125v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03125">http://arxiv.org/abs/2310.03125</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03125]] Shielding the Unseen: Privacy Protection through Poisoning NeRF with Spatial Deformation(http://arxiv.org/abs/2310.03125)</code></li>
<li>Summary: <p>In this paper, we introduce an innovative method of safeguarding user privacy
against the generative capabilities of Neural Radiance Fields (NeRF) models.
Our novel poisoning attack method induces changes to observed views that are
imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to
accurately reconstruct a 3D scene. To achieve this, we devise a bi-level
optimization algorithm incorporating a Projected Gradient Descent (PGD)-based
spatial deformation. We extensively test our approach on two common NeRF
benchmark datasets consisting of 29 real-world scenes with high-quality images.
Our results compellingly demonstrate that our privacy-preserving method
significantly impairs NeRF's performance across these benchmark datasets.
Additionally, we show that our method is adaptable and versatile, functioning
across various perturbation strengths and NeRF architectures. This work offers
valuable insights into NeRF's vulnerabilities and emphasizes the need to
account for such potential privacy risks when developing robust 3D scene
reconstruction algorithms. Our study contributes to the larger conversation
surrounding responsible AI and generative machine learning, aiming to protect
user privacy and respect creative ownership in the digital age.
</p></li>
</ul>

<h3>Title: PoseAction: Action Recognition for Patients in the Ward using Deep Learning Approaches. (arXiv:2310.03288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03288">http://arxiv.org/abs/2310.03288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03288]] PoseAction: Action Recognition for Patients in the Ward using Deep Learning Approaches(http://arxiv.org/abs/2310.03288)</code></li>
<li>Summary: <p>Real-time intelligent detection and prediction of subjects' behavior
particularly their movements or actions is critical in the ward. This approach
offers the advantage of reducing in-hospital care costs and improving the
efficiency of healthcare workers, which is especially true for scenarios at
night or during peak admission periods. Therefore, in this work, we propose
using computer vision (CV) and deep learning (DL) methods for detecting
subjects and recognizing their actions. We utilize OpenPose as an accurate
subject detector for recognizing the positions of human subjects in the video
stream. Additionally, we employ AlphAction's Asynchronous Interaction
Aggregation (AIA) network to predict the actions of detected subjects. This
integrated model, referred to as PoseAction, is proposed. At the same time, the
proposed model is further trained to predict 12 common actions in ward areas,
such as staggering, chest pain, and falling down, using medical-related video
clips from the NTU RGB+D and NTU RGB+D 120 datasets. The results demonstrate
that PoseAction achieves the highest classification mAP of 98.72% (IoU@0.5).
Additionally, this study develops an online real-time mode for action
recognition, which strongly supports the clinical translation of PoseAction.
Furthermore, using OpenPose's function for recognizing face key points, we also
implement face blurring, which is a practical solution to address the privacy
protection concerns of patients and healthcare workers. Nevertheless, the
training data for PoseAction is currently limited, particularly in terms of
label diversity. Consequently, the subsequent step involves utilizing a more
diverse dataset (including general actions) to train the model's parameters for
improved generalization.
</p></li>
</ul>

<h3>Title: DP-SGD for non-decomposable objective functions. (arXiv:2310.03104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03104">http://arxiv.org/abs/2310.03104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03104]] DP-SGD for non-decomposable objective functions(http://arxiv.org/abs/2310.03104)</code></li>
<li>Summary: <p>Unsupervised pre-training is a common step in developing computer vision
models and large language models. In this setting, the absence of labels
requires the use of similarity-based loss functions, such as contrastive loss,
that favor minimizing the distance between similar inputs and maximizing the
distance between distinct inputs. As privacy concerns mount, training these
models using differential privacy has become more important. However, due to
how inputs are generated for these losses, one of their undesirable properties
is that their $L_2$ sensitivity can grow with increasing batch size. This
property is particularly disadvantageous for differentially private training
methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD
variant for similarity based loss functions -- in particular the commonly used
contrastive loss -- that manipulates gradients of the objective function in a
novel way to obtain a senstivity of the summed gradient that is $O(1)$ for
batch size $n$. We test our DP-SGD variant on some preliminary CIFAR-10
pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, our
method's performance comes close to that of a non-private model and generally
outperforms DP-SGD applied directly to the contrastive loss.
</p></li>
</ul>

<h3>Title: Hadamard Domain Training with Integers for Class Incremental Quantized Learning. (arXiv:2310.03675v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03675">http://arxiv.org/abs/2310.03675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03675]] Hadamard Domain Training with Integers for Class Incremental Quantized Learning(http://arxiv.org/abs/2310.03675)</code></li>
<li>Summary: <p>Continual learning is a desirable feature in many modern machine learning
applications, which allows in-field adaptation and updating, ranging from
accommodating distribution shift, to fine-tuning, and to learning new tasks.
For applications with privacy and low latency requirements, the compute and
memory demands imposed by continual learning can be cost-prohibitive for
resource-constraint edge platforms. Reducing computational precision through
fully quantized training (FQT) simultaneously reduces memory footprint and
increases compute efficiency for both training and inference. However,
aggressive quantization especially integer FQT typically degrades model
accuracy to unacceptable levels. In this paper, we propose a technique that
leverages inexpensive Hadamard transforms to enable low-precision training with
only integer matrix multiplications. We further determine which tensors need
stochastic rounding and propose tiled matrix multiplication to enable low-bit
width accumulators. We demonstrate the effectiveness of our technique on
several human activity recognition datasets and CIFAR100 in a class incremental
learning setting. We achieve less than 0.5% and 3% accuracy degradation while
we quantize all matrix multiplications inputs down to 4-bits with 8-bit
accumulators.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks. (arXiv:2310.03707v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03707">http://arxiv.org/abs/2310.03707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03707]] OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks(http://arxiv.org/abs/2310.03707)</code></li>
<li>Summary: <p>Evasion Attacks (EA) are used to test the robustness of trained neural
networks by distorting input data to misguide the model into incorrect
classifications. Creating these attacks is a challenging task, especially with
the ever-increasing complexity of models and datasets. In this work, we
introduce a self-supervised, computationally economical method for generating
adversarial examples, designed for the unseen black-box setting. Adapting
techniques from representation learning, our method generates on-manifold EAs
that are encouraged to resemble the data distribution. These attacks are
comparable in effectiveness compared to the state-of-the-art when attacking the
model trained on, but are significantly more effective when attacking unseen
models, as the attacks are more related to the data rather than the model
itself. Our experiments consistently demonstrate the method is effective across
various models, unseen data categories, and even defended models, suggesting a
significant role for on-manifold EAs when targeting unseen models.
</p></li>
</ul>

<h3>Title: Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors. (arXiv:2310.03166v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03166">http://arxiv.org/abs/2310.03166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03166]] Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors(http://arxiv.org/abs/2310.03166)</code></li>
<li>Summary: <p>Machine-learning phishing webpage detectors (ML-PWD) have been shown to
suffer from adversarial manipulations of the HTML code of the input webpage.
Nevertheless, the attacks recently proposed have demonstrated limited
effectiveness due to their lack of optimizing the usage of the adopted
manipulations, and they focus solely on specific elements of the HTML code. In
this work, we overcome these limitations by first designing a novel set of
fine-grained manipulations which allow to modify the HTML code of the input
phishing webpage without compromising its maliciousness and visual appearance,
i.e., the manipulations are functionality- and rendering-preserving by design.
We then select which manipulations should be applied to bypass the target
detector by a query-efficient black-box optimization algorithm. Our experiments
show that our attacks are able to raze to the ground the performance of current
state-of-the-art ML-PWD using just 30 queries, thus overcoming the weaker
attacks developed in previous work, and enabling a much fairer robustness
evaluation of ML-PWD.
</p></li>
</ul>

<h3>Title: ResolverFuzz: Automated Discovery of DNS Resolver Vulnerabilities with Query-Response Fuzzing. (arXiv:2310.03202v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03202">http://arxiv.org/abs/2310.03202</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03202]] ResolverFuzz: Automated Discovery of DNS Resolver Vulnerabilities with Query-Response Fuzzing(http://arxiv.org/abs/2310.03202)</code></li>
<li>Summary: <p>Domain Name System (DNS) is a critical component of the Internet. DNS
resolvers, which act as the cache between DNS clients and DNS nameservers, are
the central piece of the DNS infrastructure, essential to the scalability of
DNS. However, finding the resolver vulnerabilities is non-trivial, and this
problem is not well addressed by the existing tools. To list a few reasons,
first, most of the known resolver vulnerabilities are non-crash bugs that
cannot be directly detected by the existing oracles (or sanitizers). Second,
there lacks rigorous specifications to be used as references to classify a test
case as a resolver bug. Third, DNS resolvers are stateful, and stateful fuzzing
is still challenging due to the large input space.
</p>
<p>In this paper, we present a new fuzzing system termed ResolverFuzz to address
the aforementioned challenges related to DNS resolvers, with a suite of new
techniques being developed. First, ResolverFuzz performs constrained stateful
fuzzing by focusing on the short query-response sequence, which has been
demonstrated as the most effective way to find resolver bugs, based on our
study of the published DNS CVEs. Second, to generate test cases that are more
likely to trigger resolver bugs, we combine probabilistic context-free grammar
(PCFG) based input generation with byte-level mutation for both queries and
responses. Third, we leverage differential testing and clustering to identify
non-crash bugs like cache poisoning bugs. We evaluated ResolverFuzz against 6
mainstream DNS software under 4 resolver modes. Overall, we identify 23
vulnerabilities that can result in cache poisoning, resource consumption, and
crash attacks. After responsible disclosure, 19 of them have been confirmed or
fixed, and 15 CVE numbers have been assigned.
</p></li>
</ul>

<h3>Title: StegGuard: Fingerprinting Self-supervised Pre-trained Encoders via Secrets Embeder and Extractor. (arXiv:2310.03380v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03380">http://arxiv.org/abs/2310.03380</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03380]] StegGuard: Fingerprinting Self-supervised Pre-trained Encoders via Secrets Embeder and Extractor(http://arxiv.org/abs/2310.03380)</code></li>
<li>Summary: <p>In this work, we propose StegGuard, a novel fingerprinting mechanism to
verify the ownership of the suspect pre-trained encoder using steganography. A
critical perspective in StegGuard is that the unique characteristic of the
transformation from an image to an embedding, conducted by the pre-trained
encoder, can be equivalently exposed how an embeder embeds secrets into images
and how an extractor extracts the secrets from encoder's embeddings with a
tolerable error after the secrets are subjected to the encoder's
transformation. While each independent encoder has a distinct transformation,
the piracy encoder has a similar transformation to the victim. Based on these,
we learn a pair of secrets embeder and extractor as the fingerprint for the
victim encoder. We introduce a frequency-domain channel attention embedding
block into the embeder to adaptively embed secrets into suitable frequency
bands. During verification, if the secrets embedded into the query images can
be extracted with an acceptable error from the suspect encoder's embeddings,
the suspect encoder is determined as piracy, otherwise independent. Extensive
experiments demonstrate that depending on a very limited number of query
images, StegGuard can reliably identify across varied independent encoders, and
is robust against model stealing related attacks including model extraction,
fine-tuning, pruning, embedding noising and shuffle.
</p></li>
</ul>

<h3>Title: The Anatomy of Deception: Technical and Human Perspectives on a Large-scale Phishing Campaign. (arXiv:2310.03498v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03498">http://arxiv.org/abs/2310.03498</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03498]] The Anatomy of Deception: Technical and Human Perspectives on a Large-scale Phishing Campaign(http://arxiv.org/abs/2310.03498)</code></li>
<li>Summary: <p>In an era dominated by digital interactions, phishing campaigns have evolved
to exploit not just technological vulnerabilities but also human traits. This
study takes an unprecedented deep dive into large-scale phishing campaigns
aimed at Meta's users, offering a dual perspective on the technical mechanics
and human elements involved. Analysing data from over 25,000 victims worldwide,
we highlight the nuances of these campaigns, from the intricate techniques
deployed by the attackers to the sentiments and behaviours of those who were
targeted. Unlike prior research conducted in controlled environments, this
investigation capitalises on the vast, diverse, and genuine data extracted
directly from active phishing campaigns, allowing for a more holistic
understanding of the drivers, facilitators, and human factors. Through the
application of advanced computational techniques, including natural language
processing and machine learning, this work unveils critical insights into the
psyche of victims and the evolving tactics of modern phishers. Our analysis
illustrates very poor password selection choices from the victims but also
persistence in the revictimisation of a significant part of the users. Finally,
we reveal many correlations regarding demographics, timing, sentiment, emotion,
and tone of the victims' responses.
</p></li>
</ul>

<h3>Title: Putting a Padlock on Lambda -- Integrating vTPMs into AWS Firecracker. (arXiv:2310.03522v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03522">http://arxiv.org/abs/2310.03522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03522]] Putting a Padlock on Lambda -- Integrating vTPMs into AWS Firecracker(http://arxiv.org/abs/2310.03522)</code></li>
<li>Summary: <p>When software services use cloud providers to run their workloads, they place
implicit trust in the cloud provider, without an explicit trust relationship.
One way to achieve such explicit trust in a computer system is to use a
hardware Trusted Platform Module (TPM), a coprocessor for trusted computing.
However, in the case of managed platform-as-a-service (PaaS) offerings, there
is currently no cloud provider that exposes TPM capabilities. In this paper, we
improve trust by integrating a virtual TPM device into the Firecracker
hypervisor, originally developed by Amazon Web Services. In addition to this,
multiple performance tests along with an attack surface analysis are performed
to evaluate the impact of the changes introduced. We discuss the results and
conclude that the slight performance decrease and attack surface increase are
acceptable trade-offs in order to enable trusted computing in PaaS offerings.
</p></li>
</ul>

<h3>Title: Digital Twin-Empowered Smart Attack Detection System for 6G Edge of Things Networks. (arXiv:2310.03554v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03554">http://arxiv.org/abs/2310.03554</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03554]] Digital Twin-Empowered Smart Attack Detection System for 6G Edge of Things Networks(http://arxiv.org/abs/2310.03554)</code></li>
<li>Summary: <p>As global Internet of Things (IoT) devices connectivity surges, a significant
portion gravitates towards the Edge of Things (EoT) network. This shift prompts
businesses to deploy infrastructure closer to end-users, enhancing
accessibility. However, the growing EoT network expands the attack surface,
necessitating robust and proactive security measures. Traditional solutions
fall short against dynamic EoT threats, highlighting the need for proactive and
intelligent systems. We introduce a digital twin-empowered smart attack
detection system for 6G EoT networks. Leveraging digital twin and edge
computing, it monitors and simulates physical assets in real time, enhancing
security. An online learning module in the proposed system optimizes the
network performance. Our system excels in proactive threat detection, ensuring
6G EoT network security. The performance evaluations demonstrate its
effectiveness, robustness, and adaptability using real datasets.
</p></li>
</ul>

<h3>Title: Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System. (arXiv:2310.03334v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03334">http://arxiv.org/abs/2310.03334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03334]] Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System(http://arxiv.org/abs/2310.03334)</code></li>
<li>Summary: <p>Network Intrusion Detection System (NIDS) is a key component in securing the
computer network from various cyber security threats and network attacks.
However, consider an unfortunate situation where the NIDS is itself attacked
and vulnerable more specifically, we can say, How to defend the defender?. In
Adversarial Machine Learning (AML), the malicious actors aim to fool the
Machine Learning (ML) and Deep Learning (DL) models to produce incorrect
predictions with intentionally crafted adversarial examples. These adversarial
perturbed examples have become the biggest vulnerability of ML and DL based
systems and are major obstacles to their adoption in real-time and
mission-critical applications such as NIDS. AML is an emerging research domain,
and it has become a necessity for the in-depth study of adversarial attacks and
their defence strategies to safeguard the computer network from various cyber
security threads. In this research work, we aim to cover important aspects
related to NIDS, adversarial attacks and its defence mechanism to increase the
robustness of the ML and DL based NIDS. We implemented four powerful
adversarial attack techniques, namely, Fast Gradient Sign Method (FGSM),
Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and
Carlini &amp; Wagner (C&amp;W) in NIDS. We analyzed its performance in terms of various
performance metrics in detail. Furthermore, the three heuristics defence
strategies, i.e., Adversarial Training (AT), Gaussian Data Augmentation (GDA)
and High Confidence (HC), are implemented to improve the NIDS robustness under
adversarial attack situations. The complete workflow is demonstrated in
real-time network with data packet flow. This research work provides the
overall background for the researchers interested in AML and its implementation
from a computer network security point of view.
</p></li>
</ul>

<h3>Title: Targeted Adversarial Attacks on Generalizable Neural Radiance Fields. (arXiv:2310.03578v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03578">http://arxiv.org/abs/2310.03578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03578]] Targeted Adversarial Attacks on Generalizable Neural Radiance Fields(http://arxiv.org/abs/2310.03578)</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for
3D scene representation and rendering. These data-driven models can learn to
synthesize high-quality images from sparse 2D observations, enabling realistic
and interactive scene reconstructions. However, the growing usage of NeRFs in
critical applications such as augmented reality, robotics, and virtual
environments could be threatened by adversarial attacks.
</p>
<p>In this paper we present how generalizable NeRFs can be attacked by both
low-intensity adversarial attacks and adversarial patches, where the later
could be robust enough to be used in real world applications. We also
demonstrate targeted attacks, where a specific, predefined output scene is
generated by these attack with success.
</p></li>
</ul>

<h3>Title: SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03684">http://arxiv.org/abs/2310.03684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03684]] SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks(http://arxiv.org/abs/2310.03684)</code></li>
<li>Summary: <p>Despite efforts to align large language models (LLMs) with human values,
widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to
jailbreaking attacks, wherein an adversary fools a targeted LLM into generating
objectionable content. To address this vulnerability, we propose SmoothLLM, the
first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our
finding that adversarially-generated prompts are brittle to character-level
changes, our defense first randomly perturbs multiple copies of a given input
prompt, and then aggregates the corresponding predictions to detect adversarial
inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to
below one percentage point, avoids unnecessary conservatism, and admits
provable guarantees on attack mitigation. Moreover, our defense uses
exponentially fewer queries than existing attacks and is compatible with any
LLM.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition. (arXiv:2310.03033v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03033">http://arxiv.org/abs/2310.03033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03033]] Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition(http://arxiv.org/abs/2310.03033)</code></li>
<li>Summary: <p>Traffic signs play a critical role in road safety and traffic management for
autonomous driving systems. Accurate traffic sign classification is essential
but challenging due to real-world complexities like adversarial examples and
occlusions. To address these issues, binary neural networks offer promise in
constructing classifiers suitable for resource-constrained devices.
</p>
<p>In our previous work, we proposed high-accuracy BNN models for traffic sign
recognition, focusing on compact size for limited computation and energy
resources. To evaluate their local robustness, this paper introduces a set of
benchmark problems featuring layers that challenge state-of-the-art
verification tools. These layers include binarized convolutions, max pooling,
batch normalization, fully connected. The difficulty of the verification
problem is given by the high number of network parameters (905k - 1.7 M), of
the input dimension (2.7k-12k), and of the number of regions (43) as well by
the fact that the neural networks are not sparse.
</p>
<p>The proposed BNN models and local robustness properties can be checked at
https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/traffic_signs_recognition.
</p>
<p>The results of the 4th International Verification of Neural Networks
Competition (VNN-COMP'23) revealed the fact that 4, out of 7, solvers can
handle many of our benchmarks randomly selected (minimum is 6, maximum is 36,
out of 45). Surprisingly, tools output also wrong results or missing
counterexample (ranging from 1 to 4). Currently, our focus lies in exploring
the possibility of achieving a greater count of solved instances by extending
the allotted time (previously set at 8 minutes). Furthermore, we are intrigued
by the reasons behind the erroneous outcomes provided by the tools for certain
benchmarks.
</p></li>
</ul>

<h3>Title: Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03149">http://arxiv.org/abs/2310.03149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03149]] Attributing Learned Concepts in Neural Networks to Training Data(http://arxiv.org/abs/2310.03149)</code></li>
<li>Summary: <p>By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</p></li>
</ul>

<h3>Title: Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models. (arXiv:2310.03182v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03182">http://arxiv.org/abs/2310.03182</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03182]] Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models(http://arxiv.org/abs/2310.03182)</code></li>
<li>Summary: <p>Medical image classification is a critical problem for healthcare, with the
potential to alleviate the workload of doctors and facilitate diagnoses of
patients. However, two challenges arise when deploying deep learning models to
real-world healthcare applications. First, neural models tend to learn spurious
correlations instead of desired features, which could fall short when
generalizing to new domains (e.g., patients with different ages). Second, these
black-box models lack interpretability. When making diagnostic predictions, it
is important to understand why a model makes a decision for trustworthy and
safety considerations. In this paper, to address these two limitations, we
propose a new paradigm to build robust and interpretable medical image
classifiers with natural language concepts. Specifically, we first query
clinical concepts from GPT-4, then transform latent image features into
explicit concepts with a vision-language model. We systematically evaluate our
method on eight medical image classification datasets to verify its
effectiveness. On challenging datasets with strong confounding factors, our
method can mitigate spurious correlations thus substantially outperform
standard visual encoders and other baselines. Finally, we show how
classification with a small number of concepts brings a level of
interpretability for understanding model decisions through case studies in real
medical data.
</p></li>
</ul>

<h3>Title: Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03358">http://arxiv.org/abs/2310.03358</a></li>
<li>Code URL: https://github.com/changzhang777/ancra</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03358]] Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention(http://arxiv.org/abs/2310.03358)</code></li>
<li>Summary: <p>Deep neural networks are vulnerable to adversarial noise. Adversarial
training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two characteristics of robust
representation: (1) $\bf{exclusion}$: the feature of natural examples keeps
away from that of other classes; (2) $\bf{alignment}$: the feature of natural
and corresponding adversarial examples is close to each other. These motivate
us to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance. Code is
available at &lt;https://github.com/changzhang777/ANCRA&gt;.
</p></li>
</ul>

<h3>Title: CSI: Enhancing the Robustness of 3D Point Cloud Recognition against Corruption. (arXiv:2310.03360v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03360">http://arxiv.org/abs/2310.03360</a></li>
<li>Code URL: https://github.com/masterwu2115/csi</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03360]] CSI: Enhancing the Robustness of 3D Point Cloud Recognition against Corruption(http://arxiv.org/abs/2310.03360)</code></li>
<li>Summary: <p>Despite recent advancements in deep neural networks for point cloud
recognition, real-world safety-critical applications present challenges due to
unavoidable data corruption. Current models often fall short in generalizing to
unforeseen distribution shifts. In this study, we harness the inherent set
property of point cloud data to introduce a novel critical subset
identification (CSI) method, aiming to bolster recognition robustness in the
face of data corruption. Our CSI framework integrates two pivotal components:
density-aware sampling (DAS) and self-entropy minimization (SEM), which cater
to static and dynamic CSI, respectively. DAS ensures efficient robust anchor
point sampling by factoring in local density, while SEM is employed during
training to accentuate the most salient point-to-point attention. Evaluations
reveal that our CSI approach yields error rates of 18.4\% and 16.3\% on
ModelNet40-C and PointCloud-C, respectively, marking a notable improvement over
state-of-the-art methods by margins of 5.2\% and 4.2\% on the respective
benchmarks. Code is available at
\href{https://github.com/masterwu2115/CSI/tree/main}{https://github.com/masterwu2115/CSI/tree/main}
</p></li>
</ul>

<h3>Title: OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon. (arXiv:2310.03388v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03388">http://arxiv.org/abs/2310.03388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03388]] OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon(http://arxiv.org/abs/2310.03388)</code></li>
<li>Summary: <p>Moving deep learning models from the laboratory setting to the open world
entails preparing them to handle unforeseen conditions. In several applications
the occurrence of novel classes during deployment poses a significant threat,
thus it is crucial to effectively detect them. Ideally, this skill should be
used when needed without requiring any further computational training effort at
every new task. Out-of-distribution detection has attracted significant
attention in the last years, however the majority of the studies deal with 2D
images ignoring the inherent 3D nature of the real-world and often confusing
between domain and semantic novelty. In this work, we focus on the latter,
considering the objects geometric structure captured by 3D point clouds
regardless of the specific domain. We advance the field by introducing
OpenPatch that builds on a large pre-trained model and simply extracts from its
intermediate features a set of patch representations that describe each known
class. For any new sample, we obtain a novelty score by evaluating whether it
can be recomposed mainly by patches of a single known class or rather via the
contribution of multiple classes. We present an extensive experimental
evaluation of our approach for the task of semantic novelty detection on
real-world point cloud samples when the reference known data are synthetic. We
demonstrate that OpenPatch excels in both the full and few-shot known sample
scenarios, showcasing its robustness across varying pre-training objectives and
network backbones. The inherent training-free nature of our method allows for
its immediate application to a wide array of real-world tasks, offering a
compelling advantage over approaches that need expensive retraining efforts.
</p></li>
</ul>

<h3>Title: Learning to Simplify Spatial-Temporal Graphs in Gait Analysis. (arXiv:2310.03396v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03396">http://arxiv.org/abs/2310.03396</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03396]] Learning to Simplify Spatial-Temporal Graphs in Gait Analysis(http://arxiv.org/abs/2310.03396)</code></li>
<li>Summary: <p>Gait analysis leverages unique walking patterns for person identification and
assessment across multiple domains. Among the methods used for gait analysis,
skeleton-based approaches have shown promise due to their robust and
interpretable features. However, these methods often rely on hand-crafted
spatial-temporal graphs that are based on human anatomy disregarding the
particularities of the dataset and task. This paper proposes a novel method to
simplify the spatial-temporal graph representation for gait-based gender
estimation, improving interpretability without losing performance. Our approach
employs two models, an upstream and a downstream model, that can adjust the
adjacency matrix for each walking instance, thereby removing the fixed nature
of the graph. By employing the Straight-Through Gumbel-Softmax trick, our model
is trainable end-to-end. We demonstrate the effectiveness of our approach on
the CASIA-B dataset for gait-based gender estimation. The resulting graphs are
interpretable and differ qualitatively from fixed graphs used in existing
models. Our research contributes to enhancing the explainability and
task-specific adaptability of gait recognition, promoting more efficient and
reliable gait-based biometrics.
</p></li>
</ul>

<h3>Title: Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on Double Covering. (arXiv:2310.03431v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03431">http://arxiv.org/abs/2310.03431</a></li>
<li>Code URL: https://github.com/jjjkkyz/dcudf</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03431]] Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on Double Covering(http://arxiv.org/abs/2310.03431)</code></li>
<li>Summary: <p>In this paper, we propose a new method, called DoubleCoverUDF, for extracting
the zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a
learned UDF and a user-specified parameter $r$ (a small positive real number)
as input and extracts an iso-surface with an iso-value $r$ using the
conventional marching cubes algorithm. We show that the computed iso-surface is
the boundary of the $r$-offset volume of the target zero level-set $S$, which
is an orientable manifold, regardless of the topology of $S$. Next, the
algorithm computes a covering map to project the boundary mesh onto $S$,
preserving the mesh's topology and avoiding folding. If $S$ is an orientable
manifold surface, our algorithm separates the double-layered mesh into a single
layer using a robust minimum-cut post-processing step. Otherwise, it keeps the
double-layered mesh as the output. We validate our algorithm by reconstructing
3D surfaces of open models and demonstrate its efficacy and effectiveness on
synthetic models and benchmark datasets. Our experimental results confirm that
our method is robust and produces meshes with better quality in terms of both
visual evaluation and quantitative measures than existing UDF-based methods.
The source code is available at https://github.com/jjjkkyz/DCUDF.
</p></li>
</ul>

<h3>Title: Mitigating the Influence of Domain Shift in Skin Lesion Classification: A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic Images. (arXiv:2310.03432v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03432">http://arxiv.org/abs/2310.03432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03432]] Mitigating the Influence of Domain Shift in Skin Lesion Classification: A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic Images(http://arxiv.org/abs/2310.03432)</code></li>
<li>Summary: <p>The potential of deep neural networks in skin lesion classification has
already been demonstrated to be on-par if not superior to the dermatologists
diagnosis. However, the performance of these models usually deteriorates when
the test data differs significantly from the training data (i.e. domain shift).
This concerning limitation for models intended to be used in real-world skin
lesion classification tasks poses a risk to patients. For example, different
image acquisition systems or previously unseen anatomical sites on the patient
can suffice to cause such domain shifts. Mitigating the negative effect of such
shifts is therefore crucial, but developing effective methods to address domain
shift has proven to be challenging. In this study, we carry out an in-depth
analysis of eight different unsupervised domain adaptation methods to analyze
their effectiveness in improving generalization for dermoscopic datasets. To
ensure robustness of our findings, we test each method on a total of ten
distinct datasets, thereby covering a variety of possible domain shifts. In
addition, we investigated which factors in the domain shifted datasets have an
impact on the effectiveness of domain adaptation methods. Our findings show
that all of the eight domain adaptation methods result in improved AUPRC for
the majority of analyzed datasets. Altogether, these results indicate that
unsupervised domain adaptations generally lead to performance improvements for
the binary melanoma-nevus classification task regardless of the nature of the
domain shift. However, small or heavily imbalanced datasets lead to a reduced
conformity of the results due to the influence of these factors on the methods
performance.
</p></li>
</ul>

<h3>Title: 3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation. (arXiv:2310.03534v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03534">http://arxiv.org/abs/2310.03534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03534]] 3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation(http://arxiv.org/abs/2310.03534)</code></li>
<li>Summary: <p>Prior methods that tackle the problem of generalizable object pose estimation
highly rely on having dense views of the unseen object. By contrast, we address
the scenario where only a single reference view of the object is available. Our
goal then is to estimate the relative object pose between this reference view
and a query image that depicts the object in a different pose. In this
scenario, robust generalization is imperative due to the presence of unseen
objects during testing and the large-scale object pose variation between the
reference and the query. To this end, we present a new
hypothesis-and-verification framework, in which we generate and evaluate
multiple pose hypotheses, ultimately selecting the most reliable one as the
relative object pose. To measure reliability, we introduce a 3D-aware
verification that explicitly applies 3D transformations to the 3D object
representations learned from the two input images. Our comprehensive
experiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior
accuracy of our approach in relative pose estimation and its robustness in
large-scale pose variations, when dealing with unseen objects.
</p></li>
</ul>

<h3>Title: Robustness-Guided Image Synthesis for Data-Free Quantization. (arXiv:2310.03661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03661">http://arxiv.org/abs/2310.03661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03661]] Robustness-Guided Image Synthesis for Data-Free Quantization(http://arxiv.org/abs/2310.03661)</code></li>
<li>Summary: <p>Quantization has emerged as a promising direction for model compression.
Recently, data-free quantization has been widely studied as a promising method
to avoid privacy concerns, which synthesizes images as an alternative to real
training data. Existing methods use classification loss to ensure the
reliability of the synthesized images. Unfortunately, even if these images are
well-classified by the pre-trained model, they still suffer from low semantics
and homogenization issues. Intuitively, these low-semantic images are sensitive
to perturbations, and the pre-trained model tends to have inconsistent output
when the generator synthesizes an image with poor semantics. To this end, we
propose Robustness-Guided Image Synthesis (RIS), a simple but effective method
to enrich the semantics of synthetic images and improve image diversity,
further boosting the performance of downstream data-free compression tasks.
Concretely, we first introduce perturbations on input and model weight, then
define the inconsistency metrics at feature and prediction levels before and
after perturbations. On the basis of inconsistency on two levels, we design a
robustness optimization objective to enhance the semantics of synthetic images.
Moreover, we also make our approach diversity-aware by forcing the generator to
synthesize images with small correlations in the label space. With RIS, we
achieve state-of-the-art performance for various settings on data-free
quantization and can be extended to other data-free compression tasks.
</p></li>
</ul>

<h3>Title: LumiNet: The Bright Side of Perceptual Knowledge Distillation. (arXiv:2310.03669v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03669">http://arxiv.org/abs/2310.03669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03669]] LumiNet: The Bright Side of Perceptual Knowledge Distillation(http://arxiv.org/abs/2310.03669)</code></li>
<li>Summary: <p>In knowledge distillation research, feature-based methods have dominated due
to their ability to effectively tap into extensive teacher models. In contrast,
logit-based approaches are considered to be less adept at extracting hidden
'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel
knowledge-transfer algorithm designed to enhance logit-based distillation. We
introduce a perception matrix that aims to recalibrate logits through
adjustments based on the model's representation capability. By meticulously
analyzing intra-class dynamics, LumiNet reconstructs more granular inter-class
relationships, enabling the student model to learn a richer breadth of
knowledge. Both teacher and student models are mapped onto this refined matrix,
with the student's goal being to minimize representational discrepancies.
Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO)
attests to LumiNet's efficacy, revealing its competitive edge over leading
feature-based methods. Moreover, in exploring the realm of transfer learning,
we assess how effectively the student model, trained using our method, adapts
to downstream tasks. Notably, when applied to Tiny ImageNet, the transferred
features exhibit remarkable performance, further underscoring LumiNet's
versatility and robustness in diverse settings. With LumiNet, we hope to steer
the research discourse towards a renewed interest in the latent capabilities of
logit-based knowledge distillation.
</p></li>
</ul>

<h3>Title: A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores. (arXiv:2310.03283v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03283">http://arxiv.org/abs/2310.03283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03283]] A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores(http://arxiv.org/abs/2310.03283)</code></li>
<li>Summary: <p>Large Language Models (LLMs), such as ChatGPT, have achieved impressive
milestones in natural language processing (NLP). Despite their impressive
performance, the models are known to pose important risks. As these models are
deployed in real-world applications, a systematic understanding of different
risks posed by these models on tasks such as natural language inference (NLI),
is much needed. In this paper, we define and formalize two distinct types of
risk: decision risk and composite risk. We also propose a risk-centric
evaluation framework, and four novel metrics, for assessing LLMs on these risks
in both in-domain and out-of-domain settings. Finally, we propose a
risk-adjusted calibration method called DwD for helping LLMs minimize these
risks in an overall NLI architecture. Detailed experiments, using four NLI
benchmarks, three baselines and two LLMs, including ChatGPT, show both the
practical utility of the evaluation framework, and the efficacy of DwD in
reducing decision and composite risk. For instance, when using DwD, an
underlying LLM is able to address an extra 20.1% of low-risk inference tasks
(but which the LLM erroneously deems high-risk without risk adjustment) and
skip a further 19.8% of high-risk tasks, which would have been answered
incorrectly.
</p></li>
</ul>

<h3>Title: Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations. (arXiv:2310.03518v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03518">http://arxiv.org/abs/2310.03518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03518]] Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations(http://arxiv.org/abs/2310.03518)</code></li>
<li>Summary: <p>In real dialogue scenarios, as there are unknown input noises in the
utterances, existing supervised slot filling models often perform poorly in
practical applications. Even though there are some studies on noise-robust
models, these works are only evaluated on rule-based synthetic datasets, which
is limiting, making it difficult to promote the research of noise-robust
methods. In this paper, we introduce a noise robustness evaluation dataset
named Noise-SF for slot filling task. The proposed dataset contains five types
of human-annotated noise, and all those noises are exactly existed in real
extensive robust-training methods of slot filling into the proposed framework.
By conducting exhaustive empirical evaluation experiments on Noise-SF, we find
that baseline models have poor performance in robustness evaluation, and the
proposed framework can effectively improve the robustness of models. Based on
the empirical experimental results, we make some forward-looking suggestions to
fuel the research in this direction. Our dataset Noise-SF will be released at
https://github.com/dongguanting/Noise-SF.
</p></li>
</ul>

<h3>Title: TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03646">http://arxiv.org/abs/2310.03646</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03646]] TRAM: Bridging Trust Regions and Sharpness Aware Minimization(http://arxiv.org/abs/2310.03646)</code></li>
<li>Summary: <p>By reducing the curvature of the loss surface in the parameter space,
Sharpness-aware minimization (SAM) yields widespread robustness improvement
under domain transfer. Instead of focusing on parameters, however, this work
considers the transferability of representations as the optimization target for
out-of-domain generalization in a fine-tuning setup. To encourage the retention
of transferable representations, we consider trust region-based fine-tuning
methods, which exploit task-specific skills without forgetting task-agnostic
representations from pre-training. We unify parameter- and representation-space
smoothing approaches by using trust region bounds to inform SAM-style
regularizers on both of these optimization surfaces. We propose Trust Region
Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat
minima and smooth, informative representations without forgetting pre-trained
structure. We find that TRAM outperforms both sharpness-aware and trust
region-based optimization methods on cross-domain language modeling and
cross-lingual transfer, where robustness to domain transfer and representation
generality are critical for success. TRAM establishes a new standard in
training generalizable models with minimal additional computation.
</p></li>
</ul>

<h3>Title: Burning the Adversarial Bridges: Robust Windows Malware Detection Against Binary-level Mutations. (arXiv:2310.03285v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03285">http://arxiv.org/abs/2310.03285</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03285]] Burning the Adversarial Bridges: Robust Windows Malware Detection Against Binary-level Mutations(http://arxiv.org/abs/2310.03285)</code></li>
<li>Summary: <p>Toward robust malware detection, we explore the attack surface of existing
malware detection systems. We conduct root-cause analyses of the practical
binary-level black-box adversarial malware examples. Additionally, we uncover
the sensitivity of volatile features within the detection engines and exhibit
their exploitability. Highlighting volatile information channels within the
software, we introduce three software pre-processing steps to eliminate the
attack surface, namely, padding removal, software stripping, and inter-section
information resetting. Further, to counter the emerging section injection
attacks, we propose a graph-based section-dependent information extraction
scheme for software representation. The proposed scheme leverages aggregated
information within various sections in the software to enable robust malware
detection and mitigate adversarial settings. Our experimental results show that
traditional malware detection models are ineffective against adversarial
threats. However, the attack surface can be largely reduced by eliminating the
volatile information. Therefore, we propose simple-yet-effective methods to
mitigate the impacts of binary manipulation attacks. Overall, our graph-based
malware detection scheme can accurately detect malware with an area under the
curve score of 88.32\% and a score of 88.19% under a combination of binary
manipulation attacks, exhibiting the efficiency of our proposed scheme.
</p></li>
</ul>

<h3>Title: Certifiably Robust Graph Contrastive Learning. (arXiv:2310.03312v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03312">http://arxiv.org/abs/2310.03312</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03312]] Certifiably Robust Graph Contrastive Learning(http://arxiv.org/abs/2310.03312)</code></li>
<li>Summary: <p>Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph
representation learning method. However, it has been shown that GCL is
vulnerable to adversarial attacks on both the graph structure and node
attributes. Although empirical approaches have been proposed to enhance the
robustness of GCL, the certifiable robustness of GCL is still remain
unexplored. In this paper, we develop the first certifiably robust framework in
GCL. Specifically, we first propose a unified criteria to evaluate and certify
the robustness of GCL. We then introduce a novel technique, RES (Randomized
Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and
this certified robustness can be provably preserved in downstream tasks.
Furthermore, an effective training method is proposed for robust GCL. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed method in providing effective certifiable robustness and enhancing the
robustness of any GCL model. The source code of RES is available at
https://github.com/ventr1c/RES-GCL.
</p></li>
</ul>

<h3>Title: QuATON: Quantization Aware Training of Optical Neurons. (arXiv:2310.03049v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03049">http://arxiv.org/abs/2310.03049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03049]] QuATON: Quantization Aware Training of Optical Neurons(http://arxiv.org/abs/2310.03049)</code></li>
<li>Summary: <p>Optical neural architectures (ONAs) use coding elements with optimized
physical parameters to perform intelligent measurements. However, fabricating
ONAs while maintaining design performances is challenging. Limitations in
fabrication techniques often limit the realizable precision of the trained
parameters. Physical constraints may also limit the range of values the
physical parameters can hold. Thus, ONAs should be trained within the
implementable constraints. However, such physics-based constraints reduce the
training objective to a constrained optimization problem, making it harder to
optimize with existing gradient-based methods. To alleviate these critical
issues that degrade performance from simulation to realization we propose a
physics-informed quantization-aware training framework. Our approach accounts
for the physical constraints during the training process, leading to robust
designs. We evaluate our approach on an ONA proposed in the literature, named a
diffractive deep neural network (D2NN), for all-optical phase imaging and for
classification of phase objects. With extensive experiments on different
quantization levels and datasets, we show that our approach leads to ONA
designs that are robust to quantization noise.
</p></li>
</ul>

<h3>Title: Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems. (arXiv:2310.03055v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03055">http://arxiv.org/abs/2310.03055</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03055]] Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems(http://arxiv.org/abs/2310.03055)</code></li>
<li>Summary: <p>A modified LAB algorithm is introduced in this paper. It builds upon the
original LAB algorithm (Reddy et al. 2023), which is a socio-inspired algorithm
that models competitive and learning behaviours within a group, establishing
hierarchical roles. The proposed algorithm incorporates the roulette wheel
approach and a reduction factor introducing inter-group competition and
iteratively narrowing down the sample space. The algorithm is validated by
solving the benchmark test problems from CEC 2005 and CEC 2017. The solutions
are validated using standard statistical tests such as two-sided and pairwise
signed rank Wilcoxon test and Friedman rank test. The algorithm exhibited
improved and superior robustness as well as search space exploration
capabilities. Furthermore, a Clustering-Based Search Space Reduction (C-SSR)
method is proposed, making the algorithm capable to solve constrained problems.
The C-SSR method enables the algorithm to identify clusters of feasible
regions, satisfying the constraints and contributing to achieve the optimal
solution. This method demonstrates its effectiveness as a potential alternative
to traditional constraint handling techniques. The results obtained using the
Modified LAB algorithm are then compared with those achieved by other recent
metaheuristic algorithms.
</p></li>
</ul>

<h3>Title: FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent. (arXiv:2310.03156v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03156">http://arxiv.org/abs/2310.03156</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03156]] FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent(http://arxiv.org/abs/2310.03156)</code></li>
<li>Summary: <p>The theoretical landscape of federated learning (FL) undergoes rapid
evolution, but its practical application encounters a series of intricate
challenges, and hyperparameter optimization is one of these critical
challenges. Amongst the diverse adjustments in hyperparameters, the adaptation
of the learning rate emerges as a crucial component, holding the promise of
significantly enhancing the efficacy of FL systems. In response to this
critical need, this paper presents FedHyper, a novel hypergradient-based
learning rate adaptation algorithm specifically designed for FL. FedHyper
serves as a universal learning rate scheduler that can adapt both global and
local rates as the training progresses. In addition, FedHyper not only
showcases unparalleled robustness to a spectrum of initial learning rate
configurations but also significantly alleviates the necessity for laborious
empirical learning rate adjustments. We provide a comprehensive theoretical
analysis of FedHyper's convergence rate and conduct extensive experiments on
vision and language benchmark datasets. The results demonstrate that FEDHYPER
consistently converges 1.1-3x faster than FedAvg and the competing baselines
while achieving superior final accuracy. Moreover, FedHyper catalyzes a
remarkable surge in accuracy, augmenting it by up to 15% compared to FedAvg
under suboptimal initial learning rate settings.
</p></li>
</ul>

<h3>Title: Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions. (arXiv:2310.03195v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03195">http://arxiv.org/abs/2310.03195</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03195]] Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions(http://arxiv.org/abs/2310.03195)</code></li>
<li>Summary: <p>Machine scheduling aims to optimize job assignments to machines while
adhering to manufacturing rules and job specifications. This optimization leads
to reduced operational costs, improved customer demand fulfillment, and
enhanced production efficiency. However, machine scheduling remains a
challenging combinatorial problem due to its NP-hard nature. Deep Reinforcement
Learning (DRL), a key component of artificial general intelligence, has shown
promise in various domains like gaming and robotics. Researchers have explored
applying DRL to machine scheduling problems since 1995. This paper offers a
comprehensive review and comparison of DRL-based approaches, highlighting their
methodology, applications, advantages, and limitations. It categorizes these
approaches based on computational components: conventional neural networks,
encoder-decoder architectures, graph neural networks, and metaheuristic
algorithms. Our review concludes that DRL-based methods outperform exact
solvers, heuristics, and tabular reinforcement learning algorithms in terms of
computation speed and generating near-global optimal solutions. These DRL-based
approaches have been successfully applied to static and dynamic scheduling
across diverse machine environments and job characteristics. However, DRL-based
schedulers face limitations in handling complex operational constraints,
configurable multi-objective optimization, generalization, scalability,
interpretability, and robustness. Addressing these challenges will be a crucial
focus for future research in this field. This paper serves as a valuable
resource for researchers to assess the current state of DRL-based machine
scheduling and identify research gaps. It also aids experts and practitioners
in selecting the appropriate DRL approach for production scheduling.
</p></li>
</ul>

<h3>Title: Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03272">http://arxiv.org/abs/2310.03272</a></li>
<li>Code URL: https://github.com/graphmatching/graph-matching</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03272]] Network Alignment with Transferable Graph Autoencoders(http://arxiv.org/abs/2310.03272)</code></li>
<li>Summary: <p>Network alignment is the task of establishing one-to-one correspondences
between the nodes of different graphs and finds a plethora of applications in
high-impact domains. However, this task is known to be NP-hard in its general
form, and existing algorithms do not scale up as the size of the graphs
increases. To tackle both challenges we propose a novel generalized graph
autoencoder architecture, designed to extract powerful and robust node
embeddings, that are tailored to the alignment task. We prove that the
generated embeddings are associated with the eigenvalues and eigenvectors of
the graphs and can achieve more accurate alignment compared to classical
spectral methods. Our proposed framework also leverages transfer learning and
data augmentation to achieve efficient network alignment at a very large scale
without retraining. Extensive experiments on both network and sub-network
alignment with real-world graphs provide corroborating evidence supporting the
effectiveness and scalability of the proposed approach.
</p></li>
</ul>

<h3>Title: Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses. (arXiv:2310.03311v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03311">http://arxiv.org/abs/2310.03311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03311]] Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses(http://arxiv.org/abs/2310.03311)</code></li>
<li>Summary: <p>Variational dimensionality reduction methods are known for their high
accuracy, generative abilities, and robustness. These methods have many
theoretical justifications. Here we introduce a unifying principle rooted in
information theory to rederive and generalize existing variational methods and
design new ones. We base our framework on an interpretation of the multivariate
information bottleneck, in which two Bayesian networks are traded off against
one another. We interpret the first network as an encoder graph, which
specifies what information to keep when compressing the data. We interpret the
second network as a decoder graph, which specifies a generative model for the
data. Using this framework, we rederive existing dimensionality reduction
methods such as the deep variational information bottleneck (DVIB), beta
variational auto-encoders (beta-VAE), and deep variational canonical
correlation analysis (DVCCA). The framework naturally introduces a trade-off
parameter between compression and reconstruction in the DVCCA family of
algorithms, resulting in the new beta-DVCCA family. In addition, we derive a
new variational dimensionality reduction method, deep variational symmetric
informational bottleneck (DVSIB), which simultaneously compresses two variables
to preserve information between their compressed representations. We implement
all of these algorithms and evaluate their ability to produce shared low
dimensional latent spaces on a modified noisy MNIST dataset. We show that
algorithms that are better matched to the structure of the data (beta-DVCCA and
DVSIB) produce better latent spaces as measured by classification accuracy and
the dimensionality of the latent variables. We believe that this framework can
be used to unify other multi-view representation learning algorithms.
Additionally, it provides a straightforward framework for deriving
problem-specific loss functions.
</p></li>
</ul>

<h3>Title: The Blame Problem in Evaluating Local Explanations, and How to Tackle it. (arXiv:2310.03466v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03466">http://arxiv.org/abs/2310.03466</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03466]] The Blame Problem in Evaluating Local Explanations, and How to Tackle it(http://arxiv.org/abs/2310.03466)</code></li>
<li>Summary: <p>The number of local model-agnostic explanation techniques proposed has grown
rapidly recently. One main reason is that the bar for developing new
explainability techniques is low due to the lack of optimal evaluation
measures. Without rigorous measures, it is hard to have concrete evidence of
whether the new explanation techniques can significantly outperform their
predecessors. Our study proposes a new taxonomy for evaluating local
explanations: robustness, evaluation using ground truth from synthetic datasets
and interpretable models, model randomization, and human-grounded evaluation.
Using this proposed taxonomy, we highlight that all categories of evaluation
methods, except those based on the ground truth from interpretable models,
suffer from a problem we call the "blame problem." In our study, we argue that
this category of evaluation measure is a more reasonable method for evaluating
local model-agnostic explanations. However, we show that even this category of
evaluation measures has further limitations. The evaluation of local
explanations remains an open research problem.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Metaverse CAN: Embracing Continuous, Active, and Non-intrusive Biometric Authentication. (arXiv:2310.03162v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03162">http://arxiv.org/abs/2310.03162</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03162]] Metaverse CAN: Embracing Continuous, Active, and Non-intrusive Biometric Authentication(http://arxiv.org/abs/2310.03162)</code></li>
<li>Summary: <p>The Metaverse is a virtual world, an immersive experience, a new
human-computer interaction, built upon various advanced technologies. How to
protect Metaverse personal information and virtual properties is also facing
new challenges, such as new attacks and new expectations of user experiences.
While traditional methods (e.g., those employed in smartphone authentication)
generally pass the basic design considerations, they are repeatedly reported to
be either unsafe or inconvenient in the Metaverse. In this paper, we address
this discrepancy by introducing CAN: a new design consideration especially for
the Metaverse. Specifically, we focus on the legacy and novel biometric
authentication systems and evaluate them thoroughly with basic and CAN
considerations. We also propose an ear-based method as one example of CAN
systems. To conclude, a continuous, active and non-intrusive biometric system
is suggested for Metaverse authentication for its capability in continuous
sessions, against imposters, and immersive experience.
</p></li>
</ul>

<h2>steal</h2>
<h3>Title: Enhancing Exfiltration Path Analysis Using Reinforcement Learning. (arXiv:2310.03667v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03667">http://arxiv.org/abs/2310.03667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03667]] Enhancing Exfiltration Path Analysis Using Reinforcement Learning(http://arxiv.org/abs/2310.03667)</code></li>
<li>Summary: <p>Building on previous work using reinforcement learning (RL) focused on
identification of exfiltration paths, this work expands the methodology to
include protocol and payload considerations. The former approach to
exfiltration path discovery, where reward and state are associated specifically
with the determination of optimal paths, are presented with these additional
realistic characteristics to account for nuances in adversarial behavior. The
paths generated are enhanced by including communication payload and protocol
into the Markov decision process (MDP) in order to more realistically emulate
attributes of network based exfiltration events. The proposed method will help
emulate complex adversarial considerations such as the size of a payload being
exported over time or the protocol on which it occurs, as is the case where
threat actors steal data over long periods of time using system native ports or
protocols to avoid detection. As such, practitioners will be able to improve
identification of expected adversary behavior under various payload and
protocol assumptions more comprehensively.
</p></li>
</ul>

<h2>extraction</h2>
<h3>Title: PrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classification. (arXiv:2310.03517v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03517">http://arxiv.org/abs/2310.03517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03517]] PrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classification(http://arxiv.org/abs/2310.03517)</code></li>
<li>Summary: <p>Few-shot image classification has received considerable attention for
addressing the challenge of poor classification performance with limited
samples in novel classes. However, numerous studies have employed sophisticated
learning strategies and diversified feature extraction methods to address this
issue. In this paper, we propose our method called PrototypeFormer, which aims
to significantly advance traditional few-shot image classification approaches
by exploring prototype relationships. Specifically, we utilize a transformer
architecture to build a prototype extraction module, aiming to extract class
representations that are more discriminative for few-shot classification.
Additionally, during the model training process, we propose a contrastive
learning-based optimization approach to optimize prototype features in few-shot
learning scenarios. Despite its simplicity, the method performs remarkably
well, with no bells and whistles. We have experimented with our approach on
several popular few-shot image classification benchmark datasets, which shows
that our method outperforms all current state-of-the-art methods. In
particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way
1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with
accuracy of 7.27% and 8.72%, respectively. The code will be released later.
</p></li>
</ul>

<h3>Title: LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction. (arXiv:2310.03414v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03414">http://arxiv.org/abs/2310.03414</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03414]] LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction(http://arxiv.org/abs/2310.03414)</code></li>
<li>Summary: <p>Multi-document summarization is a challenging task due to its inherent
subjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4
among DUC-2004 reference summaries. In this work, we aim to enhance the
objectivity of news summarization by focusing on the main event of a group of
related news documents and presenting it coherently with sufficient context.
Our primary objective is to succinctly report the main event, ensuring that the
summary remains objective and informative. To achieve this, we employ an
extract-rewrite approach that incorporates a main-event biased
monotone-submodular function for content selection. This enables us to extract
the most crucial information related to the main event from the document
cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for
rewriting the extracted content into a coherent text. The evaluation using
objective metrics and human evaluators confirms the effectiveness of our
approach, as it surpasses potential baselines, demonstrating excellence in both
content coverage, coherence, and informativeness.
</p></li>
</ul>

<h3>Title: GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction. (arXiv:2310.03668v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03668">http://arxiv.org/abs/2310.03668</a></li>
<li>Code URL: https://github.com/hitz-zentroa/gollie</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03668]] GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction(http://arxiv.org/abs/2310.03668)</code></li>
<li>Summary: <p>Large Language Models (LLMs) combined with instruction tuning have made
significant progress when generalizing to unseen tasks. However, they have been
less successful in Information Extraction (IE), lagging behind task-specific
models. Typically, IE tasks are characterized by complex annotation guidelines
which describe the task and give examples to humans. Previous attempts to
leverage such information have failed, even with the largest models, as they
are not able to follow the guidelines out-of-the-box. In this paper we propose
GoLLIE (Guideline-following Large Language Model for IE), a model able to
improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to
comply with annotation guidelines. Comprehensive evaluation empirically
demonstrates that GoLLIE is able to generalize to and follow unseen guidelines,
outperforming previous attempts at zero-shot information extraction. The
ablation study shows that detailed guidelines is key for good results.
</p></li>
</ul>

<h3>Title: IoTScent: Enhancing Forensic Capabilities in Internet of Things Gateways. (arXiv:2310.03401v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03401">http://arxiv.org/abs/2310.03401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03401]] IoTScent: Enhancing Forensic Capabilities in Internet of Things Gateways(http://arxiv.org/abs/2310.03401)</code></li>
<li>Summary: <p>The widespread deployment of Consumer Internet of Things devices in proximity
to human activities makes them digital observers of our daily actions. This has
led to a new field of digital forensics, known as IoT Forensics, where digital
traces generated by IoT devices can serve as key evidence for forensic
investigations. Thus, there is a need to develop tools that can efficiently
acquire and store network traces from IoT ecosystems. This paper presents
IoTScent, an open-source IoT forensic tool that enables IoT gateways and Home
Automation platforms to perform IoT traffic capture and analysis. Unlike other
works focusing on IP-based protocols, IoTScent is specifically designed to
operate over IEEE 802.15.4-based traffic, which is the basis for many
IoT-specific protocols such as Zigbee, 6LoWPAN and Thread. IoTScent offers live
traffic capture and feature extraction capabilities, providing a framework for
forensic data collection that simplifies the task of setting up a data
collection pipeline, automating the data collection process, and providing
ready-made features that can be used for forensic evidence extraction. This
work provides a comprehensive description of the IoTScent tool, including a
practical use case that demonstrates the use of the tool to perform device
identification from Zigbee traffic. The study presented here significantly
contributes to the ongoing research in IoT Forensics by addressing the
challenges faced in the field and publicly releasing the IoTScent tool.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FLAIM: AIM-based Synthetic Data Generation in the Federated Setting. (arXiv:2310.03447v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03447">http://arxiv.org/abs/2310.03447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03447]] FLAIM: AIM-based Synthetic Data Generation in the Federated Setting(http://arxiv.org/abs/2310.03447)</code></li>
<li>Summary: <p>Preserving individual privacy while enabling collaborative data sharing is
crucial for organizations. Synthetic data generation is one solution, producing
artificial data that mirrors the statistical properties of private data. While
numerous techniques have been devised under differential privacy, they
predominantly assume data is centralized. However, data is often distributed
across multiple clients in a federated manner. In this work, we initiate the
study of federated synthetic tabular data generation. Building upon a SOTA
central method known as AIM, we present DistAIM and FLAIM. We show it is
straightforward to distribute AIM, extending a recent approach based on secure
multi-party computation which necessitates additional overhead, making it less
suited to federated scenarios. We then demonstrate that naively federating AIM
can lead to substantial degradation in utility under the presence of
heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach
that maintains a private proxy of heterogeneity. We simulate our methods across
a range of benchmark datasets under different degrees of heterogeneity and show
this can improve utility while reducing overhead.
</p></li>
</ul>

<h3>Title: Dual Prompt Tuning for Domain-Aware Federated Learning. (arXiv:2310.03103v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03103">http://arxiv.org/abs/2310.03103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03103]] Dual Prompt Tuning for Domain-Aware Federated Learning(http://arxiv.org/abs/2310.03103)</code></li>
<li>Summary: <p>Federated learning is a distributed machine learning paradigm that allows
multiple clients to collaboratively train a shared model with their local data.
Nonetheless, conventional federated learning algorithms often struggle to
generalize well due to the ubiquitous domain shift across clients. In this
work, we consider a challenging yet realistic federated learning scenario where
the training data of each client originates from different domains. We address
the challenges of domain shift by leveraging the technique of prompt learning,
and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT).
Specifically, Fed-DPT employs a pre-trained vision-language model and then
applies both visual and textual prompt tuning to facilitate domain adaptation
over decentralized data. Extensive experiments of Fed-DPT demonstrate its
significant effectiveness in domain-aware federated learning. With a
pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT
attains 68.4% average accuracy over six domains in the DomainNet dataset, which
improves the original CLIP by a large margin of 14.8%.
</p></li>
</ul>

<h3>Title: Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03123">http://arxiv.org/abs/2310.03123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03123]] Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models(http://arxiv.org/abs/2310.03123)</code></li>
<li>Summary: <p>With the blowout development of pre-trained models (PTMs), the efficient
tuning of these models for diverse downstream applications has emerged as a
pivotal research concern. Although recent investigations into prompt tuning
have provided promising avenues, three salient challenges persist: (1) memory
constraint: the continuous growth in the size of open-source PTMs renders
fine-tuning, even a fraction of their parameters, challenging for many
practitioners. (2) model privacy: existing PTMs often function as public API
services, with their parameters inaccessible for effective or tailored
fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates
high-quality datasets, which are typically localized and not shared to public.
To optimally harness each local dataset while navigating memory constraints and
preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT).
This innovative approach eschews reliance on parameter architectures and
private dataset access, instead capitalizing on a central server that aids
local users in collaboratively training a prompt generator through regular
aggregation. Local users leverage API-driven learning via a zero-order
optimizer, obviating the need for PTM deployment. Relative to extensive
fine-tuning, Fed-BBPT proficiently sidesteps memory challenges tied to PTM
storage and fine-tuning on local machines, tapping into comprehensive,
high-quality, yet private training datasets. A thorough evaluation across 40
datasets spanning CV and NLP tasks underscores the robustness of our proposed
model.
</p></li>
</ul>

<h3>Title: Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly. (arXiv:2310.03150v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03150">http://arxiv.org/abs/2310.03150</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03150]] Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly(http://arxiv.org/abs/2310.03150)</code></li>
<li>Summary: <p>Large Language Models (LLM) and foundation models are popular as they offer
new opportunities for individuals and businesses to improve natural language
processing, interact with data, and retrieve information faster. However,
training or fine-tuning LLMs requires a vast amount of data, which can be
challenging to access due to legal or technical restrictions and may require
private computing resources. Federated Learning (FL) is a solution designed to
overcome these challenges and expand data access for deep learning
applications.
</p>
<p>This paper takes a hardware-centric approach to explore how LLMs can be
brought to modern edge computing systems. Our study fine-tunes the FLAN-T5
model family, ranging from 80M to 3B parameters, using FL for a text
summarization task. We provide a micro-level hardware benchmark, compare the
model FLOP utilization to a state-of-the-art data center GPU, and study the
network utilization in realistic conditions. Our contribution is twofold:
First, we evaluate the current capabilities of edge computing systems and their
potential for LLM FL workloads. Second, by comparing these systems with a
data-center GPU, we demonstrate the potential for improvement and the next
steps toward achieving greater computational efficiency at the edge.
</p></li>
</ul>

<h3>Title: FedNAR: Federated Optimization with Normalized Annealing Regularization. (arXiv:2310.03163v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03163">http://arxiv.org/abs/2310.03163</a></li>
<li>Code URL: https://github.com/ljb121002/fednar</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03163]] FedNAR: Federated Optimization with Normalized Annealing Regularization(http://arxiv.org/abs/2310.03163)</code></li>
<li>Summary: <p>Weight decay is a standard technique to improve generalization performance in
modern deep neural network optimization, and is also widely adopted in
federated learning (FL) to prevent overfitting in local clients. In this paper,
we first explore the choices of weight decay and identify that weight decay
value appreciably influences the convergence of existing FL algorithms. While
preventing overfitting is crucial, weight decay can introduce a different
optimization goal towards the global objective, which is further amplified in
FL due to multiple local updates and heterogeneous data distribution. To
address this challenge, we develop {\it Federated optimization with Normalized
Annealing Regularization} (FedNAR), a simple yet effective and versatile
algorithmic plug-in that can be seamlessly integrated into any existing FL
algorithms. Essentially, we regulate the magnitude of each update by performing
co-clipping of the gradient and weight decay. We provide a comprehensive
theoretical analysis of FedNAR's convergence rate and conduct extensive
experiments on both vision and language datasets with different backbone
federated optimization algorithms. Our experimental results consistently
demonstrate that incorporating FedNAR into existing FL algorithms leads to
accelerated convergence and heightened model accuracy. Moreover, FedNAR
exhibits resilience in the face of various hyperparameter configurations.
Specifically, FedNAR has the ability to self-adjust the weight decay when the
initial specification is not optimal, while the accuracy of traditional FL
algorithms would markedly decline. Our codes are released at
\href{https://github.com/ljb121002/fednar}{https://github.com/ljb121002/fednar}.
</p></li>
</ul>

<h3>Title: Digital Ethics in Federated Learning. (arXiv:2310.03178v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03178">http://arxiv.org/abs/2310.03178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03178]] Digital Ethics in Federated Learning(http://arxiv.org/abs/2310.03178)</code></li>
<li>Summary: <p>The Internet of Things (IoT) consistently generates vast amounts of data,
sparking increasing concern over the protection of data privacy and the
limitation of data misuse. Federated learning (FL) facilitates collaborative
capabilities among multiple parties by sharing machine learning (ML) model
parameters instead of raw user data, and it has recently gained significant
attention for its potential in privacy preservation and learning efficiency
enhancement. In this paper, we highlight the digital ethics concerns that arise
when human-centric devices serve as clients in FL. More specifically,
challenges of game dynamics, fairness, incentive, and continuity arise in FL
due to differences in perspectives and objectives between clients and the
server. We analyze these challenges and their solutions from the perspectives
of both the client and the server, and through the viewpoints of centralized
and decentralized FL. Finally, we explore the opportunities in FL for
human-centric IoT as directions for future development.
</p></li>
</ul>

<h3>Title: Which mode is better for federated learning? Centralized or Decentralized. (arXiv:2310.03461v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03461">http://arxiv.org/abs/2310.03461</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03461]] Which mode is better for federated learning? Centralized or Decentralized(http://arxiv.org/abs/2310.03461)</code></li>
<li>Summary: <p>Both centralized and decentralized approaches have shown excellent
performance and great application value in federated learning (FL). However,
current studies do not provide sufficient evidence to show which one performs
better. Although from the optimization perspective, decentralized methods can
approach the comparable convergence of centralized methods with less
communication, its test performance has always been inefficient in empirical
studies. To comprehensively explore their behaviors in FL, we study their
excess risks, including the joint analysis of both optimization and
generalization. We prove that on smooth non-convex objectives, 1) centralized
FL (CFL) always generalizes better than decentralized FL (DFL); 2) from
perspectives of the excess risk and test error in CFL, adopting partial
participation is superior to full participation; and, 3) there is a necessary
requirement for the topology in DFL to avoid performance collapse as the
training scale increases. Based on some simple hardware metrics, we could
evaluate which framework is better in practice. Extensive experiments are
conducted on common setups in FL to validate that our theoretical analysis is
contextually valid in practical scenarios.
</p></li>
</ul>

<h3>Title: Solving a Class of Non-Convex Minimax Optimization in Federated Learning. (arXiv:2310.03613v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03613">http://arxiv.org/abs/2310.03613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03613]] Solving a Class of Non-Convex Minimax Optimization in Federated Learning(http://arxiv.org/abs/2310.03613)</code></li>
<li>Summary: <p>The minimax problems arise throughout machine learning applications, ranging
from adversarial training and policy evaluation in reinforcement learning to
AUROC maximization. To address the large-scale data challenges across multiple
clients with communication-efficient distributed training, federated learning
(FL) is gaining popularity. Many optimization algorithms for minimax problems
have been developed in the centralized setting (\emph{i.e.} single-machine).
Nonetheless, the algorithm for minimax problems under FL is still
underexplored. In this paper, we study a class of federated nonconvex minimax
optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and
reduce existing complexity results for the most common minimax problems. For
nonconvex-concave problems, we propose FedSGDA+ and reduce the communication
complexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave and
nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known
sample complexity of $O(\kappa^{3} N^{-1}\varepsilon^{-3})$ and the best-known
communication complexity of $O(\kappa^{2}\varepsilon^{-2})$. FedSGDA-M is the
first algorithm to match the best sample complexity $O(\varepsilon^{-3})$
achieved by the single-machine method under the nonconvex-strongly-concave
setting. Extensive experimental results on fair classification and AUROC
maximization show the efficiency of our algorithms.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Towards Unified Deep Image Deraining: A Survey and A New Benchmark. (arXiv:2310.03535v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03535">http://arxiv.org/abs/2310.03535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03535]] Towards Unified Deep Image Deraining: A Survey and A New Benchmark(http://arxiv.org/abs/2310.03535)</code></li>
<li>Summary: <p>Recent years have witnessed significant advances in image deraining due to
the kinds of effective image priors and deep learning models. As each deraining
approach has individual settings (e.g., training and test datasets, evaluation
criteria), how to fairly evaluate existing approaches comprehensively is not a
trivial task. Although existing surveys aim to review of image deraining
approaches comprehensively, few of them focus on providing unify evaluation
settings to examine the deraining capability and practicality evaluation. In
this paper, we provide a comprehensive review of existing image deraining
method and provide a unify evaluation setting to evaluate the performance of
image deraining methods. We construct a new high-quality benchmark named
HQ-RAIN to further conduct extensive evaluation, consisting of 5,000 paired
high-resolution synthetic images with higher harmony and realism. We also
discuss the existing challenges and highlight several future research
opportunities worth exploring. To facilitate the reproduction and tracking of
the latest deraining technologies for general users, we build an online
platform to provide the off-the-shelf toolkit, involving the large-scale
performance evaluation. This online platform and the proposed new benchmark are
publicly available and will be regularly updated at <a href="http://www.deraining.tech/.">this http URL</a>
</p></li>
</ul>

<h3>Title: Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data. (arXiv:2310.03146v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03146">http://arxiv.org/abs/2310.03146</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03146]] Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data(http://arxiv.org/abs/2310.03146)</code></li>
<li>Summary: <p>Traditional deep learning (DL) suffers from two core problems. Firstly, it
assumes training samples are independent and identically distributed. However,
numerous real-world datasets group samples by shared measurements (e.g., study
participants or cells), violating this assumption. In these scenarios, DL can
show compromised performance, limited generalization, and interpretability
issues, coupled with cluster confounding causing Type 1 and 2 errors. Secondly,
models are typically trained for overall accuracy, often neglecting
underrepresented groups and introducing biases in crucial areas like loan
approvals or determining health insurance rates, such biases can significantly
impact one's quality of life. To address both of these challenges
simultaneously, we present a mixed effects deep learning (MEDL) framework. MEDL
separately quantifies cluster-invariant fixed effects (FE) and cluster-specific
random effects (RE) through the introduction of: 1) a cluster adversary which
encourages the learning of cluster-invariant FE, 2) a Bayesian neural network
which quantifies the RE, and a mixing function combining the FE an RE into a
mixed-effect prediction. We marry this MEDL with adversarial debiasing, which
promotes equality-of-odds fairness across FE, RE, and ME predictions for
fairness-sensitive variables. We evaluated our approach using three datasets:
two from census/finance focusing on income classification and one from
healthcare predicting hospitalization duration, a regression task. Our
framework notably enhances fairness across all sensitive variables-increasing
fairness up to 82% for age, 43% for race, 86% for sex, and 27% for
marital-status. Besides promoting fairness, our method maintains the robust
performance and clarity of MEDL. It's versatile, suitable for various dataset
types and tasks, making it broadly applicable. Our GitHub repository houses the
implementation.
</p></li>
</ul>

<h3>Title: Rethinking Fairness for Human-AI Collaboration. (arXiv:2310.03647v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03647">http://arxiv.org/abs/2310.03647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03647]] Rethinking Fairness for Human-AI Collaboration(http://arxiv.org/abs/2310.03647)</code></li>
<li>Summary: <p>Existing approaches to algorithmic fairness aim to ensure equitable outcomes
if human decision-makers comply perfectly with algorithmic decisions. However,
perfect compliance with the algorithm is rarely a reality or even a desirable
outcome in human-AI collaboration. Yet, recent studies have shown that
selective compliance with fair algorithms can amplify discrimination relative
to the prior human policy. As a consequence, ensuring equitable outcomes
requires fundamentally different algorithmic design principles that ensure
robustness to the decision-maker's (a priori unknown) compliance pattern. We
define the notion of compliance-robustly fair algorithmic recommendations that
are guaranteed to (weakly) improve fairness in decisions, regardless of the
human's compliance pattern. We propose a simple optimization strategy to
identify the best performance-improving compliance-robustly fair policy.
However, we show that it may be infeasible to design algorithmic
recommendations that are simultaneously fair in isolation, compliance-robustly
fair, and more accurate than the human policy; thus, if our goal is to improve
the equity and accuracy of human-AI collaboration, it may not be desirable to
enforce traditional fairness constraints.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook. (arXiv:2310.03086v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03086">http://arxiv.org/abs/2310.03086</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03086]] Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook(http://arxiv.org/abs/2310.03086)</code></li>
<li>Summary: <p>Deep learning has become a powerful tool in computational biology,
revolutionising the analysis and interpretation of biological data over time.
In our article review, we delve into various aspects of deep learning in
computational biology. Specifically, we examine its history, advantages, and
challenges. Our focus is on two primary applications: DNA sequence
classification and prediction, as well as protein structure prediction from
sequence data. Additionally, we provide insights into the outlook for this
field. To fully harness the potential of deep learning in computational
biology, it is crucial to address the challenges that come with it. These
challenges include the requirement for large, labelled datasets and the
interpretability of deep learning models. The use of deep learning in the
analysis of DNA sequences has brought about a significant transformation in the
detection of genomic variants and the analysis of gene expression. This has
greatly contributed to the advancement of personalised medicine and drug
discovery. Convolutional neural networks (CNNs) have been shown to be highly
accurate in predicting genetic variations and gene expression levels. Deep
learning techniques are used for analysing epigenetic data, including DNA
methylation and histone modifications. This provides valuable insights into
metabolic conditions and gene regulation. The field of protein structure
prediction has been significantly impacted by deep learning, which has enabled
accurate determination of the three-dimensional shape of proteins and
prediction of their interactions. The future of deep learning in computational
biology looks promising. With the development of advanced deep learning models
and interpretation techniques, there is potential to overcome current
challenges and further our understanding of biological systems.
</p></li>
</ul>

<h3>Title: Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data. (arXiv:2310.03111v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03111">http://arxiv.org/abs/2310.03111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03111]] Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data(http://arxiv.org/abs/2310.03111)</code></li>
<li>Summary: <p>Characterizing the relationship between neural population activity and
behavioral data is a central goal of neuroscience. While latent variable models
(LVMs) are successful in describing high-dimensional time-series data, they are
typically only designed for a single type of data, making it difficult to
identify structure shared across different experimental data modalities. Here,
we address this shortcoming by proposing an unsupervised LVM which extracts
temporally evolving shared and independent latents for distinct, simultaneously
recorded experimental modalities. We do this by combining Gaussian Process
Factor Analysis (GPFA), an interpretable LVM for neural spiking data with
temporally smooth latent space, with Gaussian Process Variational Autoencoders
(GP-VAEs), which similarly use a GP prior to characterize correlations in a
latent space, but admit rich expressivity due to a deep neural network mapping
to observations. We achieve interpretability in our model by partitioning
latent variability into components that are either shared between or
independent to each modality. We parameterize the latents of our model in the
Fourier domain, and show improved latent identification using this approach
over standard GP-VAE methods. We validate our model on simulated multi-modal
data consisting of Poisson spike counts and MNIST images that scale and rotate
smoothly over time. We show that the multi-modal GP-VAE (MM-GPVAE) is able to
not only identify the shared and independent latent structure across modalities
accurately, but provides good reconstructions of both images and neural rates
on held-out trials. Finally, we demonstrate our framework on two real world
multi-modal experimental settings: Drosophila whole-brain calcium imaging
alongside tracked limb positions, and Manduca sexta spike train measurements
from ten wing muscles as the animal tracks a visual stimulus.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: EAG-RS: A Novel Explainability-guided ROI-Selection Framework for ASD Diagnosis via Inter-regional Relation Learning. (arXiv:2310.03404v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03404">http://arxiv.org/abs/2310.03404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03404]] EAG-RS: A Novel Explainability-guided ROI-Selection Framework for ASD Diagnosis via Inter-regional Relation Learning(http://arxiv.org/abs/2310.03404)</code></li>
<li>Summary: <p>Deep learning models based on resting-state functional magnetic resonance
imaging (rs-fMRI) have been widely used to diagnose brain diseases,
particularly autism spectrum disorder (ASD). Existing studies have leveraged
the functional connectivity (FC) of rs-fMRI, achieving notable classification
performance. However, they have significant limitations, including the lack of
adequate information while using linear low-order FC as inputs to the model,
not considering individual characteristics (i.e., different symptoms or varying
stages of severity) among patients with ASD, and the non-explainability of the
decision process. To cover these limitations, we propose a novel
explainability-guided region of interest (ROI) selection (EAG-RS) framework
that identifies non-linear high-order functional associations among brain
regions by leveraging an explainable artificial intelligence technique and
selects class-discriminative regions for brain disease identification. The
proposed framework includes three steps: (i) inter-regional relation learning
to estimate non-linear relations through random seed-based network masking,
(ii) explainable connection-wise relevance score estimation to explore
high-order relations between functional connections, and (iii) non-linear
high-order FC-based diagnosis-informative ROI selection and classifier learning
to identify ASD. We validated the effectiveness of our proposed method by
conducting experiments using the Autism Brain Imaging Database Exchange (ABIDE)
dataset, demonstrating that the proposed method outperforms other comparative
methods in terms of various evaluation metrics. Furthermore, we qualitatively
analyzed the selected ROIs and identified ASD subtypes linked to previous
neuroscientific studies.
</p></li>
</ul>

<h3>Title: CLASSify: A Web-Based Tool for Machine Learning. (arXiv:2310.03618v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03618">http://arxiv.org/abs/2310.03618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03618]] CLASSify: A Web-Based Tool for Machine Learning(http://arxiv.org/abs/2310.03618)</code></li>
<li>Summary: <p>Machine learning classification problems are widespread in bioinformatics,
but the technical knowledge required to perform model training, optimization,
and inference can prevent researchers from utilizing this technology. This
article presents an automated tool for machine learning classification problems
to simplify the process of training models and producing results while
providing informative visualizations and insights into the data. This tool
supports both binary and multiclass classification problems, and it provides
access to a variety of models and methods. Synthetic data can be generated
within the interface to fill missing values, balance class labels, or generate
entirely new datasets. It also provides support for feature evaluation and
generates explainability scores to indicate which features influence the output
the most. We present CLASSify, an open-source tool for simplifying the user
experience of solving classification problems without the need for knowledge of
machine learning.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models. (arXiv:2310.03270v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03270">http://arxiv.org/abs/2310.03270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03270]] EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models(http://arxiv.org/abs/2310.03270)</code></li>
<li>Summary: <p>Diffusion models have demonstrated remarkable capabilities in image synthesis
and related generative tasks. Nevertheless, their practicality for low-latency
real-world applications is constrained by substantial computational costs and
latency issues. Quantization is a dominant way to compress and accelerate
diffusion models, where post-training quantization (PTQ) and quantization-aware
training (QAT) are two main approaches, each bearing its own properties. While
PTQ exhibits efficiency in terms of both time and data usage, it may lead to
diminished performance in low bit-width. On the other hand, QAT can alleviate
performance degradation but comes with substantial demands on computational and
data resources. To capitalize on the advantages while avoiding their respective
drawbacks, we introduce a data-free and parameter-efficient fine-tuning
framework for low-bit diffusion models, dubbed EfficientDM, to achieve
QAT-level performance with PTQ-like efficiency. Specifically, we propose a
quantization-aware variant of the low-rank adapter (QALoRA) that can be merged
with model weights and jointly quantized to low bit-width. The fine-tuning
process distills the denoising capabilities of the full-precision model into
its quantized counterpart, eliminating the requirement for training data. We
also introduce scale-aware optimization and employ temporal learned step-size
quantization to further enhance performance. Extensive experimental results
demonstrate that our method significantly outperforms previous PTQ-based
diffusion models while maintaining similar time and data efficiency.
Specifically, there is only a marginal 0.05 sFID increase when quantizing both
weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to
QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization
speed with comparable generation quality.
</p></li>
</ul>

<h3>Title: Denoising Diffusion Step-aware Models. (arXiv:2310.03337v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03337">http://arxiv.org/abs/2310.03337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03337]] Denoising Diffusion Step-aware Models(http://arxiv.org/abs/2310.03337)</code></li>
<li>Summary: <p>Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for
data generation across various domains. However, a significant bottleneck is
the necessity for whole-network computation during every step of the generative
process, leading to high computational overheads. This paper presents a novel
framework, Denoising Diffusion Step-aware Models (DDSM), to address this
challenge. Unlike conventional approaches, DDSM employs a spectrum of neural
networks whose sizes are adapted according to the importance of each generative
step, as determined through evolutionary search. This step-wise network
variation effectively circumvents redundant computational efforts, particularly
in less critical steps, thereby enhancing the efficiency of the diffusion
model. Furthermore, the step-aware design can be seamlessly integrated with
other efficiency-geared diffusion models such as DDIMs and latent diffusion,
thus broadening the scope of computational savings. Empirical evaluations
demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61%
for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all
without compromising the generation quality. Our code and models will be
publicly available.
</p></li>
</ul>

<h3>Title: Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior. (arXiv:2310.03363v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03363">http://arxiv.org/abs/2310.03363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03363]] Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior(http://arxiv.org/abs/2310.03363)</code></li>
<li>Summary: <p>Speech-to-face generation is an intriguing area of research that focuses on
generating realistic facial images based on a speaker's audio speech. However,
state-of-the-art methods employing GAN-based architectures lack stability and
cannot generate realistic face images. To fill this gap, we propose a novel
speech-to-face generation framework, which leverages a Speech-Conditioned
Latent Diffusion Model, called SCLDM. To the best of our knowledge, this is the
first work to harness the exceptional modeling capabilities of diffusion models
for speech-to-face generation. Preserving the shared identity information
between speech and face is crucial in generating realistic results. Therefore,
we employ contrastive pre-training for both the speech encoder and the face
encoder. This pre-training strategy facilitates effective alignment between the
attributes of speech, such as age and gender, and the corresponding facial
characteristics in the face images. Furthermore, we tackle the challenge posed
by excessive diversity in the synthesis process caused by the diffusion model.
To overcome this challenge, we introduce the concept of residuals by
integrating a statistical face prior to the diffusion process. This addition
helps to eliminate the shared component across the faces and enhances the
subtle variations captured by the speech condition. Extensive quantitative,
qualitative, and user study experiments demonstrate that our method can produce
more realistic face images while preserving the identity of the speaker better
than state-of-the-art methods. Highlighting the notable enhancements, our
method demonstrates significant gains in all metrics on the AVSpeech dataset
and Voxceleb dataset, particularly noteworthy are the improvements of 32.17 and
32.72 on the cosine distance metric for the two datasets, respectively.
</p></li>
</ul>

<h3>Title: ACT-Net: Anchor-context Action Detection in Surgery Videos. (arXiv:2310.03377v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03377">http://arxiv.org/abs/2310.03377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03377]] ACT-Net: Anchor-context Action Detection in Surgery Videos(http://arxiv.org/abs/2310.03377)</code></li>
<li>Summary: <p>Recognition and localization of surgical detailed actions is an essential
component of developing a context-aware decision support system. However, most
existing detection algorithms fail to provide high-accuracy action classes even
having their locations, as they do not consider the surgery procedure's
regularity in the whole video. This limitation hinders their application.
Moreover, implementing the predictions in clinical applications seriously needs
to convey model confidence to earn entrustment, which is unexplored in surgical
action prediction. In this paper, to accurately detect fine-grained actions
that happen at every moment, we propose an anchor-context action detection
network (ACTNet), including an anchor-context detection (ACD) module and a
class conditional diffusion (CCD) module, to answer the following questions: 1)
where the actions happen; 2) what actions are; 3) how confidence predictions
are. Specifically, the proposed ACD module spatially and temporally highlights
the regions interacting with the extracted anchor in surgery video, which
outputs action location and its class distribution based on anchor-context
interactions. Considering the full distribution of action classes in videos,
the CCD module adopts a denoising diffusion-based generative model conditioned
on our ACD estimator to further reconstruct accurately the action predictions.
Moreover, we utilize the stochastic nature of the diffusion model outputs to
access model confidence for each prediction. Our method reports the
state-of-the-art performance, with improvements of 4.0% mAP against baseline on
the surgical video dataset.
</p></li>
</ul>

<h3>Title: FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators. (arXiv:2310.03420v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03420">http://arxiv.org/abs/2310.03420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03420]] FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators(http://arxiv.org/abs/2310.03420)</code></li>
<li>Summary: <p>Matching cross-modality features between images and point clouds is a
fundamental problem for image-to-point cloud registration. However, due to the
modality difference between images and points, it is difficult to learn robust
and discriminative cross-modality features by existing metric learning methods
for feature matching. Instead of applying metric learning on cross-modality
data, we propose to unify the modality between images and point clouds by
pretrained large-scale models first, and then establish robust correspondence
within the same modality. We show that the intermediate features, called
diffusion features, extracted by depth-to-image diffusion models are
semantically consistent between images and point clouds, which enables the
building of coarse but robust cross-modality correspondences. We further
extract geometric features on depth maps produced by the monocular depth
estimator. By matching such geometric features, we significantly improve the
accuracy of the coarse correspondences produced by diffusion features.
Extensive experiments demonstrate that without any task-specific training,
direct utilization of both features produces accurate image-to-point cloud
registration. On three public indoor and outdoor benchmarks, the proposed
method averagely achieves a 20.6 percent improvement in Inlier Ratio, a
three-fold higher Inlier Number, and a 48.6 percent improvement in Registration
Recall than existing state-of-the-arts.
</p></li>
</ul>

<h3>Title: Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion. (arXiv:2310.03502v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03502">http://arxiv.org/abs/2310.03502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03502]] Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion(http://arxiv.org/abs/2310.03502)</code></li>
<li>Summary: <p>Text-to-image generation is a significant domain in modern computer vision
and has achieved substantial improvements through the evolution of generative
architectures. Among these, there are diffusion-based models that have
demonstrated essential quality enhancements. These models are generally split
into two categories: pixel-level and latent-level approaches. We present
Kandinsky1, a novel exploration of latent diffusion architecture, combining the
principles of the image prior models with latent diffusion techniques. The
image prior model is trained separately to map text embeddings to image
embeddings of CLIP. Another distinct feature of the proposed model is the
modified MoVQ implementation, which serves as the image autoencoder component.
Overall, the designed model contains 3.3B parameters. We also deployed a
user-friendly demo system that supports diverse generative modes such as
text-to-image generation, image fusion, text and image fusion, image variations
generation, and text-guided inpainting/outpainting. Additionally, we released
the source code and checkpoints for the Kandinsky models. Experimental
evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking
our model as the top open-source performer in terms of measurable image
generation quality.
</p></li>
</ul>

<h3>Title: Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints. (arXiv:2310.03602v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03602">http://arxiv.org/abs/2310.03602</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03602]] Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints(http://arxiv.org/abs/2310.03602)</code></li>
<li>Summary: <p>Text-driven 3D indoor scene generation could be useful for gaming, film
industry, and AR/VR applications. However, existing methods cannot faithfully
capture the room layout, nor do they allow flexible editing of individual
objects in the room. To address these problems, we present Ctrl-Room, which is
able to generate convincing 3D rooms with designer-style layouts and
high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables
versatile interactive editing operations such as resizing or moving individual
furniture items. Our key insight is to separate the modeling of layouts and
appearance. %how to model the room that takes into account both scene texture
and geometry at the same time. To this end, Our proposed method consists of two
stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The
`Layout Generation Stage' trains a text-conditional diffusion model to learn
the layout distribution with our holistic scene code parameterization. Next,
the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a
vivid panoramic image of the room guided by the 3D scene layout and text
prompt. In this way, we achieve a high-quality 3D room with convincing layouts
and lively textures. Benefiting from the scene code parameterization, we can
easily edit the generated room model through our mask-guided editing module,
without expensive editing-specific training. Extensive experiments on the
Structured3D dataset demonstrate that our method outperforms existing methods
in producing more reasonable, view-consistent, and editable 3D rooms from
natural language prompts.
</p></li>
</ul>

<h3>Title: Learning Energy-Based Prior Model with Diffusion-Amortized MCMC. (arXiv:2310.03218v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03218">http://arxiv.org/abs/2310.03218</a></li>
<li>Code URL: https://github.com/yupeiyu98/diffusion-amortized-mcmc</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03218]] Learning Energy-Based Prior Model with Diffusion-Amortized MCMC(http://arxiv.org/abs/2310.03218)</code></li>
<li>Summary: <p>Latent space Energy-Based Models (EBMs), also known as energy-based priors,
have drawn growing interests in the field of generative modeling due to its
flexibility in the formulation and strong modeling power of the latent space.
However, the common practice of learning latent space EBMs with non-convergent
short-run MCMC for prior and posterior sampling is hindering the model from
further progress; the degenerate MCMC sampling quality in practice often leads
to degraded generation quality and instability in training, especially with
highly multi-modal and/or high-dimensional target distributions. To remedy this
sampling issue, in this paper we introduce a simple but effective
diffusion-based amortization method for long-run MCMC sampling and develop a
novel learning algorithm for the latent space EBM based on it. We provide
theoretical evidence that the learned amortization of MCMC is a valid long-run
MCMC sampler. Experiments on several image modeling benchmark datasets
demonstrate the superior performance of our method compared with strong
counterparts
</p></li>
</ul>

<h3>Title: Stochastic interpolants with data-dependent couplings. (arXiv:2310.03725v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03725">http://arxiv.org/abs/2310.03725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03725]] Stochastic interpolants with data-dependent couplings(http://arxiv.org/abs/2310.03725)</code></li>
<li>Summary: <p>Generative models inspired by dynamical transport of measure -- such as flows
and diffusions -- construct a continuous-time map between two probability
densities. Conventionally, one of these is the target density, only accessible
through samples, while the other is taken as a simple base density that is
data-agnostic. In this work, using the framework of stochastic interpolants, we
formalize how to \textit{couple} the base and the target densities. This
enables us to incorporate information about class labels or continuous
embeddings to construct dynamical transport maps that serve as conditional
generative models. We show that these transport maps can be learned by solving
a simple square loss regression problem analogous to the standard independent
setting. We demonstrate the usefulness of constructing dependent couplings in
practice through experiments in super-resolution and in-painting.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition. (arXiv:2310.03108v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03108">http://arxiv.org/abs/2310.03108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03108]] Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition(http://arxiv.org/abs/2310.03108)</code></li>
<li>Summary: <p>Video violence recognition based on deep learning concerns accurate yet
scalable human violence recognition. Currently, most state-of-the-art video
violence recognition studies use CNN-based models to represent and categorize
videos. However, recent studies suggest that pre-trained transformers are more
accurate than CNN-based models on various video analysis benchmarks. Yet these
models are not thoroughly evaluated for video violence recognition. This paper
introduces a novel transformer-based Mixture of Experts (MoE) video violence
recognition system. Through an intelligent combination of large vision
transformers and efficient transformer architectures, the proposed system not
only takes advantage of the vision transformer architecture but also reduces
the cost of utilizing large vision transformers. The proposed architecture
maximizes violence recognition system accuracy while actively reducing
computational costs through a reinforcement learning-based router. The
empirical results show the proposed MoE architecture's superiority over
CNN-based models by achieving 92.4% accuracy on the RWF dataset.
</p></li>
</ul>

<h3>Title: A Complementary Global and Local Knowledge Network for Ultrasound denoising with Fine-grained Refinement. (arXiv:2310.03402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03402">http://arxiv.org/abs/2310.03402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03402]] A Complementary Global and Local Knowledge Network for Ultrasound denoising with Fine-grained Refinement(http://arxiv.org/abs/2310.03402)</code></li>
<li>Summary: <p>Ultrasound imaging serves as an effective and non-invasive diagnostic tool
commonly employed in clinical examinations. However, the presence of speckle
noise in ultrasound images invariably degrades image quality, impeding the
performance of subsequent tasks, such as segmentation and classification.
Existing methods for speckle noise reduction frequently induce excessive image
smoothing or fail to preserve detailed information adequately. In this paper,
we propose a complementary global and local knowledge network for ultrasound
denoising with fine-grained refinement. Initially, the proposed architecture
employs the L-CSwinTransformer as encoder to capture global information,
incorporating CNN as decoder to fuse local features. We expand the resolution
of the feature at different stages to extract more global information compared
to the original CSwinTransformer. Subsequently, we integrate Fine-grained
Refinement Block (FRB) within the skip-connection stage to further augment
features. We validate our model on two public datasets, HC18 and BUSI.
Experimental results demonstrate that our model can achieve competitive
performance in both quantitative metrics and visual performance. Our code will
be available at https://github.com/AAlkaid/USDenoising.
</p></li>
</ul>

<h3>Title: Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization. (arXiv:2310.03456v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03456">http://arxiv.org/abs/2310.03456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03456]] Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization(http://arxiv.org/abs/2310.03456)</code></li>
<li>Summary: <p>Temporal Action Localization (TAL) aims to identify actions' start, end, and
class labels in untrimmed videos. While recent advancements using transformer
networks and Feature Pyramid Networks (FPN) have enhanced visual feature
recognition in TAL tasks, less progress has been made in the integration of
audio features into such frameworks. This paper introduces the Multi-Resolution
Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge
audio-visual data across different temporal resolutions. Central to our
approach is a hierarchical gated cross-attention mechanism, which discerningly
weighs the importance of audio information at diverse temporal scales. Such a
technique not only refines the precision of regression boundaries but also
bolsters classification confidence. Importantly, MRAV-FF is versatile, making
it compatible with existing FPN TAL architectures and offering a significant
enhancement in performance when audio data is available.
</p></li>
</ul>

<h3>Title: Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery. (arXiv:2310.03513v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03513">http://arxiv.org/abs/2310.03513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03513]] Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery(http://arxiv.org/abs/2310.03513)</code></li>
<li>Summary: <p>Self-supervised learning (SSL) models have recently demonstrated remarkable
performance across various tasks, including image segmentation. This study
delves into the emergent characteristics of the Self-Distillation with No
Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR)
imagery. We pre-train a vision transformer (ViT)-based DINO model using
unlabeled SAR data, and later fine-tune the model to predict high-resolution
land cover maps. We rigorously evaluate the utility of attention maps generated
by the ViT backbone, and compare them with the model's token embedding space.
We observe a small improvement in model performance with pre-training compared
to training from scratch, and discuss the limitations and opportunities of SSL
for remote sensing and land cover segmentation. Beyond small performance
increases, we show that ViT attention maps hold great intrinsic value for
remote sensing, and could provide useful inputs to other algorithms. With this,
our work lays the ground-work for bigger and better SSL models for Earth
Observation.
</p></li>
</ul>

<h3>Title: Drag View: Generalizable Novel View Synthesis with Unposed Imagery. (arXiv:2310.03704v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03704">http://arxiv.org/abs/2310.03704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03704]] Drag View: Generalizable Novel View Synthesis with Unposed Imagery(http://arxiv.org/abs/2310.03704)</code></li>
<li>Summary: <p>We introduce DragView, a novel and interactive framework for generating novel
views of unseen scenes. DragView initializes the new view from a single source
image, and the rendering is supported by a sparse set of unposed multi-view
images, all seamlessly executed within a single feed-forward pass. Our approach
begins with users dragging a source view through a local relative coordinate
system. Pixel-aligned features are obtained by projecting the sampled 3D points
along the target ray onto the source view. We then incorporate a view-dependent
modulation layer to effectively handle occlusion during the projection.
Additionally, we broaden the epipolar attention mechanism to encompass all
source pixels, facilitating the aggregation of initialized coordinate-aligned
point features from other unposed views. Finally, we employ another transformer
to decode ray features into final pixel intensities. Crucially, our framework
does not rely on either 2D prior models or the explicit estimation of camera
poses. During testing, DragView showcases the capability to generalize to new
scenes unseen during training, also utilizing only unposed support images,
enabling the generation of photo-realistic new views characterized by flexible
camera trajectories. In our experiments, we conduct a comprehensive comparison
of the performance of DragView with recent scene representation networks
operating under pose-free conditions, as well as with generalizable NeRFs
subject to noisy test camera poses. DragView consistently demonstrates its
superior performance in view synthesis quality, while also being more
user-friendly. Project page: https://zhiwenfan.github.io/DragView/.
</p></li>
</ul>

<h3>Title: Can Language Models Employ the Socratic Method? Experiments with Code Debugging. (arXiv:2310.03210v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03210">http://arxiv.org/abs/2310.03210</a></li>
<li>Code URL: https://github.com/taisazero/socratic-debugging-benchmark</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03210]] Can Language Models Employ the Socratic Method? Experiments with Code Debugging(http://arxiv.org/abs/2310.03210)</code></li>
<li>Summary: <p>When employing the Socratic method of teaching, instructors guide students
toward solving a problem on their own rather than providing the solution
directly. While this strategy can substantially improve learning outcomes, it
is usually time-consuming and cognitively demanding. Automated Socratic
conversational agents can augment human instruction and provide the necessary
scale, however their development is hampered by the lack of suitable data for
training and evaluation. In this paper, we introduce a manually created dataset
of multi-turn Socratic advice that is aimed at helping a novice programmer fix
buggy solutions to simple computational problems. The dataset is then used for
benchmarking the Socratic debugging abilities of a number of language models,
ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5
to zero-shot and chain of thought prompting of the much larger GPT-4. The code
and datasets are made freely available for research at the link below.
https://github.com/taisazero/socratic-debugging-benchmark
</p></li>
</ul>

<h3>Title: Neural Language Model Pruning for Automatic Speech Recognition. (arXiv:2310.03424v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03424">http://arxiv.org/abs/2310.03424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03424]] Neural Language Model Pruning for Automatic Speech Recognition(http://arxiv.org/abs/2310.03424)</code></li>
<li>Summary: <p>We study model pruning methods applied to Transformer-based neural network
language models for automatic speech recognition. We explore three aspects of
the pruning frame work, namely criterion, method and scheduler, analyzing their
contribution in terms of accuracy and inference speed. To the best of our
knowledge, such in-depth analyses on large-scale recognition systems has not
been reported in the literature. In addition, we propose a variant of low-rank
approximation suitable for incrementally compressing models, and delivering
multiple models with varied target sizes. Among other results, we show that a)
data-driven pruning outperforms magnitude-driven in several scenarios; b)
incremental pruning achieves higher accuracy compared to one-shot pruning,
especially when targeting smaller sizes; and c) low-rank approximation presents
the best trade-off between size reduction and inference speed-up for moderate
compression.
</p></li>
</ul>

<h3>Title: DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. (arXiv:2310.03686v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03686">http://arxiv.org/abs/2310.03686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03686]] DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers(http://arxiv.org/abs/2310.03686)</code></li>
<li>Summary: <p>In recent years, many interpretability methods have been proposed to help
interpret the internal states of Transformer-models, at different levels of
precision and complexity. Here, to analyze encoder-decoder Transformers, we
propose a simple, new method: DecoderLens. Inspired by the LogitLens (for
decoder-only Transformers), this method involves allowing the decoder to
cross-attend representations of intermediate encoder layers instead of using
the final encoder output, as is normally done in encoder-decoder models. The
method thus maps previously uninterpretable vector representations to
human-interpretable sequences of words or symbols. We report results from the
DecoderLens applied to models trained on question answering, logical reasoning,
speech recognition and machine translation. The DecoderLens reveals several
specific subtasks that are solved at low or intermediate layers, shedding new
light on the information flow inside the encoder component of this important
class of models.
</p></li>
</ul>

<h3>Title: FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03605">http://arxiv.org/abs/2310.03605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03605]] FASER: Binary Code Similarity Search through the use of Intermediate Representations(http://arxiv.org/abs/2310.03605)</code></li>
<li>Summary: <p>Being able to identify functions of interest in cross-architecture software
is useful whether you are analysing for malware, securing the software supply
chain or conducting vulnerability research. Cross-Architecture Binary Code
Similarity Search has been explored in numerous studies and has used a wide
range of different data sources to achieve its goals. The data sources
typically used draw on common structures derived from binaries such as function
control flow graphs or binary level call graphs, the output of the disassembly
process or the outputs of a dynamic analysis approach. One data source which
has received less attention is binary intermediate representations. Binary
Intermediate representations possess two interesting properties: they are cross
architecture by their very nature and encode the semantics of a function
explicitly to support downstream usage. Within this paper we propose Function
as a String Encoded Representation (FASER) which combines long document
transformers with the use of intermediate representations to create a model
capable of cross architecture function search without the need for manual
feature engineering, pre-training or a dynamic analysis step. We compare our
approach against a series of baseline approaches for two tasks; A general
function search task and a targeted vulnerability search task. Our approach
demonstrates strong performance across both tasks, performing better than all
baseline approaches.
</p></li>
</ul>

<h3>Title: Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03052">http://arxiv.org/abs/2310.03052</a></li>
<li>Code URL: https://github.com/cosmoquester/memoria</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03052]] Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing(http://arxiv.org/abs/2310.03052)</code></li>
<li>Summary: <p>Transformers have demonstrated their success in various domains and tasks.
However, Transformers struggle with long input sequences due to their limited
capacity. While one solution is to increase input length, endlessly stretching
the length is unrealistic. Furthermore, humans selectively remember and use
only relevant information from inputs, unlike Transformers which process all
raw data from start to end. We introduce Memoria, a general memory network that
applies Hebbian theory which is a major theory explaining human memory
formulation to enhance long-term dependencies in neural networks. Memoria
stores and retrieves information called engram at multiple memory levels of
working memory, short-term memory, and long-term memory, using connection
weights that change according to Hebb's rule. Through experiments with popular
Transformer-based models like BERT and GPT, we present that Memoria
significantly improves the ability to consider long-term dependencies in
various tasks. Results show that Memoria outperformed existing methodologies in
sorting and language modeling, and long text classification.
</p></li>
</ul>

<h3>Title: Neural architecture impact on identifying temporally extended Reinforcement Learning tasks. (arXiv:2310.03161v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03161">http://arxiv.org/abs/2310.03161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03161]] Neural architecture impact on identifying temporally extended Reinforcement Learning tasks(http://arxiv.org/abs/2310.03161)</code></li>
<li>Summary: <p>Inspired by recent developments in attention models for image classification
and natural language processing, we present various Attention based
architectures in reinforcement learning (RL) domain, capable of performing well
on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep
Reinforcement learning techniques in various fields like robotics, gaming and
healthcare, they suffer from a major drawback that neural networks are
difficult to interpret. We try to get around this problem with the help of
Attention based models. In Attention based models, extracting and overlaying of
attention map onto images allows for direct observation of information used by
agent to select actions and easier interpretation of logic behind the chosen
actions. Our models in addition to playing well on gym-Atari environments, also
provide insights on how agent perceives its environment. In addition, motivated
by recent developments in attention based video-classification models using
Vision Transformer, we come up with an architecture based on Vision
Transformer, for image-based RL domain too. Compared to previous works in
Vision Transformer, our model is faster to train and requires fewer
computational resources. 3
</p></li>
</ul>

<h3>Title: TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03223">http://arxiv.org/abs/2310.03223</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03223]] TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design(http://arxiv.org/abs/2310.03223)</code></li>
<li>Summary: <p>We seek to automate the generation of drug-like compounds conditioned to
specific protein pocket targets. Most current methods approximate the
protein-molecule distribution of a finite dataset and, therefore struggle to
generate molecules with significant binding improvement over the training
dataset. We instead frame the pocket-conditioned molecular generation task as
an RL problem and develop TacoGFN, a target conditional Generative Flow Network
model. Our method is explicitly encouraged to generate molecules with desired
properties as opposed to fitting on a pre-existing data distribution. To this
end, we develop transformer-based docking score prediction to speed up docking
score computation and propose TacoGFN to explore molecule space efficiently.
Furthermore, we incorporate several rounds of active learning where generated
samples are queried using a docking oracle to improve the docking score
prediction. This approach allows us to accurately explore as much of the
molecule landscape as we can afford computationally. Empirically, molecules
generated using TacoGFN and its variants significantly outperform all baseline
methods across every property (Docking score, QED, SA, Lipinski), while being
orders of magnitude faster.
</p></li>
</ul>

<h3>Title: Molecule Design by Latent Prompt Transformer. (arXiv:2310.03253v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03253">http://arxiv.org/abs/2310.03253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03253]] Molecule Design by Latent Prompt Transformer(http://arxiv.org/abs/2310.03253)</code></li>
<li>Summary: <p>This paper proposes a latent prompt Transformer model for solving challenging
optimization problems such as molecule design, where the goal is to find
molecules with optimal values of a target chemical or biological property that
can be computed by an existing software. Our proposed model consists of three
components. (1) A latent vector whose prior distribution is modeled by a Unet
transformation of a Gaussian white noise vector. (2) A molecule generation
model that generates the string-based representation of molecule conditional on
the latent vector in (1). We adopt the causal Transformer model that takes the
latent vector in (1) as prompt. (3) A property prediction model that predicts
the value of the target property of a molecule based on a non-linear regression
on the latent vector in (1). We call the proposed model the latent prompt
Transformer model. After initial training of the model on existing molecules
and their property values, we then gradually shift the model distribution
towards the region that supports desired values of the target property for the
purpose of molecule design. Our experiments show that our proposed model
achieves state of the art performances on several benchmark molecule design
tasks.
</p></li>
</ul>

<h3>Title: LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03294">http://arxiv.org/abs/2310.03294</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03294]] LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers(http://arxiv.org/abs/2310.03294)</code></li>
<li>Summary: <p>Increasing the context length of large language models (LLMs) unlocks
fundamentally new capabilities, but also significantly increases the memory
footprints of training. Previous model-parallel systems such as Megatron-LM
partition and compute different attention heads in parallel, resulting in large
communication volumes, so they cannot scale beyond the number of attention
heads, thereby hindering its adoption. In this paper, we introduce a new
approach, LightSeq, for long-context LLMs training. LightSeq has many notable
advantages. First, LightSeq partitions over the sequence dimension, hence is
agnostic to model architectures and readily applicable for models with varying
numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query
attention. Second, LightSeq not only requires up to 4.7x less communication
than Megatron-LM on popular LLMs but also overlaps the communication with
computation. To further reduce the training time, LightSeq features a novel
gradient checkpointing scheme to bypass an forward computation for
memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants
with sequence lengths from 32K to 512K. Through comprehensive experiments on
single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x
end-to-end speedup, and a 2-8x longer sequence length on models with fewer
heads, compared to Megatron-LM. Codes will be available at
https://github.com/RulinShao/LightSeq.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models. (arXiv:2310.03291v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03291">http://arxiv.org/abs/2310.03291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03291]] SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models(http://arxiv.org/abs/2310.03291)</code></li>
<li>Summary: <p>In this paper, we propose ``SimVLG'', a streamlined framework for the
pre-training of computationally intensive vision-language generative models,
leveraging frozen pre-trained large language models (LLMs). The prevailing
paradigm in vision-language pre-training (VLP) typically involves a two-stage
optimization process: an initial resource-intensive phase dedicated to
general-purpose vision-language representation learning, aimed at extracting
and consolidating pertinent visual features, followed by a subsequent phase
focusing on end-to-end alignment between visual and linguistic modalities. Our
one-stage, single-loss framework circumvents the aforementioned computationally
demanding first stage of training by gradually merging similar visual tokens
during training. This gradual merging process effectively compacts the visual
information while preserving the richness of semantic content, leading to fast
convergence without sacrificing performance. Our experiments show that our
approach can speed up the training of vision-language models by a factor
$\times 5$ without noticeable impact on the overall performance. Additionally,
we show that our models can achieve comparable performance to current
vision-language models with only $1/10$ of the data. Finally, we demonstrate
how our image-text models can be easily adapted to video-language generative
tasks through a novel soft attentive temporal token merging modules.
</p></li>
</ul>

<h3>Title: Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference. (arXiv:2310.03184v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03184">http://arxiv.org/abs/2310.03184</a></li>
<li>Code URL: https://github.com/digitalharborfoundation/rag-for-math-qa</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03184]] Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference(http://arxiv.org/abs/2310.03184)</code></li>
<li>Summary: <p>For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.
</p></li>
</ul>

<h3>Title: Learning Energy Decompositions for Partial Inference of GFlowNets. (arXiv:2310.03301v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03301">http://arxiv.org/abs/2310.03301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03301]] Learning Energy Decompositions for Partial Inference of GFlowNets(http://arxiv.org/abs/2310.03301)</code></li>
<li>Summary: <p>This paper studies generative flow networks (GFlowNets) to sample objects
from the Boltzmann energy distribution via a sequence of actions. In
particular, we focus on improving GFlowNet with partial inference: training
flow functions with the evaluation of the intermediate states or transitions.
To this end, the recently developed forward-looking GFlowNet reparameterizes
the flow functions based on evaluating the energy of intermediate states.
However, such an evaluation of intermediate energies may (i) be too expensive
or impossible to evaluate and (ii) even provide misleading training signals
under large energy fluctuations along the sequence of actions. To resolve this
issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our
main idea is to (i) decompose the energy of an object into learnable potential
functions defined on state transitions and (ii) reparameterize the flow
functions using the potential functions. In particular, to produce informative
local credits, we propose to regularize the potential to change smoothly over
the sequence of actions. It is also noteworthy that training GFlowNet with our
learned potential can preserve the optimal policy. We empirically verify the
superiority of LED-GFN in five problems including the generation of
unstructured and maximum independent sets, molecular graphs, and RNA sequences.
</p></li>
</ul>

<h3>Title: Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03419">http://arxiv.org/abs/2310.03419</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03419]] Pre-Training and Fine-Tuning Generative Flow Networks(http://arxiv.org/abs/2310.03419)</code></li>
<li>Summary: <p>Generative Flow Networks (GFlowNets) are amortized samplers that learn
stochastic policies to sequentially generate compositional objects from a given
unnormalized reward distribution. They can generate diverse sets of high-reward
objects, which is an important consideration in scientific discovery tasks.
However, as they are typically trained from a given extrinsic reward function,
it remains an important open challenge about how to leverage the power of
pre-training and train GFlowNets in an unsupervised fashion for efficient
adaptation to downstream tasks. Inspired by recent successes of unsupervised
pre-training in various domains, we introduce a novel approach for reward-free
pre-training of GFlowNets. By framing the training as a self-supervised
problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to
explore the candidate space. Specifically, OC-GFN learns to reach any targeted
outcomes, akin to goal-conditioned policies in reinforcement learning. We show
that the pre-trained OC-GFN model can allow for a direct extraction of a policy
capable of sampling from any new reward functions in downstream tasks.
Nonetheless, adapting OC-GFN on a downstream task-specific reward involves an
intractable marginalization over possible outcomes. We propose a novel way to
approximate this marginalization by learning an amortized predictor enabling
efficient fine-tuning. Extensive experimental results validate the efficacy of
our approach, demonstrating the effectiveness of pre-training the OC-GFN, and
its ability to swiftly adapt to downstream tasks and discover modes more
efficiently. This work may serve as a foundation for further exploration of
pre-training strategies in the context of GFlowNets.
</p></li>
</ul>

<h3>Title: Multimarginal generative modeling with stochastic interpolants. (arXiv:2310.03695v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03695">http://arxiv.org/abs/2310.03695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03695]] Multimarginal generative modeling with stochastic interpolants(http://arxiv.org/abs/2310.03695)</code></li>
<li>Summary: <p>Given a set of $K$ probability densities, we consider the multimarginal
generative modeling problem of learning a joint distribution that recovers
these densities as marginals. The structure of this joint distribution should
identify multi-way correspondences among the prescribed marginals. We formalize
an approach to this task within a generalization of the stochastic interpolant
framework, leading to efficient learning algorithms built upon dynamical
transport of measure. Our generative models are defined by velocity and score
fields that can be characterized as the minimizers of simple quadratic
objectives, and they are defined on a simplex that generalizes the time
variable in the usual dynamical transport framework. The resulting transport on
the simplex is influenced by all marginals, and we show that multi-way
correspondences can be extracted. The identification of such correspondences
has applications to style transfer, algorithmic fairness, and data
decorruption. In addition, the multimarginal perspective enables an efficient
algorithm for reducing the dynamical transport cost in the ordinary
two-marginal setting. We demonstrate these capacities with several numerical
examples.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03211">http://arxiv.org/abs/2310.03211</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03211]] On the Performance of Multimodal Language Models(http://arxiv.org/abs/2310.03211)</code></li>
<li>Summary: <p>Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
</p></li>
</ul>

<h3>Title: Investigating the Limitation of CLIP Models: The Worst-Performing Categories. (arXiv:2310.03324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03324">http://arxiv.org/abs/2310.03324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03324]] Investigating the Limitation of CLIP Models: The Worst-Performing Categories(http://arxiv.org/abs/2310.03324)</code></li>
<li>Summary: <p>Contrastive Language-Image Pre-training (CLIP) provides a foundation model by
integrating natural language into visual concepts, enabling zero-shot
recognition on downstream tasks. It is usually expected that satisfactory
overall accuracy can be achieved across numerous domains through well-designed
textual prompts. However, we found that their performance in the worst
categories is significantly inferior to the overall performance. For example,
on ImageNet, there are a total of 10 categories with class-wise accuracy as low
as 0\%, even though the overall performance has achieved 64.1\%. This
phenomenon reveals the potential risks associated with using CLIP models,
particularly in risk-sensitive applications where specific categories hold
significant importance. To address this issue, we investigate the alignment
between the two modalities in the CLIP model and propose the Class-wise
Matching Margin (\cmm) to measure the inference confusion. \cmm\ can
effectively identify the worst-performing categories and estimate the potential
performance of the candidate prompts. We further query large language models to
enrich descriptions of worst-performing categories and build a weighted
ensemble to highlight the efficient prompts. Experimental results clearly
verify the effectiveness of our proposal, where the accuracy on the worst-10
categories on ImageNet is boosted to 5.2\%, without manual prompt engineering,
laborious optimization, or access to labeled validation data.
</p></li>
</ul>

<h3>Title: How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03031">http://arxiv.org/abs/2310.03031</a></li>
<li>Code URL: https://github.com/Ognatai/bias_chatGPT</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03031]] How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses(http://arxiv.org/abs/2310.03031)</code></li>
<li>Summary: <p>With the introduction of ChatGPT, OpenAI made large language models (LLM)
accessible to users with limited IT expertise. However, users with no
background in natural language processing (NLP) might lack a proper
understanding of LLMs. Thus the awareness of their inherent limitations, and
therefore will take the systems' output at face value. In this paper, we
systematically analyse prompts and the generated responses to identify possible
problematic issues with a special focus on gender biases, which users need to
be aware of when processing the system's output. We explore how ChatGPT reacts
in English and German if prompted to answer from a female, male, or neutral
perspective. In an in-depth investigation, we examine selected prompts and
analyse to what extent responses differ if the system is prompted several times
in an identical way. On this basis, we show that ChatGPT is indeed useful for
helping non-IT users draft texts for their daily work. However, it is
absolutely crucial to thoroughly check the system's responses for biases as
well as for syntactic and grammatical mistakes.
</p></li>
</ul>

<h3>Title: How FaR Are Large Language Models From Agents with Theory-of-Mind?. (arXiv:2310.03051v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03051">http://arxiv.org/abs/2310.03051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03051]] How FaR Are Large Language Models From Agents with Theory-of-Mind?(http://arxiv.org/abs/2310.03051)</code></li>
<li>Summary: <p>"Thinking is for Doing." Humans can infer other people's mental states from
observations--an ability called Theory-of-Mind (ToM)--and subsequently act
pragmatically on those inferences. Existing question answering benchmarks such
as ToMi ask models questions to make inferences about beliefs of characters in
a story, but do not test whether models can then use these inferences to guide
their actions. We propose a new evaluation paradigm for large language models
(LLMs): Thinking for Doing (T4D), which requires models to connect inferences
about others' mental states to actions in social scenarios. Experiments on T4D
demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking
characters' beliefs in stories, but they struggle to translate this capability
into strategic action. Our analysis reveals the core challenge for LLMs lies in
identifying the implicit inferences about mental states without being
explicitly asked about as in ToMi, that lead to choosing the correct action in
T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee
and Reflect (FaR), which provides a reasoning structure that encourages LLMs to
anticipate future challenges and reason about potential actions. FaR boosts
GPT-4's performance from 50% to 71% on T4D, outperforming other prompting
methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to
diverse out-of-distribution story structures and scenarios that also require
ToM inferences to choose an action, consistently outperforming other methods
including few-shot in-context learning.
</p></li>
</ul>

<h3>Title: Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03094">http://arxiv.org/abs/2310.03094</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03094]] Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning(http://arxiv.org/abs/2310.03094)</code></li>
<li>Summary: <p>Large language models (LLMs) such as GPT-4 have exhibited remarkable
performance in a variety of tasks, but this strong performance often comes with
the high expense of using paid API services. In this paper, we are motivated to
study building an LLM cascade to save the cost of using LLMs, particularly for
performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline
follows the intuition that simpler questions can be addressed by a weaker but
more affordable LLM, whereas only the challenging questions necessitate the
stronger and more expensive LLM. To realize this decision-making, we consider
the "answer consistency" of the weaker LLM as a signal of the question
difficulty and propose several methods for the answer sampling and consistency
checking, including one leveraging a mixture of two thought representations
(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six
reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and
stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can
achieve performance comparable to using solely the stronger LLM but require
only 40% of its cost.
</p></li>
</ul>

<h3>Title: $\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis. (arXiv:2310.03173v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03173">http://arxiv.org/abs/2310.03173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03173]] $\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis(http://arxiv.org/abs/2310.03173)</code></li>
<li>Summary: <p>Program synthesis aims to create accurate, executable code from natural
language descriptions. This field has leveraged the power of reinforcement
learning (RL) in conjunction with large language models (LLMs), significantly
enhancing code generation capabilities. This integration focuses on directly
optimizing functional correctness, transcending conventional supervised losses.
While current literature predominantly favors policy-based algorithms,
attributes of program synthesis suggest a natural compatibility with
value-based methods. This stems from rich collection of off-policy programs
developed by human programmers, and the straightforward verification of
generated programs through automated unit testing (i.e. easily obtainable
rewards in RL language). Diverging from the predominant use of policy-based
algorithms, our work explores the applicability of value-based approaches,
leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman
coder). Yet, training value-based methods presents challenges due to the
enormous search space inherent to program synthesis. To this end, we propose an
initialization protocol for RL agents utilizing pre-trained LMs and a
conservative Bellman operator to reduce training complexities. Moreover, we
demonstrate how to leverage the learned value functions as a dual strategy to
post-process generated programs. Our empirical evaluations demonstrated
$\mathcal{B}$-Coder's capability in achieving state-of-the-art performance
compared with policy-based methods. Remarkably, this achievement is reached
with minimal reward engineering effort, highlighting the effectiveness of
value-based RL, independent of reward designs.
</p></li>
</ul>

<h3>Title: FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. (arXiv:2310.03214v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03214">http://arxiv.org/abs/2310.03214</a></li>
<li>Code URL: https://github.com/freshllms/freshqa</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03214]] FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation(http://arxiv.org/abs/2310.03214)</code></li>
<li>Summary: <p>Most large language models (LLMs) are trained once and never updated; thus,
they lack the ability to dynamically adapt to our ever-changing world. In this
work, we perform a detailed study of the factuality of LLM-generated text in
the context of answering questions that test current world knowledge.
Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a
diverse range of question and answer types, including questions that require
fast-changing world knowledge as well as questions with false premises that
need to be debunked. We benchmark a diverse array of both closed and
open-source LLMs under a two-mode evaluation procedure that allows us to
measure both correctness and hallucination. Through human evaluations involving
more than 50K judgments, we shed light on limitations of these models and
demonstrate significant room for improvement: for instance, all models
(regardless of model size) struggle on questions that involve fast-changing
knowledge and false premises. Motivated by these results, we present
FreshPrompt, a simple few-shot prompting method that substantially boosts the
performance of an LLM on FreshQA by incorporating relevant and up-to-date
information retrieved from a search engine into the prompt. Our experiments
show that FreshPrompt outperforms both competing search engine-augmented
prompting methods such as Self-Ask (Press et al., 2022) as well as commercial
systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that
both the number of retrieved evidences and their order play a key role in
influencing the correctness of LLM-generated answers. Additionally, instructing
the LLM to generate concise and direct answers helps reduce hallucination
compared to encouraging more verbose answers. To facilitate future work, we
release FreshQA at github.com/freshllms/freshqa and commit to updating it at
regular intervals.
</p></li>
</ul>

<h3>Title: Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. (arXiv:2310.03249v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03249">http://arxiv.org/abs/2310.03249</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03249]] Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning(http://arxiv.org/abs/2310.03249)</code></li>
<li>Summary: <p>Large language models (LLMs) have achieved remarkable success across a wide
spectrum of tasks; however, they still face limitations in scenarios that
demand long-term planning and spatial reasoning. To facilitate this line of
research, in this work, we propose a new benchmark, termed $\textbf{P}$ath
$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage
($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by
formulating ''path planning'' tasks that require an LLM to navigate to target
locations while avoiding obstacles and adhering to constraints. Leveraging this
benchmark, we systematically investigate LLMs including GPT-4 via different
few-shot prompting methodologies and BART and T5 of various sizes via
fine-tuning. Our experimental results show the promise of few-shot GPT-4 in
spatial reasoning, when it is prompted to reason and act interleavedly,
although it still fails to make long-term temporal reasoning. In contrast,
while fine-tuned LLMs achieved impressive results on in-distribution reasoning
tasks, they struggled to generalize to larger environments or environments with
more obstacles.
</p></li>
</ul>

<h3>Title: Unlock Predictable Scaling from Emergent Abilities. (arXiv:2310.03262v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03262">http://arxiv.org/abs/2310.03262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03262]] Unlock Predictable Scaling from Emergent Abilities(http://arxiv.org/abs/2310.03262)</code></li>
<li>Summary: <p>The scientific scale-up of large language models (LLMs) necessitates a
comprehensive understanding of their scaling properties. However, the existing
literature on the scaling properties only yields an incomplete answer:
optimization loss decreases predictably as the model size increases, in line
with established scaling law; yet no scaling law for task has been established
and the task performances are far from predictable during scaling. Task
performances typically show minor gains on small models until they improve
dramatically once models exceed a size threshold, exemplifying the ``emergent
abilities''. In this study, we discover that small models, although they
exhibit minor performance, demonstrate critical and consistent task performance
improvements that are not captured by conventional evaluation strategies due to
insufficient measurement resolution. To measure such improvements, we introduce
PassUntil, an evaluation strategy through massive sampling in the decoding
phase. We conduct quantitative investigations into the scaling law of task
performance. Firstly, a strict task scaling law is identified, enhancing the
predictability of task performances. Remarkably, we are able to predict the
performance of the 2.4B model on code generation with merely 0.05\% deviation
before training starts. Secondly, underpinned by PassUntil, we observe concrete
evidence of emergent abilities and ascertain that they are not in conflict with
the continuity of performance improvement. Their semblance to break-through is
that their scaling curve cannot be fitted by standard scaling law function. We
then introduce a mathematical definition for the emergent abilities. Through
the definition, we refute a prevalent ``multi-step reasoning hypothesis''
regarding the genesis of emergent abilities and propose a new hypothesis with a
satisfying fit to the observed scaling curve.
</p></li>
</ul>

<h3>Title: A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions. (arXiv:2310.03293v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03293">http://arxiv.org/abs/2310.03293</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03293]] A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions(http://arxiv.org/abs/2310.03293)</code></li>
<li>Summary: <p>Large Language Models (LLMs), such as ChatGPT, have recently been applied to
various NLP tasks due to its open-domain generation capabilities. However,
there are two issues with applying LLMs to dialogue tasks. 1. During the
dialogue process, users may have implicit intentions that might be overlooked
by LLMs. Consequently, generated responses couldn't align with the user's
intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.
In certain specific domains, their knowledge may be incomplete, and LLMs cannot
update the latest knowledge in real-time. To tackle these issues, we propose a
framework~\emph{using LLM to \textbf{E}nhance dialogue response generation by
asking questions to \textbf{D}etect user's \textbf{I}mplicit
in\textbf{T}entions} (\textbf{EDIT}). Firstly, EDIT generates open questions
related to the dialogue context as the potential user's intention; Then, EDIT
answers those questions by interacting with LLMs and searching in
domain-specific knowledge bases respectively, and use LLMs to choose the proper
answers to questions as extra knowledge; Finally, EDIT enhances response
generation by explicitly integrating those extra knowledge. Besides, previous
question generation works only focus on asking questions with answers in
context. In order to ask open questions, we construct a Context-Open-Question
(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and
Holl-E), EDIT outperformed other LLMs.
</p></li>
</ul>

<h3>Title: Learning Personalized Story Evaluation. (arXiv:2310.03304v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03304">http://arxiv.org/abs/2310.03304</a></li>
<li>Code URL: https://github.com/dqwang122/perse</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03304]] Learning Personalized Story Evaluation(http://arxiv.org/abs/2310.03304)</code></li>
<li>Summary: <p>While large language models (LLMs) have shown impressive results for more
objective tasks such as QA and retrieval, it remains nontrivial to evaluate
their performance on open-ended text generation for reasons including (1) data
contamination; (2) multi-dimensional evaluation criteria; and (3)
subjectiveness stemming from reviewers' personal preferences. To address such
issues, we propose to model personalization in an uncontaminated open-ended
generation assessment. We create two new datasets Per-MPST and Per-DOC for
personalized story evaluation, by re-purposing existing datasets with proper
anonymization and new personalized labels. We further develop a personalized
story evaluation model PERSE to infer reviewer preferences and provide a
personalized evaluation. Specifically, given a few exemplary reviews from a
particular reviewer, PERSE predicts either a detailed review or fine-grained
comparison in several aspects (such as interestingness and surprise) for that
reviewer on a new text input. Experimental results show that PERSE outperforms
GPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on
pairwise preference prediction accuracy. Both datasets and code will be
released at https://github.com/dqwang122/PerSE.
</p></li>
</ul>

<h3>Title: Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03309">http://arxiv.org/abs/2310.03309</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03309]] Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning(http://arxiv.org/abs/2310.03309)</code></li>
<li>Summary: <p>Exploiting large language models (LLMs) to tackle deductive reasoning has
garnered growing attention. It still remains highly challenging to achieve
satisfactory results in complex deductive problems, characterized by plenty of
premises (i.e., facts or rules) entailing intricate relationships among
entities and requiring multi-hop reasoning. One intuitive solution is to
decompose the original task into smaller sub-tasks, and then chain the multiple
casual reasoning steps together in a forward (e.g., Selection-Inference) or
backward (e.g., LAMBADA) direction. However, these techniques inevitably
necessitate a large number of overall stages, leading to computationally
expensive operations and a higher possibility of making misleading steps. In
addition to stage-by-stage decomposition, we draw inspiration from another
aspect of human problem-solving. Humans tend to distill the most relevant
information and organize their thoughts systematically (e.g., creating mind
maps), which assists them in answering questions or drawing conclusions
precisely and quickly. In light of this, we propose a novel reasoning approach
named Concise and Organized Perception (COP). COP carefully analyzes the given
statements to efficiently identify the most pertinent information while
eliminating redundancy. It then prompts the LLMs in a more organized form that
adapts to the model's inference process. By perceiving concise and organized
proofs, the deductive reasoning abilities of LLMs can be better elicited, and
the risk of acquiring errors caused by excessive reasoning stages is mitigated.
Furthermore, our approach can be combined with the aforementioned ones to
further boost their performance. Extensive experimental results on three
popular deductive benchmarks (i.e., ProofWriter, PrOntoQA and PrOntoQA-OOD)
show that COP significantly outperforms previous state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise. (arXiv:2310.03328v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03328">http://arxiv.org/abs/2310.03328</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03328]] Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise(http://arxiv.org/abs/2310.03328)</code></li>
<li>Summary: <p>While large language models (LLMs) like GPT-4 have recently demonstrated
astonishing zero-shot capabilities in general domain tasks, they often generate
content with hallucinations in specific domains such as Chinese law, hindering
their application in these areas. This is typically due to the absence of
training data that encompasses such a specific domain, preventing GPT-4 from
acquiring in-domain knowledge. A pressing challenge is that it's not plausible
to continue training LLMs of such scale on in-domain data.
</p>
<p>This paper introduces a simple and effective domain adaptation framework for
GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process.
The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain
by continuing learning on in-domain data. When solving a task, we leverage the
adapted LLM to generate a draft answer given a task query. Then, the draft
answer will be used to \textbf{retrieve} supporting evidence candidates from an
external in-domain knowledge base. Finally, the draft answer and retrieved
evidence are concatenated into a whole prompt to let GPT-4 assess the evidence
and \textbf{revise} the draft answer to generate the final answer.
</p>
<p>Our proposal combines the advantages of the efficiency of adapting a smaller
7B model with the evidence-assessing capability of GPT-4 and effectively
prevents GPT-4 from generating hallucinatory content. In the zero-shot setting
of four Chinese legal tasks, our method improves accuracy by 33.3\% compared to
the direct generation by GPT-4. When compared to two stronger retrieval-based
baselines, our method outperforms them by 15.4\% and 23.9\%. Our code will be
released
</p></li>
</ul>

<h3>Title: Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03368">http://arxiv.org/abs/2310.03368</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03368]] Evaluating Hallucinations in Chinese Large Language Models(http://arxiv.org/abs/2310.03368)</code></li>
<li>Summary: <p>In this paper, we establish a benchmark named HalluQA (Chinese Hallucination
Question-Answering) to measure the hallucination phenomenon in Chinese large
language models. HalluQA contains 450 meticulously designed adversarial
questions, spanning multiple domains, and takes into account Chinese historical
culture, customs, and social phenomena. During the construction of HalluQA, we
consider two types of hallucinations: imitative falsehoods and factual errors,
and we construct adversarial samples based on GLM-130B and ChatGPT. For
evaluation, we design an automated evaluation method using GPT-4 to judge
whether a model output is hallucinated. We conduct extensive experiments on 24
large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk
and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than
50%. This indicates that HalluQA is highly challenging. We analyze the primary
types of hallucinations in different types of models and their causes.
Additionally, we discuss which types of hallucinations should be prioritized
for different types of models.
</p></li>
</ul>

<h3>Title: Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03376">http://arxiv.org/abs/2310.03376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03376]] Procedural Text Mining with Large Language Models(http://arxiv.org/abs/2310.03376)</code></li>
<li>Summary: <p>Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.
</p></li>
</ul>

<h3>Title: Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards. (arXiv:2310.03473v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03473">http://arxiv.org/abs/2310.03473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03473]] Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards(http://arxiv.org/abs/2310.03473)</code></li>
<li>Summary: <p>Memory-efficient large language models are good at refining text input for
better readability. However, controllability is a matter of concern when it
comes to text generation tasks with long inputs, such as multi-document
summarization. In this work, we investigate for a generic controllable approach
for multi-document summarization that leverages the capabilities of LLMs to
refine the text. In particular, we train a controllable content extraction
scheme to extract the text that will be refined by an LLM. The scheme is
designed with a novel coverage and coherence intuitive policy, which is duly
rewarded by a passively trained LLM. Our approach yields competitive results in
the evaluation using ROUGE metrics and outperforms potential baselines in
coherence, as per human evaluation.
</p></li>
</ul>

<h3>Title: Redefining Digital Health Interfaces with Large Language Models. (arXiv:2310.03560v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03560">http://arxiv.org/abs/2310.03560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03560]] Redefining Digital Health Interfaces with Large Language Models(http://arxiv.org/abs/2310.03560)</code></li>
<li>Summary: <p>Digital health tools have the potential to significantly improve the delivery
of healthcare services. However, their use remains comparatively limited due,
in part, to challenges surrounding usability and trust. Recently, Large
Language Models (LLMs) have emerged as general-purpose models with the ability
to process complex information and produce human-quality text, presenting a
wealth of potential applications in healthcare. Directly applying LLMs in
clinical settings is not straightforward, with LLMs susceptible to providing
inconsistent or nonsensical answers. We demonstrate how LLMs can utilize
external tools to provide a novel interface between clinicians and digital
technologies. This enhances the utility and practical impact of digital
healthcare tools and AI models while addressing current issues with using LLM
in clinical settings such as hallucinations. We illustrate our approach with
examples from cardiovascular disease and diabetes risk prediction, highlighting
the benefit compared to traditional interfaces for digital tools.
</p></li>
</ul>

<h3>Title: MapperGPT: Large Language Models for Linking and Mapping Entities. (arXiv:2310.03666v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03666">http://arxiv.org/abs/2310.03666</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03666]] MapperGPT: Large Language Models for Linking and Mapping Entities(http://arxiv.org/abs/2310.03666)</code></li>
<li>Summary: <p>Aligning terminological resources, including ontologies, controlled
vocabularies, taxonomies, and value sets is a critical part of data integration
in many domains such as healthcare, chemistry, and biomedical research. Entity
mapping is the process of determining correspondences between entities across
these resources, such as gene identifiers, disease concepts, or chemical entity
identifiers. Many tools have been developed to compute such mappings based on
common structural features and lexical information such as labels and synonyms.
Lexical approaches in particular often provide very high recall, but low
precision, due to lexical ambiguity. As a consequence of this, mapping efforts
often resort to a labor intensive manual mapping refinement through a human
curator.
</p>
<p>Large Language Models (LLMs), such as the ones employed by ChatGPT, have
generalizable abilities to perform a wide range of tasks, including
question-answering and information extraction. Here we present MapperGPT, an
approach that uses LLMs to review and refine mapping relationships as a
post-processing step, in concert with existing high-recall methods that are
based on lexical and structural heuristics.
</p>
<p>We evaluated MapperGPT on a series of alignment tasks from different domains,
including anatomy, developmental biology, and renal diseases. We devised a
collection of tasks that are designed to be particularly challenging for
lexical methods. We show that when used in combination with high-recall
methods, MapperGPT can provide a substantial improvement in accuracy, beating
state-of-the-art (SOTA) methods such as LogMap.
</p></li>
</ul>

<h3>Title: Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. (arXiv:2310.03693v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03693">http://arxiv.org/abs/2310.03693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03693]] Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!(http://arxiv.org/abs/2310.03693)</code></li>
<li>Summary: <p>Optimizing large language models (LLMs) for downstream use cases often
involves the customization of pre-trained LLMs through further fine-tuning.
Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5
Turbo on custom datasets also encourage this practice. But, what are the safety
costs associated with such custom fine-tuning? We note that while existing
safety alignment infrastructures can restrict harmful behaviors of LLMs at
inference time, they do not cover safety risks when fine-tuning privileges are
extended to end-users. Our red teaming studies find that the safety alignment
of LLMs can be compromised by fine-tuning with only a few adversarially
designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety
guardrails by fine-tuning it on only 10 such examples at a cost of less than
$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful
instructions. Disconcertingly, our research also reveals that, even without
malicious intent, simply fine-tuning with benign and commonly used datasets can
also inadvertently degrade the safety alignment of LLMs, though to a lesser
extent. These findings suggest that fine-tuning aligned LLMs introduces new
safety risks that current safety infrastructures fall short of addressing --
even if a model's initial safety alignment is impeccable, it is not necessarily
to be maintained after custom fine-tuning. We outline and critically analyze
potential mitigations and advocate for further research efforts toward
reinforcing safety protocols for the custom fine-tuning of aligned LLMs.
</p></li>
</ul>

<h3>Title: Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03710">http://arxiv.org/abs/2310.03710</a></li>
<li>Code URL: https://github.com/wang-research-lab/agentinstruct</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03710]] Agent Instructs Large Language Models to be General Zero-Shot Reasoners(http://arxiv.org/abs/2310.03710)</code></li>
<li>Summary: <p>We introduce a method to improve the zero-shot reasoning abilities of large
language models on general language understanding tasks. Specifically, we build
an autonomous agent to instruct the reasoning process of large language models.
We show this approach further unleashes the zero-shot reasoning abilities of
large language models to more tasks. We study the performance of our method on
a wide set of datasets spanning generation, classification, and reasoning. We
show that our method generalizes to most tasks and obtains state-of-the-art
zero-shot performance on 20 of the 29 datasets that we evaluate. For instance,
our method boosts the performance of state-of-the-art large language models by
a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and
GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement
in reasoning is striking, with an average increase of 10.5%. With our method,
Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
</p></li>
</ul>

<h3>Title: A Long Way to Go: Investigating Length Correlations in RLHF. (arXiv:2310.03716v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03716">http://arxiv.org/abs/2310.03716</a></li>
<li>Code URL: https://github.com/prasanns/rlhf-length-biases</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03716]] A Long Way to Go: Investigating Length Correlations in RLHF(http://arxiv.org/abs/2310.03716)</code></li>
<li>Summary: <p>Great successes have been reported using Reinforcement Learning from Human
Feedback (RLHF) to align large language models. Open-source preference datasets
and reward models have enabled wider experimentation beyond generic chat
settings, particularly to make systems more "helpful" for tasks like web
question answering, summarization, and multi-turn dialogue. When optimizing for
helpfulness, RLHF has been consistently observed to drive models to produce
longer outputs. This paper demonstrates that optimizing for response length is
a significant factor behind RLHF's reported improvements in these settings.
First, we study the relationship between reward and length for reward models
trained on three open-source preference datasets for helpfulness. Here, length
correlates strongly with reward, and improvements in reward score are driven in
large part by shifting the distribution over output lengths. We then explore
interventions during both RL and reward model learning to see if we can achieve
the same downstream improvements as RLHF without increasing length. While our
interventions mitigate length increases, they aren't uniformly effective across
settings. Furthermore, we find that even running RLHF with a reward based
solely on length can reproduce most of the downstream improvements over the
initial policy model, showing that reward models in these settings have a long
way to go.
</p></li>
</ul>

<h3>Title: Misusing Tools in Large Language Models With Visual Adversarial Examples. (arXiv:2310.03185v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03185">http://arxiv.org/abs/2310.03185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03185]] Misusing Tools in Large Language Models With Visual Adversarial Examples(http://arxiv.org/abs/2310.03185)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are being enhanced with the ability to use tools
and to process multiple modalities. These new capabilities bring new benefits
and also new security risks. In this work, we show that an attacker can use
visual adversarial examples to cause attacker-desired tool usage. For example,
the attacker could cause a victim LLM to delete calendar events, leak private
conversations and book hotels. Different from prior work, our attacks can
affect the confidentiality and integrity of user resources connected to the LLM
while being stealthy and generalizable to multiple input prompts. We construct
these attacks using gradient-based adversarial training and characterize
performance along multiple dimensions. We find that our adversarial images can
manipulate the LLM to invoke tools following real-world syntax almost always
(~98%) while maintaining high similarity to clean images (~0.9 SSIM).
Furthermore, using human scoring and automated metrics, we find that the
attacks do not noticeably affect the conversation (and its semantics) between
the user and the LLM.
</p></li>
</ul>

<h3>Title: UniPredict: Large Language Models are Universal Tabular Predictors. (arXiv:2310.03266v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03266">http://arxiv.org/abs/2310.03266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03266]] UniPredict: Large Language Models are Universal Tabular Predictors(http://arxiv.org/abs/2310.03266)</code></li>
<li>Summary: <p>Tabular data prediction is a fundamental machine learning task for many
applications. Existing methods predominantly employ discriminative modeling and
operate under the assumption of a fixed target column, necessitating
re-training for every new predictive task. Inspired by the generative power of
large language models (LLMs), this paper exploits the idea of building
universal tabular data predictors based on generative modeling, namely
UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets
with the capability of comprehending diverse tabular inputs and predicting for
target variables following the input instructions. Specifically, we train a
single LLM on an aggregation of 169 tabular datasets with diverse targets and
compare its performance against baselines that are trained on each dataset
separately. We observe this versatile UniPredict model demonstrates an
advantage over other models, ranging from 5.4% to 13.4%, when compared with the
best tree-boosting baseline and the best neural network baseline, respectively.
We further test UniPredict in few-shot learning settings on another 62 tabular
datasets. Our method achieves strong performance in quickly adapting to new
tasks, where our method outperforms XGBoost over 100% on the low-resource setup
and shows a significant margin over all baselines. We envision that UniPredict
sheds light on developing a universal tabular data prediction system that
learns from data at scale and serves a wide range of prediction tasks.
</p></li>
</ul>

<h3>Title: Benchmarking Large Language Models As AI Research Agents. (arXiv:2310.03302v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03302">http://arxiv.org/abs/2310.03302</a></li>
<li>Code URL: https://github.com/snap-stanford/mlagentbench</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03302]] Benchmarking Large Language Models As AI Research Agents(http://arxiv.org/abs/2310.03302)</code></li>
<li>Summary: <p>Scientific experimentation involves an iterative process of creating
hypotheses, designing experiments, running experiments, and analyzing the
results. Can we build AI research agents to perform these long-horizon tasks?
To take a step towards building and evaluating research agents on such
open-ended decision-making tasks, we focus on the problem of machine learning
engineering: given a task description and a dataset, build a high-performing
model. In this paper, we propose MLAgentBench, a suite of ML tasks for
benchmarking AI research agents. Agents can perform actions like
reading/writing files, executing code, and inspecting outputs. With these
actions, agents could run experiments, analyze the results, and modify the code
of entire machine learning pipelines, such as data processing, architecture,
training processes, etc. The benchmark then automatically evaluates the agent's
performance objectively over various metrics related to performance and
efficiency. We also design an LLM-based research agent to automatically perform
experimentation loops in such an environment. Empirically, we find that a
GPT-4-based research agent can feasibly build compelling ML models over many
tasks in MLAgentBench, displaying highly interpretable plans and actions.
However, the success rates vary considerably; they span from almost 90\% on
well-established older datasets to as low as 10\% on recent Kaggle Challenges
-- unavailable during the LLM model's pretraining -- and even 0\% on newer
research challenges like BabyLM. Finally, we identify several key challenges
for LLM-based research agents such as long-term planning and hallucination. Our
code is released at https://github.com/snap-stanford/MLAgentBench.
</p></li>
</ul>

<h3>Title: Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning. (arXiv:2310.03400v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03400">http://arxiv.org/abs/2310.03400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03400]] Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning(http://arxiv.org/abs/2310.03400)</code></li>
<li>Summary: <p>Nowadays, billions of people engage in communication and express their
opinions on the internet daily. Unfortunately, not all of these expressions are
friendly or compliant, making content moderation an indispensable task. With
the successful development of Large Language Models (LLMs) in recent years,
LLM-based methods have become a feasible solution for handling tasks in various
domains. However, in the field of content moderation, there is still a lack of
detailed work that systematically introduces implementation details. In this
paper, we introduce how to fine-tune an LLM model that can be privately
deployed for content moderation. Specifically, we discuss whether incorporating
reasons during the fine-tuning process would be better or if it should be
treated as a classification task directly. We also explore the benefits of
utilizing reasons generated by more powerful LLMs for fine-tuning privately
deployed models and the impact of different processing approaches when the
answers generated by the more powerful LLMs are incorrect. We report the entire
research process and the key findings in this paper, hoping to provide valuable
experience for researchers who are fine-tuning privately deployed models in
their domain-specific research.
</p></li>
</ul>

<h3>Title: HeaP: Hierarchical Policies for Web Actions using LLMs. (arXiv:2310.03720v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03720">http://arxiv.org/abs/2310.03720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03720]] HeaP: Hierarchical Policies for Web Actions using LLMs(http://arxiv.org/abs/2310.03720)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated remarkable capabilities in
performing a range of instruction following tasks in few and zero-shot
settings. However, teaching LLMs to perform tasks on the web presents
fundamental challenges -- combinatorially large open-world tasks and variations
across web interfaces. We tackle these challenges by leveraging LLMs to
decompose web tasks into a collection of sub-tasks, each of which can be solved
by a low-level, closed-loop policy. These policies constitute a shared grammar
across tasks, i.e., new web tasks can be expressed as a composition of these
policies. We propose a novel framework, Hierarchical Policies for Web Actions
using LLMs (HeaP), that learns a set of hierarchical LLM prompts from
demonstrations for planning high-level tasks and executing them via a sequence
of low-level policies. We evaluate HeaP against a range of baselines on a suite
of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as
live website interactions, and show that it is able to outperform prior works
using orders of magnitude less data.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: A quantum moving target segmentation algorithm for grayscale video. (arXiv:2310.03038v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03038">http://arxiv.org/abs/2310.03038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03038]] A quantum moving target segmentation algorithm for grayscale video(http://arxiv.org/abs/2310.03038)</code></li>
<li>Summary: <p>The moving target segmentation (MTS) aims to segment out moving targets in
the video, however, the classical algorithm faces the huge challenge of
real-time processing in the current video era. Some scholars have successfully
demonstrated the quantum advantages in some video processing tasks, but not
concerning moving target segmentation. In this paper, a quantum moving target
segmentation algorithm for grayscale video is proposed, which can use quantum
mechanism to simultaneously calculate the difference of all pixels in all
adjacent frames and then quickly segment out the moving target. In addition, a
feasible quantum comparator is designed to distinguish the grayscale values
with the threshold. Then several quantum circuit units, including three-frame
difference, binarization and AND operation, are designed in detail, and then
are combined together to construct the complete quantum circuits for segmenting
the moving target. For a quantum video with $2^m$ frames (every frame is a
$2^n\times 2^n$ image with $q$ grayscale levels), the complexity of our
algorithm can be reduced to O$(n^2 + q)$. Compared with the classic
counterpart, it is an exponential speedup, while its complexity is also
superior to the existing quantum algorithms. Finally, the experiment is
conducted on IBM Q to show the feasibility of our algorithm in the noisy
intermediate-scale quantum (NISQ) era.
</p></li>
</ul>

<h3>Title: Ablation Study to Clarify the Mechanism of Object Segmentation in Multi-Object Representation Learning. (arXiv:2310.03273v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03273">http://arxiv.org/abs/2310.03273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03273]] Ablation Study to Clarify the Mechanism of Object Segmentation in Multi-Object Representation Learning(http://arxiv.org/abs/2310.03273)</code></li>
<li>Summary: <p>Multi-object representation learning aims to represent complex real-world
visual input using the composition of multiple objects. Representation learning
methods have often used unsupervised learning to segment an input image into
individual objects and encode these objects into each latent vector. However,
it is not clear how previous methods have achieved the appropriate segmentation
of individual objects. Additionally, most of the previous methods regularize
the latent vectors using a Variational Autoencoder (VAE). Therefore, it is not
clear whether VAE regularization contributes to appropriate object
segmentation. To elucidate the mechanism of object segmentation in multi-object
representation learning, we conducted an ablation study on MONet, which is a
typical method. MONet represents multiple objects using pairs that consist of
an attention mask and the latent vector corresponding to the attention mask.
Each latent vector is encoded from the input image and attention mask. Then,
the component image and attention mask are decoded from each latent vector. The
loss function of MONet consists of 1) the sum of reconstruction losses between
the input image and decoded component image, 2) the VAE regularization loss of
the latent vector, and 3) the reconstruction loss of the attention mask to
explicitly encode shape information. We conducted an ablation study on these
three loss functions to investigate the effect on segmentation performance. Our
results showed that the VAE regularization loss did not affect segmentation
performance and the others losses did affect it. Based on this result, we
hypothesize that it is important to maximize the attention mask of the image
region best represented by a single latent vector corresponding to the
attention mask. We confirmed this hypothesis by evaluating a new loss function
with the same mechanism as the hypothesis.
</p></li>
</ul>

<h3>Title: Combining Datasets with Different Label Sets for Improved Nucleus Segmentation and Classification. (arXiv:2310.03346v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03346">http://arxiv.org/abs/2310.03346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03346]] Combining Datasets with Different Label Sets for Improved Nucleus Segmentation and Classification(http://arxiv.org/abs/2310.03346)</code></li>
<li>Summary: <p>Segmentation and classification of cell nuclei in histopathology images using
deep neural networks (DNNs) can save pathologists' time for diagnosing various
diseases, including cancers, by automating cell counting and morphometric
assessments. It is now well-known that the accuracy of DNNs increases with the
sizes of annotated datasets available for training. Although multiple datasets
of histopathology images with nuclear annotations and class labels have been
made publicly available, the set of class labels differ across these datasets.
We propose a method to train DNNs for instance segmentation and classification
on multiple datasets where the set of classes across the datasets are related
but not the same. Specifically, our method is designed to utilize a
coarse-to-fine class hierarchy, where the set of classes labeled and annotated
in a dataset can be at any level of the hierarchy, as long as the classes are
mutually exclusive. Within a dataset, the set of classes need not even be at
the same level of the class hierarchy tree. Our results demonstrate that
segmentation and classification metrics for the class set used by the test
split of a dataset can improve by pre-training on another dataset that may even
have a different set of classes due to the expansion of the training set
enabled by our method. Furthermore, generalization to previously unseen
datasets also improves by combining multiple other datasets with different sets
of classes for training. The improvement is both qualitative and quantitative.
The proposed method can be adapted for various loss functions, DNN
architectures, and application domains.
</p></li>
</ul>

<h3>Title: Ammonia-Net: A Multi-task Joint Learning Model for Multi-class Segmentation and Classification in Tooth-marked Tongue Diagnosis. (arXiv:2310.03472v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03472">http://arxiv.org/abs/2310.03472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03472]] Ammonia-Net: A Multi-task Joint Learning Model for Multi-class Segmentation and Classification in Tooth-marked Tongue Diagnosis(http://arxiv.org/abs/2310.03472)</code></li>
<li>Summary: <p>In Traditional Chinese Medicine, the tooth marks on the tongue, stemming from
prolonged dental pressure, serve as a crucial indicator for assessing qi (yang)
deficiency, which is intrinsically linked to visceral health. Manual diagnosis
of tooth-marked tongue solely relies on experience. Nonetheless, the diversity
in shape, color, and type of tooth marks poses a challenge to diagnostic
accuracy and consistency. To address these problems, herein we propose a
multi-task joint learning model named Ammonia-Net. This model employs a
convolutional neural network-based architecture, specifically designed for
multi-class segmentation and classification of tongue images. Ammonia-Net
performs semantic segmentation of tongue images to identify tongue and tooth
marks. With the assistance of segmentation output, it classifies the images
into the desired number of classes: healthy tongue, light tongue, moderate
tongue, and severe tongue. As far as we know, this is the first attempt to
apply the semantic segmentation results of tooth marks for tooth-marked tongue
classification. To train Ammonia-Net, we collect 856 tongue images from 856
subjects. After a number of extensive experiments, the experimental results
show that the proposed model achieves 99.06% accuracy in the two-class
classification task of tooth-marked tongue identification and 80.02%. As for
the segmentation task, mIoU for tongue and tooth marks amounts to 71.65%.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
