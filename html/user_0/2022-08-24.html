<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Cluster Based Secure Multi-Party Computation in Federated Learning for Histopathology Images. (arXiv:2208.10919v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10919">http://arxiv.org/abs/2208.10919</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10919] Cluster Based Secure Multi-Party Computation in Federated Learning for Histopathology Images](http://arxiv.org/abs/2208.10919)</code></li>
<li>Summary: <p>Federated learning (FL) is a decentralized method enabling hospitals to
collaboratively learn a model without sharing private patient data for
training. In FL, participant hospitals periodically exchange training results
rather than training samples with a central server. However, having access to
model parameters or gradients can expose private training data samples. To
address this challenge, we adopt secure multiparty computation (SMC) to
establish a privacy-preserving federated learning framework. In our proposed
method, the hospitals are divided into clusters. After local training, each
hospital splits its model weights among other hospitals in the same cluster
such that no single hospital can retrieve other hospitals' weights on its own.
Then, all hospitals sum up the received weights, sending the results to the
central server. Finally, the central server aggregates the results, retrieving
the average of models' weights and updating the model without having access to
individual hospitals' weights. We conduct experiments on a publicly available
repository, The Cancer Genome Atlas (TCGA). We compare the performance of the
proposed framework with differential privacy and federated averaging as the
baseline. The results reveal that compared to differential privacy, our
framework can achieve higher accuracy with no privacy leakage risk at a cost of
higher communication overhead.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Toward Better Target Representation for Source-Free and Black-Box Domain Adaptation. (arXiv:2208.10531v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10531">http://arxiv.org/abs/2208.10531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10531] Toward Better Target Representation for Source-Free and Black-Box Domain Adaptation](http://arxiv.org/abs/2208.10531)</code></li>
<li>Summary: <p>Domain adaptation aims at aligning the labeled source domain and the
unlabeled target domain, and most existing approaches assume the source data is
accessible. Unfortunately, this paradigm raises concerns in data privacy and
security. Recent studies try to dispel these concerns by the Source-Free
setting, which adapts the source-trained model towards target domain without
exposing the source data. However, the Source-Free paradigm is still at risk of
data leakage due to adversarial attacks to the source model. Hence, the
Black-Box setting is proposed, where only the outputs of source model can be
utilized. In this paper, we address both the Source-Free adaptation and the
Black-Box adaptation, proposing a novel method named better target
representation from Frequency Mixup and Mutual Learning (FMML). Specifically,
we introduce a new data augmentation technique as Frequency MixUp, which
highlights task-relevant objects in the interpolations, thus enhancing
class-consistency and linear behavior for target models. Moreover, we introduce
a network regularization method called Mutual Learning to the domain adaptation
problem. It transfers knowledge inside the target model via self-knowledge
distillation and thus alleviates overfitting on the source domain by learning
multi-scale target representations. Extensive experiments show that our method
achieves state-of-the-art performance on several benchmark datasets under both
settings.
</p></li>
</ul>

<h3>Title: An Evolutionary Approach for Creating of Diverse Classifier Ensembles. (arXiv:2208.10996v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10996">http://arxiv.org/abs/2208.10996</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10996] An Evolutionary Approach for Creating of Diverse Classifier Ensembles](http://arxiv.org/abs/2208.10996)</code></li>
<li>Summary: <p>Classification is one of the most studied tasks in data mining and machine
learning areas and many works in the literature have been presented to solve
classification problems for multiple fields of knowledge such as medicine,
biology, security, and remote sensing. Since there is no single classifier that
achieves the best results for all kinds of applications, a good alternative is
to adopt classifier fusion strategies. A key point in the success of classifier
fusion approaches is the combination of diversity and accuracy among
classifiers belonging to an ensemble. With a large amount of classification
models available in the literature, one challenge is the choice of the most
suitable classifiers to compose the final classification system, which
generates the need of classifier selection strategies. We address this point by
proposing a framework for classifier selection and fusion based on a four-step
protocol called CIF-E (Classifiers, Initialization, Fitness function, and
Evolutionary algorithm). We implement and evaluate 24 varied ensemble
approaches following the proposed CIF-E protocol and we are able to find the
most accurate approach. A comparative analysis has also been performed among
the best approaches and many other baselines from the literature. The
experiments show that the proposed evolutionary approach based on Univariate
Marginal Distribution Algorithm (UMDA) can outperform the state-of-the-art
literature approaches in many well-known UCI datasets.
</p></li>
</ul>

<h3>Title: Explaining Bias in Deep Face Recognition via Image Characteristics. (arXiv:2208.11099v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11099">http://arxiv.org/abs/2208.11099</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11099] Explaining Bias in Deep Face Recognition via Image Characteristics](http://arxiv.org/abs/2208.11099)</code></li>
<li>Summary: <p>In this paper, we propose a novel explanatory framework aimed to provide a
better understanding of how face recognition models perform as the underlying
data characteristics (protected attributes: gender, ethnicity, age;
non-protected attributes: facial hair, makeup, accessories, face orientation
and occlusion, image distortion, emotions) on which they are tested change.
With our framework, we evaluate ten state-of-the-art face recognition models,
comparing their fairness in terms of security and usability on two data sets,
involving six groups based on gender and ethnicity. We then analyze the impact
of image characteristics on models performance. Our results show that trends
appearing in a single-attribute analysis disappear or reverse when
multi-attribute groups are considered, and that performance disparities are
also related to non-protected attributes. Source code: https://cutt.ly/2XwRLiA.
</p></li>
</ul>

<h3>Title: SoK: Explainable Machine Learning for Computer Security Applications. (arXiv:2208.10605v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10605">http://arxiv.org/abs/2208.10605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10605] SoK: Explainable Machine Learning for Computer Security Applications](http://arxiv.org/abs/2208.10605)</code></li>
<li>Summary: <p>Explainable Artificial Intelligence (XAI) is a promising solution to improve
the transparency of machine learning (ML) pipelines. We systematize the
increasingly growing (but fragmented) microcosm of studies that develop and
utilize XAI methods for defensive and offensive cybersecurity tasks. We
identify 3 cybersecurity stakeholders, i.e., model users, designers, and
adversaries, that utilize XAI for 5 different objectives within an ML pipeline,
namely 1) XAI-enabled decision support, 2) applied XAI for security tasks, 3)
model verification via XAI, 4) explanation verification &amp; robustness, and 5)
offensive use of explanations. We further classify the literature w.r.t. the
targeted security domain. Our analysis of the literature indicates that many of
the XAI applications are designed with little understanding of how they might
be integrated into analyst workflows -- user studies for explanation evaluation
are conducted in only 14% of the cases. The literature also rarely disentangles
the role of the various stakeholders. Particularly, the role of the model
designer is minimized within the security literature. To this end, we present
an illustrative use case accentuating the role of model designers. We
demonstrate cases where XAI can help in model verification and cases where it
may lead to erroneous conclusions instead. The systematization and use case
enable us to challenge several assumptions and present open problems that can
help shape the future of XAI within cybersecurity
</p></li>
</ul>

<h3>Title: AppGNN: Approximation-Aware Functional Reverse Engineering using Graph Neural Networks. (arXiv:2208.10868v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10868">http://arxiv.org/abs/2208.10868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10868] AppGNN: Approximation-Aware Functional Reverse Engineering using Graph Neural Networks](http://arxiv.org/abs/2208.10868)</code></li>
<li>Summary: <p>The globalization of the Integrated Circuit (IC) market is attracting an
ever-growing number of partners, while remarkably lengthening the supply chain.
Thereby, security concerns, such as those imposed by functional Reverse
Engineering (RE), have become quintessential. RE leads to disclosure of
confidential information to competitors, potentially enabling the theft of
intellectual property. Traditional functional RE methods analyze a given
gate-level netlist through employing pattern matching towards reconstructing
the underlying basic blocks, and hence, reverse engineer the circuit's
function.
</p></li>
</ul>

<p>In this work, we are the first to demonstrate that applying Approximate
Computing (AxC) principles to circuits significantly improves the resiliency
against RE. This is attributed to the increased complexity in the underlying
pattern-matching process. The resiliency remains effective even for Graph
Neural Networks (GNNs) that are presently one of the most powerful
state-of-the-art techniques in functional RE. Using AxC, we demonstrate a
substantial reduction in GNN average classification accuracy-- from 98% to a
mere 53%. To surmount the challenges introduced by AxC in RE, we propose the
highly promising AppGNN platform, which enables GNNs (still being trained on
exact circuits) to: (i) perform accurate classifications, and (ii) reverse
engineer the circuit functionality, notwithstanding the applied approximation
technique. AppGNN accomplishes this by implementing a novel graph-based node
sampling approach that mimics generic approximation methodologies, requiring
zero knowledge of the targeted approximation type.
</p>
<p>We perform an extensive evaluation and show that, using our method, we can
improve the classification accuracy from 53% to 81% when classifying
approximate adder circuits that have been generated using evolutionary
algorithms, which our method is oblivious of.
</p>

<h3>Title: Towards a Formal Approach for Detection of Vulnerabilities in the Android Permissions System. (arXiv:2208.11062v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11062">http://arxiv.org/abs/2208.11062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11062] Towards a Formal Approach for Detection of Vulnerabilities in the Android Permissions System](http://arxiv.org/abs/2208.11062)</code></li>
<li>Summary: <p>Android is a widely used operating system that employs a permission-based
access control model. The Android Permissions System (APS) is responsible for
mediating application resource requests. APS is a critical component of the
Android security mechanism; hence, a failure in the design of APS can
potentially lead to vulnerabilities that grant unauthorized access to resources
by malicious applications. In this paper, we present a formal approach for
modeling and verifying the security properties of APS. We demonstrate the
usability of the proposed approach by showcasing the detection of a well-known
vulnerability found in Android's custom permissions.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Split-U-Net: Preventing Data Leakage in Split Learning for Collaborative Multi-Modal Brain Tumor Segmentation. (arXiv:2208.10553v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10553">http://arxiv.org/abs/2208.10553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10553] Split-U-Net: Preventing Data Leakage in Split Learning for Collaborative Multi-Modal Brain Tumor Segmentation](http://arxiv.org/abs/2208.10553)</code></li>
<li>Summary: <p>Split learning (SL) has been proposed to train deep learning models in a
decentralized manner. For decentralized healthcare applications with vertical
data partitioning, SL can be beneficial as it allows institutes with
complementary features or images for a shared set of patients to jointly
develop more robust and generalizable models. In this work, we propose
"Split-U-Net" and successfully apply SL for collaborative biomedical image
segmentation. Nonetheless, SL requires the exchanging of intermediate
activation maps and gradients to allow training models across different feature
spaces, which might leak data and raise privacy concerns. Therefore, we also
quantify the amount of data leakage in common SL scenarios for biomedical image
segmentation and provide ways to counteract such leakage by applying
appropriate defense strategies.
</p></li>
</ul>

<h3>Title: Hierarchical Perceptual Noise Injection for Social Media Fingerprint Privacy Protection. (arXiv:2208.10688v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10688">http://arxiv.org/abs/2208.10688</a></li>
<li>Code URL: <a href="https://github.com/nlsde-safety-team/fingersafe">https://github.com/nlsde-safety-team/fingersafe</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10688] Hierarchical Perceptual Noise Injection for Social Media Fingerprint Privacy Protection](http://arxiv.org/abs/2208.10688)</code></li>
<li>Summary: <p>Billions of people are sharing their daily life images on social media every
day. However, their biometric information (e.g., fingerprint) could be easily
stolen from these images. The threat of fingerprint leakage from social media
raises a strong desire for anonymizing shared images while maintaining image
qualities, since fingerprints act as a lifelong individual biometric password.
To guard the fingerprint leakage, adversarial attack emerges as a solution by
adding imperceptible perturbations on images. However, existing works are
either weak in black-box transferability or appear unnatural. Motivated by
visual perception hierarchy (i.e., high-level perception exploits model-shared
semantics that transfer well across models while low-level perception extracts
primitive stimulus and will cause high visual sensitivities given suspicious
stimulus), we propose FingerSafe, a hierarchical perceptual protective noise
injection framework to address the mentioned problems. For black-box
transferability, we inject protective noises on fingerprint orientation field
to perturb the model-shared high-level semantics (i.e., fingerprint ridges).
Considering visual naturalness, we suppress the low-level local contrast
stimulus by regularizing the response of Lateral Geniculate Nucleus. Our
FingerSafe is the first to provide feasible fingerprint protection in both
digital (up to 94.12%) and realistic scenarios (Twitter and Facebook, up to
68.75%). Our code can be found at
https://github.com/nlsde-safety-team/FingerSafe.
</p></li>
</ul>

<h3>Title: Joint Privacy Enhancement and Quantization in Federated Learning. (arXiv:2208.10888v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10888">http://arxiv.org/abs/2208.10888</a></li>
<li>Code URL: <a href="https://github.com/langnatalie/jopeq">https://github.com/langnatalie/jopeq</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10888] Joint Privacy Enhancement and Quantization in Federated Learning](http://arxiv.org/abs/2208.10888)</code></li>
<li>Summary: <p>Federated learning (FL) is an emerging paradigm for training machine learning
models using possibly private data available at edge devices. The distributed
operation of FL gives rise to challenges that are not encountered in
centralized machine learning, including the need to preserve the privacy of the
local datasets, and the communication load due to the repeated exchange of
updated models. These challenges are often tackled individually via techniques
that induce some distortion on the updated models, e.g., local differential
privacy (LDP) mechanisms and lossy compression. In this work we propose a
method coined joint privacy enhancement and quantization (JoPEQ), which jointly
implements lossy compression and privacy enhancement in FL settings. In
particular, JoPEQ utilizes vector quantization based on random lattice, a
universal compression technique whose byproduct distortion is statistically
equivalent to additive noise. This distortion is leveraged to enhance privacy
by augmenting the model updates with dedicated multivariate privacy preserving
noise. We show that JoPEQ simultaneously quantizes data according to a required
bit-rate while holding a desired privacy level, without notably affecting the
utility of the learned model. This is shown via analytical LDP guarantees,
distortion and convergence bounds derivation, and numerical studies. Finally,
we empirically assert that JoPEQ demolishes common attacks known to exploit
privacy leakage.
</p></li>
</ul>

<h3>Title: Evaluating Machine Unlearning via Epistemic Uncertainty. (arXiv:2208.10836v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10836">http://arxiv.org/abs/2208.10836</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10836] Evaluating Machine Unlearning via Epistemic Uncertainty](http://arxiv.org/abs/2208.10836)</code></li>
<li>Summary: <p>There has been a growing interest in Machine Unlearning recently, primarily
due to legal requirements such as the General Data Protection Regulation (GDPR)
and the California Consumer Privacy Act. Thus, multiple approaches were
presented to remove the influence of specific target data points from a trained
model. However, when evaluating the success of unlearning, current approaches
either use adversarial attacks or compare their results to the optimal
solution, which usually incorporates retraining from scratch. We argue that
both ways are insufficient in practice. In this work, we present an evaluation
metric for Machine Unlearning algorithms based on epistemic uncertainty. This
is the first definition of a general evaluation metric for Machine Unlearning
to our best knowledge.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Optimal Bootstrapping of PoW Blockchains. (arXiv:2208.10618v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10618">http://arxiv.org/abs/2208.10618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10618] Optimal Bootstrapping of PoW Blockchains](http://arxiv.org/abs/2208.10618)</code></li>
<li>Summary: <p>Proof of Work (PoW) blockchains are susceptible to adversarial majority
mining attacks in the early stages due to incipient participation and
corresponding low net hash power. Bootstrapping ensures safety and liveness
during the transient stage by protecting against a majority mining attack,
allowing a PoW chain to grow the participation base and corresponding mining
hash power. Liveness is especially important since a loss of liveness will lead
to loss of honest mining rewards, decreasing honest participation, hence
creating an undesired spiral; indeed existing bootstrapping mechanisms offer
especially weak liveness guarantees.
</p></li>
</ul>

<p>In this paper, we propose Advocate, a new bootstrapping methodology, which
achieves two main results: (a) optimal liveness and low latency under a
super-majority adversary for the Nakamoto longest chain protocol and (b)
immediate black-box generalization to a variety of parallel-chain based scaling
architectures, including OHIE and Prism. We demonstrate via a full-stack
implementation the robustness of Advocate under a 90% adversarial majority.
</p>

<h3>Title: Decentralized Collaborative Learning with Probabilistic Data Protection. (arXiv:2208.10674v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10674">http://arxiv.org/abs/2208.10674</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10674] Decentralized Collaborative Learning with Probabilistic Data Protection](http://arxiv.org/abs/2208.10674)</code></li>
<li>Summary: <p>We discuss future directions of Blockchain as a collaborative value
co-creation platform, in which network participants can gain extra insights
that cannot be accessed when disconnected from the others. As such, we propose
a decentralized machine learning framework that is carefully designed to
respect the values of democracy, diversity, and privacy. Specifically, we
propose a federated multi-task learning framework that integrates a
privacy-preserving dynamic consensus algorithm. We show that a specific network
topology called the expander graph dramatically improves the scalability of
global consensus building. We conclude the paper by making some remarks on open
problems.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Adversarial Vulnerability of Temporal Feature Networks for Object Detection. (arXiv:2208.10773v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10773">http://arxiv.org/abs/2208.10773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10773] Adversarial Vulnerability of Temporal Feature Networks for Object Detection](http://arxiv.org/abs/2208.10773)</code></li>
<li>Summary: <p>Taking into account information across the temporal domain helps to improve
environment perception in autonomous driving. However, it has not been studied
so far whether temporally fused neural networks are vulnerable to deliberately
generated perturbations, i.e. adversarial attacks, or whether temporal history
is an inherent defense against them. In this work, we study whether temporal
feature networks for object detection are vulnerable to universal adversarial
attacks. We evaluate attacks of two types: imperceptible noise for the whole
image and locally-bound adversarial patch. In both cases, perturbations are
generated in a white-box manner using PGD. Our experiments confirm, that
attacking even a portion of a temporal input suffices to fool the network. We
visually assess generated perturbations to gain insights into the functioning
of attacks. To enhance the robustness, we apply adversarial training using
5-PGD. Our experiments on KITTI and nuScenes datasets demonstrate, that a model
robustified via K-PGD is able to withstand the studied attacks while keeping
the mAP-based performance comparable to that of an unattacked model.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: State Of The Art In Open-Set Iris Presentation Attack Detection. (arXiv:2208.10564v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10564">http://arxiv.org/abs/2208.10564</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10564] State Of The Art In Open-Set Iris Presentation Attack Detection](http://arxiv.org/abs/2208.10564)</code></li>
<li>Summary: <p>Research in presentation attack detection (PAD) for iris recognition has
largely moved beyond evaluation in "closed-set" scenarios, to emphasize ability
to generalize to presentation attack types not present in the training data.
This paper offers several contributions to understand and extend the
state-of-the-art in open-set iris PAD. First, it describes the most
authoritative evaluation to date of iris PAD. We have curated the largest
publicly-available image dataset for this problem, drawing from 26 benchmarks
previously released by various groups, and adding 150,000 images being released
with the journal version of this paper, to create a set of 450,000 images
representing authentic iris and seven types of presentation attack instrument
(PAI). We formulate a leave-one-PAI-out evaluation protocol, and show that even
the best algorithms in the closed-set evaluations exhibit catastrophic failures
on multiple attack types in the open-set scenario. This includes algorithms
performing well in the most recent LivDet-Iris 2020 competition, which may come
from the fact that the LivDet-Iris protocol emphasizes sequestered images
rather than unseen attack types. Second, we evaluate the accuracy of five
open-source iris presentation attack algorithms available today, one of which
is newly-proposed in this paper, and build an ensemble method that beats the
winner of the LivDet-Iris 2020 by a substantial margin. This paper demonstrates
that closed-set iris PAD, when all PAIs are known during training, is a solved
problem, with multiple algorithms showing very high accuracy, while open-set
iris PAD, when evaluated correctly, is far from being solved. The newly-created
dataset, new open-source algorithms, and evaluation protocol, made publicly
available with the journal version of this paper, provide the experimental
artifacts that researchers can use to measure progress on this important
problem.
</p></li>
</ul>

<h3>Title: RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN. (arXiv:2208.10608v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10608">http://arxiv.org/abs/2208.10608</a></li>
<li>Code URL: <a href="https://github.com/huyvnphan/eccv2022-ribac">https://github.com/huyvnphan/eccv2022-ribac</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10608] RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN](http://arxiv.org/abs/2208.10608)</code></li>
<li>Summary: <p>Recently backdoor attack has become an emerging threat to the security of
deep neural network (DNN) models. To date, most of the existing studies focus
on backdoor attack against the uncompressed model; while the vulnerability of
compressed DNNs, which are widely used in the practical applications, is little
exploited yet. In this paper, we propose to study and develop Robust and
Imperceptible Backdoor Attack against Compact DNN models (RIBAC). By performing
systematic analysis and exploration on the important design knobs, we propose a
framework that can learn the proper trigger patterns, model parameters and
pruning masks in an efficient way. Thereby achieving high trigger stealthiness,
high attack success rate and high model efficiency simultaneously. Extensive
evaluations across different datasets, including the test against the
state-of-the-art defense mechanisms, demonstrate the high robustness,
stealthiness and model efficiency of RIBAC. Code is available at
https://github.com/huyvnphan/ECCV2022-RIBAC
</p></li>
</ul>

<h3>Title: A Comprehensive Study of Real-Time Object Detection Networks Across Multiple Domains: A Survey. (arXiv:2208.10895v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10895">http://arxiv.org/abs/2208.10895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10895] A Comprehensive Study of Real-Time Object Detection Networks Across Multiple Domains: A Survey](http://arxiv.org/abs/2208.10895)</code></li>
<li>Summary: <p>Deep neural network based object detectors are continuously evolving and are
used in a multitude of applications, each having its own set of requirements.
While safety-critical applications need high accuracy and reliability,
low-latency tasks need resource and energy-efficient networks. Real-time
detectors, which are a necessity in high-impact real-world applications, are
continuously proposed, but they overemphasize the improvements in accuracy and
speed while other capabilities such as versatility, robustness, resource and
energy efficiency are omitted. A reference benchmark for existing networks does
not exist, nor does a standard evaluation guideline for designing new networks,
which results in ambiguous and inconsistent comparisons. We, thus, conduct a
comprehensive study on multiple real-time detectors (anchor-, keypoint-, and
transformer-based) on a wide range of datasets and report results on an
extensive set of metrics. We also study the impact of variables such as image
size, anchor dimensions, confidence thresholds, and architecture layers on the
overall performance. We analyze the robustness of detection networks against
distribution shifts, natural corruptions, and adversarial attacks. Also, we
provide a calibration analysis to gauge the reliability of the predictions.
Finally, to highlight the real-world impact, we conduct two unique case
studies, on autonomous driving and healthcare applications. To further gauge
the capability of networks in critical real-time applications, we report the
performance after deploying the detection networks on edge devices. Our
extensive empirical study can act as a guideline for the industrial community
to make an informed choice on the existing networks. We also hope to inspire
the research community towards a new direction in the design and evaluation of
networks that focuses on a bigger and holistic overview for a far-reaching
impact.
</p></li>
</ul>

<h3>Title: DepthFake: a depth-based strategy for detecting Deepfake videos. (arXiv:2208.11074v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11074">http://arxiv.org/abs/2208.11074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11074] DepthFake: a depth-based strategy for detecting Deepfake videos](http://arxiv.org/abs/2208.11074)</code></li>
<li>Summary: <p>Fake content has grown at an incredible rate over the past few years. The
spread of social media and online platforms makes their dissemination on a
large scale increasingly accessible by malicious actors. In parallel, due to
the growing diffusion of fake image generation methods, many Deep
Learning-based detection techniques have been proposed. Most of those methods
rely on extracting salient features from RGB images to detect through a binary
classifier if the image is fake or real. In this paper, we proposed DepthFake,
a study on how to improve classical RGB-based approaches with depth-maps. The
depth information is extracted from RGB images with recent monocular depth
estimation techniques. Here, we demonstrate the effective contribution of
depth-maps to the deepfake detection task on robust pre-trained architectures.
The proposed RGBD approach is in fact able to achieve an average improvement of
3.20% and up to 11.7% for some deepfake attacks with respect to standard RGB
architectures over the FaceForensic++ dataset.
</p></li>
</ul>

<h3>Title: Getting Bored of Cyberwar: Exploring the Role of the Cybercrime Underground in the Russia-Ukraine Conflict. (arXiv:2208.10629v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10629">http://arxiv.org/abs/2208.10629</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10629] Getting Bored of Cyberwar: Exploring the Role of the Cybercrime Underground in the Russia-Ukraine Conflict](http://arxiv.org/abs/2208.10629)</code></li>
<li>Summary: <p>There has been substantial commentary on the role of cyberattacks,
hacktivists, and the cybercrime underground in the Russia-Ukraine conflict.
Drawing on a range of data sources, we argue that the widely-held narrative of
a cyberwar fought by committed 'hacktivists' linked to cybercrime groups is
misleading. We collected 281K web defacement attacks, 1.7M reflected DDoS
attacks, and 441 announcements (with 58K replies) of a volunteer hacking
discussion group for two months before and four months after the invasion. To
enrich our quantitative analysis, we conducted interviews with website defacers
who were active in attacking sites in Russia and Ukraine during the period. Our
findings indicate that the conflict briefly but significantly caught the
attention of the low-level cybercrime community, with notable shifts in the
geographical distribution of both defacement and DDoS attacks. However, the
role of these players in so-called cyberwarfare is minor, and they do not
resemble the 'hacktivists' imagined in popular criminological accounts. Initial
waves of interest led to more defacers participating in attack campaigns, but
rather than targeting critical infrastructure, there were mass attacks against
random websites within '.ru' and '.ua'. We can find no evidence of high-profile
actions of the kind hypothesised by the prevalent narrative. The much-vaunted
role of the 'IT Army of Ukraine' co-ordination group is mixed; the targets they
promoted were seldom defaced although they were often subjected to DDoS
attacks. Our main finding is that there was a clear loss of interest in
carrying out defacements and DDoS attacks after just a few weeks. Contrary to
some expert predictions, the cybercrime underground's involvement in the
conflict appears to have been minor and short-lived; it is unlikely to escalate
further.
</p></li>
</ul>

<h3>Title: Machine Learning-Enabled Cyber Attack Prediction and Mitigation for EV Charging Stations. (arXiv:2208.10644v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10644">http://arxiv.org/abs/2208.10644</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10644] Machine Learning-Enabled Cyber Attack Prediction and Mitigation for EV Charging Stations](http://arxiv.org/abs/2208.10644)</code></li>
<li>Summary: <p>Safe and reliable electric vehicle charging stations (EVCSs) have become
imperative in an intelligent transportation infrastructure. Over the years,
there has been a rapid increase in the deployment of EVCSs to address the
upsurging charging demands. However, advances in information and communication
technologies (ICT) have rendered this cyber-physical system (CPS) vulnerable to
suffering cyber threats, thereby destabilizing the charging ecosystem and even
the entire electric grid infrastructure. This paper develops an advanced
cybersecurity framework, where STRIDE threat modeling is used to identify
potential vulnerabilities in an EVCS. Further, the weighted attack defense tree
approach is employed to create multiple attack scenarios, followed by
developing Hidden Markov Model (HMM) and Partially Observable Monte-Carlo
Planning (POMCP) algorithms for modeling the security attacks. Also, potential
mitigation strategies are suggested for the identified threats.
</p></li>
</ul>

<h3>Title: Transferability Ranking of Adversarial Examples. (arXiv:2208.10878v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10878">http://arxiv.org/abs/2208.10878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10878] Transferability Ranking of Adversarial Examples](http://arxiv.org/abs/2208.10878)</code></li>
<li>Summary: <p>Adversarial examples can be used to maliciously and covertly change a model's
prediction. It is known that an adversarial example designed for one model can
transfer to other models as well. This poses a major threat because it means
that attackers can target systems in a blackbox manner.
</p></li>
</ul>

<p>In the domain of transferability, researchers have proposed ways to make
attacks more transferable and to make models more robust to transferred
examples. However, to the best of our knowledge, there are no works which
propose a means for ranking the transferability of an adversarial example in
the perspective of a blackbox attacker. This is an important task because an
attacker is likely to use only a select set of examples, and therefore will
want to select the samples which are most likely to transfer.
</p>
<p>In this paper we suggest a method for ranking the transferability of
adversarial examples without access to the victim's model. To accomplish this,
we define and estimate the expected transferability of a sample given limited
information about the victim. We also explore practical scenarios: where the
adversary can select the best sample to attack and where the adversary must use
a specific sample but can choose different perturbations. Through our
experiments, we found that our ranking method can increase an attacker's
success rate by up to 80% compared to the baseline (random selection without
ranking).
</p>

<h3>Title: Different Spectral Representations in Optimized Artificial Neural Networks and Brains. (arXiv:2208.10576v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10576">http://arxiv.org/abs/2208.10576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10576] Different Spectral Representations in Optimized Artificial Neural Networks and Brains](http://arxiv.org/abs/2208.10576)</code></li>
<li>Summary: <p>Recent studies suggest that artificial neural networks (ANNs) that match the
spectral properties of the mammalian visual cortex -- namely, the $\sim 1/n$
eigenspectrum of the covariance matrix of neural activities -- achieve higher
object recognition performance and robustness to adversarial attacks than those
that do not. To our knowledge, however, no previous work systematically
explored how modifying the ANN's spectral properties affects performance. To
fill this gap, we performed a systematic search over spectral regularizers,
forcing the ANN's eigenspectrum to follow $1/n^\alpha$ power laws with
different exponents $\alpha$. We found that larger powers (around 2--3) lead to
better validation accuracy and more robustness to adversarial attacks on dense
networks. This surprising finding applied to both shallow and deep networks and
it overturns the notion that the brain-like spectrum (corresponding to $\alpha
\sim 1$) always optimizes ANN performance and/or robustness. For convolutional
networks, the best $\alpha$ values depend on the task complexity and evaluation
metric: lower $\alpha$ values optimized validation accuracy and robustness to
adversarial attack for networks performing a simple object recognition task
(categorizing MNIST images of handwritten digits); for a more complex task
(categorizing CIFAR-10 natural images), we found that lower $\alpha$ values
optimized validation accuracy whereas higher $\alpha$ values optimized
adversarial robustness. These results have two main implications. First, they
cast doubt on the notion that brain-like spectral properties ($\alpha \sim 1$)
\emph{always} optimize ANN performance. Second, they demonstrate the potential
for fine-tuned spectral regularizers to optimize a chosen design metric, i.e.,
accuracy and/or robustness.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Learning Visibility for Robust Dense Human Body Estimation. (arXiv:2208.10652v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10652">http://arxiv.org/abs/2208.10652</a></li>
<li>Code URL: <a href="https://github.com/chhankyao/visdb">https://github.com/chhankyao/visdb</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10652] Learning Visibility for Robust Dense Human Body Estimation](http://arxiv.org/abs/2208.10652)</code></li>
<li>Summary: <p>Estimating 3D human pose and shape from 2D images is a crucial yet
challenging task. While prior methods with model-based representations can
perform reasonably well on whole-body images, they often fail when parts of the
body are occluded or outside the frame. Moreover, these results usually do not
faithfully capture the human silhouettes due to their limited representation
power of deformable models (e.g., representing only the naked body). An
alternative approach is to estimate dense vertices of a predefined template
body in the image space. Such representations are effective in localizing
vertices within an image but cannot handle out-of-frame body parts. In this
work, we learn dense human body estimation that is robust to partial
observations. We explicitly model the visibility of human joints and vertices
in the x, y, and z axes separately. The visibility in x and y axes help
distinguishing out-of-frame cases, and the visibility in depth axis corresponds
to occlusions (either self-occlusions or occlusions by other objects). We
obtain pseudo ground-truths of visibility labels from dense UV correspondences
and train a neural network to predict visibility along with 3D coordinates. We
show that visibility can serve as 1) an additional signal to resolve depth
ordering ambiguities of self-occluded vertices and 2) a regularization term
when fitting a human body model to the predictions. Extensive experiments on
multiple 3D human datasets demonstrate that visibility modeling significantly
improves the accuracy of human body estimation, especially for partial-body
cases. Our project page with code is at: https://github.com/chhankyao/visdb.
</p></li>
</ul>

<h3>Title: A First Look at Dataset Bias in License Plate Recognition. (arXiv:2208.10657v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10657">http://arxiv.org/abs/2208.10657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10657] A First Look at Dataset Bias in License Plate Recognition](http://arxiv.org/abs/2208.10657)</code></li>
<li>Summary: <p>Public datasets have played a key role in advancing the state of the art in
License Plate Recognition (LPR). Although dataset bias has been recognized as a
severe problem in the computer vision community, it has been largely overlooked
in the LPR literature. LPR models are usually trained and evaluated separately
on each dataset. In this scenario, they have often proven robust in the dataset
they were trained in but showed limited performance in unseen ones. Therefore,
this work investigates the dataset bias problem in the LPR context. We
performed experiments on eight datasets, four collected in Brazil and four in
mainland China, and observed that each dataset has a unique, identifiable
"signature" since a lightweight classification model predicts the source
dataset of a license plate (LP) image with more than 95% accuracy. In our
discussion, we draw attention to the fact that most LPR models are probably
exploiting such signatures to improve the results achieved in each dataset at
the cost of losing generalization capability. These results emphasize the
importance of evaluating LPR models in cross-dataset setups, as they provide a
better indication of generalization (hence real-world performance) than
within-dataset ones.
</p></li>
</ul>

<h3>Title: Unsupervised Fish Trajectory Tracking and Segmentation. (arXiv:2208.10662v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10662">http://arxiv.org/abs/2208.10662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10662] Unsupervised Fish Trajectory Tracking and Segmentation](http://arxiv.org/abs/2208.10662)</code></li>
<li>Summary: <p>DNN for fish tracking and segmentation based on high-quality labels is
expensive. Alternative unsupervised approaches rely on spatial and temporal
variations that naturally occur in video data to generate noisy
pseudo-ground-truth labels. These pseudo-labels are used to train a multi-task
deep neural network. In this paper, we propose a three-stage framework for
robust fish tracking and segmentation, where the first stage is an optical flow
model, which generates the pseudo labels using spatial and temporal consistency
between frames. In the second stage, a self-supervised model refines the
pseudo-labels incrementally. In the third stage, the refined labels are used to
train a segmentation network. No human annotations are used during the training
or inference. Extensive experiments are performed to validate our method on
three public underwater video datasets and to demonstrate that it is highly
effective for video annotation and segmentation. We also evaluate the
robustness of our framework to different imaging conditions and discuss the
limitations of our current implementation.
</p></li>
</ul>

<h3>Title: Learning from Noisy Labels with Coarse-to-Fine Sample Credibility Modeling. (arXiv:2208.10683v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10683">http://arxiv.org/abs/2208.10683</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10683] Learning from Noisy Labels with Coarse-to-Fine Sample Credibility Modeling](http://arxiv.org/abs/2208.10683)</code></li>
<li>Summary: <p>Training deep neural network (DNN) with noisy labels is practically
challenging since inaccurate labels severely degrade the generalization ability
of DNN. Previous efforts tend to handle part or full data in a unified
denoising flow via identifying noisy data with a coarse small-loss criterion to
mitigate the interference from noisy labels, ignoring the fact that the
difficulties of noisy samples are different, thus a rigid and unified data
selection pipeline cannot tackle this problem well. In this paper, we first
propose a coarse-to-fine robust learning method called CREMA, to handle noisy
data in a divide-and-conquer manner. In coarse-level, clean and noisy sets are
firstly separated in terms of credibility in a statistical sense. Since it is
practically impossible to categorize all noisy samples correctly, we further
process them in a fine-grained manner via modeling the credibility of each
sample. Specifically, for the clean set, we deliberately design a memory-based
modulation scheme to dynamically adjust the contribution of each sample in
terms of its historical credibility sequence during training, thus alleviating
the effect from noisy samples incorrectly grouped into the clean set.
Meanwhile, for samples categorized into the noisy set, a selective label update
strategy is proposed to correct noisy labels while mitigating the problem of
correction error. Extensive experiments are conducted on benchmarks of
different modalities, including image classification (CIFAR, Clothing1M etc)
and text recognition (IMDB), with either synthetic or natural semantic noises,
demonstrating the superiority and generality of CREMA.
</p></li>
</ul>

<h3>Title: Bag of Tricks for Out-of-Distribution Generalization. (arXiv:2208.10722v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10722">http://arxiv.org/abs/2208.10722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10722] Bag of Tricks for Out-of-Distribution Generalization](http://arxiv.org/abs/2208.10722)</code></li>
<li>Summary: <p>Recently, out-of-distribution (OOD) generalization has attracted attention to
the robustness and generalization ability of deep learning based models, and
accordingly, many strategies have been made to address different aspects
related to this issue. However, most existing algorithms for OOD generalization
are complicated and specifically designed for certain dataset. To alleviate
this problem, nicochallenge-2022 provides NICO++, a large-scale dataset with
diverse context information. In this paper, based on systematic analysis of
different schemes on NICO++ dataset, we propose a simple but effective learning
framework via coupling bag of tricks, including multi-objective framework
design, data augmentations, training and inference strategies. Our algorithm is
memory-efficient and easily-equipped, without complicated modules and does not
require for large pre-trained models. It achieves an excellent performance with
Top-1 accuracy of 88.16% on public test set and 75.65% on private test set, and
ranks 1st in domain generalization task of nicochallenge-2022.
</p></li>
</ul>

<h3>Title: In-Air Imaging Sonar Sensor Network with Real-Time Processing Using GPUs. (arXiv:2208.10839v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10839">http://arxiv.org/abs/2208.10839</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10839] In-Air Imaging Sonar Sensor Network with Real-Time Processing Using GPUs](http://arxiv.org/abs/2208.10839)</code></li>
<li>Summary: <p>For autonomous navigation and robotic applications, sensing the environment
correctly is crucial. Many sensing modalities for this purpose exist. In recent
years, one such modality that is being used is in-air imaging sonar. It is
ideal in complex environments with rough conditions such as dust or fog.
However, like with most sensing modalities, to sense the full environment
around the mobile platform, multiple such sensors are needed to capture the
full 360-degree range. Currently the processing algorithms used to create this
data are insufficient to do so for multiple sensors at a reasonably fast update
rate. Furthermore, a flexible and robust framework is needed to easily
implement multiple imaging sonar sensors into any setup and serve multiple
application types for the data. In this paper we present a sensor network
framework designed for this novel sensing modality. Furthermore, an
implementation of the processing algorithm on a Graphics Processing Unit is
proposed to potentially decrease the computing time to allow for real-time
processing of one or more imaging sonar sensors at a sufficiently high update
rate.
</p></li>
</ul>

<h3>Title: Robust DNN Watermarking via Fixed Embedding Weights with Optimized Distribution. (arXiv:2208.10973v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10973">http://arxiv.org/abs/2208.10973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10973] Robust DNN Watermarking via Fixed Embedding Weights with Optimized Distribution](http://arxiv.org/abs/2208.10973)</code></li>
<li>Summary: <p>Watermarking has been proposed as a way to protect the Intellectual Property
Rights (IPR) of Deep Neural Networks (DNNs) and track their use. Several
methods have been proposed that embed the watermark into the trainable
parameters of the network (white box watermarking) or into the input-output
mappping implemented by the network in correspondence to specific inputs (black
box watermarking). In both cases, achieving robustness against fine tuning,
model compression and, even more, transfer learning, is one of the most
difficult challenges researchers are trying to face with. In this paper, we
propose a new white-box, multi-bit watermarking algorithm with strong
robustness properties, including retraining for transfer learning. Robustness
is achieved thanks to a new information coding strategy according to which the
watermark message is spread across a number of fixed weights, whose position
depends on a secret key. The weights hosting the watermark are set prior to
training, and are left unchanged throughout the entire training procedure. The
distribution of the weights carrying out the message is theoretically optimised
to make sure that the watermarked weights are indistinguishable from the other
weights, while at the same time keeping their amplitude as large as possible to
improve robustness against retraining. We carried out several experiments
demonstrating the capability of the proposed scheme to provide high payloads
with practically no impact on the network accuracy, at the same time retaining
excellent robustness against network modifications an re-use, including
retraining for transfer learning.
</p></li>
</ul>

<h3>Title: Quality Matters: Embracing Quality Clues for Robust 3D Multi-Object Tracking. (arXiv:2208.10976v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10976">http://arxiv.org/abs/2208.10976</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10976] Quality Matters: Embracing Quality Clues for Robust 3D Multi-Object Tracking](http://arxiv.org/abs/2208.10976)</code></li>
<li>Summary: <p>3D Multi-Object Tracking (MOT) has achieved tremendous achievement thanks to
the rapid development of 3D object detection and 2D MOT. Recent advanced works
generally employ a series of object attributes, e.g., position, size, velocity,
and appearance, to provide the clues for the association in 3D MOT. However,
these cues may not be reliable due to some visual noise, such as occlusion and
blur, leading to tracking performance bottleneck. To reveal the dilemma, we
conduct extensive empirical analysis to expose the key bottleneck of each clue
and how they correlate with each other. The analysis results motivate us to
efficiently absorb the merits among all cues, and adaptively produce an optimal
tacking manner. Specifically, we present Location and Velocity Quality
Learning, which efficiently guides the network to estimate the quality of
predicted object attributes. Based on these quality estimations, we propose a
quality-aware object association (QOA) strategy to leverage the quality score
as an important reference factor for achieving robust association. Despite its
simplicity, extensive experiments indicate that the proposed strategy
significantly boosts tracking performance by 2.2% AMOTA and our method
outperforms all existing state-of-the-art works on nuScenes by a large margin.
Moreover, QTrack achieves 48.0% and 51.1% AMOTA tracking performance on the
nuScenes validation and test sets, which significantly reduces the performance
gap between pure camera and LiDAR based trackers.
</p></li>
</ul>

<h3>Title: Evaluating Out-of-Distribution Detectors Through Adversarial Generation of Outliers. (arXiv:2208.10940v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10940">http://arxiv.org/abs/2208.10940</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10940] Evaluating Out-of-Distribution Detectors Through Adversarial Generation of Outliers](http://arxiv.org/abs/2208.10940)</code></li>
<li>Summary: <p>A reliable evaluation method is essential for building a robust
out-of-distribution (OOD) detector. Current robustness evaluation protocols for
OOD detectors rely on injecting perturbations to outlier data. However, the
perturbations are unlikely to occur naturally or not relevant to the content of
data, providing a limited assessment of robustness. In this paper, we propose
Evaluation-via-Generation for OOD detectors (EvG), a new protocol for
investigating the robustness of OOD detectors under more realistic modes of
variation in outliers. EvG utilizes a generative model to synthesize plausible
outliers, and employs MCMC sampling to find outliers misclassified as
in-distribution with the highest confidence by a detector. We perform a
comprehensive benchmark comparison of the performance of state-of-the-art OOD
detectors using EvG, uncovering previously overlooked weaknesses.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Transductive Decoupled Variational Inference for Few-Shot Classification. (arXiv:2208.10559v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10559">http://arxiv.org/abs/2208.10559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10559] Transductive Decoupled Variational Inference for Few-Shot Classification](http://arxiv.org/abs/2208.10559)</code></li>
<li>Summary: <p>The versatility to learn from a handful of samples is the hallmark of human
intelligence. Few-shot learning is an endeavour to transcend this capability
down to machines. Inspired by the promise and power of probabilistic deep
learning, we propose a novel variational inference network for few-shot
classification (coined as TRIDENT) to decouple the representation of an image
into semantic and label latent variables, and simultaneously infer them in an
intertwined fashion. To induce task-awareness, as part of the inference
mechanics of TRIDENT, we exploit information across both query and support
images of a few-shot task using a novel built-in attention-based transductive
feature extraction module (we call AttFEX). Our extensive experimental results
corroborate the efficacy of TRIDENT and demonstrate that, using the simplest of
backbones, it sets a new state-of-the-art in the most commonly adopted datasets
miniImageNet and tieredImageNet (offering up to 4% and 5% improvements,
respectively), as well as for the recent challenging cross-domain miniImagenet
--> CUB scenario offering a significant margin (up to 20% improvement) beyond
the best existing cross-domain baselines. Code and experimentation can be found
in our GitHub repository: https://github.com/anujinho/trident
</p></li>
</ul>

<h3>Title: A Constrained Deformable Convolutional Network for Efficient Single Image Dynamic Scene Blind Deblurring with Spatially-Variant Motion Blur Kernels Estimation. (arXiv:2208.10711v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10711">http://arxiv.org/abs/2208.10711</a></li>
<li>Code URL: <a href="https://github.com/wuyang1002431655/cdcn">https://github.com/wuyang1002431655/cdcn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10711] A Constrained Deformable Convolutional Network for Efficient Single Image Dynamic Scene Blind Deblurring with Spatially-Variant Motion Blur Kernels Estimation](http://arxiv.org/abs/2208.10711)</code></li>
<li>Summary: <p>Most existing deep-learning-based single image dynamic scene blind deblurring
(SIDSBD) methods usually design deep networks to directly remove the
spatially-variant motion blurs from one inputted motion blurred image, without
blur kernels estimation. In this paper, inspired by the Projective Motion Path
Blur (PMPB) model and deformable convolution, we propose a novel constrained
deformable convolutional network (CDCN) for efficient single image dynamic
scene blind deblurring, which simultaneously achieves accurate
spatially-variant motion blur kernels estimation and the high-quality image
restoration from only one observed motion blurred image. In our proposed CDCN,
we first construct a novel multi-scale multi-level multi-input multi-output
(MSML-MIMO) encoder-decoder architecture for more powerful features extraction
ability. Second, different from the DLVBD methods that use multiple consecutive
frames, a novel constrained deformable convolution reblurring (CDCR) strategy
is proposed, in which the deformable convolution is first applied to blurred
features of the inputted single motion blurred image for learning the sampling
points of motion blur kernel of each pixel, which is similar to the estimation
of the motion density function of the camera shake in the PMPB model, and then
a novel PMPB-based reblurring loss function is proposed to constrain the
learned sampling points convergence, which can make the learned sampling points
match with the relative motion trajectory of each pixel better and promote the
accuracy of the spatially-variant motion blur kernels estimation.
</p></li>
</ul>

<h3>Title: Multi-Modal Representation Learning with SAT for Commodity Verification. (arXiv:2208.11064v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11064">http://arxiv.org/abs/2208.11064</a></li>
<li>Code URL: <a href="https://github.com/hanchenchen/ccks2022-track2-solution">https://github.com/hanchenchen/ccks2022-track2-solution</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11064] Multi-Modal Representation Learning with SAT for Commodity Verification](http://arxiv.org/abs/2208.11064)</code></li>
<li>Summary: <p>In this paper, we propose a method to identify identical commodities. In
e-commerce scenarios, commodities are usually described by both images and
text. By definition, identical commodities are those that have identical key
attributes and are cognitively identical to consumers. There are two main
challenges: 1) The extraction and fusion of multi-modal representation. 2) The
ability to verify whether two commodities are identical by comparing the
distance between representations with a threshold. To address the above
problems, we propose an end-to-end identical commodity verification method
based on self-adaptive thresholds. We use a dual-stream network to extract
commodity embeddings and threshold embeddings separately and then concatenate
them to obtain commodity representation. Our method is able to obtain different
thresholds according to different commodities while maintaining the
indexability of the entire commodity representation. We experimentally validate
the effectiveness of our multimodal feature fusion and the advantages of
self-adaptive thresholds. Besides, our method achieves an F1 score of 0.8936
and takes the 3rd place on the leaderboard for the second task of the CCKS-2022
Knowledge Graph Evaluation for Digital Commerce Competition. Code and
pretrained models are available at
https://github.com/hanchenchen/CCKS2022-track2-solution.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: A Review of Federated Learning in Energy Systems. (arXiv:2208.10941v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10941">http://arxiv.org/abs/2208.10941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10941] A Review of Federated Learning in Energy Systems](http://arxiv.org/abs/2208.10941)</code></li>
<li>Summary: <p>With increasing concerns for data privacy and ownership, recent years have
witnessed a paradigm shift in machine learning (ML). An emerging paradigm,
federated learning (FL), has gained great attention and has become a novel
design for machine learning implementations. FL enables the ML model training
at data silos under the coordination of a central server, eliminating
communication overhead and without sharing raw data. In this paper, we conduct
a review of the FL paradigm and, in particular, compare the types, the network
structures, and the global model aggregation methods. Then, we conducted a
comprehensive review of FL applications in the energy domain (refer to the
smart grid in this paper). We provide a thematic classification of FL to
address a variety of energy-related problems, including demand response,
identification, prediction, and federated optimizations. We describe the
taxonomy in detail and conclude with a discussion of various aspects, including
challenges, opportunities, and limitations in its energy informatics
applications, such as energy system modeling and design, privacy, and
evolution.
</p></li>
</ul>

<h3>Title: FedMCSA: Personalized Federated Learning via Model Components Self-Attention. (arXiv:2208.10731v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10731">http://arxiv.org/abs/2208.10731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10731] FedMCSA: Personalized Federated Learning via Model Components Self-Attention](http://arxiv.org/abs/2208.10731)</code></li>
<li>Summary: <p>Federated learning (FL) facilitates multiple clients to jointly train a
machine learning model without sharing their private data. However, Non-IID
data of clients presents a tough challenge for FL. Existing personalized FL
approaches rely heavily on the default treatment of one complete model as a
basic unit and ignore the significance of different layers on Non-IID data of
clients. In this work, we propose a new framework, federated model components
self-attention (FedMCSA), to handle Non-IID data in FL, which employs model
components self-attention mechanism to granularly promote cooperation between
different clients. This mechanism facilitates collaboration between similar
model components while reducing interference between model components with
large differences. We conduct extensive experiments to demonstrate that FedMCSA
outperforms the previous methods on four benchmark datasets. Furthermore, we
empirically show the effectiveness of the model components self-attention
mechanism, which is complementary to existing personalized FL and can
significantly improve the performance of FL.
</p></li>
</ul>

<h3>Title: Application of federated learning techniques for arrhythmia classification using 12-lead ECG signals. (arXiv:2208.10993v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10993">http://arxiv.org/abs/2208.10993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10993] Application of federated learning techniques for arrhythmia classification using 12-lead ECG signals](http://arxiv.org/abs/2208.10993)</code></li>
<li>Summary: <p>Background: AI-based analysis of sufficiently large, curated medical datasets
has been shown to be promising for providing early detection, faster diagnosis,
better decision-making, and more effective treatment. However, accessing such
highly confidential and very sensitive medical data, obtained from a variety of
sources, is usually highly restricted since improper use, unsafe storage, data
leakage or abuse could violate a person's privacy. In this work we apply a
federated learning paradigm over a heterogeneous, siloed sets of
high-definition electrocardiogram arriving from 12-leads ECG sensors arrays to
train AI models. We evaluated the capacity of the resulting models to achieve
equivalent performance when compared to state-of-the-art models trained when
the same data is collected in a central place. Methods: We propose a privacy
preserving methodology for training AI models based on the federated learning
paradigm over a heterogeneous, distributed, dataset. The methodology is applied
to a broad range of machine learning techniques based on gradient boosting,
convolutional neural network and recurrent neural networks with long short-term
memory. The models were trained over a ECG dataset containing 12-leads
recordings collected from 43,059 patients from six geographically separate and
heterogeneous sources. Findings: The resulting set of AI models for detecting
cardiovascular abnormalities achieved comparable predictive performances
against models trained using a centralised learning approach. Interpretation:
The approach of compute parameters contributing to the global model locally and
then exchange only such parameters instead of the whole sensitive data as in ML
contributes to preserve medical data privacy.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Evaluation of group fairness measures in student performance prediction problems. (arXiv:2208.10625v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10625">http://arxiv.org/abs/2208.10625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10625] Evaluation of group fairness measures in student performance prediction problems](http://arxiv.org/abs/2208.10625)</code></li>
<li>Summary: <p>Predicting students' academic performance is one of the key tasks of
educational data mining (EDM). Traditionally, the high forecasting quality of
such models was deemed critical. More recently, the issues of fairness and
discrimination w.r.t. protected attributes, such as gender or race, have gained
attention. Although there are several fairness-aware learning approaches in
EDM, a comparative evaluation of these measures is still missing. In this
paper, we evaluate different group fairness measures for student performance
prediction problems on various educational datasets and fairness-aware learning
models. Our study shows that the choice of the fairness measure is important,
likewise for the choice of the grade threshold.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: GenTUS: Simulating User Behaviour and Language in Task-oriented Dialogues with Generative Transformers. (arXiv:2208.10817v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10817">http://arxiv.org/abs/2208.10817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10817] GenTUS: Simulating User Behaviour and Language in Task-oriented Dialogues with Generative Transformers](http://arxiv.org/abs/2208.10817)</code></li>
<li>Summary: <p>User simulators (USs) are commonly used to train task-oriented dialogue
systems (DSs) via reinforcement learning. The interactions often take place on
semantic level for efficiency, but there is still a gap from semantic actions
to natural language, which causes a mismatch between training and deployment
environment. Incorporating a natural language generation (NLG) module with USs
during training can partly deal with this problem. However, since the policy
and NLG of USs are optimised separately, these simulated user utterances may
not be natural enough in a given context. In this work, we propose a generative
transformer-based user simulator (GenTUS). GenTUS consists of an
encoder-decoder structure, which means it can optimise both the user policy and
natural language generation jointly. GenTUS generates both semantic actions and
natural language utterances, preserving interpretability and enhancing language
variation. In addition, by representing the inputs and outputs as word
sequences and by using a large pre-trained language model we can achieve
generalisability in feature representation. We evaluate GenTUS with automatic
metrics and human evaluation. Our results show that GenTUS generates more
natural language and is able to transfer to an unseen ontology in a zero-shot
fashion. In addition, its behaviour can be further shaped with reinforcement
learning opening the door to training specialised user simulators.
</p></li>
</ul>

<h3>Title: Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis. (arXiv:2208.10609v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10609">http://arxiv.org/abs/2208.10609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10609] Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis](http://arxiv.org/abs/2208.10609)</code></li>
<li>Summary: <p>Graph neural networks (GNNs) are highly effective on a variety of
graph-related tasks; however, they lack interpretability and transparency.
Current explainability approaches are typically local and treat GNNs as
black-boxes. They do not look inside the model, inhibiting human trust in the
model and explanations. Motivated by the ability of neurons to detect
high-level semantic concepts in vision models, we perform a novel analysis on
the behaviour of individual GNN neurons to answer questions about GNN
interpretability, and propose new metrics for evaluating the interpretability
of GNN neurons. We propose a novel approach for producing global explanations
for GNNs using neuron-level concepts to enable practitioners to have a
high-level view of the model. Specifically, (i) to the best of our knowledge,
this is the first work which shows that GNN neurons act as concept detectors
and have strong alignment with concepts formulated as logical compositions of
node degree and neighbourhood properties; (ii) we quantitatively assess the
importance of detected concepts, and identify a trade-off between training
duration and neuron-level interpretability; (iii) we demonstrate that our
global explainability approach has advantages over the current state-of-the-art
-- we can disentangle the explanation into individual interpretable concepts
backed by logical descriptions, which reduces potential for bias and improves
user-friendliness.
</p></li>
</ul>

<h3>Title: Application of Causal Inference to Analytical Customer Relationship Management in Banking and Insurance. (arXiv:2208.10916v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10916">http://arxiv.org/abs/2208.10916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10916] Application of Causal Inference to Analytical Customer Relationship Management in Banking and Insurance](http://arxiv.org/abs/2208.10916)</code></li>
<li>Summary: <p>Of late, in order to have better acceptability among various domain,
researchers have argued that machine intelligence algorithms must be able to
provide explanations that humans can understand causally. This aspect, also
known as causability, achieves a specific level of human-level explainability.
A specific class of algorithms known as counterfactuals may be able to provide
causability. In statistics, causality has been studied and applied for many
years, but not in great detail in artificial intelligence (AI). In a
first-of-its-kind study, we employed the principles of causal inference to
provide explainability for solving the analytical customer relationship
management (ACRM) problems. In the context of banking and insurance, current
research on interpretability tries to address causality-related questions like
why did this model make such decisions, and was the model's choice influenced
by a particular factor? We propose a solution in the form of an intervention,
wherein the effect of changing the distribution of features of ACRM datasets is
studied on the target feature. Subsequently, a set of counterfactuals is also
obtained that may be furnished to any customer who demands an explanation of
the decision taken by the bank/insurance company. Except for the credit card
churn prediction dataset, good quality counterfactuals were generated for the
loan default, insurance fraud detection, and credit card fraud detection
datasets, where changes in no more than three features are observed.
</p></li>
</ul>

<h3>Title: Regularized impurity reduction: Accurate decision trees with complexity guarantees. (arXiv:2208.10949v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.10949">http://arxiv.org/abs/2208.10949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.10949] Regularized impurity reduction: Accurate decision trees with complexity guarantees](http://arxiv.org/abs/2208.10949)</code></li>
<li>Summary: <p>Decision trees are popular classification models, providing high accuracy and
intuitive explanations. However, as the tree size grows the model
interpretability deteriorates. Traditional tree-induction algorithms, such as
C4.5 and CART, rely on impurity-reduction functions that promote the
discriminative power of each split. Thus, although these traditional methods
are accurate in practice, there has been no theoretical guarantee that they
will produce small trees. In this paper, we justify the use of a general family
of impurity functions, including the popular functions of entropy and
Gini-index, in scenarios where small trees are desirable, by showing that a
simple enhancement can equip them with complexity guarantees. We consider a
general setting, where objects to be classified are drawn from an arbitrary
probability distribution, classification can be binary or multi-class, and
splitting tests are associated with non-uniform costs. As a measure of tree
complexity, we adopt the expected cost to classify an object drawn from the
input distribution, which, in the uniform-cost case, is the expected number of
tests. We propose a tree-induction algorithm that gives a logarithmic
approximation guarantee on the tree complexity. This approximation factor is
tight up to a constant factor under mild assumptions. The algorithm recursively
selects a test that maximizes a greedy criterion defined as a weighted sum of
three components. The first two components encourage the selection of tests
that improve the balance and the cost-efficiency of the tree, respectively,
while the third impurity-reduction component encourages the selection of more
discriminative tests. As shown in our empirical evaluation, compared to the
original heuristics, the enhanced algorithms strike an excellent balance
between predictive accuracy and tree complexity.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
