<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-22</h1>
<h3>Title: Measuring Diversity in Co-creative Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Francisco Ibarrola, Kazjon Grace</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13826">https://arxiv.org/abs/2403.13826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13826">https://arxiv.org/pdf/2403.13826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13826]] Measuring Diversity in Co-creative Image Generation(https://arxiv.org/abs/2403.13826)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quality and diversity have been proposed as reasonable heuristics for assessing content generated by co-creative systems, but to date there has been little agreement around what constitutes the latter or how to measure it. Proposed approaches for assessing generative models in terms of diversity have limitations in that they compare the model's outputs to a ground truth that in the era of large pre-trained generative models might not be available, or entail an impractical number of computations. We propose an alternative based on entropy of neural network encodings for comparing diversity between sets of images that does not require ground-truth knowledge and is easy to compute. We also compare two pre-trained networks and show how the choice relates to the notion of diversity that we want to evaluate. We conclude with a discussion of the potential applications of these measures for ideation in interactive systems, model evaluation, and more broadly within computational creativity.</li>
</ul>

<h3>Title: SMART: Automatically Scaling Down Language Models with Accuracy  Guarantees for Reduced Processing Fees</h3>
<ul>
<li><strong>Authors: </strong>Saehan Jo, Immanuel Trummer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13835">https://arxiv.org/abs/2403.13835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13835">https://arxiv.org/pdf/2403.13835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13835]] SMART: Automatically Scaling Down Language Models with Accuracy  Guarantees for Reduced Processing Fees(https://arxiv.org/abs/2403.13835)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advancement of Large Language Models (LLMs) has significantly boosted performance in natural language processing (NLP) tasks. However, the deployment of high-performance LLMs incurs substantial costs, primarily due to the increased number of parameters aimed at enhancing model performance. This has made the use of state-of-the-art LLMs more expensive for end-users. AI service providers, such as OpenAI and Anthropic, often offer multiple versions of LLMs with varying prices and performance. However, end-users still face challenges in choosing the appropriate LLM for their tasks that balance result quality with cost. We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel LLM framework designed to minimize the inference costs of NLP tasks while ensuring sufficient result quality. It enables users to specify an accuracy constraint in terms of the equivalence of outputs to those of the most powerful LLM. SMART then generates results that deviate from the outputs of this LLM only with a probability below a user-defined threshold. SMART employs a profiling phase that evaluates the performance of multiple LLMs to identify those that meet the user-defined accuracy level. SMART optimizes the tradeoff between profiling overheads and the anticipated cost savings resulting from profiling. Moreover, our approach significantly reduces inference costs by strategically leveraging a mix of LLMs. Our experiments on three real-world datasets show that, based on OpenAI models, SMART achieves significant cost savings, up to 25.6x in comparison to GPT-4.</li>
</ul>

<h3>Title: Circuit Transformer: End-to-end Circuit Design by Predicting the Next  Gate</h3>
<ul>
<li><strong>Authors: </strong>Xihan Li, Xing Li, Lei Chen, Xing Zhang, Mingxuan Yuan, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13838">https://arxiv.org/abs/2403.13838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13838">https://arxiv.org/pdf/2403.13838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13838]] Circuit Transformer: End-to-end Circuit Design by Predicting the Next  Gate(https://arxiv.org/abs/2403.13838)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Language, a prominent human ability to express through sequential symbols, has been computationally mastered by recent advances of large language models (LLMs). By predicting the next word recurrently with huge neural models, LLMs have shown unprecedented capabilities in understanding and reasoning. Circuit, as the "language" of electronic design, specifies the functionality of an electronic device by cascade connections of logic gates. Then, can circuits also be mastered by a a sufficiently large "circuit model", which can conquer electronic design tasks by simply predicting the next logic gate? In this work, we take the first step to explore such possibilities. Two primary barriers impede the straightforward application of LLMs to circuits: their complex, non-sequential structure, and the intolerance of hallucination due to strict constraints (e.g., equivalence). For the first barrier, we encode a circuit as a memory-less, depth-first traversal trajectory, which allows Transformer-based neural models to better leverage its structural information, and predict the next gate on the trajectory as a circuit model. For the second barrier, we introduce an equivalence-preserving decoding process, which ensures that every token in the generated trajectory adheres to the specified equivalence constraints. Moreover, the circuit model can also be regarded as a stochastic policy to tackle optimization-oriented circuit design tasks. Experimentally, we trained a Transformer-based model of 88M parameters, named "Circuit Transformer", which demonstrates impressive performance in end-to-end logic synthesis. With Monte-Carlo tree search, Circuit Transformer significantly improves over resyn2 while retaining strict equivalence, showcasing the potential of generative AI in conquering electronic design challenges.</li>
</ul>

<h3>Title: Whose Side Are You On? Investigating the Political Stance of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pagnarasmey Pit, Xingjun Ma, Mike Conway, Qingyu Chen, James Bailey, Henry Pit, Putrasmey Keo, Watey Diep, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13840">https://arxiv.org/abs/2403.13840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13840">https://arxiv.org/pdf/2403.13840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13840]] Whose Side Are You On? Investigating the Political Stance of Large  Language Models(https://arxiv.org/abs/2403.13840)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have gained significant popularity for their application in various everyday tasks such as text generation, summarization, and information retrieval. As the widespread adoption of LLMs continues to surge, it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias. In this paper, we propose a quantitative framework and pipeline designed to systematically investigate the political orientation of LLMs. Our investigation delves into the political alignment of LLMs across a spectrum of eight polarizing topics, spanning from abortion to LGBTQ issues. Across topics, the results indicate that LLMs exhibit a tendency to provide responses that closely align with liberal or left-leaning perspectives rather than conservative or right-leaning ones when user queries include details pertaining to occupation, race, or political affiliation. The findings presented in this study not only reaffirm earlier observations regarding the left-leaning characteristics of LLMs but also surface particular attributes, such as occupation, that are particularly susceptible to such inclinations even when directly steered towards conservatism. As a recommendation to avoid these models providing politicised responses, users should be mindful when crafting queries, and exercise caution in selecting neutral prompt language.</li>
</ul>

<h3>Title: Integrating Wearable Sensor Data and Self-reported Diaries for  Personalized Affect Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhongqi Yang, Yuning Wang, Ken S. Yamashita, Maryam Sabah, Elahe Khatibi, Iman Azimi, Nikil Dutt, Jessica L. Borelli, Amir M. Rahmani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13841">https://arxiv.org/abs/2403.13841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13841">https://arxiv.org/pdf/2403.13841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13841]] Integrating Wearable Sensor Data and Self-reported Diaries for  Personalized Affect Forecasting(https://arxiv.org/abs/2403.13841)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Emotional states, as indicators of affect, are pivotal to overall health, making their accurate prediction before onset crucial. Current studies are primarily centered on immediate short-term affect detection using data from wearable and mobile devices. These studies typically focus on objective sensory measures, often neglecting other forms of self-reported information like diaries and notes. In this paper, we propose a multimodal deep learning model for affect status forecasting. This model combines a transformer encoder with a pre-trained language model, facilitating the integrated analysis of objective metrics and self-reported diaries. To validate our model, we conduct a longitudinal study, enrolling college students and monitoring them over a year, to collect an extensive dataset including physiological, environmental, sleep, metabolic, and physical activity parameters, alongside open-ended textual diaries provided by the participants. Our results demonstrate that the proposed model achieves predictive accuracy of 82.50% for positive affect and 82.76% for negative affect, a full week in advance. The effectiveness of our model is further elevated by its explainability.</li>
</ul>

<h3>Title: Machine Learning and Vision Transformers for Thyroid Carcinoma  Diagnosis: A review</h3>
<ul>
<li><strong>Authors: </strong>Yassine Habchi, Hamza Kheddar, Yassine Himeur, Abdelkrim Boukabou, Ammar Chouchane, Abdelmalik Ouamane, Shadi Atalla, Wathiq Mansoor</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13843">https://arxiv.org/abs/2403.13843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13843">https://arxiv.org/pdf/2403.13843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13843]] Machine Learning and Vision Transformers for Thyroid Carcinoma  Diagnosis: A review(https://arxiv.org/abs/2403.13843)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The growing interest in developing smart diagnostic systems to help medical experts process extensive data for treating incurable diseases has been notable. In particular, the challenge of identifying thyroid cancer (TC) has seen progress with the use of machine learning (ML) and big data analysis, incorporating transformers to evaluate TC prognosis and determine the risk of malignancy in individuals. This review article presents a summary of various studies on AIbased approaches, especially those employing transformers, for diagnosing TC. It introduces a new categorization system for these methods based on artifcial intelligence (AI) algorithms, the goals of the framework, and the computing environments used. Additionally, it scrutinizes and contrasts the available TC datasets by their features. The paper highlights the importance of AI instruments in aiding the diagnosis and treatment of TC through supervised, unsupervised, or mixed approaches, with a special focus on the ongoing importance of transformers in medical diagnostics and disease management. It further discusses the progress made and the continuing obstacles in this area. Lastly, it explores future directions and focuses within this research feld.</li>
</ul>

<h3>Title: Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting  Framework for Incremental Zero-Shot Fault Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Jiancheng Zhao, Jiaqi Yue, Chunhui Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13845">https://arxiv.org/abs/2403.13845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13845">https://arxiv.org/pdf/2403.13845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13845]] Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting  Framework for Incremental Zero-Shot Fault Diagnosis(https://arxiv.org/abs/2403.13845)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Zero-shot fault diagnosis (ZSFD) is capable of identifying unseen faults via predicting fault attributes labeled by human experts. We first recognize the demand of ZSFD to deal with continuous changes in industrial processes, i.e., the model's ability to adapt to new fault categories and attributes while avoiding forgetting the diagnosis ability learned previously. To overcome the issue that the existing ZSFD paradigm cannot learn from evolving streams of training data in industrial scenarios, the incremental ZSFD (IZSFD) paradigm is proposed for the first time, which incorporates category increment and attribute increment for both traditional ZSFD and generalized ZSFD paradigms. To achieve IZSFD, we present a broad-deep mixed anti-forgetting framework (BDMAFF) that aims to learn from new fault categories and attributes. To tackle the issue of forgetting, BDMAFF effectively accumulates previously acquired knowledge from two perspectives: features and attribute prototypes. The feature memory is established through a deep generative model that employs anti-forgetting training strategies, ensuring the generation quality of historical categories is supervised and maintained. The diagnosis model SEEs the UNSEEN faults with the help of generated samples from the generative model. The attribute prototype memory is established through a diagnosis model inspired by the broad learning system. Unlike traditional incremental learning algorithms, BDMAFF introduces a memory-driven iterative update strategy for the diagnosis model, which allows the model to learn new faults and attributes without requiring the storage of all historical training samples. The effectiveness of the proposed method is verified by a real hydraulic system and the Tennessee-Eastman benchmark process.</li>
</ul>

<h3>Title: A Clustering Method with Graph Maximum Decoding Information</h3>
<ul>
<li><strong>Authors: </strong>Xinrun Xu, Manying Lv, Yurong Wu, Zhanbiao Lian, Zhiming Ding, Jin Yan, Shan Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13846">https://arxiv.org/abs/2403.13846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13846">https://arxiv.org/pdf/2403.13846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13846]] A Clustering Method with Graph Maximum Decoding Information(https://arxiv.org/abs/2403.13846)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>The clustering method based on graph models has garnered increased attention for its widespread applicability across various knowledge domains. Its adaptability to integrate seamlessly with other relevant applications endows the graph model-based clustering analysis with the ability to robustly extract "natural associations" or "graph structures" within datasets, facilitating the modelling of relationships between data points. Despite its efficacy, the current clustering method utilizing the graph-based model overlooks the uncertainty associated with random walk access between nodes and the embedded structural information in the data. To address this gap, we present a novel Clustering method for Maximizing Decoding Information within graph-based models, named CMDI. CMDI innovatively incorporates two-dimensional structural information theory into the clustering process, consisting of two phases: graph structure extraction and graph vertex partitioning. Within CMDI, graph partitioning is reformulated as an abstract clustering problem, leveraging maximum decoding information to minimize uncertainty associated with random visits to vertices. Empirical evaluations on three real-world datasets demonstrate that CMDI outperforms classical baseline methods, exhibiting a superior decoding information ratio (DI-R). Furthermore, CMDI showcases heightened efficiency, particularly when considering prior knowledge (PK). These findings underscore the effectiveness of CMDI in enhancing decoding information quality and computational efficiency, positioning it as a valuable tool in graph-based clustering analyses.</li>
</ul>

<h3>Title: Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule  Lists</h3>
<ul>
<li><strong>Authors: </strong>Timothée Ly (LAAS-ROC), Julien Ferry (EPM), Marie-José Huguet (LAAS-ROC), Sébastien Gambs (UQAM), Ulrich Aivodji (ETS)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13848">https://arxiv.org/abs/2403.13848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13848">https://arxiv.org/pdf/2403.13848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13848]] Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule  Lists(https://arxiv.org/abs/2403.13848)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Differentially-private (DP) mechanisms can be embedded into the design of a machine learningalgorithm to protect the resulting model against privacy leakage, although this often comes with asignificant loss of accuracy. In this paper, we aim at improving this trade-off for rule lists modelsby establishing the smooth sensitivity of the Gini impurity and leveraging it to propose a DP greedyrule list algorithm. In particular, our theoretical analysis and experimental results demonstrate thatthe DP rule lists models integrating smooth sensitivity have higher accuracy that those using otherDP frameworks based on global sensitivity.</li>
</ul>

<h3>Title: Spatio-Temporal Fluid Dynamics Modeling via Physical-Awareness and  Parameter Diffusion Guidance</h3>
<ul>
<li><strong>Authors: </strong>Hao Wu, Fan Xu, Yifan Duan, Ziwei Niu, Weiyan Wang, Gaofeng Lu, Kun Wang, Yuxuan Liang, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13850">https://arxiv.org/abs/2403.13850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13850">https://arxiv.org/pdf/2403.13850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13850]] Spatio-Temporal Fluid Dynamics Modeling via Physical-Awareness and  Parameter Diffusion Guidance(https://arxiv.org/abs/2403.13850)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>This paper proposes a two-stage framework named ST-PAD for spatio-temporal fluid dynamics modeling in the field of earth sciences, aiming to achieve high-precision simulation and prediction of fluid dynamics through spatio-temporal physics awareness and parameter diffusion guidance. In the upstream stage, we design a vector quantization reconstruction module with temporal evolution characteristics, ensuring balanced and resilient parameter distribution by introducing general physical constraints. In the downstream stage, a diffusion probability network involving parameters is utilized to generate high-quality future states of fluids, while enhancing the model's generalization ability by perceiving parameters in various physical setups. Extensive experiments on multiple benchmark datasets have verified the effectiveness and robustness of the ST-PAD framework, which showcase that ST-PAD outperforms current mainstream models in fluid dynamics modeling and prediction, especially in effectively capturing local representations and maintaining significant advantages in OOD generations.</li>
</ul>

<h3>Title: DiffImpute: Tabular Data Imputation With Denoising Diffusion  Probabilistic Model</h3>
<ul>
<li><strong>Authors: </strong>Yizhu Wen, Kai Yi, Jing Ke, Yiqing Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13863">https://arxiv.org/abs/2403.13863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13863">https://arxiv.org/pdf/2403.13863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13863]] DiffImpute: Tabular Data Imputation With Denoising Diffusion  Probabilistic Model(https://arxiv.org/abs/2403.13863)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Tabular data plays a crucial role in various domains but often suffers from missing values, thereby curtailing its potential utility. Traditional imputation techniques frequently yield suboptimal results and impose substantial computational burdens, leading to inaccuracies in subsequent modeling tasks. To address these challenges, we propose DiffImpute, a novel Denoising Diffusion Probabilistic Model (DDPM). Specifically, DiffImpute is trained on complete tabular datasets, ensuring that it can produce credible imputations for missing entries without undermining the authenticity of the existing data. Innovatively, it can be applied to various settings of Missing Completely At Random (MCAR) and Missing At Random (MAR). To effectively handle the tabular features in DDPM, we tailor four tabular denoising networks, spanning MLP, ResNet, Transformer, and U-Net. We also propose Harmonization to enhance coherence between observed and imputed data by infusing the data back and denoising them multiple times during the sampling stage. To enable efficient inference while maintaining imputation performance, we propose a refined non-Markovian sampling process that works along with Harmonization. Empirical evaluations on seven diverse datasets underscore the prowess of DiffImpute. Specifically, when paired with the Transformer as the denoising network, it consistently outperforms its competitors, boasting an average ranking of 1.7 and the most minimal standard deviation. In contrast, the next best method lags with a ranking of 2.8 and a standard deviation of 0.9. The code is available at https://github.com/Dendiiiii/DiffImpute.</li>
</ul>

<h3>Title: Optimal Transport for Fairness: Archival Data Repair using Small  Research Data Sets</h3>
<ul>
<li><strong>Authors: </strong>Abigail Langbridge, Anthony Quinn, Robert Shorten</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13864">https://arxiv.org/abs/2403.13864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13864">https://arxiv.org/pdf/2403.13864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13864]] Optimal Transport for Fairness: Archival Data Repair using Small  Research Data Sets(https://arxiv.org/abs/2403.13864)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>With the advent of the AI Act and other regulations, there is now an urgent need for algorithms that repair unfairness in training data. In this paper, we define fairness in terms of conditional independence between protected attributes ($S$) and features ($X$), given unprotected attributes ($U$). We address the important setting in which torrents of archival data need to be repaired, using only a small proportion of these data, which are $S|U$-labelled (the research data). We use the latter to design optimal transport (OT)-based repair plans on interpolated supports. This allows {\em off-sample}, labelled, archival data to be repaired, subject to stationarity assumptions. It also significantly reduces the size of the supports of the OT plans, with correspondingly large savings in the cost of their design and of their {\em sequential\/} application to the off-sample data. We provide detailed experimental results with simulated and benchmark real data (the Adult data set). Our performance figures demonstrate effective repair -- in the sense of quenching conditional dependence -- of large quantities of off-sample, labelled (archival) data.</li>
</ul>

<h3>Title: The Bid Picture: Auction-Inspired Multi-player Generative Adversarial  Networks Training</h3>
<ul>
<li><strong>Authors: </strong>Joo Yong Shim, Jean Seong Bjorn Choe, Jong-Kook Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13866">https://arxiv.org/abs/2403.13866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13866">https://arxiv.org/pdf/2403.13866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13866]] The Bid Picture: Auction-Inspired Multi-player Generative Adversarial  Networks Training(https://arxiv.org/abs/2403.13866)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This article proposes auction-inspired multi-player generative adversarial networks training, which mitigates the mode collapse problem of GANs. Mode collapse occurs when an over-fitted generator generates a limited range of samples, often concentrating on a small subset of the data distribution. Despite the restricted diversity of generated samples, the discriminator can still be deceived into distinguishing these samples as real samples from the actual distribution. In the absence of external standards, a model cannot recognize its failure during the training phase. We extend the two-player game of generative adversarial networks to the multi-player game. During the training, the values of each model are determined by the bids submitted by other players in an auction-like process.</li>
</ul>

<h3>Title: Capsule Neural Networks as Noise Stabilizer for Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Soyeon Kim, Jihyeon Seong, Hyunkyung Han, Jaesik Choi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13867">https://arxiv.org/abs/2403.13867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13867">https://arxiv.org/pdf/2403.13867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13867]] Capsule Neural Networks as Noise Stabilizer for Time Series Data(https://arxiv.org/abs/2403.13867)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Capsule Neural Networks utilize capsules, which bind neurons into a single vector and learn position equivariant features, which makes them more robust than original Convolutional Neural Networks. CapsNets employ an affine transformation matrix and dynamic routing with coupling coefficients to learn robustly. In this paper, we investigate the effectiveness of CapsNets in analyzing highly sensitive and noisy time series sensor data. To demonstrate CapsNets robustness, we compare their performance with original CNNs on electrocardiogram data, a medical time series sensor data with complex patterns and noise. Our study provides empirical evidence that CapsNets function as noise stabilizers, as investigated by manual and adversarial attack experiments using the fast gradient sign method and three manual attacks, including offset shifting, gradual drift, and temporal lagging. In summary, CapsNets outperform CNNs in both manual and adversarial attacked data. Our findings suggest that CapsNets can be effectively applied to various sensor systems to improve their resilience to noise attacks. These results have significant implications for designing and implementing robust machine learning models in real world applications. Additionally, this study contributes to the effectiveness of CapsNet models in handling noisy data and highlights their potential for addressing the challenges of noise data in time series analysis.</li>
</ul>

<h3>Title: ExMap: Leveraging Explainability Heatmaps for Unsupervised Group  Robustness to Spurious Correlations</h3>
<ul>
<li><strong>Authors: </strong>Rwiddhi Chakraborty, Adrian Sletten, Michael Kampffmeyer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13870">https://arxiv.org/abs/2403.13870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13870">https://arxiv.org/pdf/2403.13870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13870]] ExMap: Leveraging Explainability Heatmaps for Unsupervised Group  Robustness to Spurious Correlations(https://arxiv.org/abs/2403.13870)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Group robustness strategies aim to mitigate learned biases in deep learning models that arise from spurious correlations present in their training datasets. However, most existing methods rely on the access to the label distribution of the groups, which is time-consuming and expensive to obtain. As a result, unsupervised group robustness strategies are sought. Based on the insight that a trained model's classification strategies can be inferred accurately based on explainability heatmaps, we introduce ExMap, an unsupervised two stage mechanism designed to enhance group robustness in traditional classifiers. ExMap utilizes a clustering module to infer pseudo-labels based on a model's explainability heatmaps, which are then used during training in lieu of actual labels. Our empirical studies validate the efficacy of ExMap - We demonstrate that it bridges the performance gap with its supervised counterparts and outperforms existing partially supervised and unsupervised methods. Additionally, ExMap can be seamlessly integrated with existing group robustness learning strategies. Finally, we demonstrate its potential in tackling the emerging issue of multiple shortcut mitigation\footnote{Code available at \url{https://github.com/rwchakra/exmap}}.</li>
</ul>

<h3>Title: Data Acquisition via Experimental Design for Decentralized Data Markets</h3>
<ul>
<li><strong>Authors: </strong>Charles Lu, Baihe Huang, Sai Praneeth Karimireddy, Praneeth Vepakomma, Michael Jordan, Ramesh Raskar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13893">https://arxiv.org/abs/2403.13893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13893">https://arxiv.org/pdf/2403.13893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13893]] Data Acquisition via Experimental Design for Decentralized Data Markets(https://arxiv.org/abs/2403.13893)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Acquiring high-quality training data is essential for current machine learning models. Data markets provide a way to increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data sellers to join the market. A major challenge for a data buyer in such a market is selecting the most valuable data points from a data seller. Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data selection problem that is inspired by linear experimental design. Our proposed data selection method achieves lower prediction error without requiring labeled validation data and can be optimized in a fast and federated procedure. The key insight of our work is that a method that directly estimates the benefit of acquiring data for test set prediction is particularly compatible with a decentralized market setting.</li>
</ul>

<h3>Title: CoMo: Controllable Motion Generation through Language Guided Pose Code  Editing</h3>
<ul>
<li><strong>Authors: </strong>Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch, Mark Yatskar, Lingjie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13900">https://arxiv.org/abs/2403.13900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13900">https://arxiv.org/pdf/2403.13900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13900]] CoMo: Controllable Motion Generation through Language Guided Pose Code  Editing(https://arxiv.org/abs/2403.13900)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-to-motion models excel at efficient human motion generation, but existing approaches lack fine-grained controllability over the generation process. Consequently, modifying subtle postures within a motion or inserting new actions at specific moments remains a challenge, limiting the applicability of these methods in diverse scenarios. In light of these challenges, we introduce CoMo, a Controllable Motion generation model, adept at accurately generating and editing motions by leveraging the knowledge priors of large language models (LLMs). Specifically, CoMo decomposes motions into discrete and semantically meaningful pose codes, with each code encapsulating the semantics of a body part, representing elementary information such as "left knee slightly bent". Given textual inputs, CoMo autoregressively generates sequences of pose codes, which are then decoded into 3D motions. Leveraging pose codes as interpretable representations, an LLM can directly intervene in motion editing by adjusting the pose codes according to editing instructions. Experiments demonstrate that CoMo achieves competitive performance in motion generation compared to state-of-the-art models while, in human studies, CoMo substantially surpasses previous work in motion editing abilities.</li>
</ul>

<h3>Title: Train & Constrain: Phonologically Informed Tongue-Twister Generation  from Topics and Paraphrases</h3>
<ul>
<li><strong>Authors: </strong>Tyler Loakman, Chen Tang, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13901">https://arxiv.org/abs/2403.13901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13901">https://arxiv.org/pdf/2403.13901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13901]] Train & Constrain: Phonologically Informed Tongue-Twister Generation  from Topics and Paraphrases(https://arxiv.org/abs/2403.13901)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Previous work in phonologically and phonetically grounded language generation has mainly focused on domains such as puns and poetry. In this article, we present new work on the generation of tongue-twisters - a form of language that is required to be conditioned on a phoneme level to maximize sound overlap, whilst maintaining semantic consistency with an input topic and still being grammatically correct. We present TwisterLister, a pipeline for generating phonologically informed tongue-twisters from Large Language Models (LLMs) that we use to generate TwistList 2.0, the largest annotated dataset of tongue-twisters to date, consisting of 17K+ examples from a combination of human and LLM authors. Our generation pipeline involves the use of a phonologically constrained vocabulary alongside LLM prompting to generate novel, non-derivative tongue-twister examples. We additionally present the results of automatic and human evaluation of smaller models trained on our generated dataset to demonstrate the extent to which phonologically motivated language types can be generated without explicit injection of phonological knowledge. Additionally, we introduce a Phoneme-Aware Constrained Decoding module (PACD) that can be integrated into any causal language model and demonstrate that this method generates good quality tongue-twisters both with and without fine-tuning the underlying language model. We also design and implement a range of automatic metrics for the task of tongue-twister generation that is phonologically motivated and captures the unique essence of tongue-twisters based on Phonemic Edit Distance (PED).</li>
</ul>

<h3>Title: Leveraging Linguistically Enhanced Embeddings for Open Information  Extraction</h3>
<ul>
<li><strong>Authors: </strong>Fauzan Farooqui, Thanmay Jayakumar, Pulkit Mathur, Mansi Radke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13903">https://arxiv.org/abs/2403.13903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13903">https://arxiv.org/pdf/2403.13903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13903]] Leveraging Linguistically Enhanced Embeddings for Open Information  Extraction(https://arxiv.org/abs/2403.13903)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Open Information Extraction (OIE) is a structured prediction (SP) task in Natural Language Processing (NLP) that aims to extract structured $n$-ary tuples - usually subject-relation-object triples - from free text. The word embeddings in the input text can be enhanced with linguistic features, usually Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However, past enhancement techniques cannot leverage the power of pretrained language models (PLMs), which themselves have been hardly used for OIE. To bridge this gap, we are the first to leverage linguistic features with a Seq2Seq PLM for OIE. We do so by introducing two methods - Weighted Addition and Linearized Concatenation. Our work can give any neural OIE architecture the key performance boost from both PLMs and linguistic features in one go. In our settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on Precision, Recall and F1 scores respectively over the baseline. Beyond this, we address other important challenges in the field: to reduce compute overheads with the features, we are the first ones to exploit Semantic Dependency Parse (SemDP) tags; to address flaws in current datasets, we create a clean synthetic dataset; finally, we contribute the first known study of OIE behaviour in SP models.</li>
</ul>

<h3>Title: Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and  Style Transfer Techniques</h3>
<ul>
<li><strong>Authors: </strong>W. Tang, D. Figueroa, D. Liu, K. Johnsson, A. Sopasakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13916">https://arxiv.org/abs/2403.13916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13916">https://arxiv.org/pdf/2403.13916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13916]] Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and  Style Transfer Techniques(https://arxiv.org/abs/2403.13916)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present novel approaches involving generative adversarial networks and diffusion models in order to synthesize high quality, live and spoof fingerprint images while preserving features such as uniqueness and diversity. We generate live fingerprints from noise with a variety of methods, and we use image translation techniques to translate live fingerprint images to spoof. To generate different types of spoof images based on limited training data we incorporate style transfer techniques through a cycle autoencoder equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability. We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation. We assess the diversity and realism of the generated live fingerprint images mainly through the Fr\'echet Inception Distance (FID) and the False Acceptance Rate (FAR). Our best diffusion model achieved a FID of 15.78. The comparable WGAN-GP model achieved slightly higher FID while performing better in the uniqueness assessment due to a slightly lower FAR when matched against the training data, indicating better creativity. Moreover, we give example images showing that a DDPM model clearly can generate realistic fingerprint images.</li>
</ul>

<h3>Title: Visually Grounded Speech Models have a Mutual Exclusivity Bias</h3>
<ul>
<li><strong>Authors: </strong>Leanne Nortje, Dan Oneaţă, Yevgen Matusevych, Herman Kamper</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13922">https://arxiv.org/abs/2403.13922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13922">https://arxiv.org/pdf/2403.13922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13922]] Visually Grounded Speech Models have a Mutual Exclusivity Bias(https://arxiv.org/abs/2403.13922)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: a novel word is mapped to a novel object rather than a familiar one. This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words. We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio. Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word. To simulate prior acoustic and visual knowledge, we experiment with several initialisation strategies using pretrained speech and vision networks. Our findings reveal the ME bias across the different initialisation approaches, with a stronger bias in models with more prior (in particular, visual) knowledge. Additional tests confirm the robustness of our results, even when different loss functions are considered.</li>
</ul>

<h3>Title: Reducing Large Language Model Bias with Emphasis on 'Restricted  Industries': Automated Dataset Augmentation and Prejudice Quantification</h3>
<ul>
<li><strong>Authors: </strong>Devam Mondal, Carlo Lipizzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13925">https://arxiv.org/abs/2403.13925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13925">https://arxiv.org/pdf/2403.13925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13925]] Reducing Large Language Model Bias with Emphasis on 'Restricted  Industries': Automated Dataset Augmentation and Prejudice Quantification(https://arxiv.org/abs/2403.13925)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the growing capabilities of large language models, there exists concerns about the biases they develop. In this paper, we propose a novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers and in the context of 'restricted industries' with limited data. We additionally create two new additional metrics, the mb-index and db-index, to quantify bias, considering the idea that bias occurs due to both intrinsic model architecture and dataset.</li>
</ul>

<h3>Title: ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual  Try-On</h3>
<ul>
<li><strong>Authors: </strong>Jeffrey Zhang, Kedan Li, Shao-Yu Chang, David Forsyth</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13951">https://arxiv.org/abs/2403.13951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13951">https://arxiv.org/pdf/2403.13951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13951]] ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual  Try-On(https://arxiv.org/abs/2403.13951)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Virtual Try-on (VTON) involves generating images of a person wearing selected garments. Diffusion-based methods, in particular, can create high-quality images, but they struggle to maintain the identities of the input garments. We identified this problem stems from the specifics in the training formulation for diffusion. To address this, we propose a unique training scheme that limits the scope in which diffusion is trained. We use a control image that perfectly aligns with the target image during training. In turn, this accurately preserves garment details during inference. We demonstrate our method not only effectively conserves garment details but also allows for layering, styling, and shoe try-on. Our method runs multi-garment try-on in a single inference cycle and can support high-quality zoomed-in generations without training in higher resolutions. Finally, we show our method surpasses prior methods in accuracy and quality.</li>
</ul>

<h3>Title: ConGeo: Robust Cross-view Geo-localization across Ground View Variations</h3>
<ul>
<li><strong>Authors: </strong>Li Mi, Chang Xu, Javiera Castillo-Navarro, Syrielle Montariol, Wen Yang, Antoine Bosselut, Devis Tuia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13965">https://arxiv.org/abs/2403.13965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13965">https://arxiv.org/pdf/2403.13965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13965]] ConGeo: Robust Cross-view Geo-localization across Ground View Variations(https://arxiv.org/abs/2403.13965)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cross-view geo-localization aims at localizing a ground-level query image by matching it to its corresponding geo-referenced aerial view. In real-world scenarios, the task requires accommodating diverse ground images captured by users with varying orientations and reduced field of views (FoVs). However, existing learning pipelines are orientation-specific or FoV-specific, demanding separate model training for different ground view variations. Such models heavily depend on the North-aligned spatial correspondence and predefined FoVs in the training data, compromising their robustness across different settings. To tackle this challenge, we propose ConGeo, a single- and cross-modal Contrastive method for Geo-localization: it enhances robustness and consistency in feature representations to improve a model's invariance to orientation and its resilience to FoV variations, by enforcing proximity between ground view variations of the same location. As a generic learning objective for cross-view geo-localization, when integrated into state-of-the-art pipelines, ConGeo significantly boosts the performance of three base models on four geo-localization benchmarks for diverse ground view variations and outperforms competing methods that train separate models for each ground view variation.</li>
</ul>

<h3>Title: SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing</h3>
<ul>
<li><strong>Authors: </strong>Florian Strohm, Mihai Bâce, Markus Kaltenecker, Andreas Bulling</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.13972">https://arxiv.org/abs/2403.13972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.13972">https://arxiv.org/pdf/2403.13972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.13972]] SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing(https://arxiv.org/abs/2403.13972)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We propose Semantic Facial Feature Control (SeFFeC) - a novel method for fine-grained face shape editing. Our method enables the manipulation of human-understandable, semantic face features, such as nose length or mouth width, which are defined by different groups of facial landmarks. In contrast to existing methods, the use of facial landmarks enables precise measurement of the facial features, which then enables training SeFFeC without any manually annotated labels. SeFFeC consists of a transformer-based encoder network that takes a latent vector of a pre-trained generative model and a facial feature embedding as input, and learns to modify the latent vector to perform the desired face edit operation. To ensure that the desired feature measurement is changed towards the target value without altering uncorrelated features, we introduced a novel semantic face feature loss. Qualitative and quantitative results show that SeFFeC enables precise and fine-grained control of 23 facial features, some of which could not previously be controlled by other methods, without requiring manual annotations. Unlike existing methods, SeFFeC also provides deterministic control over the exact values of the facial features and more localised and disentangled face edits.</li>
</ul>

<h3>Title: Uncertainty Driven Active Learning for Image Segmentation in Underwater  Inspection</h3>
<ul>
<li><strong>Authors: </strong>Luiza Ribeiro Marnet, Yury Brodskiy, Stella Grasshof, Andrzej Wasowski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14002">https://arxiv.org/abs/2403.14002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14002">https://arxiv.org/pdf/2403.14002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14002]] Uncertainty Driven Active Learning for Image Segmentation in Underwater  Inspection(https://arxiv.org/abs/2403.14002)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Active learning aims to select the minimum amount of data to train a model that performs similarly to a model trained with the entire dataset. We study the potential of active learning for image segmentation in underwater infrastructure inspection tasks, where large amounts of data are typically collected. The pipeline inspection images are usually semantically repetitive but with great variations in quality. We use mutual information as the acquisition function, calculated using Monte Carlo dropout. To assess the effectiveness of the framework, DenseNet and HyperSeg are trained with the CamVid dataset using active learning. In addition, HyperSeg is trained with a pipeline inspection dataset of over 50,000 images. For the pipeline dataset, HyperSeg with active learning achieved 67.5% meanIoU using 12.5% of the data, and 61.4% with the same amount of randomly selected images. This shows that using active learning for segmentation models in underwater inspection tasks can lower the cost significantly.</li>
</ul>

<h3>Title: Multi-Modal Hallucination Control by Visual Information Grounding</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14003">https://arxiv.org/abs/2403.14003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14003">https://arxiv.org/pdf/2403.14003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14003]] Multi-Modal Hallucination Control by Visual Information Grounding(https://arxiv.org/abs/2403.14003)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image. We investigate this phenomenon, usually referred to as "hallucination" and show that it stems from an excessive reliance on the language prior. In particular, we show that as more tokens are generated, the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations. To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification. M3ID amplifies the influence of the reference image over the language prior, hence favoring the generation of tokens with higher mutual information with the visual prompt. M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training and with minimal computational overhead. If training is an option, we show that M3ID can be paired with Direct Preference Optimization (DPO) to improve the model's reliance on the prompt image without requiring any labels. Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24%.</li>
</ul>

<h3>Title: A Signal Injection Attack Against Zero Involvement Pairing and  Authentication for the Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Isaac Ahlgren, Jack West, Kyuin Lee, George Thiruvathukal, Neil Klingensmith</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14018">https://arxiv.org/abs/2403.14018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14018">https://arxiv.org/pdf/2403.14018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14018]] A Signal Injection Attack Against Zero Involvement Pairing and  Authentication for the Internet of Things(https://arxiv.org/abs/2403.14018)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>Zero Involvement Pairing and Authentication (ZIPA) is a promising technique for autoprovisioning large networks of Internet-of-Things (IoT) devices. In this work, we present the first successful signal injection attack on a ZIPA system. Most existing ZIPA systems assume there is a negligible amount of influence from the unsecured outside space on the secured inside space. In reality, environmental signals do leak from adjacent unsecured spaces and influence the environment of the secured space. Our attack takes advantage of this fact to perform a signal injection attack on the popular Schurmann & Sigg algorithm. The keys generated by the adversary with a signal injection attack at 95 dBA is within the standard error of the legitimate device.</li>
</ul>

<h3>Title: Zero-Knowledge Proof of Distinct Identity: a Standard-compatible  Sybil-resistant Pseudonym Extension for C-ITS</h3>
<ul>
<li><strong>Authors: </strong>Ye Tao, Hongyi Wu, Ehsan Javanmardi, Manabu Tsukada, Hiroshi Esaki</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14020">https://arxiv.org/abs/2403.14020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14020">https://arxiv.org/pdf/2403.14020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14020]] Zero-Knowledge Proof of Distinct Identity: a Standard-compatible  Sybil-resistant Pseudonym Extension for C-ITS(https://arxiv.org/abs/2403.14020)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Pseudonyms are widely used in Cooperative Intelligent Transport Systems (C-ITS) to protect the location privacy of vehicles. However, the unlinkability nature of pseudonyms also enables Sybil attacks, where a malicious vehicle can pretend to be multiple vehicles at the same time. In this paper, we propose a novel protocol called zero-knowledge Proof of Distinct Identity (zk-PoDI,) which allows a vehicle to prove that it is not the owner of another pseudonym in the local area, without revealing its actual identity. Zk-PoDI is based on the Diophantine equation and zk-SNARK, and does not rely on any specific pseudonym design or infrastructure assistance. We show that zk-PoDI satisfies all the requirements for a practical Sybil-resistance pseudonym system, and it has low latency, adjustable difficulty, moderate computation overhead, and negligible communication cost. We also discuss the future work of implementing and evaluating zk-PoDI in a realistic city-scale simulation environment.</li>
</ul>

<h3>Title: A system capable of verifiably and privately screening global DNA  synthesis</h3>
<ul>
<li><strong>Authors: </strong>Carsten Baum (1 and 2), Jens Berlips (3), Walther Chen (3), Hongrui Cui (4), Ivan Damgard (1), Jiangbin Dong (5), Kevin M. Esvelt (3 and 6), Mingyu Gao (5 and 12), Dana Gretton (3 and 6), Leonard Foner (3), Martin Kysel (3), Kaiyi Zhang (4), Juanru Li (4), Xiang Li (5), Omer Paneth (7), Ronald L. Rivest (7), Francesca Sage-Ling (3), Adi Shamir (8), Yue Shen (10), Meicen Sun (11), Vinod Vaikuntanathan (7), Lynn Van Hauwe (3), Theia Vogel (3), Benjamin Weinstein-Raun (3), Yun Wang (10), Daniel Wichs (9), Stephen Wooster (3), Andrew C. Yao (3 and 5 and 12), Yu Yu (4 and 12), Haoling Zhang (10) ((1) Department of Computer Science, Aarhus University, Denmark, (2) DTU Compute, Technical University of Denmark, Denmark, (3) SecureDNA Foundation, Switzerland, (4) Department of Computer Science and Engineering, Shanghai Jiao Tong University, China, (5) Institute for Interdisciplinary Information Sciences, Tsinghua University, China, (6) Media Lab, Massachusetts Institute of Technology, USA, (7) Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, USA, (8) Department of Applied Mathematics, Weizmann Institute of Science, Israel, (9) Department of Computer Science, Northeastern University, USA, (10) China National GeneBank, China, (11) Department of Political Science, Massachusetts Institute of Technology, USA, (12) Shanghai Qi Zhi Institute, China)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14023">https://arxiv.org/abs/2403.14023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14023">https://arxiv.org/pdf/2403.14023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14023]] A system capable of verifiably and privately screening global DNA  synthesis(https://arxiv.org/abs/2403.14023)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Printing custom DNA sequences is essential to scientific and biomedical research, but the technology can be used to manufacture plagues as well as cures. Just as ink printers recognize and reject attempts to counterfeit money, DNA synthesizers and assemblers should deny unauthorized requests to make viral DNA that could be used to ignite a pandemic. There are three complications. First, we don't need to quickly update printers to deal with newly discovered currencies, whereas we regularly learn of new viruses and other biological threats. Second, anti-counterfeiting specifications on a local printer can't be extracted and misused by malicious actors, unlike information on biological threats. Finally, any screening must keep the inspected DNA sequences private, as they may constitute valuable trade secrets. Here we describe SecureDNA, a free, privacy-preserving, and fully automated system capable of verifiably screening all DNA synthesis orders of 30+ base pairs against an up-to-date database of hazards, and its operational performance and specificity when applied to 67 million base pairs of DNA synthesized by providers in the United States, Europe, and China.</li>
</ul>

<h3>Title: EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship  Detection through Edge-Cloud Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Huang, Hanning Chen, Yang Ni, Arghavan Rezvani, Sanggeon Yun, Sungheon Jeon, Eric Pedley, Mohsen Imani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14027">https://arxiv.org/abs/2403.14027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14027">https://arxiv.org/pdf/2403.14027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14027]] EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship  Detection through Edge-Cloud Collaboration(https://arxiv.org/abs/2403.14027)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Detecting marine objects inshore presents challenges owing to algorithmic intricacies and complexities in system deployment. We propose a difficulty-aware edge-cloud collaborative sensing system that splits the task into object localization and fine-grained classification. Objects are classified either at the edge or within the cloud, based on their estimated difficulty. The framework comprises a low-power device-tailored front-end model for object localization, classification, and difficulty estimation, along with a transformer-graph convolutional network-based back-end model for fine-grained classification. Our system demonstrates superior performance (mAP@0.5 +4.3%}) on widely used marine object detection datasets, significantly reducing both data transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the system level. We validate the proposed system across various embedded system platforms and in real-world scenarios involving drone deployment.</li>
</ul>

<h3>Title: Leveraging Thermal Modality to Enhance Reconstruction in Low-Light  Conditions</h3>
<ul>
<li><strong>Authors: </strong>Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14053">https://arxiv.org/abs/2403.14053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14053">https://arxiv.org/pdf/2403.14053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14053]] Leveraging Thermal Modality to Enhance Reconstruction in Low-Light  Conditions(https://arxiv.org/abs/2403.14053)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on multimodal NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction.</li>
</ul>

<h3>Title: Semantics from Space: Satellite-Guided Thermal Semantic Segmentation  Annotation for Aerial Field Robots</h3>
<ul>
<li><strong>Authors: </strong>Connor Lee, Saraswati Soedarmadji, Matthew Anderson, Anthony J. Clark, Soon-Jo Chung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14056">https://arxiv.org/abs/2403.14056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14056">https://arxiv.org/pdf/2403.14056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14056]] Semantics from Space: Satellite-Guided Thermal Semantic Segmentation  Annotation for Aerial Field Robots(https://arxiv.org/abs/2403.14056)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates. This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale. By incorporating a thermal-conditioned refinement step with visual foundation models, our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-to-no cost. It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular zero-shot semantic segmentation methods based on large vision-language models currently used for generating annotations for RGB imagery. Code will be available at: https://github.com/connorlee77/aerial-auto-segment.</li>
</ul>

<h3>Title: DiffSTOCK: Probabilistic relational Stock Market Predictions using  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Divyanshu Daiya, Monika Yadav, Harshit Singh Rao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, q-fin.CP, q-fin.PM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14063">https://arxiv.org/abs/2403.14063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14063">https://arxiv.org/pdf/2403.14063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14063]] DiffSTOCK: Probabilistic relational Stock Market Predictions using  Diffusion Models(https://arxiv.org/abs/2403.14063)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>In this work, we propose an approach to generalize denoising diffusion probabilistic models for stock market predictions and portfolio management. Present works have demonstrated the efficacy of modeling interstock relations for market time-series forecasting and utilized Graph-based learning models for value prediction and portfolio management. Though convincing, these deterministic approaches still fall short of handling uncertainties i.e., due to the low signal-to-noise ratio of the financial data, it is quite challenging to learn effective deterministic models. Since the probabilistic methods have shown to effectively emulate higher uncertainties for time-series predictions. To this end, we showcase effective utilisation of Denoising Diffusion Probabilistic Models (DDPM), to develop an architecture for providing better market predictions conditioned on the historical financial indicators and inter-stock relations. Additionally, we also provide a novel deterministic architecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit inter-stock relations along with historical stock features. We demonstrate that our model achieves SOTA performance for movement predication and Portfolio management.</li>
</ul>

<h3>Title: EventDance: Unsupervised Source-free Cross-modal Adaptation for  Event-based Object Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xu Zheng, Lin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14082">https://arxiv.org/abs/2403.14082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14082">https://arxiv.org/pdf/2403.14082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14082]] EventDance: Unsupervised Source-free Cross-modal Adaptation for  Event-based Object Recognition(https://arxiv.org/abs/2403.14082)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In this paper, we make the first attempt at achieving the cross-modal (i.e., image-to-events) adaptation for event-based object recognition without accessing any labeled source image data owning to privacy and commercial issues. Tackling this novel problem is non-trivial due to the novelty of event cameras and the distinct modality gap between images and events. In particular, as only the source model is available, a hurdle is how to extract the knowledge from the source model by only using the unlabeled target event data while achieving knowledge transfer. To this end, we propose a novel framework, dubbed EventDance for this unsupervised source-free cross-modal adaptation problem. Importantly, inspired by event-to-video reconstruction methods, we propose a reconstruction-based modality bridging (RMB) module, which reconstructs intensity frames from events in a self-supervised manner. This makes it possible to build up the surrogate images to extract the knowledge (i.e., labels) from the source model. We then propose a multi-representation knowledge adaptation (MKA) module that transfers the knowledge to target models learning events with multiple representation types for fully exploring the spatiotemporal information of events. The two modules connecting the source and target models are mutually updated so as to achieve the best performance. Experiments on three benchmark datasets with two adaption settings show that EventDance is on par with prior methods utilizing the source data.</li>
</ul>

<h3>Title: Science based AI model certification for untrained operational  environments with application in traffic state estimation</h3>
<ul>
<li><strong>Authors: </strong>Daryl Mupupuni, Anupama Guntu, Liang Hong, Kamrul Hasan, Leehyun Keel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14093">https://arxiv.org/abs/2403.14093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14093">https://arxiv.org/pdf/2403.14093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14093]] Science based AI model certification for untrained operational  environments with application in traffic state estimation(https://arxiv.org/abs/2403.14093)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training. Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data. However, interpreting the opaque nature of AI's black-box models remains a persistent challenge. Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in untrained operational environments. The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models. This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions. The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation. Through simulation results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models. By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments. This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions.</li>
</ul>

<h3>Title: Text-Enhanced Data-free Approach for Federated Class-Incremental  Learning</h3>
<ul>
<li><strong>Authors: </strong>Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Dinh Phung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14101">https://arxiv.org/abs/2403.14101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14101">https://arxiv.org/pdf/2403.14101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14101]] Text-Enhanced Data-free Approach for Federated Class-Incremental  Learning(https://arxiv.org/abs/2403.14101)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, data-free</a></li>
<li><strong>Abstract: </strong>Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal issue, involving the dynamic addition of new classes in the context of federated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a crucial role in addressing catastrophic forgetting and data privacy problems. However, prior approaches lack the crucial synergy between DFKT and the model training phases, causing DFKT to encounter difficulties in generating high-quality data from a non-anchored latent space of the old task model. In this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge Transfer) to address this issue by utilizing label text embeddings (LTE) produced by pretrained language models. Specifically, during the model training phase, our approach treats LTE as anchor points and constrains the feature embeddings of corresponding training samples around them, enriching the surrounding area with more meaningful information. In the DFKT phase, by using these LTE anchors, LANDER can synthesize more meaningful samples, thereby effectively addressing the forgetting problem. Additionally, instead of tightly constraining embeddings toward the anchor, the Bounding Loss is introduced to encourage sample embeddings to remain flexible within a defined radius. This approach preserves the natural differences in sample embeddings and mitigates the embedding overlap caused by heterogeneous federated settings. Extensive experiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that LANDER significantly outperforms previous methods and achieves state-of-the-art performance in FCIL. The code is available at https://github.com/tmtuan1307/lander.</li>
</ul>

<h3>Title: MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical  Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bin Xie, Hao Tang, Bin Duan, Dawen Cai, Yan Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14103">https://arxiv.org/abs/2403.14103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14103">https://arxiv.org/pdf/2403.14103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14103]] MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical  Image Segmentation(https://arxiv.org/abs/2403.14103)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Segment Anything Model~(SAM), a prompt-driven foundation model for natural image segmentation, has demonstrated impressive zero-shot performance. However, SAM does not work when directly applied to medical image segmentation tasks, since SAM lacks the functionality to predict semantic labels for predicted masks and needs to provide extra prompts, such as points or boxes, to segment target regions. Meanwhile, there is a huge gap between 2D natural images and 3D medical images, so the performance of SAM is imperfect for medical image segmentation tasks. Following the above issues, we propose MaskSAM, a novel mask classification prompt-free SAM adaptation framework for medical image segmentation. We design a prompt generator combined with the image encoder in SAM to generate a set of auxiliary classifier tokens, auxiliary binary masks, and auxiliary bounding boxes. Each pair of auxiliary mask and box prompts, which can solve the requirements of extra prompts, is associated with class label predictions by the sum of the auxiliary classifier token and the learnable global classifier tokens in the mask decoder of SAM to solve the predictions of semantic labels. Meanwhile, we design a 3D depth-convolution adapter for image embeddings and a 3D depth-MLP adapter for prompt embeddings. We inject one of them into each transformer block in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Our method achieves state-of-the-art performance on AMOS2022, 90.52% Dice, which improved by 2.7% compared to nnUNet. Our method surpasses nnUNet by 1.7% on ACDC and 1.0% on Synapse datasets.</li>
</ul>

<h3>Title: HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic  Encryption</h3>
<ul>
<li><strong>Authors: </strong>Seewoo Lee, Garam Lee, Jung Woo Kim, Junbum Shin, Mun-Kyu Lee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14111">https://arxiv.org/abs/2403.14111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14111">https://arxiv.org/pdf/2403.14111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14111]] HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic  Encryption(https://arxiv.org/abs/2403.14111)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Transfer learning is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and fine-tuning new classification layers to a model pre-trained on large datasets. Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in transfer learning in the machine learning as a service setting, most of them only focused on encrypted inference. In this study, we present HETAL, an efficient Homomorphic Encryption based Transfer Learning algorithm, that protects the client's privacy in training tasks by encrypting the client data using the CKKS homomorphic encryption scheme. HETAL is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training. We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster than prior methods, and a highly precise softmax approximation algorithm with increased coverage. The experimental results for five well-known benchmark datasets show total training times of 567-3442 seconds, which is less than an hour.</li>
</ul>

<h3>Title: Benchmarking Chinese Commonsense Reasoning of LLMs: From  Chinese-Specifics to Reasoning-Memorization Correlations</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14112">https://arxiv.org/abs/2403.14112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14112">https://arxiv.org/pdf/2403.14112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14112]] Benchmarking Chinese Commonsense Reasoning of LLMs: From  Chinese-Specifics to Reasoning-Memorization Correlations(https://arxiv.org/abs/2403.14112)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .</li>
</ul>

<h3>Title: Training point-based deep learning networks for forest segmentation with  synthetic data</h3>
<ul>
<li><strong>Authors: </strong>Francisco Raverta Capua, Juan Schandin, Pablo De Cristóforis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14115">https://arxiv.org/abs/2403.14115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14115">https://arxiv.org/pdf/2403.14115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14115]] Training point-based deep learning networks for forest segmentation with  synthetic data(https://arxiv.org/abs/2403.14115)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Remote sensing through unmanned aerial systems (UAS) has been increasing in forestry in recent years, along with using machine learning for data processing. Deep learning architectures, extensively applied in natural language and image processing, have recently been extended to the point cloud domain. However, the availability of point cloud datasets for training and testing remains limited. Creating forested environment point cloud datasets is expensive, requires high-precision sensors, and is time-consuming as manual point classification is required. Moreover, forest areas could be inaccessible or dangerous for humans, further complicating data collection. Then, a question arises whether it is possible to use synthetic data to train deep learning networks without the need to rely on large volumes of real forest data. To answer this question, we developed a realistic simulator that procedurally generates synthetic forest scenes. Thanks to this, we have conducted a comparative study of different state-of-the-art point-based deep learning networks for forest segmentation. Using created datasets, we determined the feasibility of using synthetic data to train deep learning networks to classify point clouds from real forest datasets. Both the simulator and the datasets are released as part of this work.</li>
</ul>

<h3>Title: From Handcrafted Features to LLMs: A Brief Survey for Machine  Translation Quality Estimation</h3>
<ul>
<li><strong>Authors: </strong>Haofei Zhao, Yilun Liu, Shimin Tao, Weibin Meng, Yimeng Chen, Xiang Geng, Chang Su, Min Zhang, Hao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14118">https://arxiv.org/abs/2403.14118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14118">https://arxiv.org/pdf/2403.14118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14118]] From Handcrafted Features to LLMs: A Brief Survey for Machine  Translation Quality Estimation(https://arxiv.org/abs/2403.14118)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine Translation Quality Estimation (MTQE) is the task of estimating the quality of machine-translated text in real time without the need for reference translations, which is of great importance for the development of MT. After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained language models (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches. Finally, the paper discusses the current challenges in QE research and provides an outlook on future research directions.</li>
</ul>

<h3>Title: Advancing IIoT with Over-the-Air Federated Learning: The Role of  Iterative Magnitude Pruning</h3>
<ul>
<li><strong>Authors: </strong>Fazal Muhammad Ali Khan, Hatem Abou-Zeid, Aryan Kaushik, Syed Ali Hassan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14120">https://arxiv.org/abs/2403.14120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14120">https://arxiv.org/pdf/2403.14120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14120]] Advancing IIoT with Over-the-Air Federated Learning: The Role of  Iterative Magnitude Pruning(https://arxiv.org/abs/2403.14120)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is the integration of federated learning (FL), which addresses data privacy and security among devices. FL enables edge sensors, also known as peripheral intelligence units (PIUs) to learn and adapt using their data locally, without explicit sharing of confidential data, to facilitate a collaborative yet confidential learning process. However, the lower memory footprint and computational power of PIUs inherently require deep neural network (DNN) models that have a very compact size. Model compression techniques such as pruning can be used to reduce the size of DNN models by removing unnecessary connections that have little impact on the model's performance, thus making the models more suitable for the limited resources of PIUs. Targeting the notion of compact yet robust DNN models, we propose the integration of iterative magnitude pruning (IMP) of the DNN model being trained in an over-the-air FL (OTA-FL) environment for IIoT. We provide a tutorial overview and also present a case study of the effectiveness of IMP in OTA-FL for an IIoT environment. Finally, we present future directions for enhancing and optimizing these deep compression techniques further, aiming to push the boundaries of IIoT capabilities in acquiring compact yet robust and high-performing DNN models.</li>
</ul>

<h3>Title: External Knowledge Enhanced 3D Scene Generation from Sketch</h3>
<ul>
<li><strong>Authors: </strong>Zijie Wu, Mingtao Feng, Yaonan Wang, He Xie, Weisheng Dong, Bo Miao, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14121">https://arxiv.org/abs/2403.14121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14121">https://arxiv.org/pdf/2403.14121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14121]] External Knowledge Enhanced 3D Scene Generation from Sketch(https://arxiv.org/abs/2403.14121)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Generating realistic 3D scenes is challenging due to the complexity of room layouts and object geometries.We propose a sketch based knowledge enhanced diffusion architecture (SEK) for generating customized, diverse, and plausible 3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the target scene and cues from an object relationship knowledge base. We first construct an external knowledge base containing object relationships and then leverage knowledge enhanced graph reasoning to assist our model in understanding hand-drawn sketches. A scene is represented as a combination of 3D objects and their relationships, and then incrementally diffused to reach a Gaussian distribution.We propose a 3D denoising scene transformer that learns to reverse the diffusion process, conditioned by a hand-drawn sketch along with knowledge cues, to regressively generate the scene including the 3D object instances as well as their layout. Experiments on the 3D-FRONT dataset show that our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and FID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest competitor DiffuScene.</li>
</ul>

<h3>Title: AI and Memory Wall</h3>
<ul>
<li><strong>Authors: </strong>Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W. Mahoney, Kurt Keutzer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14123">https://arxiv.org/abs/2403.14123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14123">https://arxiv.org/pdf/2403.14123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14123]] AI and Memory Wall(https://arxiv.org/abs/2403.14123)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training LLMs. However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.</li>
</ul>

<h3>Title: Soft Masked Transformer for Point Cloud Processing with Skip  Attention-Based Upsampling</h3>
<ul>
<li><strong>Authors: </strong>Yong He, Hongshan Yu, Muhammad Ibrahim, Xiaoyan Liu, Tongjia Chen, Anwaar Ulhaq, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14124">https://arxiv.org/abs/2403.14124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14124">https://arxiv.org/pdf/2403.14124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14124]] Soft Masked Transformer for Point Cloud Processing with Skip  Attention-Based Upsampling(https://arxiv.org/abs/2403.14124)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Point cloud processing methods leverage local and global point features %at the feature level to cater to downstream tasks, yet they often overlook the task-level context inherent in point clouds during the encoding stage. We argue that integrating task-level information into the encoding stage significantly enhances performance. To that end, we propose SMTransformer which incorporates task-level information into a vector-based transformer by utilizing a soft mask generated from task-level queries and keys to learn the attention weights. Additionally, to facilitate effective communication between features from the encoding and decoding layers in high-level tasks such as segmentation, we introduce a skip-attention-based up-sampling block. This block dynamically fuses features from various resolution points across the encoding and decoding layers. To mitigate the increase in network parameters and training time resulting from the complexity of the aforementioned blocks, we propose a novel shared position encoding strategy. This strategy allows various transformer blocks to share the same position information over the same resolution points, thereby reducing network parameters and training time without compromising accuracy.Experimental comparisons with existing methods on multiple datasets demonstrate the efficacy of SMTransformer and skip-attention-based up-sampling for point cloud processing tasks, including semantic segmentation and classification. In particular, we achieve state-of-the-art semantic segmentation results of 73.4% mIoU on S3DIS Area 5 and 62.4% mIoU on SWAN dataset</li>
</ul>

<h3>Title: 3D Object Detection from Point Cloud via Voting Step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Haoran Hou, Mingtao Feng, Zijie Wu, Weisheng Dong, Qing Zhu, Yaonan Wang, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14133">https://arxiv.org/abs/2403.14133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14133">https://arxiv.org/pdf/2403.14133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14133]] 3D Object Detection from Point Cloud via Voting Step Diffusion(https://arxiv.org/abs/2403.14133)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D object detection is a fundamental task in scene understanding. Numerous research efforts have been dedicated to better incorporate Hough voting into the 3D object detection pipeline. However, due to the noisy, cluttered, and partial nature of real 3D scans, existing voting-based methods often receive votes from the partial surfaces of individual objects together with severe noises, leading to sub-optimal detection performance. In this work, we focus on the distributional properties of point clouds and formulate the voting process as generating new points in the high-density region of the distribution of object centers. To achieve this, we propose a new method to move random 3D points toward the high-density region of the distribution by estimating the score function of the distribution with a noise conditioned score network. Specifically, we first generate a set of object center proposals to coarsely identify the high-density region of the object center distribution. To estimate the score function, we perturb the generated object center proposals by adding normalized Gaussian noise, and then jointly estimate the score function of all perturbed distributions. Finally, we generate new votes by moving random 3D points to the high-density region of the object center distribution according to the estimated score function. Extensive experiments on two large scale indoor 3D scene datasets, SUN RGB-D and ScanNet V2, demonstrate the superiority of our proposed method. The code will be released at https://github.com/HHrEtvP/DiffVote.</li>
</ul>

<h3>Title: Learning Decomposable and Debiased Representations via Attribute-Centric  Information Bottlenecks</h3>
<ul>
<li><strong>Authors: </strong>Jinyung Hong, Eun Som Jeon, Changhoon Kim, Keun Hee Park, Utkarsh Nath, Yezhou Yang, Pavan Turaga, Theodore P. Pavlic</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14140">https://arxiv.org/abs/2403.14140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14140">https://arxiv.org/pdf/2403.14140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14140]] Learning Decomposable and Debiased Representations via Attribute-Centric  Information Bottlenecks(https://arxiv.org/abs/2403.14140)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Biased attributes, spuriously correlated with target labels in a dataset, can problematically lead to neural networks that learn improper shortcuts for classifications and limit their capabilities for out-of-distribution (OOD) generalization. Although many debiasing approaches have been proposed to ensure correct predictions from biased datasets, few studies have considered learning latent embedding consisting of intrinsic and biased attributes that contribute to improved performance and explain how the model pays attention to attributes. In this paper, we propose a novel debiasing framework, Debiasing Global Workspace, introducing attention-based information bottlenecks for learning compositional representations of attributes without defining specific bias types. Based on our observation that learning shape-centric representation helps robust performance on OOD datasets, we adopt those abilities to learn robust and generalizable representations of decomposable latent embeddings corresponding to intrinsic and biasing attributes. We conduct comprehensive evaluations on biased datasets, along with both quantitative and qualitative analyses, to showcase our approach's efficacy in attribute-centric representation learning and its ability to differentiate between intrinsic and bias-related features.</li>
</ul>

<h3>Title: Empowering Segmentation Ability to Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Yang, Peng-Tao Jiang, Jing Wang, Hao Zhang, Kai Zhao, Jinwei Chen, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14141">https://arxiv.org/abs/2403.14141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14141">https://arxiv.org/pdf/2403.14141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14141]] Empowering Segmentation Ability to Multi-modal Large Language Models(https://arxiv.org/abs/2403.14141)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-modal large language models (MLLMs) can understand image-language prompts and demonstrate impressive reasoning ability. In this paper, we extend MLLMs' output by empowering MLLMs with the segmentation ability. The extended MLLMs can both output language responses to the image-language prompts and segment the regions that the complex question or query in the language prompts focuses on. To this end, the existing work, LISA, enlarges the original word embeddings with an additional segment token and fine-tunes dialogue generation and query-focused segmentation together, where the feature of the segment token is used to prompt the segment-anything model. Although they achieve superior segmentation performance, we observe that the dialogue ability decreases by a large margin compared to the original MLLMs. To maintain the original MLLMs' dialogue ability, we propose a novel MLLMs framework, coined as LLaVASeg, which leverages a chain-of-thought prompting strategy to instruct the MLLMs to segment the target region queried by the user. The MLLMs are first prompted to reason about the simple description of the target region from the complicated user query, then extract the visual attributes of the target region according to the understanding of MLLMs to the image. These visual attributes, such as color and relative locations, are utilized to prompt the downstream segmentation model. Experiments show that the proposed method keeps the original dialogue ability and equips the MLLMs' model with strong reasoning segmentation ability. The code is available at https://github.com/YuqiYang213/LLaVASeg.</li>
</ul>

<h3>Title: Efficient Video Diffusion Models via Content-Frame Motion-Latent  Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14148">https://arxiv.org/abs/2403.14148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14148">https://arxiv.org/pdf/2403.14148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14148]] Efficient Video Diffusion Models via Content-Frame Motion-Latent  Decomposition(https://arxiv.org/abs/2403.14148)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image diffusion model, which has not been done in previous latent video diffusion models. This leads to considerably better quality generation and reduced computational costs. For instance, CMD can sample a video 7.7$\times$ faster than prior approaches by generating a video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4.</li>
</ul>

<h3>Title: Deep Learning for Trajectory Data Management and Mining: A Survey and  Beyond</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Yuxuan Liang, Yuanshao Zhu, Yanchuan Chang, Kang Luo, Haomin Wen, Lei Li, Yanwei Yu, Qingsong Wen, Chao Chen, Kai Zheng, Yunjun Gao, Xiaofang Zhou, Yu Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14151">https://arxiv.org/abs/2403.14151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14151">https://arxiv.org/pdf/2403.14151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14151]] Deep Learning for Trajectory Data Management and Mining: A Survey and  Beyond(https://arxiv.org/abs/2403.14151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related recommendation, trajectory classification, travel time estimation, anomaly detection, and mobility generation). Notably, we encapsulate recent advancements in Large Language Models (LLMs) that hold the potential to augment trajectory computing. Additionally, we summarize application scenarios, public datasets, and toolkits. Finally, we outline current challenges in DL4Traj research and propose future directions. Relevant papers and open-source resources have been collated and are continuously updated at: \href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.</li>
</ul>

<h3>Title: Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image  Customization</h3>
<ul>
<li><strong>Authors: </strong>Yeji Song, Jimyeong Kim, Wonhark Park, Wonsik Shin, Wonjong Rhee, Nojun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14155">https://arxiv.org/abs/2403.14155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14155">https://arxiv.org/pdf/2403.14155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14155]] Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image  Customization(https://arxiv.org/abs/2403.14155)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>In a surge of text-to-image (T2I) models and their customization methods that generate new images of a user-provided subject, current works focus on alleviating the costs incurred by a lengthy per-subject optimization. These zero-shot customization methods encode the image of a specified subject into a visual embedding which is then utilized alongside the textual embedding for diffusion guidance. The visual embedding incorporates intrinsic information about the subject, while the textual embedding provides a new, transient context. However, the existing methods often 1) are significantly affected by the input images, eg., generating images with the same pose, and 2) exhibit deterioration in the subject's identity. We first pin down the problem and show that redundant pose information in the visual embedding interferes with the textual embedding containing the desired pose information. To address this issue, we propose orthogonal visual embedding which effectively harmonizes with the given textual embedding. We also adopt the visual-only embedding and inject the subject's clear features utilizing a self-attention swap. Our results demonstrate the effectiveness and robustness of our method, which offers highly flexible zero-shot generation while effectively maintaining the subject's identity.</li>
</ul>

<h3>Title: Policy Mirror Descent with Lookahead</h3>
<ul>
<li><strong>Authors: </strong>Kimon Protopapas, Anas Barakat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14156">https://arxiv.org/abs/2403.14156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14156">https://arxiv.org/pdf/2403.14156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14156]] Policy Mirror Descent with Lookahead(https://arxiv.org/abs/2403.14156)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies. We propose an inexact version of $h$-PMD where lookahead action values are estimated. Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work. Finally, we extend our result to linear function approximation to scale to large state spaces. Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size.</li>
</ul>

<h3>Title: Volumetric Environment Representation for Vision-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Wenguan Wang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14158">https://arxiv.org/abs/2403.14158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14158">https://arxiv.org/pdf/2403.14158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14158]] Volumetric Environment Representation for Vision-Language Navigation(https://arxiv.org/abs/2403.14158)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Vision-language navigation (VLN) requires an agent to navigate through an 3D environment based on visual observations and natural language instructions. It is clear that the pivotal factor for successful navigation lies in the comprehensive scene understanding. Previous VLN agents employ monocular frameworks to extract 2D features of perspective views directly. Though straightforward, they struggle for capturing 3D geometry and semantics, leading to a partial and incomplete environment representation. To achieve a comprehensive 3D representation with fine-grained details, we introduce a Volumetric Environment Representation (VER), which voxelizes the physical world into structured 3D cells. For each cell, VER aggregates multi-view 2D features into such a unified 3D space via 2D-3D sampling. Through coarse-to-fine feature extraction and multi-task learning for VER, our agent predicts 3D occupancy, 3D room layout, and 3D bounding boxes jointly. Based on online collected VERs, our agent performs volume state estimation and builds episodic memory for predicting the next step. Experimental results show our environment representations from multi-task learning lead to evident performance gains on VLN. Our model achieves state-of-the-art performance across VLN benchmarks (R2R, REVERIE, and R4R).</li>
</ul>

<h3>Title: MMIDR: Teaching Large Language Model to Interpret Multimodal  Misinformation via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo Xu, Chuang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14171">https://arxiv.org/abs/2403.14171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14171">https://arxiv.org/pdf/2403.14171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14171]] MMIDR: Teaching Large Language Model to Interpret Multimodal  Misinformation via Knowledge Distillation(https://arxiv.org/abs/2403.14171)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic detection of multimodal misinformation has gained a widespread attention recently. However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored. Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation. To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation. Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs. To explore several research questions regarding the performance of LLMs in multimodal misinformation detection tasks, we construct an instruction-following multimodal misinformation dataset and conduct comprehensive experiments. The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.</li>
</ul>

<h3>Title: OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Kwanyoung Kim, Yujin Oh, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14183">https://arxiv.org/abs/2403.14183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14183">https://arxiv.org/pdf/2403.14183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14183]] OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic  Segmentation(https://arxiv.org/abs/2403.14183)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settings. Through extensive experiments, we demonstrate that OTSeg achieves state-of-the-art (SOTA) performance with significant gains on Zero-Shot Semantic Segmentation (ZS3) tasks across three benchmark datasets.</li>
</ul>

<h3>Title: PECI-Net: Bolus segmentation from video fluoroscopic swallowing study  images using preprocessing ensemble and cascaded inference</h3>
<ul>
<li><strong>Authors: </strong>Dougho Park, Younghun Kim, Harim Kang, Junmyeoung Lee, Jinyoung Choi, Taeyeon Kim, Sangeok Lee, Seokil Son, Minsol Kim, Injung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14191">https://arxiv.org/abs/2403.14191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14191">https://arxiv.org/pdf/2403.14191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14191]] PECI-Net: Bolus segmentation from video fluoroscopic swallowing study  images using preprocessing ensemble and cascaded inference(https://arxiv.org/abs/2403.14191)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Bolus segmentation is crucial for the automated detection of swallowing disorders in videofluoroscopic swallowing studies (VFSS). However, it is difficult for the model to accurately segment a bolus region in a VFSS image because VFSS images are translucent, have low contrast and unclear region boundaries, and lack color information. To overcome these challenges, we propose PECI-Net, a network architecture for VFSS image analysis that combines two novel techniques: the preprocessing ensemble network (PEN) and the cascaded inference network (CIN). PEN enhances the sharpness and contrast of the VFSS image by combining multiple preprocessing algorithms in a learnable way. CIN reduces ambiguity in bolus segmentation by using context from other regions through cascaded inference. Moreover, CIN prevents undesirable side effects from unreliably segmented regions by referring to the context in an asymmetric way. In experiments, PECI-Net exhibited higher performance than four recently developed baseline models, outperforming TernausNet, the best among the baseline models, by 4.54\% and the widely used UNet by 10.83\%. The results of the ablation studies confirm that CIN and PEN are effective in improving bolus segmentation performance.</li>
</ul>

<h3>Title: Debiasing surgeon: fantastic weights and how to find them</h3>
<ul>
<li><strong>Authors: </strong>Rémi Nahon, Ivan Luiz De Moura Matos, Van-Tam Nguyen, Enzo Tartaglione</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14200">https://arxiv.org/abs/2403.14200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14200">https://arxiv.org/pdf/2403.14200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14200]] Debiasing surgeon: fantastic weights and how to find them(https://arxiv.org/abs/2403.14200)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic biases that can lead to unfair models, emerges. Several debiasing approaches have been proposed in the realm of deep learning, employing more or less sophisticated approaches to discourage these models from massively employing these biases. However, a question emerges: is this extra complexity really necessary? Is a vanilla-trained model already embodying some ``unbiased sub-networks'' that can be used in isolation and propose a solution without relying on the algorithmic biases? In this work, we show that such a sub-network typically exists, and can be extracted from a vanilla-trained model without requiring additional training. We further validate that such specific architecture is incapable of learning a specific bias, suggesting that there are possible architectural countermeasures to the problem of biases in deep neural networks.</li>
</ul>

<h3>Title: Unsupervised Audio-Visual Segmentation with Modality Alignment</h3>
<ul>
<li><strong>Authors: </strong>Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiangkang Deng, Xiatian Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14203">https://arxiv.org/abs/2403.14203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14203">https://arxiv.org/pdf/2403.14203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14203]] Unsupervised Audio-Visual Segmentation with Modality Alignment(https://arxiv.org/abs/2403.14203)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the object in a visual scene that produces a given sound. Current AVS methods rely on costly fine-grained annotations of mask-audio pairs, making them impractical for scalability. To address this, we introduce unsupervised AVS, eliminating the need for such expensive annotation. To tackle this more challenging problem, we propose an unsupervised learning method, named Modality Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf foundation models like DINO, SAM, and ImageBind. This approach leverages their knowledge complementarity and optimizes their joint usage for multi-modality association. Initially, we estimate positive and negative image pairs in the feature space. For pixel-level association, we introduce an audio-visual adapter and a novel pixel matching aggregation strategy within the image-level contrastive learning framework. This allows for a flexible connection between object appearance and audio signal at the pixel level, with tolerance to imaging variations such as translation and rotation. Extensive experiments on the AVSBench (single and multi-object splits) and AVSS datasets demonstrate that our MoCA outperforms strongly designed baseline methods and approaches supervised counterparts, particularly in complex scenarios with multiple auditory objects. Notably when comparing mIoU, MoCA achieves a substantial improvement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and AVSS (+19.23%) audio-visual segmentation challenges.</li>
</ul>

<h3>Title: Automatic Annotation of Grammaticality in Child-Caregiver Conversations</h3>
<ul>
<li><strong>Authors: </strong>Mitja Nikolaus (ILCB, LPL, LIS, TALEP), Abhishek Agrawal (ILCB, LIS, TALEP), Petros Kaklamanis, Alex Warstadt (SED), Abdellah Fourtassi (ILCB, LIS, TALEP)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14208">https://arxiv.org/abs/2403.14208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14208">https://arxiv.org/pdf/2403.14208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14208]] Automatic Annotation of Grammaticality in Child-Caregiver Conversations(https://arxiv.org/abs/2403.14208)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The acquisition of grammar has been a central question to adjudicate between theories of language acquisition. In order to conduct faster, more reproducible, and larger-scale corpus studies on grammaticality in child-caregiver conversations, tools for automatic annotation can offer an effective alternative to tedious manual annotation. We propose a coding scheme for context-dependent grammaticality in child-caregiver conversations and annotate more than 4,000 utterances from a large corpus of transcribed conversations. Based on these annotations, we train and evaluate a range of NLP models. Our results show that fine-tuned Transformer-based models perform best, achieving human inter-annotation agreement levels.As a first application and sanity check of this tool, we use the trained models to annotate a corpus almost two orders of magnitude larger than the manually annotated data and verify that children's grammaticality shows a steady increase with age.This work contributes to the growing literature on applying state-of-the-art NLP methods to help study child language acquisition at scale.</li>
</ul>

<h3>Title: Toward Multi-class Anomaly Detection: Exploring Class-aware Unified  Model against Inter-class Interference</h3>
<ul>
<li><strong>Authors: </strong>Xi Jiang, Ying Chen, Qiang Nie, Jianlin Liu, Yong Liu, Chengjie Wang, Feng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14213">https://arxiv.org/abs/2403.14213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14213">https://arxiv.org/pdf/2403.14213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14213]] Toward Multi-class Anomaly Detection: Exploring Class-aware Unified  Model against Inter-class Interference(https://arxiv.org/abs/2403.14213)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the context of high usability in single-class anomaly detection models, recent academic research has become concerned about the more complex multi-class anomaly detection. Although several papers have designed unified models for this task, they often overlook the utility of class labels, a potent tool for mitigating inter-class interference. To address this issue, we introduce a Multi-class Implicit Neural representation Transformer for unified Anomaly Detection (MINT-AD), which leverages the fine-grained category information in the training stage. By learning the multi-class distributions, the model generates class-aware query embeddings for the transformer decoder, mitigating inter-class interference within the reconstruction model. Utilizing such an implicit neural representation network, MINT-AD can project category and position information into a feature embedding space, further supervised by classification and prior probability loss functions. Experimental results on multiple datasets demonstrate that MINT-AD outperforms existing unified training models.</li>
</ul>

<h3>Title: Improving the Robustness of Large Language Models via Consistency  Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zhao Yukun, Yan Lingyong, Sun Weiwei, Xing Guoliang, Wang Shuaiqiang, Meng Chong, Cheng Zhicong, Ren Zhaochun, Yin Dawei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14221">https://arxiv.org/abs/2403.14221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14221">https://arxiv.org/pdf/2403.14221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14221]] Improving the Robustness of Large Language Models via Consistency  Alignment(https://arxiv.org/abs/2403.14221)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework.</li>
</ul>

<h3>Title: A Unified Framework for Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14236">https://arxiv.org/abs/2403.14236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14236">https://arxiv.org/pdf/2403.14236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14236]] A Unified Framework for Model Editing(https://arxiv.org/abs/2403.14236)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objective of MEMIT and show that these edit-distribution algorithms should be considered separate entities worthy of their own line of research. Finally, we present EMMET - an Equality-constrained Mass Model Editing algorithm for Transformers, a new batched memory-editing algorithm. With EMMET, we present a closed form solution for the equality-constrained version of the preservation-memorization objective. We show that EMMET is able to perform batched-edits on par with MEMIT up to a batch-size of 256 and discuss the challenges in stabilizing EMMET. By articulating the "locate-and-edit" model editing algorithms under a simple conceptual framework of "preservation-memorization", we aim to bridge the gap between intuition and mathematics and hope to simplify the journey for future researchers in model editing.</li>
</ul>

<h3>Title: Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large  Language Models with Machine Learning in tele-dermatology</h3>
<ul>
<li><strong>Authors: </strong>Dimitrios P. Panagoulias, Evridiki Tsoureli-Nikita, Maria Virvou, George A. Tsihrintzis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14243">https://arxiv.org/abs/2403.14243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14243">https://arxiv.org/pdf/2403.14243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14243]] Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large  Language Models with Machine Learning in tele-dermatology(https://arxiv.org/abs/2403.14243)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>The rise of Artificial Intelligence creates great promise in the field of medical discovery, diagnostics and patient management. However, the vast complexity of all medical domains require a more complex approach that combines machine learning algorithms, classifiers, segmentation algorithms and, lately, large language models. In this paper, we describe, implement and assess an Artificial Intelligence-empowered system and methodology aimed at assisting the diagnosis process of skin lesions and other skin conditions within the field of dermatology that aims to holistically address the diagnostic process in this domain. The workflow integrates large language, transformer-based vision models and sophisticated machine learning tools. This holistic approach achieves a nuanced interpretation of dermatological conditions that simulates and facilitates a dermatologist's workflow. We assess our proposed methodology through a thorough cross-model validation technique embedded in an evaluation pipeline that utilizes publicly available medical case studies of skin conditions and relevant images. To quantitatively score the system performance, advanced machine learning and natural language processing tools are employed which focus on similarity comparison and natural language inference. Additionally, we incorporate a human expert evaluation process based on a structured checklist to further validate our results. We implemented the proposed methodology in a system which achieved approximate (weighted) scores of 0.87 for both contextual understanding and diagnostic accuracy, demonstrating the efficacy of our approach in enhancing dermatological analysis. The proposed methodology is expected to prove useful in the development of next-generation tele-dermatology applications, enhancing remote consultation capabilities and access to care, especially in underserved areas.</li>
</ul>

<h3>Title: LayoutLLM: Large Language Model Instruction Tuning for Visually Rich  Document Understanding</h3>
<ul>
<li><strong>Authors: </strong>Masato Fujitake</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14252">https://arxiv.org/abs/2403.14252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14252">https://arxiv.org/pdf/2403.14252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14252]] LayoutLLM: Large Language Model Instruction Tuning for Visually Rich  Document Understanding(https://arxiv.org/abs/2403.14252)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experiments demonstrate improvement over the baseline model in various document analysis tasks.</li>
</ul>

<h3>Title: K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional  Expression</h3>
<ul>
<li><strong>Authors: </strong>Kyuhee Kim, Surin Lee, Sangah Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14253">https://arxiv.org/abs/2403.14253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14253">https://arxiv.org/pdf/2403.14253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14253]] K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional  Expression(https://arxiv.org/abs/2403.14253)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In many literary texts, emotions are indirectly conveyed through descriptions of actions, facial expressions, and appearances, necessitating emotion inference for narrative understanding. In this paper, we introduce K-Act2Emo, a Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional expressions and the emotions inferable from them. We categorize reasoning types into inferences in positive situations, inferences in negative situations, and inferences when expressions do not serve as emotional cues. Unlike existing CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results validate its effectiveness for training emotion inference models. Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo outperforms various existing Korean large language models, achieving performance levels comparable to GPT-4 Turbo.</li>
</ul>

<h3>Title: ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion  Classification</h3>
<ul>
<li><strong>Authors: </strong>Sehee Lim, Yejin Kim, Chi-Hyun Choi, Jy-yong Sohn, Byung-Hoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14255">https://arxiv.org/abs/2403.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14255">https://arxiv.org/pdf/2403.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14255]] ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion  Classification(https://arxiv.org/abs/2403.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Improving the accessibility of psychotherapy with the aid of Large Language Models (LLMs) is garnering a significant attention in recent years. Recognizing cognitive distortions from the interviewee's utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. In this paper, we propose ERD, which improves LLM-based cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the reasoning steps by multiple agents. Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score. Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.</li>
</ul>

<h3>Title: LLM-based Extraction of Contradictions from Patents</h3>
<ul>
<li><strong>Authors: </strong>Stefan Trapp, Joachim Warschat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14258">https://arxiv.org/abs/2403.14258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14258">https://arxiv.org/pdf/2403.14258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14258]] LLM-based Extraction of Contradictions from Patents(https://arxiv.org/abs/2403.14258)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products. However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents. Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods. Recent advances in patent retrieval and analysis mainly focus on dense vectors based on neural AI Transformer language models like Google BERT. They are, for example, used for dense retrieval, question answering or summarization and key concept extraction. A research focus within the methods for patent summarization and key concept extraction are generic inventive concepts respectively TRIZ concepts like problems, solutions, advantage of invention, parameters, and contradictions. Succeeding rule-based approaches, finetuned BERT-like language models for sentence-wise classification represent the state-of-the-art of inventive concept extraction. While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models. This paper goes one step further, as it presents a method to extract TRIZ contradictions from patent texts based on Prompt Engineering using a generative Large Language Model (LLM), namely OpenAI's GPT-4. Contradiction detection, sentence extraction, contradiction summarization, parameter extraction and assignment to the 39 abstract TRIZ engineering parameters are all performed in a single prompt using the LangChain framework. Our results show that "off-the-shelf" GPT-4 is a serious alternative to existing approaches.</li>
</ul>

<h3>Title: A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity  Identification</h3>
<ul>
<li><strong>Authors: </strong>Seungkwon Kim, Sangyeon Kim, Seung-Hun Nam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14264">https://arxiv.org/abs/2403.14264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14264">https://arxiv.org/pdf/2403.14264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14264]] A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity  Identification(https://arxiv.org/abs/2403.14264)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Portrait stylization is a challenging task involving the transformation of an input portrait image into a specific style while preserving its inherent characteristics. The recent introduction of Stable Diffusion (SD) has significantly improved the quality of outcomes in this field. However, a practical stylization framework that can effectively filter harmful input content and preserve the distinct characteristics of an input, such as skin-tone, while maintaining the quality of stylization remains lacking. These challenges have hindered the wide deployment of such a framework. To address these issues, this study proposes a portrait stylization framework that incorporates a nudity content identification module (NCIM) and a skin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM showed good performance in enhancing explicit content filtering, and STAPSM accurately represented a diverse range of skin tones. Our proposed framework has been successfully deployed in practice, and it has effectively satisfied critical requirements of real-world applications.</li>
</ul>

<h3>Title: Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship  Detection</h3>
<ul>
<li><strong>Authors: </strong>Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14270">https://arxiv.org/abs/2403.14270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14270">https://arxiv.org/pdf/2403.14270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14270]] Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship  Detection(https://arxiv.org/abs/2403.14270)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Visual relationship detection aims to identify objects and their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of object and relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-time inference speeds. We provide analyses of zero-shot performance, ablations, and real-world qualitative examples.</li>
</ul>

<h3>Title: Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D  Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Francesco Di Felice, Alberto Remus, Stefano Gasperini, Benjamin Busam, Lionel Ott, Federico Tombari, Roland Siegwart, Carlo Alberto Avizzano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14279">https://arxiv.org/abs/2403.14279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14279">https://arxiv.org/pdf/2403.14279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14279]] Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D  Pose Estimation(https://arxiv.org/abs/2403.14279)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Estimating the pose of objects through vision is essential to make robotic platforms interact with the environment. Yet, it presents many challenges, often related to the lack of flexibility and generalizability of state-of-the-art solutions. Diffusion models are a cutting-edge neural architecture transforming 2D and 3D computer vision, outlining remarkable performances in zero-shot novel-view synthesis. Such a use case is particularly intriguing for reconstructing 3D objects. However, localizing objects in unstructured environments is rather unexplored. To this end, this work presents Zero123-6D to demonstrate the utility of Diffusion Model-based novel-view-synthesizers in enhancing RGB 6D pose estimation at category-level by integrating them with feature extraction techniques. The outlined method exploits such a novel view synthesizer to expand a sparse set of RGB-only reference views for the zero-shot 6D pose estimation task. Experiments are quantitatively analyzed on the CO3D dataset, showcasing increased performance over baselines, a substantial reduction in data requirements, and the removal of the necessity of depth information.</li>
</ul>

<h3>Title: Large Language Models for Blockchain Security: A Systematic Literature  Review</h3>
<ul>
<li><strong>Authors: </strong>Zheyuan He, Zihao Li, Sen Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14280">https://arxiv.org/abs/2403.14280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14280">https://arxiv.org/pdf/2403.14280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14280]] Large Language Models for Blockchain Security: A Systematic Literature  Review(https://arxiv.org/abs/2403.14280)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful tools in various domains involving blockchain security (BS). Several recent studies are exploring LLMs applied to BS. However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security. To fill this gap, we conduct a literature review on LLM4BS. As the first review of LLM's application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how LLMs contribute to enhancing the security of blockchain systems. Through a thorough examination of scholarly works, we delve into the integration of LLMs into various aspects of blockchain security. We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, identity verification, anomaly detection, vulnerable repair, and so on. Furthermore, we critically assess the challenges and limitations associated with leveraging LLMs for blockchain security, considering factors such as scalability, privacy concerns, and adversarial attacks. Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.</li>
</ul>

<h3>Title: How to be fair? A study of label and selection bias</h3>
<ul>
<li><strong>Authors: </strong>Marco Favier, Toon Calders, Sam Pinxteren, Jonathan Meyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14282">https://arxiv.org/abs/2403.14282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14282">https://arxiv.org/pdf/2403.14282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14282]] How to be fair? A study of label and selection bias(https://arxiv.org/abs/2403.14282)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by the bias measure they optimize. In this paper we illustrate this principle for label and selection bias on the one hand, and demographic parity and ``We're All Equal'' on the other hand. Our theoretical analysis allows to explain the results of Wick et al. and we also show that there are situations where minimizing fairness measures does not result in the fairest possible distribution.</li>
</ul>

<h3>Title: Open-Vocabulary Attention Maps with Token Optimization for Semantic  Segmentation in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pablo Marcos-Manchón, Roberto Alcover-Couso, Juan C. SanMiguel, Jose M. Martínez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14291">https://arxiv.org/abs/2403.14291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14291">https://arxiv.org/pdf/2403.14291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14291]] Open-Vocabulary Attention Maps with Token Optimization for Semantic  Segmentation in Diffusion Models(https://arxiv.org/abs/2403.14291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Diffusion models represent a new paradigm in text-to-image generation. Beyond generating high-quality images from text prompts, models such as Stable Diffusion have been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to prompt words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text prompt. In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for text-to-image diffusion models that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable Diffusion extensions. The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images' pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining.</li>
</ul>

<h3>Title: HySim: An Efficient Hybrid Similarity Measure for Patch Matching in  Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Saad Noufel, Nadir Maaroufi, Mehdi Najib, Mohamed Bakhouya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14292">https://arxiv.org/abs/2403.14292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14292">https://arxiv.org/pdf/2403.14292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14292]] HySim: An Efficient Hybrid Similarity Measure for Patch Matching in  Image Inpainting(https://arxiv.org/abs/2403.14292)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inpainting, for filling missing image regions, is a crucial task in various applications, such as medical imaging and remote sensing. Trending data-driven approaches efficiency, for image inpainting, often requires extensive data preprocessing. In this sense, there is still a need for model-driven approaches in case of application constrained with data availability and quality, especially for those related for time series forecasting using image inpainting techniques. This paper proposes an improved modeldriven approach relying on patch-based techniques. Our approach deviates from the standard Sum of Squared Differences (SSD) similarity measure by introducing a Hybrid Similarity (HySim), which combines both strengths of Chebychev and Minkowski distances. This hybridization enhances patch selection, leading to high-quality inpainting results with reduced mismatch errors. Experimental results proved the effectiveness of our approach against other model-driven techniques, such as diffusion or patch-based approaches, showcasing its effectiveness in achieving visually pleasing restorations.</li>
</ul>

<h3>Title: Impact Assessment of Missing Data in Model Predictions for Earth  Observation Applications</h3>
<ul>
<li><strong>Authors: </strong>Francisco Mena, Diego Arenas, Marcela Charfuelan, Marlon Nuske, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14297">https://arxiv.org/abs/2403.14297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14297">https://arxiv.org/pdf/2403.14297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14297]] Impact Assessment of Missing Data in Model Predictions for Earth  Observation Applications(https://arxiv.org/abs/2403.14297)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models. However, there is a common assumption that data sources will be persistently available. Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures. In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks. We compare the predictive quality of different methods and find that some are naturally more robust to missing data. The Ensemble strategy, in particular, achieves a prediction robustness up to 100%. We evidence that missing scenarios are significantly more challenging in regression than classification tasks. Finally, we find that the optical view is the most critical view when it is missing individually.</li>
</ul>

<h3>Title: ChainLM: Empowering Large Language Models with Improved Chain-of-Thought  Prompting</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14312">https://arxiv.org/abs/2403.14312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14312">https://arxiv.org/pdf/2403.14312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14312]] ChainLM: Empowering Large Language Models with Improved Chain-of-Thought  Prompting(https://arxiv.org/abs/2403.14312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at https://github.com/RUCAIBox/ChainLM.</li>
</ul>

<h3>Title: A Lightweight Attention-based Deep Network via Multi-Scale Feature  Fusion for Multi-View Facial Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ali Ezati, Mohammadreza Dezyani, Rajib Rana, Roozbeh Rajabi, Ahmad Ayatollahi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14318">https://arxiv.org/abs/2403.14318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14318">https://arxiv.org/pdf/2403.14318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14318]] A Lightweight Attention-based Deep Network via Multi-Scale Feature  Fusion for Multi-View Facial Expression Recognition(https://arxiv.org/abs/2403.14318)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Convolutional neural networks (CNNs) and their variations have shown effectiveness in facial expression recognition (FER). However, they face challenges when dealing with high computational complexity and multi-view head poses in real-world scenarios. We introduce a lightweight attentional network incorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For the first challenge, we have carefully designed a lightweight fully convolutional network (FCN). We address the second challenge by presenting two novel components, namely mass attention (MassAtt) and point wise feature selection (PWFS) blocks. The MassAtt block simultaneously generates channel and spatial attention maps to recalibrate feature maps by emphasizing important features while suppressing irrelevant ones. On the other hand, the PWFS block employs a feature selection mechanism that discards less meaningful features prior to the fusion process. This mechanism distinguishes it from previous methods that directly fuse multi-scale features. Our proposed approach achieved results comparable to state-of-the-art methods in terms of parameter counts and robustness to pose variation, with accuracy rates of 90.77% on KDEF, 70.44% on FER-2013, and 86.96% on FERPlus datasets. The code for LANMSFF is available at https://github.com/AE-1129/LANMSFF.</li>
</ul>

<h3>Title: CFPL-FAS: Class Free Prompt Learning for Generalizable Face  Anti-spoofing</h3>
<ul>
<li><strong>Authors: </strong>Ajian Liu, Shuai Xue, Jianwen Gan, Jun Wan, Yanyan Liang, Jiankang Deng, Sergio Escalera, Zhen Lei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14333">https://arxiv.org/abs/2403.14333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14333">https://arxiv.org/pdf/2403.14333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14333]] CFPL-FAS: Class Free Prompt Learning for Generalizable Face  Anti-spoofing(https://arxiv.org/abs/2403.14333)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model's performance on unseen domains. Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization. In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier's weights for exploring generalizable visual features. Specifically, we propose a novel Class Free Prompt Learning (CFPL) paradigm for DG FAS, which utilizes two lightweight transformers, namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic prompts conditioned on content and style features by using a set of learnable query vectors, respectively. Thus, the generalizable prompt can be learned by two improvements: (1) A Prompt-Text Matched (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description. (2) A Diversified Style Prompt (DSP) technology is proposed to diversify the learning of style prompts by mixing feature statistics between instance-specific styles. Finally, the learned text features modulate visual features to generalization through the designed Prompt Modulation (PM). Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets.</li>
</ul>

<h3>Title: FFT-based Selection and Optimization of Statistics for Robust  Recognition of Severely Corrupted Images</h3>
<ul>
<li><strong>Authors: </strong>Elena Camuffo, Umberto Michieli, Jijoong Moon, Daehyun Kim, Mete Ozay</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14335">https://arxiv.org/abs/2403.14335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14335">https://arxiv.org/pdf/2403.14335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14335]] FFT-based Selection and Optimization of Statistics for Robust  Recognition of Severely Corrupted Images(https://arxiv.org/abs/2403.14335)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Improving model robustness in case of corrupted images is among the key challenges to enable robust vision systems on smart devices, such as robotic agents. Particularly, robust test-time performance is imperative for most of the applications. This paper presents a novel approach to improve robustness of any classification model, especially on severely corrupted images. Our method (FROST) employs high-frequency features to detect input image corruption type, and select layer-wise feature normalization statistics. FROST provides the state-of-the-art results for different models and datasets, outperforming competitors on ImageNet-C by up to 37.1% relative gain, improving baseline of 40.9% mCE on severe corruptions.</li>
</ul>

<h3>Title: $\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Daniel Trippa, Cesare Campagnano, Maria Sofia Bucarelli, Gabriele Tolomei, Fabrizio Silvestri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14339">https://arxiv.org/abs/2403.14339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14339">https://arxiv.org/pdf/2403.14339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14339]] $\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning(https://arxiv.org/abs/2403.14339)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Machine Unlearning, the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic machine Unlearning ($\nabla \tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\nabla \tau$ offers multiple benefits over existing approaches. It enables the unlearning of large sections of the training dataset (up to 30%). It is versatile, supporting various unlearning tasks (such as subset forgetting or class removal) and applicable across different domains (images, text, etc.). Importantly, $\nabla \tau$ requires no hyperparameter adjustments, making it a more appealing option than retraining the model from scratch. We evaluate our framework's effectiveness using a set of well-established Membership Inference Attack metrics, demonstrating up to 10% enhancements in performance compared to state-of-the-art methods without compromising the original model's accuracy.</li>
</ul>

<h3>Title: Exploring Task Unification in Graph Representation Learning via  Generative Approach</h3>
<ul>
<li><strong>Authors: </strong>Yulan Hu, Sheng Ouyang, Zhirui Yang, Ge Chen, Junchen Wan, Xiao Wang, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14340">https://arxiv.org/abs/2403.14340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14340">https://arxiv.org/pdf/2403.14340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14340]] Exploring Task Unification in Graph Representation Learning via  Generative Approach(https://arxiv.org/abs/2403.14340)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Graphs are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and graph-level tasks to transfer learning. However, designing specific tasks for each type of graph data is often costly and lacks generalizability. Recent endeavors under the "Pre-training + Fine-tuning" or "Pre-training + Prompt" paradigms aim to design a unified framework capable of generalizing across multiple graph tasks. Among these, graph autoencoders (GAEs), generative self-supervised models, have demonstrated their potential in effectively addressing various graph tasks. Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse graph tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages. To address these challenges, we propose GA^2E, a unified adversarially masked autoencoder capable of addressing the above challenges seamlessly. Specifically, GA^2E proposes to use the subgraph as the meta-structure, which remains consistent across all graph tasks (ranging from node-, edge-, and graph-level to transfer learning) and all stages (both during training and inference). Further, GA^2E operates in a \textbf{"Generate then Discriminate"} manner. It leverages the masked GAE to reconstruct the input subgraph whilst treating it as a generator to compel the reconstructed graphs resemble the input subgraph. Furthermore, GA^2E introduces an auxiliary discriminator to discern the authenticity between the reconstructed (generated) subgraph and the input subgraph, thus ensuring the robustness of the graph representation through adversarial training mechanisms. We validate GA^2E's capabilities through extensive experiments on 21 datasets across four types of graph tasks.</li>
</ul>

<h3>Title: Adversary-Augmented Simulation to evaluate client-fairness on  HyperLedger Fabric</h3>
<ul>
<li><strong>Authors: </strong>Erwan Mahe, Rouwaida Abdallah, Sara Tucci-Piergiovanni, Pierre-Yves Piriou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14342">https://arxiv.org/abs/2403.14342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14342">https://arxiv.org/pdf/2403.14342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14342]] Adversary-Augmented Simulation to evaluate client-fairness on  HyperLedger Fabric(https://arxiv.org/abs/2403.14342)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, fair</a></li>
<li><strong>Abstract: </strong>This paper presents a novel adversary model specifically tailored to distributed systems, with the aim to asses the security of blockchain technologies. Building upon literature on adversarial assumptions and capabilities, we include classical notions of failure and communication models to classify and bind the use of adversarial actions. We focus on the effect of these actions on properties of distributed protocols. A significant effort of our research is the integration of this model into the Multi-Agent eXperimenter (MAX) framework. This integration enables realistic simulations of adversarial attacks on blockchain systems. In particular, we have simulated attacks violating a form of client-fairness on HyperLedger Fabric.</li>
</ul>

<h3>Title: Towards Efficient Information Fusion: Concentric Dual Fusion Attention  Based Multiple Instance Learning for Whole Slide Images</h3>
<ul>
<li><strong>Authors: </strong>Yujian Liu, Ruoxuan Wu, Xinjie Shen, Zihuang Lu, Lingyu Liang, Haiyu Zhou, Shipu Xu, Shaoai Cai, Shidang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14346">https://arxiv.org/abs/2403.14346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14346">https://arxiv.org/pdf/2403.14346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14346]] Towards Efficient Information Fusion: Concentric Dual Fusion Attention  Based Multiple Instance Learning for Whole Slide Images(https://arxiv.org/abs/2403.14346)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the realm of digital pathology, multi-magnification Multiple Instance Learning (multi-mag MIL) has proven effective in leveraging the hierarchical structure of Whole Slide Images (WSIs) to reduce information loss and redundant data. However, current methods fall short in bridging the domain gap between pretrained models and medical imaging, and often fail to account for spatial relationships across different magnifications. Addressing these challenges, we introduce the Concentric Dual Fusion Attention-MIL (CDFA-MIL) framework,which innovatively combines point-to-area feature-colum attention and point-to-point concentric-row attention using concentric patch. This approach is designed to effectively fuse correlated information, enhancing feature representation and providing stronger correlation guidance for WSI analysis. CDFA-MIL distinguishes itself by offering a robust fusion strategy that leads to superior WSI recognition. Its application has demonstrated exceptional performance, significantly surpassing existing MIL methods in accuracy and F1 scores on prominent datasets like Camelyon16 and TCGA-NSCLC. Specifically, CDFA-MIL achieved an average accuracy and F1-score of 93.7\% and 94.1\% respectively on these datasets, marking a notable advancement over traditional MIL approaches.</li>
</ul>

<h3>Title: On the Concept Trustworthiness in Concept Bottleneck Models</h3>
<ul>
<li><strong>Authors: </strong>Qihan Huang, Jie Song, Jingwen Hu, Haofei Zhang, Yong Wang, Mingli Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14349">https://arxiv.org/abs/2403.14349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14349">https://arxiv.org/pdf/2403.14349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14349]] On the Concept Trustworthiness in Concept Bottleneck Models(https://arxiv.org/abs/2403.14349)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBMs), which break down the reasoning process into the input-to-concept mapping and the concept-to-label prediction, have garnered significant attention due to their remarkable interpretability achieved by the interpretable concept bottleneck. However, despite the transparency of the concept-to-label prediction, the mapping from the input to the intermediate concept remains a black box, giving rise to concerns about the trustworthiness of the learned concepts (i.e., these concepts may be predicted based on spurious cues). The issue of concept untrustworthiness greatly hampers the interpretability of CBMs, thereby hindering their further advancement. To conduct a comprehensive analysis on this issue, in this study we establish a benchmark to assess the trustworthiness of concepts in CBMs. A pioneering metric, referred to as concept trustworthiness score, is proposed to gauge whether the concepts are derived from relevant regions. Additionally, an enhanced CBM is introduced, enabling concept predictions to be made specifically from distinct parts of the feature map, thereby facilitating the exploration of their related regions. Besides, we introduce three modules, namely the cross-layer alignment (CLA) module, the cross-image alignment (CIA) module, and the prediction alignment (PA) module, to further enhance the concept trustworthiness within the elaborated CBM. The experiments on five datasets across ten architectures demonstrate that without using any concept localization annotations during training, our model improves the concept trustworthiness by a large margin, meanwhile achieving superior accuracy to the state-of-the-arts. Our code is available at https://github.com/hqhQAQ/ProtoCBM.</li>
</ul>

<h3>Title: Annotation-Efficient Polyp Segmentation via Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Duojun Huang, Xinyu Xiong, De-Jun Fan, Feng Gao, Xiao-Jian Wu, Guanbin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14350">https://arxiv.org/abs/2403.14350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14350">https://arxiv.org/pdf/2403.14350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14350]] Annotation-Efficient Polyp Segmentation via Active Learning(https://arxiv.org/abs/2403.14350)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning-based techniques have proven effective in polyp segmentation tasks when provided with sufficient pixel-wise labeled data. However, the high cost of manual annotation has created a bottleneck for model generalization. To minimize annotation costs, we propose a deep active learning framework for annotation-efficient polyp segmentation. In practice, we measure the uncertainty of each sample by examining the similarity between features masked by the prediction map of the polyp and the background area. Since the segmentation model tends to perform weak in samples with indistinguishable features of foreground and background areas, uncertainty sampling facilitates the fitting of under-learning data. Furthermore, clustering image-level features weighted by uncertainty identify samples that are both uncertain and representative. To enhance the selectivity of the active selection strategy, we propose a novel unsupervised feature discrepancy learning mechanism. The selection strategy and feature optimization work in tandem to achieve optimal performance with a limited annotation budget. Extensive experimental results have demonstrated that our proposed method achieved state-of-the-art performance compared to other competitors on both a public dataset and a large-scale in-house dataset.</li>
</ul>

<h3>Title: LDTR: Transformer-based Lane Detection with Anchor-chain Representation</h3>
<ul>
<li><strong>Authors: </strong>Zhongyu Yang, Chen Shen, Wei Shao, Tengfei Xing, Runbo Hu, Pengfei Xu, Hua Chai, Ruini Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14354">https://arxiv.org/abs/2403.14354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14354">https://arxiv.org/pdf/2403.14354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14354]] LDTR: Transformer-based Lane Detection with Anchor-chain Representation(https://arxiv.org/abs/2403.14354)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite recent advances in lane detection methods, scenarios with limited- or no-visual-clue of lanes due to factors such as lighting conditions and occlusion remain challenging and crucial for automated driving. Moreover, current lane representations require complex post-processing and struggle with specific instances. Inspired by the DETR architecture, we propose LDTR, a transformer-based model to address these issues. Lanes are modeled with a novel anchor-chain, regarding a lane as a whole from the beginning, which enables LDTR to handle special lanes inherently. To enhance lane instance perception, LDTR incorporates a novel multi-referenced deformable attention module to distribute attention around the object. Additionally, LDTR incorporates two line IoU algorithms to improve convergence efficiency and employs a Gaussian heatmap auxiliary branch to enhance model representation capability during training. To evaluate lane detection models, we rely on Frechet distance, parameterized F1-score, and additional synthetic metrics. Experimental results demonstrate that LDTR achieves state-of-the-art performance on well-known datasets.</li>
</ul>

<h3>Title: Exploring the Potential of Large Language Models in Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14358">https://arxiv.org/abs/2403.14358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14358">https://arxiv.org/pdf/2403.14358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14358]] Exploring the Potential of Large Language Models in Graph Generation(https://arxiv.org/abs/2403.14358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification. However, the abilities of LLMs for graph generation remain unexplored in the literature. Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation. Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation. We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance. Besides, LLMs show potential in generating molecules with specific properties. These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research.</li>
</ul>

<h3>Title: Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen  Domains by Intrinsic Learning from Redundant LLM Semantics</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Yue, Jiancheng Zhao, Chunhui Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14362">https://arxiv.org/abs/2403.14362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14362">https://arxiv.org/pdf/2403.14362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14362]] Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen  Domains by Intrinsic Learning from Redundant LLM Semantics(https://arxiv.org/abs/2403.14362)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen classes against domain shift problem (DSP) where data of unseen classes may be misclassified as seen classes. However, existing GZSL is still limited to seen domains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which addresses GZSL towards unseen domains. Different from existing GZSL methods which alleviate DSP by generating features of unseen classes with semantics, CDGZSL needs to construct a common feature space across domains and acquire the corresponding intrinsic semantics shared among domains to transfer from seen to unseen domains. Considering the information asymmetry problem caused by redundant class semantics annotated with large language models (LLMs), we present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates the non-intrinsic semantics not shared across all domains under the guidance of inter-class feature relationships, and Unseen-class Meta Generation (UMG), which preserves intrinsic semantics to maintain connectivity between seen and unseen classes by simulating feature generation. MDASR effectively aligns the redundant semantic space with the common feature space, mitigating the information asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on the Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics for these datasets as the benchmark.</li>
</ul>

<h3>Title: WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for  Atomic Factual Knowledge Update in Causal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14364">https://arxiv.org/abs/2403.14364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14364">https://arxiv.org/pdf/2403.14364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14364]] WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for  Atomic Factual Knowledge Update in Causal Language Models(https://arxiv.org/abs/2403.14364)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are "unknown" to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions. We also present an evaluation of existing update algorithms on WikiFactDiff.</li>
</ul>

<h3>Title: SyncTweedies: A General Generative Framework Based on Synchronized  Diffusions</h3>
<ul>
<li><strong>Authors: </strong>Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14370">https://arxiv.org/abs/2403.14370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14370">https://arxiv.org/pdf/2403.14370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14370]] SyncTweedies: A General Generative Framework Based on Synchronized  Diffusions(https://arxiv.org/abs/2403.14370)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes. We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications. In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedie's formula while conducting denoising in multiple instance spaces. This case also provides the best quality with the widest applicability to downstream tasks. We name this case SyncTweedies. In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods.</li>
</ul>

<h3>Title: Loop Improvement: An Efficient Approach for Extracting Shared Features  from Heterogeneous Data without Central Server</h3>
<ul>
<li><strong>Authors: </strong>Fei Li, Chu Kiong Loo, Wei Shiung Liew, Xiaofeng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14371">https://arxiv.org/abs/2403.14371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14371">https://arxiv.org/pdf/2403.14371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14371]] Loop Improvement: An Efficient Approach for Extracting Shared Features  from Heterogeneous Data without Central Server(https://arxiv.org/abs/2403.14371)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, federate</a></li>
<li><strong>Abstract: </strong>In federated learning, data heterogeneity significantly impacts performance. A typical solution involves segregating these parameters into shared and personalized components, a concept also relevant in multi-task learning. Addressing this, we propose "Loop Improvement" (LI), a novel method enhancing this separation and feature extraction without necessitating a central server or data interchange among participants. Our experiments reveal LI's superiority in several aspects: In personalized federated learning environments, LI consistently outperforms the advanced FedALA algorithm in accuracy across diverse scenarios. Additionally, LI's feature extractor closely matches the performance achieved when aggregating data from all clients. In global model contexts, employing LI with stacked personalized layers and an additional network also yields comparable results to combined client data scenarios. Furthermore, LI's adaptability extends to multi-task learning, streamlining the extraction of common features across tasks and obviating the need for simultaneous training. This approach not only enhances individual task performance but also achieves accuracy levels on par with classic multi-task learning methods where all tasks are trained simultaneously. LI integrates a loop topology with layer-wise and end-to-end training, compatible with various neural network models. This paper also delves into the theoretical underpinnings of LI's effectiveness, offering insights into its potential applications. The code is on https://github.com/axedge1983/LI</li>
</ul>

<h3>Title: FIT-RAG: Black-Box RAG with Factual Information and Token Reduction</h3>
<ul>
<li><strong>Authors: </strong>Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14374">https://arxiv.org/abs/2403.14374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14374">https://arxiv.org/pdf/2403.14374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14374]] FIT-RAG: Black-Box RAG with Factual Information and Token Reduction(https://arxiv.org/abs/2403.14374)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this paper proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.</li>
</ul>

<h3>Title: From Large to Tiny: Distilling and Refining Mathematical Expertise for  Math Word Problems with Weakly Supervision</h3>
<ul>
<li><strong>Authors: </strong>Qingwen Lin, Boyan Xu, Zhengting Huang, Ruichu Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14390">https://arxiv.org/abs/2403.14390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14390">https://arxiv.org/pdf/2403.14390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14390]] From Large to Tiny: Distilling and Refining Mathematical Expertise for  Math Word Problems with Weakly Supervision(https://arxiv.org/abs/2403.14390)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal. Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions. The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly. However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight. In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models. In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to distill mathematical knowledge from LLMs to construct problem-equation pairs required for supervised training. In \emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method. Finally, We train a small model using distilled data generated through two-stage methods. As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than ChatGPT.</li>
</ul>

<h3>Title: A Bag of Tricks for Few-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Shuvendu Roy, Chunjong Park, Aldi Fahrezi, Ali Etemad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14392">https://arxiv.org/abs/2403.14392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14392">https://arxiv.org/pdf/2403.14392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14392]] A Bag of Tricks for Few-Shot Class-Incremental Learning(https://arxiv.org/abs/2403.14392)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which is a challenging form of continual learning that involves continuous adaptation to new tasks with limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones. Our proposed bag of tricks brings together eight key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL. We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones. On the other hand, adaptability tricks focus on the effective learning of new classes. Finally, training tricks improve the overall performance without compromising stability or adaptability. We perform extensive experiments on three benchmark datasets, CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed framework. Our detailed analysis shows that our approach substantially improves both stability and adaptability, establishing a new state-of-the-art by outperforming prior works in the area. We believe our method provides a go-to solution and establishes a robust baseline for future research in this area.</li>
</ul>

<h3>Title: Building Accurate Translation-Tailored LLMs with Language Aware  Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Changtong Zan, Liang Ding, Li Shen, Yibing Zhen, Weifeng Liu, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14399">https://arxiv.org/abs/2403.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14399">https://arxiv.org/pdf/2403.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14399]] Building Accurate Translation-Tailored LLMs with Language Aware  Instruction Tuning(https://arxiv.org/abs/2403.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations. However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information. In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples. Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model's general task performance on AlpacaEval. Code and models will be released at \url{https://github.com/alphadl/LanguageAware_Tuning}.</li>
</ul>

<h3>Title: Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Dingchen Yang, Bowen Cao, Guang Chen, Changjun Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14401">https://arxiv.org/abs/2403.14401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14401">https://arxiv.org/pdf/2403.14401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14401]] Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination(https://arxiv.org/abs/2403.14401)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal Large Language Models (MLLMs) demonstrate remarkable success across various vision-language tasks. However, they suffer from visual hallucination, where the generated responses diverge from the provided image. Are MLLMs completely oblivious to accurate visual cues when they hallucinate? Our investigation reveals that the visual branch may simultaneously advocate both accurate and non-existent content. To address this issue, we propose Pensieve, a training-free method inspired by our observation that analogous visual hallucinations can arise among images sharing common semantic and appearance characteristics. During inference, Pensieve enables MLLMs to retrospect relevant images as references and compare them with the test image. This paradigm assists MLLMs in downgrading hallucinatory content mistakenly supported by the visual input. Experiments on Whoops, MME, POPE, and LLaVA Bench demonstrate the efficacy of Pensieve in mitigating visual hallucination, surpassing other advanced decoding strategies. Additionally, Pensieve aids MLLMs in identifying details in the image and enhancing the specificity of image descriptions.</li>
</ul>

<h3>Title: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language  Models through Question Complexity</h3>
<ul>
<li><strong>Authors: </strong>Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14403">https://arxiv.org/abs/2403.14403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14403">https://arxiv.org/pdf/2403.14403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14403]] Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language  Models through Question Complexity(https://arxiv.org/abs/2403.14403)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.</li>
</ul>

<h3>Title: Physics-Informed Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jan-Hendrik Bastek, WaiChing Sun, Dennis M. Kochmann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14404">https://arxiv.org/abs/2403.14404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14404">https://arxiv.org/pdf/2403.14404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14404]] Physics-Informed Diffusion Models(https://arxiv.org/abs/2403.14404)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.</li>
</ul>

<h3>Title: Locating and Mitigating Gender Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14409">https://arxiv.org/abs/2403.14409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14409">https://arxiv.org/pdf/2403.14409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14409]] Locating and Mitigating Gender Bias in Large Language Models(https://arxiv.org/abs/2403.14409)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.</li>
</ul>

<h3>Title: OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14418">https://arxiv.org/abs/2403.14418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14418">https://arxiv.org/pdf/2403.14418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14418]] OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation(https://arxiv.org/abs/2403.14418)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The booming of 3D recognition in the 2020s began with the introduction of point cloud transformers. They quickly overwhelmed sparse CNNs and became state-of-the-art models, especially in 3D semantic segmentation. However, sparse CNNs are still valuable networks, due to their efficiency treasure, and ease of application. In this work, we reexamine the design distinctions and test the limits of what a sparse CNN can achieve. We discover that the key credit to the performance difference is adaptivity. Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap. This exploration led to the creation of Omni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal computational cost. Without any self-attention modules, OA-CNNs favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation benchmarks respectively, while maintaining at most 5x better speed than transformer counterparts. This revelation highlights the potential of pure sparse CNNs to outperform transformer-related networks.</li>
</ul>

<h3>Title: DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14421">https://arxiv.org/abs/2403.14421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14421">https://arxiv.org/pdf/2403.14421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14421]] DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning(https://arxiv.org/abs/2403.14421)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only retrieval for up to $10,000$ queries.</li>
</ul>

<h3>Title: FHAUC: Privacy Preserving AUC Calculation for Federated Learning using  Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Cem Ata Baykara, Ali Burak Ünal, Mete Akgün</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14428">https://arxiv.org/abs/2403.14428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14428">https://arxiv.org/pdf/2403.14428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14428]] FHAUC: Privacy Preserving AUC Calculation for Federated Learning using  Fully Homomorphic Encryption(https://arxiv.org/abs/2403.14428)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Ensuring data privacy is a significant challenge for machine learning applications, not only during model training but also during evaluation. Federated learning has gained significant research interest in recent years as a result. Current research on federated learning primarily focuses on preserving privacy during the training phase. However, model evaluation has not been adequately addressed, despite the potential for significant privacy leaks during this phase as well. In this paper, we demonstrate that the state-of-the-art AUC computation method for federated learning systems, which utilizes differential privacy, still leaks sensitive information about the test data while also requiring a trusted central entity to perform the computations. More importantly, we show that the performance of this method becomes completely unusable as the data size decreases. In this context, we propose an efficient, accurate, robust, and more secure evaluation algorithm capable of computing the AUC in horizontal federated learning systems. Our approach not only enhances security compared to the current state-of-the-art but also surpasses the state-of-the-art AUC computation method in both approximation performance and computational robustness, as demonstrated by experimental results. To illustrate, our approach can efficiently calculate the AUC of a federated learning system involving 100 parties, achieving 99.93% accuracy in just 0.68 seconds, regardless of data size, while providing complete data privacy.</li>
</ul>

<h3>Title: Style-Extracting Diffusion Models for Semi-Supervised Histopathology  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mathias Öttl, Frauke Wilm, Jana Steenpass, Jingna Qiu, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Bernhard Kainz, Katharina Breininger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14429">https://arxiv.org/abs/2403.14429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14429">https://arxiv.org/pdf/2403.14429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14429]] Style-Extracting Diffusion Models for Semi-Supervised Histopathology  Segmentation(https://arxiv.org/abs/2403.14429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning-based image generation has seen significant advancements with diffusion models, notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting Diffusion Models, featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation. We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs. This architecture enables the generation of images with unseen styles in a zero-shot manner, by leveraging styles from unseen images, resulting in more diverse generations. In this work, we use the image layout as target condition and first show the capability of our method on a natural image dataset as a proof-of-concept. We further demonstrate its versatility in histopathology, where we combine prior knowledge about tissue composition and unannotated data to create diverse synthetic images with known layouts. This allows us to generate additional synthetic data to train a segmentation network in a semi-supervised fashion. We verify the added value of the generated images by showing improved segmentation results and lower performance variability between patients when synthetic images are included during segmentation training. Our code will be made publicly available at [LINK].</li>
</ul>

<h3>Title: Ranking Distillation for Open-Ended Video Question Answering with  Insufficient Labels</h3>
<ul>
<li><strong>Authors: </strong>Tianming Liang, Chaolei Tan, Beihao Xia, Wei-Shi Zheng, Jian-Fang Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14430">https://arxiv.org/abs/2403.14430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14430">https://arxiv.org/pdf/2403.14430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14430]] Ranking Distillation for Open-Ended Video Question Answering with  Insufficient Labels(https://arxiv.org/abs/2403.14430)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper focuses on open-ended video question answering, which aims to find the correct answers from a large answer set in response to a video-related question. This is essentially a multi-label classification task, since a question may have multiple answers. However, due to annotation costs, the labels in existing benchmarks are always extremely insufficient, typically one answer per question. As a result, existing works tend to directly treat all the unlabeled answers as negative labels, leading to limited ability for generalization. In this work, we introduce a simple yet effective ranking distillation framework (RADI) to mitigate this problem without additional manual annotation. RADI employs a teacher model trained with incomplete labels to generate rankings for potential answers, which contain rich knowledge about label priority as well as label-associated visual cues, thereby enriching the insufficient labeling information. To avoid overconfidence in the imperfect teacher model, we further present two robust and parameter-free ranking distillation approaches: a pairwise approach which introduces adaptive soft margins to dynamically refine the optimization constraints on various pairwise rankings, and a listwise approach which adopts sampling-based partial listwise learning to resist the bias in teacher ranking. Extensive experiments on five popular benchmarks consistently show that both our pairwise and listwise RADIs outperform state-of-the-art methods. Further analysis demonstrates the effectiveness of our methods on the insufficient labeling problem.</li>
</ul>

<h3>Title: A Multimodal Approach to Device-Directed Speech Detection with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dominik Wager, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14438">https://arxiv.org/abs/2403.14438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14438">https://arxiv.org/pdf/2403.14438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14438]] A Multimodal Approach to Device-Directed Speech Detection with Large  Language Models(https://arxiv.org/abs/2403.14438)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset.</li>
</ul>

<h3>Title: RoDLA: Benchmarking the Robustness of Document Layout Analysis Models</h3>
<ul>
<li><strong>Authors: </strong>Yufan Chen, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ruiping Liu, Philip Torr, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14442">https://arxiv.org/abs/2403.14442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14442">https://arxiv.org/pdf/2403.14442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14442]] RoDLA: Benchmarking the Robustness of Document Layout Analysis Models(https://arxiv.org/abs/2403.14442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Before developing a Document Layout Analysis (DLA) model in real-world applications, conducting comprehensive robustness testing is essential. However, the robustness of DLA models remains underexplored in the literature. To address this, we are the first to introduce a robustness benchmark for DLA models, which includes 450K document images of three datasets. To cover realistic corruptions, we propose a perturbation taxonomy with 36 common document perturbations inspired by real-world document processing. Additionally, to better understand document perturbation impacts, we propose two metrics, Mean Perturbation Effect (mPE) for perturbation assessment and Mean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we introduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA), which improves attention mechanisms to boost extraction of robust features. Experiments on the proposed benchmarks (PubLayNet-P, DocLayNet-P, and M$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of 115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA achieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.</li>
</ul>

<h3>Title: More than Just Statistical Recurrence: Human and Machine Unsupervised  Learning of Māori Word Segmentation across Morphological Processes</h3>
<ul>
<li><strong>Authors: </strong>Ashvini Varatharaj, Simon Todd</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14444">https://arxiv.org/abs/2403.14444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14444">https://arxiv.org/pdf/2403.14444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14444]] More than Just Statistical Recurrence: Human and Machine Unsupervised  Learning of Māori Word Segmentation across Morphological Processes(https://arxiv.org/abs/2403.14444)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Non-M\=aori-speaking New Zealanders (NMS)are able to segment M\=aori words in a highlysimilar way to fluent speakers (Panther et al.,2024). This ability is assumed to derive through the identification and extraction of statistically recurrent forms. We examine this assumption by asking how NMS segmentations compare to those produced by Morfessor, an unsupervised machine learning model that operates based on statistical recurrence, across words formed by a variety of morphological processes. Both NMS and Morfessor succeed in segmenting words formed by concatenative processes (compounding and affixation without allomorphy), but NMS also succeed for words that invoke templates (reduplication and allomorphy) and other cues to morphological structure, implying that their learning process is sensitive to more than just statistical recurrence.</li>
</ul>

<h3>Title: gTBLS: Generating Tables from Text by Conditional Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Anirudh Sundar, Christopher Richardson, Larry Heck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14457">https://arxiv.org/abs/2403.14457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14457">https://arxiv.org/pdf/2403.14457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14457]] gTBLS: Generating Tables from Text by Conditional Question Answering(https://arxiv.org/abs/2403.14457)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Distilling large, unstructured text into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the Transformer's attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates questions using these headers and fine-tunes a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained Large Language Models in a zero-shot configuration, presenting a solution for table generation in situations where fine-tuning is not feasible. gTBLS improves prior approaches by up to 10% in BERTScore on the table construction task and up to 20% on the table content generation task of the E2E, WikiTableText, WikiBio, and RotoWire datasets.</li>
</ul>

<h3>Title: Multi-Level Explanations for Generative Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14459">https://arxiv.org/abs/2403.14459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14459">https://arxiv.org/pdf/2403.14459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14459]] Multi-Level Explanations for Generative Language Models(https://arxiv.org/abs/2403.14459)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Perturbation-based explanation methods such as LIME and SHAP are commonly applied to text classification. This work focuses on their extension to generative language models. To address the challenges of text as output and long text inputs, we propose a general framework called MExGen that can be instantiated with different attribution algorithms. To handle text output, we introduce the notion of scalarizers for mapping text to real numbers and investigate multiple possibilities. To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and context-grounded question answering. The results show that our framework can provide more locally faithful explanations of generated outputs.</li>
</ul>

<h3>Title: Universal Feature Selection for Simultaneous Interpretability of  Multitask Datasets</h3>
<ul>
<li><strong>Authors: </strong>Matt Raymond, Jacob Charles Saldinger, Paolo Elvati, Clayton Scott, Angela Violi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14466">https://arxiv.org/abs/2403.14466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14466">https://arxiv.org/pdf/2403.14466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14466]] Universal Feature Selection for Simultaneous Interpretability of  Multitask Datasets(https://arxiv.org/abs/2403.14466)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Extracting meaningful features from complex, high-dimensional datasets across scientific domains remains challenging. Current methods often struggle with scalability, limiting their applicability to large datasets, or make restrictive assumptions about feature-property relationships, hindering their ability to capture complex interactions. BoUTS's general and scalable feature selection algorithm surpasses these limitations to identify both universal features relevant to all datasets and task-specific features predictive for specific subsets. Evaluated on seven diverse chemical regression datasets, BoUTS achieves state-of-the-art feature sparsity while maintaining prediction accuracy comparable to specialized methods. Notably, BoUTS's universal features enable domain-specific knowledge transfer between datasets, and suggest deep connections in seemingly-disparate chemical datasets. We expect these results to have important repercussions in manually-guided inverse problems. Beyond its current application, BoUTS holds immense potential for elucidating data-poor systems by leveraging information from similar data-rich systems. BoUTS represents a significant leap in cross-domain feature selection, potentially leading to advancements in various scientific fields.</li>
</ul>

<h3>Title: ChatGPT Alternative Solutions: Large Language Models Survey</h3>
<ul>
<li><strong>Authors: </strong>Hanieh Alipour, Nick Pendar, Kohinoor Roy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14469">https://arxiv.org/abs/2403.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14469">https://arxiv.org/pdf/2403.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14469]] ChatGPT Alternative Solutions: Large Language Models Survey(https://arxiv.org/abs/2403.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights. A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms. Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs. Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature. By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories. This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.</li>
</ul>

<h3>Title: Detoxifying Large Language Models via Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14472">https://arxiv.org/abs/2403.14472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14472">https://arxiv.org/pdf/2403.14472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14472]] Detoxifying Large Language Models via Knowledge Editing(https://arxiv.org/abs/2403.14472)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.</li>
</ul>

<h3>Title: HyperGALE: ASD Classification via Hypergraph Gated Attention with  Learnable Hyperedges</h3>
<ul>
<li><strong>Authors: </strong>Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14484">https://arxiv.org/abs/2403.14484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14484">https://arxiv.org/pdf/2403.14484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14484]] HyperGALE: ASD Classification via Hypergraph Gated Attention with  Learnable Hyperedges(https://arxiv.org/abs/2403.14484)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by varied social cognitive challenges and repetitive behavioral patterns. Identifying reliable brain imaging-based biomarkers for ASD has been a persistent challenge due to the spectrum's diverse symptomatology. Existing baselines in the field have made significant strides in this direction, yet there remains room for improvement in both performance and interpretability. We propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating learned hyperedges and gated attention mechanisms. This approach has led to substantial improvements in the model's ability to interpret complex brain graph data, offering deeper insights into ASD biomarker characterization. Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves interpretability but also demonstrates statistically significant enhancements in key performance metrics compared to both previous baselines and the foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD research highlights the potential of sophisticated graph-based techniques in neurodevelopmental studies. The source code and implementation instructions are available at GitHub:https://github.com/mehular0ra/HyperGALE.</li>
</ul>

<h3>Title: Adversary-Robust Graph-Based Learning of WSIs</h3>
<ul>
<li><strong>Authors: </strong>Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14489">https://arxiv.org/abs/2403.14489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14489">https://arxiv.org/pdf/2403.14489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14489]] Adversary-Robust Graph-Based Learning of WSIs(https://arxiv.org/abs/2403.14489)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Enhancing the robustness of deep learning models against adversarial attacks is crucial, especially in critical domains like healthcare where significant financial interests heighten the risk of such attacks. Whole slide images (WSIs) are high-resolution, digitized versions of tissue samples mounted on glass slides, scanned using sophisticated imaging equipment. The digital analysis of WSIs presents unique challenges due to their gigapixel size and multi-resolution storage format. In this work, we aim at improving the robustness of cancer Gleason grading classification systems against adversarial attacks, addressing challenges at both the image and graph levels. As regards the proposed algorithm, we develop a novel and innovative graph-based model which utilizes GNN to extract features from the graph representation of WSIs. A denoising module, along with a pooling layer is incorporated to manage the impact of adversarial attacks on the WSIs. The process concludes with a transformer module that classifies various grades of prostate cancer based on the processed data. To assess the effectiveness of the proposed method, we conducted a comparative analysis using two scenarios. Initially, we trained and tested the model without the denoiser using WSIs that had not been exposed to any attack. We then introduced a range of attacks at either the image or graph level and processed them through the proposed network. The performance of the model was evaluated in terms of accuracy and kappa scores. The results from this comparison showed a significant improvement in cancer diagnosis accuracy, highlighting the robustness and efficiency of the proposed method in handling adversarial challenges in the context of medical imaging.</li>
</ul>

<h3>Title: Learning to Project for Cross-Task Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Dylan Auty, Roy Miles, Benedikt Kolbeinsson, Krystian Mikolajczyk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14494">https://arxiv.org/abs/2403.14494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14494">https://arxiv.org/pdf/2403.14494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14494]] Learning to Project for Cross-Task Knowledge Distillation(https://arxiv.org/abs/2403.14494)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Traditional knowledge distillation (KD) relies on a proficient teacher trained on the target task, which is not always available. In this setting, cross-task distillation can be used, enabling the use of any teacher model trained on a different task. However, many KD methods prove ineffective when applied to this cross-task setting. To address this limitation, we propose a simple modification: the use of an inverted projection. We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student's performance. We find that this simple modification is sufficient for extending many KD methods to the cross-task setting, where the teacher and student tasks can be very different. In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost. Our method can obtain significant performance improvements (up to 7%) when using even a randomly-initialised teacher on various tasks such as depth estimation, image translation, and semantic segmentation, despite the lack of any learned knowledge to transfer. To provide conceptual and analytical insights into this result, we show that using an inverted projection allows the distillation loss to be decomposed into a knowledge transfer and a spectral regularisation component. Through this analysis we are additionally able to propose a novel regularisation loss that allows teacher-free distillation, enabling performance improvements of up to 8.57% on ImageNet with no additional training costs.</li>
</ul>

<h3>Title: View-decoupled Transformer for Person Re-identification under  Aerial-ground Camera Network</h3>
<ul>
<li><strong>Authors: </strong>Quan Zhang, Lei Wang, Vishal M. Patel, Xiaohua Xie, Jianhuang Lai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14513">https://arxiv.org/abs/2403.14513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14513">https://arxiv.org/pdf/2403.14513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14513]] View-decoupled Transformer for Person Re-identification under  Aerial-ground Camera Network(https://arxiv.org/abs/2403.14513)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Existing person re-identification methods have achieved remarkable advances in appearance-based identity association across homogeneous cameras, such as ground-ground matching. However, as a more practical scenario, aerial-ground person re-identification (AGPReID) among heterogeneous cameras has received minimal attention. To alleviate the disruption of discriminative identity representation by dramatic view discrepancy as the most significant challenge in AGPReID, the view-decoupled transformer (VDT) is proposed as a simple yet effective framework. Two major components are designed in VDT to decouple view-related and view-unrelated features, namely hierarchical subtractive separation and orthogonal loss, where the former separates these two features inside the VDT, and the latter constrains these two to be independent. In addition, we contribute a large-scale AGPReID dataset called CARGO, consisting of five/eight aerial/ground cameras, 5,000 identities, and 108,563 images. Experiments on two datasets show that VDT is a feasible and effective solution for AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on CARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational complexity. Our project is available at https://github.com/LinlyAC/VDT-AGPReID</li>
</ul>

<h3>Title: Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient  Inference</h3>
<ul>
<li><strong>Authors: </strong>Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14520">https://arxiv.org/abs/2403.14520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14520">https://arxiv.org/pdf/2403.14520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14520]] Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient  Inference(https://arxiv.org/abs/2403.14520)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, the application of multimodal large language models (MLLM) in various fields has achieved remarkable success. However, as the foundation model for many downstream tasks, current MLLMs are composed of the well-known Transformer network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective multi-modal Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, \textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction benchmarks show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at: https://sites.google.com/view/cobravlm.</li>
</ul>

<h3>Title: Object-Centric Domain Randomization for 3D Shape Reconstruction in the  Wild</h3>
<ul>
<li><strong>Authors: </strong>Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14539">https://arxiv.org/abs/2403.14539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14539">https://arxiv.org/pdf/2403.14539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14539]] Object-Centric Domain Randomization for 3D Shape Reconstruction in the  Wild(https://arxiv.org/abs/2403.14539)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of <3D shape, 2D image>-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry prior which is consistent across various domains. We validate its effectiveness by substantially improving 3D shape reconstruction models on a real-world benchmark. In a scale-up evaluation, our pre-training achieves 23.6% superior results compared with the pre-training on high-quality computer graphics renderings.</li>
</ul>

<h3>Title: EDT: Improving Large Language Models' Generation by Entropy-based  Dynamic Temperature Sampling</h3>
<ul>
<li><strong>Authors: </strong>Shimao Zhang, Yu Bao, Shujian Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14541">https://arxiv.org/abs/2403.14541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14541">https://arxiv.org/pdf/2403.14541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14541]] EDT: Improving Large Language Models' Generation by Entropy-based  Dynamic Temperature Sampling(https://arxiv.org/abs/2403.14541)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for LLMs' generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. Our experiments show that EDT significantly outperforms the existing strategies across different tasks.</li>
</ul>

<h3>Title: Token Transformation Matters: Towards Faithful Post-hoc Explanation for  Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Junyi Wu, Bin Duan, Weitai Kang, Hao Tang, Yan Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14552">https://arxiv.org/abs/2403.14552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14552">https://arxiv.org/pdf/2403.14552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14552]] Token Transformation Matters: Towards Faithful Post-hoc Explanation for  Vision Transformer(https://arxiv.org/abs/2403.14552)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>While Transformers have rapidly gained popularity in various computer vision applications, post-hoc explanations of their internal mechanisms remain largely unexplored. Vision Transformers extract visual information by representing image regions as transformed tokens and integrating them via attention weights. However, existing post-hoc explanation methods merely consider these attention weights, neglecting crucial information from the transformed tokens, which fails to accurately illustrate the rationales behind the models' predictions. To incorporate the influence of token transformation into interpretation, we propose TokenTM, a novel post-hoc explanation method that utilizes our introduced measurement of token transformation effects. Specifically, we quantify token transformation effects by measuring changes in token lengths and correlations in their directions pre- and post-transformation. Moreover, we develop initialization and aggregation rules to integrate both attention weights and token transformation effects across all layers, capturing holistic token contributions throughout the model. Experimental results on segmentation and perturbation tests demonstrate the superiority of our proposed TokenTM compared to state-of-the-art Vision Transformer explanation methods.</li>
</ul>

<h3>Title: A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'  Formative Assessment Responses in Science</h3>
<ul>
<li><strong>Authors: </strong>Clayton Cohn, Nicole Hutchins, Tuan Le, Gautam Biswas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14565">https://arxiv.org/abs/2403.14565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14565">https://arxiv.org/pdf/2403.14565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14565]] A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'  Formative Assessment Responses in Science(https://arxiv.org/abs/2403.14565)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.</li>
</ul>

<h3>Title: RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants  in the Biomedical Domain</h3>
<ul>
<li><strong>Authors: </strong>William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van Bergen Gonzalez Bueno, Lea Goetz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14578">https://arxiv.org/abs/2403.14578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14578">https://arxiv.org/pdf/2403.14578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14578]] RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants  in the Biomedical Domain(https://arxiv.org/abs/2403.14578)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched. In this work we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation LLMs can serve as reliable assistants in the biomedical domain. We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case. We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions. We evaluate LLM performance using semantic similarity with a ground truth response, through an evaluator LLM.</li>
</ul>

<h3>Title: Global, robust and comparable digital carbon assets</h3>
<ul>
<li><strong>Authors: </strong>Sadiq Jaffer, Michael Dales, Patrick Ferris, Thomas Swinfield, Derek Sorensen, Robin Message, Anil Madhavapeddy, Srinivasan Keshav</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14581">https://arxiv.org/abs/2403.14581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14581">https://arxiv.org/pdf/2403.14581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14581]] Global, robust and comparable digital carbon assets(https://arxiv.org/abs/2403.14581)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Carbon credits purchased in the voluntary carbon market allow unavoidable emissions, such as from international flights for essential travel, to be offset by an equivalent climate benefit, such as avoiding emissions from tropical deforestation. However, many concerns regarding the credibility of these offsetting claims have been raised. Moreover, the credit market is manual, therefore inefficient and unscalable, and non-fungible, therefore illiquid. To address these issues, we propose an efficient digital methodology that combines remote sensing data, modern econometric techniques, and on-chain certification and trading to create a new digital carbon asset (the PACT stablecoin) against which carbon offsetting claims can be transparently verified. PACT stablecoins are produced as outputs from a reproducible computational pipeline for estimating the climate benefits of carbon offset projects that not only quantifies the CO2 emissions involved, but also allows for similar credits to be pooled based on their co-benefits such as biodiversity and jurisdictional attributes, increasing liquidity through fungibility within pools. We implement and evaluate the PACT carbon stablecoin on the Tezos blockchain, which is designed to facilitate low-cost transactions while minimizing environmental impact. Our implementation includes a contract for a registry for tracking issuance, ownership, and retirement of credits, and a custodian contract to bridge on-chain and off-chain transactions. Our work brings scale and trust to the voluntary carbon market by providing a transparent, scalable, and efficient framework for high integrity carbon credit transactions.</li>
</ul>

<h3>Title: Large Language Models for Multi-Choice Question Classification of  Medical Subjects</h3>
<ul>
<li><strong>Authors: </strong>Víctor Ponce-López</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14582">https://arxiv.org/abs/2403.14582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14582">https://arxiv.org/pdf/2403.14582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14582]] Large Language Models for Multi-Choice Question Classification of  Medical Subjects(https://arxiv.org/abs/2403.14582)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects. This is an important and challenging task for automatic question answering. To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects. Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain.</li>
</ul>

<h3>Title: VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yun-Jin Li, Mariia Gladkova, Yan Xia, Rui Wang, Daniel Cremers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14594">https://arxiv.org/abs/2403.14594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14594">https://arxiv.org/pdf/2403.14594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14594]] VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition(https://arxiv.org/abs/2403.14594)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent works on the global place recognition treat the task as a retrieval problem, where an off-the-shelf global descriptor is commonly designed in image-based and LiDAR-based modalities. However, it is non-trivial to perform accurate image-LiDAR global place recognition since extracting consistent and robust global descriptors from different domains (2D images and 3D point clouds) is challenging. To address this issue, we propose a novel Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel correspondences in a self-supervised manner and brings them into a shared feature space. Specifically, VXP is trained in a two-stage manner that first explicitly exploits local feature correspondences and enforces similarity of global descriptors. Extensive experiments on the three benchmarks (Oxford RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the state-of-the-art cross-modal retrieval by a large margin.</li>
</ul>

<h3>Title: PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model</h3>
<ul>
<li><strong>Authors: </strong>Zheng Zhang, Yeyao Ma, Enming Zhang, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14598">https://arxiv.org/abs/2403.14598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14598">https://arxiv.org/pdf/2403.14598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14598]] PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model(https://arxiv.org/abs/2403.14598)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>PSALM is a powerful extension of the Large Multi-modal Model (LMM) to address the segmentation task challenges. To overcome the limitation of the LMM being limited to textual output, PSALM incorporates a mask decoder and a well-designed input schema to handle a variety of segmentation tasks. This schema includes images, task instructions, conditional prompts, and mask tokens, which enable the model to generate and classify segmentation masks effectively. The flexible design of PSALM supports joint training across multiple datasets and tasks, leading to improved performance and task generalization. PSALM achieves superior results on several benchmarks, such as RefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive, and further exhibits zero-shot capabilities on unseen tasks, such as open-vocabulary segmentation, generalized referring expression segmentation and video object segmentation, making a significant step towards a GPT moment in computer vision. Through extensive experiments, PSALM demonstrates its potential to transform the domain of image segmentation, leveraging the robust visual understanding capabilities of LMMs as seen in natural language processing. Code and models are available at https://github.com/zamling/PSALM.</li>
</ul>

<h3>Title: ReNoise: Real Image Inversion Through Iterative Noising</h3>
<ul>
<li><strong>Authors: </strong>Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, Daniel Cohen-Or</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14602">https://arxiv.org/abs/2403.14602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14602">https://arxiv.org/pdf/2403.14602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14602]] ReNoise: Real Image Inversion Through Iterative Noising(https://arxiv.org/abs/2403.14602)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-guided diffusion models have unlocked powerful image manipulation capabilities. However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained diffusion model. Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps. In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations. Building on reversing the diffusion sampling process, our method employs an iterative renoising mechanism at each inversion sampling step. This mechanism refines the approximation of a predicted point along the forward diffusion trajectory, by iteratively applying the pretrained diffusion model, and averaging these predictions. We evaluate the performance of our ReNoise technique using various sampling algorithms and models, including recent accelerated diffusion models. Through comprehensive evaluations and comparisons, we show its effectiveness in terms of both accuracy and speed. Furthermore, we confirm that our method preserves editability by demonstrating text-driven image editing on real images.</li>
</ul>

<h3>Title: Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Han, Chao Gao, Jinyang Liu, Jeff (Jun)Zhang, Sai Qian Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14608">https://arxiv.org/abs/2403.14608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14608">https://arxiv.org/pdf/2403.14608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14608]] Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey(https://arxiv.org/abs/2403.14608)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different PEFT algorithms. This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications.</li>
</ul>

<h3>Title: DreamReward: Text-to-3D Generation with Human Preference</h3>
<ul>
<li><strong>Authors: </strong>Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14613">https://arxiv.org/abs/2403.14613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14613">https://arxiv.org/pdf/2403.14613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14613]] DreamReward: Text-to-3D Generation with Human Preference(https://arxiv.org/abs/2403.14613)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D content creation from text prompts has shown remarkable success recently. However, current text-to-3D methods often generate 3D results that do not align well with human preferences. In this paper, we present a comprehensive framework, coined DreamReward, to learn and improve text-to-3D models from human preference feedback. To begin with, we collect 25k expert comparisons based on a systematic annotation pipeline including rating and ranking. Then, we build Reward3D -- the first general-purpose text-to-3D human preference reward model to effectively encode human preferences. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL), a direct tuning algorithm to optimize the multi-view diffusion models with a redefined scorer. Grounded by theoretical proof and extensive experiment comparisons, our DreamReward successfully generates high-fidelity and 3D consistent results with significant boosts in prompt alignment with human intention. Our results demonstrate the great potential for learning from human feedback to improve text-to-3D models.</li>
</ul>

<h3>Title: Hierarchical Text-to-Vision Self Supervised Alignment for Improved  Histopathology Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Hasindri Watawana, Kanchana Ranasinghe, Tariq Mahmood, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14616">https://arxiv.org/abs/2403.14616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14616">https://arxiv.org/pdf/2403.14616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14616]] Hierarchical Text-to-Vision Self Supervised Alignment for Improved  Histopathology Representation Learning(https://arxiv.org/abs/2403.14616)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Self-supervised representation learning has been highly promising for histopathology image analysis with numerous approaches leveraging their patient-slide-patch hierarchy to learn better representations. In this paper, we explore how the combination of domain specific natural language information with such hierarchical visual representations can benefit rich representation learning for medical image tasks. Building on automated language description generation for features visible in histopathology images, we present a novel language-tied self-supervised learning framework, Hierarchical Language-tied Self-Supervision (HLSS) for histopathology images. We explore contrastive objectives and granular language description based text alignment at multiple hierarchies to inject language modality information into the visual representations. Our resulting model achieves state-of-the-art performance on two medical imaging benchmarks, OpenSRH and TCGA datasets. Our framework also provides better interpretability with our language aligned representation space. Code is available at https://github.com/Hasindri/HLSS.</li>
</ul>

<h3>Title: Videoshop: Localized Semantic Video Editing with Noise-Extrapolated  Diffusion Inversion</h3>
<ul>
<li><strong>Authors: </strong>Xiang Fan, Anand Bhattad, Ranjay Krishna</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14617">https://arxiv.org/abs/2403.14617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14617">https://arxiv.org/pdf/2403.14617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14617]] Videoshop: Localized Semantic Video Editing with Noise-Extrapolated  Diffusion Inversion(https://arxiv.org/abs/2403.14617)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.</li>
</ul>

<h3>Title: ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D  Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham, Qianyi Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14619">https://arxiv.org/abs/2403.14619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14619">https://arxiv.org/pdf/2403.14619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14619]] ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D  Decomposition(https://arxiv.org/abs/2403.14619)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D decomposition/segmentation still remains a challenge as large-scale 3D annotated data is not readily available. Contemporary approaches typically leverage 2D machine-generated segments, integrating them for 3D consistency. While the majority of these methods are based on NeRFs, they face a potential weakness that the instance/semantic embedding features derive from independent MLPs, thus preventing the segmentation network from learning the geometric details of the objects directly through radiance and density. In this paper, we propose ClusteringSDF, a novel approach to achieve both segmentation and reconstruction in 3D via the neural implicit surface representation, specifically Signal Distance Function (SDF), where the segmentation rendering is directly integrated with the volume rendering of neural implicit surfaces. Although based on ObjectSDF++, ClusteringSDF no longer requires the ground-truth segments for supervision while maintaining the capability of reconstructing individual object surfaces, but purely with the noisy and inconsistent labels from pre-trained models.As the core of ClusteringSDF, we introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D and the experimental results on the challenging scenes from ScanNet and Replica datasets show that ClusteringSDF can achieve competitive performance compared against the state-of-the-art with significantly reduced training time.</li>
</ul>

<h3>Title: GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction  and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14621">https://arxiv.org/abs/2403.14621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14621">https://arxiv.org/pdf/2403.14621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14621]] GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction  and Generation(https://arxiv.org/abs/2403.14621)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: https://justimyhxu.github.io/projects/grm/.</li>
</ul>

<h3>Title: Simplified Diffusion Schrödinger Bridge</h3>
<ul>
<li><strong>Authors: </strong>Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14623">https://arxiv.org/abs/2403.14623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14623">https://arxiv.org/pdf/2403.14623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14623]] Simplified Diffusion Schrödinger Bridge(https://arxiv.org/abs/2403.14623)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.</li>
</ul>

<h3>Title: MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual  Math Problems?</h3>
<ul>
<li><strong>Authors: </strong>Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14624">https://arxiv.org/abs/2403.14624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14624">https://arxiv.org/pdf/2403.14624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14624]] MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual  Math Problems?(https://arxiv.org/abs/2403.14624)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning. In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging True or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark may provide unique insights to guide the future development of MLLMs. Project page: https://mathverse-cuhk.github.io</li>
</ul>

<h3>Title: LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT  Descriptors</h3>
<ul>
<li><strong>Authors: </strong>Saksham Suri, Matthew Walmer, Kamal Gupta, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14625">https://arxiv.org/abs/2403.14625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14625">https://arxiv.org/pdf/2403.14625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14625]] LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT  Descriptors(https://arxiv.org/abs/2403.14625)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present a simple self-supervised method to enhance the performance of ViT features for dense downstream tasks. Our Lightweight Feature Transform (LiFT) is a straightforward and compact postprocessing network that can be applied to enhance the features of any pre-trained ViT backbone. LiFT is fast and easy to train with a self-supervised objective, and it boosts the density of ViT features for minimal extra inference cost. Furthermore, we demonstrate that LiFT can be applied with approaches that use additional task-specific downstream modules, as we integrate LiFT with ViTDet for COCO detection and segmentation. Despite the simplicity of LiFT, we find that it is not simply learning a more complex version of bilinear interpolation. Instead, our LiFT training protocol leads to several desirable emergent properties that benefit ViT features in dense downstream tasks. This includes greater scale invariance for features, and better object boundary maps. By simply training LiFT for a few epochs, we show improved performance on keypoint correspondence, detection, segmentation, and object discovery tasks. Overall, LiFT provides an easy way to unlock the benefits of denser feature arrays for a fraction of the computational cost. For more details, refer to our project page at https://www.cs.umd.edu/~sakshams/LiFT/.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
