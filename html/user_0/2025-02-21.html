<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-21</h1>
<h3>Title: Smaller But Better: Unifying Layout Generation with Smaller Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Peirong Zhang, Jiaxin Zhang, Jiahuan Cao, Hongliang Li, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14005">https://arxiv.org/abs/2502.14005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14005">https://arxiv.org/pdf/2502.14005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14005]] Smaller But Better: Unifying Layout Generation with Smaller Large Language Models(https://arxiv.org/abs/2502.14005)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose LGGPT, an LLM-based model tailored for unified layout generation. First, we propose Arbitrary Layout Instruction (ALI) and Universal Layout Response (ULR) as the uniform I/O template. ALI accommodates arbitrary layout generation task inputs across multiple layout domains, enabling LGGPT to unify both task-generic and domain-generic layout generation hitherto unexplored. Collectively, ALI and ULR boast a succinct structure that forgoes superfluous tokens typically found in existing HTML-based formats, facilitating efficient instruction tuning and boosting unified generation performance. In addition, we propose an Interval Quantization Encoding (IQE) strategy that compresses ALI into a more condensed structure. IQE precisely preserves valid layout clues while eliminating the less informative placeholders, facilitating LGGPT to capture complex and variable layout generation conditions during the unified training process. Experimental results demonstrate that LGGPT achieves superior or on par performance compared to existing methods. Notably, LGGPT strikes a prominent balance between proficiency and efficiency with a compact 1.5B parameter LLM, which beats prior 7B or 175B models even in the most extensive and challenging unified scenario. Furthermore, we underscore the necessity of employing LLMs for unified layout generation and suggest that 1.5B could be an optimal parameter size by comparing LLMs of varying scales. Code is available at this https URL.</li>
</ul>

<h3>Title: MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Qin, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14008">https://arxiv.org/abs/2502.14008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14008">https://arxiv.org/pdf/2502.14008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14008]] MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures(https://arxiv.org/abs/2502.14008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The remarkable performance of large language models (LLMs) in various language tasks has attracted considerable attention. However, the ever-increasing size of these models presents growing challenges for deployment and inference. Structured pruning, an effective model compression technique, is gaining increasing attention due to its ability to enhance inference efficiency. Nevertheless, most previous optimization-based structured pruning methods sacrifice the uniform structure across layers for greater flexibility to maintain performance. The heterogeneous structure hinders the effective utilization of off-the-shelf inference acceleration techniques and impedes efficient configuration for continued training. To address this issue, we propose a novel masking learning paradigm based on minimax optimization to obtain the uniform pruned structure by optimizing the masks under sparsity regularization. Extensive experimental results demonstrate that our method can maintain high performance while ensuring the uniformity of the pruned model structure, thereby outperforming existing SOTA methods.</li>
</ul>

<h3>Title: Which Attention Heads Matter for In-Context Learning?</h3>
<ul>
<li><strong>Authors: </strong>Kayo Yin, Jacob Steinhardt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14010">https://arxiv.org/abs/2502.14010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14010">https://arxiv.org/pdf/2502.14010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14010]] Which Attention Heads Matter for In-Context Learning?(https://arxiv.org/abs/2502.14010)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit impressive in-context learning (ICL) capability, enabling them to perform new tasks using only a few demonstrations in the prompt. Two different mechanisms have been proposed to explain ICL: induction heads that find and copy relevant tokens, and function vector (FV) heads whose activations compute a latent encoding of the ICL task. To better understand which of the two distinct mechanisms drives ICL, we study and compare induction heads and FV heads in 12 language models. Through detailed ablations, we discover that few-shot ICL performance depends primarily on FV heads, especially in larger models. In addition, we uncover that FV and induction heads are connected: many FV heads start as induction heads during training before transitioning to the FV mechanism. This leads us to speculate that induction facilitates learning the more complex FV mechanism that ultimately drives ICL.</li>
</ul>

<h3>Title: Cyber security of OT networks: A tutorial and overview</h3>
<ul>
<li><strong>Authors: </strong>Sumit Kumar, Harsh Vardhan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14017">https://arxiv.org/abs/2502.14017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14017">https://arxiv.org/pdf/2502.14017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14017]] Cyber security of OT networks: A tutorial and overview(https://arxiv.org/abs/2502.14017)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>This manuscript explores the cybersecurity challenges of Operational Technology (OT) networks, focusing on their critical role in industrial environments such as manufacturing, energy, and utilities. As OT systems increasingly integrate with Information Technology (IT) systems due to Industry 4.0 initiatives, they become more vulnerable to cyberattacks, which pose risks not only to data but also to physical infrastructure. The study examines key components of OT systems, such as SCADA (Supervisory Control and Data Acquisition), PLCs (Programmable Logic Controllers), and RTUs (Remote Terminal Units), and analyzes recent cyberattacks targeting OT environments. Furthermore, it highlights the security concerns arising from the convergence of IT and OT systems, examining attack vectors and the growing threats posed by malware, ransomware, and nation-state actors. Finally, the paper discusses modern approaches and tools used to secure these environments, providing insights into improving the cybersecurity posture of OT networks.</li>
</ul>

<h3>Title: Dynamic Activation with Knowledge Distillation for Energy-Efficient Spiking NN Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Orestis Konstantaropoulos, Theodoris Mallios, Maria Papadopouli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14023">https://arxiv.org/abs/2502.14023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14023">https://arxiv.org/pdf/2502.14023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14023]] Dynamic Activation with Knowledge Distillation for Energy-Efficient Spiking NN Ensembles(https://arxiv.org/abs/2502.14023)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While foundation AI models excel at tasks like classification and decision-making, their high energy consumption makes them unsuitable for energy-constrained applications. Inspired by the brain's efficiency, spiking neural networks (SNNs) have emerged as a viable alternative due to their event-driven nature and compatibility with neuromorphic chips. This work introduces a novel system that combines knowledge distillation and ensemble learning to bridge the performance gap between artificial neural networks (ANNs) and SNNs. A foundation AI model acts as a teacher network, guiding smaller student SNNs organized into an ensemble, called Spiking Neural Ensemble (SNE). SNE enables the disentanglement of the teacher's knowledge, allowing each student to specialize in predicting a distinct aspect of it, while processing the same input. The core innovation of SNE is the adaptive activation of a subset of SNN models of an ensemble, leveraging knowledge-distillation, enhanced with an informed-partitioning (disentanglement) of the teacher's feature space. By dynamically activating only a subset of these student SNNs, the system balances accuracy and energy efficiency, achieving substantial energy savings with minimal accuracy loss. Moreover, SNE is significantly more efficient than the teacher network, reducing computational requirements by up to 20x with only a 2% drop in accuracy on the CIFAR-10 dataset. This disentanglement procedure achieves an accuracy improvement of up to 2.4% on the CIFAR-10 dataset compared to other partitioning schemes. Finally, we comparatively analyze SNE performance under noisy conditions, demonstrating enhanced robustness compared to its ANN teacher. In summary, SNE offers a promising new direction for energy-constrained applications.</li>
</ul>

<h3>Title: DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Giorgio Franceschelli, Mirco Musolesi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14037">https://arxiv.org/abs/2502.14037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14037">https://arxiv.org/pdf/2502.14037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14037]] DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation(https://arxiv.org/abs/2502.14037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their increasing performance, large language models still tend to reproduce training data, generate several repetitions, and focus on the most common grammatical structures and words. A possible cause is the decoding strategy adopted: the most common ones either consider only the most probable tokens, reducing output diversity, or increase the likelihood of unlikely tokens at the cost of output accuracy and correctness. In this paper, we propose a family of three new decoding methods by leveraging a mathematical analysis of the token probability distribution. In particular, the difference between consecutive, sorted probabilities can be used to avoid incorrect tokens and increase the chance of low-probable but accurate words. Experiments concerning math problem solving, extreme summarization, and the divergent association task show that our approach consistently performs at least as well as current alternatives in terms of quality and diversity.</li>
</ul>

<h3>Title: Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Shi, Quanzheng Li, Jin Sun, Xiang Li, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14044">https://arxiv.org/abs/2502.14044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14044">https://arxiv.org/pdf/2502.14044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14044]] Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data(https://arxiv.org/abs/2502.14044)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Large multimodal models (LMMs) have shown impressive capabilities in a wide range of visual tasks. However, they often struggle with fine-grained visual reasoning, failing to identify domain-specific objectives and provide justifiable explanations for their predictions. To address this, we propose a novel visual rejection sampling framework to improve the cognition and explainability of LMMs using self-synthesized data. Specifically, visual fine-tuning requires images, queries, and target answers. Our approach begins by synthesizing interpretable answers that include human-verifiable visual features. These features are based on expert-defined concepts, carefully selected based on their alignment with the image content. After each round of fine-tuning, we apply a reward model-free filtering mechanism to select the highest-quality interpretable answers for the next round of tuning. This iterative process of data synthesis and fine-tuning progressively improves the model's ability to generate accurate and reasonable explanations. Experimental results demonstrate the effectiveness of our method in improving both the accuracy and explainability of specialized visual classification tasks.</li>
</ul>

<h3>Title: Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Xianjun Yang, Shaoliang Nie, Lijuan Liu, Suchin Gururangan, Ujjwal Karn, Rui Hou, Madian Khabsa, Yuning Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14050">https://arxiv.org/abs/2502.14050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14050">https://arxiv.org/pdf/2502.14050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14050]] Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder(https://arxiv.org/abs/2502.14050)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Current pre-trained large language models typically need instruction tuning to align with human preferences. However, instruction tuning data is often quantity-saturated due to the large volume of data collection and fast model iteration, leaving coreset data selection important but underexplored. On the other hand, existing quality-driven data selection methods such as LIMA (NeurIPS 2023 (Zhou et al., 2024)) and AlpaGasus (ICLR 2024 (Chen et al.)) generally ignore the equal importance of data diversity and complexity. In this work, we aim to design a diversity-aware data selection strategy and creatively propose using sparse autoencoders to tackle the challenge of data diversity measure. In addition, sparse autoencoders can also provide more interpretability of model behavior and explain, e.g., the surprising effectiveness of selecting the longest response (ICML 2024 (Zhao et al.)). Using effective data selection, we experimentally prove that models trained on our selected data can outperform other methods in terms of model capabilities, reduce training cost, and potentially gain more control over model behaviors.</li>
</ul>

<h3>Title: RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression</h3>
<ul>
<li><strong>Authors: </strong>Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14051">https://arxiv.org/abs/2502.14051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14051">https://arxiv.org/pdf/2502.14051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14051]] RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression(https://arxiv.org/abs/2502.14051)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based Large Language Models rely critically on KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy designed specifically to reduce both memory bandwidth and capacity demand of KV cache during the decode phase. RocketKV contains two consecutive stages. In the first stage, it performs coarse-grain KV cache eviction on the input sequence tokens with SnapKV++, a method improved upon SnapKV by introducing adaptive pooling size and full compatibility with grouped-query attention. In the second stage, it adopts a hybrid attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensional reductions. Combining these two stages, RocketKV achieves significant KV cache fetching bandwidth and storage savings while maintaining comparable accuracy to full KV cache attention. We show that RocketKV provides end-to-end speedup by up to 3$\times$ as well as peak memory reduction by up to 31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks.</li>
</ul>

<h3>Title: EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Fang, Thomas PÃ¶llabauer, Tristan Wirth, Sarah Berkei, Volker Knauthe, Arjan Kuijper</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14061">https://arxiv.org/abs/2502.14061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14061">https://arxiv.org/pdf/2502.14061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14061]] EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation(https://arxiv.org/abs/2502.14061)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In industrial applications requiring real-time feedback, such as quality control and robotic manipulation, the demand for high-speed and accurate pose estimation remains critical. Despite advances improving speed and accuracy in pose estimation, finding a balance between computational efficiency and accuracy poses significant challenges in dynamic environments. Most current algorithms lack scalability in estimation time, especially for diverse datasets, and the state-of-the-art (SOTA) methods are often too slow. This study focuses on developing a fast and scalable set of pose estimators based on GDRNPP to meet or exceed current benchmarks in accuracy and robustness, particularly addressing the efficiency-accuracy trade-off essential in real-time scenarios. We propose the AMIS algorithm to tailor the utilized model according to an application-specific trade-off between inference time and accuracy. We further show the effectiveness of the AMIS-based model choice on four prominent benchmark datasets (LM-O, YCB-V, T-LESS, and ITODD).</li>
</ul>

<h3>Title: PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhao, Zeyu Zhang, Yi Xu, Yi Yao, Yan Huang, Wenxin Zhang, Zirui Song, Xiuying Chen, Yang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14063">https://arxiv.org/abs/2502.14063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14063">https://arxiv.org/pdf/2502.14063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14063]] PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection(https://arxiv.org/abs/2502.14063)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Pedestrian detection in intelligent transportation systems has made significant progress but faces two critical challenges: (1) insufficient fusion of complementary information between visible and infrared spectra, particularly in complex scenarios, and (2) sensitivity to illumination changes, such as low-light or overexposed conditions, leading to degraded performance. To address these issues, we propose PedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection. PedDet introduces the Multi-scale Spectral Feature Perception Module (MSFPM) to adaptively fuse visible and infrared features, enhancing robustness and flexibility in feature extraction. Additionally, the Illumination Robustness Feature Decoupling Module (IRFDM) improves detection stability under varying lighting by decoupling pedestrian and background features. We further design a contrastive alignment to enhance intermodal feature discrimination. Experiments on LLVIP and MSDS datasets demonstrate that PedDet achieves state-of-the-art performance, improving the mAP by 6.6% with superior detection accuracy even in low-light conditions, marking a significant step forward for road safety. Code will be available at this https URL.</li>
</ul>

<h3>Title: Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging</h3>
<ul>
<li><strong>Authors: </strong>Shansong Wang, Mojtaba Safari, Qiang Li, Chih-Wei Chang, Richard LJ Qiu, Justin Roper, David S. Yu, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14064">https://arxiv.org/abs/2502.14064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14064">https://arxiv.org/pdf/2502.14064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14064]] Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging(https://arxiv.org/abs/2502.14064)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Vision foundation models (VFMs) are pre-trained on extensive image datasets to learn general representations for diverse types of data. These models can subsequently be fine-tuned for specific downstream tasks, significantly boosting performance across a broad range of applications. However, existing vision foundation models that claim to be applicable to various radiology tasks are mostly pre-trained on 3D computed tomography (CT), which benefits from the availability of extensive 3D CT databases. Significant differences between CT and magnetic resonance imaging (MRI) in imaging principles, signal characteristics, and data distribution may hinder their practical performance and versatility in MRI-specific applications. Here, we propose Triad, a vision foundation model for 3D MRI. Triad adopts a widely used autoencoder architecture to learn robust representations from 131,170 3D MRI volumes and uses organ-independent imaging descriptions to constrain the semantic distribution of the visual modality. The above pre-training dataset is called Triad-131K, which is currently the largest 3D MRI pre-training dataset. We evaluate Triad across three tasks, namely, organ/tumor segmentation, organ/cancer classification, and medical image registration, in two data modalities (within-domain and out-of-domain) settings using 25 downstream datasets. By initializing models with Triad's pre-trained weights, nnUNet-Triad improves segmentation performance by 6.88% compared to nnUNet-Scratch across 17 datasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch in classification tasks across five datasets. SwinUNETR-Triad improves by 4.00% compared to SwinUNETR-Scratch in registration tasks across two datasets. Our study demonstrates that pre-training can maximize performance when the data modalities and organs of upstream and downstream tasks are consistent.</li>
</ul>

<h3>Title: A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing</h3>
<ul>
<li><strong>Authors: </strong>Shreya Ghosh, Yi-Huan Chen, Ching-Hsiang Huang, Abu Shafin Mohammad Mahdee Jameel, Chien Chou Ho, Aly El Gamal, Samuel Labi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14068">https://arxiv.org/abs/2502.14068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14068">https://arxiv.org/pdf/2502.14068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14068]] A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing(https://arxiv.org/abs/2502.14068)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack addresses common problems such as blurriness due to high speed, color inversion from the camera, and absence of lane markings on the track. Consequently, we propose RaceGAN, a baseline model based on a Generative Adversarial Network (GAN) that effectively addresses these challenges. The proposed model demonstrates superior performance compared to current state-of-the-art machine learning models in track detection. The dataset and code for this work are available at this http URL.</li>
</ul>

<h3>Title: DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Daewon Chae, June Suk Choi, Jinkyu Kim, Kimin Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14070">https://arxiv.org/abs/2502.14070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14070">https://arxiv.org/pdf/2502.14070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14070]] DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image Diffusion Models(https://arxiv.org/abs/2502.14070)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Fine-tuning text-to-image diffusion models to maximize rewards has proven effective for enhancing model performance. However, reward fine-tuning methods often suffer from slow convergence due to online sample generation. Therefore, obtaining diverse samples with strong reward signals is crucial for improving sample efficiency and overall performance. In this work, we introduce DiffExp, a simple yet effective exploration strategy for reward fine-tuning of text-to-image models. Our approach employs two key strategies: (a) dynamically adjusting the scale of classifier-free guidance to enhance sample diversity, and (b) randomly weighting phrases of the text prompt to exploit high-quality reward signals. We demonstrate that these strategies significantly enhance exploration during online sample generation, improving the sample efficiency of recent reward fine-tuning methods, such as DDPO and AlignProp.</li>
</ul>

<h3>Title: Towards Vector Optimization on Low-Dimensional Vector Symbolic Architecture</h3>
<ul>
<li><strong>Authors: </strong>Shijin Duan, Yejia Liu, Gaowen Liu, Ramana Rao Kompella, Shaolei Ren, Xiaolin Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14075">https://arxiv.org/abs/2502.14075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14075">https://arxiv.org/pdf/2502.14075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14075]] Towards Vector Optimization on Low-Dimensional Vector Symbolic Architecture(https://arxiv.org/abs/2502.14075)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Vector Symbolic Architecture (VSA) is emerging in machine learning due to its efficiency, but they are hindered by issues of hyperdimensionality and accuracy. As a promising mitigation, the Low-Dimensional Computing (LDC) method significantly reduces the vector dimension by ~100 times while maintaining accuracy, by employing a gradient-based optimization. Despite its potential, LDC optimization for VSA is still underexplored. Our investigation into vector updates underscores the importance of stable, adaptive dynamics in LDC training. We also reveal the overlooked yet critical roles of batch normalization (BN) and knowledge distillation (KD) in standard approaches. Besides the accuracy boost, BN does not add computational overhead during inference, and KD significantly enhances inference confidence. Through extensive experiments and ablation studies across multiple benchmarks, we provide a thorough evaluation of our approach and extend the interpretability of binary neural network optimization similar to LDC, previously unaddressed in BNN literature.</li>
</ul>

<h3>Title: Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral</h3>
<ul>
<li><strong>Authors: </strong>Shivani Kumar, David Jurgens</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14083">https://arxiv.org/abs/2502.14083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14083">https://arxiv.org/pdf/2502.14083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14083]] Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral(https://arxiv.org/abs/2502.14083)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Moral reasoning is a complex cognitive process shaped by individual experiences and cultural contexts and presents unique challenges for computational analysis. While natural language processing (NLP) offers promising tools for studying this phenomenon, current research lacks cohesion, employing discordant datasets and tasks that examine isolated aspects of moral reasoning. We bridge this gap with UniMoral, a unified dataset integrating psychologically grounded and social-media-derived moral dilemmas annotated with labels for action choices, ethical principles, contributing factors, and consequences, alongside annotators' moral and cultural profiles. Recognizing the cultural relativity of moral reasoning, UniMoral spans six languages, Arabic, Chinese, English, Hindi, Russian, and Spanish, capturing diverse socio-cultural contexts. We demonstrate UniMoral's utility through a benchmark evaluations of three large language models (LLMs) across four tasks: action prediction, moral typology classification, factor attribution analysis, and consequence generation. Key findings reveal that while implicitly embedded moral contexts enhance the moral reasoning capability of LLMs, there remains a critical need for increasingly specialized approaches to further advance moral reasoning in these models.</li>
</ul>

<h3>Title: Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Cole Gawin, Yidan Sun, Mayank Kejriwal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14086">https://arxiv.org/abs/2502.14086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14086">https://arxiv.org/pdf/2502.14086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14086]] Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning(https://arxiv.org/abs/2502.14086)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance in generating human-like text and solving reasoning tasks of moderate complexity, such as question-answering and mathematical problem-solving. However, their capabilities in tasks requiring deeper cognitive skills, such as common-sense understanding and abstract reasoning, remain under-explored. In this paper, we systematically evaluate abstract common-sense reasoning in LLMs using the ConceptNet knowledge graph. We propose two prompting approaches: instruct prompting, where models predict plausible semantic relationships based on provided definitions, and few-shot prompting, where models identify relations using examples as guidance. Our experiments with the gpt-4o-mini model show that in instruct prompting, consistent performance is obtained when ranking multiple relations but with substantial decline when the model is restricted to predicting only one relation. In few-shot prompting, the model's accuracy improves significantly when selecting from five relations rather than the full set, although with notable bias toward certain relations. These results suggest significant gaps still, even in commercially used LLMs' abstract common-sense reasoning abilities, compared to human-level understanding. However, the findings also highlight the promise of careful prompt engineering, based on selective retrieval, for obtaining better performance.</li>
</ul>

<h3>Title: Learning from End User Data with Shuffled Differential Privacy over Kernel Densities</h3>
<ul>
<li><strong>Authors: </strong>Tal Wagner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14087">https://arxiv.org/abs/2502.14087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14087">https://arxiv.org/pdf/2502.14087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14087]] Learning from End User Data with Shuffled Differential Privacy over Kernel Densities(https://arxiv.org/abs/2502.14087)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>We study a setting of collecting and learning from private data distributed across end users. In the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. This model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy. Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. We use it to privately learn a classifier from the end user data, by learning a private density function per class. Moreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. Our experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP.</li>
</ul>

<h3>Title: Regression in EO: Are VLMs Up to the Challenge?</h3>
<ul>
<li><strong>Authors: </strong>Xizhe Xue, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14088">https://arxiv.org/abs/2502.14088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14088">https://arxiv.org/pdf/2502.14088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14088]] Regression in EO: Are VLMs Up to the Challenge?(https://arxiv.org/abs/2502.14088)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Earth Observation (EO) data encompass a vast range of remotely sensed information, featuring multi-sensor and multi-temporal, playing an indispensable role in understanding our planet's dynamics. Recently, Vision Language Models (VLMs) have achieved remarkable success in perception and reasoning tasks, bringing new insights and opportunities to the EO field. However, the potential for EO applications, especially for scientific regression related applications remains largely unexplored. This paper bridges that gap by systematically examining the challenges and opportunities of adapting VLMs for EO regression tasks. The discussion first contrasts the distinctive properties of EO data with conventional computer vision datasets, then identifies four core obstacles in applying VLMs to EO regression: 1) the absence of dedicated benchmarks, 2) the discrete-versus-continuous representation mismatch, 3) cumulative error accumulation, and 4) the suboptimal nature of text-centric training objectives for numerical tasks. Next, a series of methodological insights and potential subtle pitfalls are explored. Lastly, we offer some promising future directions for designing robust, domain-aware solutions. Our findings highlight the promise of VLMs for scientific regression in EO, setting the stage for more precise and interpretable modeling of critical environmental processes.</li>
</ul>

<h3>Title: CND-IDS: Continual Novelty Detection for Intrusion Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Sean Fuhrman, Onat Gungor, Tajana Rosing</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14094">https://arxiv.org/abs/2502.14094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14094">https://arxiv.org/pdf/2502.14094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14094]] CND-IDS: Continual Novelty Detection for Intrusion Detection Systems(https://arxiv.org/abs/2502.14094)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Intrusion detection systems (IDS) play a crucial role in IoT and network security by monitoring system data and alerting to suspicious activities. Machine learning (ML) has emerged as a promising solution for IDS, offering highly accurate intrusion detection. However, ML-IDS solutions often overlook two critical aspects needed to build reliable systems: continually changing data streams and a lack of attack labels. Streaming network traffic and associated cyber attacks are continually changing, which can degrade the performance of deployed ML models. Labeling attack data, such as zero-day attacks, in real-world intrusion scenarios may not be feasible, making the use of ML solutions that do not rely on attack labels necessary. To address both these challenges, we propose CND-IDS, a continual novelty detection IDS framework which consists of (i) a learning-based feature extractor that continuously updates new feature representations of the system data, and (ii) a novelty detector that identifies new cyber attacks by leveraging principal component analysis (PCA) reconstruction. Our results on realistic intrusion datasets show that CND-IDS achieves up to 6.1x F-score improvement, and up to 6.5x improved forward transfer over the SOTA unsupervised continual learning algorithm. Our code will be released upon acceptance.</li>
</ul>

<h3>Title: Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Karl Elbakian, Samuel Carton</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14095">https://arxiv.org/abs/2502.14095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14095">https://arxiv.org/pdf/2502.14095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14095]] Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning(https://arxiv.org/abs/2502.14095)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A key aspect of alignment is the proper use of within-document evidence to construct document-level decisions. We analyze the relationship between the retrieval and interpretation of within-document evidence for large language model in a few-shot setting. Specifically, we measure the extent to which model prediction errors are associated with evidence retrieval errors with respect to gold-standard human-annotated extractive evidence for five datasets, using two popular closed proprietary models. We perform two ablation studies to investigate when both label prediction and evidence retrieval errors can be attributed to qualities of the relevant evidence. We find that there is a strong empirical relationship between model prediction and evidence retrieval error, but that evidence retrieval error is mostly not associated with evidence interpretation error--a hopeful sign for downstream applications built on this mechanism.</li>
</ul>

<h3>Title: Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach</h3>
<ul>
<li><strong>Authors: </strong>Shenglai Zeng, Pengfei He, Kai Guo, Tianqi Zheng, Hanqing Lu, Yue Xing, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14100">https://arxiv.org/abs/2502.14100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14100">https://arxiv.org/pdf/2502.14100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14100]] Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach(https://arxiv.org/abs/2502.14100)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) enhanced with external contexts, such as through retrieval-augmented generation (RAG), often face challenges in handling imperfect evidence. They tend to over-rely on external knowledge, making them vulnerable to misleading and unhelpful contexts. To address this, we propose the concept of context-robust LLMs, which can effectively balance internal knowledge with external context, similar to human cognitive processes. Specifically, context-robust LLMs should rely on external context only when lacking internal knowledge, identify contradictions between internal and external knowledge, and disregard unhelpful contexts. To achieve this goal, we introduce Grft, a lightweight and plug-and-play gated representation fine-tuning approach. Grft consists of two key components: a gating mechanism to detect and filter problematic inputs, and low-rank representation adapters to adjust hidden representations. By training a lightweight intervention function with only 0.0004\% of model size on fewer than 200 examples, Grft can effectively adapt LLMs towards context-robust behaviors.</li>
</ul>

<h3>Title: SALTY: Explainable Artificial Intelligence Guided Structural Analysis for Hardware Trojan Detection</h3>
<ul>
<li><strong>Authors: </strong>Tanzim Mahfuz, Pravin Gaikwad, Tasneem Suha, Swarup Bhunia, Prabuddha Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14116">https://arxiv.org/abs/2502.14116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14116">https://arxiv.org/pdf/2502.14116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14116]] SALTY: Explainable Artificial Intelligence Guided Structural Analysis for Hardware Trojan Detection(https://arxiv.org/abs/2502.14116)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Hardware Trojans are malicious modifications in digital designs that can be inserted by untrusted supply chain entities. Hardware Trojans can give rise to diverse attack vectors such as information leakage (e.g. MOLES Trojan) and denial-of-service (rarely triggered bit flip). Such an attack in critical systems (e.g. healthcare and aviation) can endanger human lives and lead to catastrophic financial loss. Several techniques have been developed to detect such malicious modifications in digital designs, particularly for designs sourced from third-party intellectual property (IP) vendors. However, most techniques have scalability concerns (due to unsound assumptions during evaluation) and lead to large number of false positive detections (false alerts). Our framework (SALTY) mitigates these concerns through the use of a novel Graph Neural Network architecture (using Jumping-Knowledge mechanism) for generating initial predictions and an Explainable Artificial Intelligence (XAI) approach for fine tuning the outcomes (post-processing). Experiments show 98% True Positive Rate (TPR) and True Negative Rate (TNR), significantly outperforming state-of-the-art techniques across a large set of standard benchmarks.</li>
</ul>

<h3>Title: A Supervised Machine-Learning Approach For Turboshaft Engine Dynamic Modeling Under Real Flight Conditions</h3>
<ul>
<li><strong>Authors: </strong>Damiano Paniccia, Francesco Aldo Tucci, Joel Guerrero, Luigi Capone, Nicoletta Sanguini, Tommaso Benacchio, Luigi Bottasso</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14120">https://arxiv.org/abs/2502.14120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14120">https://arxiv.org/pdf/2502.14120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14120]] A Supervised Machine-Learning Approach For Turboshaft Engine Dynamic Modeling Under Real Flight Conditions(https://arxiv.org/abs/2502.14120)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Rotorcraft engines are highly complex, nonlinear thermodynamic systems that operate under varying environmental and flight conditions. Simulating their dynamics is crucial for design, fault diagnostics, and deterioration control phases, and requires robust and reliable control systems to estimate engine performance throughout flight envelope. However, the development of detailed physical models of the engine based on numerical simulations is a very challenging task due to the complex and entangled physics driving the engine. In this scenario, data-driven machine-learning techniques are of great interest to the aircraft engine community, due to their ability to describe nonlinear systems' dynamic behavior and enable online performance estimation, achieving excellent results with accuracy competitive with the state of the art. In this work, we explore different Neural Network architectures to model the turboshaft engine of Leonardo's AW189P4 prototype, aiming to predict the engine torque. The models are trained on an extensive database of real flight tests featuring a variety of operational maneuvers performed under different flight conditions, providing a comprehensive representation of the engine's performance. To complement the neural network approach, we apply Sparse Identification of Nonlinear Dynamics (SINDy) to derive a low-dimensional dynamical model from the available data, describing the relationship between fuel flow and engine torque. The resulting model showcases SINDy's capability to recover the actual physics underlying the engine dynamics and demonstrates its potential for investigating more complex aspects of the engine. The results prove that data-driven engine models can exploit a wider range of parameters than standard transfer function-based approaches, enabling the use of trained schemes to simulate nonlinear effects in different engines and helicopters.</li>
</ul>

<h3>Title: Benchmarking LLMs for Political Science: A United Nations Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yueqing Liang, Liangwei Yang, Chen Wang, Congying Xia, Rui Meng, Xiongxiao Xu, Haoran Wang, Ali Payani, Kai Shu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14122">https://arxiv.org/abs/2502.14122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14122">https://arxiv.org/pdf/2502.14122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14122]] Benchmarking LLMs for Political Science: A United Nations Perspective(https://arxiv.org/abs/2502.14122)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved significant advances in natural language processing, yet their potential for high-stake political decision-making remains largely unexplored. This paper addresses the gap by focusing on the application of LLMs to the United Nations (UN) decision-making process, where the stakes are particularly high and political decisions can have far-reaching consequences. We introduce a novel dataset comprising publicly available UN Security Council (UNSC) records from 1994 to 2024, including draft resolutions, voting records, and diplomatic speeches. Using this dataset, we propose the United Nations Benchmark (UNBench), the first comprehensive benchmark designed to evaluate LLMs across four interconnected political science tasks: co-penholder judgment, representative voting simulation, draft adoption prediction, and representative statement generation. These tasks span the three stages of the UN decision-making process--drafting, voting, and discussing--and aim to assess LLMs' ability to understand and simulate political dynamics. Our experimental analysis demonstrates the potential and challenges of applying LLMs in this domain, providing insights into their strengths and limitations in political science. This work contributes to the growing intersection of AI and political science, opening new avenues for research and practical applications in global governance. The UNBench Repository can be accessed at: this https URL.</li>
</ul>

<h3>Title: Understanding SGD with Exponential Moving Average: A Case Study in Linear Regression</h3>
<ul>
<li><strong>Authors: </strong>Xuheng Li, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14123">https://arxiv.org/abs/2502.14123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14123">https://arxiv.org/pdf/2502.14123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14123]] Understanding SGD with Exponential Moving Average: A Case Study in Linear Regression(https://arxiv.org/abs/2502.14123)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Exponential moving average (EMA) has recently gained significant popularity in training modern deep learning models, especially diffusion-based generative models. However, there have been few theoretical results explaining the effectiveness of EMA. In this paper, to better understand EMA, we establish the risk bound of online SGD with EMA for high-dimensional linear regression, one of the simplest overparameterized learning tasks that shares similarities with neural networks. Our results indicate that (i) the variance error of SGD with EMA is always smaller than that of SGD without averaging, and (ii) unlike SGD with iterate averaging from the beginning, the bias error of SGD with EMA decays exponentially in every eigen-subspace of the data covariance matrix. Additionally, we develop proof techniques applicable to the analysis of a broad class of averaging schemes.</li>
</ul>

<h3>Title: Modular Prompt Learning Improves Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenhan Huang, Tejaswini Pedapati, Pin-Yu Chen, Jianxi Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14125">https://arxiv.org/abs/2502.14125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14125">https://arxiv.org/pdf/2502.14125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14125]] Modular Prompt Learning Improves Vision-Language Models(https://arxiv.org/abs/2502.14125)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pre-trained vision-language models are able to interpret visual concepts and language semantics. Prompt learning, a method of constructing prompts for text encoders or image encoders, elicits the potentials of pre-trained models and readily adapts them to new scenarios. Compared to fine-tuning, prompt learning enables the model to achieve comparable or better performance using fewer trainable parameters. Besides, prompt learning freezes the pre-trained model and avoids the catastrophic forgetting issue in the fine-tuning. Continuous prompts inserted into the input of every transformer layer (i.e. deep prompts) can improve the performances of pre-trained models on downstream tasks. For i-th transformer layer, the inserted prompts replace previously inserted prompts in the $(i-1)$-th layer. Although the self-attention mechanism contextualizes newly inserted prompts for the current layer and embeddings from the previous layer's output, removing all inserted prompts from the previous layer inevitably loses information contained in the continuous prompts. In this work, we propose Modular Prompt Learning (MPL) that is designed to promote the preservation of information contained in the inserted prompts. We evaluate the proposed method on base-to-new generalization and cross-dataset tasks. On average of 11 datasets, our method achieves 0.7% performance gain on the base-to-new generalization task compared to the state-of-the-art method. The largest improvement on the individual dataset is 10.7% (EuroSAT dataset).</li>
</ul>

<h3>Title: Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above</h3>
<ul>
<li><strong>Authors: </strong>Nishant Balepur, Rachel Rudinger, Jordan Lee Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14127">https://arxiv.org/abs/2502.14127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14127">https://arxiv.org/pdf/2502.14127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14127]] Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above(https://arxiv.org/abs/2502.14127)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Multiple choice question answering (MCQA) is popular for LLM evaluation due to its simplicity and human-like testing, but we argue for its reform. We first reveal flaws in MCQA's format, as it struggles to: 1) test generation/subjectivity; 2) match LLM use cases; and 3) fully test knowledge. We instead advocate for generative formats based on human testing-where LLMs construct and explain answers-better capturing user needs and knowledge while remaining easy to score. We then show even when MCQA is a useful format, its datasets suffer from: leakage; unanswerability; shortcuts; and saturation. In each issue, we give fixes from education, like rubrics to guide MCQ writing; scoring methods to bridle guessing; and Item Response Theory to build harder MCQs. Lastly, we discuss LLM errors in MCQA-robustness, biases, and unfaithful explanations-showing how our prior solutions better measure or address these issues. While we do not need to desert MCQA, we encourage more efforts in refining the task based on educational testing, advancing evaluations.</li>
</ul>

<h3>Title: Self-Regularization with Latent Space Explanations for Controllable LLM-based Classification</h3>
<ul>
<li><strong>Authors: </strong>Xuansheng Wu, Wenhao Yu, Xiaoming Zhai, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14133">https://arxiv.org/abs/2502.14133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14133">https://arxiv.org/pdf/2502.14133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14133]] Self-Regularization with Latent Space Explanations for Controllable LLM-based Classification(https://arxiv.org/abs/2502.14133)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair, large language model</a></li>
<li><strong>Abstract: </strong>Modern text classification methods heavily rely on contextual embeddings from large language models (LLMs). Compared to human-engineered features, these embeddings provide automatic and effective representations for classification model training. However, they also introduce a challenge: we lose the ability to manually remove unintended features, such as sensitive or task-irrelevant features, to guarantee regulatory compliance or improve the generalizability of classification models. This limitation arises because LLM embeddings are opaque and difficult to interpret. In this paper, we propose a novel framework to identify and regularize unintended features in the LLM latent space. Specifically, we first pre-train a sparse autoencoder (SAE) to extract interpretable features from LLM latent spaces. To ensure the SAE can capture task-specific features, we further fine-tune it on task-specific datasets. In training the classification model, we propose a simple and effective regularizer, by minimizing the similarity between the classifier weights and the identified unintended feature, to remove the impacts of these unintended features toward classification. We evaluate the proposed framework on three real-world tasks, including toxic chat detection, reward modeling, and disease diagnosis. Results show that the proposed framework can significantly improve the classifier's generalizability by regularizing those features that are not semantically correlated to each task. This work pioneers controllable text classification on LLM latent spaces by leveraging interpreted features to address generalizability, fairness, and privacy challenges. We will release our code and data once accepted.</li>
</ul>

<h3>Title: ModSkill: Physical Character Skill Modularization</h3>
<ul>
<li><strong>Authors: </strong>Yiming Huang, Zhiyang Dou, Lingjie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14140">https://arxiv.org/abs/2502.14140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14140">https://arxiv.org/pdf/2502.14140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14140]] ModSkill: Physical Character Skill Modularization(https://arxiv.org/abs/2502.14140)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human motion is highly diverse and dynamic, posing challenges for imitation learning algorithms that aim to generalize motor skills for controlling simulated characters. Previous methods typically rely on a universal full-body controller for tracking reference motion (tracking-based model) or a unified full-body skill embedding space (skill embedding). However, these approaches often struggle to generalize and scale to larger motion datasets. In this work, we introduce a novel skill learning framework, ModSkill, that decouples complex full-body skills into compositional, modular skills for independent body parts. Our framework features a skill modularization attention layer that processes policy observations into modular skill embeddings that guide low-level controllers for each body part. We also propose an Active Skill Learning approach with Generative Adaptive Sampling, using large motion generation models to adaptively enhance policy learning in challenging tracking scenarios. Our results show that this modularized skill learning framework, enhanced by generative sampling, outperforms existing methods in precise full-body motion tracking and enables reusable skill embeddings for diverse goal-driven tasks.</li>
</ul>

<h3>Title: Token Adaptation via Side Graph Convolution for Temporally and Spatially Efficient Fine-tuning of 3D Point Cloud Transformers</h3>
<ul>
<li><strong>Authors: </strong>Takahiko Furuya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14142">https://arxiv.org/abs/2502.14142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14142">https://arxiv.org/pdf/2502.14142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14142]] Token Adaptation via Side Graph Convolution for Temporally and Spatially Efficient Fine-tuning of 3D Point Cloud Transformers(https://arxiv.org/abs/2502.14142)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloud Transformers has emerged as a promising technique for 3D point cloud analysis. While existing PEFT methods attempt to minimize the number of tunable parameters, they still suffer from high temporal and spatial computational costs during fine-tuning. This paper proposes a novel PEFT algorithm for 3D point cloud Transformers, called Side Token Adaptation on a neighborhood Graph (STAG), to achieve superior temporal and spatial efficiency. STAG employs a graph convolutional side network that operates in parallel with a frozen backbone Transformer to adapt tokens to downstream tasks. STAG's side network realizes high efficiency through three key components: connection with the backbone that enables reduced gradient computation, parameter sharing framework, and efficient graph convolution. Furthermore, we present Point Cloud Classification 13 (PCC13), a new benchmark comprising diverse publicly available 3D point cloud datasets, enabling comprehensive evaluation of PEFT methods. Extensive experiments using multiple pre-trained models and PCC13 demonstrates the effectiveness of STAG. Specifically, STAG maintains classification accuracy comparable to existing methods while reducing tunable parameters to only 0.43M and achieving significant reductions in both computational time and memory consumption for fine-tuning. Code and benchmark will be available at: this https URL</li>
</ul>

<h3>Title: Deep learning based infrared small object segmentation: Challenges and future directions</h3>
<ul>
<li><strong>Authors: </strong>Zhengeng Yang, Hongshan Yu, Jianjun Zhang, Qiang Tang, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14168">https://arxiv.org/abs/2502.14168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14168">https://arxiv.org/pdf/2502.14168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14168]] Deep learning based infrared small object segmentation: Challenges and future directions(https://arxiv.org/abs/2502.14168)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Infrared sensing is a core method for supporting unmanned systems, such as autonomous vehicles and drones. Recently, infrared sensors have been widely deployed on mobile and stationary platforms for detection and classification of objects from long distances and in wide field of views. Given its success in the vision image analysis domain, deep learning has also been applied for object recognition in infrared images. However, techniques that have proven successful in visible light perception face new challenges in the infrared domain. These challenges include extremely low signal-to-noise ratios in infrared images, very small and blurred objects of interest, and limited availability of labeled/unlabeled training data due to the specialized nature of infrared sensors. Numerous methods have been proposed in the literature for the detection and classification of small objects in infrared images achieving varied levels of success. There is a need for a survey paper that critically analyzes existing techniques in this domain, identifies unsolved challenges and provides future research directions. This paper fills the gap and offers a concise and insightful review of deep learning-based methods. It also identifies the challenges faced by existing infrared object segmentation methods and provides a structured review of existing infrared perception methods from the perspective of these challenges and highlights the motivations behind the various approaches. Finally, this review suggests promising future directions based on recent advancements within this domain.</li>
</ul>

<h3>Title: Blockchain-based Framework for Scalable and Incentivized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Bijun Wu, Oshani Seneviratne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14170">https://arxiv.org/abs/2502.14170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14170">https://arxiv.org/pdf/2502.14170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14170]] Blockchain-based Framework for Scalable and Incentivized Federated Learning(https://arxiv.org/abs/2502.14170)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, fair, large language model</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training without sharing raw data, preserving privacy while harnessing distributed datasets. However, traditional FL systems often rely on centralized aggregating mechanisms, introducing trust issues, single points of failure, and limited mechanisms for incentivizing meaningful client contributions. These challenges are exacerbated as FL scales to train resource-intensive models, such as large language models (LLMs), requiring scalable, decentralized solutions. This paper presents a blockchain-based FL framework that addresses these limitations by integrating smart contracts and a novel hybrid incentive mechanism. The framework automates critical FL tasks, including client registration, update validation, reward distribution, and maintaining a transparent global state. The hybrid incentive mechanism combines on-chain alignment-based rewards, off-chain fairness checks, and consistency multipliers to ensure fairness, transparency, and sustained engagement. We evaluate the framework through gas cost analysis, demonstrating its feasibility for different scales of federated learning scenarios.</li>
</ul>

<h3>Title: Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction</h3>
<ul>
<li><strong>Authors: </strong>Mohammadmahdi Jafari, Devin Yuncheng Hua, Hao Xue, Flora Salim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14171">https://arxiv.org/abs/2502.14171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14171">https://arxiv.org/pdf/2502.14171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14171]] Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction(https://arxiv.org/abs/2502.14171)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.</li>
</ul>

<h3>Title: On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems</h3>
<ul>
<li><strong>Authors: </strong>Shokhrukh Ibragimov, Arnulf Jentzen, Benno Kuckuck</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14180">https://arxiv.org/abs/2502.14180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14180">https://arxiv.org/pdf/2502.14180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14180]] On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems(https://arxiv.org/abs/2502.14180)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions. We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order logic statements in Zermelo-Fraenkel set theory. While the resolution of these questions does not require any knowledge beyond basic notation of first-order logic and set theory, it does require a degree of planning and logical reasoning, which can be controlled up to arbitrarily high difficulty by the complexity of the generated statements. Furthermore, we do extensive evaluations of the performance of various large language models, including recent models such as DeepSeek-R1 and OpenAI's o3-mini, on these datasets. All of the datasets along with the code used for generating them, as well as all data from the evaluations is publicly available at this https URL.</li>
</ul>

<h3>Title: Multi-Faceted Studies on Data Poisoning can Advance LLM Development</h3>
<ul>
<li><strong>Authors: </strong>Pengfei He, Yue Xing, Han Xu, Zhen Xiang, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14182">https://arxiv.org/abs/2502.14182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14182">https://arxiv.org/pdf/2502.14182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14182]] Multi-Faceted Studies on Data Poisoning can Advance LLM Development(https://arxiv.org/abs/2502.14182)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages, diverse data sources, and varied inference methods. While prior research on data poisoning attacks has primarily focused on the safety vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended. Given these challenges, this position paper proposes rethinking the role of data poisoning and argue that multi-faceted studies on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate and address real safety risks to LLMs. From a trustworthiness perspective, data poisoning can be leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs, and hallucinations. Moreover, from a mechanism perspective, data poisoning can provide valuable insights into LLMs, particularly the interplay between data and model behavior, driving a deeper understanding of their underlying mechanisms.</li>
</ul>

<h3>Title: Bayesian SegNet for Semantic Segmentation with Improved Interpretation of Microstructural Evolution During Irradiation of Materials</h3>
<ul>
<li><strong>Authors: </strong>Marjolein Oostrom, Alex Hagen, Nicole LaHaye, Karl Pazdernik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14184">https://arxiv.org/abs/2502.14184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14184">https://arxiv.org/pdf/2502.14184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14184]] Bayesian SegNet for Semantic Segmentation with Improved Interpretation of Microstructural Evolution During Irradiation of Materials(https://arxiv.org/abs/2502.14184)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Understanding the relationship between the evolution of microstructures of irradiated LiAlO2 pellets and tritium diffusion, retention and release could improve predictions of tritium-producing burnable absorber rod performance. Given expert-labeled segmented images of irradiated and unirradiated pellets, we trained Deep Convolutional Neural Networks to segment images into defect, grain, and boundary classes. Qualitative microstructural information was calculated from these segmented images to facilitate the comparison of unirradiated and irradiated pellets. We tested modifications to improve the sensitivity of the model, including incorporating meta-data into the model and utilizing uncertainty quantification. The predicted segmentation was similar to the expert-labeled segmentation for most methods of microstructural qualification, including pixel proportion, defect area, and defect density. Overall, the high performance metrics for the best models for both irradiated and unirradiated images shows that utilizing neural network models is a viable alternative to expert-labeled images.</li>
</ul>

<h3>Title: Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs. Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Fernando Spadea, Oshani Seneviratne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14187">https://arxiv.org/abs/2502.14187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14187">https://arxiv.org/pdf/2502.14187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14187]] Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs. Direct Preference Optimization(https://arxiv.org/abs/2502.14187)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>We evaluate Kahneman-Tversky Optimization (KTO) as a fine-tuning method for large language models (LLMs) in federated learning (FL) settings, comparing it against Direct Preference Optimization (DPO). Using Alpaca-7B as the base model, we fine-tune on a realistic dataset under both methods and evaluate performance using MT-Bench-1, Vicuna, and AdvBench benchmarks. Additionally, we introduce a redistributed dataset setup, where only KTO is applicable due to its ability to handle single-response feedback, unlike DPO's reliance on paired responses. Our results demonstrate that KTO, in both its original (KTOO) and redistributed (KTOR) configurations, consistently outperforms DPO across all benchmarks. In the redistributed setup, KTO further validates its flexibility and resilience by maintaining superior performance in scenarios where DPO cannot be applied. These findings establish KTO as a robust and scalable fine-tuning method for FL, motivating its adoption for privacy-preserving, decentralized, and heterogeneous environments.</li>
</ul>

<h3>Title: QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification</h3>
<ul>
<li><strong>Authors: </strong>Hajar Sakai, Sarah S. Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14189">https://arxiv.org/abs/2502.14189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14189">https://arxiv.org/pdf/2502.14189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14189]] QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification(https://arxiv.org/abs/2502.14189)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The escalating volume of collected healthcare textual data presents a unique challenge for automated Multi-Label Text Classification (MLTC), which is primarily due to the scarcity of annotated texts for training and their nuanced nature. Traditional machine learning models often fail to fully capture the array of expressed topics. However, Large Language Models (LLMs) have demonstrated remarkable effectiveness across numerous Natural Language Processing (NLP) tasks in various domains, which show impressive computational efficiency and suitability for unsupervised learning through prompt engineering. Consequently, these LLMs promise an effective MLTC of medical narratives. However, when dealing with various labels, different prompts can be relevant depending on the topic. To address these challenges, the proposed approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT, PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and BART provides topics' assignment probabilities, which results in four classifications, all in a 0-shot setting. The outputs are then combined using ensemble learning and processed through a meta-classifier to produce the final MLTC result. The approach is evaluated using three samples of annotated texts, which contrast it with traditional and single-model methods. The results show significant improvements across the majority of the topics in the classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and 80.16% with standard deviations of 0.025 and 0.011, respectively). This research advances MLTC using LLMs and provides an efficient and scalable solution to rapidly categorize healthcare-related text data without further training.</li>
</ul>

<h3>Title: NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM</h3>
<ul>
<li><strong>Authors: </strong>Jiayin Lan, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14192">https://arxiv.org/abs/2502.14192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14192">https://arxiv.org/pdf/2502.14192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14192]] NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM(https://arxiv.org/abs/2502.14192)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely applied in question answering over scientific research papers. To enhance the professionalism and accuracy of responses, many studies employ external knowledge augmentation. However, existing structures of external knowledge in scientific literature often focus solely on either paper entities or domain concepts, neglecting the intrinsic connections between papers through shared domain concepts. This results in less comprehensive and specific answers when addressing questions that combine papers and concepts. To address this, we propose a novel knowledge graph framework that captures deep conceptual relations between academic papers, constructing a relational network via intra-paper semantic elements and inter-paper citation relations. Using a few-shot knowledge graph construction method based on LLM, we develop NLP-AKG, an academic knowledge graph for the NLP domain, by extracting 620,353 entities and 2,271,584 relations from 60,826 papers in ACL Anthology. Based on this, we propose a 'sub-graph community summary' method and validate its effectiveness on three NLP scientific literature question answering datasets.</li>
</ul>

<h3>Title: Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Shang, Zhenyu Li, Pengjie Xu, Jinwei Qiao, Gang Chen, Zihan Ruan, Weijun Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14195">https://arxiv.org/abs/2502.14195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14195">https://arxiv.org/pdf/2502.14195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14195]] Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition(https://arxiv.org/abs/2502.14195)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mobile robots necessitate advanced natural language understanding capabilities to accurately identify locations and perform tasks such as package delivery. However, traditional visual place recognition (VPR) methods rely solely on single-view visual information and cannot interpret human language descriptions. To overcome this challenge, we bridge text and vision by proposing a multiview (360Â° views of the surroundings) text-vision registration approach called Text4VPR for place recognition task, which is the first method that exclusively utilizes textual descriptions to match a database of images. Text4VPR employs the frozen T5 language model to extract global textual embeddings. Additionally, it utilizes the Sinkhorn algorithm with temperature coefficient to assign local tokens to their respective clusters, thereby aggregating visual descriptors from images. During the training stage, Text4VPR emphasizes the alignment between individual text-image pairs for precise textual description. In the inference stage, Text4VPR uses the Cascaded Cross-Attention Cosine Alignment (CCCA) to address the internal mismatch between text and image groups. Subsequently, Text4VPR performs precisely place match based on the descriptions of text-image groups. On Street360Loc, the first text to image VPR dataset we created, Text4VPR builds a robust baseline, achieving a leading top-1 accuracy of 57% and a leading top-10 accuracy of 92% within a 5-meter radius on the test set, which indicates that localization from textual descriptions to images is not only feasible but also holds significant potential for further advancement, as shown in Figure 1.</li>
</ul>

<h3>Title: Adaptive Sparsified Graph Learning Framework for Vessel Behavior Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Jeehong Kim, Minchan Kim, Jaeseong Ju, Youngseok Hwang, Wonhee Lee, Hyunwoo Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14197">https://arxiv.org/abs/2502.14197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14197">https://arxiv.org/pdf/2502.14197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14197]] Adaptive Sparsified Graph Learning Framework for Vessel Behavior Anomalies(https://arxiv.org/abs/2502.14197)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph neural networks have emerged as a powerful tool for learning spatiotemporal interactions. However, conventional approaches often rely on predefined graphs, which may obscure the precise relationships being modeled. Additionally, existing methods typically define nodes based on fixed spatial locations, a strategy that is ill-suited for dynamic environments like maritime environments. Our method introduces an innovative graph representation where timestamps are modeled as distinct nodes, allowing temporal dependencies to be explicitly captured through graph edges. This setup is extended to construct a multi-ship graph that effectively captures spatial interactions while preserving graph sparsity. The graph is processed using Graph Convolutional Network layers to capture spatiotemporal patterns, with a forecasting layer for feature prediction and a Variational Graph Autoencoder for reconstruction, enabling robust anomaly detection.</li>
</ul>

<h3>Title: On-the-fly Preference Alignment via Principle-Guided Decoding</h3>
<ul>
<li><strong>Authors: </strong>Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14204">https://arxiv.org/abs/2502.14204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14204">https://arxiv.org/pdf/2502.14204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14204]] On-the-fly Preference Alignment via Principle-Guided Decoding(https://arxiv.org/abs/2502.14204)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapidly expanding landscape of large language models, aligning model generations with human values and preferences is becoming increasingly important. Popular alignment methods, such as Reinforcement Learning from Human Feedback, have shown significant success in guiding models with greater control. However, these methods require considerable computational resources, which is inefficient, and substantial collection of training data to accommodate the diverse and pluralistic nature of human preferences, which is impractical. These limitations significantly constrain the scope and efficacy of both task-specific and general preference alignment methods. In this work, we introduce On-the-fly Preference Alignment via Principle-Guided Decoding (OPAD) to directly align model outputs with human preferences during inference, eliminating the need for fine-tuning. Our approach involves first curating a surrogate solution to an otherwise infeasible optimization problem and then designing a principle-guided reward function based on this surrogate. The final aligned policy is derived by maximizing this customized reward, which exploits the discrepancy between the constrained policy and its unconstrained counterpart. OPAD directly modifies the model's predictions during inference, ensuring principle adherence without incurring the computational overhead of retraining or fine-tuning. Experiments show that OPAD achieves competitive or superior performance in both general and personalized alignment tasks, demonstrating its efficiency and effectiveness compared to state-of-the-art baselines.</li>
</ul>

<h3>Title: Accurate Forgetting for Heterogeneous Federated Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Abudukelimu Wuerkaixi, Sen Cui, Jingfeng Zhang, Kunda Yan, Bo Han, Gang Niu, Lei Fang, Changshui Zhang, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14205">https://arxiv.org/abs/2502.14205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14205">https://arxiv.org/pdf/2502.14205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14205]] Accurate Forgetting for Heterogeneous Federated Continual Learning(https://arxiv.org/abs/2502.14205)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, generative</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed a burgeoning interest in federated learning (FL). However, the contexts in which clients engage in sequential learning remain under-explored. Bridging FL and continual learning (CL) gives rise to a challenging practical problem: federated continual learning (FCL). Existing research in FCL primarily focuses on mitigating the catastrophic forgetting issue of continual learning while collaborating with other clients. We argue that the forgetting phenomena are not invariably detrimental. In this paper, we consider a more practical and challenging FCL setting characterized by potentially unrelated or even antagonistic data/tasks across different clients. In the FL scenario, statistical heterogeneity and data noise among clients may exhibit spurious correlations which result in biased feature learning. While existing CL strategies focus on a complete utilization of previous knowledge, we found that forgetting biased information is beneficial in our study. Therefore, we propose a new concept accurate forgetting (AF) and develop a novel generative-replay method~\method~which selectively utilizes previous knowledge in federated networks. We employ a probabilistic framework based on a normalizing flow model to quantify the credibility of previous knowledge. Comprehensive experiments affirm the superiority of our method over baselines.</li>
</ul>

<h3>Title: Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language Models via Dual-Stage Prompts Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Chang, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14211">https://arxiv.org/abs/2502.14211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14211">https://arxiv.org/pdf/2502.14211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14211]] Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language Models via Dual-Stage Prompts Optimization(https://arxiv.org/abs/2502.14211)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) face significant challenges when balancing multiple high-level objectives, such as generating coherent, relevant, and high-quality responses while maintaining efficient task adaptation across diverse tasks. To address these challenges, we introduce Transfer-Prompting, a novel two-stage framework designed to enhance cross-task adaptation in prompt generation. The framework comprises two key components: (1) source prompt construction, which refines the original prompts on source task datasets to generate source prompts with enhanced generalization ability, and (2) target prompt generation, which enhances cross-task adaptation of target prompts by fine-tuning a set of high-scored source prompts on task-specific datasets. In each optimization cycle, a reference LLM generates candidate prompts based on historical prompt-score pairs and task descriptions in our designed reference prompt. These candidate prompts are refined iteratively, while a scorer LLM evaluates their effectiveness using the multi-dimensional metrics designed in the objective prompts evaluator-a novel contribution in this work that provides a holistic evaluation of prompt quality and task performance. This feedback loop facilitates continuous refinement, optimizing both prompt quality and task-specific outcomes. We validate Transfer-Prompting through extensive experiments across 25 LLMs, including 7 foundational models and 18 specialized models, evaluated on 9 diverse datasets. The results demonstrate that Transfer-Prompting significantly improves task-specific performance, highlighting its potential for enhancing cross-task adaptation in LLMs. The code is available at this https URL.</li>
</ul>

<h3>Title: H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Zhen Huang, Ronghao Xu, Xiaoqian Zhou, Yangbo Wei, Suhua Wang, Xiaoxin Sun, Han Li, Qingsong Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14221">https://arxiv.org/abs/2502.14221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14221">https://arxiv.org/pdf/2502.14221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14221]] H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical Imaging(https://arxiv.org/abs/2502.14221)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks. However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency. Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures. The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs. Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis. In this work, We propose a \textbf{H}ybrid \textbf{3}D \textbf{DE}tection \textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data. This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling. To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs. Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness. Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations. We aready open-source our project, including code, data and model weights.</li>
</ul>

<h3>Title: Designing Parameter and Compute Efficient Diffusion Transformers using Distillation</h3>
<ul>
<li><strong>Authors: </strong>Vignesh Sundaresha</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14226">https://arxiv.org/abs/2502.14226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14226">https://arxiv.org/pdf/2502.14226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14226]] Designing Parameter and Compute Efficient Diffusion Transformers using Distillation(https://arxiv.org/abs/2502.14226)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) with billions of model parameters form the backbone of popular image and video generation models like DALL.E, Stable-Diffusion and SORA. Though these models are necessary in many low-latency applications like Augmented/Virtual Reality, they cannot be deployed on resource-constrained Edge devices (like Apple Vision Pro or Meta Ray-Ban glasses) due to their huge computational complexity. To overcome this, we turn to knowledge distillation and perform a thorough design-space exploration to achieve the best DiT for a given parameter size. In particular, we provide principles for how to choose design knobs such as depth, width, attention heads and distillation setup for a DiT. During the process, a three-way trade-off emerges between model performance, size and speed that is crucial for Edge implementation of diffusion. We also propose two distillation approaches - Teaching Assistant (TA) method and Multi-In-One (MI1) method - to perform feature distillation in the DiT context. Unlike existing solutions, we demonstrate and benchmark the efficacy of our approaches on practical Edge devices such as NVIDIA Jetson Orin Nano.</li>
</ul>

<h3>Title: SleepGMUformer: A gated multimodal temporal neural network for sleep staging</h3>
<ul>
<li><strong>Authors: </strong>Chenjun Zhao, Xuesen Niu, Xinglin Yu, Long Chen, Na Lv, Huiyu Zhou, Aite Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14227">https://arxiv.org/abs/2502.14227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14227">https://arxiv.org/pdf/2502.14227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14227]] SleepGMUformer: A gated multimodal temporal neural network for sleep staging(https://arxiv.org/abs/2502.14227)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Sleep staging is a key method for assessing sleep quality and diagnosing sleep disorders. However, current deep learning methods face challenges: 1) postfusion techniques ignore the varying contributions of different modalities; 2) unprocessed sleep data can interfere with frequency-domain information. To tackle these issues, this paper proposes a gated multimodal temporal neural network for multidomain sleep data, including heart rate, motion, steps, EEG (Fpz-Cz, Pz-Oz), and EOG from WristHR-Motion-Sleep and SleepEDF-78. The model integrates: 1) a pre-processing module for feature alignment, missing value handling, and EEG de-trending; 2) a feature extraction module for complex sleep features in the time dimension; and 3) a dynamic fusion module for real-time modality this http URL show classification accuracies of 85.03% on SleepEDF-78 and 94.54% on WristHR-Motion-Sleep datasets. The model handles heterogeneous datasets and outperforms state-of-the-art models by 1.00%-4.00%.</li>
</ul>

<h3>Title: Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14245">https://arxiv.org/abs/2502.14245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14245">https://arxiv.org/pdf/2502.14245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14245]] Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering(https://arxiv.org/abs/2502.14245)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we identify a critical problem, "lost-in-retrieval", in retrieval-augmented multi-hop question answering (QA): the key entities are missed in LLMs' sub-question decomposition. "Lost-in-retrieval" significantly degrades the retrieval performance, which disrupts the reasoning chain and leads to the incorrect answers. To resolve this problem, we propose a progressive retrieval and rewriting method, namely ChainRAG, which sequentially handles each sub-question by completing missing key entities and retrieving relevant sentences from a sentence graph for answer generation. Each step in our retrieval and rewriting process builds upon the previous one, creating a seamless chain that leads to accurate retrieval and answers. Finally, all retrieved sentences and sub-question answers are integrated to generate a comprehensive answer to the original question. We evaluate ChainRAG on three multi-hop QA datasets$\unicode{x2013}$MuSiQue, 2Wiki, and HotpotQA$\unicode{x2013}$using three large language models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results demonstrate that ChainRAG consistently outperforms baselines in both effectiveness and efficiency.</li>
</ul>

<h3>Title: Effects of Prompt Length on Domain-specific Tasks for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qibang Liu, Wenzhe Wang, Jeffrey Willard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14255">https://arxiv.org/abs/2502.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14255">https://arxiv.org/pdf/2502.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14255]] Effects of Prompt Length on Domain-specific Tasks for Large Language Models(https://arxiv.org/abs/2502.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models have garnered significant attention for their strong performance in various natural language tasks, such as machine translation and question answering. These models demonstrate an impressive ability to generalize across diverse tasks. However, their effectiveness in tackling domain-specific tasks, such as financial sentiment analysis and monetary policy understanding, remains a topic of debate, as these tasks often require specialized knowledge and precise reasoning. To address such challenges, researchers design various prompts to unlock the models' abilities. By carefully crafting input prompts, researchers can guide these models to produce more accurate responses. Consequently, prompt engineering has become a key focus of study. Despite the advancements in both models and prompt engineering, the relationship between the two-specifically, how prompt design impacts models' ability to perform domain-specific tasks-remains underexplored. This paper aims to bridge this research gap.</li>
</ul>

<h3>Title: LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Sujeong Im, Jungwoo Oh, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14259">https://arxiv.org/abs/2502.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14259">https://arxiv.org/pdf/2502.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14259]] LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records(https://arxiv.org/abs/2502.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.</li>
</ul>

<h3>Title: Money Recognition for the Visually Impaired: A Case Study on Sri Lankan Banknotes</h3>
<ul>
<li><strong>Authors: </strong>Akshaan Bandara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14267">https://arxiv.org/abs/2502.14267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14267">https://arxiv.org/pdf/2502.14267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14267]] Money Recognition for the Visually Impaired: A Case Study on Sri Lankan Banknotes(https://arxiv.org/abs/2502.14267)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Currency note recognition is a critical accessibility need for blind individuals, as identifying banknotes accurately can impact their independence and security in financial transactions. Several traditional and technological initiatives have been taken to date. Nevertheless, these approaches are less user-friendly and have made it more challenging for blind people to identify banknotes. This research proposes a user-friendly stand-alone system for the identification of Sri Lankan currency notes. A custom-created dataset of images of Sri Lankan currency notes was used to fine-tune an EfficientDet model. The currency note recognition model achieved 0.9847 AP on the validation dataset and performs exceptionally well in real-world scenarios. The high accuracy and the intuitive interface have enabled blind individuals to quickly and accurately identify currency denominations, ultimately encouraging accessibility and independence.</li>
</ul>

<h3>Title: MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard Correctness Labels</h3>
<ul>
<li><strong>Authors: </strong>Xiaoou Liu, Zhen Lin, Longchao Da, Chacha Chen, Shubhendu Trivedi, Hua Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14268">https://arxiv.org/abs/2502.14268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14268">https://arxiv.org/pdf/2502.14268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14268]] MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard Correctness Labels(https://arxiv.org/abs/2502.14268)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) require robust confidence estimation, particularly in critical domains like healthcare and law where unreliable outputs can lead to significant consequences. Despite much recent work in confidence estimation, current evaluation frameworks rely on correctness functions -- various heuristics that are often noisy, expensive, and possibly introduce systematic biases. These methodological weaknesses tend to distort evaluation metrics and thus the comparative ranking of confidence measures. We introduce MCQA-Eval, an evaluation framework for assessing confidence measures in Natural Language Generation (NLG) that eliminates dependence on an explicit correctness function by leveraging gold-standard correctness labels from multiple-choice datasets. MCQA-Eval enables systematic comparison of both internal state-based white-box (e.g. logit-based) and consistency-based black-box confidence measures, providing a unified evaluation methodology across different approaches. Through extensive experiments on multiple LLMs and widely used QA datasets, we report that MCQA-Eval provides efficient and more reliable assessments of confidence estimation methods than existing approaches.</li>
</ul>

<h3>Title: PaperHelper: Knowledge-Based LLM QA Paper Reading Assistant</h3>
<ul>
<li><strong>Authors: </strong>Congrui Yin, Evan Wei, Zhongxing Zhang, Zaifu Zhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14271">https://arxiv.org/abs/2502.14271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14271">https://arxiv.org/pdf/2502.14271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14271]] PaperHelper: Knowledge-Based LLM QA Paper Reading Assistant(https://arxiv.org/abs/2502.14271)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In the paper, we introduce a paper reading assistant, PaperHelper, a potent tool designed to enhance the capabilities of researchers in efficiently browsing and understanding scientific literature. Utilizing the Retrieval-Augmented Generation (RAG) framework, PaperHelper effectively minimizes hallucinations commonly encountered in large language models (LLMs), optimizing the extraction of accurate, high-quality knowledge. The implementation of advanced technologies such as RAFT and RAG Fusion significantly boosts the performance, accuracy, and reliability of the LLMs-based literature review process. Additionally, PaperHelper features a user-friendly interface that facilitates the batch downloading of documents and uses the Mermaid format to illustrate structural relationships between documents. Experimental results demonstrate that PaperHelper, based on a fine-tuned GPT-4 API, achieves an F1 Score of 60.04, with a latency of only 5.8 seconds, outperforming the basic RAG model by 7\% in F1 Score.</li>
</ul>

<h3>Title: Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yanggan Gu, Junzhuo Li, Sirui Huang, Xin Zou, Zhenghua Li, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14272">https://arxiv.org/abs/2502.14272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14272">https://arxiv.org/pdf/2502.14272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14272]] Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models(https://arxiv.org/abs/2502.14272)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by comparing pairwise responses, overlooking the extent of difference between responses. This limitation hinders student SLMs from capturing the nuanced preferences for multiple responses. In this paper, we propose a Preference-Aligned Distillation (PAD) framework, which models teacher's preference knowledge as a probability distribution over all potential preferences, thereby providing more nuanced supervisory signals. Our insight in developing PAD is rooted in the demonstration that language models can serve as reward functions, reflecting their intrinsic preferences. Based on this, PAD comprises three key steps: (1) sampling diverse responses using high-temperature; (2) computing rewards for both teacher and student to construct their intrinsic preference; and (3) training the student's intrinsic preference distribution to align with the teacher's. Experiments on four mainstream alignment benchmarks demonstrate that PAD consistently and significantly outperforms existing approaches, achieving over 20\% improvement on AlpacaEval 2 and Arena-Hard, indicating superior alignment with human preferences. Notably, on MT-Bench, using the \textsc{Gemma} model family, the student trained by PAD surpasses its teacher, further validating the effectiveness of our PAD.</li>
</ul>

<h3>Title: LLM-EvRep: Learning an LLM-Compatible Event Representation Using a Self-Supervised Framework</h3>
<ul>
<li><strong>Authors: </strong>Zongyou Yu, Qiang Qu, Qian Zhang, Nan Zhang, Xiaoming Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14273">https://arxiv.org/abs/2502.14273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14273">https://arxiv.org/pdf/2502.14273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14273]] LLM-EvRep: Learning an LLM-Compatible Event Representation Using a Self-Supervised Framework(https://arxiv.org/abs/2502.14273)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in event-based recognition have demonstrated significant promise, yet most existing approaches rely on extensive training, limiting their adaptability for efficient processing of event-driven visual content. Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot capabilities across diverse domains, but their application to event-based visual recognition remains largely unexplored. To bridge this gap, we propose \textbf{LLM-EvGen}, an event representation generator that produces LLM-compatible event representations \textbf{LLM-EvRep}, thereby enhancing the performance of LLMs on event recognition tasks. The generator is trained using a self-supervised framework, aligning the generated representations with semantic consistency and structural fidelity. Comprehensive experiments were conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results demonstrate that our method, \textbf{LLM-EvRep}, outperforms the event-to-video method, E2VID, by 15.93\%, 0.82\%, and 50.21\%, respectively, in recognition tasks when evaluated using GPT-4o.</li>
</ul>

<h3>Title: Fact or Guesswork? Evaluating Large Language Model's Medical Knowledge with Structured One-Hop Judgment</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Li, Yiwei Wang, Kai Zhang, Yujun Cai, Bryan Hooi, Nanyun Peng, Kai-Wei Chang, Jin Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14275">https://arxiv.org/abs/2502.14275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14275">https://arxiv.org/pdf/2502.14275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14275]] Fact or Guesswork? Evaluating Large Language Model's Medical Knowledge with Structured One-Hop Judgment(https://arxiv.org/abs/2502.14275)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely adopted in various downstream task domains. However, their ability to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess complex reasoning or multi-hop inference, making it difficult to isolate LLMs' inherent medical knowledge from their reasoning capabilities. Given the high-stakes nature of medical applications, where incorrect information can have critical consequences, it is essential to evaluate how well LLMs encode, retain, and recall fundamental medical facts. To bridge this gap, we introduce the Medical Knowledge Judgment, a dataset specifically designed to measure LLMs' one-hop factual medical knowledge. MKJ is constructed from the Unified Medical Language System (UMLS), a large-scale repository of standardized biomedical vocabularies and knowledge graphs. We frame knowledge assessment as a binary judgment task, requiring LLMs to verify the correctness of medical statements extracted from reliable and structured knowledge sources. Our experiments reveal that LLMs struggle with factual medical knowledge retention, exhibiting significant performance variance across different semantic categories, particularly for rare medical conditions. Furthermore, LLMs show poor calibration, often being overconfident in incorrect answers. To mitigate these issues, we explore retrieval-augmented generation, demonstrating its effectiveness in improving factual accuracy and reducing uncertainty in medical decision-making.</li>
</ul>

<h3>Title: STeCa: Step-level Trajectory Calibration for LLM Agent Learning</h3>
<ul>
<li><strong>Authors: </strong>Hanlin Wang, Jian Wang, Chak Tou Leong, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14276">https://arxiv.org/abs/2502.14276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14276">https://arxiv.org/pdf/2502.14276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14276]] STeCa: Step-level Trajectory Calibration for LLM Agent Learning(https://arxiv.org/abs/2502.14276)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: OrchardDepth: Precise Metric Depth Estimation of Orchard Scene from Monocular Camera Images</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Zheng, Henry Williams, Bruce A MacDonald</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14279">https://arxiv.org/abs/2502.14279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14279">https://arxiv.org/pdf/2502.14279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14279]] OrchardDepth: Precise Metric Depth Estimation of Orchard Scene from Monocular Camera Images(https://arxiv.org/abs/2502.14279)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation is a rudimentary task in robotic perception. Recently, with the development of more accurate and robust neural network models and different types of datasets, monocular depth estimation has significantly improved performance and efficiency. However, most of the research in this area focuses on very concentrated domains. In particular, most of the benchmarks in outdoor scenarios belong to urban environments for the improvement of autonomous driving devices, and these benchmarks have a massive disparity with the orchard/vineyard environment, which is hardly helpful for research in the primary industry. Therefore, we propose OrchardDepth, which fills the gap in the estimation of the metric depth of the monocular camera in the orchard/vineyard environment. In addition, we present a new retraining method to improve the training result by monitoring the consistent regularization between dense depth maps and sparse points. Our method improves the RMSE of depth estimation in the orchard environment from 1.5337 to 0.6738, proving our method's validation.</li>
</ul>

<h3>Title: EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts</h3>
<ul>
<li><strong>Authors: </strong>Subhajit Chaudhury, Payel Das, Sarathkrishna Swaminathan, Georgios Kollias, Elliot Nelson, Khushbu Pahwa, Tejaswini Pedapati, Igor Melnyk, Matthew Riemer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14280">https://arxiv.org/abs/2502.14280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14280">https://arxiv.org/pdf/2502.14280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14280]] EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts(https://arxiv.org/abs/2502.14280)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have yielded impressive successes on many language tasks. However, efficient processing of long contexts using LLMs remains a significant challenge. We introduce \textbf{EpMAN} -- a method for processing long contexts in an \textit{episodic memory} module while \textit{holistically attending to} semantically relevant context chunks. The output of \textit{episodic attention} is then used to reweigh the decoder's self-attention to the stored KV cache of the context during training and generation. When an LLM decoder is trained using \textbf{EpMAN}, its performance on multiple challenging single-hop long-context recall and question-answering benchmarks is found to be stronger and more robust across the range from 16k to 256k tokens than baseline decoders trained with self-attention, and popular retrieval-augmented generation frameworks.</li>
</ul>

<h3>Title: Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts</h3>
<ul>
<li><strong>Authors: </strong>Weipeng Huang, Qin Li, Yang Xiao, Cheng Qiao, Tie Cai, Junwei Liao, Neil J. Hurley, Guangyuan Piao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14281">https://arxiv.org/abs/2502.14281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14281">https://arxiv.org/pdf/2502.14281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14281]] Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts(https://arxiv.org/abs/2502.14281)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Noise in data appears to be inevitable in most real-world machine learning applications and would cause severe overfitting problems. Not only can data features contain noise, but labels are also prone to be noisy due to human input. In this paper, rather than noisy label learning in multiclass classifications, we instead focus on the less explored area of noisy label learning for multilabel classifications. Specifically, we investigate the post-correction of predictions generated from classifiers learned with noisy labels. The reasons are two-fold. Firstly, this approach can directly work with the trained models to save computational resources. Secondly, it could be applied on top of other noisy label correction techniques to achieve further improvements. To handle this problem, we appeal to deep generative approaches that are possible for uncertainty estimation. Our model posits that label noise arises from a stochastic shift in the latent variable, providing a more robust and beneficial means for noisy learning. We develop both unsupervised and semi-supervised learning methods for our model. The extensive empirical study presents solid evidence to that our approach is able to consistently improve the independent models and performs better than a number of existing methods across various noisy label settings. Moreover, a comprehensive empirical analysis of the proposed method is carried out to validate its robustness, including sensitivity analysis and an ablation study, among other elements.</li>
</ul>

<h3>Title: Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach</h3>
<ul>
<li><strong>Authors: </strong>Yurong Wu, Fangwen Mu, Qiuhong Zhang, Jinjing Zhao, Xinrun Xu, Lingrui Mei, Yang Wu, Lin Shi, Junjie Wang, Zhiming Ding, Yiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14285">https://arxiv.org/abs/2502.14285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14285">https://arxiv.org/pdf/2502.14285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14285]] Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach(https://arxiv.org/abs/2502.14285)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Drift: Decoding-time Personalized Alignments with Implicit User Preferences</h3>
<ul>
<li><strong>Authors: </strong>Minbeom Kim, Kang-il Lee, Seongho Joo, Hwaran Lee, Minbeom Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14289">https://arxiv.org/abs/2502.14289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14289">https://arxiv.org/pdf/2502.14289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14289]] Drift: Decoding-time Personalized Alignments with Implicit User Preferences(https://arxiv.org/abs/2502.14289)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalized alignments for individual users have been a long-standing goal in large language models (LLMs). We introduce Drift, a novel framework that personalizes LLMs at decoding time with implicit user preferences. Traditional Reinforcement Learning from Human Feedback (RLHF) requires thousands of annotated examples and expensive gradient updates. In contrast, Drift personalizes LLMs in a training-free manner, using only a few dozen examples to steer a frozen model through efficient preference modeling. Our approach models user preferences as a composition of predefined, interpretable attributes and aligns them at decoding time to enable personalized generation. Experiments on both a synthetic persona dataset (Perspective) and a real human-annotated dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines while using only 50-100 examples. Our results and analysis show that Drift is both computationally efficient and interpretable.</li>
</ul>

<h3>Title: A Note on Efficient Privacy-Preserving Similarity Search for Encrypted Vectors</h3>
<ul>
<li><strong>Authors: </strong>Dongfang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14291">https://arxiv.org/abs/2502.14291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14291">https://arxiv.org/pdf/2502.14291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14291]] A Note on Efficient Privacy-Preserving Similarity Search for Encrypted Vectors(https://arxiv.org/abs/2502.14291)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Traditional approaches to vector similarity search over encrypted data rely on fully homomorphic encryption (FHE) to enable computation without decryption. However, the substantial computational overhead of FHE makes it impractical for large-scale real-time applications. This work explores a more efficient alternative: using additively homomorphic encryption (AHE) for privacy-preserving similarity search. We consider scenarios where either the query vector or the database vectors remain encrypted, a setting that frequently arises in applications such as confidential recommender systems and secure federated learning. While AHE only supports addition and scalar multiplication, we show that it is sufficient to compute inner product similarity--one of the most widely used similarity measures in vector retrieval. Compared to FHE-based solutions, our approach significantly reduces computational overhead by avoiding ciphertext-ciphertext multiplications and bootstrapping, while still preserving correctness and privacy. We present an efficient algorithm for encrypted similarity search under AHE and analyze its error growth and security implications. Our method provides a scalable and practical solution for privacy-preserving vector search in real-world machine learning applications.</li>
</ul>

<h3>Title: Generalization Certificates for Adversarially Robust Bayesian Linear Regression</h3>
<ul>
<li><strong>Authors: </strong>Mahalakshmi Sabanayagam, Russell Tsuchida, Cheng Soon Ong, Debarghya Ghoshdastidar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14298">https://arxiv.org/abs/2502.14298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14298">https://arxiv.org/pdf/2502.14298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14298]] Generalization Certificates for Adversarially Robust Bayesian Linear Regression(https://arxiv.org/abs/2502.14298)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Adversarial robustness of machine learning models is critical to ensuring reliable performance under data perturbations. Recent progress has been on point estimators, and this paper considers distributional predictors. First, using the link between exponential families and Bregman divergences, we formulate an adversarial Bregman divergence loss as an adversarial negative log-likelihood. Using the geometric properties of Bregman divergences, we compute the adversarial perturbation for such models in closed-form. Second, under such losses, we introduce \emph{adversarially robust posteriors}, by exploiting the optimization-centric view of generalized Bayesian inference. Third, we derive the \emph{first} rigorous generalization certificates in the context of an adversarial extension of Bayesian linear regression by leveraging the PAC-Bayesian framework. Finally, experiments on real and synthetic datasets demonstrate the superior robustness of the derived adversarially robust posterior over Bayes posterior, and also validate our theoretical guarantees.</li>
</ul>

<h3>Title: SEA-HELM: Southeast Asian Holistic Evaluation of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yosephine Susanto, Adithya Venkatadri Hulagadri, Jann Railey Montalan, Jian Gang Ngui, Xian Bin Yong, Weiqi Leong, Hamsawardhini Rengarajan, Peerat Limkonchotiwat, Yifan Mai, William Chandra Tjhi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14301">https://arxiv.org/abs/2502.14301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14301">https://arxiv.org/pdf/2502.14301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14301]] SEA-HELM: Southeast Asian Holistic Evaluation of Language Models(https://arxiv.org/abs/2502.14301)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and authentic evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasizes SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner.</li>
</ul>

<h3>Title: MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shrey Pandit, Jiawei Xu, Junyuan Hong, Zhangyang Wang, Tianlong Chen, Kaidi Xu, Ying Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14302">https://arxiv.org/abs/2502.14302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14302">https://arxiv.org/pdf/2502.14302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14302]] MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models(https://arxiv.org/abs/2502.14302)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting "hard" category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a "not sure" category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.</li>
</ul>

<h3>Title: Î¼RL: Discovering Transient Execution Vulnerabilities Using Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>M. Caner Tol, Kemal Derya, Berk Sunar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14307">https://arxiv.org/abs/2502.14307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14307">https://arxiv.org/pdf/2502.14307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14307]] Î¼RL: Discovering Transient Execution Vulnerabilities Using Reinforcement Learning(https://arxiv.org/abs/2502.14307)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>We propose using reinforcement learning to address the challenges of discovering microarchitectural vulnerabilities, such as Spectre and Meltdown, which exploit subtle interactions in modern processors. Traditional methods like random fuzzing fail to efficiently explore the vast instruction space and often miss vulnerabilities that manifest under specific conditions. To overcome this, we introduce an intelligent, feedback-driven approach using RL. Our RL agents interact with the processor, learning from real-time feedback to prioritize instruction sequences more likely to reveal vulnerabilities, significantly improving the efficiency of the discovery process. We also demonstrate that RL systems adapt effectively to various microarchitectures, providing a scalable solution across processor generations. By automating the exploration process, we reduce the need for human intervention, enabling continuous learning that uncovers hidden vulnerabilities. Additionally, our approach detects subtle signals, such as timing anomalies or unusual cache behavior, that may indicate microarchitectural weaknesses. This proposal advances hardware security testing by introducing a more efficient, adaptive, and systematic framework for protecting modern processors. When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL agent was indeed able to generate instruction sequences that cause significant observable byte leakages through transient execution without generating any $\mu$code assists, faults or interrupts. The newly identified leaky sequences stem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW, CLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give credence to the proposed approach.</li>
</ul>

<h3>Title: On Theoretical Limits of Learning with Label Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Puning Zhao, Chuan Ma, Li Shen, Shaowei Wang, Rongfei Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14309">https://arxiv.org/abs/2502.14309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14309">https://arxiv.org/pdf/2502.14309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14309]] On Theoretical Limits of Learning with Label Differential Privacy(https://arxiv.org/abs/2502.14309)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Label differential privacy (DP) is designed for learning problems involving private labels and public features. While various methods have been proposed for learning under label DP, the theoretical limits remain largely unexplored. In this paper, we investigate the fundamental limits of learning with label DP in both local and central models for both classification and regression tasks, characterized by minimax convergence rates. We establish lower bounds by converting each task into a multiple hypothesis testing problem and bounding the test error. Additionally, we develop algorithms that yield matching upper bounds. Our results demonstrate that under label local DP (LDP), the risk has a significantly faster convergence rate than that under full LDP, i.e. protecting both features and labels, indicating the advantages of relaxing the DP definition to focus solely on labels. In contrast, under the label central DP (CDP), the risk is only reduced by a constant factor compared to full DP, indicating that the relaxation of CDP only has limited benefits on the performance.</li>
</ul>

<h3>Title: ODVerse33: Is the New YOLO Version Always Better? A Multi Domain benchmark from YOLO v5 to v11</h3>
<ul>
<li><strong>Authors: </strong>Tianyou Jiang, Yang Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14314">https://arxiv.org/abs/2502.14314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14314">https://arxiv.org/pdf/2502.14314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14314]] ODVerse33: Is the New YOLO Version Always Better? A Multi Domain benchmark from YOLO v5 to v11(https://arxiv.org/abs/2502.14314)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>You Look Only Once (YOLO) models have been widely used for building real-time object detectors across various domains. With the increasing frequency of new YOLO versions being released, key questions arise. Are the newer versions always better than their previous versions? What are the core innovations in each YOLO version and how do these changes translate into real-world performance gains? In this paper, we summarize the key innovations from YOLOv1 to YOLOv11, introduce a comprehensive benchmark called ODverse33, which includes 33 datasets spanning 11 diverse domains (Autonomous driving, Agricultural, Underwater, Medical, Videogame, Industrial, Aerial, Wildlife, Retail, Microscopic, and Security), and explore the practical impact of model improvements in real-world, multi-domain applications through extensive experimental results. We hope this study can provide some guidance to the extensive users of object detection models and give some references for future real-time object detector development.</li>
</ul>

<h3>Title: Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Amir Hossein Yari, Fajri Koto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14315">https://arxiv.org/abs/2502.14315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14315">https://arxiv.org/pdf/2502.14315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14315]] Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension(https://arxiv.org/abs/2502.14315)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance of multilingual large language models (mLLMs) in various natural language processing tasks, their ability to understand procedural texts, particularly those with culture-specific content, remains largely unexplored. Texts describing cultural procedures, including rituals, traditional craftsmanship, and social etiquette, require an inherent understanding of cultural context, presenting a significant challenge for mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate mLLMs' ability to process and reason about culturally diverse procedural texts across multiple languages using various methodologies to assess their performance. Our findings indicate that (1) mLLMs face difficulties with culturally contextualized procedural texts, showing notable performance declines in low-resource languages, (2) model performance fluctuates across cultural domains, with some areas presenting greater difficulties, and (3) language models exhibit better performance on multiple-choice tasks within conversational frameworks compared to direct questioning. These results underscore the current limitations of mLLMs in handling culturally nuanced procedural texts and highlight the need for culturally aware benchmarks like CAPTex to enhance their adaptability and comprehension across diverse linguistic and cultural landscapes.</li>
</ul>

<h3>Title: Textured 3D Regenerative Morphing with 3D Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Songlin Yang, Yushi Lan, Honghua Chen, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14316">https://arxiv.org/abs/2502.14316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14316">https://arxiv.org/pdf/2502.14316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14316]] Textured 3D Regenerative Morphing with 3D Diffusion Prior(https://arxiv.org/abs/2502.14316)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Textured 3D morphing creates smooth and plausible interpolation sequences between two 3D objects, focusing on transitions in both shape and texture. This is important for creative applications like visual effects in filmmaking. Previous methods rely on establishing point-to-point correspondences and determining smooth deformation trajectories, which inherently restrict them to shape-only morphing on untextured, topologically aligned datasets. This restriction leads to labor-intensive preprocessing and poor generalization. To overcome these challenges, we propose a method for 3D regenerative morphing using a 3D diffusion prior. Unlike previous methods that depend on explicit correspondences and deformations, our method eliminates the additional need for obtaining correspondence and uses the 3D diffusion prior to generate morphing. Specifically, we introduce a 3D diffusion model and interpolate the source and target information at three levels: initial noise, model parameters, and condition features. We then explore an Attention Fusion strategy to generate more smooth morphing sequences. To further improve the plausibility of semantic interpolation and the generated 3D surfaces, we propose two strategies: (a) Token Reordering, where we match approximate tokens based on semantic analysis to guide implicit correspondences in the denoising process of the diffusion model, and (b) Low-Frequency Enhancement, where we enhance low-frequency signals in the tokens to improve the quality of generated surfaces. Experimental results show that our method achieves superior smoothness and plausibility in 3D morphing across diverse cross-category object pairs, offering a novel regenerative method for 3D morphing with textured representations.</li>
</ul>

<h3>Title: ParallelComp: Parallel Long-Context Compressor for Length Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Jing Xiong, Jianghan Shen, Chuanyang Zheng, Zhongwei Wan, Chenyang Zhao, Chiwun Yang, Fanghua Ye, Hongxia Yang, Lingpeng Kong, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14317">https://arxiv.org/abs/2502.14317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14317">https://arxiv.org/pdf/2502.14317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14317]] ParallelComp: Parallel Long-Context Compressor for Length Extrapolation(https://arxiv.org/abs/2502.14317)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficiently handling long contexts is crucial for large language models (LLMs). While rotary position embeddings (RoPEs) enhance length generalization, effective length extrapolation remains challenging and often requires costly fine-tuning. In contrast, recent training-free approaches suffer from the attention sink phenomenon, leading to severe performance degradation. In this paper, we introduce ParallelComp, a novel training-free method for long-context extrapolation that extends LLMs' context length from 4K to 128K while maintaining high throughput and preserving perplexity, and integrates seamlessly with Flash Attention. Our analysis offers new insights into attention biases in parallel attention mechanisms and provides practical solutions to tackle these challenges. To mitigate the attention sink issue, we propose an attention calibration strategy that reduces biases, ensuring more stable long-range attention. Additionally, we introduce a chunk eviction strategy to efficiently manage ultra-long contexts on a single A100 80GB GPU. To further enhance efficiency, we propose a parallel KV cache eviction technique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x acceleration in the prefilling stage with negligible performance loss due to attention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's performance on long-context tasks using an 8B model trained on 8K-length context, outperforming powerful closed-source models such as Claude-2 and Kimi-Chat.</li>
</ul>

<h3>Title: Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Fodor</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14318">https://arxiv.org/abs/2502.14318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14318">https://arxiv.org/pdf/2502.14318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14318]] Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models(https://arxiv.org/abs/2502.14318)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) regularly demonstrate new and impressive performance on a wide range of language, knowledge, and reasoning benchmarks. Such rapid progress has led many commentators to argue that LLM general cognitive capabilities have likewise rapidly improved, with the implication that such models are becoming progressively more capable on various real-world tasks. Here I summarise theoretical and empirical considerations to challenge this narrative. I argue that inherent limitations with the benchmarking paradigm, along with specific limitations of existing benchmarks, render benchmark performance highly unsuitable as a metric for generalisable competence over cognitive tasks. I also contend that alternative methods for assessing LLM capabilities, including adversarial stimuli and interpretability techniques, have shown that LLMs do not have robust competence in many language and reasoning tasks, and often fail to learn representations which facilitate generalisable inferences. I conclude that benchmark performance should not be used as a reliable indicator of general LLM cognitive capabilities.</li>
</ul>

<h3>Title: Browser Fingerprint Detection and Anti-Tracking</h3>
<ul>
<li><strong>Authors: </strong>Kaitong Lin, Huazhu Cao, Amin Milani Fard</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14326">https://arxiv.org/abs/2502.14326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14326">https://arxiv.org/pdf/2502.14326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14326]] Browser Fingerprint Detection and Anti-Tracking(https://arxiv.org/abs/2502.14326)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Digital fingerprints have brought great convenience and benefits to many online businesses. However, they pose a significant threat to the privacy and security of ordinary users. In this paper, we investigate the effectiveness of current anti-tracking methods against digital fingerprints and design a browser extension that can effectively resist digital fingerprints and record the website's collection of digital fingerprint-related information.</li>
</ul>

<h3>Title: A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics</h3>
<ul>
<li><strong>Authors: </strong>Ting-Ruen Wei, Haowei Liu, Xuyang Wu, Yi Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14333">https://arxiv.org/abs/2502.14333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14333">https://arxiv.org/pdf/2502.14333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14333]] A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics(https://arxiv.org/abs/2502.14333)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in large language models (LLM) found chain-of-thought prompting strategies to improve the reasoning ability of LLMs by encouraging problem solving through multiple steps. Therefore, subsequent research aimed to integrate the multi-step reasoning process into the LLM itself through process rewards as feedback and achieved improvements over prompting strategies. Due to the cost of step-level annotation, some turn to outcome rewards as feedback. Aside from these training-based approaches, training-free techniques leverage frozen LLMs or external tools for feedback at each step to enhance the reasoning process. With the abundance of work in mathematics due to its logical nature, we present a survey of strategies utilizing feedback at the step and outcome levels to enhance multi-step math reasoning for LLMs. As multi-step reasoning emerges a crucial component in scaling LLMs, we hope to establish its foundation for easier understanding and empower further research.</li>
</ul>

<h3>Title: Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective</h3>
<ul>
<li><strong>Authors: </strong>Ruichen Shao, Bei Li, Gangao Liu, Yang Chen, Xiang Zhou, Jingang Wang, Xunliang Cai, Peng Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14340">https://arxiv.org/abs/2502.14340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14340">https://arxiv.org/pdf/2502.14340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14340]] Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective(https://arxiv.org/abs/2502.14340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. Despite its advantages, DPO suffers from a length bias, generating responses longer than those from the reference model. Existing solutions like SimPO and SamPO address this issue but uniformly treat the contribution of rewards across sequences, overlooking temporal dynamics. To this end, we propose an enhanced preference optimization method that incorporates a temporal decay factor controlled by a gamma parameter. This dynamic weighting mechanism adjusts the influence of each reward based on its position in the sequence, prioritizing earlier tokens that are more critical for alignment. By adaptively focusing on more relevant feedback, our approach mitigates overfitting to less pertinent data and remains responsive to evolving human preferences. Experimental results on several benchmarks show that our approach consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures and sizes. Furthermore, additional experiments on mathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our method enhances performance without compromising general capabilities. Our codebase would be available at \url{this https URL}.</li>
</ul>

<h3>Title: SegAnyPET: Universal Promptable Segmentation from Positron Emission Tomography Images</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Le Xue, Wenbo Zhang, Lanlan Li, Yuchen Liu, Chen Jiang, Yuan Cheng, Yuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14351">https://arxiv.org/abs/2502.14351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14351">https://arxiv.org/pdf/2502.14351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14351]] SegAnyPET: Universal Promptable Segmentation from Positron Emission Tomography Images(https://arxiv.org/abs/2502.14351)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Positron Emission Tomography (PET) imaging plays a crucial role in modern medical diagnostics by revealing the metabolic processes within a patient's body, which is essential for quantification of therapy response and monitoring treatment progress. However, the segmentation of PET images presents unique challenges due to their lower contrast and less distinct boundaries compared to other structural medical modalities. Recent developments in segmentation foundation models have shown superior versatility across diverse natural image segmentation tasks. Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit poor generalization ability when adapted to molecular PET imaging. In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images. Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images. To issue the challenge of discrepant annotation quality of PET images, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data. Experimental results demonstrate that SegAnyPET can correctly segment seen and unseen targets using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation. As the first foundation model for PET images, we believe that SegAnyPET will advance the applications to various downstream tasks for molecular imaging.</li>
</ul>

<h3>Title: SR-LLM: Rethinking the Structured Representation in Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jiahuan Zhang, Tianheng Wang, Hanqing Wu, Ziyi Huang, Yulong Wu, Dongbai Chen, Linfeng Song, Yue Zhang, Guozheng Rao, Kaicheng Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14352">https://arxiv.org/abs/2502.14352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14352">https://arxiv.org/pdf/2502.14352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14352]] SR-LLM: Rethinking the Structured Representation in Large Language Model(https://arxiv.org/abs/2502.14352)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Structured representations, exemplified by Abstract Meaning Representation (AMR), have long been pivotal in computational linguistics. However, their role remains ambiguous in the Large Language Models (LLMs) era. Initial attempts to integrate structured representation into LLMs via a zero-shot setting yielded inferior performance. We hypothesize that such a decline stems from the structure information being passed into LLMs in a code format unfamiliar to LLMs' training corpora. Consequently, we propose SR-LLM, an innovative framework with two settings to explore a superior way of integrating structured representation with LLMs from training-free and training-dependent perspectives. The former integrates structural information through natural language descriptions in LLM prompts, whereas its counterpart augments the model's inference capability through fine-tuning on linguistically described structured representations. Performance improvements were observed in widely downstream datasets, with particularly notable gains of 3.17% and 12.38% in PAWS. To the best of our knowledge, this work represents the pioneering demonstration that leveraging structural representations can substantially enhance LLMs' inference capability. We hope that our work sheds light and encourages future research to enhance the reasoning and interoperability of LLMs by structure data.</li>
</ul>

<h3>Title: PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xinpeng Shou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14370">https://arxiv.org/abs/2502.14370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14370">https://arxiv.org/pdf/2502.14370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14370]] PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy Optimization(https://arxiv.org/abs/2502.14370)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Model inversion attacks pose a significant privacy risk by attempting to reconstruct private training data from trained models. Most of the existing methods either depend on gradient estimation or require white-box access to model parameters, which limits their applicability in practical scenarios. In this paper, we propose PPO-MI, a novel reinforcement learning-based framework for black-box model inversion attacks. Our approach formulates the inversion task as a Markov Decision Process, where an agent navigates the latent space of a generative model to reconstruct private training samples using only model predictions. By employing Proximal Policy Optimization (PPO) with a momentum-based state transition mechanism, along with a reward function balancing prediction accuracy and exploration, PPO-MI ensures efficient latent space exploration and high query efficiency. We conduct extensive experiments illustrates that PPO-MI outperforms the existing methods while require less attack knowledge, and it is robust across various model architectures and datasets. These results underline its effectiveness and generalizability in practical black-box scenarios, raising important considerations for the privacy vulnerabilities of deployed machine learning models.</li>
</ul>

<h3>Title: CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual Try-on guided by Tri-zone Priors</h3>
<ul>
<li><strong>Authors: </strong>Donghao Luo, Yujie Liang, Xu Peng, Xiaobin Hu, Boyuan Jiang, Chengming Xu, Taisong Jin, Chengjie Wang, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14373">https://arxiv.org/abs/2502.14373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14373">https://arxiv.org/pdf/2502.14373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14373]] CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual Try-on guided by Tri-zone Priors(https://arxiv.org/abs/2502.14373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite remarkable progress in image-based virtual try-on systems, generating realistic and robust fitting images for cross-category virtual try-on remains a challenging task. The primary difficulty arises from the absence of human-like reasoning, which involves addressing size mismatches between garments and models while recognizing and leveraging the distinct functionalities of various regions within the model images. To address this issue, we draw inspiration from human cognitive processes and disentangle the complex reasoning required for cross-category try-on into a structured framework. This framework systematically decomposes the model image into three distinct regions: try-on, reconstruction, and imagination zones. Each zone plays a specific role in accommodating the garment and facilitating realistic synthesis. To endow the model with robust reasoning capabilities for cross-category scenarios, we propose an iterative data constructor. This constructor encompasses diverse scenarios, including intra-category try-on, any-to-dress transformations (replacing any garment category with a dress), and dress-to-any transformations (replacing a dress with another garment category). Utilizing the generated dataset, we introduce a tri-zone priors generator that intelligently predicts the try-on, reconstruction, and imagination zones by analyzing how the input garment is expected to align with the model image. Guided by these tri-zone priors, our proposed method, CrossVTON, achieves state-of-the-art performance, surpassing existing baselines in both qualitative and quantitative evaluations. Notably, it demonstrates superior capability in handling cross-category virtual try-on, meeting the complex demands of real-world applications.</li>
</ul>

<h3>Title: VFL-RPS: Relevant Participant Selection in Vertical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Afsana Khan, Marijn ten Thij, Guangzhi Tang, Anna Wilbik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14375">https://arxiv.org/abs/2502.14375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14375">https://arxiv.org/pdf/2502.14375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14375]] VFL-RPS: Relevant Participant Selection in Vertical Federated Learning(https://arxiv.org/abs/2502.14375)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) allows collaboration between different parties, while ensuring that the data across these parties is not shared. However, not every collaboration is helpful in terms of the resulting model performance. Therefore, it is an important challenge to select the correct participants in a collaboration. As it currently stands, most of the efforts in participant selection in the literature have focused on Horizontal Federated Learning (HFL), which assumes that all features are the same across all participants, disregarding the possibility of different features across participants which is captured in Vertical Federated Learning (VFL). To close this gap in the literature, we propose a novel method VFL-RPS for participant selection in VFL, as a pre-training step. We have tested our method on several data sets performing both regression and classification tasks, showing that our method leads to comparable results as using all data by only selecting a few participants. In addition, we show that our method outperforms existing methods for participant selection in VFL.</li>
</ul>

<h3>Title: A Similarity Paradigm Through Textual Regularization Without Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14376">https://arxiv.org/abs/2502.14376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14376">https://arxiv.org/pdf/2502.14376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14376]] A Similarity Paradigm Through Textual Regularization Without Forgetting(https://arxiv.org/abs/2502.14376)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods.</li>
</ul>

<h3>Title: RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Ke Cao, Jing Wang, Ao Ma, Jiasong Feng, Zhanjie Zhang, Xuanhua He, Shanyuan Liu, Bo Cheng, Dawei Leng, Yuhui Yin, Jie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14377">https://arxiv.org/abs/2502.14377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14377">https://arxiv.org/pdf/2502.14377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14377]] RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers(https://arxiv.org/abs/2502.14377)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The Diffusion Transformer plays a pivotal role in advancing text-to-image and text-to-video generation, owing primarily to its inherent scalability. However, existing controlled diffusion transformer methods incur significant parameter and computational overheads and suffer from inefficient resource allocation due to their failure to account for the varying relevance of control information across different transformer layers. To address this, we propose the Relevance-Guided Efficient Controllable Generation framework, RelaCtrl, enabling efficient and resource-optimized integration of control signals into the Diffusion Transformer. First, we evaluate the relevance of each layer in the Diffusion Transformer to the control information by assessing the "ControlNet Relevance Score"-i.e., the impact of skipping each control layer on both the quality of generation and the control effectiveness during inference. Based on the strength of the relevance, we then tailor the positioning, parameter scale, and modeling capacity of the control layers to reduce unnecessary parameters and redundant computations. Additionally, to further improve efficiency, we replace the self-attention and FFN in the commonly used copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM), enabling efficient implementation of both the token mixer and channel mixer. Both qualitative and quantitative experimental results demonstrate that our approach achieves superior performance with only 15% of the parameters and computational complexity compared to PixArt-delta. More examples are available at this https URL.</li>
</ul>

<h3>Title: S*: Test Time Scaling for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E. Gonzalez, Ion Stoica</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14382">https://arxiv.org/abs/2502.14382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14382">https://arxiv.org/pdf/2502.14382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14382]] S*: Test Time Scaling for Code Generation(https://arxiv.org/abs/2502.14382)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under this https URL.</li>
</ul>

<h3>Title: Leveraging Small LLMs for Argument Mining in Education: Argument Component Identification, Classification, and Assessment</h3>
<ul>
<li><strong>Authors: </strong>Lucile Favero, Juan Antonio PÃ©rez-Ortiz, Tanja KÃ¤ser, Nuria Oliver</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14389">https://arxiv.org/abs/2502.14389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14389">https://arxiv.org/pdf/2502.14389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14389]] Leveraging Small LLMs for Argument Mining in Education: Argument Component Identification, Classification, and Assessment(https://arxiv.org/abs/2502.14389)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Argument mining algorithms analyze the argumentative structure of essays, making them a valuable tool for enhancing education by providing targeted feedback on the students' argumentation skills. While current methods often use encoder or encoder-decoder deep learning architectures, decoder-only models remain largely unexplored, offering a promising research direction. This paper proposes leveraging open-source, small Large Language Models (LLMs) for argument mining through few-shot prompting and fine-tuning. These models' small size and open-source nature ensure accessibility, privacy, and computational efficiency, enabling schools and educators to adopt and deploy them locally. Specifically, we perform three tasks: segmentation of student essays into arguments, classification of the arguments by type, and assessment of their quality. We empirically evaluate the models on the Feedback Prize - Predicting Effective Arguments dataset of grade 6-12 students essays and demonstrate how fine-tuned small LLMs outperform baseline methods in segmenting the essays and determining the argument types while few-shot prompting yields comparable performance to that of the baselines in assessing quality. This work highlights the educational potential of small, open-source LLMs to provide real-time, personalized feedback, enhancing independent learning and writing skills while ensuring low computational cost and privacy.</li>
</ul>

<h3>Title: Enhancing Portuguese Variety Identification with Cross-Domain Approaches</h3>
<ul>
<li><strong>Authors: </strong>Hugo Sousa, RÃºben Almeida, PurificaÃ§Ã£o Silvano, InÃªs Cantante, Ricardo Campos, AlÃ­pio Jorge</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14394">https://arxiv.org/abs/2502.14394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14394">https://arxiv.org/pdf/2502.14394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14394]] Enhancing Portuguese Variety Identification with Cross-Domain Approaches(https://arxiv.org/abs/2502.14394)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in natural language processing have raised expectations for generative models to produce coherent text across diverse language varieties. In the particular case of the Portuguese language, the predominance of Brazilian Portuguese corpora online introduces linguistic biases in these models, limiting their applicability outside of Brazil. To address this gap and promote the creation of European Portuguese resources, we developed a cross-domain language variety identifier (LVI) to discriminate between European and Brazilian Portuguese. Motivated by the findings of our literature review, we compiled the PtBrVarId corpus, a cross-domain LVI dataset, and study the effectiveness of transformer-based LVI classifiers for cross-domain scenarios. Although this research focuses on two Portuguese varieties, our contribution can be extended to other varieties and languages. We open source the code, corpus, and models to foster further research in this task.</li>
</ul>

<h3>Title: PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data</h3>
<ul>
<li><strong>Authors: </strong>Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, Jiaming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14397">https://arxiv.org/abs/2502.14397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14397">https://arxiv.org/pdf/2502.14397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14397]] PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data(https://arxiv.org/abs/2502.14397)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage training strategy. Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism. Additionally, we release a PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation.</li>
</ul>

<h3>Title: Unstructured Evidence Attribution for Long Context Query Focused Summarization</h3>
<ul>
<li><strong>Authors: </strong>Dustin Wright, Zain Muhammad Mujahid, Lu Wang, Isabelle Augenstein, David Jurgens</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14409">https://arxiv.org/abs/2502.14409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14409">https://arxiv.org/pdf/2502.14409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14409]] Unstructured Evidence Attribution for Long Context Query Focused Summarization(https://arxiv.org/abs/2502.14409)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query. Extracting and properly citing evidence spans could help improve the transparency and reliability of these summaries. At the same time, LLMs suffer from positional biases in terms of which information they understand and attend to, which could affect evidence citation. Whereas previous work has focused on evidence citation with predefined levels of granularity (e.g. sentence, paragraph, document, etc.), we propose the task of long-context query focused summarization with unstructured evidence citation. We show how existing systems struggle to generate and properly cite unstructured evidence from their context, and that evidence tends to be "lost-in-the-middle". To help mitigate this, we create the Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated using a novel domain-agnostic pipeline which can be used as supervision to adapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4 datasets with varying document types and lengths that LLMs adapted with SUnsET data generate more relevant and factually consistent evidence than their base models, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries.</li>
</ul>

<h3>Title: Evaluating Precise Geolocation Inference Capabilities of Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Neel Jay, Hieu Minh Nguyen, Trung Dung Hoang, Jacob Haimes</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14412">https://arxiv.org/abs/2502.14412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14412">https://arxiv.org/pdf/2502.14412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14412]] Evaluating Precise Geolocation Inference Capabilities of Vision Language Models(https://arxiv.org/abs/2502.14412)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The prevalence of Vision-Language Models (VLMs) raises important questions about privacy in an era where visual information is increasingly available. While foundation VLMs demonstrate broad knowledge and learned capabilities, we specifically investigate their ability to infer geographic location from previously unseen image data. This paper introduces a benchmark dataset collected from Google Street View that represents its global distribution of coverage. Foundation models are evaluated on single-image geolocation inference, with many achieving median distance errors of <300 km. We further evaluate VLM "agents" with access to supplemental tools, observing up to a 30.6% decrease in distance error. Our findings establish that modern foundation VLMs can act as powerful image geolocation tools, without being specifically trained for this task. When coupled with increasing accessibility of these models, our findings have greater implications for online privacy. We discuss these risks, as well as future work in this area.</li>
</ul>

<h3>Title: Towards Efficient Automatic Self-Pruning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Fei Chao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14413">https://arxiv.org/abs/2502.14413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14413">https://arxiv.org/pdf/2502.14413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14413]] Towards Efficient Automatic Self-Pruning of Large Language Models(https://arxiv.org/abs/2502.14413)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite exceptional capabilities, Large Language Models (LLMs) still face deployment challenges due to their enormous size. Post-training structured pruning is a promising solution that prunes LLMs without the need for retraining, reducing computational overhead, and it is hardware-deployment friendly. However, the training-free nature of post-training structured pruning leads to significant performance degradation. We argue that the key to mitigating this issue lies in accurately determining the pruning rate for each layer. Meanwhile, we find that LLMs may have prior knowledge about their own redundancy. Based on this insight, we introduce $\textbf{Self-Pruner}$ an end-to-end automatic self-pruning framework for LLMs, which efficiently search layer-wise pruning rates. Specifically, $\textbf{Self-Pruner}$ leverages LLMs to autonomously execute the entire evolutionary search process to search for pruning rate configurations. In this process, LLMs are used to generate populations, select parent solutions from the current population, and perform crossover and mutation operations to produce offspring solutions. In this way, LLMs automatically generate and evaluate a large number of candidate solutions, effectively converging to find the pruning rate configurations with minimal human intervention. Extensive experiments demonstrate $\textbf{Self-Pruner}$'s better performance compared to existing state-of-the-art methods. Notably, $\textbf{Self-Pruner}$ prunes LLaMA-2-70B to 49B level with only 0.80$\%$ drop in accuracy across seven commonsense reasoning tasks, achieving a 1.39$\times$ speedup on NVIDIA A100 80GB GPU. Further pruning to 35B level resulted in only a 3.80$\%$ decrease in accuracy while obtaining a 1.70$\times$ speedup.</li>
</ul>

<h3>Title: A Survey on Data Contamination for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxing Cheng, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14425">https://arxiv.org/abs/2502.14425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14425">https://arxiv.org/pdf/2502.14425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14425]] A Survey on Data Contamination for Large Language Models(https://arxiv.org/abs/2502.14425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.</li>
</ul>

<h3>Title: Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Artem Vazhentsev, Lyudmila Rvanova, Ivan Lazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Artem Shelmanov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14427">https://arxiv.org/abs/2502.14427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14427">https://arxiv.org/pdf/2502.14427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14427]] Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models(https://arxiv.org/abs/2502.14427)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a well-established UQ technique in classification tasks - for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores. Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks. Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications.</li>
</ul>

<h3>Title: Cardiac Evidence Backtracking for Eating Behavior Monitoring using Collocative Electrocardiogram Imagining</h3>
<ul>
<li><strong>Authors: </strong>Xu-Lu Zhang, Zhen-Qun Yang, Dong-Mei Jiang, Ga Liao, Qing Li, Ramesh Jain, Xiao-Yong Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14430">https://arxiv.org/abs/2502.14430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14430">https://arxiv.org/pdf/2502.14430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14430]] Cardiac Evidence Backtracking for Eating Behavior Monitoring using Collocative Electrocardiogram Imagining(https://arxiv.org/abs/2502.14430)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Eating monitoring has remained an open challenge in medical research for years due to the lack of non-invasive sensors for continuous monitoring and the reliable methods for automatic behavior detection. In this paper, we present a pilot study using the wearable 24-hour ECG for sensing and tailoring the sophisticated deep learning for ad-hoc and interpretable detection. This is accomplished using a collocative learning framework in which 1) we construct collocative tensors as pseudo-images from 1D ECG signals to improve the feasibility of 2D image-based deep models; 2) we formulate the cardiac logic of analyzing the ECG data in a comparative way as periodic attention regulators so as to guide the deep inference to collect evidence in a human comprehensible manner; and 3) we improve the interpretability of the framework by enabling the backtracking of evidence with a set of methods designed for Class Activation Mapping (CAM) decoding and decision tree/forest generation. The effectiveness of the proposed framework has been validated on the largest ECG dataset of eating behavior with superior performance over conventional models, and its capacity of cardiac evidence mining has also been verified through the consistency of the evidence it backtracked and that of the previous medical studies.</li>
</ul>

<h3>Title: An Enhancement of Jiang, Z., et al.s Compression-Based Classification Algorithm Applied to News Article Categorization</h3>
<ul>
<li><strong>Authors: </strong>Sean Lester C. Benavides, Cid Antonio F. Masapol, Jonathan C. Morano, Dan Michael A. Cortez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14444">https://arxiv.org/abs/2502.14444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14444">https://arxiv.org/pdf/2502.14444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14444]] An Enhancement of Jiang, Z., et al.s Compression-Based Classification Algorithm Applied to News Article Categorization(https://arxiv.org/abs/2502.14444)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>This study enhances Jiang et al.'s compression-based classification algorithm by addressing its limitations in detecting semantic similarities between text documents. The proposed improvements focus on unigram extraction and optimized concatenation, eliminating reliance on entire document compression. By compressing extracted unigrams, the algorithm mitigates sliding window limitations inherent to gzip, improving compression efficiency and similarity detection. The optimized concatenation strategy replaces direct concatenation with the union of unigrams, reducing redundancy and enhancing the accuracy of Normalized Compression Distance (NCD) calculations. Experimental results across datasets of varying sizes and complexities demonstrate an average accuracy improvement of 5.73%, with gains of up to 11% on datasets containing longer documents. Notably, these improvements are more pronounced in datasets with high-label diversity and complex text structures. The methodology achieves these results while maintaining computational efficiency, making it suitable for resource-constrained environments. This study provides a robust, scalable solution for text classification, emphasizing lightweight preprocessing techniques to achieve efficient compression, which in turn enables more accurate classification.</li>
</ul>

<h3>Title: PredictaBoard: Benchmarking LLM Score Predictability</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Pacchiardi, Konstantinos Voudouris, Ben Slater, Fernando MartÃ­nez-Plumed, JosÃ© HernÃ¡ndez-Orallo, Lexin Zhou, Wout Schellaert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14445">https://arxiv.org/abs/2502.14445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14445">https://arxiv.org/pdf/2502.14445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14445]] PredictaBoard: Benchmarking LLM Score Predictability(https://arxiv.org/abs/2502.14445)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable "safe zone" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at this https URL</li>
</ul>

<h3>Title: Optimal word order for non-causal text generation with Large Language Models: the Spanish case</h3>
<ul>
<li><strong>Authors: </strong>Andrea Busto-CastiÃ±eira, Silvia GarcÃ­a-MÃ©ndez, Francisco de Arriba-PÃ©rez, Francisco J. GonzÃ¡lez-CastaÃ±o</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14451">https://arxiv.org/abs/2502.14451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14451">https://arxiv.org/pdf/2502.14451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14451]] Optimal word order for non-causal text generation with Large Language Models: the Spanish case(https://arxiv.org/abs/2502.14451)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Natural Language Generation (NLG) popularity has increased owing to the progress in Large Language Models (LLMs), with zero-shot inference capabilities. However, most neural systems utilize decoder-only causal (unidirectional) transformer models, which are effective for English but may reduce the richness of languages with less strict word order, subject omission, or different relative clause attachment preferences. This is the first work that analytically addresses optimal text generation order for non-causal language models. We present a novel Viterbi algorithm-based methodology for maximum likelihood word order estimation. We analyze the non-causal most-likelihood order probability for NLG in Spanish and, then, the probability of generating the same phrases with Spanish causal NLG. This comparative analysis reveals that causal NLG prefers English-like SVO structures. We also analyze the relationship between optimal generation order and causal left-to-right generation order using Spearman's rank correlation. Our results demonstrate that the ideal order predicted by the maximum likelihood estimator is not closely related to the causal order and may be influenced by the syntactic structure of the target sentence.</li>
</ul>

<h3>Title: Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing</h3>
<ul>
<li><strong>Authors: </strong>Aviv Bick, Tobias Katsch, Nimit Sohoni, Arjun Desai, Albert Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14458">https://arxiv.org/abs/2502.14458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14458">https://arxiv.org/pdf/2502.14458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14458]] Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing(https://arxiv.org/abs/2502.14458)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Llamba, a family of efficient recurrent language models distilled from Llama-3.x into the Mamba architecture. The series includes Llamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput and handle significantly larger batch sizes than Transformer-based models while maintaining comparable benchmark performance. Furthermore, Llamba demonstrates the effectiveness of cross-architecture distillation using MOHAWK (Bick et al., 2024), achieving these results with less than 0.1% of the training data typically used for models of similar size. To take full advantage of their efficiency, we provide an optimized implementation of Llamba for resource-constrained devices such as smartphones and edge platforms, offering a practical and memory-efficient alternative to Transformers. Overall, Llamba improves the tradeoff between speed, memory efficiency, and performance, making high-quality language models more accessible.</li>
</ul>

<h3>Title: PQBFL: A Post-Quantum Blockchain-based Protocol for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Hadi GHaravi, Jorge Granjal, Edmundo Monteiro</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14464">https://arxiv.org/abs/2502.14464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14464">https://arxiv.org/pdf/2502.14464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14464]] PQBFL: A Post-Quantum Blockchain-based Protocol for Federated Learning(https://arxiv.org/abs/2502.14464)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate</a></li>
<li><strong>Abstract: </strong>One of the goals of Federated Learning (FL) is to collaboratively train a global model using local models from remote participants. However, the FL process is susceptible to various security challenges, including interception and tampering models, information leakage through shared gradients, and privacy breaches that expose participant identities or data, particularly in sensitive domains such as medical environments. Furthermore, the advent of quantum computing poses a critical threat to existing cryptographic protocols through the Shor and Grover algorithms, causing security concerns in the communication of FL systems. To address these challenges, we propose a Post-Quantum Blockchain-based protocol for Federated Learning (PQBFL) that utilizes post-quantum cryptographic (PQC) algorithms and blockchain to enhance model security and participant identity privacy in FL systems. It employs a hybrid communication strategy that combines off-chain and on-chain channels to optimize cost efficiency, improve security, and preserve participant privacy while ensuring accountability for reputation-based authentication in FL systems. The PQBFL specifically addresses the security requirement for the iterative nature of FL, which is a less notable point in the literature. Hence, it leverages ratcheting mechanisms to provide forward secrecy and post-compromise security during all the rounds of the learning process. In conclusion, PQBFL provides a secure and resilient solution for federated learning that is well-suited to the quantum computing era.</li>
</ul>

<h3>Title: Enhancing Smart Environments with Context-Aware Chatbots using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aurora Polo-RodrÃ­guez, Laura Fiorini, Erika Rovini, Filippo Cavallo, Javier Medina-Quero</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14469">https://arxiv.org/abs/2502.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14469">https://arxiv.org/pdf/2502.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14469]] Enhancing Smart Environments with Context-Aware Chatbots using Large Language Models(https://arxiv.org/abs/2502.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work presents a novel architecture for context-aware interactions within smart environments, leveraging Large Language Models (LLMs) to enhance user experiences. Our system integrates user location data obtained through UWB tags and sensor-equipped smart homes with real-time human activity recognition (HAR) to provide a comprehensive understanding of user context. This contextual information is then fed to an LLM-powered chatbot, enabling it to generate personalised interactions and recommendations based on the user's current activity and environment. This approach moves beyond traditional static chatbot interactions by dynamically adapting to the user's real-time situation. A case study conducted from a real-world dataset demonstrates the feasibility and effectiveness of our proposed architecture, showcasing its potential to create more intuitive and helpful interactions within smart homes. The results highlight the significant benefits of integrating LLM with real-time activity and location data to deliver personalised and contextually relevant user experiences.</li>
</ul>

<h3>Title: Integrating Extra Modality Helps Segmentor Find Camouflaged Objects Well</h3>
<ul>
<li><strong>Authors: </strong>Chengyu Fang, Chunming He, Longxiang Tang, Yuelin Zhang, Chenyang Zhu, Yuqi Shen, Chubin Chen, Guoxia Xu, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14471">https://arxiv.org/abs/2502.14471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14471">https://arxiv.org/pdf/2502.14471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14471]] Integrating Extra Modality Helps Segmentor Find Camouflaged Objects Well(https://arxiv.org/abs/2502.14471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Camouflaged Object Segmentation (COS) remains a challenging problem due to the subtle visual differences between camouflaged objects and backgrounds. Owing to the exceedingly limited visual cues available from visible spectrum, previous RGB single-modality approaches often struggle to achieve satisfactory results, prompting the exploration of multimodal data to enhance detection accuracy. In this work, we present UniCOS, a novel framework that effectively leverages diverse data modalities to improve segmentation performance. UniCOS comprises two key components: a multimodal segmentor, UniSEG, and a cross-modal knowledge learning module, UniLearner. UniSEG employs a state space fusion mechanism to integrate cross-modal features within a unified state space, enhancing contextual understanding and improving robustness to integration of heterogeneous data. Additionally, it includes a fusion-feedback mechanism that facilitate feature extraction. UniLearner exploits multimodal data unrelated to the COS task to improve the segmentation ability of the COS models by generating pseudo-modal content and cross-modal semantic associations. Extensive experiments demonstrate that UniSEG outperforms existing Multimodal COS (MCOS) segmentors, regardless of whether real or pseudo-multimodal COS data is available. Moreover, in scenarios where multimodal COS data is unavailable but multimodal non-COS data is accessible, UniLearner effectively exploits these data to enhance segmentation performance. Our code will be made publicly available on \href{this https URL}{GitHub}.</li>
</ul>

<h3>Title: Argument-Based Comparative Question Answering Evaluation Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Irina Nikishina, Saba Anwar, Nikolay Dolgov, Maria Manina, Daria Ignatenko, Viktor Moskvoretskii, Artem Shelmanov, Tim Baldwin, Chris Biemann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14476">https://arxiv.org/abs/2502.14476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14476">https://arxiv.org/pdf/2502.14476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14476]] Argument-Based Comparative Question Answering Evaluation Benchmark(https://arxiv.org/abs/2502.14476)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we aim to solve the problems standing in the way of automatic comparative question answering. To this end, we propose an evaluation framework to assess the quality of comparative question answering summaries. We formulate 15 criteria for assessing comparative answers created using manual annotation and annotation from 6 large language models and two comparative question asnwering datasets. We perform our tests using several LLMs and manual annotation under different settings and demonstrate the constituency of both evaluations. Our results demonstrate that the Llama-3 70B Instruct model demonstrates the best results for summary evaluation, while GPT-4 is the best for answering comparative questions. All used data, code, and evaluation results are publicly available\footnote{\url{this https URL}}.</li>
</ul>

<h3>Title: Unshackling Context Length: An Efficient Selective Attention Approach through Query-Key Compression</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Wang, Tong Teng, Tianyu Guo, An Xiao, Duyu Tang, Hanting Chen, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14477">https://arxiv.org/abs/2502.14477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14477">https://arxiv.org/pdf/2502.14477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14477]] Unshackling Context Length: An Efficient Selective Attention Approach through Query-Key Compression(https://arxiv.org/abs/2502.14477)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Handling long-context sequences efficiently remains a significant challenge in large language models (LLMs). Existing methods for token selection in sequence extrapolation either employ a permanent eviction strategy or select tokens by chunk, which may lead to the loss of critical information. We propose Efficient Selective Attention (ESA), a novel approach that extends context length by efficiently selecting the most critical tokens at the token level to compute attention. ESA reduces the computational complexity of token selection by compressing query and key vectors into lower-dimensional representations. We evaluate ESA on long sequence benchmarks with maximum lengths up to 256k using open-source LLMs with context lengths of 8k and 32k. ESA outperforms other selective attention methods, especially in tasks requiring the retrieval of multiple pieces of information, achieving comparable performance to full-attention extrapolation methods across various tasks, with superior results in certain tasks.</li>
</ul>

<h3>Title: NLoRA: NystrÃ¶m-Initiated Low-Rank Adaptation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenlu Guo, Yuan Wu, Yi Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14482">https://arxiv.org/abs/2502.14482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14482">https://arxiv.org/pdf/2502.14482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14482]] NLoRA: NystrÃ¶m-Initiated Low-Rank Adaptation for Large Language Models(https://arxiv.org/abs/2502.14482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) is essential for adapting large language models (LLMs), with low-rank adaptation (LoRA) being the most popular approach. However, LoRA suffers from slow convergence, and some recent LoRA variants, such as PiSSA, primarily rely on Singular Value Decomposition (SVD) for initialization, leading to expensive computation. To mitigate these problems, we use the NystrÃ¶m method, which follows a three-matrix manipulation. We first introduce StructuredLoRA (SLoRA), which investigates adding a small intermediate matrix between the low-rank matrices A and B. Secondly, we propose NystrÃ¶mLoRA (NLoRA), which leverages NystrÃ¶m-based initialization for SLoRA to improve its effectiveness and efficiency. Finally, we propose IntermediateTune (IntTune), which explores fine-tuning exclusively on the intermediate matrix of NLoRA to further boost LLM efficiency. We evaluate our methods on five natural language generation (NLG) tasks and eight natural language understanding (NLU) tasks. On GSM8K, SLoRA and NLoRA achieve accuracies of 56.48% and 57.70%, surpassing LoRA by 33.52% and 36.41%, with only 3.67 million additional trainable parameters. IntTune improves average NLG performance over LoRA by 7.45% while using only 1.25% of its parameters. These results demonstrate the efficiency and effectiveness of our approach in enhancing model performance with minimal parameter overhead.</li>
</ul>

<h3>Title: How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation</h3>
<ul>
<li><strong>Authors: </strong>Zhuohang Long, Siyuan Wang, Shujun Liu, Yuhang Lai, Xuanjing Huang, Zhongyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14486">https://arxiv.org/abs/2502.14486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14486">https://arxiv.org/pdf/2502.14486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14486]] How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation(https://arxiv.org/abs/2502.14486)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, generative</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks, where harmful prompts bypass generative models' built-in safety, raise serious concerns about model vulnerability. While many defense methods have been proposed, the trade-offs between safety and helpfulness, and their application to Large Vision-Language Models (LVLMs), are not well understood. This paper systematically examines jailbreak defenses by reframing the standard generation task as a binary classification problem to assess model refusal tendencies for both harmful and benign queries. We identify two key defense mechanisms: safety shift, which increases refusal rates across all queries, and harmfulness discrimination, which improves the model's ability to distinguish between harmful and benign inputs. Using these mechanisms, we develop two ensemble defense strategies-inter-mechanism ensembles and intra-mechanism ensembles-to balance safety and helpfulness. Experiments on the MM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these strategies effectively improve model safety or optimize the trade-off between safety and helpfulness.</li>
</ul>

<h3>Title: CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor Top-K Vision Alignment and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Yukai Shi, Cidan Shi, Zhipeng Weng, Yin Tian, Xiaoyu Xian, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14493">https://arxiv.org/abs/2502.14493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14493">https://arxiv.org/pdf/2502.14493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14493]] CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor Top-K Vision Alignment and Beyond(https://arxiv.org/abs/2502.14493)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Infrared and visible image fusion (IVIF) is increasingly applied in critical fields such as video surveillance and autonomous driving systems. Significant progress has been made in deep learning-based fusion methods. However, these models frequently encounter out-of-distribution (OOD) scenes in real-world applications, which severely impact their performance and reliability. Therefore, addressing the challenge of OOD data is crucial for the safe deployment of these models in open-world environments. Unlike existing research, our focus is on the challenges posed by OOD data in real-world applications and on enhancing the robustness and generalization of models. In this paper, we propose an infrared-visible fusion framework based on Multi-View Augmentation. For external data augmentation, Top-k Selective Vision Alignment is employed to mitigate distribution shifts between datasets by performing RGB-wise transformations on visible images. This strategy effectively introduces augmented samples, enhancing the adaptability of the model to complex real-world scenarios. Additionally, for internal data augmentation, self-supervised learning is established using Weak-Aggressive Augmentation. This enables the model to learn more robust and general feature representations during the fusion process, thereby improving robustness and generalization. Extensive experiments demonstrate that the proposed method exhibits superior performance and robustness across various conditions and environments. Our approach significantly enhances the reliability and stability of IVIF tasks in practical applications.</li>
</ul>

<h3>Title: StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Jinnan Li, Jinzhe Li, Yue Wang, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14494">https://arxiv.org/abs/2502.14494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14494">https://arxiv.org/pdf/2502.14494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14494]] StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following(https://arxiv.org/abs/2502.14494)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multi-turn from single-turn interactions. This structural dependency not only reflects user intent but also establishes a second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines a structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Nearshore Underwater Target Detection Meets UAV-borne Hyperspectral Remote Sensing: A Novel Hybrid-level Contrastive Learning Framework and Benchmark Dataset</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Qi, Chuanhong Zhou, Xingyue Liu, Chen Chen, Dehui Zhu, Kangcheng Bin, Ping Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14495">https://arxiv.org/abs/2502.14495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14495">https://arxiv.org/pdf/2502.14495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14495]] Nearshore Underwater Target Detection Meets UAV-borne Hyperspectral Remote Sensing: A Novel Hybrid-level Contrastive Learning Framework and Benchmark Dataset(https://arxiv.org/abs/2502.14495)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>UAV-borne hyperspectral remote sensing has emerged as a promising approach for underwater target detection (UTD). However, its effectiveness is hindered by spectral distortions in nearshore environments, which compromise the accuracy of traditional hyperspectral UTD (HUTD) methods that rely on bathymetric model. These distortions lead to significant uncertainty in target and background spectra, challenging the detection process. To address this, we propose the Hyperspectral Underwater Contrastive Learning Network (HUCLNet), a novel framework that integrates contrastive learning with a self-paced learning paradigm for robust HUTD in nearshore regions. HUCLNet extracts discriminative features from distorted hyperspectral data through contrastive learning, while the self-paced learning strategy selectively prioritizes the most informative samples. Additionally, a reliability-guided clustering strategy enhances the robustness of learned this http URL evaluate the method effectiveness, we conduct a novel nearshore HUTD benchmark dataset, ATR2-HUTD, covering three diverse scenarios with varying water types and turbidity, and target types. Extensive experiments demonstrate that HUCLNet significantly outperforms state-of-the-art methods. The dataset and code will be publicly available at: this https URL</li>
</ul>

<h3>Title: MLGym: A New Framework and Benchmark for Advancing AI Research Agents</h3>
<ul>
<li><strong>Authors: </strong>Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang Wang, Roberta Raileanu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14499">https://arxiv.org/abs/2502.14499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14499">https://arxiv.org/pdf/2502.14499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14499]] MLGym: A New Framework and Benchmark for Advancing AI Research Agents(https://arxiv.org/abs/2502.14499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.</li>
</ul>

<h3>Title: How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?</h3>
<ul>
<li><strong>Authors: </strong>Sergey Pletenev, Maria Marina, Daniil Moskovskiy, Vasily Konovalov, Pavel Braslavski, Alexander Panchenko, Mikhail Salnikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14502">https://arxiv.org/abs/2502.14502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14502">https://arxiv.org/pdf/2502.14502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14502]] How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?(https://arxiv.org/abs/2502.14502)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.</li>
</ul>

<h3>Title: LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of 4D Radar and Camera</h3>
<ul>
<li><strong>Authors: </strong>Weiyi Xiong, Zean Zou, Qiuchi Zhao, Fengchun He, Bing Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14503">https://arxiv.org/abs/2502.14503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14503">https://arxiv.org/pdf/2502.14503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14503]] LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of 4D Radar and Camera(https://arxiv.org/abs/2502.14503)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As the previous state-of-the-art 4D radar-camera fusion-based 3D object detection method, LXL utilizes the predicted image depth distribution maps and radar 3D occupancy grids to assist the sampling-based image view transformation. However, the depth prediction lacks accuracy and consistency, and the concatenation-based fusion in LXL impedes the model robustness. In this work, we propose LXLv2, where modifications are made to overcome the limitations and improve the performance. Specifically, considering the position error in radar measurements, we devise a one-to-many depth supervision strategy via radar points, where the radar cross section (RCS) value is further exploited to adjust the supervision area for object-level depth consistency. Additionally, a channel and spatial attention-based fusion module named CSAFusion is introduced to improve feature adaptiveness. Experimental results on the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can outperform LXL in detection accuracy, inference speed and robustness, demonstrating the effectiveness of the model.</li>
</ul>

<h3>Title: Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases</h3>
<ul>
<li><strong>Authors: </strong>Rena Gao, Xuetong Wu, Tatsuki Kuribayashi, Mingrui Ye, Siya Qi, Carsten Roever, Yuanxing Liu, Zheng Yuan, Jey Han Lau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14507">https://arxiv.org/abs/2502.14507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14507">https://arxiv.org/pdf/2502.14507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14507]] Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases(https://arxiv.org/abs/2502.14507)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates Large Language Models' (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1). In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data. Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures. Results show that modern LLMs (e.g., Qwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations). Our results reveal the potential of LLMs for L2 dialogue generation and evaluation for future educational applications.</li>
</ul>

<h3>Title: Generative adversarial networks vs large language models: a comparative study on synthetic tabular data generation</h3>
<ul>
<li><strong>Authors: </strong>Austin A. Barr, Robert Rozman, Eddie Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14523">https://arxiv.org/abs/2502.14523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14523">https://arxiv.org/pdf/2502.14523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14523]] Generative adversarial networks vs large language models: a comparative study on synthetic tabular data generation(https://arxiv.org/abs/2502.14523)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>We propose a new framework for zero-shot generation of synthetic tabular data. Using the large language model (LLM) GPT-4o and plain-language prompting, we demonstrate the ability to generate high-fidelity tabular data without task-specific fine-tuning or access to real-world data (RWD) for pre-training. To benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated synthetic data against data generated with the conditional tabular generative adversarial network (CTGAN), across three open-access datasets: Iris, Fish Measurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o outperformed CTGAN in preserving means, 95% confidence intervals, bivariate correlations, and data privacy of RWD, even at amplified sample sizes. Notably, correlations between parameters were consistently preserved with appropriate direction and strength. However, refinement is necessary to better retain distributional characteristics. These findings highlight the potential of LLMs in tabular data synthesis, offering an accessible alternative to generative adversarial networks and variational autoencoders.</li>
</ul>

<h3>Title: CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenhong Zhou, Zherui Li, Jie Zhang, Yuanhe Zhang, Kun Wang, Yang Liu, Qing Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14529">https://arxiv.org/abs/2502.14529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14529">https://arxiv.org/pdf/2502.14529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14529]] CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models(https://arxiv.org/abs/2502.14529)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: this https URL.</li>
</ul>

<h3>Title: LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Chang, Chenlu Guo, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14538">https://arxiv.org/abs/2502.14538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14538">https://arxiv.org/pdf/2502.14538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14538]] LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization(https://arxiv.org/abs/2502.14538)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success in natural language processing, but their full fine-tuning remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), have emerged as a practical solution by approximating parameter updates with low-rank matrices. However, LoRA often exhibits a "double descent" phenomenon during fine-tuning, where model performance degrades due to overfitting and limited expressiveness caused by low-rank constraints. To address this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation Optimization), a novel method that leverages gradient and weight norms to generate targeted perturbations. By optimizing the sharpness of the loss landscape, LoRA-GGPO guides the model toward flatter minima, mitigating the double descent problem and improving generalization. Extensive experiments on natural language understanding (NLU) and generation (NLG) tasks demonstrate that LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore, extended experiments specifically designed to analyze the double descent phenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing more robust and generalizable models. Our work provides a robust and efficient solution for fine-tuning LLMs, with broad applicability in real-world scenarios. The code is available at this https URL.</li>
</ul>

<h3>Title: LLM-based User Profile Management for Recommender System</h3>
<ul>
<li><strong>Authors: </strong>Seunghwan Bang, Hwanjun Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14541">https://arxiv.org/abs/2502.14541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14541">https://arxiv.org/pdf/2502.14541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14541]] LLM-based User Profile Management for Recommender System(https://arxiv.org/abs/2502.14541)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.</li>
</ul>

<h3>Title: Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Eric Egli, Matteo Manica, Jannis Born</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14553">https://arxiv.org/abs/2502.14553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14553">https://arxiv.org/pdf/2502.14553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14553]] Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling(https://arxiv.org/abs/2502.14553)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Bytes form the basis of the digital world and thus are a promising building block for multimodal foundation models. Recently, Byte Language Models (BLMs) have emerged to overcome tokenization, yet the excessive length of bytestreams requires new architectural paradigms. Therefore, we present the Multiscale Byte Language Model (MBLM), a model-agnostic hierarchical decoder stack that allows training with context windows of $5$M bytes on single GPU in full model precision. We thoroughly examine MBLM's performance with Transformer and Mamba blocks on both unimodal and multimodal tasks. Our experiments demonstrate that hybrid architectures are efficient in handling extremely long byte sequences during training while achieving near-linear generational efficiency. To the best of our knowledge, we present the first evaluation of BLMs on visual Q\&A tasks and find that, despite serializing images and the absence of an encoder, a MBLM with pure next token prediction can match custom CNN-LSTM architectures with designated classification heads. We show that MBLMs exhibit strong adaptability in integrating diverse data representations, including pixel and image filestream bytes, underlining their potential toward omnimodal foundation models. Source code is publicly available at: this https URL</li>
</ul>

<h3>Title: FUIA: Model Inversion Attack against Federated Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Lei Zhou, Youwen Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14558">https://arxiv.org/abs/2502.14558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14558">https://arxiv.org/pdf/2502.14558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14558]] FUIA: Model Inversion Attack against Federated Unlearning(https://arxiv.org/abs/2502.14558)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>With the introduction of regulations related to the ``right to be forgotten", federated learning (FL) is facing new privacy compliance challenges. To address these challenges, researchers have proposed federated unlearning (FU). However, existing FU research has primarily focused on improving the efficiency of unlearning, with less attention paid to the potential privacy vulnerabilities inherent in these methods. To address this gap, we draw inspiration from gradient inversion attacks in FL and propose the federated unlearning inversion attack (FUIA). The FUIA is specifically designed for the three types of FU (sample unlearning, client unlearning, and class unlearning), aiming to provide a comprehensive analysis of the privacy leakage risks associated with FU. In FUIA, the server acts as an honest-but-curious attacker, recording and exploiting the model differences before and after unlearning to expose the features and labels of forgotten data. FUIA significantly leaks the privacy of forgotten data and can target all types of FU. This attack contradicts the goal of FU to eliminate specific data influence, instead exploiting its vulnerabilities to recover forgotten data and expose its privacy flaws. Extensive experimental results show that FUIA can effectively reveal the private information of forgotten data. To mitigate this privacy leakage, we also explore two potential defense methods, although these come at the cost of reduced unlearning effectiveness and the usability of the unlearned model.</li>
</ul>

<h3>Title: Less is More: Improving LLM Alignment via Preference Data Selection</h3>
<ul>
<li><strong>Authors: </strong>Xun Deng, Han Zhong, Rui Ai, Fuli Feng, Zheng Wang, Xiangnan He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14560">https://arxiv.org/abs/2502.14560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14560">https://arxiv.org/pdf/2502.14560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14560]] Less is More: Improving LLM Alignment via Preference Data Selection(https://arxiv.org/abs/2502.14560)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To accurately estimate margins for data selection, we propose a dual-margin guided approach that considers both external reward margins and implicit DPO reward margins. Extensive experiments demonstrate that our method reduces computational cost dramatically while improving performance. Remarkably, by using just 10\% of the Ultrafeedback dataset, our approach achieves 3\% to 8\% improvements across various Llama and Mistral series models on the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\% improvement with 25\% online data, while further reducing training time. These results highlight the potential of data selection strategies for advancing preference optimization.</li>
</ul>

<h3>Title: Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs</h3>
<ul>
<li><strong>Authors: </strong>Paris Koloveas, Serafeim Chatzopoulos, Thanasis Vergoulis, Christos Tryfonopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14561">https://arxiv.org/abs/2502.14561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14561">https://arxiv.org/pdf/2502.14561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14561]] Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs(https://arxiv.org/abs/2502.14561)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work investigates the ability of open Large Language Models (LLMs) to predict citation intent through in-context learning and fine-tuning. Unlike traditional approaches that rely on pre-trained models like SciBERT, which require extensive domain-specific pretraining and specialized architectures, we demonstrate that general-purpose LLMs can be adapted to this task with minimal task-specific data. We evaluate twelve model variations across five prominent open LLM families using zero, one, few, and many-shot prompting to assess performance across scenarios. Our experimental study identifies the top-performing model through extensive experimentation of in-context learning-related parameters, which we fine-tune to further enhance task performance. The results highlight the strengths and limitations of LLMs in recognizing citation intents, providing valuable insights for model selection and prompt engineering. Additionally, we make our end-to-end evaluation framework and models openly available for future use.</li>
</ul>

<h3>Title: ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification</h3>
<ul>
<li><strong>Authors: </strong>Hyunseok Lee, Seunghyuk Oh, Jaehyung Kim, Jinwoo Shin, Jihoon Tack</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14565">https://arxiv.org/abs/2502.14565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14565">https://arxiv.org/pdf/2502.14565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14565]] ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification(https://arxiv.org/abs/2502.14565)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-awareness, i.e., the ability to assess and correct one's own generation, is a fundamental aspect of human intelligence, making its replication in large language models (LLMs) an important yet challenging task. Previous works tackle this by employing extensive reinforcement learning or rather relying on large external verifiers. In this work, we propose Refine via Intrinsic Self-Verification (ReVISE), an efficient and effective framework that enables LLMs to self-correct their outputs through self-verification. The core idea of ReVISE is to enable LLMs to verify their reasoning processes and continually rethink reasoning trajectories based on its verification. We introduce a structured curriculum based upon online preference learning to implement this efficiently. Specifically, as ReVISE involves two challenging tasks (i.e., self-verification and reasoning correction), we tackle each task sequentially using curriculum learning, collecting both failed and successful reasoning paths to construct preference pairs for efficient training. During inference, our approach enjoys natural test-time scaling by integrating self-verification and correction capabilities, further enhanced by our proposed confidence-aware decoding mechanism. Our experiments on various reasoning tasks demonstrate that ReVISE achieves efficient self-correction and significantly improves reasoning performance.</li>
</ul>

<h3>Title: Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining</h3>
<ul>
<li><strong>Authors: </strong>Wonhyeok Choi, Kyumin Hwang, Wei Peng, Minwoo Choi, Sunghoon Im</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14573">https://arxiv.org/abs/2502.14573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14573">https://arxiv.org/pdf/2502.14573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14573]] Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining(https://arxiv.org/abs/2502.14573)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Self-supervised monocular depth estimation (SSMDE) aims to predict the dense depth map of a monocular image, by learning depth from RGB image sequences, eliminating the need for ground-truth depth labels. Although this approach simplifies data acquisition compared to supervised methods, it struggles with reflective surfaces, as they violate the assumptions of Lambertian reflectance, leading to inaccurate training on such surfaces. To tackle this problem, we propose a novel training strategy for an SSMDE by leveraging triplet mining to pinpoint reflective regions at the pixel level, guided by the camera geometry between different viewpoints. The proposed reflection-aware triplet mining loss specifically penalizes the inappropriate photometric error minimization on the localized reflective regions while preserving depth accuracy in non-reflective areas. We also incorporate a reflection-aware knowledge distillation method that enables a student model to selectively learn the pixel-level knowledge from reflective and non-reflective regions. This results in robust depth estimation across areas. Evaluation results on multiple datasets demonstrate that our method effectively enhances depth quality on reflective surfaces and outperforms state-of-the-art SSMDE baselines.</li>
</ul>

<h3>Title: A Theory for Conditional Generative Modeling on Multiple Data Sources</h3>
<ul>
<li><strong>Authors: </strong>Rongzhen Wang, Yan Zhang, Chenyu Zheng, Chongxuan Li, Guoqiang Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14583">https://arxiv.org/abs/2502.14583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14583">https://arxiv.org/pdf/2502.14583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14583]] A Theory for Conditional Generative Modeling on Multiple Data Sources(https://arxiv.org/abs/2502.14583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities. However, the interaction among these sources remains theoretically underexplored. This paper takes the first step toward a rigorous analysis of multi-source training in conditional generative modeling, where each condition represents a distinct data source. Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number. Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training. We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers. The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training. Simulations and real-world experiments validate our theory. Code is available at: \url{this https URL}.</li>
</ul>

<h3>Title: Moshi Moshi? A Model Selection Hijacking Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Petrucci, Luca Pajola, Francesco Marchiori, Luca Pasa, Mauro conti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14586">https://arxiv.org/abs/2502.14586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14586">https://arxiv.org/pdf/2502.14586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14586]] Moshi Moshi? A Model Selection Hijacking Adversarial Attack(https://arxiv.org/abs/2502.14586)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Model selection is a fundamental task in Machine Learning~(ML), focusing on selecting the most suitable model from a pool of candidates by evaluating their performance on specific metrics. This process ensures optimal performance, computational efficiency, and adaptability to diverse tasks and environments. Despite its critical role, its security from the perspective of adversarial ML remains unexplored. This risk is heightened in the Machine-Learning-as-a-Service model, where users delegate the training phase and the model selection process to third-party providers, supplying data and training strategies. Therefore, attacks on model selection could harm both the user and the provider, undermining model performance and driving up operational costs. In this work, we present MOSHI (MOdel Selection HIjacking adversarial attack), the first adversarial attack specifically targeting model selection. Our novel approach manipulates model selection data to favor the adversary, even without prior knowledge of the system. Utilizing a framework based on Variational Auto Encoders, we provide evidence that an attacker can induce inefficiencies in ML deployment. We test our attack on diverse computer vision and speech recognition benchmark tasks and different settings, obtaining an average attack success rate of 75.42%. In particular, our attack causes an average 88.30% decrease in generalization capabilities, an 83.33% increase in latency, and an increase of up to 105.85% in energy consumption. These results highlight the significant vulnerabilities in model selection processes and their potential impact on real-world applications.</li>
</ul>

<h3>Title: Behavioral Analysis of Information Salience in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jan Trienes, JÃ¶rg SchlÃ¶tterer, Junyi Jessy Li, Christin Seifert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14613">https://arxiv.org/abs/2502.14613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14613">https://arxiv.org/pdf/2502.14613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14613]] Behavioral Analysis of Information Salience in Large Language Models(https://arxiv.org/abs/2502.14613)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at text summarization, a task that requires models to select content based on its importance. However, the exact notion of salience that LLMs have internalized remains unclear. To bridge this gap, we introduce an explainable framework to systematically derive and investigate information salience in LLMs through their summarization behavior. Using length-controlled summarization as a behavioral probe into the content selection process, and tracing the answerability of Questions Under Discussion throughout, we derive a proxy for how models prioritize information. Our experiments on 13 models across four datasets reveal that LLMs have a nuanced, hierarchical notion of salience, generally consistent across model families and sizes. While models show highly consistent behavior and hence salience patterns, this notion of salience cannot be accessed through introspection, and only weakly correlates with human perceptions of information salience.</li>
</ul>

<h3>Title: FIND: Fine-grained Information Density Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14614">https://arxiv.org/abs/2502.14614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14614">https://arxiv.org/pdf/2502.14614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14614]] FIND: Fine-grained Information Density Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis(https://arxiv.org/abs/2502.14614)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Large Language Models (LLMs), which integrate external knowledge into LLMs, have shown remarkable performance in various medical domains, including clinical diagnosis. However, existing RAG methods struggle to effectively assess task difficulty to make retrieval decisions, thereby failing to meet the clinical requirements for balancing efficiency and accuracy. So in this paper, we propose FIND (\textbf{F}ine-grained \textbf{In}formation \textbf{D}ensity Guided Adaptive RAG), a novel framework that improves the reliability of RAG in disease diagnosis scenarios. FIND incorporates a fine-grained adaptive control module to determine whether retrieval is necessary based on the information density of the input. By optimizing the retrieval process and implementing a knowledge filtering module, FIND ensures that the retrieval is better suited to clinical scenarios. Experiments on three Chinese electronic medical record datasets demonstrate that FIND significantly outperforms various baseline methods, highlighting its effectiveness in clinical diagnosis tasks.</li>
</ul>

<h3>Title: Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jiangyuan Liu, Hongxuan Ma, Yuxin Guo, Yuhao Zhao, Chi Zhang, Wei Sui, Wei Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14616">https://arxiv.org/abs/2502.14616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14616">https://arxiv.org/pdf/2502.14616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14616]] Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion(https://arxiv.org/abs/2502.14616)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Transparent object perception is indispensable for numerous robotic tasks. However, accurately segmenting and estimating the depth of transparent objects remain challenging due to complex optical properties. Existing methods primarily delve into only one task using extra inputs or specialized sensors, neglecting the valuable interactions among tasks and the subsequent refinement process, leading to suboptimal and blurry predictions. To address these issues, we propose a monocular framework, which is the first to excel in both segmentation and depth estimation of transparent objects, with only a single-image input. Specifically, we devise a novel semantic and geometric fusion module, effectively integrating the multi-scale information between tasks. In addition, drawing inspiration from human perception of objects, we further incorporate an iterative strategy, which progressively refines initial features for clearer results. Experiments on two challenging synthetic and real-world datasets demonstrate that our model surpasses state-of-the-art monocular, stereo, and multi-view methods by a large margin of about 38.8%-46.2% with only a single RGB input. Codes and models are publicly available at this https URL.</li>
</ul>

<h3>Title: Reward Models Identify Consistency, Not Causality</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Xu, Hanze Dong, Lei Wang, Caiming Xiong, Junnan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14619">https://arxiv.org/abs/2502.14619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14619">https://arxiv.org/pdf/2502.14619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14619]] Reward Models Identify Consistency, Not Causality(https://arxiv.org/abs/2502.14619)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) play a crucial role in aligning large language models (LLMs) with human preferences and enhancing reasoning quality. Traditionally, RMs are trained to rank candidate outputs based on their correctness and coherence. However, in this work, we present several surprising findings that challenge common assumptions about RM behavior. Our analysis reveals that state-of-the-art reward models prioritize structural consistency over causal correctness. Specifically, removing the problem statement has minimal impact on reward scores, whereas altering numerical values or disrupting the reasoning flow significantly affects RM outputs. Furthermore, RMs exhibit a strong dependence on complete reasoning trajectories truncated or incomplete steps lead to significant variations in reward assignments, indicating that RMs primarily rely on learned reasoning patterns rather than explicit problem comprehension. These findings hold across multiple architectures, datasets, and tasks, leading to three key insights: (1) RMs primarily assess coherence rather than true reasoning quality; (2) The role of explicit problem comprehension in reward assignment is overstated; (3) Current RMs may be more effective at ranking responses than verifying logical validity. Our results suggest a fundamental limitation in existing reward modeling approaches, emphasizing the need for a shift toward causality-aware reward models that go beyond consistency-driven evaluation.</li>
</ul>

<h3>Title: Multi-Record Web Page Information Extraction From News Websites</h3>
<ul>
<li><strong>Authors: </strong>Alexander Kustenkov, Maksim Varlamov, Alexander Yatskov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14625">https://arxiv.org/abs/2502.14625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14625">https://arxiv.org/pdf/2502.14625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14625]] Multi-Record Web Page Information Extraction From News Websites(https://arxiv.org/abs/2502.14625)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this paper, we focused on the problem of extracting information from web pages containing many records, a task of growing importance in the era of massive web data. Recently, the development of neural network methods has improved the quality of information extraction from web pages. Nevertheless, most of the research and datasets are aimed at studying detailed pages. This has left multi-record "list pages" relatively understudied, despite their widespread presence and practical significance. To address this gap, we created a large-scale, open-access dataset specifically designed for list pages. This is the first dataset for this task in the Russian language. Our dataset contains 13,120 web pages with news lists, significantly exceeding existing datasets in both scale and complexity. Our dataset contains attributes of various types, including optional and multi-valued, providing a realistic representation of real-world list pages. These features make our dataset a valuable resource for studying information extraction from pages containing many records. Furthermore, we proposed our own multi-stage information extraction methods. In this work, we explore and demonstrate several strategies for applying MarkupLM to the specific challenges of multi-record web pages. Our experiments validate the advantages of our methods. By releasing our dataset to the public, we aim to advance the field of information extraction from multi-record pages.</li>
</ul>

<h3>Title: PEARL: Towards Permutation-Resilient LLMs</h3>
<ul>
<li><strong>Authors: </strong>Liang Chen, Li Shen, Yang Deng, Xiaoyan Zhao, Bin Liang, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14628">https://arxiv.org/abs/2502.14628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14628">https://arxiv.org/pdf/2502.14628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14628]] PEARL: Towards Permutation-Resilient LLMs(https://arxiv.org/abs/2502.14628)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack - difficult for model providers to detect - that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the model's inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM's robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities.</li>
</ul>

<h3>Title: Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for High-Entropy Alloy Discovery</h3>
<ul>
<li><strong>Authors: </strong>Minh-Quyet Ha, Dinh-Khiet Le, Duc-Anh Dao, Tien-Sinh Vu, Duong-Nguyen Nguyen, Viet-Cuong Nguyen, Hiori Kino, Van-Nam Huynh, Hieu-Chi Dam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14631">https://arxiv.org/abs/2502.14631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14631">https://arxiv.org/pdf/2502.14631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14631]] Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for High-Entropy Alloy Discovery(https://arxiv.org/abs/2502.14631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Discovering novel high-entropy alloys (HEAs) with desirable properties is challenging due to the vast compositional space and complex phase formation mechanisms. Efficient exploration of this space requires a strategic approach that integrates heterogeneous knowledge sources. Here, we propose a framework that systematically combines knowledge extracted from computational material datasets with domain knowledge distilled from scientific literature using large language models (LLMs). A central feature of this approach is the explicit consideration of element substitutability, identifying chemically similar elements that can be interchanged to potentially stabilize desired HEAs. Dempster-Shafer theory, a mathematical framework for reasoning under uncertainty, is employed to model and combine substitutabilities based on aggregated evidence from multiple sources. The framework predicts the phase stability of candidate HEA compositions and is systematically evaluated on both quaternary alloy systems, demonstrating superior performance compared to baseline machine learning models and methods reliant on single-source evidence in cross-validation experiments. By leveraging multi-source knowledge, the framework retains robust predictive power even when key elements are absent from the training data, underscoring its potential for knowledge transfer and extrapolation. Furthermore, the enhanced interpretability of the methodology offers insights into the fundamental factors governing HEA formation. Overall, this work provides a promising strategy for accelerating HEA discovery by integrating computational and textual knowledge sources, enabling efficient exploration of vast compositional spaces with improved generalization and interpretability.</li>
</ul>

<h3>Title: CER: Confidence Enhanced Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ali Razghandi, Seyed Mohammad Hadi Hosseini, Mahdieh Soleymani Baghshah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14634">https://arxiv.org/abs/2502.14634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14634">https://arxiv.org/pdf/2502.14634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14634]] CER: Confidence Enhanced Reasoning in LLMs(https://arxiv.org/abs/2502.14634)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the reliability of Large Language Models (LLMs) in complex reasoning tasks remains a formidable challenge, particularly in scenarios that demand precise mathematical calculations and knowledge-intensive open-domain generation. In this work, we introduce an uncertainty-aware framework designed to enhance the accuracy of LLM responses by systematically incorporating model confidence at critical decision points. We propose an approach that encourages multi-step reasoning in LLMs and quantify the confidence of intermediate answers such as numerical results in mathematical reasoning and proper nouns in open-domain generation. Then, the overall confidence of each reasoning chain is evaluated based on confidence of these critical intermediate steps. Finally, we aggregate the answer of generated response paths in a way that reflects the reliability of each generated content (as opposed to self-consistency in which each generated chain contributes equally to majority voting). We conducted extensive experiments in five datasets, three mathematical datasets and two open-domain datasets, using four LLMs. The results consistently validate the effectiveness of our novel confidence aggregation method, leading to an accuracy improvement of up to 7.4% and 5.8% over baseline approaches in math and open-domain generation tasks, respectively. Code is publicly available at this https URL Aquasar11/CER.</li>
</ul>

<h3>Title: ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation</h3>
<ul>
<li><strong>Authors: </strong>Angxiao Yue, Zichong Wang, Hongteng Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14637">https://arxiv.org/abs/2502.14637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14637">https://arxiv.org/pdf/2502.14637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14637]] ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation(https://arxiv.org/abs/2502.14637)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications. Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency. In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format. We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves state-of-the-art performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37x faster than RFDiffusion and 62x faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at this https URL.</li>
</ul>

<h3>Title: Length-Controlled Margin-Based Preference Optimization without Reference Model</h3>
<ul>
<li><strong>Authors: </strong>Gengxu Li, Tingyu Xia, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14643">https://arxiv.org/abs/2502.14643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14643">https://arxiv.org/pdf/2502.14643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14643]] Length-Controlled Margin-Based Preference Optimization without Reference Model(https://arxiv.org/abs/2502.14643)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), designed to improve training simplicity and stability by redefining reward functions. However, DPO is hindered by several limitations, including length bias, memory inefficiency, and probability degradation. To address these challenges, we propose Length-Controlled Margin-Based Preference Optimization (LMPO), a more efficient and robust alternative. LMPO introduces a uniform reference model as an upper bound for the DPO loss, enabling a more accurate approximation of the original optimization objective. Additionally, an average log-probability optimization strategy is employed to minimize discrepancies between training and inference phases. A key innovation of LMPO lies in its Length-Controlled Margin-Based loss function, integrated within the Bradley-Terry framework. This loss function regulates response length while simultaneously widening the margin between preferred and rejected outputs. By doing so, it mitigates probability degradation for both accepted and discarded responses, addressing a significant limitation of existing methods. We evaluate LMPO against state-of-the-art preference optimization techniques on two open-ended large language models, Mistral and LLaMA3, across six conditional benchmarks. Our experimental results demonstrate that LMPO effectively controls response length, reduces probability degradation, and outperforms existing approaches. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yansheng Mao, Yufei Xu, Jiaqi Li, Fanxu Meng, Haotong Yang, Zilong Zheng, Xiyuan Wang, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14644">https://arxiv.org/abs/2502.14644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14644">https://arxiv.org/pdf/2502.14644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14644]] LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning(https://arxiv.org/abs/2502.14644)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long context understanding remains challenging for large language models due to their limited context windows. This paper presents Long Input Fine-Tuning (LIFT), a novel framework for long-context modeling that can improve the long-context performance of arbitrary (short-context) LLMs by dynamically adapting model parameters based on the long input. Importantly, LIFT, rather than endlessly extending the context window size to accommodate increasingly longer inputs in context, chooses to store and absorb the long input in parameter. By fine-tuning the long input into model parameters, LIFT allows short-context LLMs to answer questions even when the required information is not provided in the context during inference. Furthermore, to enhance LIFT performance while maintaining the original in-context learning (ICL) capabilities, we introduce Gated Memory, a specialized attention adapter that automatically balances long input memorization and ICL. We provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.</li>
</ul>

<h3>Title: Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14645">https://arxiv.org/abs/2502.14645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14645">https://arxiv.org/pdf/2502.14645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14645]] Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs(https://arxiv.org/abs/2502.14645)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings.</li>
</ul>

<h3>Title: MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only VIBE MRI</h3>
<ul>
<li><strong>Authors: </strong>Robert Graf, Hendrik MÃ¶ller, Sophie Starck, Matan Atad, Philipp Braun, Jonathan Stelter, Annette Peters, Lilian Krist, Stefan N. Willich, Henry VÃ¶lzke, Robin BÃ¼low, Klaus Berger, Tobias Pischon, Thoralf Niendorf, Johannes Paetzold, Dimitrios Karampinos, Daniel Rueckert, Jan Kirschke</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14659">https://arxiv.org/abs/2502.14659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14659">https://arxiv.org/pdf/2502.14659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14659]] MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only VIBE MRI(https://arxiv.org/abs/2502.14659)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Volume Interpolated Breath-Hold Examination (VIBE) MRI generates images suitable for water and fat signal composition estimation. While the two-point VIBE provides water-fat-separated images, the six-point VIBE allows estimation of the effective transversal relaxation rate R2* and the proton density fat fraction (PDFF), which are imaging markers for health and disease. Ambiguity during signal reconstruction can lead to water-fat swaps. This shortcoming challenges the application of VIBE-MRI for automated PDFF analyses of large-scale clinical data and of population studies. This study develops an automated pipeline to detect and correct water-fat swaps in non-contrast-enhanced VIBE images. Our three-step pipeline begins with training a segmentation network to classify volumes as "fat-like" or "water-like," using synthetic water-fat swaps generated by merging fat and water volumes with Perlin noise. Next, a denoising diffusion image-to-image network predicts water volumes as signal priors for correction. Finally, we integrate this prior into a physics-constrained model to recover accurate water and fat signals. Our approach achieves a < 1% error rate in water-fat swap detection for a 6-point VIBE. Notably, swaps disproportionately affect individuals in the Underweight and Class 3 Obesity BMI categories. Our correction algorithm ensures accurate solution selection in chemical phase MRIs, enabling reliable PDFF estimation. This forms a solid technical foundation for automated large-scale population imaging analysis.</li>
</ul>

<h3>Title: Beyond the Surface: Uncovering Implicit Locations with LLMs for Personalized Local News</h3>
<ul>
<li><strong>Authors: </strong>Gali Katz, Hai Sitton, Guy Gonen, Yohay Kaplan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14660">https://arxiv.org/abs/2502.14660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14660">https://arxiv.org/pdf/2502.14660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14660]] Beyond the Surface: Uncovering Implicit Locations with LLMs for Personalized Local News(https://arxiv.org/abs/2502.14660)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>News recommendation systems personalize homepage content to boost engagement, but factors like content type, editorial stance, and geographic focus impact recommendations. Local newspapers balance coverage across regions, yet identifying local articles is challenging due to implicit location cues like slang or landmarks. Traditional methods, such as Named Entity Recognition (NER) and Knowledge Graphs, infer locations, but Large Language Models (LLMs) offer new possibilities while raising concerns about accuracy and explainability. This paper explores LLMs for local article classification in Taboola's "Homepage For You" system, comparing them to traditional techniques. Key findings: (1) Knowledge Graphs enhance NER models' ability to detect implicit locations, (2) LLMs outperform traditional methods, and (3) LLMs can effectively identify local content without requiring Knowledge Graph integration. Offline evaluations showed LLMs excel at implicit location classification, while online A/B tests showed a significant increased in local views. A scalable pipeline integrating LLM-based location classification boosted local article distribution by 27%, preserving newspapers' brand identity and enhancing homepage personalization.</li>
</ul>

<h3>Title: InstructAgent: Building User Controllable Recommender via LLM Agent</h3>
<ul>
<li><strong>Authors: </strong>Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14662">https://arxiv.org/abs/2502.14662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14662">https://arxiv.org/pdf/2502.14662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14662]] InstructAgent: Building User Controllable Recommender via LLM Agent(https://arxiv.org/abs/2502.14662)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as $\dataset$, along with user instructions for each record.</li>
</ul>

<h3>Title: AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO</h3>
<ul>
<li><strong>Authors: </strong>Alan Dao (Gia Tuan Dao), Dinh Bach Vu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14669">https://arxiv.org/abs/2502.14669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14669">https://arxiv.org/pdf/2502.14669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14669]] AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO(https://arxiv.org/abs/2502.14669)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive capabilities in language processing, yet they often struggle with tasks requiring genuine visual spatial reasoning. In this paper, we introduce a novel two-stage training framework designed to equip standard LLMs with visual reasoning abilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT) on a curated dataset of tokenized maze representations to teach the model to predict step-by-step movement commands. Next, we apply Group Relative Policy Optimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted reward function to refine the model's sequential decision-making and encourage emergent chain-of-thought behaviors. Experimental results on synthetically generated mazes show that while a baseline model fails to navigate the maze, the SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning boosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more robust and self-corrective reasoning, highlighting the potential of our approach to bridge the gap between language models and visual spatial tasks. These findings offer promising implications for applications in robotics, autonomous navigation, and other domains that require integrated visual and sequential reasoning.</li>
</ul>

<h3>Title: Explanations of Deep Language Models Explain Language Representations in the Brain</h3>
<ul>
<li><strong>Authors: </strong>Maryam Rahimi (1), Yadollah Yaghoobzadeh (2 and 4), Mohammad Reza Daliri (1 and 3) ((1) Biomedical Engineering Department, School of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran, (2) Electrical and Computer Engineering Department, University of Tehran, Tehran, Iran, (3) School of Cognitive Sciences, Institute for Research in Fundamental Sciences, Tehran, Iran, (4) Tehran Institute for Advanced Studies, Khatam University, Tehran, Iran)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14671">https://arxiv.org/abs/2502.14671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14671">https://arxiv.org/pdf/2502.14671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14671]] Explanations of Deep Language Models Explain Language Representations in the Brain(https://arxiv.org/abs/2502.14671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in artificial intelligence have given rise to large language models (LLMs) that not only achieve human-like performance but also share computational principles with the brain's language processing mechanisms. While previous research has primarily focused on aligning LLMs' internal representations with neural activity, we introduce a novel approach that leverages explainable AI (XAI) methods to forge deeper connections between the two domains. Using attribution methods, we quantified how preceding words contribute to an LLM's next-word predictions and employed these explanations to predict fMRI recordings from participants listening to the same narratives. Our findings demonstrate that attribution methods robustly predict brain activity across the language network, surpassing traditional internal representations in early language areas. This alignment is hierarchical: early-layer explanations correspond to the initial stages of language processing in the brain, while later layers align with more advanced stages. Moreover, the layers more influential on LLM next-word prediction$\unicode{x2014}$those with higher attribution scores$\unicode{x2014}$exhibited stronger alignment with neural activity. This work establishes a bidirectional bridge between AI and neuroscience. First, we demonstrate that attribution methods offer a powerful lens for investigating the neural mechanisms of language comprehension, revealing how meaning emerges from preceding context. Second, we propose using brain alignment as a metric to evaluate the validity of attribution methods, providing a framework for assessing their biological plausibility.</li>
</ul>

<h3>Title: Data-Constrained Synthesis of Training Data for De-Identification</h3>
<ul>
<li><strong>Authors: </strong>Thomas Vakili, Aron Henriksson, Hercules Dalianis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14677">https://arxiv.org/abs/2502.14677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14677">https://arxiv.org/pdf/2502.14677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14677]] Data-Constrained Synthesis of Training Data for De-Identification(https://arxiv.org/abs/2502.14677)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.</li>
</ul>

<h3>Title: How to Get Your LLM to Generate Challenging Problems for Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Arkil Patel, Siva Reddy, Dzmitry Bahdanau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14678">https://arxiv.org/abs/2502.14678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14678">https://arxiv.org/pdf/2502.14678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14678]] How to Get Your LLM to Generate Challenging Problems for Evaluation(https://arxiv.org/abs/2502.14678)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The pace of evolution of Large Language Models (LLMs) necessitates new approaches for rigorous and comprehensive evaluation. Traditional human annotation is increasingly impracticable due to the complexities and costs involved in generating high-quality, challenging problems. In this work, we introduce CHASE, a unified framework to synthetically generate challenging problems using LLMs without human involvement. For a given task, our approach builds a hard problem in a bottom-up manner from simpler components. Moreover, our framework decomposes the generation process into independently verifiable sub-tasks, thereby ensuring a high level of quality and correctness. We implement CHASE to create evaluation benchmarks across three diverse domains: (1) document-based question answering, (2) repository-level code completion, and (3) math reasoning. The performance of state-of-the-art LLMs on these synthetic benchmarks lies in the range of 40-60% accuracy, thereby demonstrating the effectiveness of our framework at generating challenging problems. We publicly release our benchmarks and code.</li>
</ul>

<h3>Title: Disentangled Latent Spaces for Reduced Order Models using Deterministic Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Henning Schwarz, Pyei Phyo Lin, Jens-Peter M. Zemke, Thomas Rung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14679">https://arxiv.org/abs/2502.14679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14679">https://arxiv.org/pdf/2502.14679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14679]] Disentangled Latent Spaces for Reduced Order Models using Deterministic Autoencoders(https://arxiv.org/abs/2502.14679)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Data-driven reduced-order models based on autoencoders generally lack interpretability compared to classical methods such as the proper orthogonal decomposition. More interpretability can be gained by disentangling the latent variables and analyzing the resulting modes. For this purpose, probabilistic $\beta$-variational autoencoders ($\beta$-VAEs) are frequently used in computational fluid dynamics and other simulation sciences. Using a benchmark periodic flow dataset, we show that competitive results can be achieved using non-probabilistic autoencoder approaches that either promote orthogonality or penalize correlation between latent variables. Compared to probabilistic autoencoders, these approaches offer more robustness with respect to the choice of hyperparameters entering the loss function. We further demonstrate the ability of a non-probabilistic approach to identify a reduced number of active latent variables by introducing a correlation penalty, a function also known from the use of $\beta$-VAE. The investigated probabilistic and non-probabilistic autoencoder models are finally used for the dimensionality reduction of aircraft ditching loads, which serves as an industrial application in this work.</li>
</ul>

<h3>Title: Bridging the Gap: Transforming Natural Language Questions into SQL Queries via Abstract Query Pattern and Contextual Schema Markup</h3>
<ul>
<li><strong>Authors: </strong>Yonghui Kong, Hongbing Hu, Dan Zhang, Siyuan Chai, Fan Zhang, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14682">https://arxiv.org/abs/2502.14682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14682">https://arxiv.org/pdf/2502.14682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14682]] Bridging the Gap: Transforming Natural Language Questions into SQL Queries via Abstract Query Pattern and Contextual Schema Markup(https://arxiv.org/abs/2502.14682)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated excellent performance in many tasks, including Text-to-SQL, due to their powerful in-context learning capabilities. They are becoming the mainstream approach for Text-to-SQL. However, these methods still have a significant gap compared to human performance, especially on complex questions. As the complexity of questions increases, the gap between questions and SQLs increases. We identify two important gaps: the structural mapping gap and the lexical mapping gap. To tackle these two gaps, we propose PAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates gaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM). AQP aims to obtain the structural pattern of the question by removing database-related information, which enables us to find structurally similar demonstrations. CSM aims to associate database-related text span in the question with specific tables or columns in the database, which alleviates the lexical mapping gap. Experimental results on the Spider and BIRD datasets demonstrate the effectiveness of our proposed method. Specifically, PAS-SQL + GPT-4o sets a new state-of-the-art on the Spider benchmark with an execution accuracy of 87.9\%, and achieves leading results on the BIRD dataset with an execution accuracy of 64.67\%.</li>
</ul>

<h3>Title: I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Zujie Liang, Feng Wei, Wujiang Xu, Lin Chen, Yuxi Qian, Xinhui Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14693">https://arxiv.org/abs/2502.14693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14693">https://arxiv.org/pdf/2502.14693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14693]] I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search(https://arxiv.org/abs/2502.14693)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making this http URL, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node's solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed this http URL to the various ML tasks, our approach demonstrates a6\% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems.</li>
</ul>

<h3>Title: Entity Framing and Role Portrayal in the News</h3>
<ul>
<li><strong>Authors: </strong>Tarek Mahmoud, Zhuohan Xie, Dimitar Dimitrov, Nikolaos Nikolaidis, PurificaÃ§Ã£o Silvano, Roman Yangarber, Shivam Sharma, Elisa Sartori, Nicolas Stefanovitch, Giovanni Da San Martino, Jakub Piskorski, Preslav Nakov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14718">https://arxiv.org/abs/2502.14718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14718">https://arxiv.org/pdf/2502.14718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14718]] Entity Framing and Role Portrayal in the News(https://arxiv.org/abs/2502.14718)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce a novel multilingual hierarchical corpus annotated for entity framing and role portrayal in news articles. The dataset uses a unique taxonomy inspired by storytelling elements, comprising 22 fine-grained roles, or archetypes, nested within three main categories: protagonist, antagonist, and innocent. Each archetype is carefully defined, capturing nuanced portrayals of entities such as guardian, martyr, and underdog for protagonists; tyrant, deceiver, and bigot for antagonists; and victim, scapegoat, and exploited for innocents. The dataset includes 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two critical domains of global significance: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been annotated with role labels. This dataset serves as a valuable resource for research into role portrayal and has broader implications for news analysis. We describe the characteristics of the dataset and the annotation process, and we report evaluation results on fine-tuned state-of-the-art multilingual transformers and hierarchical zero-shot learning using LLMs at the level of a document, a paragraph, and a sentence.</li>
</ul>

<h3>Title: Multi-dataset synergistic in supervised learning to pre-label structural components in point clouds from shell construction scenes</h3>
<ul>
<li><strong>Authors: </strong>Lukas Rauch, Thomas Braml</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14721">https://arxiv.org/abs/2502.14721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14721">https://arxiv.org/pdf/2502.14721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14721]] Multi-dataset synergistic in supervised learning to pre-label structural components in point clouds from shell construction scenes(https://arxiv.org/abs/2502.14721)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The significant effort required to annotate data for new training datasets hinders computer vision research and machine learning in the construction industry. This work explores adapting standard datasets and the latest transformer model architectures for point cloud semantic segmentation in the context of shell construction sites. Unlike common approaches focused on object segmentation of building interiors and furniture, this study addressed the challenges of segmenting complex structural components in Architecture, Engineering, and Construction (AEC). We establish a baseline through supervised training and a custom validation dataset, evaluate the cross-domain inference with large-scale indoor datasets, and utilize transfer learning to maximize segmentation performance with minimal new data. The findings indicate that with minimal fine-tuning, pre-trained transformer architectures offer an effective strategy for building component segmentation. Our results are promising for automating the annotation of new, previously unseen data when creating larger training resources and for the segmentation of frequently recurring objects.</li>
</ul>

<h3>Title: SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines</h3>
<ul>
<li><strong>Authors: </strong>M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixing Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jingyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, Ge Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14739">https://arxiv.org/abs/2502.14739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14739">https://arxiv.org/pdf/2502.14739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14739]] SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines(https://arxiv.org/abs/2502.14739)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope.</li>
</ul>

<h3>Title: YOLOv12: A Breakdown of the Key Architectural Features</h3>
<ul>
<li><strong>Authors: </strong>Mujadded Al Rabbani Alif, Muhammad Hussain</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14740">https://arxiv.org/abs/2502.14740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14740">https://arxiv.org/pdf/2502.14740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14740]] YOLOv12: A Breakdown of the Key Architectural Features(https://arxiv.org/abs/2502.14740)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, extraction</a></li>
<li><strong>Abstract: </strong>This paper presents an architectural analysis of YOLOv12, a significant advancement in single-stage, real-time object detection building upon the strengths of its predecessors while introducing key improvements. The model incorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and FlashAttention-driven area-based attention, improving feature extraction, enhanced efficiency, and robust detections. With multiple model variants, similar to its predecessors, YOLOv12 offers scalable solutions for both latency-sensitive and high-accuracy applications. Experimental results manifest consistent gains in mean average precision (mAP) and inference speed, making YOLOv12 a compelling choice for applications in autonomous systems, security, and real-time analytics. By achieving an optimal balance between computational efficiency and performance, YOLOv12 sets a new benchmark for real-time computer vision, facilitating deployment across diverse hardware platforms, from edge devices to high-performance clusters.</li>
</ul>

<h3>Title: HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States</h3>
<ul>
<li><strong>Authors: </strong>Yilei Jiang, Xinyan Gao, Tianshuo Peng, Yingshui Tan, Xiaoyong Zhu, Bo Zheng, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14744">https://arxiv.org/abs/2502.14744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14744">https://arxiv.org/pdf/2502.14744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14744]] HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States(https://arxiv.org/abs/2502.14744)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at this https URL.</li>
</ul>

<h3>Title: Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zongxia Li, Lorena Calvo-BartolomÃ©, Alexander Hoyle, Paiheng Xu, Alden Dima, Juan Francisco Fung, Jordan Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14748">https://arxiv.org/abs/2502.14748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14748">https://arxiv.org/pdf/2502.14748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14748]] Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs(https://arxiv.org/abs/2502.14748)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A common use of NLP is to facilitate the understanding of large document collections, with a shift from using traditional topic models to Large Language Models. Yet the effectiveness of using LLM for large corpus understanding in real-world applications remains under-explored. This study measures the knowledge users acquire with unsupervised, supervised LLM-based exploratory approaches or traditional topic models on two datasets. While LLM-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents. Adding human supervision to the LLM generation process improves data exploration by mitigating hallucination and over-genericity but requires greater human effort. In contrast, traditional. models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly. We show that LLMs struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints. Dataset available at https://huggingface. co/datasets/zli12321/Bills.</li>
</ul>

<h3>Title: TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators</h3>
<ul>
<li><strong>Authors: </strong>Jianling Li, Shangzhan Li, Zhenye Gao, Qi Shi, Yuxuan Li, Zefan Wang, Jiacheng Huang, Haojie Wang, Jianrong Wang, Xu Han, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14752">https://arxiv.org/abs/2502.14752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14752">https://arxiv.org/pdf/2502.14752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14752]] TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators(https://arxiv.org/abs/2502.14752)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Triton, a high-level Python-like language designed for building efficient GPU kernels, is widely adopted in deep learning frameworks due to its portability, flexibility, and accessibility. However, programming and parallel optimization still require considerable trial and error from Triton developers. Despite advances in large language models (LLMs) for conventional code generation, these models struggle to generate accurate, performance-optimized Triton code, as they lack awareness of its specifications and the complexities of GPU programming. More critically, there is an urgent need for systematic evaluations tailored to Triton. In this work, we introduce TritonBench, the first comprehensive benchmark for Triton operator generation. TritonBench features two evaluation channels: a curated set of 184 real-world operators from GitHub and a collection of operators aligned with PyTorch interfaces. Unlike conventional code benchmarks prioritizing functional correctness, TritonBench also profiles efficiency performance on widely deployed GPUs aligned with industry applications. Our study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators, highlighting a significant gap in high-performance code generation. TritonBench will be available at this https URL.</li>
</ul>

<h3>Title: On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Juraj Vladika, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14759">https://arxiv.org/abs/2502.14759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14759">https://arxiv.org/pdf/2502.14759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14759]] On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2502.14759)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has emerged as an approach to augment large language models (LLMs) by reducing their reliance on static knowledge and improving answer factuality. RAG retrieves relevant context snippets and generates an answer based on them. Despite its increasing industrial adoption, systematic exploration of RAG components is lacking, particularly regarding the ideal size of provided context, and the choice of base LLM and retrieval method. To help guide development of robust RAG systems, we evaluate various context sizes, BM25 and semantic search as retrievers, and eight base LLMs. Moving away from the usual RAG evaluation with short answers, we explore the more challenging long-form question answering in two domains, where a good answer has to utilize the entire context. Our findings indicate that final QA performance improves steadily with up to 15 snippets but stagnates or declines beyond that. Finally, we show that different general-purpose LLMs excel in the biomedical domain than the encyclopedic one, and that open-domain evidence retrieval in large corpora is challenging.</li>
</ul>

<h3>Title: Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Priyanka Kargupta, Ishika Agarwal, Tal August, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14767">https://arxiv.org/abs/2502.14767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14767">https://arxiv.org/pdf/2502.14767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14767]] Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis(https://arxiv.org/abs/2502.14767)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.</li>
</ul>

<h3>Title: Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Fei Chao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14770">https://arxiv.org/abs/2502.14770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14770">https://arxiv.org/pdf/2502.14770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14770]] Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective(https://arxiv.org/abs/2502.14770)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenge of determining the layer-wise sparsity rates of large language models (LLMs) through a theoretical perspective. Specifically, we identify a critical issue of ''$\textbf{reconstruction error explosion}$'' in existing LLMs sparsification methods. This refers to the cumulative effect of reconstruction errors throughout the sparsification process, where errors from earlier layers propagate and amplify in subsequent layers. As a result, the overall reconstruction error increases significantly, leading to a substantial degradation in model performance. Through theoretical analysis, we derive a simple yet effective approach to layer-wise sparsity allocation that mitigates this issue. Our method uses a monotonically increasing arithmetic progression, reducing the process of determining sparsity rates for multiple layers to the determination of a single common difference hyperparameter. Remarkably, this allows for the optimal layer-wise sparsity rates to be identified with just a few trials. Both our theoretical analysis and experimental results demonstrate that this sparsity allocation scheme is near optimal. Extensive experiments show that our method significantly improves the performance of sparse LLMs across various architectures, outperforming existing layer-wise sparsity methods. Furthermore, it enhances the performance of various compression techniques and is applicable to vision and multimodal models. Notably, our method achieves a reduction of 52.10 in perplexity for the 70$\%$ sparse LLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by 10.50$\%$, and delivers speedups of 2.63$\times$ and 2.23$\times$ on CPU and GPU, respectively.</li>
</ul>

<h3>Title: SurveyX: Academic Survey Automation via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Simin Niu, Shichao Song, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, Zhiyu li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14776">https://arxiv.org/abs/2502.14776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14776">https://arxiv.org/pdf/2502.14776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14776]] SurveyX: Academic Survey Automation via Large Language Models(https://arxiv.org/abs/2502.14776)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on this http URL</li>
</ul>

<h3>Title: DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hongji Yang, Wencheng Han, Yucheng Zhou, Jianbing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14779">https://arxiv.org/abs/2502.14779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14779">https://arxiv.org/pdf/2502.14779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14779]] DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models(https://arxiv.org/abs/2502.14779)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and precisely controllable framework for multi-condition image generation. The core idea behind DC-ControlNet is to decouple control conditions, transforming global control into a hierarchical system that integrates distinct elements, contents, and layouts. This enables users to mix these individual conditions with greater flexibility, leading to more efficient and accurate image generation control. Previous ControlNet-based models rely solely on global conditions, which affect the entire image and lack the ability of element- or region-specific control. This limitation reduces flexibility and can cause condition misunderstandings in multi-conditional image generation. To address these challenges, we propose both intra-element and Inter-element Controllers in DC-ControlNet. The Intra-Element Controller handles different types of control signals within individual elements, accurately describing the content and layout characteristics of the object. For interactions between elements, we introduce the Inter-Element Controller, which accurately handles multi-element interactions and occlusion based on user-defined relationships. Extensive evaluations show that DC-ControlNet significantly outperforms existing ControlNet models and Layout-to-Image generative models in terms of control flexibility and precision in multi-condition control.</li>
</ul>

<h3>Title: ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</h3>
<ul>
<li><strong>Authors: </strong>Abhijit Mishra, Richard Noh, Hsiang Fu, Mingda Li, Minji Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14780">https://arxiv.org/abs/2502.14780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14780">https://arxiv.org/pdf/2502.14780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14780]] ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting(https://arxiv.org/abs/2502.14780)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (<500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.</li>
</ul>

<h3>Title: SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features</h3>
<ul>
<li><strong>Authors: </strong>Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier HÃ©naff, Jeremiah Harmsen, Andreas Steiner, Xiaohua Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14786">https://arxiv.org/abs/2502.14786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14786">https://arxiv.org/pdf/2502.14786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14786]] SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features(https://arxiv.org/abs/2502.14786)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).</li>
</ul>

<h3>Title: Structurally Disentangled Feature Fields Distillation for 3D Understanding and Editing</h3>
<ul>
<li><strong>Authors: </strong>Yoel Levy, David Shavin, Itai Lang, Sagie Benaim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14789">https://arxiv.org/abs/2502.14789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14789">https://arxiv.org/pdf/2502.14789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14789]] Structurally Disentangled Feature Fields Distillation for 3D Understanding and Editing(https://arxiv.org/abs/2502.14789)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent work has demonstrated the ability to leverage or distill pre-trained 2D features obtained using large pre-trained 2D models into 3D features, enabling impressive 3D editing and understanding capabilities using only 2D supervision. Although impressive, models assume that 3D features are captured using a single feature field and often make a simplifying assumption that features are view-independent. In this work, we propose instead to capture 3D features using multiple disentangled feature fields that capture different structural components of 3D features involving view-dependent and view-independent components, which can be learned from 2D feature supervision only. Subsequently, each element can be controlled in isolation, enabling semantic and structural understanding and editing capabilities. For instance, using a user click, one can segment 3D features corresponding to a given object and then segment, edit, or remove their view-dependent (reflective) properties. We evaluate our approach on the task of 3D segmentation and demonstrate a set of novel understanding and editing tasks.</li>
</ul>

<h3>Title: An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces</h3>
<ul>
<li><strong>Authors: </strong>Alexander Terenin, Jeffrey Negrea</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14790">https://arxiv.org/abs/2502.14790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14790">https://arxiv.org/pdf/2502.14790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14790]] An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces(https://arxiv.org/abs/2502.14790)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We develop an analysis of Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling with a certain Gaussian process prior widely-used in the Bayesian optimization literature has a $\mathcal{O}(\beta\sqrt{T\log(1+\lambda)})$ rate against a $\beta$-bounded $\lambda$-Lipschitz~adversary.</li>
</ul>

<h3>Title: Rapid Word Learning Through Meta In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Wentao Wang, Guangyuan Jiang, Tal Linzen, Brenden M. Lake</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14791">https://arxiv.org/abs/2502.14791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14791">https://arxiv.org/pdf/2502.14791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14791]] Rapid Word Learning Through Meta In-Context Learning(https://arxiv.org/abs/2502.14791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.</li>
</ul>

<h3>Title: RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Henrique PiÃ±eiro Monteagudo, Leonardo Taccari, Aurel Pjetri, Francesco Sambo, Samuele Salti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14792">https://arxiv.org/abs/2502.14792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14792">https://arxiv.org/pdf/2502.14792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14792]] RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation(https://arxiv.org/abs/2502.14792)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention as a useful representation of the environment to tackle assisted and autonomous driving tasks. However, most of the existing work focuses on the fully supervised setting, training networks on large annotated datasets. In this work, we present RendBEV, a new method for the self-supervised training of BEV semantic segmentation networks, leveraging differentiable volumetric rendering to receive supervision from semantic perspective views computed by a 2D semantic segmentation model. Our method enables zero-shot BEV semantic segmentation, and already delivers competitive results in this challenging setting. When used as pretraining to then fine-tune on labeled BEV ground-truth, our method significantly boosts performance in low-annotation regimes, and sets a new state of the art when fine-tuning on all available labels.</li>
</ul>

<h3>Title: A Survey on Text-Driven 360-Degree Panorama Generation</h3>
<ul>
<li><strong>Authors: </strong>Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14799">https://arxiv.org/abs/2502.14799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14799">https://arxiv.org/pdf/2502.14799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14799]] A Survey on Text-Driven 360-Degree Panorama Generation(https://arxiv.org/abs/2502.14799)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at this https URL.</li>
</ul>

<h3>Title: AVD2: Accident Video Diffusion for Accident Video Description</h3>
<ul>
<li><strong>Authors: </strong>Cheng Li, Keyuan Zhou, Tong Liu, Yu Wang, Mingqiao Zhuang, Huan-ang Gao, Bu Jin, Hao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14801">https://arxiv.org/abs/2502.14801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14801">https://arxiv.org/pdf/2502.14801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14801]] AVD2: Accident Video Diffusion for Accident Video Description(https://arxiv.org/abs/2502.14801)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Traffic accidents present complex challenges for autonomous driving, often featuring unpredictable scenarios that hinder accurate system interpretation and this http URL, prevailing methodologies fall short in elucidating the causes of accidents and proposing preventive measures due to the paucity of training data specific to accident this http URL this work, we introduce AVD2 (Accident Video Diffusion for Accident Video Description), a novel framework that enhances accident scene understanding by generating accident videos that aligned with detailed natural language descriptions and reasoning, resulting in the contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding) dataset. Empirical results reveal that the integration of the EMM-AU dataset establishes state-of-the-art performance across both automated metrics and human evaluations, markedly advancing the domains of accident analysis and prevention. Project resources are available at this https URL</li>
</ul>

<h3>Title: From RAG to Memory: Non-Parametric Continual Learning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bernal JimÃ©nez GutiÃ©rrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, Yu Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14802">https://arxiv.org/abs/2502.14802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14802">https://arxiv.org/pdf/2502.14802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14802]] From RAG to Memory: Non-Parametric Continual Learning for Large Language Models(https://arxiv.org/abs/2502.14802)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at this https URL.</li>
</ul>

<h3>Title: PREM: Privately Answering Statistical Queries with Relative Error</h3>
<ul>
<li><strong>Authors: </strong>Badih Ghazi, CristÃ³bal GuzmÃ¡n, Pritish Kamath, Alexander Knop, Ravi Kumar, Pasin Manurangsi, Sushant Sachdeva</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14809">https://arxiv.org/abs/2502.14809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14809">https://arxiv.org/pdf/2502.14809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14809]] PREM: Privately Answering Statistical Queries with Relative Error(https://arxiv.org/abs/2502.14809)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We introduce $\mathsf{PREM}$ (Private Relative Error Multiplicative weight update), a new framework for generating synthetic data that achieves a relative error guarantee for statistical queries under $(\varepsilon, \delta)$ differential privacy (DP). Namely, for a domain ${\cal X}$, a family ${\cal F}$ of queries $f : {\cal X} \to \{0, 1\}$, and $\zeta > 0$, our framework yields a mechanism that on input dataset $D \in {\cal X}^n$ outputs a synthetic dataset $\widehat{D} \in {\cal X}^n$ such that all statistical queries in ${\cal F}$ on $D$, namely $\sum_{x \in D} f(x)$ for $f \in {\cal F}$, are within a $1 \pm \zeta$ multiplicative factor of the corresponding value on $\widehat{D}$ up to an additive error that is polynomial in $\log |{\cal F}|$, $\log |{\cal X}|$, $\log n$, $\log(1/\delta)$, $1/\varepsilon$, and $1/\zeta$. In contrast, any $(\varepsilon, \delta)$-DP mechanism is known to require worst-case additive error that is polynomial in at least one of $n, |{\cal F}|$, or $|{\cal X}|$. We complement our algorithm with nearly matching lower bounds.</li>
</ul>

<h3>Title: Dynamic Low-Rank Sparse Adaptation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Yang Liu, Jing Lin, Yiwu Yao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14816">https://arxiv.org/abs/2502.14816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14816">https://arxiv.org/pdf/2502.14816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14816]] Dynamic Low-Rank Sparse Adaptation for Large Language Models(https://arxiv.org/abs/2502.14816)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA), a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby efficiently determining the layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by 16.32$\%$, achieving a 2.60$\times$ speedup on CPU and 2.23$\times$ speedup on GPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU. Code is available at this https URL.</li>
</ul>

<h3>Title: eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables</h3>
<ul>
<li><strong>Authors: </strong>Luis Antonio GutiÃ©rrez Guanilo, Mir Tafseer Nayeem, Cristian LÃ³pez, Davood Rafiei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14820">https://arxiv.org/abs/2502.14820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14820">https://arxiv.org/pdf/2502.14820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14820]] eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables(https://arxiv.org/abs/2502.14820)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets. To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries. Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data. Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments. Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows. This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges.</li>
</ul>

<h3>Title: Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison</h3>
<ul>
<li><strong>Authors: </strong>Aiswarya Baby, Tintu Thankom Koshy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14827">https://arxiv.org/abs/2502.14827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14827">https://arxiv.org/pdf/2502.14827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14827]] Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison(https://arxiv.org/abs/2502.14827)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual Question Answering (VQA) has emerged as a pivotal task in the intersection of computer vision and natural language processing, requiring models to understand and reason about visual content in response to natural language questions. Analyzing VQA datasets is essential for developing robust models that can handle the complexities of multimodal reasoning. Several approaches have been developed to examine these datasets, each offering distinct perspectives on question diversity, answer distribution, and visual-textual correlations. Despite significant progress, existing VQA models face challenges related to dataset bias, limited model complexity, commonsense reasoning gaps, rigid evaluation methods, and generalization to real world scenarios. This paper presents a comprehensive comparative study of five advanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA, each employing distinct methodologies to address these challenges.</li>
</ul>

<h3>Title: Fundamental Limitations in Defending LLM Finetuning APIs</h3>
<ul>
<li><strong>Authors: </strong>Xander Davies, Eric Winsor, Tomek Korbak, Alexandra Souly, Robert Kirk, Christian Schroeder de Witt, Yarin Gal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14828">https://arxiv.org/abs/2502.14828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14828">https://arxiv.org/pdf/2502.14828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14828]] Fundamental Limitations in Defending LLM Finetuning APIs(https://arxiv.org/abs/2502.14828)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>LLM developers have imposed technical interventions to prevent fine-tuning misuse attacks, attacks where adversaries evade safeguards by fine-tuning the model using a public API. Previous work has established several successful attacks against specific fine-tuning API defences. In this work, we show that defences of fine-tuning APIs that seek to detect individual harmful training or inference samples ('pointwise' detection) are fundamentally limited in their ability to prevent fine-tuning attacks. We construct 'pointwise-undetectable' attacks that repurpose entropy in benign model outputs (e.g. semantic or syntactic variations) to covertly transmit dangerous knowledge. Our attacks are composed solely of unsuspicious benign samples that can be collected from the model before fine-tuning, meaning training and inference samples are all individually benign and low-perplexity. We test our attacks against the OpenAI fine-tuning API, finding they succeed in eliciting answers to harmful multiple-choice questions, and that they evade an enhanced monitoring system we design that successfully detects other fine-tuning attacks. We encourage the community to develop defences that tackle the fundamental limitations we uncover in pointwise fine-tuning API defences.</li>
</ul>

<h3>Title: Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Danni Liu, Jan Niehues</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14830">https://arxiv.org/abs/2502.14830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14830">https://arxiv.org/pdf/2502.14830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14830]] Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs(https://arxiv.org/abs/2502.14830)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (this https URL).</li>
</ul>

<h3>Title: Improving the Diffusability of Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, Aliaksandr Siarohin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14831">https://arxiv.org/abs/2502.14831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14831">https://arxiv.org/pdf/2502.14831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14831]] Improving the Diffusability of Autoencoders(https://arxiv.org/abs/2502.14831)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Latent diffusion models have emerged as the leading approach for generating high-quality images and videos, utilizing compressed latent representations to reduce the computational burden of the diffusion process. While recent advancements have primarily focused on scaling diffusion backbones and improving autoencoder reconstruction quality, the interaction between these components has received comparatively less attention. In this work, we perform a spectral analysis of modern autoencoders and identify inordinate high-frequency components in their latent spaces, which are especially pronounced in the autoencoders with a large bottleneck channel size. We hypothesize that this high-frequency component interferes with the coarse-to-fine nature of the diffusion synthesis process and hinders the generation quality. To mitigate the issue, we propose scale equivariance: a simple regularization strategy that aligns latent and RGB spaces across frequencies by enforcing scale equivariance in the decoder. It requires minimal code changes and only up to 20K autoencoder fine-tuning steps, yet significantly improves generation quality, reducing FID by 19% for image generation on ImageNet-1K 256x256 and FVD by at least 44% for video generation on Kinetics-700 17x256x256.</li>
</ul>

<h3>Title: Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14833">https://arxiv.org/abs/2502.14833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14833">https://arxiv.org/pdf/2502.14833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14833]] Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide(https://arxiv.org/abs/2502.14833)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Deep learning (DL) has demonstrated significant potential across various safety-critical applications, yet ensuring its robustness remains a key challenge. While adversarial robustness has been extensively studied in worst-case scenarios, probabilistic robustness (PR) offers a more practical perspective by quantifying the likelihood of failures under stochastic perturbations. This paper provides a concise yet comprehensive overview of PR, covering its formal definitions, evaluation and enhancement methods. We introduce a reformulated ''min-max'' optimisation framework for adversarial training specifically designed to improve PR. Furthermore, we explore the integration of PR verification evidence into system-level safety assurance, addressing challenges in translating DL model-level robustness to system-level claims. Finally, we highlight open research questions, including benchmarking PR evaluation methods, extending PR to generative AI tasks, and developing rigorous methodologies and case studies for system-level integration.</li>
</ul>

<h3>Title: Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tao Ji, Bin Guo, Yuanbin Wu, Qipeng Guo, Lixing Shen, Zhan Chen, Xipeng Qiu, Qi Zhang, Tao Gui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14837">https://arxiv.org/abs/2502.14837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14837">https://arxiv.org/pdf/2502.14837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14837]] Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs(https://arxiv.org/abs/2502.14837)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging. This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE, we remove RoPE from dimensions of queries and keys that contribute less to the attention scores, for low-rank approximation, we introduce joint SVD approximations based on the pre-trained parameters of keys and values. These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (0.3% to 0.6%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 0.5% drop in LongBench performance.</li>
</ul>

<h3>Title: Revealing and Mitigating Over-Attention in Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Pinzheng Wang, Zecheng Tang, Keyan Zhou, Juntao Li, Qiaoming Zhu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14838">https://arxiv.org/abs/2502.14838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14838">https://arxiv.org/pdf/2502.14838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14838]] Revealing and Mitigating Over-Attention in Knowledge Editing(https://arxiv.org/abs/2502.14838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data. To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via efficiently modifying a very small percentage of parameters. % However, those methods can lead to the problem of Specificity Failure: when the content related to the edited knowledge occurs in the context, it can inadvertently corrupt other pre-existing knowledge. However, those methods can lead to the problem of Specificity Failure, where the existing knowledge and capabilities are severely degraded due to editing. Our preliminary indicates that Specificity Failure primarily stems from the model's attention heads assigning excessive attention scores to entities related to the edited knowledge, thereby unduly focusing on specific snippets within the context, which we denote as the Attention Drift phenomenon. To mitigate such Attention Drift issue, we introduce a simple yet effective method Selective Attention Drift Restriction}(SADR), which introduces an additional regularization term during the knowledge editing process to restrict changes in the attention weight distribution, thereby preventing undue focus on the edited entity. Experiments on five frequently used strong LLMs demonstrate the effectiveness of our method, where SADR can significantly mitigate Specificity Failure in the predominant knowledge editing tasks.</li>
</ul>

<h3>Title: Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Christopher Clark</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14846">https://arxiv.org/abs/2502.14846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14846">https://arxiv.org/pdf/2502.14846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14846]] Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation(https://arxiv.org/abs/2502.14846)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., "nutrition fact labels"), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.</li>
</ul>

<h3>Title: Red-Teaming LLM Multi-Agent Systems via Communication Attacks</h3>
<ul>
<li><strong>Authors: </strong>Pengfei He, Yupin Lin, Shen Dong, Han Xu, Yue Xing, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14847">https://arxiv.org/abs/2502.14847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14847">https://arxiv.org/pdf/2502.14847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14847]] Red-Teaming LLM Multi-Agent Systems via Communication Attacks(https://arxiv.org/abs/2502.14847)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized complex problem-solving capability by enabling sophisticated agent collaboration through message-based communications. While the communication framework is crucial for agent coordination, it also introduces a critical yet unexplored security vulnerability. In this work, we introduce Agent-in-the-Middle (AiTM), a novel attack that exploits the fundamental communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages. Unlike existing attacks that compromise individual agents, AiTM demonstrates how an adversary can compromise entire multi-agent systems by only manipulating the messages passing between agents. To enable the attack under the challenges of limited control and role-restricted communication format, we develop an LLM-powered adversarial agent with a reflection mechanism that generates contextually-aware malicious instructions. Our comprehensive evaluation across various frameworks, communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures in multi-agent systems.</li>
</ul>

<h3>Title: GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jianwen Luo, Yiming Huang, Jinxiang Meng, Fangyu Lei, Shizhu He, Xiao Liu, Shanshan Jiang, Bin Dong, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14848">https://arxiv.org/abs/2502.14848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14848">https://arxiv.org/pdf/2502.14848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14848]] GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks(https://arxiv.org/abs/2502.14848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown great promise in tool-making, yet existing frameworks often struggle to efficiently construct reliable toolsets and are limited to single-task settings. To address these challenges, we propose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that dynamically constructs and evolves a hierarchical graph of reusable tools across multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft), agent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date, TabMWP). Our results show that GATE achieves up to 4.3x faster milestone completion in Minecraft compared to the previous SOTA, and provides an average improvement of 9.23% over existing tool-making methods in code generation tasks and 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution, balancing tool quantity, complexity, and functionality while maintaining high efficiency. Code and data are available at \url{this https URL}.</li>
</ul>

<h3>Title: Prompt-to-Leaderboard</h3>
<ul>
<li><strong>Authors: </strong>Evan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios N. Angelopoulos, Ion Stoica</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14855">https://arxiv.org/abs/2502.14855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14855">https://arxiv.org/pdf/2502.14855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14855]] Prompt-to-Leaderboard(https://arxiv.org/abs/2502.14855)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: this https URL.</li>
</ul>

<h3>Title: FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling</h3>
<ul>
<li><strong>Authors: </strong>Weilin Zhao, Tengyu Pan, Xu Han, Yudi Zhang, Ao Sun, Yuxiang Huang, Kaihuo Zhang, Weilun Zhao, Yuxuan Li, Jianyong Wang, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14856">https://arxiv.org/abs/2502.14856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14856">https://arxiv.org/pdf/2502.14856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14856]] FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling(https://arxiv.org/abs/2502.14856)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12$\times$ speedup over the state-of-the-art speculative sampling method EAGLE-2.</li>
</ul>

<h3>Title: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shuyue Stella Li, Jimin Mun, Faeze Brahman, Jonathan S. Ilgen, Yulia Tsvetkov, Maarten Sap</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14860">https://arxiv.org/abs/2502.14860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14860">https://arxiv.org/pdf/2502.14860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14860]] Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning(https://arxiv.org/abs/2502.14860)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decisionmaking. We present ALFA, a framework that improves LLM question-asking by (i) decomposing the notion of a "good" question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains.</li>
</ul>

<h3>Title: Interpretable Text Embeddings and Text Similarity Explanation: A Primer</h3>
<ul>
<li><strong>Authors: </strong>Juri Opitz, Lucas MÃ¶ller, Andrianos Michail, Simon Clematide</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14862">https://arxiv.org/abs/2502.14862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14862">https://arxiv.org/pdf/2502.14862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14862]] Interpretable Text Embeddings and Text Similarity Explanation: A Primer(https://arxiv.org/abs/2502.14862)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Text embeddings and text embedding models are a backbone of many AI and NLP systems, particularly those involving search. However, interpretability challenges persist, especially in explaining obtained similarity scores, which is crucial for applications requiring transparency. In this paper, we give a structured overview of interpretability methods specializing in explaining those similarity scores, an emerging research area. We study the methods' individual ideas and techniques, evaluating their potential for improving interpretability of text embeddings and explaining predicted similarities.</li>
</ul>

<h3>Title: Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts</h3>
<ul>
<li><strong>Authors: </strong>Sara Ghaboura, Ketan More, Ritesh Thawkar, Wafa Alghallabi, Omkar Thawakar, Fahad Shahbaz Khan, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14865">https://arxiv.org/abs/2502.14865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14865">https://arxiv.org/pdf/2502.14865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14865]] Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts(https://arxiv.org/abs/2502.14865)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding historical and cultural artifacts demands human expertise and advanced computational techniques, yet the process remains complex and time-intensive. While large multimodal models offer promising support, their evaluation and improvement require a standardized benchmark. To address this, we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning 266 distinct cultures across 10 major historical regions. Designed for AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological discoveries, TimeTravel provides a structured dataset and robust evaluation framework to assess AI models' capabilities in classification, interpretation, and historical comprehension. By integrating AI with historical research, TimeTravel fosters AI-powered tools for historians, archaeologists, researchers, and cultural tourists to extract valuable insights while ensuring technology contributes meaningfully to historical discovery and cultural heritage preservation. We evaluate contemporary AI models on TimeTravel, highlighting their strengths and identifying areas for improvement. Our goal is to establish AI as a reliable partner in preserving cultural heritage, ensuring that technological advancements contribute meaningfully to historical discovery. Our code is available at: \url{this https URL}.</li>
</ul>

<h3>Title: LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.14866">https://arxiv.org/abs/2502.14866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.14866">https://arxiv.org/pdf/2502.14866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.14866]] LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention(https://arxiv.org/abs/2502.14866)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
