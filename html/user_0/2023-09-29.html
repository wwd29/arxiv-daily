<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: AgEncID: Aggregate Encryption Individual Decryption of Key for FPGA Bitstream IP Cores in Cloud. (arXiv:2309.16282v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16282">http://arxiv.org/abs/2309.16282</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16282]] AgEncID: Aggregate Encryption Individual Decryption of Key for FPGA Bitstream IP Cores in Cloud(http://arxiv.org/abs/2309.16282)</code></li>
<li>Summary: <p>Cloud computing platforms are progressively adopting Field Programmable Gate
Arrays to deploy specialized hardware accelerators for specific computational
tasks. However, the security of FPGA-based bitstream for Intellectual Property,
IP cores from unauthorized interception in cloud environments remains a
prominent concern. Existing methodologies for protection of such bitstreams
possess several limitations, such as requiring a large number of keys, tying
bitstreams to specific FPGAs, and relying on trusted third parties. This paper
proposes Aggregate Encryption and Individual Decryption, a cryptosystem based
on key aggregation to enhance the security of FPGA-based bitstream for IP cores
and to address the pitfalls of previous related works. In our proposed scheme,
IP providers can encrypt their bitstreams with a single key for a set S of FPGA
boards, with which the bitstreams can directly be decrypted on any of the FPGA
boards in S. Aggregate encryption of the key is performed in a way which
ensures that the key can solely be obtained onboard through individual
decryption employing the board's private key, thus facilitating secure key
provisioning. The proposed cryptosystem is evaluated mainly on Zynq FPGAs. The
outcomes demonstrate that our cryptosystem not only outperforms existing
techniques with respect to resource, time and energy significantly but also
upholds robust security assurances.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: FLIP: Cross-domain Face Anti-spoofing with Language Guidance. (arXiv:2309.16649v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16649">http://arxiv.org/abs/2309.16649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16649]] FLIP: Cross-domain Face Anti-spoofing with Language Guidance(http://arxiv.org/abs/2309.16649)</code></li>
<li>Summary: <p>Face anti-spoofing (FAS) or presentation attack detection is an essential
component of face recognition systems deployed in security-critical
applications. Existing FAS methods have poor generalizability to unseen spoof
types, camera sensors, and environmental conditions. Recently, vision
transformer (ViT) models have been shown to be effective for the FAS task due
to their ability to capture long-range dependencies among image patches.
However, adaptive modules or auxiliary loss functions are often required to
adapt pre-trained ViT weights learned on large-scale datasets such as ImageNet.
In this work, we first show that initializing ViTs with multimodal (e.g., CLIP)
pre-trained weights improves generalizability for the FAS task, which is in
line with the zero-shot transfer capabilities of vision-language pre-trained
(VLP) models. We then propose a novel approach for robust cross-domain FAS by
grounding visual representations with the help of natural language.
Specifically, we show that aligning the image representation with an ensemble
of class descriptions (based on natural language semantics) improves FAS
generalizability in low-data regimes. Finally, we propose a multimodal
contrastive learning strategy to boost feature generalization further and
bridge the gap between source and target domains. Extensive experiments on
three standard protocols demonstrate that our method significantly outperforms
the state-of-the-art methods, achieving better zero-shot transfer performance
than five-shot transfer of adaptive ViTs. Code:
https://github.com/koushiksrivats/FLIP
</p></li>
</ul>

<h3>Title: Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems. (arXiv:2309.15995v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15995">http://arxiv.org/abs/2309.15995</a></li>
<li>Code URL: https://github.com/xuqinghua-china/tosem</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15995]] Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems(http://arxiv.org/abs/2309.15995)</code></li>
<li>Summary: <p>Anomaly detection is critical to ensure the security of cyber-physical
systems (CPS). However, due to the increasing complexity of attacks and CPS
themselves, anomaly detection in CPS is becoming more and more challenging. In
our previous work, we proposed a digital twin-based anomaly detection method,
called ATTAIN, which takes advantage of both historical and real-time data of
CPS. However, such data vary significantly in terms of difficulty. Therefore,
similar to human learning processes, deep learning models (e.g., ATTAIN) can
benefit from an easy-to-difficult curriculum. To this end, in this paper, we
present a novel approach, named digitaL twin-based Anomaly deTecTion wIth
Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum
learning to optimize its learning paradigm. LATTICE attributes each sample with
a difficulty score, before being fed into a training scheduler. The training
scheduler samples batches of training data based on these difficulty scores
such that learning from easy to difficult data can be performed. To evaluate
LATTICE, we use five publicly available datasets collected from five real-world
CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art
anomaly detectors. Evaluation results show that LATTICE outperforms the three
baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also,
on average, reduces the training time of ATTAIN by 4.2% on the five datasets
and is on par with the baselines in terms of detection delay time.
</p></li>
</ul>

<h3>Title: Cyber Sentinel: Exploring Conversational Agents in Streamlining Security Tasks with GPT-4. (arXiv:2309.16422v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16422">http://arxiv.org/abs/2309.16422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16422]] Cyber Sentinel: Exploring Conversational Agents in Streamlining Security Tasks with GPT-4(http://arxiv.org/abs/2309.16422)</code></li>
<li>Summary: <p>In an era where cyberspace is both a battleground and a backbone of modern
society, the urgency of safeguarding digital assets against ever-evolving
threats is paramount. This paper introduces Cyber Sentinel, an innovative
task-oriented cybersecurity dialogue system that is effectively capable of
managing two core functions: explaining potential cyber threats within an
organization to the user, and taking proactive/reactive security actions when
instructed by the user. Cyber Sentinel embodies the fusion of artificial
intelligence, cybersecurity domain expertise, and real-time data analysis to
combat the multifaceted challenges posed by cyber adversaries. This article
delves into the process of creating such a system and how it can interact with
other components typically found in cybersecurity organizations. Our work is a
novel approach to task-oriented dialogue systems, leveraging the power of
chaining GPT-4 models combined with prompt engineering across all sub-tasks. We
also highlight its pivotal role in enhancing cybersecurity communication and
interaction, concluding that not only does this framework enhance the system's
transparency (Explainable AI) but also streamlines the decision-making process
and responding to threats (Actionable AI), therefore marking a significant
advancement in the realm of cybersecurity communication.
</p></li>
</ul>

<h3>Title: Efficient Hardware Implementation of Constant Time Sampling for HQC. (arXiv:2309.16493v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16493">http://arxiv.org/abs/2309.16493</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16493]] Efficient Hardware Implementation of Constant Time Sampling for HQC(http://arxiv.org/abs/2309.16493)</code></li>
<li>Summary: <p>HQC is one of the code-based finalists in the last round of the NIST post
quantum cryptography standardization process. In this process, security and
implementation efficiency are key metrics for the selection of the candidates.
A critical compute kernel with respect to efficient hardware implementations
and security in HQC is the sampling method used to derive random numbers. Due
to its security criticality, recently an updated sampling algorithm was
presented to increase its robustness against side-channel attacks.
</p>
<p>In this paper, we pursue a cross layer approach to optimize this new sampling
algorithm to enable an efficient hardware implementation without comprising the
original algorithmic security and side-channel attack robustness.
</p>
<p>We compare our cross layer based implementation to a direct hardware
implementation of the original algorithm and to optimized implementations of
the previous sampler version. All implementations are evaluated using the
Xilinx Artix 7 FPGA. Our results show that our approach reduces the latency by
a factor of 24 compared to the original algorithm and by a factor of 28
compared to the previously used sampler with significantly less resources.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble. (arXiv:2309.16082v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16082">http://arxiv.org/abs/2309.16082</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16082]] Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble(http://arxiv.org/abs/2309.16082)</code></li>
<li>Summary: <p>Recent research has shown that language models have a tendency to memorize
rare or unique token sequences in the training corpus. After deploying a model,
practitioners might be asked to delete any personal information from the model
by individuals' requests. Re-training the underlying model every time
individuals would like to practice their rights to be forgotten is
computationally expensive. We employ a teacher-student framework and propose a
novel leave-one-out ensemble method to unlearn the targeted textual sequences
that need to be forgotten from the model. In our approach, multiple teachers
are trained on disjoint sets; for each targeted sequence to be removed, we
exclude the teacher trained on the set containing this sequence and aggregate
the predictions from remaining teachers to provide supervision during
fine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that the
proposed method achieves superior privacy-utility trade-offs than other
counterparts.
</p></li>
</ul>

<h3>Title: OPPO: An Ontology for Describing Fine-Grained Data Practices in Privacy Policies of Online Social Networks. (arXiv:2309.15971v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15971">http://arxiv.org/abs/2309.15971</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15971]] OPPO: An Ontology for Describing Fine-Grained Data Practices in Privacy Policies of Online Social Networks(http://arxiv.org/abs/2309.15971)</code></li>
<li>Summary: <p>Privacy policies outline the data practices of Online Social Networks (OSN)
to comply with privacy regulations such as the EU-GDPR and CCPA. Several
ontologies for modeling privacy regulations, policies, and compliance have
emerged in recent years. However, they are limited in various ways: (1) they
specifically model what is required of privacy policies according to one
specific privacy regulation such as GDPR; (2) they provide taxonomies of
concepts but are not sufficiently axiomatized to afford automated reasoning
with them; and (3) they do not model data practices of privacy policies in
sufficient detail to allow assessing the transparency of policies. This paper
presents an OWL Ontology for Privacy Policies of OSNs, OPPO, that aims to fill
these gaps by formalizing detailed data practices from OSNS' privacy policies.
OPPO is grounded in BFO, IAO, OMRSE, and OBI, and its design is guided by the
use case of representing and reasoning over the content of OSNs' privacy
policies and evaluating policies' transparency in greater detail.
</p></li>
</ul>

<h3>Title: Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey. (arXiv:2309.16398v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16398">http://arxiv.org/abs/2309.16398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16398]] Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey(http://arxiv.org/abs/2309.16398)</code></li>
<li>Summary: <p>Differential Privacy has become a widely popular method for data protection
in machine learning, especially since it allows formulating strict mathematical
privacy guarantees. This survey provides an overview of the state-of-the-art of
differentially private centralized deep learning, thorough analyses of recent
advances and open problems, as well as a discussion of potential future
developments in the field. Based on a systematic literature review, the
following topics are addressed: auditing and evaluation methods for private
models, improvements of privacy-utility trade-offs, protection against a broad
range of threats and attacks, differentially private generative models, and
emerging application domains.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Nonconvex third-order Tensor Recovery Based on Logarithmic Minimax Function. (arXiv:2309.16208v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16208">http://arxiv.org/abs/2309.16208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16208]] Nonconvex third-order Tensor Recovery Based on Logarithmic Minimax Function(http://arxiv.org/abs/2309.16208)</code></li>
<li>Summary: <p>Recent researches have shown that low-rank tensor recovery based non-convex
relaxation has gained extensive attention. In this context, we propose a new
Logarithmic Minimax (LM) function. The comparative analysis between the LM
function and the Logarithmic, Minimax concave penalty (MCP), and Minimax
Logarithmic concave penalty (MLCP) functions reveals that the proposed function
can protect large singular values while imposing stronger penalization on small
singular values. Based on this, we define a weighted tensor LM norm as a
non-convex relaxation for tensor tubal rank. Subsequently, we propose the
TLM-based low-rank tensor completion (LRTC) model and the TLM-based tensor
robust principal component analysis (TRPCA) model respectively. Furthermore, we
provide theoretical convergence guarantees for the proposed methods.
Comprehensive experiments were conducted on various real datasets, and a
comparison analysis was made with the similar EMLCP method. The results
demonstrate that the proposed method outperforms the state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation. (arXiv:2309.16173v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16173">http://arxiv.org/abs/2309.16173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16173]] Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation(http://arxiv.org/abs/2309.16173)</code></li>
<li>Summary: <p>Graph unlearning has emerged as a pivotal method to delete information from a
pre-trained graph neural network (GNN). One may delete nodes, a class of nodes,
edges, or a class of edges. An unlearning method enables the GNN model to
comply with data protection regulations (i.e., the right to be forgotten),
adapt to evolving data distributions, and reduce the GPU-hours carbon footprint
by avoiding repetitive retraining. Existing partitioning and aggregation-based
methods have limitations due to their poor handling of local graph dependencies
and additional overhead costs. More recently, GNNDelete offered a
model-agnostic approach that alleviates some of these issues. Our work takes a
novel approach to address these challenges in graph unlearning through
knowledge distillation, as it distills to delete in GNN (D2DGN). It is a
model-agnostic distillation framework where the complete graph knowledge is
divided and marked for retention and deletion. It performs distillation with
response-based soft targets and feature-based node embedding while minimizing
KL divergence. The unlearned model effectively removes the influence of deleted
graph elements while preserving knowledge about the retained graph elements.
D2DGN surpasses the performance of existing methods when evaluated on various
real-world graph datasets by up to $43.1\%$ (AUC) in edge and node unlearning
tasks. Other notable advantages include better efficiency, better performance
in removing target elements, preservation of performance for the retained
elements, and zero overhead costs. Notably, our D2DGN surpasses the
state-of-the-art GNNDelete in AUC by $2.4\%$, improves membership inference
ratio by $+1.3$, requires $10.2\times10^6$ fewer FLOPs per forward pass and up
to $\mathbf{3.2}\times$ faster.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Compilation as a Defense: Enhancing DL Model Attack Robustness via Tensor Optimization. (arXiv:2309.16577v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16577">http://arxiv.org/abs/2309.16577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16577]] Compilation as a Defense: Enhancing DL Model Attack Robustness via Tensor Optimization(http://arxiv.org/abs/2309.16577)</code></li>
<li>Summary: <p>Adversarial Machine Learning (AML) is a rapidly growing field of security
research, with an often overlooked area being model attacks through
side-channels. Previous works show such attacks to be serious threats, though
little progress has been made on efficient remediation strategies that avoid
costly model re-engineering. This work demonstrates a new defense against AML
side-channel attacks using model compilation techniques, namely tensor
optimization. We show relative model attack effectiveness decreases of up to
43% using tensor optimization, discuss the implications, and direction of
future work.
</p></li>
</ul>

<h3>Title: Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials. (arXiv:2309.16571v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16571">http://arxiv.org/abs/2309.16571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16571]] Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials(http://arxiv.org/abs/2309.16571)</code></li>
<li>Summary: <p>Additive manufacturing has revolutionized the manufacturing of complex parts
by enabling direct material joining and offers several advantages such as
cost-effective manufacturing of complex parts, reducing manufacturing waste,
and opening new possibilities for manufacturing automation. One group of
materials for which additive manufacturing holds great potential for enhancing
component performance and properties is Functionally Graded Materials (FGMs).
FGMs are advanced composite materials that exhibit smoothly varying properties
making them desirable for applications in aerospace, automobile, biomedical,
and defense industries. Such composition differs from traditional composite
materials, since the location-dependent composition changes gradually in FGMs,
leading to enhanced properties. Recently, machine learning techniques have
emerged as a promising means for fabrication of FGMs through optimizing
processing parameters, improving product quality, and detecting manufacturing
defects. This paper first provides a brief literature review of works related
to FGM fabrication, followed by reviewing works on employing machine learning
in additive manufacturing, Afterward, we provide an overview of published works
in the literature related to the application of machine learning methods in
Directed Energy Deposition and for fabrication of FGMs.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency. (arXiv:2309.16211v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16211">http://arxiv.org/abs/2309.16211</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16211]] VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency(http://arxiv.org/abs/2309.16211)</code></li>
<li>Summary: <p>The role of data in building AI systems has recently been emphasized by the
emerging concept of data-centric AI. Unfortunately, in the real-world, datasets
may contain dirty samples, such as poisoned samples from backdoor attack, noisy
labels in crowdsourcing, and even hybrids of them. The presence of such dirty
samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect
dirty samples to improve the quality and realiability of dataset. Existing
detectors only focus on detecting poisoned samples or noisy labels, that are
often prone to weak generalization when dealing with dirty samples from other
domains.In this paper, we find a commonality of various dirty samples is
visual-linguistic inconsistency between images and associated labels. To
capture the semantic inconsistency between modalities, we propose versatile
data cleanser (VDC) leveraging the surpassing capabilities of multimodal large
language models (MLLM) in cross-modal alignment and reasoning.It consists of
three consecutive modules: the visual question generation module to generate
insightful questions about the image; the visual question answering module to
acquire the semantics of the visual content by answering the questions with
MLLM; followed by the visual answer evaluation module to evaluate the
inconsistency.Extensive experiments demonstrate its superior performance and
generalization to various categories and types of dirty samples.
</p></li>
</ul>

<h3>Title: Random and Safe Cache Architecture to Defeat Cache Timing Attacks. (arXiv:2309.16172v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16172">http://arxiv.org/abs/2309.16172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16172]] Random and Safe Cache Architecture to Defeat Cache Timing Attacks(http://arxiv.org/abs/2309.16172)</code></li>
<li>Summary: <p>Caches have been exploited to leak secret information due to the different
times they take to handle memory accesses. Cache timing attacks include
non-speculative cache side and covert channel attacks and cache-based
speculative execution attacks. We first present a systematic view of the attack
and defense space and show that no existing defense has addressed both
speculative and non-speculative cache timing attack families, which we do in
this paper. We propose Random and Safe (RaS) cache architectures to decorrelate
the cache state changes from memory requests. RaS fills the cache with ``safe''
cache lines that are likely to be used in the future, rather than with
demand-fetched, security-sensitive lines. RaS captures a group of safe
addresses during runtime and fetches addresses randomly displaced from these
addresses. Our proposed RaS architecture is flexible to allow
security-performance trade-offs. We show different designs of RaS architectures
that can defeat cache side-channel attacks and cache-based speculative
execution attacks. The RaS variant against cache-based speculative execution
attacks has 4.2% average performance overhead and other RaS variants against
both attack families have 7.9% to 45.2% average overhead. For some benchmarks,
RaS defenses improve the performance while providing security.
</p></li>
</ul>

<h3>Title: Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective. (arXiv:2309.16456v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16456">http://arxiv.org/abs/2309.16456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16456]] Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective(http://arxiv.org/abs/2309.16456)</code></li>
<li>Summary: <p>Existing approaches defend against backdoor attacks in federated learning
(FL) mainly through a) mitigating the impact of infected models, or b)
excluding infected models. The former negatively impacts model accuracy, while
the latter usually relies on globally clear boundaries between benign and
infected model updates. However, model updates are easy to be mixed and
scattered throughout in reality due to the diverse distributions of local data.
This work focuses on excluding infected models in FL. Unlike previous
perspectives from a global view, we propose Snowball, a novel anti-backdoor FL
framework through bidirectional elections from an individual perspective
inspired by one principle deduced by us and two principles in FL and deep
learning. It is characterized by a) bottom-up election, where each candidate
model update votes to several peer ones such that a few model updates are
elected as selectees for aggregation; and b) top-down election, where selectees
progressively enlarge themselves through picking up from the candidates. We
compare Snowball with state-of-the-art defenses to backdoor attacks in FL on
five real-world datasets, demonstrating its superior resistance to backdoor
attacks and slight impact on the accuracy of the global model.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness. (arXiv:2309.15991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15991">http://arxiv.org/abs/2309.15991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15991]] Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness(http://arxiv.org/abs/2309.15991)</code></li>
<li>Summary: <p>Artificial neural networks typically struggle in generalizing to
out-of-context examples. One reason for this limitation is caused by having
datasets that incorporate only partial information regarding the potential
correlational structure of the world. In this work, we propose TIDA (Targeted
Image-editing Data Augmentation), a targeted data augmentation method focused
on improving models' human-like abilities (e.g., gender recognition) by filling
the correlational structure gap using a text-to-image generative model. More
specifically, TIDA identifies specific skills in captions describing images
(e.g., the presence of a specific gender in the image), changes the caption
(e.g., "woman" to "man"), and then uses a text-to-image model to edit the image
in order to match the novel caption (e.g., uniquely changing a woman to a man
while maintaining the context identical). Based on the Flickr30K benchmark, we
show that, compared with the original data set, a TIDA-enhanced dataset related
to gender, color, and counting abilities induces better performance in several
image captioning metrics. Furthermore, on top of relying on the classical BLEU
metric, we conduct a fine-grained analysis of the improvements of our models
against the baseline in different ways. We compared text-to-image generative
models and found different behaviors of the image captioning models in terms of
encoding visual encoding and textual decoding.
</p></li>
</ul>

<h3>Title: Q-REG: End-to-End Trainable Point Cloud Registration with Surface Curvature. (arXiv:2309.16023v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16023">http://arxiv.org/abs/2309.16023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16023]] Q-REG: End-to-End Trainable Point Cloud Registration with Surface Curvature(http://arxiv.org/abs/2309.16023)</code></li>
<li>Summary: <p>Point cloud registration has seen recent success with several learning-based
methods that focus on correspondence matching and, as such, optimize only for
this objective. Following the learning step of correspondence matching, they
evaluate the estimated rigid transformation with a RANSAC-like framework. While
it is an indispensable component of these methods, it prevents a fully
end-to-end training, leaving the objective to minimize the pose error
nonserved. We present a novel solution, Q-REG, which utilizes rich geometric
information to estimate the rigid pose from a single correspondence. Q-REG
allows to formalize the robust estimation as an exhaustive search, hence
enabling end-to-end training that optimizes over both objectives of
correspondence matching and rigid pose estimation. We demonstrate in the
experiments that Q-REG is agnostic to the correspondence matching method and
provides consistent improvement both when used only in inference and in
end-to-end training. It sets a new state-of-the-art on the 3DMatch, KITTI, and
ModelNet benchmarks.
</p></li>
</ul>

<h3>Title: Handbook on Leveraging Lines for Two-View Relative Pose Estimation. (arXiv:2309.16040v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16040">http://arxiv.org/abs/2309.16040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16040]] Handbook on Leveraging Lines for Two-View Relative Pose Estimation(http://arxiv.org/abs/2309.16040)</code></li>
<li>Summary: <p>We propose an approach for estimating the relative pose between calibrated
image pairs by jointly exploiting points, lines, and their coincidences in a
hybrid manner. We investigate all possible configurations where these data
modalities can be used together and review the minimal solvers available in the
literature. Our hybrid framework combines the advantages of all configurations,
enabling robust and accurate estimation in challenging environments. In
addition, we design a method for jointly estimating multiple vanishing point
correspondences in two images, and a bundle adjustment that considers all
relevant data modalities. Experiments on various indoor and outdoor datasets
show that our approach outperforms point-based methods, improving
AUC@10$^\circ$ by 1-7 points while running at comparable speeds. The source
code of the solvers and hybrid framework will be made public.
</p></li>
</ul>

<h3>Title: BEVHeight++: Toward Robust Visual Centric 3D Object Detection. (arXiv:2309.16179v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16179">http://arxiv.org/abs/2309.16179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16179]] BEVHeight++: Toward Robust Visual Centric 3D Object Detection(http://arxiv.org/abs/2309.16179)</code></li>
<li>Summary: <p>While most recent autonomous driving system focuses on developing perception
methods on ego-vehicle sensors, people tend to overlook an alternative approach
to leverage intelligent roadside cameras to extend the perception ability
beyond the visual range. We discover that the state-of-the-art vision-centric
bird's eye view detection methods have inferior performances on roadside
cameras. This is because these methods mainly focus on recovering the depth
regarding the camera center, where the depth difference between the car and the
ground quickly shrinks while the distance increases. In this paper, we propose
a simple yet effective approach, dubbed BEVHeight++, to address this issue. In
essence, we regress the height to the ground to achieve a distance-agnostic
formulation to ease the optimization process of camera-only perception methods.
By incorporating both height and depth encoding techniques, we achieve a more
accurate and robust projection from 2D to BEV spaces. On popular 3D detection
benchmarks of roadside cameras, our method surpasses all previous
vision-centric methods by a significant margin. In terms of the ego-vehicle
scenario, our BEVHeight++ possesses superior over depth-only methods.
Specifically, it yields a notable improvement of +1.9% NDS and +1.1% mAP over
BEVDepth when evaluated on the nuScenes validation set. Moreover, on the
nuScenes test set, our method achieves substantial advancements, with an
increase of +2.8% NDS and +1.7% mAP, respectively.
</p></li>
</ul>

<h3>Title: Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation Robustness via Hypernetworks. (arXiv:2309.16207v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16207">http://arxiv.org/abs/2309.16207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16207]] Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation Robustness via Hypernetworks(http://arxiv.org/abs/2309.16207)</code></li>
<li>Summary: <p>Adversarial training serves as one of the most popular and effective methods
to defend against adversarial perturbations. However, most defense mechanisms
only consider a single type of perturbation while various attack methods might
be adopted to perform stronger adversarial attacks against the deployed model
in real-world scenarios, e.g., $\ell_2$ or $\ell_\infty$. Defending against
various attacks can be a challenging problem since multi-perturbation
adversarial training and its variants only achieve suboptimal robustness
trade-offs, due to the theoretical limit to multi-perturbation robustness for a
single model. Besides, it is impractical to deploy large models in some
storage-efficient scenarios. To settle down these drawbacks, in this paper we
propose a novel multi-perturbation adversarial training framework,
parameter-saving adversarial training (PSAT), to reinforce multi-perturbation
robustness with an advantageous side effect of saving parameters, which
leverages hypernetworks to train specialized models against a single
perturbation and aggregate these specialized models to defend against multiple
perturbations. Eventually, we extensively evaluate and compare our proposed
method with state-of-the-art single/multi-perturbation robust methods against
various latest attack methods on different datasets, showing the robustness
superiority and parameter efficiency of our proposed method, e.g., for the
CIFAR-10 dataset with ResNet-50 as the backbone, PSAT saves approximately 80\%
of parameters with achieving the state-of-the-art robustness trade-off
accuracy.
</p></li>
</ul>

<h3>Title: Nondestructive chicken egg fertility detection using CNN-transfer learning algorithms. (arXiv:2309.16257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16257">http://arxiv.org/abs/2309.16257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16257]] Nondestructive chicken egg fertility detection using CNN-transfer learning algorithms(http://arxiv.org/abs/2309.16257)</code></li>
<li>Summary: <p>This study explored the application of CNN-Transfer Learning for
nondestructive chicken egg fertility detection for precision poultry hatchery
practices. Four models, VGG16, ResNet50, InceptionNet, and MobileNet, were
trained and evaluated on a dataset (200 single egg images) using augmented
images (rotation, flip, scale, translation, and reflection). Although the
training results demonstrated that all models achieved high accuracy,
indicating their ability to accurately learn and classify chicken eggs'
fertility state, when evaluated on the testing set, variations in accuracy and
performance were observed. InceptionNet exhibited the best overall performance,
accurately classifying fertile and non-fertile eggs. It demonstrated excellent
performance in both training and testing sets in all parameters of the
evaluation metrics. In testing set, it achieved an accuracy of 0.98, a
sensitivity of 1 for detecting fertile eggs, and a specificity of 0.96 for
identifying non-fertile eggs. The higher performance is attributed to its
unique architecture efficiently capturing features at different scales leading
to improved accuracy and robustness. Further optimization and fine-tuning of
the models might necessary to address the limitations in accurately detecting
fertile and non-fertile eggs in case of other models. This study highlighted
the potential of CNN-Transfer Learning for nondestructive fertility detection
and emphasizes the need for further research to enhance the models'
capabilities and ensure accurate classification.
</p></li>
</ul>

<h3>Title: Aperture Diffraction for Compact Snapshot Spectral Imaging. (arXiv:2309.16372v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16372">http://arxiv.org/abs/2309.16372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16372]] Aperture Diffraction for Compact Snapshot Spectral Imaging(http://arxiv.org/abs/2309.16372)</code></li>
<li>Summary: <p>We demonstrate a compact, cost-effective snapshot spectral imaging system
named Aperture Diffraction Imaging Spectrometer (ADIS), which consists only of
an imaging lens with an ultra-thin orthogonal aperture mask and a mosaic filter
sensor, requiring no additional physical footprint compared to common RGB
cameras. Then we introduce a new optical design that each point in the object
space is multiplexed to discrete encoding locations on the mosaic filter sensor
by diffraction-based spatial-spectral projection engineering generated from the
orthogonal mask. The orthogonal projection is uniformly accepted to obtain a
weakly calibration-dependent data form to enhance modulation robustness.
Meanwhile, the Cascade Shift-Shuffle Spectral Transformer (CSST) with strong
perception of the diffraction degeneration is designed to solve a
sparsity-constrained inverse problem, realizing the volume reconstruction from
2D measurements with Large amount of aliasing. Our system is evaluated by
elaborating the imaging optical theory and reconstruction algorithm with
demonstrating the experimental imaging under a single exposure. Ultimately, we
achieve the sub-super-pixel spatial resolution and high spectral resolution
imaging. The code will be available at: https://github.com/Krito-ex/CSST.
</p></li>
</ul>

<h3>Title: Biomedical Image Splicing Detection using Uncertainty-Guided Refinement. (arXiv:2309.16388v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16388">http://arxiv.org/abs/2309.16388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16388]] Biomedical Image Splicing Detection using Uncertainty-Guided Refinement(http://arxiv.org/abs/2309.16388)</code></li>
<li>Summary: <p>Recently, a surge in biomedical academic publications suspected of image
manipulation has led to numerous retractions, turning biomedical image
forensics into a research hotspot. While manipulation detectors are concerning,
the specific detection of splicing traces in biomedical images remains
underexplored. The disruptive factors within biomedical images, such as
artifacts, abnormal patterns, and noises, show misleading features like the
splicing traces, greatly increasing the challenge for this task. Moreover, the
scarcity of high-quality spliced biomedical images also limits potential
advancements in this field. In this work, we propose an Uncertainty-guided
Refinement Network (URN) to mitigate the effects of these disruptive factors.
Our URN can explicitly suppress the propagation of unreliable information flow
caused by disruptive factors among regions, thereby obtaining robust features.
Moreover, URN enables a concentration on the refinement of uncertainly
predicted regions during the decoding phase. Besides, we construct a dataset
for Biomedical image Splicing (BioSp) detection, which consists of 1,290
spliced images. Compared with existing datasets, BioSp comprises the largest
number of spliced images and the most diverse sources. Comprehensive
experiments on three benchmark datasets demonstrate the superiority of the
proposed method. Meanwhile, we verify the generalizability of URN when against
cross-dataset domain shifts and its robustness to resist post-processing
approaches. Our BioSp dataset will be released upon acceptance.
</p></li>
</ul>

<h3>Title: Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering. (arXiv:2309.16451v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16451">http://arxiv.org/abs/2309.16451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16451]] Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering(http://arxiv.org/abs/2309.16451)</code></li>
<li>Summary: <p>Existing deep learning models have achieved promising performance in
recognizing skin diseases from dermoscopic images. However, these models can
only recognize samples from predefined categories, when they are deployed in
the clinic, data from new unknown categories are constantly emerging.
Therefore, it is crucial to automatically discover and identify new semantic
categories from new data. In this paper, we propose a new novel class discovery
framework for automatically discovering new semantic classes from dermoscopy
image datasets based on the knowledge of known classes. Specifically, we first
use contrastive learning to learn a robust and unbiased feature representation
based on all data from known and unknown categories. We then propose an
uncertainty-aware multi-view cross pseudo-supervision strategy, which is
trained jointly on all categories of data using pseudo labels generated by a
self-labeling strategy. Finally, we further refine the pseudo label by
aggregating neighborhood information through local sample similarity to improve
the clustering performance of the model for unknown categories. We conducted
extensive experiments on the dermatology dataset ISIC 2019, and the
experimental results show that our approach can effectively leverage knowledge
from known categories to discover new semantic categories. We also further
validated the effectiveness of the different modules through extensive ablation
experiments. Our code will be released soon.
</p></li>
</ul>

<h3>Title: Rethinking Domain Generalization: Discriminability and Generalizability. (arXiv:2309.16483v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16483">http://arxiv.org/abs/2309.16483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16483]] Rethinking Domain Generalization: Discriminability and Generalizability(http://arxiv.org/abs/2309.16483)</code></li>
<li>Summary: <p>Domain generalization (DG) endeavors to develop robust models that possess
strong generalizability while preserving excellent discriminability.
Nonetheless, pivotal DG techniques tend to improve the feature generalizability
by learning domain-invariant representations, inadvertently overlooking the
feature discriminability. On the one hand, the simultaneous attainment of
generalizability and discriminability of features presents a complex challenge,
often entailing inherent contradictions. This challenge becomes particularly
pronounced when domain-invariant features manifest reduced discriminability
owing to the inclusion of unstable factors, \emph{i.e.,} spurious correlations.
On the other hand, prevailing domain-invariant methods can be categorized as
category-level alignment, susceptible to discarding indispensable features
possessing substantial generalizability and narrowing intra-class variations.
To surmount these obstacles, we rethink DG from a new perspective that
concurrently imbues features with formidable discriminability and robust
generalizability, and present a novel framework, namely, Discriminative
Microscopic Distribution Alignment (DMDA). DMDA incorporates two core
components: Selective Channel Pruning~(SCP) and Micro-level Distribution
Alignment (MDA). Concretely, SCP attempts to curtail redundancy within neural
networks, prioritizing stable attributes conducive to accurate classification.
This approach alleviates the adverse effect of spurious domain invariance and
amplifies the feature discriminability. Besides, MDA accentuates micro-level
alignment within each class, going beyond mere category-level alignment. This
strategy accommodates sufficient generalizable features and facilitates
within-class variations. Extensive experiments on four benchmark datasets
corroborate the efficacy of our method.
</p></li>
</ul>

<h3>Title: HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs. (arXiv:2309.16524v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16524">http://arxiv.org/abs/2309.16524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16524]] HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs(http://arxiv.org/abs/2309.16524)</code></li>
<li>Summary: <p>Robots are becoming increasingly integrated into our lives, assisting us in
various tasks. To ensure effective collaboration between humans and robots, it
is essential that they understand our intentions and anticipate our actions. In
this paper, we propose a Human-Object Interaction (HOI) anticipation framework
for collaborative robots. We propose an efficient and robust transformer-based
model to detect and anticipate HOIs from videos. This enhanced anticipation
empowers robots to proactively assist humans, resulting in more efficient and
intuitive collaborations. Our model outperforms state-of-the-art results in HOI
detection and anticipation in VidHOI dataset with an increase of 1.76% and
1.04% in mAP respectively while being 15.4 times faster. We showcase the
effectiveness of our approach through experimental results in a real robot,
demonstrating that the robot's ability to anticipate HOIs is key for better
Human-Robot Interaction. More information can be found on our project webpage:
https://evm7.github.io/HOI4ABOT_page/
</p></li>
</ul>

<h3>Title: Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection. (arXiv:2309.16592v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16592">http://arxiv.org/abs/2309.16592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16592]] Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection(http://arxiv.org/abs/2309.16592)</code></li>
<li>Summary: <p>The primary bottleneck towards obtaining good recognition performance in IR
images is the lack of sufficient labeled training data, owing to the cost of
acquiring such data. Realizing that object detection methods for the RGB
modality are quite robust (at least for some commonplace classes, like person,
car, etc.), thanks to the giant training sets that exist, in this work we seek
to leverage cues from the RGB modality to scale object detectors to the IR
modality, while preserving model performance in the RGB modality. At the core
of our method, is a novel tensor decomposition method called TensorFact which
splits the convolution kernels of a layer of a Convolutional Neural Network
(CNN) into low-rank factor matrices, with fewer parameters than the original
CNN. We first pretrain these factor matrices on the RGB modality, for which
plenty of training data are assumed to exist and then augment only a few
trainable parameters for training on the IR modality to avoid over-fitting,
while encouraging them to capture complementary cues from those trained only on
the RGB modality. We validate our approach empirically by first assessing how
well our TensorFact decomposed network performs at the task of detecting
objects in RGB images vis-a-vis the original network and then look at how well
it adapts to IR images of the FLIR ADAS v1 dataset. For the latter, we train
models under scenarios that pose challenges stemming from data paucity. From
the experiments, we observe that: (i) TensorFact shows performance gains on RGB
images; (ii) further, this pre-trained model, when fine-tuned, outperforms a
standard state-of-the-art object detector on the FLIR ADAS v1 dataset by about
4% in terms of mAP 50 score.
</p></li>
</ul>

<h3>Title: Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16672">http://arxiv.org/abs/2309.16672</a></li>
<li>Code URL: https://github.com/sutkarsh/flow_inv</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16672]] Learning to Transform for Generalizable Instance-wise Invariance(http://arxiv.org/abs/2309.16672)</code></li>
<li>Summary: <p>Computer vision research has long aimed to build systems that are robust to
spatial transformations found in natural data. Traditionally, this is done
using data augmentation or hard-coding invariances into the architecture.
However, too much or too little invariance can hurt, and the correct amount is
unknown a priori and dependent on the instance. Ideally, the appropriate
invariance would be learned from data and inferred at test-time.
</p>
<p>We treat invariance as a prediction problem. Given any image, we use a
normalizing flow to predict a distribution over transformations and average the
predictions over them. Since this distribution only depends on the instance, we
can align instances before classifying them and generalize invariance across
classes. The same distribution can also be used to adapt to out-of-distribution
poses. This normalizing flow is trained end-to-end and can learn a much larger
range of transformations than Augerino and InstaAug. When used as data
augmentation, our method shows accuracy and robustness gains on CIFAR 10,
CIFAR10-LT, and TinyImageNet.
</p></li>
</ul>

<h3>Title: Projection based fuzzy least squares twin support vector machine for class imbalance problems. (arXiv:2309.15886v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15886">http://arxiv.org/abs/2309.15886</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15886]] Projection based fuzzy least squares twin support vector machine for class imbalance problems(http://arxiv.org/abs/2309.15886)</code></li>
<li>Summary: <p>Class imbalance is a major problem in many real world classification tasks.
Due to the imbalance in the number of samples, the support vector machine (SVM)
classifier gets biased toward the majority class. Furthermore, these samples
are often observed with a certain degree of noise. Therefore, to remove these
problems we propose a novel fuzzy based approach to deal with class imbalanced
as well noisy datasets. We propose two approaches to address these problems.
The first approach is based on the intuitionistic fuzzy membership, termed as
robust energy-based intuitionistic fuzzy least squares twin support vector
machine (IF-RELSTSVM). Furthermore, we introduce the concept of
hyperplane-based fuzzy membership in our second approach, where the final
classifier is termed as robust energy-based fuzzy least square twin support
vector machine (F-RELSTSVM). By using this technique, the membership values are
based on a projection based approach, where the data points are projected on
the hyperplanes. The performance of the proposed algorithms is evaluated on
several benchmark and synthetic datasets. The experimental results show that
the proposed IF-RELSTSVM and F-RELSTSVM models outperform the baseline
algorithms. Statistical tests are performed to check the significance of the
proposed algorithms. The results show the applicability of the proposed
algorithms on noisy as well as imbalanced datasets.
</p></li>
</ul>

<h3>Title: Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16096">http://arxiv.org/abs/2309.16096</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16096]] Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness(http://arxiv.org/abs/2309.16096)</code></li>
<li>Summary: <p>The susceptibility of modern machine learning classifiers to adversarial
examples has motivated theoretical results suggesting that these might be
unavoidable. However, these results can be too general to be applicable to
natural data distributions. Indeed, humans are quite robust for tasks involving
vision. This apparent conflict motivates a deeper dive into the question: Are
adversarial examples truly unavoidable? In this work, we theoretically
demonstrate that a key property of the data distribution -- concentration on
small-volume subsets of the input space -- determines whether a robust
classifier exists. We further demonstrate that, for a data distribution
concentrated on a union of low-dimensional linear subspaces, exploiting data
structure naturally leads to classifiers that enjoy good robustness guarantees,
improving upon methods for provable certification in certain regimes.
</p></li>
</ul>

<h3>Title: Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics. (arXiv:2309.16109v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16109">http://arxiv.org/abs/2309.16109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16109]] Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics(http://arxiv.org/abs/2309.16109)</code></li>
<li>Summary: <p>Contrastive learning is a self-supervised representation learning framework,
where two positive views generated through data augmentation are made similar
by an attraction force in a data representation space, while a repulsive force
makes them far from negative examples. Non-contrastive learning, represented by
BYOL and SimSiam, further gets rid of negative examples and improves
computational efficiency. While learned representations may collapse into a
single point due to the lack of the repulsive force at first sight, Tian et al.
(2021) revealed through the learning dynamics analysis that the representations
can avoid collapse if data augmentation is sufficiently stronger than
regularization. However, their analysis does not take into account
commonly-used feature normalization, a normalizer before measuring the
similarity of representations, and hence excessively strong regularization may
collapse the dynamics, which is an unnatural behavior under the presence of
feature normalization. Therefore, we extend the previous theory based on the L2
loss by considering the cosine loss, which involves feature normalization. We
show that the cosine loss induces sixth-order dynamics (while the L2 loss
induces a third-order one), in which a stable equilibrium dynamically emerges
even if there are only collapsed solutions with given initial parameters. Thus,
we offer a new understanding that feature normalization plays an important role
in robustly preventing the dynamics collapse.
</p></li>
</ul>

<h3>Title: On the Trade-offs between Adversarial Robustness and Actionable Explanations. (arXiv:2309.16452v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16452">http://arxiv.org/abs/2309.16452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16452]] On the Trade-offs between Adversarial Robustness and Actionable Explanations(http://arxiv.org/abs/2309.16452)</code></li>
<li>Summary: <p>As machine learning models are increasingly being employed in various
high-stakes settings, it becomes important to ensure that predictions of these
models are not only adversarially robust, but also readily explainable to
relevant stakeholders. However, it is unclear if these two notions can be
simultaneously achieved or if there exist trade-offs between them. In this
work, we make one of the first attempts at studying the impact of adversarially
robust models on actionable explanations which provide end users with a means
for recourse. We theoretically and empirically analyze the cost (ease of
implementation) and validity (probability of obtaining a positive model
prediction) of recourses output by state-of-the-art algorithms when the
underlying models are adversarially robust vs. non-robust. More specifically,
we derive theoretical bounds on the differences between the cost and the
validity of the recourses generated by state-of-the-art algorithms for
adversarially robust vs. non-robust linear and non-linear models. Our empirical
results with multiple real-world datasets validate our theoretical results and
show the impact of varying degrees of model robustness on the cost and validity
of the resulting recourses. Our analyses demonstrate that adversarially robust
models significantly increase the cost and reduce the validity of the resulting
recourses, thus shedding light on the inherent trade-offs between adversarial
robustness and actionable explanations
</p></li>
</ul>

<h3>Title: Robust Offline Reinforcement Learning -- Certify the Confidence Interval. (arXiv:2309.16631v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16631">http://arxiv.org/abs/2309.16631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16631]] Robust Offline Reinforcement Learning -- Certify the Confidence Interval(http://arxiv.org/abs/2309.16631)</code></li>
<li>Summary: <p>Currently, reinforcement learning (RL), especially deep RL, has received more
and more attention in the research area. However, the security of RL has been
an obvious problem due to the attack manners becoming mature. In order to
defend against such adversarial attacks, several practical approaches are
developed, such as adversarial training, data filtering, etc. However, these
methods are mostly based on empirical algorithms and experiments, without
rigorous theoretical analysis of the robustness of the algorithms. In this
paper, we develop an algorithm to certify the robustness of a given policy
offline with random smoothing, which could be proven and conducted as
efficiently as ones without random smoothing. Experiments on different
environments confirm the correctness of our algorithm.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: OSM-Net: One-to-Many One-shot Talking Head Generation with Spontaneous Head Motions. (arXiv:2309.16148v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16148">http://arxiv.org/abs/2309.16148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16148]] OSM-Net: One-to-Many One-shot Talking Head Generation with Spontaneous Head Motions(http://arxiv.org/abs/2309.16148)</code></li>
<li>Summary: <p>One-shot talking head generation has no explicit head movement reference,
thus it is difficult to generate talking heads with head motions. Some existing
works only edit the mouth area and generate still talking heads, leading to
unreal talking head performance. Other works construct one-to-one mapping
between audio signal and head motion sequences, introducing ambiguity
correspondences into the mapping since people can behave differently in head
motions when speaking the same content. This unreasonable mapping form fails to
model the diversity and produces either nearly static or even exaggerated head
motions, which are unnatural and strange. Therefore, the one-shot talking head
generation task is actually a one-to-many ill-posed problem and people present
diverse head motions when speaking. Based on the above observation, we propose
OSM-Net, a \textit{one-to-many} one-shot talking head generation network with
natural head motions. OSM-Net constructs a motion space that contains rich and
various clip-level head motion features. Each basis of the space represents a
feature of meaningful head motion in a clip rather than just a frame, thus
providing more coherent and natural motion changes in talking heads. The
driving audio is mapped into the motion space, around which various motion
features can be sampled within a reasonable range to achieve the one-to-many
mapping. Besides, the landmark constraint and time window feature input improve
the accurate expression feature extraction and video generation. Extensive
experiments show that OSM-Net generates more natural realistic head motions
under reasonable one-to-many mapping paradigm compared with other methods.
</p></li>
</ul>

<h3>Title: An Enhanced Low-Resolution Image Recognition Method for Traffic Environments. (arXiv:2309.16390v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16390">http://arxiv.org/abs/2309.16390</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16390]] An Enhanced Low-Resolution Image Recognition Method for Traffic Environments(http://arxiv.org/abs/2309.16390)</code></li>
<li>Summary: <p>Currently, low-resolution image recognition is confronted with a significant
challenge in the field of intelligent traffic perception. Compared to
high-resolution images, low-resolution images suffer from small size, low
quality, and lack of detail, leading to a notable decrease in the accuracy of
traditional neural network recognition algorithms. The key to low-resolution
image recognition lies in effective feature extraction. Therefore, this paper
delves into the fundamental dimensions of residual modules and their impact on
feature extraction and computational efficiency. Based on experiments, we
introduce a dual-branch residual network structure that leverages the basic
architecture of residual networks and a common feature subspace algorithm.
Additionally, it incorporates the utilization of intermediate-layer features to
enhance the accuracy of low-resolution image recognition. Furthermore, we
employ knowledge distillation to reduce network parameters and computational
overhead. Experimental results validate the effectiveness of this algorithm for
low-resolution image recognition in traffic environments.
</p></li>
</ul>

<h3>Title: Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization. (arXiv:2309.16494v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16494">http://arxiv.org/abs/2309.16494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16494]] Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization(http://arxiv.org/abs/2309.16494)</code></li>
<li>Summary: <p>Recently, deep learning-based methods have dominated image dehazing domain.
Although very competitive dehazing performance has been achieved with
sophisticated models, effective solutions for extracting useful features are
still under-explored. In addition, non-local network, which has made a
breakthrough in many vision tasks, has not been appropriately applied to image
dehazing. Thus, a multi-receptive-field non-local network (MRFNLN) consisting
of the multi-stream feature attention block (MSFAB) and cross non-local block
(CNLB) is presented in this paper. We start with extracting richer features for
dehazing. Specifically, we design a multi-stream feature extraction (MSFE)
sub-block, which contains three parallel convolutions with different receptive
fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$) for extracting multi-scale
features. Following MSFE, we employ an attention sub-block to make the model
adaptively focus on important channels/regions. The MSFE and attention
sub-blocks constitute our MSFAB. Then, we design a cross non-local block
(CNLB), which can capture long-range dependencies beyond the query. Instead of
the same input source of query branch, the key and value branches are enhanced
by fusing more preceding features. CNLB is computation-friendly by leveraging a
spatial pyramid down-sampling (SPDS) strategy to reduce the computation and
memory consumption without sacrificing the performance. Last but not least, a
novel detail-focused contrastive regularization (DFCR) is presented by
emphasizing the low-level details and ignoring the high-level semantic
information in the representation space. Comprehensive experimental results
demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art
dehazing methods with less than 1.5 Million parameters.
</p></li>
</ul>

<h3>Title: Social Media Fashion Knowledge Extraction as Captioning. (arXiv:2309.16270v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16270">http://arxiv.org/abs/2309.16270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16270]] Social Media Fashion Knowledge Extraction as Captioning(http://arxiv.org/abs/2309.16270)</code></li>
<li>Summary: <p>Social media plays a significant role in boosting the fashion industry, where
a massive amount of fashion-related posts are generated every day. In order to
obtain the rich fashion information from the posts, we study the task of social
media fashion knowledge extraction. Fashion knowledge, which typically consists
of the occasion, person attributes, and fashion item information, can be
effectively represented as a set of tuples. Most previous studies on fashion
knowledge extraction are based on the fashion product images without
considering the rich text information in social media posts. Existing work on
fashion knowledge extraction in social media is classification-based and
requires to manually determine a set of fashion knowledge categories in
advance. In our work, we propose to cast the task as a captioning problem to
capture the interplay of the multimodal post information. Specifically, we
transform the fashion knowledge tuples into a natural language caption with a
sentence transformation method. Our framework then aims to generate the
sentence-based fashion knowledge directly from the social media post. Inspired
by the big success of pre-trained models, we build our model based on a
multimodal pre-trained generative model and design several auxiliary tasks for
enhancing the knowledge extraction. Since there is no existing dataset which
can be directly borrowed to our task, we introduce a dataset consisting of
social media posts with manual fashion knowledge annotation. Extensive
experiments are conducted to demonstrate the effectiveness of our model.
</p></li>
</ul>

<h3>Title: A Comprehensive Survey of Document-level Relation Extraction (2016-2022). (arXiv:2309.16396v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16396">http://arxiv.org/abs/2309.16396</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16396]] A Comprehensive Survey of Document-level Relation Extraction (2016-2022)(http://arxiv.org/abs/2309.16396)</code></li>
<li>Summary: <p>Document-level relation extraction (DocRE) is an active area of research in
natural language processing (NLP) concerned with identifying and extracting
relationships between entities beyond sentence boundaries. Compared to the more
traditional sentence-level relation extraction, DocRE provides a broader
context for analysis and is more challenging because it involves identifying
relationships that may span multiple sentences or paragraphs. This task has
gained increased interest as a viable solution to build and populate knowledge
bases automatically from unstructured large-scale documents (e.g., scientific
papers, legal contracts, or news articles), in order to have a better
understanding of relationships between entities. This paper aims to provide a
comprehensive overview of recent advances in this field, highlighting its
different applications in comparison to sentence-level relation extraction.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning. (arXiv:2309.16286v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16286">http://arxiv.org/abs/2309.16286</a></li>
<li>Code URL: https://github.com/wenkehuang/fccl</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16286]] Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning(http://arxiv.org/abs/2309.16286)</code></li>
<li>Summary: <p>Federated learning is an important privacy-preserving multi-party learning
paradigm, involving collaborative learning with others and local updating on
private data. Model heterogeneity and catastrophic forgetting are two crucial
challenges, which greatly limit the applicability and generalizability. This
paper presents a novel FCCL+, federated correlation and similarity learning
with non-target distillation, facilitating the both intra-domain
discriminability and inter-domain generalization. For heterogeneity issue, we
leverage irrelevant unlabeled public data for communication between the
heterogeneous participants. We construct cross-correlation matrix and align
instance similarity distribution on both logits and feature levels, which
effectively overcomes the communication barrier and improves the generalizable
ability. For catastrophic forgetting in local updating stage, FCCL+ introduces
Federated Non Target Distillation, which retains inter-domain knowledge while
avoiding the optimization conflict issue, fulling distilling privileged
inter-domain information through depicting posterior classes relation.
Considering that there is no standard benchmark for evaluating existing
heterogeneous federated learning under the same setting, we present a
comprehensive benchmark with extensive representative methods under four domain
shift scenarios, supporting both heterogeneous and homogeneous federated
settings. Empirical results demonstrate the superiority of our method and the
efficiency of modules on various scenarios.
</p></li>
</ul>

<h3>Title: EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect. (arXiv:2309.16338v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16338">http://arxiv.org/abs/2309.16338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16338]] EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect(http://arxiv.org/abs/2309.16338)</code></li>
<li>Summary: <p>Recent advances in federated learning (FL) enable collaborative training of
machine learning (ML) models from large-scale and widely dispersed clients
while protecting their privacy. However, when different clients' datasets are
heterogeneous, traditional FL mechanisms produce a global model that does not
adequately represent the poorer clients with limited data resources, resulting
in lower accuracy and higher bias on their local data. According to the Matthew
effect, which describes how the advantaged gain more advantage and the
disadvantaged lose more over time, deploying such a global model in client
applications may worsen the resource disparity among the clients and harm the
principles of social welfare and fairness. To mitigate the Matthew effect, we
propose Egalitarian Fairness Federated Learning (EFFL), where egalitarian
fairness refers to the global model learned from FL has: (1) equal accuracy
among clients; (2) equal decision bias among clients. Besides achieving
egalitarian fairness among the clients, EFFL also aims for performance
optimality, minimizing the empirical risk loss and the bias for each client;
both are essential for any ML model training, whether centralized or
decentralized. We formulate EFFL as a constrained multi-constrained
multi-objectives optimization (MCMOO) problem, with the decision bias and
egalitarian fairness as constraints and the minimization of the empirical risk
losses on all clients as multiple objectives to be optimized. We propose a
gradient-based three-stage algorithm to obtain the Pareto optimal solutions
within the constraint space. Extensive experiments demonstrate that EFFL
outperforms other state-of-the-art FL algorithms in achieving a
high-performance global model with enhanced egalitarian fairness among all
clients.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Diverse Target and Contribution Scheduling for Domain Generalization. (arXiv:2309.16460v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16460">http://arxiv.org/abs/2309.16460</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16460]] Diverse Target and Contribution Scheduling for Domain Generalization(http://arxiv.org/abs/2309.16460)</code></li>
<li>Summary: <p>Generalization under the distribution shift has been a great challenge in
computer vision. The prevailing practice of directly employing the one-hot
labels as the training targets in domain generalization~(DG) can lead to
gradient conflicts, making it insufficient for capturing the intrinsic class
characteristics and hard to increase the intra-class variation. Besides,
existing methods in DG mostly overlook the distinct contributions of source
(seen) domains, resulting in uneven learning from these domains. To address
these issues, we firstly present a theoretical and empirical analysis of the
existence of gradient conflicts in DG, unveiling the previously unexplored
relationship between distribution shifts and gradient conflicts during the
optimization process. In this paper, we present a novel perspective of DG from
the empirical source domain's risk and propose a new paradigm for DG called
Diverse Target and Contribution Scheduling (DTCS). DTCS comprises two
innovative modules: Diverse Target Supervision (DTS) and Diverse Contribution
Balance (DCB), with the aim of addressing the limitations associated with the
common utilization of one-hot labels and equal contributions for source domains
in DG. In specific, DTS employs distinct soft labels as training targets to
account for various feature distributions across domains and thereby mitigates
the gradient conflicts, and DCB dynamically balances the contributions of
source domains by ensuring a fair decline in losses of different source
domains. Extensive experiments with analysis on four benchmark datasets show
that the proposed method achieves a competitive performance in comparison with
the state-of-the-art approaches, demonstrating the effectiveness and advantages
of the proposed DTCS.
</p></li>
</ul>

<h3>Title: At Which Training Stage Does Cocde Data Help LLMs Reasoning?. (arXiv:2309.16298v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16298">http://arxiv.org/abs/2309.16298</a></li>
<li>Code URL: https://github.com/yingweima2022/codellm</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16298]] At Which Training Stage Does Cocde Data Help LLMs Reasoning?(http://arxiv.org/abs/2309.16298)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have exhibited remarkable reasoning capabilities
and become the foundation of language technologies. Inspired by the great
success of code data in training LLMs, we naturally wonder at which training
stage introducing code data can really help LLMs reasoning. To this end, this
paper systematically explores the impact of code data on LLMs at different
stages. Concretely, we introduce the code data at the pre-training stage,
instruction-tuning stage, and both of them, respectively. Then, the reasoning
capability of LLMs is comprehensively and fairly evaluated via six reasoning
tasks in five domains. We critically analyze the experimental results and
provide conclusions with insights. First, pre-training LLMs with the mixture of
code and text can significantly enhance LLMs' general reasoning capability
almost without negative transfer on other tasks. Besides, at the
instruction-tuning stage, code data endows LLMs the task-specific reasoning
capability. Moreover, the dynamic mixing strategy of code and text data assists
LLMs to learn reasoning capability step-by-step during training. These insights
deepen the understanding of LLMs regarding reasoning ability for their
application, such as scientific question answering, legal support, etc. The
source code and model parameters are released at the
link:~\url{https://github.com/yingweima2022/CodeLLM}.
</p></li>
</ul>

<h3>Title: Human Feedback is not Gold Standard. (arXiv:2309.16349v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16349">http://arxiv.org/abs/2309.16349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16349]] Human Feedback is not Gold Standard(http://arxiv.org/abs/2309.16349)</code></li>
<li>Summary: <p>Human feedback has become the de facto standard for evaluating the
performance of Large Language Models, and is increasingly being used as a
training objective. However, it is not clear which properties of a generated
output this single `preference' score captures. We hypothesise that preference
scores are subjective and open to undesirable biases. We critically analyse the
use of human feedback for both training and evaluation, to verify whether it
fully captures a range of crucial error criteria. We find that while preference
scores have fairly good coverage, they under-represent important aspects like
factuality. We further hypothesise that both preference scores and error
annotation may be affected by confounders, and leverage instruction-tuned
models to generate outputs that vary along two possible confounding dimensions:
assertiveness and complexity. We find that the assertiveness of an output skews
the perceived rate of factuality errors, indicating that human annotations are
not a fully reliable evaluation metric or training objective. Finally, we offer
preliminary evidence that using human feedback as a training objective
disproportionately increases the assertiveness of model outputs. We encourage
future work to carefully consider whether preference scores are well aligned
with the desired objective.
</p></li>
</ul>

<h3>Title: Improving Adaptive Online Learning Using Refined Discretization. (arXiv:2309.16044v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16044">http://arxiv.org/abs/2309.16044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16044]] Improving Adaptive Online Learning Using Refined Discretization(http://arxiv.org/abs/2309.16044)</code></li>
<li>Summary: <p>We study unconstrained Online Linear Optimization with Lipschitz losses. The
goal is to simultaneously achieve ($i$) second order gradient adaptivity; and
($ii$) comparator norm adaptivity also known as "parameter freeness" in the
literature. Existing regret bounds (Cutkosky and Orabona, 2018; Mhammedi and
Koolen, 2020; Jacobsen and Cutkosky, 2022) have the suboptimal $O(\sqrt{V_T\log
V_T})$ dependence on the gradient variance $V_T$, while the present work
improves it to the optimal rate $O(\sqrt{V_T})$ using a novel
continuous-time-inspired algorithm, without any impractical doubling trick.
This result can be extended to the setting with unknown Lipschitz constant,
eliminating the range ratio problem from prior works (Mhammedi and Koolen,
2020).
</p>
<p>Concretely, we first show that the aimed simultaneous adaptivity can be
achieved fairly easily in a continuous time analogue of the problem, where the
environment is modeled by an arbitrary continuous semimartingale. Then, our key
innovation is a new discretization argument that preserves such adaptivity in
the discrete time adversarial setting. This refines a non-gradient-adaptive
discretization argument from (Harvey et al., 2023), both algorithmically and
analytically, which could be of independent interest.
</p></li>
</ul>

<h3>Title: Max-Sliced Mutual Information. (arXiv:2309.16200v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16200">http://arxiv.org/abs/2309.16200</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16200]] Max-Sliced Mutual Information(http://arxiv.org/abs/2309.16200)</code></li>
<li>Summary: <p>Quantifying the dependence between high-dimensional random variables is
central to statistical learning and inference. Two classical methods are
canonical correlation analysis (CCA), which identifies maximally correlated
projected versions of the original variables, and Shannon's mutual information,
which is a universal dependence measure that also captures high-order
dependencies. However, CCA only accounts for linear dependence, which may be
insufficient for certain applications, while mutual information is often
infeasible to compute/estimate in high dimensions. This work proposes a middle
ground in the form of a scalable information-theoretic generalization of CCA,
termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual
information between low-dimensional projections of the high-dimensional
variables, which reduces back to CCA in the Gaussian case. It enjoys the best
of both worlds: capturing intricate dependencies in the data while being
amenable to fast computation and scalable estimation from samples. We show that
mSMI retains favorable structural properties of Shannon's mutual information,
like variational forms and identification of independence. We then study
statistical estimation of mSMI, propose an efficiently computable neural
estimator, and couple it with formal non-asymptotic error bounds. We present
experiments that demonstrate the utility of mSMI for several tasks,
encompassing independence testing, multi-view representation learning,
algorithmic fairness, and generative modeling. We observe that mSMI
consistently outperforms competing methods with little-to-no computational
overhead.
</p></li>
</ul>

<h3>Title: Towards Poisoning Fair Representations. (arXiv:2309.16487v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16487">http://arxiv.org/abs/2309.16487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16487]] Towards Poisoning Fair Representations(http://arxiv.org/abs/2309.16487)</code></li>
<li>Summary: <p>Fair machine learning seeks to mitigate model prediction bias against certain
demographic subgroups such as elder and female. Recently, fair representation
learning (FRL) trained by deep neural networks has demonstrated superior
performance, whereby representations containing no demographic information are
inferred from the data and then used as the input to classification or other
downstream tasks. Despite the development of FRL methods, their vulnerability
under data poisoning attack, a popular protocol to benchmark model robustness
under adversarial scenarios, is under-explored. Data poisoning attacks have
been developed for classical fair machine learning methods which incorporate
fairness constraints into shallow-model classifiers. Nonetheless, these attacks
fall short in FRL due to notably different fairness goals and model
architectures. This work proposes the first data poisoning framework attacking
FRL. We induce the model to output unfair representations that contain as much
demographic information as possible by injecting carefully crafted poisoning
samples into the training data. This attack entails a prohibitive bilevel
optimization, wherefore an effective approximated solution is proposed. A
theoretical analysis on the needed number of poisoning samples is derived and
sheds light on defending against the attack. Experiments on benchmark fairness
datasets and state-of-the-art fair representation learning models demonstrate
the superiority of our attack.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16042">http://arxiv.org/abs/2309.16042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16042]] Towards Best Practices of Activation Patching in Language Models: Metrics and Methods(http://arxiv.org/abs/2309.16042)</code></li>
<li>Summary: <p>Mechanistic interpretability seeks to understand the internal mechanisms of
machine learning models, where localization -- identifying the important model
components -- is a key step. Activation patching, also known as causal tracing
or interchange intervention, is a standard technique for this task (Vig et al.,
2020), but the literature contains many variants with little consensus on the
choice of hyperparameters or methodology. In this work, we systematically
examine the impact of methodological details in activation patching, including
evaluation metrics and corruption methods. In several settings of localization
and circuit discovery in language models, we find that varying these
hyperparameters could lead to disparate interpretability results. Backed by
empirical observations, we give conceptual arguments for why certain metrics or
methods may be preferred. Finally, we provide recommendations for the best
practices of activation patching going forwards.
</p></li>
</ul>

<h3>Title: Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies. (arXiv:2309.16025v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16025">http://arxiv.org/abs/2309.16025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16025]] Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies(http://arxiv.org/abs/2309.16025)</code></li>
<li>Summary: <p>Current methods of imitation learning (IL), primarily based on deep neural
networks, offer efficient means for obtaining driving policies from real-world
data but suffer from significant limitations in interpretability and
generalizability. These shortcomings are particularly concerning in
safety-critical applications like autonomous driving. In this paper, we address
these limitations by introducing Symbolic Imitation Learning (SIL), a
groundbreaking method that employs Inductive Logic Programming (ILP) to learn
driving policies which are transparent, explainable and generalisable from
available datasets. Utilizing the real-world highD dataset, we subject our
method to a rigorous comparative analysis against prevailing
neural-network-based IL methods. Our results demonstrate that SIL not only
enhances the interpretability of driving policies but also significantly
improves their applicability across varied driving situations. Hence, this work
offers a novel pathway to more reliable and safer autonomous driving systems,
underscoring the potential of integrating ILP into the domain of IL.
</p></li>
</ul>

<h3>Title: Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings. (arXiv:2309.16564v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16564">http://arxiv.org/abs/2309.16564</a></li>
<li>Code URL: https://github.com/euranova/augment_to_interpret</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16564]] Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings(http://arxiv.org/abs/2309.16564)</code></li>
<li>Summary: <p>Unsupervised learning allows us to leverage unlabelled data, which has become
abundantly available, and to create embeddings that are usable on a variety of
downstream tasks. However, the typical lack of interpretability of unsupervised
representation learning has become a limiting factor with regard to recent
transparent-AI regulations. In this paper, we study graph representation
learning and we show that data augmentation that preserves semantics can be
learned and used to produce interpretations. Our framework, which we named
INGENIOUS, creates inherently interpretable embeddings and eliminates the need
for costly additional post-hoc analysis. We also introduce additional metrics
addressing the lack of formalism and metrics in the understudied area of
unsupervised-representation learning interpretability. Our results are
supported by an experimental study applied to both graph-level and node-level
tasks and show that interpretable embeddings provide state-of-the-art
performance on subsequent downstream tasks.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: DiffGAN-F2S: Symmetric and Efficient Denoising Diffusion GANs for Structural Connectivity Prediction from Brain fMRI. (arXiv:2309.16205v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16205">http://arxiv.org/abs/2309.16205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16205]] DiffGAN-F2S: Symmetric and Efficient Denoising Diffusion GANs for Structural Connectivity Prediction from Brain fMRI(http://arxiv.org/abs/2309.16205)</code></li>
<li>Summary: <p>Mapping from functional connectivity (FC) to structural connectivity (SC) can
facilitate multimodal brain network fusion and discover potential biomarkers
for clinical implications. However, it is challenging to directly bridge the
reliable non-linear mapping relations between SC and functional magnetic
resonance imaging (fMRI). In this paper, a novel diffusision generative
adversarial network-based fMRI-to-SC (DiffGAN-F2S) model is proposed to predict
SC from brain fMRI in an end-to-end manner. To be specific, the proposed
DiffGAN-F2S leverages denoising diffusion probabilistic models (DDPMs) and
adversarial learning to efficiently generate high-fidelity SC through a few
steps from fMRI. By designing the dual-channel multi-head spatial attention
(DMSA) and graph convolutional modules, the symmetric graph generator first
captures global relations among direct and indirect connected brain regions,
then models the local brain region interactions. It can uncover the complex
mapping relations between fMRI and structural connectivity. Furthermore, the
spatially connected consistency loss is devised to constrain the generator to
preserve global-local topological information for accurate intrinsic SC
prediction. Testing on the public Alzheimer's Disease Neuroimaging Initiative
(ADNI) dataset, the proposed model can effectively generate empirical
SC-preserved connectivity from four-dimensional imaging data and shows superior
performance in SC prediction compared with other related models. Furthermore,
the proposed model can identify the vast majority of important brain regions
and connections derived from the empirical method, providing an alternative way
to fuse multimodal brain networks and analyze clinical disease.
</p></li>
</ul>

<h3>Title: Object Motion Guided Human Motion Synthesis. (arXiv:2309.16237v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16237">http://arxiv.org/abs/2309.16237</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16237]] Object Motion Guided Human Motion Synthesis(http://arxiv.org/abs/2309.16237)</code></li>
<li>Summary: <p>Modeling human behaviors in contextual environments has a wide range of
applications in character animation, embodied AI, VR/AR, and robotics. In
real-world scenarios, humans frequently interact with the environment and
manipulate various objects to complete daily tasks. In this work, we study the
problem of full-body human motion synthesis for the manipulation of large-sized
objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a
conditional diffusion framework that can generate full-body manipulation
behaviors from only the object motion. Since naively applying diffusion models
fails to precisely enforce contact constraints between the hands and the
object, OMOMO learns two separate denoising processes to first predict hand
positions from object motion and subsequently synthesize full-body poses based
on the predicted hand positions. By employing the hand positions as an
intermediate representation between the two denoising processes, we can
explicitly enforce contact constraints, resulting in more physically plausible
manipulation motions. With the learned model, we develop a novel system that
captures full-body human manipulation motions by simply attaching a smartphone
to the object being manipulated. Through extensive experiments, we demonstrate
the effectiveness of our proposed pipeline and its ability to generalize to
unseen objects. Additionally, as high-quality human-object interaction datasets
are scarce, we collect a large-scale dataset consisting of 3D object geometry,
object motion, and human motion. Our dataset contains human-object interaction
motion for 15 objects, with a total duration of approximately 10 hours.
</p></li>
</ul>

<h3>Title: Distilling ODE Solvers of Diffusion Models into Smaller Steps. (arXiv:2309.16421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16421">http://arxiv.org/abs/2309.16421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16421]] Distilling ODE Solvers of Diffusion Models into Smaller Steps(http://arxiv.org/abs/2309.16421)</code></li>
<li>Summary: <p>Distillation techniques have substantially improved the sampling speed of
diffusion models, allowing of the generation within only one step or a few
steps. However, these distillation methods require extensive training for each
dataset, sampler, and network, which limits their practical applicability. To
address this limitation, we propose a straightforward distillation approach,
Distilled-ODE solvers (D-ODE solvers), that optimizes the ODE solver rather
than training the denoising network. D-ODE solvers are formulated by simply
applying a single parameter adjustment to existing ODE solvers. Subsequently,
D-ODE solvers with smaller steps are optimized by ODE solvers with larger steps
through distillation over a batch of samples. Our comprehensive experiments
indicate that D-ODE solvers outperform existing ODE solvers, including DDIM,
PNDM, DPM-Solver, DEIS, and EDM, especially when generating samples with fewer
steps. Our method incur negligible computational overhead compared to previous
distillation techniques, enabling simple and rapid integration with previous
samplers. Qualitative analysis further shows that D-ODE solvers enhance image
quality while preserving the sampling trajectory of ODE solvers.
</p></li>
</ul>

<h3>Title: CCEdit: Creative and Controllable Video Editing via Diffusion Models. (arXiv:2309.16496v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16496">http://arxiv.org/abs/2309.16496</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16496]] CCEdit: Creative and Controllable Video Editing via Diffusion Models(http://arxiv.org/abs/2309.16496)</code></li>
<li>Summary: <p>In this work, we present CCEdit, a versatile framework designed to address
the challenges of creative and controllable video editing. CCEdit accommodates
a wide spectrum of user editing requirements and enables enhanced creative
control through an innovative approach that decouples video structure and
appearance. We leverage the foundational ControlNet architecture to preserve
structural integrity, while seamlessly integrating adaptable temporal modules
compatible with state-of-the-art personalization techniques for text-to-image
generation, such as DreamBooth and LoRA.Furthermore, we introduce
reference-conditioned video editing, empowering users to exercise precise
creative control over video editing through the more manageable process of
editing key frames. Our extensive experimental evaluations confirm the
exceptional functionality and editing capabilities of the proposed CCEdit
framework. Demo video is available at
https://www.youtube.com/watch?v=UQw4jq-igN4.
</p></li>
</ul>

<h3>Title: KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing. (arXiv:2309.16608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16608">http://arxiv.org/abs/2309.16608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16608]] KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing(http://arxiv.org/abs/2309.16608)</code></li>
<li>Summary: <p>Text-conditioned image editing is a recently emerged and highly practical
task, and its potential is immeasurable. However, most of the concurrent
methods are unable to perform action editing, i.e. they can not produce results
that conform to the action semantics of the editing prompt and preserve the
content of the original image. To solve the problem of action editing, we
propose KV Inversion, a method that can achieve satisfactory reconstruction
performance and action editing, which can solve two major problems: 1) the
edited result can match the corresponding action, and 2) the edited object can
retain the texture and identity of the original real image. In addition, our
method does not require training the Stable Diffusion model itself, nor does it
require scanning a large-scale dataset to perform time-consuming training.
</p></li>
</ul>

<h3>Title: DeepPCR: Parallelizing Sequential Operations in Neural Networks. (arXiv:2309.16318v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16318">http://arxiv.org/abs/2309.16318</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16318]] DeepPCR: Parallelizing Sequential Operations in Neural Networks(http://arxiv.org/abs/2309.16318)</code></li>
<li>Summary: <p>Parallelization techniques have become ubiquitous for accelerating inference
and training of deep neural networks. Despite this, several operations are
still performed in a sequential manner. For instance, the forward and backward
passes are executed layer-by-layer, and the output of diffusion models is
produced by applying a sequence of denoising steps. This sequential approach
results in a computational cost proportional to the number of steps involved,
presenting a potential bottleneck as the number of steps increases. In this
work, we introduce DeepPCR, a novel algorithm which parallelizes typically
sequential operations used in inference and training of neural networks.
DeepPCR is based on interpreting a sequence of $L$ steps as the solution of a
specific system of equations, which we recover using the Parallel Cyclic
Reduction algorithm. This reduces the complexity of computing the sequential
operations from $\mathcal{O}(L)$ to $\mathcal{O}(\log_2L)$, thus yielding a
speedup for large $L$. To verify the theoretical lower complexity of the
algorithm, and to identify regimes for speedup, we test the effectiveness of
DeepPCR in parallelizing the forward and backward pass in multi-layer
perceptrons, and reach speedups of up to $30\times$ for forward and $200\times$
for backward pass. We additionally showcase the flexibility of DeepPCR by
parallelizing training of ResNets with as many as 1024 layers, and generation
in diffusion models, enabling up to $7\times$ faster training and $11\times$
faster generation, respectively, when compared to the sequential approach.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts. (arXiv:2309.15915v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15915">http://arxiv.org/abs/2309.15915</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15915]] Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts(http://arxiv.org/abs/2309.15915)</code></li>
<li>Summary: <p>Recent vision-language models are driven by large-scale pretrained models.
However, adapting pretrained models on limited data presents challenges such as
overfitting, catastrophic forgetting, and the cross-modal gap between vision
and language. We introduce a parameter-efficient method to address these
challenges, combining multimodal prompt learning and a transformer-based
mapping network, while keeping the pretrained models frozen. Our experiments on
several video question answering benchmarks demonstrate the superiority of our
approach in terms of performance and parameter efficiency on both zero-shot and
few-shot settings. Our code is available at https://engindeniz.github.io/vitis.
</p></li>
</ul>

<h3>Title: GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes. (arXiv:2309.16019v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16019">http://arxiv.org/abs/2309.16019</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16019]] GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes(http://arxiv.org/abs/2309.16019)</code></li>
<li>Summary: <p>This paper tackles the challenges of self-supervised monocular depth
estimation in indoor scenes caused by large rotation between frames and low
texture. We ease the learning process by obtaining coarse camera poses from
monocular sequences through multi-view geometry to deal with the former.
However, we found that limited by the scale ambiguity across different scenes
in the training dataset, a na\"ive introduction of geometric coarse poses
cannot play a positive role in performance improvement, which is
counter-intuitive. To address this problem, we propose to refine those poses
during training through rotation and translation/scale optimization. To soften
the effect of the low texture, we combine the global reasoning of vision
transformers with an overfitting-aware, iterative self-distillation mechanism,
providing more accurate depth guidance coming from the network itself.
Experiments on NYUv2, ScanNet, 7scenes, and KITTI datasets support the
effectiveness of each component in our framework, which sets a new
state-of-the-art for indoor self-supervised monocular depth estimation, as well
as outstanding generalization ability. Code and models are available at
https://github.com/zxcqlf/GasMono
</p></li>
</ul>

<h3>Title: Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16108">http://arxiv.org/abs/2309.16108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16108]] Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words(http://arxiv.org/abs/2309.16108)</code></li>
<li>Summary: <p>Vision Transformer (ViT) has emerged as a powerful architecture in the realm
of modern computer vision. However, its application in certain imaging fields,
such as microscopy and satellite imaging, presents unique challenges. In these
domains, images often contain multiple channels, each carrying semantically
distinct and independent information. Furthermore, the model must demonstrate
robustness to sparsity in input channels, as they may not be densely available
during training or testing. In this paper, we propose a modification to the ViT
architecture that enhances reasoning across the input channels and introduce
Hierarchical Channel Sampling (HCS) as an additional regularization technique
to ensure robustness when only partial channels are presented during test time.
Our proposed model, ChannelViT, constructs patch tokens independently from each
input channel and utilizes a learnable channel embedding that is added to the
patch tokens, similar to positional embeddings. We evaluate the performance of
ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat
(satellite imaging). Our results show that ChannelViT outperforms ViT on
classification tasks and generalizes well, even when a subset of input channels
is used during testing. Across our experiments, HCS proves to be a powerful
regularizer, independent of the architecture employed, suggesting itself as a
straightforward technique for robust ViT training. Lastly, we find that
ChannelViT generalizes effectively even when there is limited access to all
channels during training, highlighting its potential for multi-channel imaging
under real-world conditions with sparse sensors.
</p></li>
</ul>

<h3>Title: MASK4D: Mask Transformer for 4D Panoptic Segmentation. (arXiv:2309.16133v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16133">http://arxiv.org/abs/2309.16133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16133]] MASK4D: Mask Transformer for 4D Panoptic Segmentation(http://arxiv.org/abs/2309.16133)</code></li>
<li>Summary: <p>Accurately perceiving and tracking instances over time is essential for the
decision-making processes of autonomous agents interacting safely in dynamic
environments. With this intention, we propose Mask4D for the challenging task
of 4D panoptic segmentation of LiDAR point clouds. Mask4D is the first
transformer-based approach unifying semantic instance segmentation and tracking
of sparse and irregular sequences of 3D point clouds into a single joint model.
Our model directly predicts semantic instances and their temporal associations
without relying on any hand-crafted non-learned association strategies such as
probabilistic clustering or voting-based center prediction. Instead, Mask4D
introduces spatio-temporal instance queries which encode the semantic and
geometric properties of each semantic tracklet in the sequence. In an in-depth
study, we find that it is critical to promote spatially compact instance
predictions as spatio-temporal instance queries tend to merge multiple
semantically similar instances, even if they are spatially distant. To this
end, we regress 6-DOF bounding box parameters from spatio-temporal instance
queries, which is used as an auxiliary task to foster spatially compact
predictions. Mask4D achieves a new state-of-the-art on the SemanticKITTI test
set with a score of 68.4 LSTQ, improving upon published top-performing methods
by at least +4.5%.
</p></li>
</ul>

<h3>Title: GAFlow: Incorporating Gaussian Attention into Optical Flow. (arXiv:2309.16217v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16217">http://arxiv.org/abs/2309.16217</a></li>
<li>Code URL: https://github.com/la30/gaflow</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16217]] GAFlow: Incorporating Gaussian Attention into Optical Flow(http://arxiv.org/abs/2309.16217)</code></li>
<li>Summary: <p>Optical flow, or the estimation of motion fields from image sequences, is one
of the fundamental problems in computer vision. Unlike most pixel-wise tasks
that aim at achieving consistent representations of the same category, optical
flow raises extra demands for obtaining local discrimination and smoothness,
which yet is not fully explored by existing approaches. In this paper, we push
Gaussian Attention (GA) into the optical flow models to accentuate local
properties during representation learning and enforce the motion affinity
during matching. Specifically, we introduce a novel Gaussian-Constrained Layer
(GCL) which can be easily plugged into existing Transformer blocks to highlight
the local neighborhood that contains fine-grained structural information.
Moreover, for reliable motion analysis, we provide a new Gaussian-Guided
Attention Module (GGAM) which not only inherits properties from Gaussian
distribution to instinctively revolve around the neighbor fields of each point
but also is empowered to put the emphasis on contextually related regions
during matching. Our fully-equipped model, namely Gaussian Attention Flow
network (GAFlow), naturally incorporates a series of novel Gaussian-based
modules into the conventional optical flow framework for reliable motion
analysis. Extensive experiments on standard optical flow datasets consistently
demonstrate the exceptional performance of the proposed approach in terms of
both generalization ability evaluation and online benchmark testing. Code is
available at https://github.com/LA30/GAFlow.
</p></li>
</ul>

<h3>Title: Multi-scale Recurrent LSTM and Transformer Network for Depth Completion. (arXiv:2309.16301v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16301">http://arxiv.org/abs/2309.16301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16301]] Multi-scale Recurrent LSTM and Transformer Network for Depth Completion(http://arxiv.org/abs/2309.16301)</code></li>
<li>Summary: <p>Lidar depth completion is a new and hot topic of depth estimation. In this
task, it is the key and difficult point to fuse the features of color space and
depth space. In this paper, we migrate the classic LSTM and Transformer modules
from NLP to depth completion and redesign them appropriately. Specifically, we
use Forget gate, Update gate, Output gate, and Skip gate to achieve the
efficient fusion of color and depth features and perform loop optimization at
multiple scales. Finally, we further fuse the deep features through the
Transformer multi-head attention mechanism. Experimental results show that
without repetitive network structure and post-processing steps, our method can
achieve state-of-the-art performance by adding our modules to a simple
encoder-decoder network structure. Our method ranks first on the current
mainstream autonomous driving KITTI benchmark dataset. It can also be regarded
as a backbone network for other methods, which likewise achieves
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16354">http://arxiv.org/abs/2309.16354</a></li>
<li>Code URL: https://github.com/transformer-vq/transformer_vq</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16354]] Transformer-VQ: Linear-Time Transformers via Vector Quantization(http://arxiv.org/abs/2309.16354)</code></li>
<li>Summary: <p>We introduce Transformer-VQ, a decoder-only transformer computing
softmax-based dense self-attention in linear time. Transformer-VQ's efficient
attention is enabled by vector-quantized keys and a novel caching mechanism. In
large-scale experiments, Transformer-VQ is shown highly competitive in quality,
with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64
(3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
</p></li>
</ul>

<h3>Title: Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds. (arXiv:2309.16435v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16435">http://arxiv.org/abs/2309.16435</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16435]] Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds(http://arxiv.org/abs/2309.16435)</code></li>
<li>Summary: <p>The perception of moving objects is crucial for autonomous robots performing
collision avoidance in dynamic environments. LiDARs and cameras tremendously
enhance scene interpretation but do not provide direct motion information and
face limitations under adverse weather. Radar sensors overcome these
limitations and provide Doppler velocities, delivering direct information on
dynamic objects. In this paper, we address the problem of moving instance
segmentation in radar point clouds to enhance scene interpretation for
safety-critical tasks. Our Radar Instance Transformer enriches the current
radar scan with temporal information without passing aggregated scans through a
neural network. We propose a full-resolution backbone to prevent information
loss in sparse point cloud processing. Our instance transformer head
incorporates essential information to enhance segmentation but also enables
reliable, class-agnostic instance assignments. In sum, our approach shows
superior performance on the new moving instance segmentation benchmarks,
including diverse environments, and provides model-agnostic modules to enhance
scene interpretation. The benchmark is based on the RadarScenes dataset and
will be made available upon acceptance.
</p></li>
</ul>

<h3>Title: HTC-DC Net: Monocular Height Estimation from Single Remote Sensing Images. (arXiv:2309.16486v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16486">http://arxiv.org/abs/2309.16486</a></li>
<li>Code URL: https://github.com/zhu-xlab/htc-dc-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16486]] HTC-DC Net: Monocular Height Estimation from Single Remote Sensing Images(http://arxiv.org/abs/2309.16486)</code></li>
<li>Summary: <p>3D geo-information is of great significance for understanding the living
environment; however, 3D perception from remote sensing data, especially on a
large scale, is restricted. To tackle this problem, we propose a method for
monocular height estimation from optical imagery, which is currently one of the
richest sources of remote sensing data. As an ill-posed problem, monocular
height estimation requires well-designed networks for enhanced representations
to improve performance. Moreover, the distribution of height values is
long-tailed with the low-height pixels, e.g., the background, as the head, and
thus trained networks are usually biased and tend to underestimate building
heights. To solve the problems, instead of formalizing the problem as a
regression task, we propose HTC-DC Net following the classification-regression
paradigm, with the head-tail cut (HTC) and the distribution-based constraints
(DCs) as the main contributions. HTC-DC Net is composed of the backbone network
as the feature extractor, the HTC-AdaBins module, and the hybrid regression
process. The HTC-AdaBins module serves as the classification phase to determine
bins adaptive to each input image. It is equipped with a vision transformer
encoder to incorporate local context with holistic information and involves an
HTC to address the long-tailed problem in monocular height estimation for
balancing the performances of foreground and background pixels. The hybrid
regression process does the regression via the smoothing of bins from the
classification phase, which is trained via DCs. The proposed network is tested
on three datasets of different resolutions, namely ISPRS Vaihingen (0.09 m),
DFC19 (1.3 m) and GBH (3 m). Experimental results show the superiority of the
proposed network over existing methods by large margins. Extensive ablation
studies demonstrate the effectiveness of each design component.
</p></li>
</ul>

<h3>Title: Vision Transformers Need Registers. (arXiv:2309.16588v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16588">http://arxiv.org/abs/2309.16588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16588]] Vision Transformers Need Registers(http://arxiv.org/abs/2309.16588)</code></li>
<li>Summary: <p>Transformers have recently emerged as a powerful tool for learning visual
representations. In this paper, we identify and characterize artifacts in
feature maps of both supervised and self-supervised ViT networks. The artifacts
correspond to high-norm tokens appearing during inference primarily in
low-informative background areas of images, that are repurposed for internal
computations. We propose a simple yet effective solution based on providing
additional tokens to the input sequence of the Vision Transformer to fill that
role. We show that this solution fixes that problem entirely for both
supervised and self-supervised models, sets a new state of the art for
self-supervised visual models on dense visual prediction tasks, enables object
discovery methods with larger models, and most importantly leads to smoother
feature maps and attention maps for downstream visual processing.
</p></li>
</ul>

<h3>Title: Deep Geometrized Cartoon Line Inbetweening. (arXiv:2309.16643v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16643">http://arxiv.org/abs/2309.16643</a></li>
<li>Code URL: https://github.com/lisiyao21/animeinbet</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16643]] Deep Geometrized Cartoon Line Inbetweening(http://arxiv.org/abs/2309.16643)</code></li>
<li>Summary: <p>We aim to address a significant but understudied problem in the anime
industry, namely the inbetweening of cartoon line drawings. Inbetweening
involves generating intermediate frames between two black-and-white line
drawings and is a time-consuming and expensive process that can benefit from
automation. However, existing frame interpolation methods that rely on matching
and warping whole raster images are unsuitable for line inbetweening and often
produce blurring artifacts that damage the intricate line structures. To
preserve the precision and detail of the line drawings, we propose a new
approach, AnimeInbet, which geometrizes raster line drawings into graphs of
endpoints and reframes the inbetweening task as a graph fusion problem with
vertex repositioning. Our method can effectively capture the sparsity and
unique structure of line drawings while preserving the details during
inbetweening. This is made possible via our novel modules, i.e., vertex
geometric embedding, a vertex correspondence Transformer, an effective
mechanism for vertex repositioning and a visibility predictor. To train our
method, we introduce MixamoLine240, a new dataset of line drawings with ground
truth vectorization and matching labels. Our experiments demonstrate that
AnimeInbet synthesizes high-quality, clean, and complete intermediate line
drawings, outperforming existing methods quantitatively and qualitatively,
especially in cases with large motions. Data and code are available at
https://github.com/lisiyao21/AnimeInbet.
</p></li>
</ul>

<h3>Title: Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors. (arXiv:2309.16646v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16646">http://arxiv.org/abs/2309.16646</a></li>
<li>Code URL: https://github.com/mikuhatsune/equivariance</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16646]] Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors(http://arxiv.org/abs/2309.16646)</code></li>
<li>Summary: <p>Dense depth and surface normal predictors should possess the equivariant
property to cropping-and-resizing -- cropping the input image should result in
cropping the same output image. However, we find that state-of-the-art depth
and normal predictors, despite having strong performances, surprisingly do not
respect equivariance. The problem exists even when crop-and-resize data
augmentation is employed during training. To remedy this, we propose an
equivariant regularization technique, consisting of an averaging procedure and
a self-consistency loss, to explicitly promote cropping-and-resizing
equivariance in depth and normal networks. Our approach can be applied to both
CNN and Transformer architectures, does not incur extra cost during testing,
and notably improves the supervised and semi-supervised learning performance of
dense predictors on Taskonomy tasks. Finally, finetuning with our loss on
unlabeled images improves not only equivariance but also accuracy of
state-of-the-art depth and normal predictors when evaluated on NYU-v2. GitHub
link: https://github.com/mikuhatsune/equivariance
</p></li>
</ul>

<h3>Title: Controllable Text Generation with Residual Memory Transformer. (arXiv:2309.16231v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16231">http://arxiv.org/abs/2309.16231</a></li>
<li>Code URL: https://github.com/littlehacker26/residual_memory_transformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16231]] Controllable Text Generation with Residual Memory Transformer(http://arxiv.org/abs/2309.16231)</code></li>
<li>Summary: <p>Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have
brought great success in text generation. However, it is still an open
challenge to control the generation process of CLM while balancing flexibility,
control granularity, and generation efficiency. In this paper, we provide a new
alternative for controllable text generation (CTG), by designing a
non-intrusive, lightweight control plugin to accompany the generation of CLM at
arbitrary time steps. The proposed control plugin, namely Residual Memory
Transformer (RMT), has an encoder-decoder setup, which can accept any types of
control conditions and cooperate with CLM through a residual learning paradigm,
to achieve a more flexible, general, and efficient CTG. Extensive experiments
are carried out on various control tasks, in the form of both automatic and
human evaluations. The results show the superiority of RMT over a range of
state-of-the-art approaches, proving the effectiveness and versatility of our
approach.
</p></li>
</ul>

<h3>Title: On the Challenges of Fully Incremental Neural Dependency Parsing. (arXiv:2309.16254v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16254">http://arxiv.org/abs/2309.16254</a></li>
<li>Code URL: https://github.com/anaezquerro/incpar</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16254]] On the Challenges of Fully Incremental Neural Dependency Parsing(http://arxiv.org/abs/2309.16254)</code></li>
<li>Summary: <p>Since the popularization of BiLSTMs and Transformer-based bidirectional
encoders, state-of-the-art syntactic parsers have lacked incrementality,
requiring access to the whole sentence and deviating from human language
processing. This paper explores whether fully incremental dependency parsing
with modern architectures can be competitive. We build parsers combining
strictly left-to-right neural encoders with fully incremental sequence-labeling
and transition-based decoders. The results show that fully incremental parsing
with modern architectures considerably lags behind bidirectional parsing,
noting the challenges of psycholinguistically plausible parsing.
</p></li>
</ul>

<h3>Title: UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers. (arXiv:2309.16275v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16275">http://arxiv.org/abs/2309.16275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16275]] UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers(http://arxiv.org/abs/2309.16275)</code></li>
<li>Summary: <p>Conspiracy theories have become a prominent and concerning aspect of online
discourse, posing challenges to information integrity and societal trust. As
such, we address conspiracy theory detection as proposed by the ACTI @ EVALITA
2023 shared task. The combination of pre-trained sentence Transformer models
and data augmentation techniques enabled us to secure first place in the final
leaderboard of both sub-tasks. Our methodology attained F1 scores of 85.71% in
the binary classification and 91.23% for the fine-grained conspiracy topic
classification, surpassing other competing systems.
</p></li>
</ul>

<h3>Title: Augmenting transformers with recursively composed multi-grained representations. (arXiv:2309.16319v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16319">http://arxiv.org/abs/2309.16319</a></li>
<li>Code URL: https://github.com/ant-research/structuredlm_rtdt</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16319]] Augmenting transformers with recursively composed multi-grained representations(http://arxiv.org/abs/2309.16319)</code></li>
<li>Summary: <p>We present ReCAT, a recursive composition augmented Transformer that is able
to explicitly model hierarchical syntactic structures of raw texts without
relying on gold trees during both learning and inference. Existing research
along this line restricts data to follow a hierarchical tree structure and thus
lacks inter-span communications. To overcome the problem, we propose a novel
contextual inside-outside (CIO) layer that learns contextualized
representations of spans through bottom-up and top-down passes, where a
bottom-up pass forms representations of high-level spans by composing low-level
spans, while a top-down pass combines information inside and outside a span. By
stacking several CIO layers between the embedding layer and the attention
layers in Transformer, the ReCAT model can perform both deep intra-span and
deep inter-span interactions, and thus generate multi-grained representations
fully contextualized with other spans. Moreover, the CIO layers can be jointly
pre-trained with Transformers, making ReCAT enjoy scaling ability, strong
performance, and interpretability at the same time. We conduct experiments on
various sentence-level and span-level tasks. Evaluation results indicate that
ReCAT can significantly outperform vanilla Transformer models on all span-level
tasks and baselines that combine recursive networks with Transformers on
natural language inference tasks. More interestingly, the hierarchical
structures induced by ReCAT exhibit strong consistency with human-annotated
syntactic trees, indicating good interpretability brought by the CIO layers.
</p></li>
</ul>

<h3>Title: Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data. (arXiv:2309.16220v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16220">http://arxiv.org/abs/2309.16220</a></li>
<li>Code URL: https://github.com/mazizmalayeri/tabmedood</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16220]] Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data(http://arxiv.org/abs/2309.16220)</code></li>
<li>Summary: <p>Despite their success, Machine Learning (ML) models do not generalize
effectively to data not originating from the training distribution. To reliably
employ ML models in real-world healthcare systems and avoid inaccurate
predictions on out-of-distribution (OOD) data, it is crucial to detect OOD
samples. Numerous OOD detection approaches have been suggested in other fields
- especially in computer vision - but it remains unclear whether the challenge
is resolved when dealing with medical tabular data. To answer this pressing
need, we propose an extensive reproducible benchmark to compare different
methods across a suite of tests including both near and far OODs. Our benchmark
leverages the latest versions of eICU and MIMIC-IV, two public datasets
encompassing tens of thousands of ICU patients in several hospitals. We
consider a wide array of density-based methods and SOTA post-hoc detectors
across diverse predictive architectures, including MLP, ResNet, and
Transformer. Our findings show that i) the problem appears to be solved for
far-OODs, but remains open for near-OODs; ii) post-hoc methods alone perform
poorly, but improve substantially when coupled with distance-based mechanisms;
iii) the transformer architecture is far less overconfident compared to MLP and
ResNet.
</p></li>
</ul>

<h3>Title: Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16397">http://arxiv.org/abs/2309.16397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16397]] Uncertainty-Aware Decision Transformer for Stochastic Driving Environments(http://arxiv.org/abs/2309.16397)</code></li>
<li>Summary: <p>Offline Reinforcement Learning (RL) has emerged as a promising framework for
learning policies without active interactions, making it especially appealing
for autonomous driving tasks. Recent successes of Transformers inspire casting
offline RL as sequence modeling, which performs well in long-horizon tasks.
However, they are overly optimistic in stochastic environments with incorrect
assumptions that the same goal can be consistently achieved by identical
actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer
(UNREST) for planning in stochastic driving environments without introducing
additional transition or complex generative models. Specifically, UNREST
estimates state uncertainties by the conditional mutual information between
transitions and returns, and segments sequences accordingly. Discovering the
`uncertainty accumulation' and `temporal locality' properties of driving
environments, UNREST replaces the global returns in decision transformers with
less uncertain truncated returns, to learn from true outcomes of agent actions
rather than environment transitions. We also dynamically evaluate environmental
uncertainty during inference for cautious planning. Extensive experimental
results demonstrate UNREST's superior performance in various driving scenarios
and the power of our uncertainty estimation strategy.
</p></li>
</ul>

<h3>Title: Compositional Program Generation for Systematic Generalization. (arXiv:2309.16467v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16467">http://arxiv.org/abs/2309.16467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16467]] Compositional Program Generation for Systematic Generalization(http://arxiv.org/abs/2309.16467)</code></li>
<li>Summary: <p>Compositional generalization is a key ability of humans that enables us to
learn new concepts from only a handful examples. Machine learning models,
including the now ubiquitous transformers, struggle to generalize in this way,
and typically require thousands of examples of a concept during training in
order to generalize meaningfully. This difference in ability between humans and
artificial neural architectures, motivates this study on a neuro-symbolic
architecture called the Compositional Program Generator (CPG). CPG has three
key features: modularity, type abstraction, and recursive composition, that
enable it to generalize both systematically to new concepts in a few-shot
manner, as well as productively by length on various sequence-to-sequence
language tasks. For each input, CPG uses a grammar of the input domain and a
parser to generate a type hierarchy in which each grammar rule is assigned its
own unique semantic module, a probabilistic copy or substitution program.
Instances with the same hierarchy are processed with the same composed program,
while those with different hierarchies may be processed with different
programs. CPG learns parameters for the semantic modules and is able to learn
the semantics for new types incrementally. Given a context-free grammar of the
input language and a dictionary mapping each word in the source language to its
interpretation in the output language, CPG can achieve perfect generalization
on the SCAN and COGS benchmarks, in both standard and extreme few-shot
settings.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: AutoEncoding Tree for City Generation and Applications. (arXiv:2309.15941v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15941">http://arxiv.org/abs/2309.15941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15941]] AutoEncoding Tree for City Generation and Applications(http://arxiv.org/abs/2309.15941)</code></li>
<li>Summary: <p>City modeling and generation have attracted an increased interest in various
applications, including gaming, urban planning, and autonomous driving. Unlike
previous works focused on the generation of single objects or indoor scenes,
the huge volumes of spatial data in cities pose a challenge to the generative
models. Furthermore, few publicly available 3D real-world city datasets also
hinder the development of methods for city generation. In this paper, we first
collect over 3,000,000 geo-referenced objects for the city of New York, Zurich,
Tokyo, Berlin, Boston and several other large cities. Based on this dataset, we
propose AETree, a tree-structured auto-encoder neural network, for city
generation. Specifically, we first propose a novel Spatial-Geometric Distance
(SGD) metric to measure the similarity between building layouts and then
construct a binary tree over the raw geometric data of building based on the
SGD metric. Next, we present a tree-structured network whose encoder learns to
extract and merge spatial information from bottom-up iteratively. The resulting
global representation is reversely decoded for reconstruction or generation. To
address the issue of long-dependency as the level of the tree increases, a Long
Short-Term Memory (LSTM) Cell is employed as a basic network element of the
proposed AETree. Moreover, we introduce a novel metric, Overlapping Area Ratio
(OAR), to quantitatively evaluate the generation results. Experiments on the
collected dataset demonstrate the effectiveness of the proposed model on 2D and
3D city generation. Furthermore, the latent features learned by AETree can
serve downstream urban planning applications.
</p></li>
</ul>

<h3>Title: Learning Effective NeRFs and SDFs Representations with 3D Generative Adversarial Networks for 3D Object Generation: Technical Report for ICCV 2023 OmniObject3D Challenge. (arXiv:2309.16110v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16110">http://arxiv.org/abs/2309.16110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16110]] Learning Effective NeRFs and SDFs Representations with 3D Generative Adversarial Networks for 3D Object Generation: Technical Report for ICCV 2023 OmniObject3D Challenge(http://arxiv.org/abs/2309.16110)</code></li>
<li>Summary: <p>In this technical report, we present a solution for 3D object generation of
ICCV 2023 OmniObject3D Challenge. In recent years, 3D object generation has
made great process and achieved promising results, but it remains a challenging
task due to the difficulty of generating complex, textured and high-fidelity
results. To resolve this problem, we study learning effective NeRFs and SDFs
representations with 3D Generative Adversarial Networks (GANs) for 3D object
generation. Specifically, inspired by recent works, we use the efficient
geometry-aware 3D GANs as the backbone incorporating with label embedding and
color mapping, which enables to train the model on different taxonomies
simultaneously. Then, through a decoder, we aggregate the resulting features to
generate Neural Radiance Fields (NeRFs) based representations for rendering
high-fidelity synthetic images. Meanwhile, we optimize Signed Distance
Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we
observe that this model can be effectively trained with only a few images of
each object from a variety of classes, instead of using a great number of
images per object or training one model per class. With this pipeline, we can
optimize an effective model for 3D object generation. This solution is one of
the final top-3-place solutions in the ICCV 2023 OmniObject3D Challenge.
</p></li>
</ul>

<h3>Title: Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples. (arXiv:2309.16143v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16143">http://arxiv.org/abs/2309.16143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16143]] Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples(http://arxiv.org/abs/2309.16143)</code></li>
<li>Summary: <p>Semi-supervised learning (SSL) is a promising approach for training deep
classification models using labeled and unlabeled datasets. However, existing
SSL methods rely on a large unlabeled dataset, which may not always be
available in many real-world applications due to legal constraints (e.g.,
GDPR). In this paper, we investigate the research question: Can we train SSL
models without real unlabeled datasets? Instead of using real unlabeled
datasets, we propose an SSL method using synthetic datasets generated from
generative foundation models trained on datasets containing millions of samples
in diverse domains (e.g., ImageNet). Our main concepts are identifying
synthetic samples that emulate unlabeled samples from generative foundation
models and training classifiers using these synthetic samples. To achieve this,
our method is formulated as an alternating optimization problem: (i)
meta-learning of generative foundation models and (ii) SSL of classifiers using
real labeled and synthetic unlabeled samples. For (i), we propose a
meta-learning objective that optimizes latent variables to generate samples
that resemble real labeled samples and minimize the validation loss. For (ii),
we propose a simple unsupervised loss function that regularizes the feature
extractors of classifiers to maximize the performance improvement obtained from
synthetic samples. We confirm that our method outperforms baselines using
generative foundation models on SSL. We also demonstrate that our methods
outperform SSL using real unlabeled datasets in scenarios with extremely small
amounts of labeled datasets. This suggests that synthetic samples have the
potential to provide improvement gains more efficiently than real unlabeled
data.
</p></li>
</ul>

<h3>Title: FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for Independence-Assumption-Free Uncertainty Estimation. (arXiv:2309.16364v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16364">http://arxiv.org/abs/2309.16364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16364]] FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for Independence-Assumption-Free Uncertainty Estimation(http://arxiv.org/abs/2309.16364)</code></li>
<li>Summary: <p>Neural radiance fields with stochasticity have garnered significant interest
by enabling the sampling of plausible radiance fields and quantifying
uncertainty for downstream tasks. Existing works rely on the independence
assumption of points in the radiance field or the pixels in input views to
obtain tractable forms of the probability density function. However, this
assumption inadvertently impacts performance when dealing with intricate
geometry and texture. In this work, we propose an independence-assumption-free
probabilistic neural radiance field based on Flow-GAN. By combining the
generative capability of adversarial learning and the powerful expressivity of
normalizing flow, our method explicitly models the density-radiance
distribution of the whole scene. We represent our probabilistic NeRF as a
mean-shifted probabilistic residual neural model. Our model is trained without
an explicit likelihood function, thereby avoiding the independence assumption.
Specifically, We downsample the training images with different strides and
centers to form fixed-size patches which are used to train the generator with
patch-based adversarial learning. Through extensive experiments, our method
demonstrates state-of-the-art performance by predicting lower rendering errors
and more reliable uncertainty on both synthetic and real-world datasets.
</p></li>
</ul>

<h3>Title: DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation. (arXiv:2309.16653v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16653">http://arxiv.org/abs/2309.16653</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16653]] DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation(http://arxiv.org/abs/2309.16653)</code></li>
<li>Summary: <p>Recent advances in 3D content creation mostly leverage optimization-based 3D
generation via score distillation sampling (SDS). Though promising results have
been exhibited, these methods often suffer from slow per-sample optimization,
limiting their practical usage. In this paper, we propose DreamGaussian, a
novel 3D content generation framework that achieves both efficiency and quality
simultaneously. Our key insight is to design a generative 3D Gaussian Splatting
model with companioned mesh extraction and texture refinement in UV space. In
contrast to the occupancy pruning used in Neural Radiance Fields, we
demonstrate that the progressive densification of 3D Gaussians converges
significantly faster for 3D generative tasks. To further enhance the texture
quality and facilitate downstream applications, we introduce an efficient
algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning
stage to refine the details. Extensive experiments demonstrate the superior
efficiency and competitive generation quality of our proposed approach.
Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes
from a single-view image, achieving approximately 10 times acceleration
compared to existing methods.
</p></li>
</ul>

<h3>Title: RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16668">http://arxiv.org/abs/2309.16668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16668]] RealFill: Reference-Driven Generation for Authentic Image Completion(http://arxiv.org/abs/2309.16668)</code></li>
<li>Summary: <p>Recent advances in generative imagery have brought forth outpainting and
inpainting models that can produce high-quality, plausible image content in
unknown regions, but the content these models hallucinate is necessarily
inauthentic, since the models lack sufficient context about the true scene. In
this work, we propose RealFill, a novel generative approach for image
completion that fills in missing regions of an image with the content that
should have been there. RealFill is a generative inpainting model that is
personalized using only a few reference images of a scene. These reference
images do not have to be aligned with the target image, and can be taken with
drastically varying viewpoints, lighting conditions, camera apertures, or image
styles. Once personalized, RealFill is able to complete a target image with
visually compelling contents that are faithful to the original scene. We
evaluate RealFill on a new image completion benchmark that covers a set of
diverse and challenging scenarios, and find that it outperforms existing
approaches by a large margin. See more results on our project page:
https://realfill.github.io
</p></li>
</ul>

<h3>Title: Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16671">http://arxiv.org/abs/2309.16671</a></li>
<li>Code URL: https://github.com/facebookresearch/metaclip</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16671]] Demystifying CLIP Data(http://arxiv.org/abs/2309.16671)</code></li>
<li>Summary: <p>Contrastive Language-Image Pre-training (CLIP) is an approach that has
advanced research and applications in computer vision, fueling modern
recognition systems and generative models. We believe that the main ingredient
to the success of CLIP is its data and not the model architecture or
pre-training objective. However, CLIP only provides very limited information
about its data and how it has been collected, leading to works that aim to
reproduce CLIP's data by filtering with its model parameters. In this work, we
intend to reveal CLIP's data curation approach and in our pursuit of making it
open to the community introduce Metadata-Curated Language-Image Pre-training
(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's
concepts) and yields a balanced subset over the metadata distribution. Our
experimental study rigorously isolates the model and training settings,
concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M
image-text data pairs outperforms CLIP's data on multiple standard benchmarks.
In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,
surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining
the same training budget, attains 72.4%. Our observations hold across various
model sizes, exemplified by ViT-H achieving 80.5%, without any
bells-and-whistles. Curation code and training data distribution on metadata is
made available at https://github.com/facebookresearch/MetaCLIP.
</p></li>
</ul>

<h3>Title: Graph-level Representation Learning with Joint-Embedding Predictive Architectures. (arXiv:2309.16014v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16014">http://arxiv.org/abs/2309.16014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16014]] Graph-level Representation Learning with Joint-Embedding Predictive Architectures(http://arxiv.org/abs/2309.16014)</code></li>
<li>Summary: <p>Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a
novel and powerful technique for self-supervised representation learning. They
aim to learn an energy-based model by predicting the latent representation of a
target signal $y$ from a context signal $x$. JEPAs bypass the need for data
augmentation and negative samples, which are typically required by contrastive
learning, while avoiding the overfitting issues associated with
generative-based pretraining. In this paper, we show that graph-level
representations can be effectively modeled using this paradigm and propose
Graph-JEPA, the first JEPA for the graph domain. In particular, we employ
masked modeling to learn embeddings for different subgraphs of the input graph.
To endow the representations with the implicit hierarchy that is often present
in graph-level concepts, we devise an alternative training objective that
consists of predicting the coordinates of the encoded subgraphs on the unit
hyperbola in the 2D plane. Extensive validation shows that Graph-JEPA can learn
representations that are expressive and competitive in both graph
classification and regression problems.
</p></li>
</ul>

<h3>Title: Compositional Sculpting of Iterative Generative Processes. (arXiv:2309.16115v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16115">http://arxiv.org/abs/2309.16115</a></li>
<li>Code URL: https://github.com/timgaripov/compositional-sculpting</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16115]] Compositional Sculpting of Iterative Generative Processes(http://arxiv.org/abs/2309.16115)</code></li>
<li>Summary: <p>High training costs of generative models and the need to fine-tune them for
specific tasks have created a strong interest in model reuse and composition. A
key challenge in composing iterative generative processes, such as GFlowNets
and diffusion models, is that to realize the desired target distribution, all
steps of the generative process need to be coordinated, and satisfy delicate
balance conditions. In this work, we propose Compositional Sculpting: a general
approach for defining compositions of iterative generative processes. We then
introduce a method for sampling from these compositions built on classifier
guidance. We showcase ways to accomplish compositional sculpting in both
GFlowNets and diffusion models. We highlight two binary operations
$\unicode{x2014}$ the harmonic mean ($p_1 \otimes p_2$) and the contrast ($p_1
\unicode{x25D1}\,p_2$) between pairs, and the generalization of these
operations to multiple component distributions. We offer empirical results on
image and molecular generation tasks.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases. (arXiv:2309.16035v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16035">http://arxiv.org/abs/2309.16035</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16035]] MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases(http://arxiv.org/abs/2309.16035)</code></li>
<li>Summary: <p>Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks like medical question answering (QA).
Moreover, they tend to function as "black-boxes," making it challenging to
modify their behavior. Addressing this, our study delves into model editing
utilizing in-context learning, aiming to improve LLM responses without the need
for fine-tuning or retraining. Specifically, we propose a comprehensive
retrieval strategy to extract medical facts from an external knowledge base,
and then we incorporate them into the query prompt for the LLM. Focusing on
medical QA using the MedQA-SMILE dataset, we evaluate the impact of different
retrieval models and the number of facts provided to the LLM. Notably, our
edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%.
This work underscores the potential of model editing to enhance LLM
performance, offering a practical approach to mitigate the challenges of
black-box LLMs.
</p></li>
</ul>

<h3>Title: The Confidence-Competence Gap in Large Language Models: A Cognitive Study. (arXiv:2309.16145v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16145">http://arxiv.org/abs/2309.16145</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16145]] The Confidence-Competence Gap in Large Language Models: A Cognitive Study(http://arxiv.org/abs/2309.16145)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have acquired ubiquitous attention for their
performances across diverse domains. Our study here searches through LLMs'
cognitive abilities and confidence dynamics. We dive deep into understanding
the alignment between their self-assessed confidence and actual performance. We
exploit these models with diverse sets of questionnaires and real-world
scenarios and extract how LLMs exhibit confidence in their responses. Our
findings reveal intriguing instances where models demonstrate high confidence
even when they answer incorrectly. This is reminiscent of the Dunning-Kruger
effect observed in human psychology. In contrast, there are cases where models
exhibit low confidence with correct answers revealing potential underestimation
biases. Our results underscore the need for a deeper understanding of their
cognitive processes. By examining the nuances of LLMs' self-assessment
mechanism, this investigation provides noteworthy revelations that serve to
advance the functionalities and broaden the potential applications of these
formidable language models.
</p></li>
</ul>

<h3>Title: AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events. (arXiv:2309.16150v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16150">http://arxiv.org/abs/2309.16150</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16150]] AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events(http://arxiv.org/abs/2309.16150)</code></li>
<li>Summary: <p>Though Vaccines are instrumental in global health, mitigating infectious
diseases and pandemic outbreaks, they can occasionally lead to adverse events
(AEs). Recently, Large Language Models (LLMs) have shown promise in effectively
identifying and cataloging AEs within clinical reports. Utilizing data from the
Vaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study
particularly focuses on AEs to evaluate LLMs' capability for AE extraction. A
variety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2,
were evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5
model (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match
and 0.816 for relaxed match. The encouraging performance of the AE-GPT
underscores LLMs' potential in processing medical data, indicating a
significant stride towards advanced AE detection, thus presumably generalizable
to other AE extraction tasks.
</p></li>
</ul>

<h3>Title: Large Language Model Soft Ideologization via AI-Self-Consciousness. (arXiv:2309.16167v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16167">http://arxiv.org/abs/2309.16167</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16167]] Large Language Model Soft Ideologization via AI-Self-Consciousness(http://arxiv.org/abs/2309.16167)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated human-level performance on a
vast spectrum of natural language tasks. However, few studies have addressed
the LLM threat and vulnerability from an ideology perspective, especially when
they are increasingly being deployed in sensitive domains, e.g., elections and
education. In this study, we explore the implications of GPT soft
ideologization through the use of AI-self-consciousness. By utilizing GPT
self-conversations, AI can be granted a vision to "comprehend" the intended
ideology, and subsequently generate finetuning data for LLM ideology injection.
When compared to traditional government ideology manipulation techniques, such
as information censorship, LLM ideologization proves advantageous; it is easy
to implement, cost-effective, and powerful, thus brimming with risks.
</p></li>
</ul>

<h3>Title: Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems. (arXiv:2309.16248v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16248">http://arxiv.org/abs/2309.16248</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16248]] Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems(http://arxiv.org/abs/2309.16248)</code></li>
<li>Summary: <p>With the recent spike in the number and availability of Large Language Models
(LLMs), it has become increasingly important to provide large and realistic
benchmarks for evaluating Knowledge Graph Question Answering (KBQA) systems. So
far the majority of benchmarks rely on pattern-based SPARQL query generation
approaches. The subsequent natural language (NL) question generation is
conducted through crowdsourcing or other automated methods, such as rule-based
paraphrasing or NL question templates. Although some of these datasets are of
considerable size, their pitfall lies in their pattern-based generation
approaches, which do not always generalize well to the vague and linguistically
diverse questions asked by humans in real-world contexts.
</p>
<p>In this paper, we introduce Spider4SPARQL - a new SPARQL benchmark dataset
featuring 9,693 previously existing manually generated NL questions and 4,721
unique, novel, and complex SPARQL queries of varying complexity. In addition to
the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs
and ontologies, which cover 138 different domains. Our complex benchmark
enables novel ways of evaluating the strengths and weaknesses of modern KGQA
systems. We evaluate the system with state-of-the-art KGQA systems as well as
LLMs, which achieve only up to 45\% execution accuracy, demonstrating that
Spider4SPARQL is a challenging benchmark for future research.
</p></li>
</ul>

<h3>Title: LawBench: Benchmarking Legal Knowledge of Large Language Models. (arXiv:2309.16289v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16289">http://arxiv.org/abs/2309.16289</a></li>
<li>Code URL: https://github.com/open-compass/lawbench</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16289]] LawBench: Benchmarking Legal Knowledge of Large Language Models(http://arxiv.org/abs/2309.16289)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated strong capabilities in various
aspects. However, when applying them to the highly specialized, safe-critical
legal domain, it is unclear how much legal knowledge they possess and whether
they can reliably perform legal-related tasks. To address this gap, we propose
a comprehensive evaluation benchmark LawBench. LawBench has been meticulously
crafted to have precise assessment of the LLMs' legal capabilities from three
cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize
needed legal concepts, articles and facts; (2) Legal knowledge understanding:
whether LLMs can comprehend entities, events and relationships within legal
text; (3) Legal knowledge applying: whether LLMs can properly utilize their
legal knowledge and make necessary reasoning steps to solve realistic legal
tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label
classification (SLC), multi-label classification (MLC), regression, extraction
and generation. We perform extensive evaluations of 51 LLMs on LawBench,
including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific
LLMs. The results show that GPT-4 remains the best-performing LLM in the legal
domain, surpassing the others by a significant margin. While fine-tuning LLMs
on legal specific text brings certain improvements, we are still a long way
from obtaining usable and reliable LLMs in legal tasks. All data, model
predictions and evaluation code are released in
https://github.com/open-compass/LawBench/. We hope this benchmark provides
in-depth understanding of the LLMs' domain-specified capabilities and speed up
the development of LLMs in the legal domain.
</p></li>
</ul>

<h3>Title: A Benchmark for Learning to Translate a New Language from One Grammar Book. (arXiv:2309.16575v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16575">http://arxiv.org/abs/2309.16575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16575]] A Benchmark for Learning to Translate a New Language from One Grammar Book(http://arxiv.org/abs/2309.16575)</code></li>
<li>Summary: <p>Large language models (LLMs) can perform impressive feats with in-context
learning or lightweight finetuning. It is natural to wonder how well these
models adapt to genuinely new tasks, but how does one find tasks that are
unseen in internet-scale training sets? We turn to a field that is explicitly
motivated and bottlenecked by a scarcity of web data: low-resource languages.
In this paper, we introduce MTOB (Machine Translation from One Book), a
benchmark for learning to translate between English and Kalamang -- a language
with less than 200 speakers and therefore virtually no presence on the web --
using several hundred pages of field linguistics reference materials. This task
framing is novel in that it asks a model to learn a language from a single
human-readable book of grammar explanations, rather than a large mined corpus
of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate
that baselines using current LLMs are promising but fall short of human
performance, achieving 44.7 chrF on Kalamang to English translation and 45.8
chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a
human who learned Kalamang from the same reference materials. We hope that MTOB
will help measure LLM capabilities along a new dimension, and that the methods
developed to solve it could help expand access to language technology for
underserved communities by leveraging qualitatively different kinds of data
than traditional machine translation.
</p></li>
</ul>

<h3>Title: GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16583">http://arxiv.org/abs/2309.16583</a></li>
<li>Code URL: https://github.com/openai/evals</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16583]] GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond(http://arxiv.org/abs/2309.16583)</code></li>
<li>Summary: <p>With the rapid advancement of large language models (LLMs), there is a
pressing need for a comprehensive evaluation suite to assess their capabilities
and limitations. Existing LLM leaderboards often reference scores reported in
other papers without consistent settings and prompts, which may inadvertently
encourage cherry-picking favored settings and prompts for better results. In
this work, we introduce GPT-Fathom, an open-source and reproducible LLM
evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+
leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across
7 capability categories, all under aligned settings. Our retrospective study on
OpenAI's earlier models offers valuable insights into the evolutionary path
from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3
progressively improves to GPT-4, including technical details like whether
adding code data improves LLM's reasoning capability, which aspects of LLM
capability can be improved by SFT and RLHF, how much is the alignment tax, etc.
Our analysis sheds light on many of these questions, aiming to improve the
transparency of advanced LLMs.
</p></li>
</ul>

<h3>Title: Qwen Technical Report. (arXiv:2309.16609v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16609">http://arxiv.org/abs/2309.16609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16609]] Qwen Technical Report(http://arxiv.org/abs/2309.16609)</code></li>
<li>Summary: <p>Large language models (LLMs) have revolutionized the field of artificial
intelligence, enabling natural language processing tasks that were previously
thought to be exclusive to humans. In this work, we introduce Qwen, the first
installment of our large language model series. Qwen is a comprehensive
language model series that encompasses distinct models with varying parameter
counts. It includes Qwen, the base pretrained language models, and Qwen-Chat,
the chat models finetuned with human alignment techniques. The base language
models consistently demonstrate superior performance across a multitude of
downstream tasks, and the chat models, particularly those trained using
Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The
chat models possess advanced tool-use and planning capabilities for creating
agent applications, showcasing impressive performance even when compared to
bigger models on complex tasks like utilizing a code interpreter. Furthermore,
we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as
well as mathematics-focused models, Math-Qwen-Chat, which are built upon base
language models. These models demonstrate significantly improved performance in
comparison with open-source models, and slightly fall behind the proprietary
models.
</p></li>
</ul>

<h3>Title: Stress Testing Chain-of-Thought Prompting for Large Language Models. (arXiv:2309.16621v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16621">http://arxiv.org/abs/2309.16621</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16621]] Stress Testing Chain-of-Thought Prompting for Large Language Models(http://arxiv.org/abs/2309.16621)</code></li>
<li>Summary: <p>This report examines the effectiveness of Chain-of-Thought (CoT) prompting in
improving the multi-step reasoning abilities of large language models (LLMs).
Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze the
impact of three types of CoT prompt perturbations, namely CoT order, CoT
values, and CoT operators on the performance of GPT-3 on various tasks. Our
findings show that incorrect CoT prompting leads to poor performance on
accuracy metrics. Correct values in the CoT is crucial for predicting correct
answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT
order are wrong, do not affect the performance as drastically when compared to
the value based perturbations. This research deepens our understanding of CoT
prompting and opens some new questions regarding the capability of LLMs to
learn reasoning in context.
</p></li>
</ul>

<h3>Title: MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention. (arXiv:2309.16639v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16639">http://arxiv.org/abs/2309.16639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16639]] MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention(http://arxiv.org/abs/2309.16639)</code></li>
<li>Summary: <p>Problematic smartphone use negatively affects physical and mental health.
Despite the wide range of prior research, existing persuasive techniques are
not flexible enough to provide dynamic persuasion content based on users'
physical contexts and mental states. We first conduct a Wizard-of-Oz study
(N=12) and an interview study (N=10) to summarize the mental states behind
problematic smartphone use: boredom, stress, and inertia. This informs our
design of four persuasion strategies: understanding, comforting, evoking, and
scaffolding habits. We leverage large language models (LLMs) to enable the
automatic and dynamic generation of effective persuasion content. We develop
MindShift, a novel LLM-powered problematic smartphone use intervention
technique. MindShift takes users' in-the-moment physical contexts, mental
states, app usage behaviors, users' goals &amp; habits as input, and generates
high-quality and flexible persuasive content with appropriate persuasion
strategies. We conduct a 5-week field experiment (N=25) to compare MindShift
with baseline techniques. The results show that MindShift significantly
improves intervention acceptance rates by 17.8-22.5% and reduces smartphone use
frequency by 12.1-14.4%. Moreover, users have a significant drop in smartphone
addiction scale scores and a rise in self-efficacy. Our study sheds light on
the potential of leveraging LLMs for context-aware persuasion in other behavior
change domains.
</p></li>
</ul>

<h3>Title: HuntGPT: Integrating Machine Learning-Based Anomaly Detection and Explainable AI with Large Language Models (LLMs). (arXiv:2309.16021v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16021">http://arxiv.org/abs/2309.16021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16021]] HuntGPT: Integrating Machine Learning-Based Anomaly Detection and Explainable AI with Large Language Models (LLMs)(http://arxiv.org/abs/2309.16021)</code></li>
<li>Summary: <p>Machine learning (ML) is crucial in network anomaly detection for proactive
threat hunting, reducing detection and response times significantly. However,
challenges in model training, maintenance, and frequent false positives impact
its acceptance and reliability. Explainable AI (XAI) attempts to mitigate these
issues, allowing cybersecurity teams to assess AI-generated alerts with
confidence, but has seen limited acceptance from incident responders. Large
Language Models (LLMs) present a solution through discerning patterns in
extensive information and adapting to different functional requirements. We
present HuntGPT, a specialized intrusion detection dashboard applying a Random
Forest classifier using the KDD99 dataset, integrating XAI frameworks like SHAP
and Lime for user-friendly and intuitive model interaction, and combined with a
GPT-3.5 Turbo, it delivers threats in an understandable format. The paper
delves into the system's architecture, components, and technical accuracy,
assessed through Certified Information Security Manager (CISM) Practice Exams,
evaluating response quality across six metrics. The results demonstrate that
conversational agents, supported by LLM and integrated with XAI, provide
robust, explainable, and actionable AI solutions in intrusion detection,
enhancing user understanding and interactive experience.
</p></li>
</ul>

<h3>Title: ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. (arXiv:2309.16119v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16119">http://arxiv.org/abs/2309.16119</a></li>
<li>Code URL: https://github.com/kuleshov-group/modulora-experiment</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16119]] ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers(http://arxiv.org/abs/2309.16119)</code></li>
<li>Summary: <p>We propose a memory-efficient finetuning algorithm for large language models
(LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit
precision on as little as one 48GB GPU. Our method, modular low-rank adaptation
(ModuLoRA), integrates any user-specified weight quantizer with finetuning via
low-rank adapters (LoRAs). Our approach relies on a simple
quantization-agnostic backward pass that adaptively materializes low-precision
LLM weights from a custom black-box quantization module. This approach enables
finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit
OPTQ quantization often outperforms finetuning that relies on less
sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains
competitive performance on text classification, natural language infernece, and
instruction following tasks using significantly less memory than existing
approaches, and we also surpass the state-of-the-art ROUGE score on a popular
summarization task. We release ModuLoRA together with a series of low-precision
models--including the first family of 3-bit instruction following Alpaca
LLMs--as part of LLMTOOLS, a user-friendly library for quantizing, running, and
finetuning LLMs on consumer GPUs.
</p></li>
</ul>

<h3>Title: Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. (arXiv:2309.16240v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16240">http://arxiv.org/abs/2309.16240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16240]] Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints(http://arxiv.org/abs/2309.16240)</code></li>
<li>Summary: <p>The increasing capabilities of large language models (LLMs) raise
opportunities for artificial general intelligence but concurrently amplify
safety concerns, such as potential misuse of AI systems, necessitating
effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has
emerged as a promising pathway towards AI alignment but brings forth challenges
due to its complexity and dependence on a separate reward model. Direct
Preference Optimization (DPO) has been proposed as an alternative, and it
remains equivalent to RLHF under the reverse KL regularization constraint. This
paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse
divergence constraints. We show that under certain $f$-divergences, including
Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the
complex relationship between the reward and optimal policy can also be
simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the
need for estimating the normalizing constant in the Bradley-Terry model and
enables a tractable mapping between the reward function and the optimal policy.
Our approach optimizes LLMs to align with human preferences in a more efficient
and supervised manner under a broad set of divergence constraints. Empirically,
adopting these divergences ensures a balance between alignment performance and
generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in
divergence efficiency, and divergence constraints directly influence expected
calibration error (ECE).
</p></li>
</ul>

<h3>Title: Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16595">http://arxiv.org/abs/2309.16595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16595]] Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why(http://arxiv.org/abs/2309.16595)</code></li>
<li>Summary: <p>This paper studies Large Language Models (LLMs) for structured
data--particularly graphs--a crucial data modality that remains underexplored
in the LLM literature. We aim to understand when and why the incorporation of
structural information inherent in graph data can improve the prediction
performance of LLMs on node classification tasks. To address the ``when''
question, we examine a variety of prompting methods for encoding structural
information, in settings where textual node features are either rich or scarce.
For the ``why'' questions, we probe into two potential contributing factors to
the LLM performance: data leakage and homophily. Our exploration of these
questions reveals that (i) LLMs can benefit from structural information,
especially when textual node features are scarce; (ii) there is no substantial
evidence indicating that the performance of LLMs is significantly attributed to
data leakage; and (iii) the performance of LLMs on a target node is strongly
positively related to the local homophily ratio of the node.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Reflection Invariance Learning for Few-shot Semantic Segmentation. (arXiv:2309.15850v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15850">http://arxiv.org/abs/2309.15850</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15850]] Reflection Invariance Learning for Few-shot Semantic Segmentation(http://arxiv.org/abs/2309.15850)</code></li>
<li>Summary: <p>Few-shot semantic segmentation (FSS) aims to segment objects of unseen
classes in query images with only a few annotated support images. Existing FSS
algorithms typically focus on mining category representations from the
single-view support to match semantic objects of the single-view query.
However, the limited annotated samples render the single-view matching struggle
to perceive the reflection invariance of novel objects, which results in a
restricted learning space for novel categories and further induces a biased
segmentation with demoted parsing performance. To address this challenge, this
paper proposes a fresh few-shot segmentation framework to mine the reflection
invariance in a multi-view matching manner. Specifically, original and
reflection support features from different perspectives with the same semantics
are learnable fused to obtain the reflection invariance prototype with a
stronger category representation ability. Simultaneously, aiming at providing
better prior guidance, the Reflection Invariance Prior Mask Generation (RIPMG)
module is proposed to integrate prior knowledge from different perspectives.
Finally, segmentation predictions from varying views are complementarily merged
in the Reflection Invariance Semantic Prediction (RISP) module to yield precise
segmentation predictions. Extensive experiments on both PASCAL-$5^\textit{i}$
and COCO-$20^\textit{i}$ datasets demonstrate the effectiveness of our approach
and show that our method could achieve state-of-the-art performance. Code is
available at \url{https://anonymous.4open.science/r/RILFS-A4D1}
</p></li>
</ul>

<h3>Title: Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation. (arXiv:2309.16127v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16127">http://arxiv.org/abs/2309.16127</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16127]] Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation(http://arxiv.org/abs/2309.16127)</code></li>
<li>Summary: <p>Many methods of semantic image segmentation have borrowed the success of open
compound domain adaptation. They minimize the style gap between the images of
source and target domains, more easily predicting the accurate pseudo
annotations for target domain's images that train segmentation network. The
existing methods globally adapt the scene style of the images, whereas the
object styles of different categories or instances are adapted improperly. This
paper proposes the Object Style Compensation, where we construct the
Object-Level Discrepancy Memory with multiple sets of discrepancy features. The
discrepancy features in a set capture the style changes of the same category's
object instances adapted from target to source domains. We learn the
discrepancy features from the images of source and target domains, storing the
discrepancy features in memory. With this memory, we select appropriate
discrepancy features for compensating the style information of the object
instances of various categories, adapting the object styles to a unified style
of source domain. Our method enables a more accurate computation of the pseudo
annotations for target domain's images, thus yielding state-of-the-art results
on different datasets.
</p></li>
</ul>

<h3>Title: Joint Correcting and Refinement for Balanced Low-Light Image Enhancement. (arXiv:2309.16128v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16128">http://arxiv.org/abs/2309.16128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16128]] Joint Correcting and Refinement for Balanced Low-Light Image Enhancement(http://arxiv.org/abs/2309.16128)</code></li>
<li>Summary: <p>Low-light image enhancement tasks demand an appropriate balance among
brightness, color, and illumination. While existing methods often focus on one
aspect of the image without considering how to pay attention to this balance,
which will cause problems of color distortion and overexposure etc. This
seriously affects both human visual perception and the performance of
high-level visual models. In this work, a novel synergistic structure is
proposed which can balance brightness, color, and illumination more
effectively. Specifically, the proposed method, so-called Joint Correcting and
Refinement Network (JCRNet), which mainly consists of three stages to balance
brightness, color, and illumination of enhancement. Stage 1: we utilize a basic
encoder-decoder and local supervision mechanism to extract local information
and more comprehensive details for enhancement. Stage 2: cross-stage feature
transmission and spatial feature transformation further facilitate color
correction and feature refinement. Stage 3: we employ a dynamic illumination
adjustment approach to embed residuals between predicted and ground truth
images into the model, adaptively adjusting illumination balance. Extensive
experiments demonstrate that the proposed method exhibits comprehensive
performance advantages over 21 state-of-the-art methods on 9 benchmark
datasets. Furthermore, a more persuasive experiment has been conducted to
validate our approach the effectiveness in downstream visual tasks (e.g.,
saliency detection). Compared to several enhancement models, the proposed
method effectively improves the segmentation results and quantitative metrics
of saliency detection. The source code will be available at
https://github.com/woshiyll/JCRNet.
</p></li>
</ul>

<h3>Title: Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling. (arXiv:2309.16139v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16139">http://arxiv.org/abs/2309.16139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16139]] Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling(http://arxiv.org/abs/2309.16139)</code></li>
<li>Summary: <p>Training high-quality instance segmentation models requires an abundance of
labeled images with instance masks and classifications, which is often
expensive to procure. Active learning addresses this challenge by striving for
optimum performance with minimal labeling cost by selecting the most
informative and representative images for labeling. Despite its potential,
active learning has been less explored in instance segmentation compared to
other tasks like image classification, which require less labeling. In this
study, we propose a post-hoc active learning algorithm that integrates
uncertainty-based sampling with diversity-based sampling. Our proposed
algorithm is not only simple and easy to implement, but it also delivers
superior performance on various datasets. Its practical application is
demonstrated on a real-world overhead imagery dataset, where it increases the
labeling efficiency fivefold.
</p></li>
</ul>

<h3>Title: Cross-City Matters: A Multimodal Remote Sensing Benchmark Dataset for Cross-City Semantic Segmentation using High-Resolution Domain Adaptation Networks. (arXiv:2309.16499v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16499">http://arxiv.org/abs/2309.16499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16499]] Cross-City Matters: A Multimodal Remote Sensing Benchmark Dataset for Cross-City Semantic Segmentation using High-Resolution Domain Adaptation Networks(http://arxiv.org/abs/2309.16499)</code></li>
<li>Summary: <p>Artificial intelligence (AI) approaches nowadays have gained remarkable
success in single-modality-dominated remote sensing (RS) applications,
especially with an emphasis on individual urban environments (e.g., single
cities or regions). Yet these AI models tend to meet the performance bottleneck
in the case studies across cities or regions, due to the lack of diverse RS
information and cutting-edge solutions with high generalization ability. To
this end, we build a new set of multimodal remote sensing benchmark datasets
(including hyperspectral, multispectral, SAR) for the study purpose of the
cross-city semantic segmentation task (called C2Seg dataset), which consists of
two cross-city scenes, i.e., Berlin-Augsburg (in Germany) and Beijing-Wuhan (in
China). Beyond the single city, we propose a high-resolution domain adaptation
network, HighDAN for short, to promote the AI model's generalization ability
from the multi-city environments. HighDAN is capable of retaining the spatially
topological structure of the studied urban scene well in a parallel high-to-low
resolution fusion fashion but also closing the gap derived from enormous
differences of RS image representations between different cities by means of
adversarial learning. In addition, the Dice loss is considered in HighDAN to
alleviate the class imbalance issue caused by factors across cities. Extensive
experiments conducted on the C2Seg dataset show the superiority of our HighDAN
in terms of segmentation performance and generalization ability, compared to
state-of-the-art competitors. The C2Seg dataset and the semantic segmentation
toolbox (involving the proposed HighDAN) will be available publicly at
https://github.com/danfenghong.
</p></li>
</ul>

<h3>Title: Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping. (arXiv:2309.16515v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16515">http://arxiv.org/abs/2309.16515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16515]] Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping(http://arxiv.org/abs/2309.16515)</code></li>
<li>Summary: <p>Deep Neural Networks (DNNs) that achieve human-level performance in general
tasks like object segmentation typically require supervised labels. In
contrast, humans are able to perform these tasks effortlessly without
supervision. To accomplish this, the human visual system makes use of
perceptual grouping. Understanding how perceptual grouping arises in an
unsupervised manner is critical for improving both models of the visual system,
and computer vision models. In this work, we propose a counterintuitive
approach to unsupervised perceptual grouping and segmentation: that they arise
because of neural noise, rather than in spite of it. We (1) mathematically
demonstrate that under realistic assumptions, neural noise can be used to
separate objects from each other, and (2) show that adding noise in a DNN
enables the network to segment images even though it was never trained on any
segmentation labels. Interestingly, we find that (3) segmenting objects using
noise results in segmentation performance that aligns with the perceptual
grouping phenomena observed in humans. We introduce the Good Gestalt (GG)
datasets -- six datasets designed to specifically test perceptual grouping, and
show that our DNN models reproduce many important phenomena in human
perception, such as illusory contours, closure, continuity, proximity, and
occlusion. Finally, we (4) demonstrate the ecological plausibility of the
method by analyzing the sensitivity of the DNN to different magnitudes of
noise. We find that some model variants consistently succeed with remarkably
low levels of neural noise ($\sigma&lt;0.001$), and surprisingly, that segmenting
this way requires as few as a handful of samples. Together, our results suggest
a novel unsupervised segmentation method requiring few assumptions, a new
explanation for the formation of perceptual grouping, and a potential benefit
of neural noise in the visual system.
</p></li>
</ul>

<h3>Title: Voting Network for Contour Levee Farmland Segmentation and Classification. (arXiv:2309.16561v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16561">http://arxiv.org/abs/2309.16561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16561]] Voting Network for Contour Levee Farmland Segmentation and Classification(http://arxiv.org/abs/2309.16561)</code></li>
<li>Summary: <p>High-resolution aerial imagery allows fine details in the segmentation of
farmlands. However, small objects and features introduce distortions to the
delineation of object boundaries, and larger contextual views are needed to
mitigate class confusion. In this work, we present an end-to-end trainable
network for segmenting farmlands with contour levees from high-resolution
aerial imagery. A fusion block is devised that includes multiple voting blocks
to achieve image segmentation and classification. We integrate the fusion block
with a backbone and produce both semantic predictions and segmentation slices.
The segmentation slices are used to perform majority voting on the predictions.
The network is trained to assign the most likely class label of a segment to
its pixels, learning the concept of farmlands rather than analyzing
constitutive pixels separately. We evaluate our method using images from the
National Agriculture Imagery Program. Our method achieved an average accuracy
of 94.34\%. Compared to the state-of-the-art methods, the proposed method
obtains an improvement of 6.96% and 2.63% in the F1 score on average.
</p></li>
</ul>

<h3>Title: Visual In-Context Learning for Few-Shot Eczema Segmentation. (arXiv:2309.16656v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16656">http://arxiv.org/abs/2309.16656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16656]] Visual In-Context Learning for Few-Shot Eczema Segmentation(http://arxiv.org/abs/2309.16656)</code></li>
<li>Summary: <p>Automated diagnosis of eczema from digital camera images is crucial for
developing applications that allow patients to self-monitor their recovery. An
important component of this is the segmentation of eczema region from such
images. Current methods for eczema segmentation rely on deep neural networks
such as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. While
effective, these methods require high volume of annotated data, which can be
difficult to obtain. Here, we investigate the capabilities of visual in-context
learning that can perform few-shot eczema segmentation with just a handful of
examples and without any need for retraining models. Specifically, we propose a
strategy for applying in-context learning for eczema segmentation with a
generalist vision model called SegGPT. When benchmarked on a dataset of
annotated eczema images, we show that SegGPT with just 2 representative example
images from the training dataset performs better (mIoU: 36.69) than a CNN U-Net
trained on 428 images (mIoU: 32.60). We also discover that using more number of
examples for SegGPT may in fact be harmful to its performance. Our result
highlights the importance of visual in-context learning in developing faster
and better solutions to skin imaging tasks. Our result also paves the way for
developing inclusive solutions that can cater to minorities in the demographics
who are typically heavily under-represented in the training data.
</p></li>
</ul>

<h3>Title: SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation. (arXiv:2309.16661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.16661">http://arxiv.org/abs/2309.16661</a></li>
<li>Code URL: https://github.com/mustansarfiaz/sa2-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.16661]] SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation(http://arxiv.org/abs/2309.16661)</code></li>
<li>Summary: <p>Microscopic image segmentation is a challenging task, wherein the objective
is to assign semantic labels to each pixel in a given microscopic image. While
convolutional neural networks (CNNs) form the foundation of many existing
frameworks, they often struggle to explicitly capture long-range dependencies.
Although transformers were initially devised to address this issue using
self-attention, it has been proven that both local and global features are
crucial for addressing diverse challenges in microscopic images, including
variations in shape, size, appearance, and target region density. In this
paper, we introduce SA2-Net, an attention-guided method that leverages
multi-scale feature learning to effectively handle diverse structures within
microscopic images. Specifically, we propose scale-aware attention (SA2) module
designed to capture inherent variations in scales and shapes of microscopic
regions, such as cells, for accurate segmentation. This module incorporates
local attention at each level of multi-stage features, as well as global
attention across multiple resolutions. Furthermore, we address the issue of
blurred region boundaries (e.g., cell boundaries) by introducing a novel
upsampling strategy called the Adaptive Up-Attention (AuA) module. This module
enhances the discriminative ability for improved localization of microscopic
regions using an explicit attention mechanism. Extensive experiments on five
challenging datasets demonstrate the benefits of our SA2-Net model. Our source
code is publicly available at \url{https://github.com/mustansarfiaz/SA2-Net}.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
