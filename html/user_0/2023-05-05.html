<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Uncertainty Aware Deep Learning Model for Secure and Trustworthy Channel Estimation in 5G Networks. (arXiv:2305.02741v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02741">http://arxiv.org/abs/2305.02741</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02741] Uncertainty Aware Deep Learning Model for Secure and Trustworthy Channel Estimation in 5G Networks](http://arxiv.org/abs/2305.02741) #secure</code></li>
<li>Summary: <p>With the rise of intelligent applications, such as self-driving cars and
augmented reality, the security and reliability of wireless communication
systems have become increasingly crucial. One of the most critical components
of ensuring a high-quality experience is channel estimation, which is
fundamental for efficient transmission and interference management in wireless
networks. However, using deep neural networks (DNNs) in channel estimation
raises security and trust concerns due to their complexity and the need for
more transparency in decision-making. This paper proposes a Monte Carlo Dropout
(MCDO)-based approach for secure and trustworthy channel estimation in 5G
networks. Our approach combines the advantages of traditional and deep learning
techniques by incorporating conventional pilot-based channel estimation as a
prior in the deep learning model. Additionally, we use MCDO to obtain
uncertainty-aware predictions, enhancing the model's security and
trustworthiness. Our experiments demonstrate that our proposed approach
outperforms traditional and deep learning-based approaches regarding security,
trustworthiness, and performance in 5G scenarios.
</p></li>
</ul>

<h3>Title: Faulting original McEliece's implementations is possible: How to mitigate this risk?. (arXiv:2305.02855v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02855">http://arxiv.org/abs/2305.02855</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02855] Faulting original McEliece's implementations is possible: How to mitigate this risk?](http://arxiv.org/abs/2305.02855) #secure</code></li>
<li>Summary: <p>Private and public actors increasingly encounter use cases where they need to
implement sensitive operations on mass-market peripherals for which they have
little or no control. They are sometimes inclined to attempt this without using
hardware-assisted equipment, such as secure elements. In this case, the
white-box attack model is particularly relevant and includes access to every
asset, retro-engineering, and binary instrumentation by attackers. At the same
time, quantum attacks are becoming more and more of a threat and challenge
traditional asymmetrical ciphers, which are treasured by private and public
actors.
</p></li>
</ul>

<p>The McEliece cryptosystem is a code-based public key algorithm introduced in
1978 that is not subject to well-known quantum attacks and that could be
implemented in an uncontrolled environment. During the NIST post-quantum
cryptography standardization process, a derived candidate commonly refer to as
classic McEliece was selected. This algorithm is however vulnerable to some
fault injection attacks while a priori, this does not apply to the original
McEliece. In this article, we thus focus on the original McEliece cryptosystem
and we study its resilience against fault injection attacks on an ARM reference
implementation. We disclose the first fault injection based attack and we
discuss on how to modify the original McEliece cryptosystem to make it
resilient to fault injection attacks.
</p>

<h2>security</h2>
<h3>Title: On the Security Risks of Knowledge Graph Reasoning. (arXiv:2305.02383v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02383">http://arxiv.org/abs/2305.02383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02383] On the Security Risks of Knowledge Graph Reasoning](http://arxiv.org/abs/2305.02383) #security</code></li>
<li>Summary: <p>Knowledge graph reasoning (KGR) -- answering complex logical queries over
large knowledge graphs -- represents an important artificial intelligence task,
entailing a range of applications (e.g., cyber threat hunting). However,
despite its surging popularity, the potential security risks of KGR are largely
unexplored, which is concerning, given the increasing use of such capability in
security-critical domains.
</p></li>
</ul>

<p>This work represents a solid initial step towards bridging the striking gap.
We systematize the security threats to KGR according to the adversary's
objectives, knowledge, and attack vectors. Further, we present ROAR, a new
class of attacks that instantiate a variety of such threats. Through empirical
evaluation in representative use cases (e.g., medical decision support, cyber
threat hunting, and commonsense reasoning), we demonstrate that ROAR is highly
effective to mislead KGR to suggest pre-defined answers for target queries, yet
with negligible impact on non-target ones. Finally, we explore potential
countermeasures against ROAR, including filtering of potentially poisoning
knowledge and training with adversarially augmented queries, which leads to
several promising research directions.
</p>

<h3>Title: Enhancing IoT Security and Privacy with Trusted Execution Environments and Machine Learning. (arXiv:2305.02584v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02584">http://arxiv.org/abs/2305.02584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02584] Enhancing IoT Security and Privacy with Trusted Execution Environments and Machine Learning](http://arxiv.org/abs/2305.02584) #security</code></li>
<li>Summary: <p>With the increasing popularity of Internet of Things (IoT) devices, security
concerns have become a major challenge: confidential information is constantly
being transmitted (sometimes inadvertently) from user devices to untrusted
cloud services. This work proposes a design to enhance security and privacy in
IoT based systems by isolating hardware peripheral drivers in a trusted
execution environment (TEE), and leveraging secure machine learning
classification techniques to filter out sensitive data, e.g., speech, images,
etc. from the associated peripheral devices before it makes its way to an
untrusted party in the cloud.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Adversarially-Guided Portrait Matting. (arXiv:2305.02981v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02981">http://arxiv.org/abs/2305.02981</a></li>
<li>Code URL: <a href="https://github.com/chroneus/stylematte">https://github.com/chroneus/stylematte</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02981] Adversarially-Guided Portrait Matting](http://arxiv.org/abs/2305.02981) #privacy</code></li>
<li>Summary: <p>We present a method for generating alpha mattes using a limited data source.
We pretrain a novel transformerbased model (StyleMatte) on portrait datasets.
We utilize this model to provide image-mask pairs for the StyleGAN3- based
network (StyleMatteGAN). This network is trained unsupervisedly and generates
previously unseen imagemask training pairs that are fed back to StyleMatte. We
demonstrate that the performance of the matte pulling network improves during
this cycle and obtains top results on the used datasets. Furthermore,
StyleMatteGAN provides high-resolution, privacy-preserving portraits with alpha
mattes, making it suitable for various image composition tasks. Our code is
available at https://github.com/chroneus/stylematte
</p></li>
</ul>

<h3>Title: Privacy in Population Protocols with Probabilistic Scheduling. (arXiv:2305.02377v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02377">http://arxiv.org/abs/2305.02377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02377] Privacy in Population Protocols with Probabilistic Scheduling](http://arxiv.org/abs/2305.02377) #privacy</code></li>
<li>Summary: <p>The population protocol model introduced by Angluin et al. in 2006 offers a
theoretical framework for designing and analyzing distributed algorithms among
limited-resource mobile agents. While the original population protocol model
considers the concept of anonymity, the issue of privacy is not investigated
thoroughly. However, there is a need for time- and space-efficient
privacy-preserving techniques in the population protocol model if these
algorithms are to be implemented in settings handling sensitive data, such as
sensor networks, IoT devices, and drones. In this work, we introduce several
formal definitions of privacy, ranging from assuring only plausible deniability
of the population input vector to having a full information-theoretic guarantee
that knowledge beyond an agent's input and output bear no influence on the
probability of a particular input vector. We then apply these definitions to
both existing and novel protocols. We show that the Remainder-computing
protocol given by Delporte-Gallet et al. in 2007 (which is proven to satisfy
output independent privacy under adversarial scheduling) is not
information-theoretically private under probabilistic scheduling. In contrast,
we provide a new algorithm and demonstrate that it correctly and
information-theoretically privately computes Remainder under probabilistic
scheduling.
</p></li>
</ul>

<h3>Title: Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02942">http://arxiv.org/abs/2305.02942</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02942] Leveraging gradient-derived metrics for data selection and valuation in differentially private training](http://arxiv.org/abs/2305.02942) #privacy</code></li>
<li>Summary: <p>Obtaining high-quality data for collaborative training of machine learning
models can be a challenging task due to A) the regulatory concerns and B) lack
of incentive to participate. The first issue can be addressed through the use
of privacy enhancing technologies (PET), one of the most frequently used one
being differentially private (DP) training. The second challenge can be
addressed by identifying which data points can be beneficial for model training
and rewarding data owners for sharing this data. However, DP in deep learning
typically adversely affects atypical (often informative) data samples, making
it difficult to assess the usefulness of individual contributions. In this work
we investigate how to leverage gradient information to identify training
samples of interest in private training settings. We show that there exist
techniques which are able to provide the clients with the tools for principled
data selection even in strictest privacy settings.
</p></li>
</ul>

<h3>Title: MLHOps: Machine Learning for Healthcare Operations. (arXiv:2305.02474v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02474">http://arxiv.org/abs/2305.02474</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02474] MLHOps: Machine Learning for Healthcare Operations](http://arxiv.org/abs/2305.02474) #privacy</code></li>
<li>Summary: <p>Machine Learning Health Operations (MLHOps) is the combination of processes
for reliable, efficient, usable, and ethical deployment and maintenance of
machine learning models in healthcare settings. This paper provides both a
survey of work in this area and guidelines for developers and clinicians to
deploy and maintain their own models in clinical practice. We cover the
foundational concepts of general machine learning operations, describe the
initial setup of MLHOps pipelines (including data sources, preparation,
engineering, and tools). We then describe long-term monitoring and updating
(including data distribution shifts and model updating) and ethical
considerations (including bias, fairness, interpretability, and privacy). This
work therefore provides guidance across the full pipeline of MLHOps from
conception to initial and ongoing deployment.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02394">http://arxiv.org/abs/2305.02394</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02394] Defending against Insertion-based Textual Backdoor Attacks via Attribution](http://arxiv.org/abs/2305.02394) #attack</code></li>
<li>Summary: <p>Textual backdoor attack, as a novel attack model, has been shown to be
effective in adding a backdoor to the model during training. Defending against
such backdoor attacks has become urgent and important. In this paper, we
propose AttDef, an efficient attribution-based pipeline to defend against two
insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard
the tokens with larger attribution scores as potential triggers since larger
attribution words contribute more to the false prediction results and therefore
are more likely to be poison triggers. Additionally, we further utilize an
external pre-trained language model to distinguish whether input is poisoned or
not. We show that our proposed method can generalize sufficiently well in two
common attack scenarios (poisoning training data and testing data), which
consistently improves previous methods. For instance, AttDef can successfully
mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34%
(3.99% up) under pre-training and post-training attack defense respectively,
achieving the new state-of-the-art performance on prediction recovery over four
benchmark datasets.
</p></li>
</ul>

<h3>Title: Backdoor Learning on Sequence to Sequence Models. (arXiv:2305.02424v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02424">http://arxiv.org/abs/2305.02424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02424] Backdoor Learning on Sequence to Sequence Models](http://arxiv.org/abs/2305.02424) #attack</code></li>
<li>Summary: <p>Backdoor learning has become an emerging research area towards building a
trustworthy machine learning system. While a lot of works have studied the
hidden danger of backdoor attacks in image or text classification, there is a
limited understanding of the model's robustness on backdoor attacks when the
output space is infinite and discrete. In this paper, we study a much more
challenging problem of testing whether sequence-to-sequence (seq2seq) models
are vulnerable to backdoor attacks. Specifically, we find by only injecting
0.2\% samples of the dataset, we can cause the seq2seq model to generate the
designated keyword and even the whole sentence. Furthermore, we utilize Byte
Pair Encoding (BPE) to create multiple new triggers, which brings new
challenges to backdoor detection since these backdoors are not static.
Extensive experiments on machine translation and text summarization have been
conducted to show our proposed methods could achieve over 90\% attack success
rate on multiple datasets and models.
</p></li>
</ul>

<h3>Title: Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence. (arXiv:2305.03010v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03010">http://arxiv.org/abs/2305.03010</a></li>
<li>Code URL: <a href="https://github.com/hkust-knowcomp/geia">https://github.com/hkust-knowcomp/geia</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03010] Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence](http://arxiv.org/abs/2305.03010) #attack</code></li>
<li>Summary: <p>Sentence-level representations are beneficial for various natural language
processing tasks. It is commonly believed that vector representations can
capture rich linguistic properties. Currently, large language models (LMs)
achieve state-of-the-art performance on sentence embedding. However, some
recent works suggest that vector representations from LMs can cause information
leakage. In this work, we further investigate the information leakage issue and
propose a generative embedding inversion attack (GEIA) that aims to reconstruct
input sequences based only on their sentence embeddings. Given the black-box
access to a language model, we treat sentence embeddings as initial tokens'
representations and train or fine-tune a powerful decoder model to decode the
whole sequences directly. We conduct extensive experiments to demonstrate that
our generative inversion attack outperforms previous embedding inversion
attacks in classification metrics and generates coherent and contextually
similar sentences as the original inputs.
</p></li>
</ul>

<h3>Title: Can Feature Engineering Help Quantum Machine Learning for Malware Detection?. (arXiv:2305.02396v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02396">http://arxiv.org/abs/2305.02396</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02396] Can Feature Engineering Help Quantum Machine Learning for Malware Detection?](http://arxiv.org/abs/2305.02396) #attack</code></li>
<li>Summary: <p>With the increasing number and sophistication of malware attacks, malware
detection systems based on machine learning (ML) grow in importance. At the
same time, many popular ML models used in malware classification are supervised
solutions. These supervised classifiers often do not generalize well to novel
malware. Therefore, they need to be re-trained frequently to detect new malware
specimens, which can be time-consuming. Our work addresses this problem in a
hybrid framework of theoretical Quantum ML, combined with feature selection
strategies to reduce the data size and malware classifier training time. The
preliminary results show that VQC with XGBoost selected features can get a
78.91% test accuracy on the simulator. The average accuracy for the model
trained using the features selected with XGBoost was 74% (+- 11.35%) on the IBM
5 qubits machines.
</p></li>
</ul>

<h3>Title: Madvex: Instrumentation-based Adversarial Attacks on Machine Learning Malware Detection. (arXiv:2305.02559v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02559">http://arxiv.org/abs/2305.02559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02559] Madvex: Instrumentation-based Adversarial Attacks on Machine Learning Malware Detection](http://arxiv.org/abs/2305.02559) #attack</code></li>
<li>Summary: <p>WebAssembly (Wasm) is a low-level binary format for web applications, which
has found widespread adoption due to its improved performance and compatibility
with existing software. However, the popularity of Wasm has also led to its
exploitation for malicious purposes, such as cryptojacking, where malicious
actors use a victim's computing resources to mine cryptocurrencies without
their consent. To counteract this threat, machine learning-based detection
methods aiming to identify cryptojacking activities within Wasm code have
emerged. It is well-known that neural networks are susceptible to adversarial
attacks, where inputs to a classifier are perturbed with minimal changes that
result in a crass misclassification. While applying changes in image
classification is easy, manipulating binaries in an automated fashion to evade
malware classification without changing functionality is non-trivial. In this
work, we propose a new approach to include adversarial examples in the code
section of binaries via instrumentation. The introduced gadgets allow for the
inclusion of arbitrary bytes, enabling efficient adversarial attacks that
reliably bypass state-of-the-art machine learning classifiers such as the
CNN-based Minos recently proposed at NDSS 2021. We analyze the cost and
reliability of instrumentation-based adversarial example generation and show
that the approach works reliably at minimal size and performance overheads.
</p></li>
</ul>

<h3>Title: Single Node Injection Label Specificity Attack on Graph Neural Networks via Reinforcement Learning. (arXiv:2305.02901v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02901">http://arxiv.org/abs/2305.02901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02901] Single Node Injection Label Specificity Attack on Graph Neural Networks via Reinforcement Learning](http://arxiv.org/abs/2305.02901) #attack</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have achieved remarkable success in various
real-world applications. However, recent studies highlight the vulnerability of
GNNs to malicious perturbations. Previous adversaries primarily focus on graph
modifications or node injections to existing graphs, yielding promising results
but with notable limitations. Graph modification attack~(GMA) requires
manipulation of the original graph, which is often impractical, while graph
injection attack~(GIA) necessitates training a surrogate model in the black-box
setting, leading to significant performance degradation due to divergence
between the surrogate architecture and the actual victim model. Furthermore,
most methods concentrate on a single attack goal and lack a generalizable
adversary to develop distinct attack strategies for diverse goals, thus
limiting precise control over victim model behavior in real-world scenarios. To
address these issues, we present a gradient-free generalizable adversary that
injects a single malicious node to manipulate the classification result of a
target node in the black-box evasion setting. We propose Gradient-free
Generalizable Single Node Injection Attack, namely G$^2$-SNIA, a reinforcement
learning framework employing Proximal Policy Optimization. By directly querying
the victim model, G$^2$-SNIA learns patterns from exploration to achieve
diverse attack goals with extremely limited attack budgets. Through
comprehensive experiments over three acknowledged benchmark datasets and four
prominent GNNs in the most challenging and realistic scenario, we demonstrate
the superior performance of our proposed G$^2$-SNIA over the existing
state-of-the-art baselines. Moreover, by comparing G$^2$-SNIA with multiple
white-box evasion baselines, we confirm its capacity to generate solutions
comparable to those of the best adversaries.
</p></li>
</ul>

<h3>Title: IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02605">http://arxiv.org/abs/2305.02605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02605] IMAP: Intrinsically Motivated Adversarial Policy](http://arxiv.org/abs/2305.02605) #attack</code></li>
<li>Summary: <p>Reinforcement learning (RL) agents are known to be vulnerable to evasion
attacks during deployment. In single-agent environments, attackers can inject
imperceptible perturbations on the policy or value network's inputs or outputs;
in multi-agent environments, attackers can control an adversarial opponent to
indirectly influence the victim's observation. Adversarial policies offer a
promising solution to craft such attacks. Still, current approaches either
require perfect or partial knowledge of the victim policy or suffer from sample
inefficiency due to the sparsity of task-related rewards. To overcome these
limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP)
for efficient black-box evasion attacks in single- and multi-agent environments
without any knowledge of the victim policy. IMAP uses four intrinsic objectives
based on state coverage, policy coverage, risk, and policy divergence to
encourage exploration and discover stronger attacking skills. We also design a
novel Bias-Reduction (BR) method to boost IMAP further. Our experiments
demonstrate the effectiveness of these intrinsic objectives and BR in improving
adversarial policy learning in the black-box setting against multiple types of
victim agents in various single- and multi-agent MuJoCo environments. Notably,
our IMAP reduces the performance of the state-of-the-art robust WocaR-PPO
agents by 34\%-54\% and achieves a SOTA attacking success rate of 83.91\% in
the two-player zero-sum game YouShallNotPass.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Cross-Stream Contrastive Learning for Self-Supervised Skeleton-Based Action Recognition. (arXiv:2305.02324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02324">http://arxiv.org/abs/2305.02324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02324] Cross-Stream Contrastive Learning for Self-Supervised Skeleton-Based Action Recognition](http://arxiv.org/abs/2305.02324) #robust</code></li>
<li>Summary: <p>Self-supervised skeleton-based action recognition enjoys a rapid growth along
with the development of contrastive learning. The existing methods rely on
imposing invariance to augmentations of 3D skeleton within a single data
stream, which merely leverages the easy positive pairs and limits the ability
to explore the complicated movement patterns. In this paper, we advocate that
the defect of single-stream contrast and the lack of necessary feature
transformation are responsible for easy positives, and therefore propose a
Cross-Stream Contrastive Learning framework for skeleton-based action
Representation learning (CSCLR). Specifically, the proposed CSCLR not only
utilizes intra-stream contrast pairs, but introduces inter-stream contrast
pairs as hard samples to formulate a better representation learning. Besides,
to further exploit the potential of positive pairs and increase the robustness
of self-supervised representation learning, we propose a Positive Feature
Transformation (PFT) strategy which adopts feature-level manipulation to
increase the variance of positive pairs. To validate the effectiveness of our
method, we conduct extensive experiments on three benchmark datasets NTU-RGB+D
60, NTU-RGB+D 120 and PKU-MMD. Experimental results show that our proposed
CSCLR exceeds the state-of-the-art methods on a diverse range of evaluation
protocols.
</p></li>
</ul>

<h3>Title: AutoML-GPT: Automatic Machine Learning with GPT. (arXiv:2305.02499v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02499">http://arxiv.org/abs/2305.02499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02499] AutoML-GPT: Automatic Machine Learning with GPT](http://arxiv.org/abs/2305.02499) #robust</code></li>
<li>Summary: <p>AI tasks encompass a wide range of domains and fields. While numerous AI
models have been designed for specific tasks and applications, they often
require considerable human efforts in finding the right model architecture,
optimization algorithm, and hyperparameters. Recent advances in large language
models (LLMs) like ChatGPT show remarkable capabilities in various aspects of
reasoning, comprehension, and interaction. Consequently, we propose developing
task-oriented prompts and automatically utilizing LLMs to automate the training
pipeline. To implement this concept, we present the AutoML-GPT, which employs
GPT as the bridge to diverse AI models and dynamically trains models with
optimized hyperparameters. AutoML-GPT dynamically takes user requests from the
model and data cards and composes the corresponding prompt paragraph.
Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct
the experiments from data processing to model architecture, hyperparameter
tuning, and predicted training log. By leveraging {\ours}'s robust language
capabilities and the available AI models, AutoML-GPT can tackle numerous
intricate AI tasks across various tasks and datasets. This approach achieves
remarkable results in computer vision, natural language processing, and other
challenging areas. Extensive experiments and ablation studies demonstrate that
our method can be general, effective, and beneficial for many AI tasks.
</p></li>
</ul>

<h3>Title: In-situ Anomaly Detection in Additive Manufacturing with Graph Neural Networks. (arXiv:2305.02695v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02695">http://arxiv.org/abs/2305.02695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02695] In-situ Anomaly Detection in Additive Manufacturing with Graph Neural Networks](http://arxiv.org/abs/2305.02695) #robust</code></li>
<li>Summary: <p>Transforming a design into a high-quality product is a challenge in metal
additive manufacturing due to rare events which can cause defects to form.
Detecting these events in-situ could, however, reduce inspection costs, enable
corrective action, and is the first step towards a future of tailored material
properties. In this study a model is trained on laser input information to
predict nominal laser melting conditions. An anomaly score is then calculated
by taking the difference between the predictions and new observations. The
model is evaluated on a dataset with known defects achieving an F1 score of
0.821. This study shows that anomaly detection methods are an important tool in
developing robust defect detection methods.
</p></li>
</ul>

<h3>Title: Incremental 3D Semantic Scene Graph Prediction from RGB Sequences. (arXiv:2305.02743v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02743">http://arxiv.org/abs/2305.02743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02743] Incremental 3D Semantic Scene Graph Prediction from RGB Sequences](http://arxiv.org/abs/2305.02743) #robust</code></li>
<li>Summary: <p>D semantic scene graphs are a powerful holistic representation as they
describe the individual objects and depict the relation between them. They are
compact high-level graphs that enable many tasks requiring scene reasoning. In
real-world settings, existing 3D estimation methods produce robust predictions
that mostly rely on dense inputs. In this work, we propose a real-time
framework that incrementally builds a consistent 3D semantic scene graph of a
scene given an RGB image sequence. Our method consists of a novel incremental
entity estimation pipeline and a scene graph prediction network. The proposed
pipeline simultaneously reconstructs a sparse point map and fuses entity
estimation from the input images. The proposed network estimates 3D semantic
scene graphs with iterative message passing using multi-view and geometric
features extracted from the scene entities. Extensive experiments on the 3RScan
dataset show the effectiveness of the proposed method in this challenging task,
outperforming state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: Forward-Forward Contrastive Learning. (arXiv:2305.02927v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02927">http://arxiv.org/abs/2305.02927</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02927] Forward-Forward Contrastive Learning](http://arxiv.org/abs/2305.02927) #robust</code></li>
<li>Summary: <p>Medical image classification is one of the most important tasks for
computer-aided diagnosis. Deep learning models, particularly convolutional
neural networks, have been successfully used for disease classification from
medical images, facilitated by automated feature learning. However, the diverse
imaging modalities and clinical pathology make it challenging to construct
generalized and robust classifications. Towards improving the model
performance, we propose a novel pretraining approach, namely Forward Forward
Contrastive Learning (FFCL), which leverages the Forward-Forward Algorithm in a
contrastive learning framework--both locally and globally. Our experimental
results on the chest X-ray dataset indicate that the proposed FFCL achieves
superior performance (3.69% accuracy over ImageNet pretrained ResNet-18) over
existing pretraining models in the pneumonia classification task. Moreover,
extensive ablation experiments support the particular local and global
contrastive pretraining design in FFCL.
</p></li>
</ul>

<h3>Title: Single-Shot Implicit Morphable Faces with Consistent Texture Parameterization. (arXiv:2305.03043v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03043">http://arxiv.org/abs/2305.03043</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03043] Single-Shot Implicit Morphable Faces with Consistent Texture Parameterization](http://arxiv.org/abs/2305.03043) #robust</code></li>
<li>Summary: <p>There is a growing demand for the accessible creation of high-quality 3D
avatars that are animatable and customizable. Although 3D morphable models
provide intuitive control for editing and animation, and robustness for
single-view face reconstruction, they cannot easily capture geometric and
appearance details. Methods based on neural implicit representations, such as
signed distance functions (SDF) or neural radiance fields, approach
photo-realism, but are difficult to animate and do not generalize well to
unseen data. To tackle this problem, we propose a novel method for constructing
implicit 3D morphable face models that are both generalizable and intuitive for
editing. Trained from a collection of high-quality 3D scans, our face model is
parameterized by geometry, expression, and texture latent codes with a learned
SDF and explicit UV texture parameterization. Once trained, we can reconstruct
an avatar from a single in-the-wild image by leveraging the learned prior to
project the image into the latent space of our model. Our implicit morphable
face models can be used to render an avatar from novel views, animate facial
expressions by modifying expression codes, and edit textures by directly
painting on the learned UV-texture maps. We demonstrate quantitatively and
qualitatively that our method improves upon photo-realism, geometry, and
expression accuracy compared to state-of-the-art methods.
</p></li>
</ul>

<h3>Title: PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives. (arXiv:2305.02364v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02364">http://arxiv.org/abs/2305.02364</a></li>
<li>Code URL: <a href="https://github.com/silin159/peacok">https://github.com/silin159/peacok</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02364] PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives](http://arxiv.org/abs/2305.02364) #robust</code></li>
<li>Summary: <p>Sustaining coherent and engaging narratives requires dialogue or storytelling
agents to understand how the personas of speakers or listeners ground the
narrative. Specifically, these agents must infer personas of their listeners to
produce statements that cater to their interests. They must also learn to
maintain consistent speaker personas for themselves throughout the narrative,
so that their counterparts feel involved in a realistic conversation or story.
</p></li>
</ul>

<p>However, personas are diverse and complex: they entail large quantities of
rich interconnected world knowledge that is challenging to robustly represent
in general narrative systems (e.g., a singer is good at singing, and may have
attended conservatoire). In this work, we construct a new large-scale persona
commonsense knowledge graph, PeaCoK, containing ~100K human-validated persona
facts. Our knowledge graph schematizes five dimensions of persona knowledge
identified in previous studies of human interactive behaviours, and distils
facts in this schema from both existing commonsense knowledge graphs and
large-scale pretrained language models. Our analysis indicates that PeaCoK
contains rich and precise world persona inferences that help downstream systems
generate more consistent and engaging narratives.
</p>

<h3>Title: Quantifying the Dissimilarity of Texts. (arXiv:2305.02457v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02457">http://arxiv.org/abs/2305.02457</a></li>
<li>Code URL: <a href="https://github.com/benjaminshade/quantifying-dissimilarity">https://github.com/benjaminshade/quantifying-dissimilarity</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02457] Quantifying the Dissimilarity of Texts](http://arxiv.org/abs/2305.02457) #robust</code></li>
<li>Summary: <p>Quantifying the dissimilarity of two texts is an important aspect of a number
of natural language processing tasks, including semantic information retrieval,
topic classification, and document clustering. In this paper, we compared the
properties and performance of different dissimilarity measures $D$ using three
different representations of texts -- vocabularies, word frequency
distributions, and vector embeddings -- and three simple tasks -- clustering
texts by author, subject, and time period. Using the Project Gutenberg
database, we found that the generalised Jensen--Shannon divergence applied to
word frequencies performed strongly across all tasks, that $D$'s based on
vector embedding representations led to stronger performance for smaller texts,
and that the optimal choice of approach was ultimately task-dependent. We also
investigated, both analytically and numerically, the behaviour of the different
$D$'s when the two texts varied in length by a factor $h$. We demonstrated that
the (natural) estimator of the Jaccard distance between vocabularies was
inconsistent and computed explicitly the $h$-dependency of the bias of the
estimator of the generalised Jensen--Shannon divergence applied to word
frequencies. We also found numerically that the Jensen--Shannon divergence and
embedding-based approaches were robust to changes in $h$, while the Jaccard
distance was not.
</p></li>
</ul>

<h3>Title: Analyzing Hong Kong's Legal Judgments from a Computational Linguistics point-of-view. (arXiv:2305.02558v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02558">http://arxiv.org/abs/2305.02558</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02558] Analyzing Hong Kong's Legal Judgments from a Computational Linguistics point-of-view](http://arxiv.org/abs/2305.02558) #robust</code></li>
<li>Summary: <p>Analysis and extraction of useful information from legal judgments using
computational linguistics was one of the earliest problems posed in the domain
of information retrieval. Presently, several commercial vendors exist who
automate such tasks. However, a crucial bottleneck arises in the form of
exorbitant pricing and lack of resources available in analysis of judgements
mete out by Hong Kong's Legal System. This paper attempts to bridge this gap by
providing several statistical, machine learning, deep learning and zero-shot
learning based methods to effectively analyze legal judgments from Hong Kong's
Court System. The methods proposed consists of: (1) Citation Network Graph
Generation, (2) PageRank Algorithm, (3) Keyword Analysis and Summarization, (4)
Sentiment Polarity, and (5) Paragrah Classification, in order to be able to
extract key insights from individual as well a group of judgments together.
This would make the overall analysis of judgments in Hong Kong less tedious and
more automated in order to extract insights quickly using fast inferencing. We
also provide an analysis of our results by benchmarking our results using Large
Language Models making robust use of the HuggingFace ecosystem.
</p></li>
</ul>

<h3>Title: BranchNorm: Robustly Scaling Extremely Deep Transformers. (arXiv:2305.02790v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02790">http://arxiv.org/abs/2305.02790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02790] BranchNorm: Robustly Scaling Extremely Deep Transformers](http://arxiv.org/abs/2305.02790) #robust</code></li>
<li>Summary: <p>Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000
layers) and reveals the promising potential of deep scaling. To stabilize the
training of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the
model update to a constant value. Although applying such a constraint can
benefit the early stage of model training, it may lead to undertrained models
during the whole training procedure. In this paper, we propose BranchNorm,
which dynamically rescales the non-residual branch of Transformer in accordance
with the training period. BranchNorm not only theoretically stabilizes the
training with smooth gradient norms at the early stage, but also encourages
better convergence in the subsequent training stage. Experiment results on
multiple translation tasks demonstrate that BranchNorm achieves a better
trade-off between training stability and converge performance.
</p></li>
</ul>

<h3>Title: Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation. (arXiv:2305.02820v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02820">http://arxiv.org/abs/2305.02820</a></li>
<li>Code URL: <a href="https://github.com/blmoistawinde/dasc">https://github.com/blmoistawinde/dasc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02820] Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation](http://arxiv.org/abs/2305.02820) #robust</code></li>
<li>Summary: <p>Controlling chatbot utterance generation with multiple attributes such as
personalities, emotions and dialogue acts is a practically useful but
under-studied problem. We propose a novel controllable generation framework
called DASC that possesses strong controllability with weighted decoding
paradigm, while improving generation quality with the grounding in an attribute
semantics space. Generation with multiple attributes is then intuitively
implemented with an interpolation of multiple attribute embeddings. Experiments
show that DASC can achieve state-of-the-art control accuracy in 3-aspect
controllable generation tasks while also producing interesting and reasonably
sensible responses, even if in an out-of-distribution robustness test.
Visualization of the meaningful representations learned in the attribute
semantic space also supports its effectiveness.
</p></li>
</ul>

<h3>Title: ReMask: A Robust Information-Masking Approach for Domain Counterfactual Generation. (arXiv:2305.02858v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02858">http://arxiv.org/abs/2305.02858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02858] ReMask: A Robust Information-Masking Approach for Domain Counterfactual Generation](http://arxiv.org/abs/2305.02858) #robust</code></li>
<li>Summary: <p>Domain shift is a big challenge in NLP, thus, many approaches resort to
learning domain-invariant features to mitigate the inference phase domain
shift. Such methods, however, fail to leverage the domain-specific nuances
relevant to the task at hand. To avoid such drawbacks, domain counterfactual
generation aims to transform a text from the source domain to a given target
domain. However, due to the limited availability of data, such frequency-based
methods often miss and lead to some valid and spurious domain-token
associations. Hence, we employ a three-step domain obfuscation approach that
involves frequency and attention norm-based masking, to mask domain-specific
cues, and unmasking to regain the domain generic context. Our experiments
empirically show that the counterfactual samples sourced from our masked text
lead to improved domain transfer on 10 out of 12 domain sentiment
classification settings, with an average of 2% accuracy improvement over the
state-of-the-art for unsupervised domain adaptation (UDA). Further, our model
outperforms the state-of-the-art by achieving 1.4% average accuracy improvement
in the adversarial domain adaptation (ADA) setting. Moreover, our model also
shows its domain adaptation efficacy on a large multi-domain intent
classification dataset where it attains state-of-the-art results. We release
the codes publicly at \url{https://github.com/declare-lab/remask}.
</p></li>
</ul>

<h3>Title: An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02897">http://arxiv.org/abs/2305.02897</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02897] An automatically discovered chain-of-thought prompt generalizes to novel models and datasets](http://arxiv.org/abs/2305.02897) #robust</code></li>
<li>Summary: <p>Emergent chain-of-thought (CoT) reasoning capabilities promise to improve
performance and explainability of large language models (LLMs). However,
uncertainties remain about how prompting strategies formulated for previous
model generations generalize to new model generations and different datasets.
In this small-scale study we compare the performance of a range of zero-shot
prompts for inducing CoT reasoning across six recently released LLMs
(davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere
command-xlarge) on a mixture of six question-answering datasets, including
datasets from scientific and medical domains. We find that a CoT prompt that
was previously discovered through automated prompt discovery shows robust
performance across experimental conditions and produces best results when
applied to the state-of-the-art model GPT-4.
</p></li>
</ul>

<h3>Title: Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA. (arXiv:2305.02544v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02544">http://arxiv.org/abs/2305.02544</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02544] Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA](http://arxiv.org/abs/2305.02544) #robust</code></li>
<li>Summary: <p>We study principal component analysis (PCA), where given a dataset in
$\mathbb{R}^d$ from a distribution, the task is to find a unit vector $v$ that
approximately maximizes the variance of the distribution after being projected
along $v$. Despite being a classical task, standard estimators fail drastically
if the data contains even a small fraction of outliers, motivating the problem
of robust PCA. Recent work has developed computationally-efficient algorithms
for robust PCA that either take super-linear time or have sub-optimal error
guarantees. Our main contribution is to develop a nearly-linear time algorithm
for robust PCA with near-optimal error guarantees. We also develop a
single-pass streaming algorithm for robust PCA with memory usage nearly-linear
in the dimension.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: A Cross-direction Task Decoupling Network for Small Logo Detection. (arXiv:2305.02503v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02503">http://arxiv.org/abs/2305.02503</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02503] A Cross-direction Task Decoupling Network for Small Logo Detection](http://arxiv.org/abs/2305.02503) #extraction</code></li>
<li>Summary: <p>Logo detection plays an integral role in many applications. However, handling
small logos is still difficult since they occupy too few pixels in the image,
which burdens the extraction of discriminative features. The aggregation of
small logos also brings a great challenge to the classification and
localization of logos. To solve these problems, we creatively propose
Cross-direction Task Decoupling Network (CTDNet) for small logo detection. We
first introduce Cross-direction Feature Pyramid (CFP) to realize
cross-direction feature fusion by adopting horizontal transmission and vertical
transmission. In addition, Multi-frequency Task Decoupling Head (MTDH)
decouples the classification and localization tasks into two branches. A multi
frequency attention convolution branch is designed to achieve more accurate
regression by combining discrete cosine transform and convolution creatively.
Comprehensive experiments on four logo datasets demonstrate the effectiveness
and efficiency of the proposed method.
</p></li>
</ul>

<h3>Title: FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02549">http://arxiv.org/abs/2305.02549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02549] FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction](http://arxiv.org/abs/2305.02549) #extraction</code></li>
<li>Summary: <p>The recent advent of self-supervised pre-training techniques has led to a
surge in the use of multimodal learning in form document understanding.
However, existing approaches that extend the mask language modeling to other
modalities require careful multi-task tuning, complex reconstruction target
designs, or additional pre-training data. In FormNetV2, we introduce a
centralized multimodal graph contrastive learning strategy to unify
self-supervised pre-training for all modalities in one loss. The graph
contrastive objective maximizes the agreement of multimodal representations,
providing a natural interplay for all modalities without special customization.
In addition, we extract image features within the bounding box that joins a
pair of tokens connected by a graph edge, capturing more targeted visual cues
without loading a sophisticated and separately pre-trained image embedder.
FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE
and Payment benchmarks with a more compact model size.
</p></li>
</ul>

<h3>Title: APR: Online Distant Point Cloud Registration Through Aggregated Point Cloud Reconstruction. (arXiv:2305.02893v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02893">http://arxiv.org/abs/2305.02893</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02893] APR: Online Distant Point Cloud Registration Through Aggregated Point Cloud Reconstruction](http://arxiv.org/abs/2305.02893) #extraction</code></li>
<li>Summary: <p>For many driving safety applications, it is of great importance to accurately
register LiDAR point clouds generated on distant moving vehicles. However, such
point clouds have extremely different point density and sensor perspective on
the same object, making registration on such point clouds very hard. In this
paper, we propose a novel feature extraction framework, called APR, for online
distant point cloud registration. Specifically, APR leverages an autoencoder
design, where the autoencoder reconstructs a denser aggregated point cloud with
several frames instead of the original single input point cloud. Our design
forces the encoder to extract features with rich local geometry information
based on one single input point cloud. Such features are then used for online
distant point cloud registration. We conduct extensive experiments against
state-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets.
Results show that APR outperforms all other extractors by a large margin,
increasing average registration recall of SOTA extractors by 7.1% on LoKITTI
and 4.6% on LoNuScenes.
</p></li>
</ul>

<h3>Title: Additive Class Distinction Maps using Branched-GANs. (arXiv:2305.02899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02899">http://arxiv.org/abs/2305.02899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02899] Additive Class Distinction Maps using Branched-GANs](http://arxiv.org/abs/2305.02899) #extraction</code></li>
<li>Summary: <p>We present a new model, training procedure and architecture to create precise
maps of distinction between two classes of images. The objective is to
comprehend, in pixel-wise resolution, the unique characteristics of a class.
These maps can facilitate self-supervised segmentation and objectdetection in
addition to new capabilities in explainable AI (XAI). Our proposed architecture
is based on image decomposition, where the output is the sum of multiple
generative networks (branched-GANs). The distinction between classes is
isolated in a dedicated branch. This approach allows clear, precise and
interpretable visualization of the unique characteristics of each class. We
show how our generic method can be used in several modalities for various
tasks, such as MRI brain tumor extraction, isolating cars in aerial photography
and obtaining feminine and masculine face features. This is a preliminary
report of our initial findings and results.
</p></li>
</ul>

<h3>Title: Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach. (arXiv:2305.02615v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02615">http://arxiv.org/abs/2305.02615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02615] Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach](http://arxiv.org/abs/2305.02615) #extraction</code></li>
<li>Summary: <p>The affective reasoning task is a set of emerging affect-based tasks in
conversation, including Emotion Recognition in Conversation (ERC),Emotion-Cause
Pair Extraction (ECPE), and Emotion-Cause Span Recognition (ECSR). Existing
methods make various assumptions on the apparent relationship while neglecting
the essential causal model due to the nonuniqueness of skeletons and
unobservability of implicit causes. This paper settled down the above two
problems and further proposed Conversational Affective Causal Discovery (CACD).
It is a novel causal discovery method showing how to discover causal
relationships in a conversation via designing a common skeleton and generating
a substitute for implicit causes. CACD contains two steps: (i) building a
common centering one graph node causal skeleton for all utterances in
variable-length conversations; (ii) Causal Auto-Encoder (CAE) correcting the
skeleton to yield causal representation through generated implicit causes and
known explicit causes. Comprehensive experiments demonstrate that our novel
method significantly outperforms the SOTA baselines in six affect-related
datasets on the three tasks.
</p></li>
</ul>

<h3>Title: Multi-Domain Learning From Insufficient Annotations. (arXiv:2305.02757v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02757">http://arxiv.org/abs/2305.02757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02757] Multi-Domain Learning From Insufficient Annotations](http://arxiv.org/abs/2305.02757) #extraction</code></li>
<li>Summary: <p>Multi-domain learning (MDL) refers to simultaneously constructing a model or
a set of models on datasets collected from different domains. Conventional
approaches emphasize domain-shared information extraction and domain-private
information preservation, following the shared-private framework (SP models),
which offers significant advantages over single-domain learning. However, the
limited availability of annotated data in each domain considerably hinders the
effectiveness of conventional supervised MDL approaches in real-world
applications. In this paper, we introduce a novel method called multi-domain
contrastive learning (MDCL) to alleviate the impact of insufficient annotations
by capturing both semantic and structural information from both labeled and
unlabeled data.Specifically, MDCL comprises two modules: inter-domain semantic
alignment and intra-domain contrast. The former aims to align annotated
instances of the same semantic category from distinct domains within a shared
hidden space, while the latter focuses on learning a cluster structure of
unlabeled instances in a private hidden space for each domain. MDCL is readily
compatible with many SP models, requiring no additional model parameters and
allowing for end-to-end training. Experimental results across five textual and
image multi-domain datasets demonstrate that MDCL brings noticeable improvement
over various SP models.Furthermore, MDCL can further be employed in
multi-domain active learning (MDAL) to achieve a superior initialization,
eventually leading to better overall performance.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Can Fair Federated Learning reduce the need for Personalisation?. (arXiv:2305.02728v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02728">http://arxiv.org/abs/2305.02728</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02728] Can Fair Federated Learning reduce the need for Personalisation?](http://arxiv.org/abs/2305.02728) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) enables training ML models on edge clients without
sharing data. However, the federated model's performance on local data varies,
disincentivising the participation of clients who benefit little from FL. Fair
FL reduces accuracy disparity by focusing on clients with higher losses while
personalisation locally fine-tunes the model. Personalisation provides a
participation incentive when an FL model underperforms relative to one trained
locally. For situations where the federated model provides a lower accuracy
than a model trained entirely locally by a client, personalisation improves the
accuracy of the pre-trained federated weights to be similar to or exceed those
of the local client model. This paper evaluates two Fair FL (FFL) algorithms as
starting points for personalisation. Our results show that FFL provides no
benefit to relative performance in a language task and may double the number of
underperforming clients for an image task. Instead, we propose
Personalisation-aware Federated Learning (PaFL) as a paradigm that
pre-emptively uses personalisation losses during training. Our technique shows
a 50% reduction in the number of underperforming clients for the language task
while lowering the number of underperforming clients in the image task instead
of doubling it. Thus, evidence indicates that it may allow a broader set of
devices to benefit from FL and represents a promising avenue for future
experimentation and theoretical analysis.
</p></li>
</ul>

<h3>Title: Efficient Personalized Federated Learning via Sparse Model-Adaptation. (arXiv:2305.02776v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02776">http://arxiv.org/abs/2305.02776</a></li>
<li>Code URL: <a href="https://github.com/yxdyc/pfedgate">https://github.com/yxdyc/pfedgate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02776] Efficient Personalized Federated Learning via Sparse Model-Adaptation](http://arxiv.org/abs/2305.02776) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) aims to train machine learning models for multiple
clients without sharing their own private data. Due to the heterogeneity of
clients' local data distribution, recent studies explore the personalized FL
that learns and deploys distinct local models with the help of auxiliary global
models. However, the clients can be heterogeneous in terms of not only local
data distribution, but also their computation and communication resources. The
capacity and efficiency of personalized models are restricted by the
lowest-resource clients, leading to sub-optimal performance and limited
practicality of personalized FL. To overcome these challenges, we propose a
novel approach named pFedGate for efficient personalized FL by adaptively and
efficiently learning sparse local models. With a lightweight trainable gating
layer, pFedGate enables clients to reach their full potential in model capacity
by generating different sparse models accounting for both the heterogeneous
data distributions and resource constraints. Meanwhile, the computation and
communication efficiency are both improved thanks to the adaptability between
the model sparsity and clients' resources. Further, we theoretically show that
the proposed pFedGate has superior complexity with guaranteed convergence and
generalization error. Extensive experiments show that pFedGate achieves
superior global accuracy, individual accuracy and efficiency simultaneously
over state-of-the-art methods. We also demonstrate that pFedGate performs
better than competitors in the novel clients participation and partial clients
participation scenarios, and can learn meaningful sparse local models adapted
to different data distributions.
</p></li>
</ul>

<h3>Title: FedCBO: Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization. (arXiv:2305.02894v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02894">http://arxiv.org/abs/2305.02894</a></li>
<li>Code URL: <a href="https://github.com/sixuli/fedcbo">https://github.com/sixuli/fedcbo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02894] FedCBO: Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization](http://arxiv.org/abs/2305.02894) #federate</code></li>
<li>Summary: <p>Federated learning is an important framework in modern machine learning that
seeks to integrate the training of learning models from multiple users, each
user having their own local data set, in a way that is sensitive to data
privacy and to communication loss constraints. In clustered federated learning,
one assumes an additional unknown group structure among users, and the goal is
to train models that are useful for each group, rather than simply training a
single global model for all users. In this paper, we propose a novel solution
to the problem of clustered federated learning that is inspired by ideas in
consensus-based optimization (CBO). Our new CBO-type method is based on a
system of interacting particles that is oblivious to group memberships. Our
model is motivated by rigorous mathematical reasoning, including a mean field
analysis describing the large number of particles limit of our particle system,
as well as convergence guarantees for the simultaneous global optimization of
general non-convex objective functions (corresponding to the loss functions of
each cluster of users) in the mean-field regime. Experimental results
demonstrate the efficacy of our FedCBO algorithm compared to other
state-of-the-art methods and help validate our methodological and theoretical
work.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02995">http://arxiv.org/abs/2305.02995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02995] On the nonlinear correlation of ML performance between data subpopulations](http://arxiv.org/abs/2305.02995) #fair</code></li>
<li>Summary: <p>Understanding the performance of machine learning (ML) models across diverse
data distributions is critically important for reliable applications. Despite
recent empirical studies positing a near-perfect linear correlation between
in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically
demonstrate that this correlation is more nuanced under subpopulation shifts.
Through rigorous experimentation and analysis across a variety of datasets,
models, and training epochs, we demonstrate that OOD performance often has a
nonlinear correlation with ID performance in subpopulation shifts. Our
findings, which contrast previous studies that have posited a linear
correlation in model performance during distribution shifts, reveal a "moon
shape" correlation (parabolic uptrend curve) between the test performance on
the majority subpopulation and the minority subpopulation. This non-trivial
nonlinear correlation holds across model architectures, hyperparameters,
training durations, and the imbalance between subpopulations. Furthermore, we
found that the nonlinearity of this "moon shape" is causally influenced by the
degree of spurious correlations in the training data. Our controlled
experiments show that stronger spurious correlation in the training data
creates more nonlinear performance correlation. We provide complementary
experimental and theoretical analyses for this phenomenon, and discuss its
implications for ML reliability and fairness. Our work highlights the
importance of understanding the nonlinear effects of model improvement on
performance in different subpopulations, and has the potential to inform the
development of more equitable and responsible machine learning models.
</p></li>
</ul>

<h3>Title: Integrating Psychometrics and Computing Perspectives on Bias and Fairness in Affective Computing: A Case Study of Automated Video Interviews. (arXiv:2305.02629v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02629">http://arxiv.org/abs/2305.02629</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02629] Integrating Psychometrics and Computing Perspectives on Bias and Fairness in Affective Computing: A Case Study of Automated Video Interviews](http://arxiv.org/abs/2305.02629) #fair</code></li>
<li>Summary: <p>We provide a psychometric-grounded exposition of bias and fairness as applied
to a typical machine learning pipeline for affective computing. We expand on an
interpersonal communication framework to elucidate how to identify sources of
bias that may arise in the process of inferring human emotions and other
psychological constructs from observed behavior. Various methods and metrics
for measuring fairness and bias are discussed along with pertinent implications
within the United States legal context. We illustrate how to measure some types
of bias and fairness in a case study involving automatic personality and
hireability inference from multimodal data collected in video interviews for
mock job applications. We encourage affective computing researchers and
practitioners to encapsulate bias and fairness in their research processes and
products and to consider their role, agency, and responsibility in promoting
equitable and just systems.
</p></li>
</ul>

<h3>Title: Maximizing Submodular Functions for Recommendation in the Presence of Biases. (arXiv:2305.02806v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02806">http://arxiv.org/abs/2305.02806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02806] Maximizing Submodular Functions for Recommendation in the Presence of Biases](http://arxiv.org/abs/2305.02806) #fair</code></li>
<li>Summary: <p>Subset selection tasks, arise in recommendation systems and search engines
and ask to select a subset of items that maximize the value for the user. The
values of subsets often display diminishing returns, and hence, submodular
functions have been used to model them. If the inputs defining the submodular
function are known, then existing algorithms can be used. In many applications,
however, inputs have been observed to have social biases that reduce the
utility of the output subset. Hence, interventions to improve the utility are
desired. Prior works focus on maximizing linear functions -- a special case of
submodular functions -- and show that fairness constraint-based interventions
can not only ensure proportional representation but also achieve near-optimal
utility in the presence of biases. We study the maximization of a family of
submodular functions that capture functions arising in the aforementioned
applications. Our first result is that, unlike linear functions,
constraint-based interventions cannot guarantee any constant fraction of the
optimal utility for this family of submodular functions. Our second result is
an algorithm for submodular maximization. The algorithm provably outputs
subsets that have near-optimal utility for this family under mild assumptions
and that proportionally represent items from each group. In empirical
evaluation, with both synthetic and real-world data, we observe that this
algorithm improves the utility of the output subset for this family of
submodular functions over baselines.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Fashionpedia-Ads: Do Your Favorite Advertisements Reveal Your Fashion Taste?. (arXiv:2305.02360v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02360">http://arxiv.org/abs/2305.02360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02360] Fashionpedia-Ads: Do Your Favorite Advertisements Reveal Your Fashion Taste?](http://arxiv.org/abs/2305.02360) #interpretability</code></li>
<li>Summary: <p>Consumers are exposed to advertisements across many different domains on the
internet, such as fashion, beauty, car, food, and others. On the other hand,
fashion represents second highest e-commerce shopping category. Does consumer
digital record behavior on various fashion ad images reveal their fashion
taste? Does ads from other domains infer their fashion taste as well? In this
paper, we study the correlation between advertisements and fashion taste.
Towards this goal, we introduce a new dataset, Fashionpedia-Ads, which asks
subjects to provide their preferences on both ad (fashion, beauty, car, and
dessert) and fashion product (social network and e-commerce style) images.
Furthermore, we exhaustively collect and annotate the emotional, visual and
textual information on the ad images from multi-perspectives (abstractive
level, physical level, captions, and brands). We open-source Fashionpedia-Ads
to enable future studies and encourage more approaches to interpretability
research between advertisements and fashion taste.
</p></li>
</ul>

<h3>Title: Evaluating Post-hoc Interpretability with Intrinsic Interpretability. (arXiv:2305.03002v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03002">http://arxiv.org/abs/2305.03002</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03002] Evaluating Post-hoc Interpretability with Intrinsic Interpretability](http://arxiv.org/abs/2305.03002) #interpretability</code></li>
<li>Summary: <p>Despite Convolutional Neural Networks having reached human-level performance
in some medical tasks, their clinical use has been hindered by their lack of
interpretability. Two major interpretability strategies have been proposed to
tackle this problem: post-hoc methods and intrinsic methods. Although there are
several post-hoc methods to interpret DL models, there is significant variation
between the explanations provided by each method, and it a difficult to
validate them due to the lack of ground-truth. To address this challenge, we
adapted the intrinsical interpretable ProtoPNet for the context of
histopathology imaging and compared the attribution maps produced by it and the
saliency maps made by post-hoc methods. To evaluate the similarity between
saliency map methods and attribution maps we adapted 10 saliency metrics from
the saliency model literature, and used the breast cancer metastases detection
dataset PatchCamelyon with 327,680 patches of histopathological images of
sentinel lymph node sections to validate the proposed approach. Overall,
SmoothGrad and Occlusion were found to have a statistically bigger overlap with
ProtoPNet while Deconvolution and Lime have been found to have the least.
</p></li>
</ul>

<h3>Title: Interpretable Sentence Representation with Variational Autoencoders and Attention. (arXiv:2305.02810v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02810">http://arxiv.org/abs/2305.02810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02810] Interpretable Sentence Representation with Variational Autoencoders and Attention](http://arxiv.org/abs/2305.02810) #interpretability</code></li>
<li>Summary: <p>In this thesis, we develop methods to enhance the interpretability of recent
representation learning techniques in natural language processing (NLP) while
accounting for the unavailability of annotated data. We choose to leverage
Variational Autoencoders (VAEs) due to their efficiency in relating
observations to latent generative factors and their effectiveness in
data-efficient learning and interpretable representation learning. As a first
contribution, we identify and remove unnecessary components in the functioning
scheme of semi-supervised VAEs making them faster, smaller and easier to
design. Our second and main contribution is to use VAEs and Transformers to
build two models with inductive bias to separate information in latent
representations into understandable concepts without annotated data. The first
model, Attention-Driven VAE (ADVAE), is able to separately represent and
control information about syntactic roles in sentences. The second model,
QKVAE, uses separate latent variables to form keys and values for its
Transformer decoder and is able to separate syntactic and semantic information
in its neural representations. In transfer experiments, QKVAE has competitive
performance compared to supervised models and equivalent performance to a
supervised model using 50K annotated samples. Additionally, QKVAE displays
improved syntactic role disentanglement capabilities compared to ADVAE.
Overall, we demonstrate that it is possible to enhance the interpretability of
state-of-the-art deep learning architectures for language modeling with
unannotated data in situations where text data is abundant but annotations are
scarce.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: A new method using deep learning to predict the response to cardiac resynchronization therapy. (arXiv:2305.02475v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02475">http://arxiv.org/abs/2305.02475</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02475] A new method using deep learning to predict the response to cardiac resynchronization therapy](http://arxiv.org/abs/2305.02475) #explainability</code></li>
<li>Summary: <p>Background. Clinical parameters measured from gated single-photon emission
computed tomography myocardial perfusion imaging (SPECT MPI) have value in
predicting cardiac resynchronization therapy (CRT) patient outcomes, but still
show limitations. The purpose of this study is to combine clinical variables,
features from electrocardiogram (ECG), and parameters from assessment of
cardiac function with polarmaps from gated SPECT MPI through deep learning (DL)
to predict CRT response. Methods. 218 patients who underwent rest gated SPECT
MPI were enrolled in this study. CRT response was defined as an increase in
left ventricular ejection fraction (LVEF) > 5% at a 6-month follow up. A DL
model was constructed by combining a pre-trained VGG16 module and a multilayer
perceptron. Two modalities of data were input to the model: polarmap images
from SPECT MPI and tabular data from clinical features and ECG parameters.
Gradient-weighted Class Activation Mapping (Grad-CAM) was applied to the VGG16
module to provide explainability for the polarmaps. For comparison, four
machine learning (ML) models were trained using only the tabular features.
Results. Modeling was performed on 218 patients who underwent CRT implantation
with a response rate of 55.5% (n = 121). The DL model demonstrated average AUC
(0.83), accuracy (0.73), sensitivity (0.76), and specificity (0.69) surpassing
the ML models and guideline criteria. Guideline recommendations presented
accuracy (0.53), sensitivity (0.75), and specificity (0.26). Conclusions. The
DL model outperformed the ML models, showcasing the additional predictive
benefit of utilizing SPECT MPI polarmaps. Incorporating additional patient data
directly in the form of medical imagery can improve CRT response prediction.
</p></li>
</ul>

<h3>Title: Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02749">http://arxiv.org/abs/2305.02749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02749] Explainable Reinforcement Learning via a Causal World Model](http://arxiv.org/abs/2305.02749) #explainability</code></li>
<li>Summary: <p>Generating explanations for reinforcement learning (RL) is challenging as
actions may produce long-term effects on the future. In this paper, we develop
a novel framework for explainable RL by learning a causal world model without
prior knowledge of the causal structure of the environment. The model captures
the influence of actions, allowing us to interpret the long-term effects of
actions through causal chains, which present how actions influence
environmental variables and finally lead to rewards. Different from most
explanatory models which suffer from low accuracy, our model remains accurate
while improving explainability, making it applicable in model-based learning.
As a result, we demonstrate that our causal model can serve as the bridge
between explainability and learning.
</p></li>
</ul>

<h2>watermark</h2>
<h3>Title: ItoV: Efficiently Adapting Deep Learning-based Image Watermarking to Video Watermarking. (arXiv:2305.02781v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02781">http://arxiv.org/abs/2305.02781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02781] ItoV: Efficiently Adapting Deep Learning-based Image Watermarking to Video Watermarking](http://arxiv.org/abs/2305.02781) #watermark</code></li>
<li>Summary: <p>Robust watermarking tries to conceal information within a cover image/video
imperceptibly that is resistant to various distortions. Recently, deep
learning-based approaches for image watermarking have made significant
advancements in robustness and invisibility. However, few studies focused on
video watermarking using deep neural networks due to the high complexity and
computational costs. Our paper aims to answer this research question: Can
well-designed deep learning-based image watermarking be efficiently adapted to
video watermarking? Our answer is positive. First, we revisit the workflow of
deep learning-based watermarking methods that leads to a critical insight:
temporal information in the video may be essential for general computer vision
tasks but not for specific video watermarking. Inspired by this insight, we
propose a method named ItoV for efficiently adapting deep learning-based Image
watermarking to Video watermarking. Specifically, ItoV merges the temporal
dimension of the video with the channel dimension to enable deep neural
networks to treat videos as images. We further explore the effects of different
convolutional blocks in video watermarking. We find that spatial convolution is
the primary influential component in video watermarking and depthwise
convolutions significantly reduce computational cost with negligible impact on
performance. In addition, we propose a new frame loss to constrain that the
watermark intensity in each video clip frame is consistent, significantly
improving the invisibility. Extensive experiments show the superior performance
of the adapted video watermarking method compared with the state-of-the-art
methods on Kinetics-600 and Inter4K datasets, which demonstrate the efficacy of
our method ItoV.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Shap-E: Generating Conditional 3D Implicit Functions. (arXiv:2305.02463v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02463">http://arxiv.org/abs/2305.02463</a></li>
<li>Code URL: <a href="https://github.com/openai/shap-e">https://github.com/openai/shap-e</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02463] Shap-E: Generating Conditional 3D Implicit Functions](http://arxiv.org/abs/2305.02463) #diffusion</code></li>
<li>Summary: <p>We present Shap-E, a conditional generative model for 3D assets. Unlike
recent work on 3D generative models which produce a single output
representation, Shap-E directly generates the parameters of implicit functions
that can be rendered as both textured meshes and neural radiance fields. We
train Shap-E in two stages: first, we train an encoder that deterministically
maps 3D assets into the parameters of an implicit function; second, we train a
conditional diffusion model on outputs of the encoder. When trained on a large
dataset of paired 3D and text data, our resulting models are capable of
generating complex and diverse 3D assets in a matter of seconds. When compared
to Point-E, an explicit generative model over point clouds, Shap-E converges
faster and reaches comparable or better sample quality despite modeling a
higher-dimensional, multi-representation output space. We release model
weights, inference code, and samples at https://github.com/openai/shap-e.
</p></li>
</ul>

<h3>Title: LayoutDM: Transformer-based Diffusion Model for Layout Generation. (arXiv:2305.02567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02567">http://arxiv.org/abs/2305.02567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02567] LayoutDM: Transformer-based Diffusion Model for Layout Generation](http://arxiv.org/abs/2305.02567) #diffusion</code></li>
<li>Summary: <p>Automatic layout generation that can synthesize high-quality layouts is an
important tool for graphic design in many applications. Though existing methods
based on generative models such as Generative Adversarial Networks (GANs) and
Variational Auto-Encoders (VAEs) have progressed, they still leave much room
for improving the quality and diversity of the results. Inspired by the recent
success of diffusion models in generating high-quality images, this paper
explores their potential for conditional layout generation and proposes
Transformer-based Layout Diffusion Model (LayoutDM) by instantiating the
conditional denoising diffusion probabilistic model (DDPM) with a purely
transformer-based architecture. Instead of using convolutional neural networks,
a transformer-based conditional Layout Denoiser is proposed to learn the
reverse diffusion process to generate samples from noised layout data.
Benefitting from both transformer and DDPM, our LayoutDM is of desired
properties such as high-quality generation, strong sample diversity, faithful
distribution coverage, and stationary training in comparison to GANs and VAEs.
Quantitative and qualitative experimental results show that our method
outperforms state-of-the-art generative models in terms of quality and
diversity.
</p></li>
</ul>

<h3>Title: Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model. (arXiv:2305.02594v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02594">http://arxiv.org/abs/2305.02594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02594] Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model](http://arxiv.org/abs/2305.02594) #diffusion</code></li>
<li>Summary: <p>Multimodal-driven talking face generation refers to animating a portrait with
the given pose, expression, and gaze transferred from the driving image and
video, or estimated from the text and audio. However, existing methods ignore
the potential of text modal, and their generators mainly follow the
source-oriented feature rearrange paradigm coupled with unstable GAN
frameworks. In this work, we first represent the emotion in the text prompt,
which could inherit rich semantics from the CLIP, allowing flexible and
generalized emotion control. We further reorganize these tasks as the
target-oriented texture transfer and adopt the Diffusion Models. More
specifically, given a textured face as the source and the rendered face
projected from the desired 3DMM coefficients as the target, our proposed
Texture-Geometry-aware Diffusion Model decomposes the complex transfer problem
into multi-conditional denoising process, where a Texture Attention-based
module accurately models the correspondences between appearance and geometry
cues contained in source and target conditions, and incorporate extra implicit
information for high-fidelity talking face generation. Additionally, TGDM can
be gracefully tailored for face swapping. We derive a novel paradigm free of
unstable seesaw-style optimization, resulting in simple, stable, and effective
training and inference schemes. Extensive experiments demonstrate the
superiority of our method.
</p></li>
</ul>

<h3>Title: Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03048">http://arxiv.org/abs/2305.03048</a></li>
<li>Code URL: <a href="https://github.com/zrrskywalker/personalize-sam">https://github.com/zrrskywalker/personalize-sam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03048] Personalize Segment Anything Model with One Shot](http://arxiv.org/abs/2305.03048) #diffusion</code></li>
<li>Summary: <p>Driven by large-data pre-training, Segment Anything Model (SAM) has been
demonstrated as a powerful and promptable framework, revolutionizing the
segmentation models. Despite the generality, customizing SAM for specific
visual concepts without man-powered prompting is under explored, e.g.,
automatically segmenting your pet dog in different images. In this paper, we
propose a training-free Personalization approach for SAM, termed as PerSAM.
Given only a single image with a reference mask, PerSAM first localizes the
target concept by a location prior, and segments it within other images or
videos via three techniques: target-guided attention, target-semantic
prompting, and cascaded post-refinement. In this way, we effectively adapt SAM
for private use without any training. To further alleviate the mask ambiguity,
we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the
entire SAM, we introduce two learnable weights for multi-scale masks, only
training 2 parameters within 10 seconds for improved performance. To
demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for
personalized evaluation, and test our methods on video object segmentation with
competitive performance. Besides, our approach can also enhance DreamBooth to
personalize Stable Diffusion for text-to-image generation, which discards the
background disturbance for better target appearance learning. Code is released
at https://github.com/ZrrSkywalker/Personalize-SAM
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder. (arXiv:2305.02541v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02541">http://arxiv.org/abs/2305.02541</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02541] Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder](http://arxiv.org/abs/2305.02541) #transformer</code></li>
<li>Summary: <p>The popular VQ-VAE models reconstruct images through learning a discrete
codebook but suffer from a significant issue in the rapid quality degradation
of image reconstruction as the compression rate rises. One major reason is that
a higher compression rate induces more loss of visual signals on the higher
frequency spectrum which reflect the details on pixel space. In this paper, a
Frequency Complement Module (FCM) architecture is proposed to capture the
missing frequency information for enhancing reconstruction quality. The FCM can
be easily incorporated into the VQ-VAE structure, and we refer to the new model
as Frequency Augmented VAE (FA-VAE). In addition, a Dynamic Spectrum Loss (DSL)
is introduced to guide the FCMs to balance between various frequencies
dynamically for optimal reconstruction. FA-VAE is further extended to the
text-to-image synthesis task, and a Cross-attention Autoregressive Transformer
(CAT) is proposed to obtain more precise semantic attributes in texts.
Extensive reconstruction experiments with different compression rates are
conducted on several benchmark datasets, and the results demonstrate that the
proposed FA-VAE is able to restore more faithfully the details compared to SOTA
methods. CAT also shows improved generation quality with better image-text
semantic alignment.
</p></li>
</ul>

<h3>Title: Towards End-to-End Semi-Supervised Table Detection with Deformable Transformer. (arXiv:2305.02769v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02769">http://arxiv.org/abs/2305.02769</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02769] Towards End-to-End Semi-Supervised Table Detection with Deformable Transformer](http://arxiv.org/abs/2305.02769) #transformer</code></li>
<li>Summary: <p>Table detection is the task of classifying and localizing table objects
within document images. With the recent development in deep learning methods,
we observe remarkable success in table detection. However, a significant amount
of labeled data is required to train these models effectively. Many
semi-supervised approaches are introduced to mitigate the need for a
substantial amount of label data. These approaches use CNN-based detectors that
rely on anchor proposals and post-processing stages such as NMS. To tackle
these limitations, this paper presents a novel end-to-end semi-supervised table
detection method that employs the deformable transformer for detecting table
objects. We evaluate our semi-supervised method on PubLayNet, DocBank, ICADR-19
and TableBank datasets, and it achieves superior performance compared to
previous methods. It outperforms the fully supervised method (Deformable
transformer) by +3.4 points on 10\% labels of TableBank-both dataset and the
previous CNN-based semi-supervised approach (Soft Teacher) by +1.8 points on
10\% labels of PubLayNet dataset. We hope this work opens new possibilities
towards semi-supervised and unsupervised table detection methods.
</p></li>
</ul>

<h3>Title: MTLSegFormer: Multi-task Learning with Transformers for Semantic Segmentation in Precision Agriculture. (arXiv:2305.02813v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02813">http://arxiv.org/abs/2305.02813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02813] MTLSegFormer: Multi-task Learning with Transformers for Semantic Segmentation in Precision Agriculture](http://arxiv.org/abs/2305.02813) #transformer</code></li>
<li>Summary: <p>Multi-task learning has proven to be effective in improving the performance
of correlated tasks. Most of the existing methods use a backbone to extract
initial features with independent branches for each task, and the exchange of
information between the branches usually occurs through the concatenation or
sum of the feature maps of the branches. However, this type of information
exchange does not directly consider the local characteristics of the image nor
the level of importance or correlation between the tasks. In this paper, we
propose a semantic segmentation method, MTLSegFormer, which combines multi-task
learning and attention mechanisms. After the backbone feature extraction, two
feature maps are learned for each task. The first map is proposed to learn
features related to its task, while the second map is obtained by applying
learned visual attention to locally re-weigh the feature maps of the other
tasks. In this way, weights are assigned to local regions of the image of other
tasks that have greater importance for the specific task. Finally, the two maps
are combined and used to solve a task. We tested the performance in two
challenging problems with correlated tasks and observed a significant
improvement in accuracy, mainly in tasks with high dependence on the others.
</p></li>
</ul>

<h3>Title: UPDExplainer: an Interpretable Transformer-based Framework for Urban Physical Disorder Detection Using Street View Imagery. (arXiv:2305.02911v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02911">http://arxiv.org/abs/2305.02911</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02911] UPDExplainer: an Interpretable Transformer-based Framework for Urban Physical Disorder Detection Using Street View Imagery](http://arxiv.org/abs/2305.02911) #transformer</code></li>
<li>Summary: <p>Urban Physical Disorder (UPD), such as old or abandoned buildings, broken
sidewalks, litter, and graffiti, has a negative impact on residents' quality of
life. They can also increase crime rates, cause social disorder, and pose a
public health risk. Currently, there is a lack of efficient and reliable
methods for detecting and understanding UPD. To bridge this gap, we propose
UPDExplainer, an interpretable transformer-based framework for UPD detection.
We first develop a UPD detection model based on the Swin Transformer
architecture, which leverages readily accessible street view images to learn
discriminative representations. In order to provide clear and comprehensible
evidence and analysis, we subsequently introduce a UPD factor identification
and ranking module that combines visual explanation maps with semantic
segmentation maps. This novel integrated approach enables us to identify the
exact objects within street view images that are responsible for physical
disorders and gain insights into the underlying causes. Experimental results on
the re-annotated Place Pulse 2.0 dataset demonstrate promising detection
performance of the proposed method, with an accuracy of 79.9%. For a
comprehensive evaluation of the method's ranking performance, we report the
mean Average Precision (mAP), R-Precision (RPrec), and Normalized Discounted
Cumulative Gain (NDCG), with success rates of 75.51%, 80.61%, and 82.58%,
respectively. We also present a case study of detecting and ranking physical
disorders in the southern region of downtown Los Angeles, California, to
demonstrate the practicality and effectiveness of our framework.
</p></li>
</ul>

<h3>Title: OctFormer: Octree-based Transformers for 3D Point Clouds. (arXiv:2305.03045v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03045">http://arxiv.org/abs/2305.03045</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03045] OctFormer: Octree-based Transformers for 3D Point Clouds](http://arxiv.org/abs/2305.03045) #transformer</code></li>
<li>Summary: <p>OctFormer can not only serve as a general and effective backbone for 3D point
cloud segmentation and object detection but also have linear complexity and is
scalable for large-scale point clouds. The key challenge in applying
transformers to point clouds is reducing the quadratic, thus overwhelming,
computation complexity of attentions. To combat this issue, several works
divide point clouds into non-overlapping windows and constrain attentions in
each local window. However, the point number in each window varies greatly,
impeding the efficient execution on GPU. Observing that attentions are robust
to the shapes of local windows, we propose a novel octree attention, which
leverages sorted shuffled keys of octrees to partition point clouds into local
windows containing a fixed number of points while permitting shapes of windows
to change freely. And we also introduce dilated octree attention to expand the
receptive field further. Our octree attention can be implemented in 10 lines of
code with open-sourced libraries and runs 17 times faster than other point
cloud attentions when the point number exceeds 200k. Built upon the octree
attention, OctFormer can be easily scaled up and achieves state-of-the-art
performances on a series of 3D segmentation and detection benchmarks,
surpassing previous sparse-voxel-based CNNs and point cloud transformers in
terms of both efficiency and effectiveness. Notably, on the challenging
ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in
mIoU. Our code and trained models are available at
https://wang-ps.github.io/octformer.
</p></li>
</ul>

<h3>Title: Tracking through Containers and Occluders in the Wild. (arXiv:2305.03052v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03052">http://arxiv.org/abs/2305.03052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03052] Tracking through Containers and Occluders in the Wild](http://arxiv.org/abs/2305.03052) #transformer</code></li>
<li>Summary: <p>Tracking objects with persistence in cluttered and dynamic environments
remains a difficult challenge for computer vision systems. In this paper, we
introduce $\textbf{TCOW}$, a new benchmark and model for visual tracking
through heavy occlusion and containment. We set up a task where the goal is to,
given a video sequence, segment both the projected extent of the target object,
as well as the surrounding container or occluder whenever one exists. To study
this task, we create a mixture of synthetic and annotated real datasets to
support both supervised learning and structured evaluation of model performance
under various forms of task variation, such as moving or nested containment. We
evaluate two recent transformer-based video models and find that while they can
be surprisingly capable of tracking targets under certain settings of task
variation, there remains a considerable performance gap before we can claim a
tracking model to have acquired a true notion of object permanence.
</p></li>
</ul>

<h3>Title: A Novel Plagiarism Detection Approach Combining BERT-based Word Embedding, Attention-based LSTMs and an Improved Differential Evolution Algorithm. (arXiv:2305.02374v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02374">http://arxiv.org/abs/2305.02374</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02374] A Novel Plagiarism Detection Approach Combining BERT-based Word Embedding, Attention-based LSTMs and an Improved Differential Evolution Algorithm](http://arxiv.org/abs/2305.02374) #transformer</code></li>
<li>Summary: <p>Detecting plagiarism involves finding similar items in two different sources.
In this article, we propose a novel method for detecting plagiarism that is
based on attention mechanism-based long short-term memory (LSTM) and
bidirectional encoder representations from transformers (BERT) word embedding,
enhanced with optimized differential evolution (DE) method for pre-training and
a focal loss function for training. BERT could be included in a downstream task
and fine-tuned as a task-specific BERT can be included in a downstream task and
fine-tuned as a task-specific structure, while the trained BERT model is
capable of detecting various linguistic characteristics. Unbalanced
classification is one of the primary issues with plagiarism detection. We
suggest a focal loss-based training technique that carefully learns minority
class instances to solve this. Another issue that we tackle is the training
phase itself, which typically employs gradient-based methods like
back-propagation for the learning process and thus suffers from some drawbacks,
including sensitivity to initialization. To initiate the BP process, we suggest
a novel DE algorithm that makes use of a clustering-based mutation operator.
Here, a winning cluster is identified for the current DE population, and a
fresh updating method is used to produce potential answers. We evaluate our
proposed approach on three benchmark datasets ( MSRP, SNLI, and SemEval2014)
and demonstrate that it performs well when compared to both conventional and
population-based methods.
</p></li>
</ul>

<h3>Title: Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02386">http://arxiv.org/abs/2305.02386</a></li>
<li>Code URL: <a href="https://github.com/ghazalkhalighinejad/approximating-cky">https://github.com/ghazalkhalighinejad/approximating-cky</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02386] Approximating CKY with Transformers](http://arxiv.org/abs/2305.02386) #transformer</code></li>
<li>Summary: <p>We investigate the ability of transformer models to approximate the CKY
algorithm, using them to directly predict a parse and thus avoid the CKY
algorithm's cubic dependence on sentence length. We find that on standard
constituency parsing benchmarks this approach achieves competitive or better
performance than comparable parsers that make use of CKY, while being faster.
We also evaluate the viability of this approach for parsing under random PCFGs.
Here we find that performance declines as the grammar becomes more ambiguous,
suggesting that the transformer is not fully capturing the CKY computation.
However, we also find that incorporating additional inductive bias is helpful,
and we propose a novel approach that makes use of gradients with respect to
chart representations in predicting the parse, in analogy with the CKY
algorithm being the subgradient of a partition function variant with respect to
the chart.
</p></li>
</ul>

<h3>Title: Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02412">http://arxiv.org/abs/2305.02412</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02412] Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents](http://arxiv.org/abs/2305.02412) #transformer</code></li>
<li>Summary: <p>Pre-trained large language models (LLMs) capture procedural knowledge about
the world. Recent work has leveraged LLM's ability to generate abstract plans
to simplify challenging control tasks, either by action scoring, or action
modeling (fine-tuning). However, the transformer architecture inherits several
constraints that make it difficult for the LLM to directly serve as the agent:
e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,
and incompatibility with non-text environments. To maintain compatibility with
a low-level trainable actor, we propose to instead use the knowledge in LLMs to
simplify the control problem, rather than solving it. We propose the Plan,
Eliminate, and Track (PET) framework. The Plan module translates a task
description into a list of high-level sub-tasks. The Eliminate module masks out
irrelevant objects and receptacles from the observation for the current
sub-task. Finally, the Track module determines whether the agent has
accomplished each sub-task. On the AlfWorld instruction following benchmark,
the PET framework leads to a significant 15% improvement over SOTA for
generalization to human goal specifications.
</p></li>
</ul>

<h3>Title: Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02459">http://arxiv.org/abs/2305.02459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02459] Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge](http://arxiv.org/abs/2305.02459) #transformer</code></li>
<li>Summary: <p>While transformer-based systems have enabled greater accuracies with fewer
training examples, data acquisition obstacles still persist for rare-class
tasks -- when the class label is very infrequent (e.g. < 5% of samples). Active
learning has in general been proposed to alleviate such challenges, but choice
of selection strategy, the criteria by which rare-class examples are chosen,
has not been systematically evaluated. Further, transformers enable iterative
transfer-learning approaches. We propose and investigate transfer- and active
learning solutions to the rare class problem of dissonance detection through
utilizing models trained on closely related tasks and the evaluation of
acquisition strategies, including a proposed probability-of-rare-class (PRC)
approach. We perform these experiments for a specific rare class problem:
collecting language samples of cognitive dissonance from social media. We find
that PRC is a simple and effective strategy to guide annotations and ultimately
improve model accuracy while transfer-learning in a specific order can improve
the cold-start performance of the learner but does not benefit iterations of
active learning.
</p></li>
</ul>

<h3>Title: Learning Language-Specific Layers for Multilingual Machine Translation. (arXiv:2305.02665v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02665">http://arxiv.org/abs/2305.02665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02665] Learning Language-Specific Layers for Multilingual Machine Translation](http://arxiv.org/abs/2305.02665) #transformer</code></li>
<li>Summary: <p>Multilingual Machine Translation promises to improve translation quality
between non-English languages. This is advantageous for several reasons, namely
lower latency (no need to translate twice), and reduced error cascades (e.g.,
avoiding losing gender and formality information when translating through
English). On the downside, adding more languages reduces model capacity per
language, which is usually countered by increasing the overall model size,
making training harder and inference slower. In this work, we introduce
Language-Specific Transformer Layers (LSLs), which allow us to increase model
capacity, while keeping the amount of computation and the number of parameters
used in the forward pass constant. The key idea is to have some layers of the
encoder be source or target language-specific, while keeping the remaining
layers shared. We study the best way to place these layers using a neural
architecture search inspired approach, and achieve an improvement of 1.3 chrF
(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and
1.9 chrF (2.2 spBLEU) on a shared decoder one.
</p></li>
</ul>

<h3>Title: 2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02869">http://arxiv.org/abs/2305.02869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02869] 2x Faster Language Model Pre-training via Masked Structural Growth](http://arxiv.org/abs/2305.02869) #transformer</code></li>
<li>Summary: <p>Acceleration of large language model pre-training is a critical issue in
present NLP research. In this paper, we focus on speeding up pre-training by
progressively growing from a small Transformer structure to a large one. There
are two main research problems related to progressive growth: growth schedule
and growth operator. For growth schedule, existing work has explored
multi-stage expansion of depth and feedforward layers. However, the impact of
each dimension on the schedule's efficiency is still an open question. For
growth operator, existing work relies on the initialization of new weights to
inherit knowledge, and achieve only non-strict function preservation, limiting
further optimization of training dynamics. To address these issues, we propose
Masked Structural Growth (MSG), including growth schedules involving all
possible dimensions and strictly function-preserving growth operators that is
independent of the initialization of new weights. Experiments show that MSG is
significantly faster than related work: we achieve a speed-up of 80% for
Bert-base and 120% for Bert-large pre-training. Moreover, MSG is able to
improve fine-tuning performances at the same time.
</p></li>
</ul>

<h3>Title: Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs. (arXiv:2305.02440v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02440">http://arxiv.org/abs/2305.02440</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02440] Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs](http://arxiv.org/abs/2305.02440) #transformer</code></li>
<li>Summary: <p>Large language models (LLMs) power many state-of-the-art systems in natural
language processing. However, these models are extremely computationally
expensive, even at inference time, raising the natural question: when is the
extra cost of deploying a larger model worth the anticipated boost in
capabilities? Better understanding this tradeoff fundamentally could benefit
from an inference efficiency metric that is both (i) easily comparable across
models from different providers, and (ii) representative of the true cost of
running queries in an isolated performance environment. Unfortunately, access
to LLMs today is largely restricted to black-box text generation APIs and raw
runtimes measured through this interface do not satisfy these desiderata: model
providers can apply various software and hardware optimizations orthogonal to
the model, and models served on shared infrastructure are susceptible to
performance contention. To circumvent these problems, we propose a new metric
for comparing inference efficiency across models. This metric puts models on
equal footing as though they were served (i) on uniform hardware and software,
and (ii) without performance contention. We call this metric the
\emph{idealized runtime}, and we propose a methodology to efficiently estimate
this metric for autoregressive Transformer models. We also propose cost-aware
variants that incorporate the number of accelerators needed to serve the model.
Using these metrics, we compare ten state-of-the-art LLMs to provide the first
analysis of inference efficiency-capability tradeoffs; we make several
observations from this analysis, including the fact that the superior inference
runtime performance of certain APIs is often a byproduct of optimizations
within the API rather than the underlying model. Our methodology also
facilitates the efficient comparison of different software and hardware stacks.
</p></li>
</ul>

<h3>Title: On the Expressivity Role of LayerNorm in Transformers' Attention. (arXiv:2305.02582v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02582">http://arxiv.org/abs/2305.02582</a></li>
<li>Code URL: <a href="https://github.com/tech-srl/layer_norm_expressivity_role">https://github.com/tech-srl/layer_norm_expressivity_role</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02582] On the Expressivity Role of LayerNorm in Transformers' Attention](http://arxiv.org/abs/2305.02582) #transformer</code></li>
<li>Summary: <p>Layer Normalization (LayerNorm) is an inherent component in all
Transformer-based models. In this paper, we show that LayerNorm is crucial to
the expressivity of the multi-head attention layer that follows it. This is in
contrast to the common belief that LayerNorm's only role is to normalize the
activations during the forward pass, and their gradients during the backward
pass. We consider a geometric interpretation of LayerNorm and show that it
consists of two components: (a) projection of the input vectors to a $d-1$
space that is orthogonal to the $\left[1,1,...,1\right]$ vector, and (b)
scaling of all vectors to the same norm of $\sqrt{d}$. We show that each of
these components is important for the attention layer that follows it in
Transformers: (a) projection allows the attention mechanism to create an
attention query that attends to all keys equally, offloading the need to learn
this operation by the attention; and (b) scaling allows each key to potentially
receive the highest attention, and prevents keys from being "un-select-able".
We show empirically that Transformers do indeed benefit from these properties
of LayeNorm in general language modeling and even in computing simple functions
such as "majority". Our code is available at
https://github.com/tech-srl/layer_norm_expressivity_role .
</p></li>
</ul>

<h3>Title: Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02866">http://arxiv.org/abs/2305.02866</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02866] Hierarchical Transformer for Scalable Graph Learning](http://arxiv.org/abs/2305.02866) #transformer</code></li>
<li>Summary: <p>Graph Transformer is gaining increasing attention in the field of machine
learning and has demonstrated state-of-the-art performance on benchmarks for
graph representation learning. However, as current implementations of Graph
Transformer primarily focus on learning representations of small-scale graphs,
the quadratic complexity of the global self-attention mechanism presents a
challenge for full-batch training when applied to larger graphs. Additionally,
conventional sampling-based methods fail to capture necessary high-level
contextual information, resulting in a significant loss of performance. In this
paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a
solution to these challenges. HSGT successfully scales the Transformer
architecture to node representation learning tasks on large-scale graphs, while
maintaining high performance. By utilizing graph hierarchies constructed
through coarsening techniques, HSGT efficiently updates and stores multi-scale
information in node embeddings at different levels. Together with
sampling-based training methods, HSGT effectively captures and aggregates
multi-level information on the hierarchical graph using only Transformer
blocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-art
performance on large-scale benchmarks with graphs containing millions of nodes
with high efficiency.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Controllable Visual-Tactile Synthesis. (arXiv:2305.03051v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03051">http://arxiv.org/abs/2305.03051</a></li>
<li>Code URL: <a href="https://github.com/ruihangao/visual-tactile-synthesis">https://github.com/ruihangao/visual-tactile-synthesis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03051] Controllable Visual-Tactile Synthesis](http://arxiv.org/abs/2305.03051) #generative</code></li>
<li>Summary: <p>Deep generative models have various content creation applications such as
graphic design, e-commerce, and virtual Try-on. However, current works mainly
focus on synthesizing realistic visual outputs, often ignoring other sensory
modalities, such as touch, which limits physical interaction with users. In
this work, we leverage deep generative models to create a multi-sensory
experience where users can touch and see the synthesized object when sliding
their fingers on a haptic surface. The main challenges lie in the significant
scale discrepancy between vision and touch sensing and the lack of explicit
mapping from touch sensing data to a haptic rendering device. To bridge this
gap, we collect high-resolution tactile data with a GelSight sensor and create
a new visuotactile clothing dataset. We then develop a conditional generative
model that synthesizes both visual and tactile outputs from a single sketch. We
evaluate our method regarding image quality and tactile rendering accuracy.
Finally, we introduce a pipeline to render high-quality visual and tactile
outputs on an electroadhesion-based haptic device for an immersive experience,
allowing for challenging materials and editable sketch inputs.
</p></li>
</ul>

<h3>Title: Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03047">http://arxiv.org/abs/2305.03047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03047] Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](http://arxiv.org/abs/2305.03047) #generative</code></li>
<li>Summary: <p>Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised
fine-tuning (SFT) with human annotations and reinforcement learning from human
feedback (RLHF) to align the output of large language models (LLMs) with human
intentions, ensuring they are helpful, ethical, and reliable. However, this
dependence can significantly constrain the true potential of AI-assistant
agents due to the high cost of obtaining human supervision and the related
issues on quality, reliability, diversity, self-consistency, and undesirable
biases. To address these challenges, we propose a novel approach called
SELF-ALIGN, which combines principle-driven reasoning and the generative power
of LLMs for the self-alignment of AI agents with minimal human supervision. Our
approach encompasses four stages: first, we use an LLM to generate synthetic
prompts, and a topic-guided method to augment the prompt diversity; second, we
use a small set of human-written principles for AI models to follow, and guide
the LLM through in-context learning from demonstrations (of principles
application) to produce helpful, ethical, and reliable responses to user's
queries; third, we fine-tune the original LLM with the high-quality
self-aligned responses so that the resulting model can generate desirable
responses for each query directly without the principle set and the
demonstrations anymore; and finally, we offer a refinement step to address the
issues of overly-brief or indirect responses. Applying SELF-ALIGN to the
LLaMA-65b base language model, we develop an AI assistant named Dromedary. With
fewer than 300 lines of human annotations (including < 200 seed prompts, 16
generic principles, and 5 exemplars for in-context learning). Dromedary
significantly surpasses the performance of several state-of-the-art AI systems,
including Text-Davinci-003 and Alpaca, on benchmark datasets with various
settings.
</p></li>
</ul>

<h3>Title: Tensorizing flows: a tool for variational inference. (arXiv:2305.02460v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02460">http://arxiv.org/abs/2305.02460</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02460] Tensorizing flows: a tool for variational inference](http://arxiv.org/abs/2305.02460) #generative</code></li>
<li>Summary: <p>Fueled by the expressive power of deep neural networks, normalizing flows
have achieved spectacular success in generative modeling, or learning to draw
new samples from a distribution given a finite dataset of training samples.
Normalizing flows have also been applied successfully to variational inference,
wherein one attempts to learn a sampler based on an expression for the
log-likelihood or energy function of the distribution, rather than on data. In
variational inference, the unimodality of the reference Gaussian distribution
used within the normalizing flow can cause difficulties in learning multimodal
distributions. We introduce an extension of normalizing flows in which the
Gaussian reference is replaced with a reference distribution that is
constructed via a tensor network, specifically a matrix product state or tensor
train. We show that by combining flows with tensor networks on difficult
variational inference tasks, we can improve on the results obtained by using
either tool without the other.
</p></li>
</ul>

<h3>Title: Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02555">http://arxiv.org/abs/2305.02555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02555] Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era](http://arxiv.org/abs/2305.02555) #generative</code></li>
<li>Summary: <p>With various AI tools such as ChatGPT becoming increasingly popular, we are
entering a true AI era. We can foresee that exceptional AI tools will soon reap
considerable profits. A crucial question arise: should AI tools share revenue
with their training data providers in additional to traditional stakeholders
and shareholders? The answer is Yes. Large AI tools, such as large language
models, always require more and better quality data to continuously improve,
but current copyright laws limit their access to various types of data. Sharing
revenue between AI tools and their data providers could transform the current
hostile zero-sum game relationship between AI tools and a majority of
copyrighted data owners into a collaborative and mutually beneficial one, which
is necessary to facilitate the development of a virtuous cycle among AI tools,
their users and data providers that drives forward AI technology and builds a
healthy AI ecosystem. However, current revenue-sharing business models do not
work for AI tools in the forthcoming AI era, since the most widely used metrics
for website-based traffic and action, such as clicks, will be replaced by new
metrics such as prompts and cost per prompt for generative AI tools. A
completely new revenue-sharing business model, which must be almost independent
of AI tools and be easily explained to data providers, needs to establish a
prompt-based scoring system to measure data engagement of each data provider.
This paper systematically discusses how to build such a scoring system for all
data providers for AI tools based on classification and content similarity
models, and outlines the requirements for AI tools or third parties to build
it. Sharing revenue with data providers using such a scoring system would
encourage more data owners to participate in the revenue-sharing program. This
will be a utilitarian AI era where all parties benefit.
</p></li>
</ul>

<h3>Title: Are VAEs Bad at Reconstructing Molecular Graphs?. (arXiv:2305.03041v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03041">http://arxiv.org/abs/2305.03041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03041] Are VAEs Bad at Reconstructing Molecular Graphs?](http://arxiv.org/abs/2305.03041) #generative</code></li>
<li>Summary: <p>Many contemporary generative models of molecules are variational
auto-encoders of molecular graphs. One term in their training loss pertains to
reconstructing the input, yet reconstruction capabilities of state-of-the-art
models have not yet been thoroughly compared on a large and chemically diverse
dataset. In this work, we show that when several state-of-the-art generative
models are evaluated under the same conditions, their reconstruction accuracy
is surprisingly low, worse than what was previously reported on seemingly
harder datasets. However, we show that improving reconstruction does not
directly lead to better sampling or optimization performance. Failed
reconstructions from the MoLeR model are usually similar to the inputs,
assembling the same motifs in a different way, and possess similar chemical
properties such as solubility. Finally, we show that the input molecule and its
failed reconstruction are usually mapped by the different encoders to
statistically distinguishable posterior distributions, hinting that posterior
collapse may not fully explain why VAEs are bad at reconstructing molecular
graphs.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: ChatGPT-steered Editing Instructor for Customization of Abstractive Summarization. (arXiv:2305.02483v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02483">http://arxiv.org/abs/2305.02483</a></li>
<li>Code URL: <a href="https://github.com/wendy-xiao/chatgpt_editing_summ">https://github.com/wendy-xiao/chatgpt_editing_summ</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02483] ChatGPT-steered Editing Instructor for Customization of Abstractive Summarization](http://arxiv.org/abs/2305.02483) #large language model</code></li>
<li>Summary: <p>Tailoring outputs of large language models, such as ChatGPT, to specific user
needs remains a challenge despite their impressive generation quality. In this
paper, we propose a tri-agent generation pipeline consisting of a generator, an
instructor, and an editor to enhance the customization of generated outputs.
The generator produces an initial output, the user-specific instructor
generates editing instructions, and the editor generates a revised output
aligned with user preferences. The inference-only large language model
(ChatGPT) serves as both the generator and the editor, while a smaller model
acts as the user-specific instructor to guide the generation process toward
user needs. The instructor is trained using editor-steered reinforcement
learning, leveraging feedback from the large-scale editor model to optimize
instruction generation. Experimental results on two abstractive summarization
datasets demonstrate the effectiveness of our approach in generating outputs
that better fulfill user expectations.
</p></li>
</ul>

<h3>Title: PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02547">http://arxiv.org/abs/2305.02547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02547] PersonaLLM: Investigating the Ability of GPT-3](http://arxiv.org/abs/2305.02547) #large language model</code></li>
<li>Summary: <p>Despite the many use cases for large language models (LLMs) in the design of
chatbots in various industries and the research showing the importance of
personalizing chatbots to cater to different personality traits, little work
has been done to evaluate whether the behaviors of personalized LLMs can
reflect certain personality traits accurately and consistently. We consider
studying the behavior of LLM-based simulated agents which refer to as LLM
personas and present a case study with GPT-3.5 (text-davinci-003) to
investigate whether LLMs can generate content with consistent, personalized
traits when assigned Big Five personality types and gender roles. We created
320 LLM personas (5 females and 5 males for each of the 32 Big Five personality
types) and prompted them to complete the classic 44-item Big Five Inventory
(BFI) and then write an 800-word story about their childhood. Results showed
that LLM personas' self-reported BFI scores are consistent with their assigned
personality types, with large effect sizes found on all five traits. Moreover,
significant correlations were found between assigned personality types and some
Linguistic Inquiry and Word Count (LIWC) psycholinguistic features of their
writings. For instance, extroversion is associated with pro-social and active
words, and neuroticism is associated with words related to negative emotions
and mental health. Besides, we only found significant differences in using
technological and cultural words in writing between LLM-generated female and
male personas. This work provides a first step for further research on
personalized LLMs and their applications in Human-AI conversation.
</p></li>
</ul>

<h3>Title: Faithful Question Answering with Monte-Carlo Planning. (arXiv:2305.02556v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02556">http://arxiv.org/abs/2305.02556</a></li>
<li>Code URL: <a href="https://github.com/Raising-hrx/FAME">https://github.com/Raising-hrx/FAME</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02556] Faithful Question Answering with Monte-Carlo Planning](http://arxiv.org/abs/2305.02556) #large language model</code></li>
<li>Summary: <p>Although large language models demonstrate remarkable question-answering
performances, revealing the intermediate reasoning steps that the models
faithfully follow remains challenging. In this paper, we propose FAME (FAithful
question answering with MontE-carlo planning) to answer questions based on
faithful reasoning steps. The reasoning steps are organized as a structured
entailment tree, which shows how premises are used to produce intermediate
conclusions that can prove the correctness of the answer. We formulate the task
as a discrete decision-making problem and solve it through the interaction of a
reasoning environment and a controller. The environment is modular and contains
several basic task-oriented modules, while the controller proposes actions to
assemble the modules. Since the search space could be large, we introduce a
Monte-Carlo planning algorithm to do a look-ahead search and select actions
that will eventually lead to high-quality steps. FAME achieves state-of-the-art
performance on the standard benchmark. It can produce valid and faithful
reasoning steps compared with large language models with a much smaller model
size.
</p></li>
</ul>

<h3>Title: Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models. (arXiv:2305.03025v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03025">http://arxiv.org/abs/2305.03025</a></li>
<li>Code URL: <a href="https://github.com/dandelionsllm/pandallm">https://github.com/dandelionsllm/pandallm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03025] Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models](http://arxiv.org/abs/2305.03025) #large language model</code></li>
<li>Summary: <p>This project focuses on enhancing open-source large language models through
instruction-tuning and providing comprehensive evaluations of their
performance. We explore how various training data factors, such as quantity,
quality, and linguistic distribution, influence the performance of
instruction-tuned models trained on publicly accessible high-quality
instruction datasets for both English and Chinese languages. Our goal is to
supplement evaluation with quantitative analyses, providing valuable insights
for the continued advancement of open-source chat models. Our model, data, and
code are publicly available for others to use and build upon.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Unsupervised Domain Adaptation for Neuron Membrane Segmentation based on Structural Features. (arXiv:2305.02569v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02569">http://arxiv.org/abs/2305.02569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02569] Unsupervised Domain Adaptation for Neuron Membrane Segmentation based on Structural Features](http://arxiv.org/abs/2305.02569) #segmentation</code></li>
<li>Summary: <p>AI-enhanced segmentation of neuronal boundaries in electron microscopy (EM)
images is crucial for automatic and accurate neuroinformatics studies. To
enhance the limited generalization ability of typical deep learning frameworks
for medical image analysis, unsupervised domain adaptation (UDA) methods have
been applied. In this work, we propose to improve the performance of UDA
methods on cross-domain neuron membrane segmentation in EM images. First, we
designed a feature weight module considering the structural features during
adaptation. Second, we introduced a structural feature-based super-resolution
approach to alleviating the domain gap by adjusting the cross-domain image
resolutions. Third, we proposed an orthogonal decomposition module to
facilitate the extraction of domain-invariant features. Extensive experiments
on two domain adaptive membrane segmentation applications have indicated the
effectiveness of our method.
</p></li>
</ul>

<h3>Title: Text Reading Order in Uncontrolled Conditions by Sparse Graph Segmentation. (arXiv:2305.02577v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02577">http://arxiv.org/abs/2305.02577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02577] Text Reading Order in Uncontrolled Conditions by Sparse Graph Segmentation](http://arxiv.org/abs/2305.02577) #segmentation</code></li>
<li>Summary: <p>Text reading order is a crucial aspect in the output of an OCR engine, with a
large impact on downstream tasks. Its difficulty lies in the large variation of
domain specific layout structures, and is further exacerbated by real-world
image degradations such as perspective distortions. We propose a lightweight,
scalable and generalizable approach to identify text reading order with a
multi-modal, multi-task graph convolutional network (GCN) running on a sparse
layout based graph. Predictions from the model provide hints of bidimensional
relations among text lines and layout region structures, upon which a
post-processing cluster-and-sort algorithm generates an ordered sequence of all
the text lines. The model is language-agnostic and runs effectively across
multi-language datasets that contain various types of images taken in
uncontrolled conditions, and it is small enough to be deployed on virtually any
platform including mobile devices.
</p></li>
</ul>

<h3>Title: Point2Tree(P2T) -- framework for parameter tuning of semantic and instance segmentation used with mobile laser scanning data in coniferous forest. (arXiv:2305.02651v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02651">http://arxiv.org/abs/2305.02651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02651] Point2Tree(P2T) -- framework for parameter tuning of semantic and instance segmentation used with mobile laser scanning data in coniferous forest](http://arxiv.org/abs/2305.02651) #segmentation</code></li>
<li>Summary: <p>This article introduces Point2Tree, a novel framework that incorporates a
three-stage process involving semantic segmentation, instance segmentation,
optimization analysis of hyperparemeters importance. It introduces a
comprehensive and modular approach to processing laser points clouds in
Forestry. We tested it on two independent datasets. The first area was located
in an actively managed boreal coniferous dominated forest in V{\aa}ler, Norway,
16 circular plots of 400 square meters were selected to cover a range of forest
conditions in terms of species composition and stand density. We trained a
model based on Pointnet++ architecture which achieves 0.92 F1-score in semantic
segmentation. As a second step in our pipeline we used graph-based approach for
instance segmentation which reached F1-score approx. 0.6. The optimization
allowed to further boost the performance of the pipeline by approx. 4 \%
points.
</p></li>
</ul>

<h3>Title: Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty. (arXiv:2305.02722v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02722">http://arxiv.org/abs/2305.02722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02722] Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty](http://arxiv.org/abs/2305.02722) #segmentation</code></li>
<li>Summary: <p>Knowledge distillation is an effective paradigm for boosting the performance
of pocket-size model, especially when multiple teacher models are available,
the student would break the upper limit again. However, it is not economical to
train diverse teacher models for the disposable distillation. In this paper, we
introduce a new concept dubbed Avatars for distillation, which are the
inference ensemble models derived from the teacher. Concretely, (1) For each
iteration of distillation training, various Avatars are generated by a
perturbation transformation. We validate that Avatars own higher upper limit of
working capacity and teaching ability, aiding the student model in learning
diverse and receptive knowledge perspectives from the teacher model. (2) During
the distillation, we propose an uncertainty-aware factor from the variance of
statistical differences between the vanilla teacher and Avatars, to adjust
Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge
Distillation AKD is fundamentally different from existing methods and refines
with the innovative view of unequal training. Comprehensive experiments
demonstrate the effectiveness of our Avatars mechanism, which polishes up the
state-of-the-art distillation methods for dense prediction without more extra
computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object
Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation,
respectively.
</p></li>
</ul>

<h3>Title: FUSegNet: A Deep Convolutional Neural Network for Foot Ulcer Segmentation. (arXiv:2305.02961v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02961">http://arxiv.org/abs/2305.02961</a></li>
<li>Code URL: <a href="https://github.com/mrinal054/fusegnet">https://github.com/mrinal054/fusegnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02961] FUSegNet: A Deep Convolutional Neural Network for Foot Ulcer Segmentation](http://arxiv.org/abs/2305.02961) #segmentation</code></li>
<li>Summary: <p>This paper presents FUSegNet, a new model for foot ulcer segmentation in
diabetes patients, which uses the pre-trained EfficientNet-b7 as a backbone to
address the issue of limited training samples. A modified spatial and channel
squeeze-and-excitation (scSE) module called parallel scSE or P-scSE is proposed
that combines additive and max-out scSE. A new arrangement is introduced for
the module by fusing it in the middle of each decoder stage. As the top decoder
stage carries a limited number of feature maps, max-out scSE is bypassed there
to form a shorted P-scSE. A set of augmentations, comprising geometric,
morphological, and intensity-based augmentations, is applied before feeding the
data into the network. The proposed model is first evaluated on a publicly
available chronic wound dataset where it achieves a data-based dice score of
92.70%, which is the highest score among the reported approaches. The model
outperforms other scSE-based UNet models in terms of Pratt's figure of merits
(PFOM) scores in most categories, which evaluates the accuracy of edge
localization. The model is then tested in the MICCAI 2021 FUSeg challenge,
where a variation of FUSegNet called x-FUSegNet is submitted. The x-FUSegNet
model, which takes the average of outputs obtained by FUSegNet using 5-fold
cross-validation, achieves a dice score of 89.23%, placing it at the top of the
FUSeg Challenge leaderboard. The source code for the model is available on
https://github.com/mrinal054/FUSegNet.
</p></li>
</ul>

<h3>Title: Unsupervised Dialogue Topic Segmentation with Topic-aware Utterance Representation. (arXiv:2305.02747v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02747">http://arxiv.org/abs/2305.02747</a></li>
<li>Code URL: <a href="https://github.com/alibabaresearch/damo-convai">https://github.com/alibabaresearch/damo-convai</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02747] Unsupervised Dialogue Topic Segmentation with Topic-aware Utterance Representation](http://arxiv.org/abs/2305.02747) #segmentation</code></li>
<li>Summary: <p>Dialogue Topic Segmentation (DTS) plays an essential role in a variety of
dialogue modeling tasks. Previous DTS methods either focus on semantic
similarity or dialogue coherence to assess topic similarity for unsupervised
dialogue segmentation. However, the topic similarity cannot be fully identified
via semantic similarity or dialogue coherence. In addition, the unlabeled
dialogue data, which contains useful clues of utterance relationships, remains
underexploited. In this paper, we propose a novel unsupervised DTS framework,
which learns topic-aware utterance representations from unlabeled dialogue data
through neighboring utterance matching and pseudo-segmentation. Extensive
experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial)
demonstrate that our method significantly outperforms the strong baseline
methods. For reproducibility, we provide our code and data
at:https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
