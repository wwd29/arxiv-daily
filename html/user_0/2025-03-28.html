<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-28</h1>
<h3>Title: ECLAIR: Enhanced Clarification for Interactive Responses in an Enterprise AI Assistant</h3>
<ul>
<li><strong>Authors: </strong>John Murzaku, Zifan Liu, Vaishnavi Muppala, Md Mehrab Tanjim, Xiang Chen, Yunyao Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20791">https://arxiv.org/abs/2503.20791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20791">https://arxiv.org/pdf/2503.20791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20791]] ECLAIR: Enhanced Clarification for Interactive Responses in an Enterprise AI Assistant(https://arxiv.org/abs/2503.20791)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable progress in understanding and generating natural language across various applications. However, they often struggle with resolving ambiguities in real-world, enterprise-level interactions, where context and domain-specific knowledge play a crucial role. In this demonstration, we introduce ECLAIR (Enhanced CLArification for Interactive Responses), a multi-agent framework for interactive disambiguation. ECLAIR enhances ambiguous user query clarification through an interactive process where custom agents are defined, ambiguity reasoning is conducted by the agents, clarification questions are generated, and user feedback is leveraged to refine the final response. When tested on real-world customer data, ECLAIR demonstrates significant improvements in clarification question generation compared to standard few-shot methods.</li>
</ul>

<h3>Title: Can Zero-Shot Commercial APIs Deliver Regulatory-Grade Clinical Text DeIdentification?</h3>
<ul>
<li><strong>Authors: </strong>Veysel Kocaman, Muhammed Santas, Yigit Gul, Mehmet Butgul, David Talby</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20794">https://arxiv.org/abs/2503.20794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20794">https://arxiv.org/pdf/2503.20794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20794]] Can Zero-Shot Commercial APIs Deliver Regulatory-Grade Clinical Text DeIdentification?(https://arxiv.org/abs/2503.20794)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>We systematically assess the performance of three leading API-based de-identification systems - Azure Health Data Services, AWS Comprehend Medical, and OpenAI GPT-4o - against our de-identification systems on a ground truth dataset of 48 clinical documents annotated by medical experts. Our analysis, conducted at both entity-level and token-level, demonstrates that our solution, Healthcare NLP, achieves the highest accuracy, with a 96% F1-score in protected health information (PHI) detection, significantly outperforming Azure (91%), AWS (83%), and GPT-4o (79%). Beyond accuracy, Healthcare NLP is also the most cost-effective solution, reducing processing costs by over 80% compared to Azure and GPT-4o. Its fixed-cost local deployment model avoids the escalating per-request fees of cloud-based services, making it a scalable and economical choice. Our results underscore a critical limitation: zero-shot commercial APIs fail to meet the accuracy, adaptability, and cost-efficiency required for regulatory-grade clinical de-identification. Healthcare NLP's superior performance, customization capabilities, and economic advantages position it as the more viable solution for healthcare organizations seeking compliance and scalability in clinical NLP workflows.</li>
</ul>

<h3>Title: EXPLICATE: Enhancing Phishing Detection through Explainable AI and LLM-Powered Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Bryan Lim, Roman Huerta, Alejandro Sotelo, Anthonie Quintela, Priyanka Kumar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20796">https://arxiv.org/abs/2503.20796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20796">https://arxiv.org/pdf/2503.20796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20796]] EXPLICATE: Enhancing Phishing Detection through Explainable AI and LLM-Powered Interpretability(https://arxiv.org/abs/2503.20796)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Sophisticated phishing attacks have emerged as a major cybersecurity threat, becoming more common and difficult to prevent. Though machine learning techniques have shown promise in detecting phishing attacks, they function mainly as "black boxes" without revealing their decision-making rationale. This lack of transparency erodes the trust of users and diminishes their effective threat response. We present EXPLICATE: a framework that enhances phishing detection through a three-component architecture: an ML-based classifier using domain-specific features, a dual-explanation layer combining LIME and SHAP for complementary feature-level insights, and an LLM enhancement using DeepSeek v3 to translate technical explanations into accessible natural language. Our experiments show that EXPLICATE attains 98.4 % accuracy on all metrics, which is on par with existing deep learning techniques but has better explainability. High-quality explanations are generated by the framework with an accuracy of 94.2 % as well as a consistency of 96.8\% between the LLM output and model prediction. We create EXPLICATE as a fully usable GUI application and a light Chrome extension, showing its applicability in many deployment situations. The research shows that high detection performance can go hand-in-hand with meaningful explainability in security applications. Most important, it addresses the critical divide between automated AI and user trust in phishing detection systems.</li>
</ul>

<h3>Title: "Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Haroon, Magdalena Wojcieszak, Anshuman Chhabra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20797">https://arxiv.org/abs/2503.20797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20797">https://arxiv.org/pdf/2503.20797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20797]] "Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection(https://arxiv.org/abs/2503.20797)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.</li>
</ul>

<h3>Title: Payload-Aware Intrusion Detection with CMAE and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yongcheol Kim, Chanjae Lee, Young Yoon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20798">https://arxiv.org/abs/2503.20798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20798">https://arxiv.org/pdf/2503.20798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20798]] Payload-Aware Intrusion Detection with CMAE and Large Language Models(https://arxiv.org/abs/2503.20798)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Intrusion Detection Systems (IDS) are crucial for identifying malicious traffic, yet traditional signature-based methods struggle with zero-day attacks and high false positive rates. AI-driven packet-capture analysis offers a promising alternative. However, existing approaches rely heavily on flow-based or statistical features, limiting their ability to detect fine-grained attack patterns. This study proposes Xavier-CMAE, an enhanced Convolutional Multi-Head Attention Ensemble (CMAE) model that improves detection accuracy while reducing computational overhead. By replacing Word2Vec embeddings with a Hex2Int tokenizer and Xavier initialization, Xavier-CMAE eliminates pre-training, accelerates training, and achieves 99.971% accuracy with a 0.018% false positive rate, outperforming Word2Vec-based methods. Additionally, we introduce LLM-CMAE, which integrates pre-trained Large Language Model (LLM) tokenizers into CMAE. While LLMs enhance feature extraction, their computational cost hinders real-time detection. LLM-CMAE balances efficiency and performance, reaching 99.969% accuracy with a 0.019% false positive rate. This work advances AI-powered IDS by (1) introducing a payload-based detection framework, (2) enhancing efficiency with Xavier-CMAE, and (3) integrating LLM tokenizers for improved real-time detection.</li>
</ul>

<h3>Title: Evidencing Unauthorized Training Data from AI Generated Content using Information Isotopes</h3>
<ul>
<li><strong>Authors: </strong>Qi Tao, Yin Jinhua, Cai Dongqi, Xie Yueqi, Wang Huili, Hu Zhiyang, Yang Peiru, Nan Guoshun, Zhou Zhili, Wang Shangguang, Lyu Lingjuan, Huang Yongfeng, Lane Nicholas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20800">https://arxiv.org/abs/2503.20800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20800">https://arxiv.org/pdf/2503.20800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20800]] Evidencing Unauthorized Training Data from AI Generated Content using Information Isotopes(https://arxiv.org/abs/2503.20800)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In light of scaling laws, many AI institutions are intensifying efforts to construct advanced AIs on extensive collections of high-quality human data. However, in a rush to stay competitive, some institutions may inadvertently or even deliberately include unauthorized data (like privacy- or intellectual property-sensitive content) for AI training, which infringes on the rights of data owners. Compounding this issue, these advanced AI services are typically built on opaque cloud platforms, which restricts access to internal information during AI training and inference, leaving only the generated outputs available for forensics. Thus, despite the introduction of legal frameworks by various countries to safeguard data rights, uncovering evidence of data misuse in modern opaque AI applications remains a significant challenge. In this paper, inspired by the ability of isotopes to trace elements within chemical reactions, we introduce the concept of information isotopes and elucidate their properties in tracing training data within opaque AI systems. Furthermore, we propose an information isotope tracing method designed to identify and provide evidence of unauthorized data usage by detecting the presence of target information isotopes in AI generations. We conduct experiments on ten AI models (including GPT-4o, Claude-3.5, and DeepSeek) and four benchmark datasets in critical domains (medical data, copyrighted books, and news). Results show that our method can distinguish training datasets from non-training datasets with 99\% accuracy and significant evidence (p-value$<0.001$) by examining a data entry equivalent in length to a research paper. The findings show the potential of our work as an inclusive tool for empowering individuals, including those without expertise in AI, to safeguard their data rights in the rapidly evolving era of AI advancements and applications.</li>
</ul>

<h3>Title: CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuhao Zhang, Bo Cheng, Jiale Han, Yuli Chen, Zhixuan Wu, Changbao Li, Pingli Gu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20802">https://arxiv.org/abs/2503.20802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20802">https://arxiv.org/pdf/2503.20802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20802]] CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models(https://arxiv.org/abs/2503.20802)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Text watermarking provides an effective solution for identifying synthetic text generated by large language models. However, existing techniques often focus on satisfying specific criteria while ignoring other key aspects, lacking a unified evaluation. To fill this gap, we propose the Comprehensive Evaluation Framework for Watermark (CEFW), a unified framework that comprehensively evaluates watermarking methods across five key dimensions: ease of detection, fidelity of text quality, minimal embedding cost, robustness to adversarial attacks, and imperceptibility to prevent imitation or forgery. By assessing watermarks according to all these key criteria, CEFW offers a thorough evaluation of their practicality and effectiveness. Moreover, we introduce a simple and effective watermarking method called Balanced Watermark (BW), which guarantees robustness and imperceptibility through balancing the way watermark information is added. Extensive experiments show that BW outperforms existing methods in overall performance across all evaluation dimensions. We release our code to the community for future research. this https URL.</li>
</ul>

<h3>Title: Leveraging VAE-Derived Latent Spaces for Enhanced Malware Detection with Machine Learning Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Bamidele Ajayi, Basel Barakat, Ken McGarry</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20803">https://arxiv.org/abs/2503.20803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20803">https://arxiv.org/pdf/2503.20803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20803]] Leveraging VAE-Derived Latent Spaces for Enhanced Malware Detection with Machine Learning Classifiers(https://arxiv.org/abs/2503.20803)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This paper assesses the performance of five machine learning classifiers: Decision Tree, Naive Bayes, LightGBM, Logistic Regression, and Random Forest using latent representations learned by a Variational Autoencoder from malware datasets. Results from the experiments conducted on different training-test splits with different random seeds reveal that all the models perform well in detecting malware with ensemble methods (LightGBM and Random Forest) performing slightly better than the rest. In addition, the use of latent features reduces the computational cost of the model and the need for extensive hyperparameter tuning for improved efficiency of the model for deployment. Statistical tests show that these improvements are significant, and thus, the practical relevance of integrating latent space representation with traditional classifiers for effective malware detection in cybersecurity is established.</li>
</ul>

<h3>Title: AED: Automatic Discovery of Effective and Diverse Vulnerabilities for Autonomous Driving Policy with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Le Qiu, Zelai Xu, Qixin Tan, Wenhao Tang, Chao Yu, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20804">https://arxiv.org/abs/2503.20804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20804">https://arxiv.org/pdf/2503.20804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20804]] AED: Automatic Discovery of Effective and Diverse Vulnerabilities for Autonomous Driving Policy with Large Language Models(https://arxiv.org/abs/2503.20804)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Assessing the safety of autonomous driving policy is of great importance, and reinforcement learning (RL) has emerged as a powerful method for discovering critical vulnerabilities in driving policies. However, existing RL-based approaches often struggle to identify vulnerabilities that are both effective-meaning the autonomous vehicle is genuinely responsible for the accidents-and diverse-meaning they span various failure types. To address these challenges, we propose AED, a framework that uses large language models (LLMs) to automatically discover effective and diverse vulnerabilities in autonomous driving policies. We first utilize an LLM to automatically design reward functions for RL training. Then we let the LLM consider a diverse set of accident types and train adversarial policies for different accident types in parallel. Finally, we use preference-based learning to filter ineffective accidents and enhance the effectiveness of each vulnerability. Experiments across multiple simulated traffic scenarios and tested policies show that AED uncovers a broader range of vulnerabilities and achieves higher attack success rates compared with expert-designed rewards, thereby reducing the need for manual reward engineering and improving the diversity and effectiveness of vulnerability discovery.</li>
</ul>

<h3>Title: SCVI: Bridging Social and Cyber Dimensions for Comprehensive Vulnerability Assessment</h3>
<ul>
<li><strong>Authors: </strong>Shutonu Mitra, Tomas Neguyen, Qi Zhang, Hyungmin Kim, Hossein Salemi, Chen-Wei Chang, Fengxiu Zhang, Michin Hong, Chang-Tien Lu, Hemant Purohit, Jin-Hee Cho</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20806">https://arxiv.org/abs/2503.20806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20806">https://arxiv.org/pdf/2503.20806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20806]] SCVI: Bridging Social and Cyber Dimensions for Comprehensive Vulnerability Assessment(https://arxiv.org/abs/2503.20806)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The rise of cyber threats on social media platforms necessitates advanced metrics to assess and mitigate social cyber vulnerabilities. This paper presents the Social Cyber Vulnerability Index (SCVI), a novel framework integrating individual-level factors (e.g., awareness, behavioral traits, psychological attributes) and attack-level characteristics (e.g., frequency, consequence, sophistication) for comprehensive socio-cyber vulnerability assessment. SCVI is validated using survey data (iPoll) and textual data (Reddit scam reports), demonstrating adaptability across modalities while revealing demographic disparities and regional vulnerabilities. Comparative analyses with the Common Vulnerability Scoring System (CVSS) and the Social Vulnerability Index (SVI) show the superior ability of SCVI to capture nuanced socio-technical risks. Monte Carlo-based weight variability analysis confirms SCVI is robust and highlights its utility in identifying high-risk groups. By addressing gaps in traditional metrics, SCVI offers actionable insights for policymakers and practitioners, advancing inclusive strategies to mitigate emerging threats such as AI-powered phishing and deepfake scams.</li>
</ul>

<h3>Title: Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for Federated Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoming Qi, Jingyang Zhang, Huazhu Fu, Guanyu Yang, Shuo Li, Yueming Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20808">https://arxiv.org/abs/2503.20808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20808">https://arxiv.org/pdf/2503.20808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20808]] Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for Federated Continual Learning(https://arxiv.org/abs/2503.20808)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated continual learning (FCL) offers an emerging pattern to facilitate the applicability of federated learning (FL) in real-world scenarios, where tasks evolve dynamically and asynchronously across clients, especially in medical scenario. Existing server-side FCL methods in nature domain construct a continually learnable server model by client aggregation on all-involved tasks. However, they are challenged by: (1) Catastrophic forgetting for previously learned tasks, leading to error accumulation in server model, making it difficult to sustain comprehensive knowledge across all tasks. (2) Biased optimization due to asynchronous tasks handled across different clients, leading to the collision of optimization targets of different clients at the same time steps. In this work, we take the first step to propose a novel server-side FCL pattern in medical domain, Dynamic Allocation Hypernetwork with adaptive model recalibration (FedDAH). It is to facilitate collaborative learning under the distinct and dynamic task streams across clients. To alleviate the catastrophic forgetting, we propose a dynamic allocation hypernetwork (DAHyper) where a continually updated hypernetwork is designed to manage the mapping between task identities and their associated model parameters, enabling the dynamic allocation of the model across clients. For the biased optimization, we introduce a novel adaptive model recalibration (AMR) to incorporate the candidate changes of historical models into current server updates, and assign weights to identical tasks across different time steps based on the similarity for continual optimization. Extensive experiments on the AMOS dataset demonstrate the superiority of our FedDAH to other FCL methods on sites with different task streams. The code is available:this https URL.</li>
</ul>

<h3>Title: Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy</h3>
<ul>
<li><strong>Authors: </strong>Joonhyun Jeong, Seyun Bae, Yeonsung Jung, Jaeryong Hwang, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20823">https://arxiv.org/abs/2503.20823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20823">https://arxiv.org/pdf/2503.20823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20823]] Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy(https://arxiv.org/abs/2503.20823)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, fair, large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable versatility of Large Language Models (LLMs) and Multimodal LLMs (MLLMs) to generalize across both language and vision tasks, LLMs and MLLMs have shown vulnerability to jailbreaking, generating textual outputs that undermine safety, ethical, and bias standards when exposed to harmful or sensitive inputs. With the recent advancement of safety alignment via preference-tuning from human feedback, LLMs and MLLMs have been equipped with safety guardrails to yield safe, ethical, and fair responses with regard to harmful inputs. However, despite the significance of safety alignment, research on the vulnerabilities remains largely underexplored. In this paper, we investigate the unexplored vulnerability of the safety alignment, examining its ability to consistently provide safety guarantees for out-of-distribution(OOD)-ifying harmful inputs that may fall outside the aligned data distribution. Our key observation is that OOD-ifying the vanilla harmful inputs highly increases the uncertainty of the model to discern the malicious intent within the input, leading to a higher chance of being jailbroken. Exploiting this vulnerability, we propose JOOD, a new Jailbreak framework via OOD-ifying inputs beyond the safety alignment. We explore various off-the-shelf visual and textual transformation techniques for OOD-ifying the harmful inputs. Notably, we observe that even simple mixing-based techniques such as image mixup prove highly effective in increasing the uncertainty of the model, thereby facilitating the bypass of the safety alignment. Experiments across diverse jailbreak scenarios demonstrate that JOOD effectively jailbreaks recent proprietary LLMs and MLLMs such as GPT-4 and o1 with high attack success rate, which previous attack approaches have consistently struggled to jailbreak. Code is available at this https URL.</li>
</ul>

<h3>Title: Multimodal Image Matching based on Frequency-domain Information of Local Energy Response</h3>
<ul>
<li><strong>Authors: </strong>Meng Yang, Jun Chen, Wenping Gong, Longsheng Wei, Xin Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20827">https://arxiv.org/abs/2503.20827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20827">https://arxiv.org/pdf/2503.20827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20827]] Multimodal Image Matching based on Frequency-domain Information of Local Energy Response(https://arxiv.org/abs/2503.20827)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Complicated nonlinear intensity differences, nonlinear local geometric distortions, noises and rotation transformation are main challenges in multimodal image matching. In order to solve these problems, we propose a method based on Frequency-domain Information of Local Energy Response called FILER. The core of FILER is the local energy response model based on frequency-domain information, which can overcome the effect of nonlinear intensity differences. To improve the robustness to local nonlinear geometric distortions and noises, we design a new edge structure enhanced feature detector and convolutional feature weighted descriptor, respectively. In addition, FILER overcomes the sensitivity of the frequency-domain information to the rotation angle and achieves rotation invariance. Extensive experiments multimodal image pairs show that FILER outperforms other state-of-the-art algorithms and has good robustness and universality.</li>
</ul>

<h3>Title: MedSegNet10: A Publicly Accessible Network Repository for Split Federated Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chamani Shiranthika, Zahra Hafezi Kafshgari, Hadi Hadizadeh, Parvaneh Saeedi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20830">https://arxiv.org/abs/2503.20830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20830">https://arxiv.org/pdf/2503.20830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20830]] MedSegNet10: A Publicly Accessible Network Repository for Split Federated Medical Image Segmentation(https://arxiv.org/abs/2503.20830)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Machine Learning (ML) and Deep Learning (DL) have shown significant promise in healthcare, particularly in medical image segmentation, which is crucial for accurate disease diagnosis and treatment planning. Despite their potential, challenges such as data privacy concerns, limited annotated data, and inadequate training data persist. Decentralized learning approaches such as federated learning (FL), split learning (SL), and split federated learning (SplitFed/SFL) address these issues effectively. This paper introduces "MedSegNet10," a publicly accessible repository designed for medical image segmentation using split-federated learning. MedSegNet10 provides a collection of pre-trained neural network architectures optimized for various medical image types, including microscopic images of human blastocysts, dermatoscopic images of skin lesions, and endoscopic images of lesions, polyps, and ulcers, with applications extending beyond these examples. By leveraging SplitFed's benefits, MedSegNet10 allows collaborative training on privately stored, horizontally split data, ensuring privacy and integrity. This repository supports researchers, practitioners, trainees, and data scientists, aiming to advance medical image segmentation while maintaining patient data privacy. The repository is available at: this https URL (password upon request to the authors).</li>
</ul>

<h3>Title: Advancing Vulnerability Classification with BERT: A Multi-Objective Learning Model</h3>
<ul>
<li><strong>Authors: </strong>Himanshu Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20831">https://arxiv.org/abs/2503.20831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20831">https://arxiv.org/pdf/2503.20831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20831]] Advancing Vulnerability Classification with BERT: A Multi-Objective Learning Model(https://arxiv.org/abs/2503.20831)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer</a></li>
<li><strong>Abstract: </strong>The rapid increase in cybersecurity vulnerabilities necessitates automated tools for analyzing and classifying vulnerability reports. This paper presents a novel Vulnerability Report Classifier that leverages the BERT (Bidirectional Encoder Representations from Transformers) model to perform multi-label classification of Common Vulnerabilities and Exposures (CVE) reports from the National Vulnerability Database (NVD). The classifier predicts both the severity (Low, Medium, High, Critical) and vulnerability types (e.g., Buffer Overflow, XSS) from textual descriptions. We introduce a custom training pipeline using a combined loss function-Cross-Entropy for severity and Binary Cross-Entropy with Logits for types-integrated into a Hugging Face Trainer subclass. Experiments on recent NVD data demonstrate promising results, with decreasing evaluation loss across epochs. The system is deployed via a REST API and a Streamlit UI, enabling real-time vulnerability analysis. This work contributes a scalable, open-source solution for cybersecurity practitioners to automate vulnerability triage.</li>
</ul>

<h3>Title: Comprehensive Manuscript Assessment with Text Summarization Using 69707 articles</h3>
<ul>
<li><strong>Authors: </strong>Qichen Sun, Yuxing Lu, Kun Xia, Li Chen, He Sun, Jinzhuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20835">https://arxiv.org/abs/2503.20835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20835">https://arxiv.org/pdf/2503.20835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20835]] Comprehensive Manuscript Assessment with Text Summarization Using 69707 articles(https://arxiv.org/abs/2503.20835)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Rapid and efficient assessment of the future impact of research articles is a significant concern for both authors and reviewers. The most common standard for measuring the impact of academic papers is the number of citations. In recent years, numerous efforts have been undertaken to predict citation counts within various citation windows. However, most of these studies focus solely on a specific academic field or require early citation counts for prediction, rendering them impractical for the early-stage evaluation of papers. In this work, we harness Scopus to curate a significantly comprehensive and large-scale dataset of information from 69707 scientific articles sourced from 99 journals spanning multiple disciplines. We propose a deep learning methodology for the impact-based classification tasks, which leverages semantic features extracted from the manuscripts and paper metadata. To summarize the semantic features, such as titles and abstracts, we employ a Transformer-based language model to encode semantic features and design a text fusion layer to capture shared information between titles and abstracts. We specifically focus on the following impact-based prediction tasks using information of scientific manuscripts in pre-publication stage: (1) The impact of journals in which the manuscripts will be published. (2) The future impact of manuscripts themselves. Extensive experiments on our datasets demonstrate the superiority of our proposed model for impact-based prediction tasks. We also demonstrate potentials in generating manuscript's feedback and improvement suggestions.</li>
</ul>

<h3>Title: Named Entity Recognition in Context</h3>
<ul>
<li><strong>Authors: </strong>Colin Brisson (CRCAO), Ayoub Kahfy, Marc Bui (AOROC), Frédéric Constant (ERMES)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20836">https://arxiv.org/abs/2503.20836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20836">https://arxiv.org/pdf/2503.20836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20836]] Named Entity Recognition in Context(https://arxiv.org/abs/2503.20836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>We present the Named Entity Recognition system developed by the Edit Dunhuang team for the EvaHan2025 competition. Our approach integrates three core components: (1) Pindola, a modern transformer-based bidirectional encoder pretrained on a large corpus of Classical Chinese texts; (2) a retrieval module that fetches relevant external context for each target sequence; and (3) a generative reasoning step that summarizes retrieved context in Classical Chinese for more robust entity disambiguation. Using this approach, we achieve an average F1 score of 85.58, improving upon the competition baseline by nearly 5 points.</li>
</ul>

<h3>Title: Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, Yong Cui, Yue Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20844">https://arxiv.org/abs/2503.20844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20844">https://arxiv.org/pdf/2503.20844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20844]] Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks(https://arxiv.org/abs/2503.20844)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (DRL) has emerged as a promising approach for robotic control, but its realworld deployment remains challenging due to its vulnerability to environmental perturbations. Existing white-box adversarial attack methods, adapted from supervised learning, fail to effectively target DRL agents as they overlook temporal dynamics and indiscriminately perturb all state dimensions, limiting their impact on long-term rewards. To address these challenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR) Attack, a white-box attack method that combines DRL with a gradient-based soft masking mechanism to dynamically identify critical state dimensions and optimize adversarial policies. AGMR selectively allocates perturbations to the most impactful state features and incorporates a dynamic adjustment mechanism to balance exploration and exploitation during training. Extensive experiments demonstrate that AGMR outperforms state-of-the-art adversarial attack methods in degrading the performance of the victim agent and enhances the victim agent's robustness through adversarial defense mechanisms.</li>
</ul>

<h3>Title: Generating Synthetic Data with Formal Privacy Guarantees: State of the Art and the Road Ahead</h3>
<ul>
<li><strong>Authors: </strong>Viktor Schlegel, Anil A Bharath, Zilong Zhao, Kevin Yee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20846">https://arxiv.org/abs/2503.20846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20846">https://arxiv.org/pdf/2503.20846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20846]] Generating Synthetic Data with Formal Privacy Guarantees: State of the Art and the Road Ahead(https://arxiv.org/abs/2503.20846)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, generative</a></li>
<li><strong>Abstract: </strong>Privacy-preserving synthetic data offers a promising solution to harness segregated data in high-stakes domains where information is compartmentalized for regulatory, privacy, or institutional reasons. This survey provides a comprehensive framework for understanding the landscape of privacy-preserving synthetic data, presenting the theoretical foundations of generative models and differential privacy followed by a review of state-of-the-art methods across tabular data, images, and text. Our synthesis of evaluation approaches highlights the fundamental trade-off between utility for down-stream tasks and privacy guarantees, while identifying critical research gaps: the lack of realistic benchmarks representing specialized domains and insufficient empirical evaluations required to contextualise formal guarantees. Through empirical analysis of four leading methods on five real-world datasets from specialized domains, we demonstrate significant performance degradation under realistic privacy constraints ($\epsilon \leq 4$), revealing a substantial gap between results reported on general domain benchmarks and performance on domain-specific data. %Our findings highlight key challenges including unaccounted privacy leakage, insufficient empirical verification of formal guarantees, and a critical deficit of realistic benchmarks. These challenges underscore the need for robust evaluation frameworks, standardized benchmarks for specialized domains, and improved techniques to address the unique requirements of privacy-sensitive fields such that this technology can deliver on its considerable potential.</li>
</ul>

<h3>Title: Unified Multimodal Discrete Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, Katerina Fragkiadaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20853">https://arxiv.org/abs/2503.20853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20853">https://arxiv.org/pdf/2503.20853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20853]] Unified Multimodal Discrete Diffusion(https://arxiv.org/abs/2503.20853)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in text generation. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), and greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model which is capable of jointly understanding and generating text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models, performing a scaling analysis and demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, enhanced controllability, editability, inpainting, and flexible trade-off between inference time and generation quality. Code and additional visualizations are available at this https URL.</li>
</ul>

<h3>Title: VinaBench: Benchmark for Faithful and Consistent Visual Narratives</h3>
<ul>
<li><strong>Authors: </strong>Silin Gao, Sheryl Mathew, Li Mi, Sepideh Mamooler, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Syrielle Montariol, Antoine Bosselut</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20871">https://arxiv.org/abs/2503.20871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20871">https://arxiv.org/pdf/2503.20871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20871]] VinaBench: Benchmark for Faithful and Consistent Visual Narratives(https://arxiv.org/abs/2503.20871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual narrative generation transforms textual narratives into sequences of images illustrating the content of the text. However, generating visual narratives that are faithful to the input text and self-consistent across generated images remains an open challenge, due to the lack of knowledge constraints used for planning the stories. In this work, we propose a new benchmark, VinaBench, to address this challenge. Our benchmark annotates the underlying commonsense and discourse constraints in visual narrative samples, offering systematic scaffolds for learning the implicit strategies of visual storytelling. Based on the incorporated narrative constraints, we further propose novel metrics to closely evaluate the consistency of generated narrative images and the alignment of generations with the input textual narrative. Our results across three generative vision models demonstrate that learning with VinaBench's knowledge constraints effectively improves the faithfulness and cohesion of generated visual narratives.</li>
</ul>

<h3>Title: BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology</h3>
<ul>
<li><strong>Authors: </strong>Amaya Gallagher-Syed, Henry Senior, Omnia Alwazzan, Elena Pontarini, Michele Bombardieri, Costantino Pitzalis, Myles J. Lewis, Michael R. Barnes, Luca Rossi, Gregory Slabaugh</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.CB, q-bio.QM, q-bio.TO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20880">https://arxiv.org/abs/2503.20880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20880">https://arxiv.org/pdf/2503.20880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20880]] BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology(https://arxiv.org/abs/2503.20880)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The development of biologically interpretable and explainable models remains a key challenge in computational pathology, particularly for multistain immunohistochemistry (IHC) analysis. We present BioX-CPath, an explainable graph neural network architecture for whole slide image (WSI) classification that leverages both spatial and semantic features across multiple stains. At its core, BioX-CPath introduces a novel Stain-Aware Attention Pooling (SAAP) module that generates biologically meaningful, stain-aware patient embeddings. Our approach achieves state-of-the-art performance on both Rheumatoid Arthritis and Sjogren's Disease multistain datasets. Beyond performance metrics, BioX-CPath provides interpretable insights through stain attention scores, entropy measures, and stain interaction scores, that permit measuring model alignment with known pathological mechanisms. This biological grounding, combined with strong classification performance, makes BioX-CPath particularly suitable for clinical applications where interpretability is key. Source code and documentation can be found at: this https URL.</li>
</ul>

<h3>Title: Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework</h3>
<ul>
<li><strong>Authors: </strong>Usama Zafar, André Teixeira, Salman Toor</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20884">https://arxiv.org/abs/2503.20884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20884">https://arxiv.org/pdf/2503.20884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20884]] Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework(https://arxiv.org/abs/2503.20884)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, federate, generative</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training across decentralized devices without sharing raw data, but it remains vulnerable to poisoning attacks that compromise model integrity. Existing defenses often rely on external datasets or predefined heuristics (e.g. number of malicious clients), limiting their effectiveness and scalability. To address these limitations, we propose a privacy-preserving defense framework that leverages a Conditional Generative Adversarial Network (cGAN) to generate synthetic data at the server for authenticating client updates, eliminating the need for external datasets. Our framework is scalable, adaptive, and seamlessly integrates into FL workflows. Extensive experiments on benchmark datasets demonstrate its robust performance against a variety of poisoning attacks, achieving high True Positive Rate (TPR) and True Negative Rate (TNR) of malicious and benign clients, respectively, while maintaining model accuracy. The proposed framework offers a practical and effective solution for securing federated learning systems.</li>
</ul>

<h3>Title: Feature Modulation for Semi-Supervised Domain Generalization without Domain Labels</h3>
<ul>
<li><strong>Authors: </strong>Venuri Amarasinghe (1), Asini Jayakody (1), Isun Randila (1), Kalinga Bandara (1), Chamuditha Jayanga Galappaththige (2), Ranga Rodrigo (1) ((1) University of Moratuwa, (2) Queensland University of Technology)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20897">https://arxiv.org/abs/2503.20897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20897">https://arxiv.org/pdf/2503.20897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20897]] Feature Modulation for Semi-Supervised Domain Generalization without Domain Labels(https://arxiv.org/abs/2503.20897)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Semi-supervised domain generalization (SSDG) leverages a small fraction of labeled data alongside unlabeled data to enhance model generalization. Most of the existing SSDG methods rely on pseudo-labeling (PL) for unlabeled data, often assuming access to domain labels-a privilege not always available. However, domain shifts introduce domain noise, leading to inconsistent PLs that degrade model performance. Methods derived from FixMatch suffer particularly from lower PL accuracy, reducing the effectiveness of unlabeled data. To address this, we tackle the more challenging domain-label agnostic SSDG, where domain labels for unlabeled data are not available during training. First, we propose a feature modulation strategy that enhances class-discriminative features while suppressing domain-specific information. This modulation shifts features toward Similar Average Representations-a modified version of class prototypes-that are robust across domains, encouraging the classifier to distinguish between closely related classes and feature extractor to form tightly clustered, domain-invariant representations. Second, to mitigate domain noise and improve pseudo-label accuracy, we introduce a loss-scaling function that dynamically lowers the fixed confidence threshold for pseudo-labels, optimizing the use of unlabeled data. With these key innovations, our approach achieves significant improvements on four major domain generalization benchmarks-even without domain labels. We will make the code available.</li>
</ul>

<h3>Title: Assessing Generative Models for Structured Data</h3>
<ul>
<li><strong>Authors: </strong>Reilly Cannon, Nicolette M. Laird, Caesar Vazquez, Andy Lin, Amy Wagler, Tony Chiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20903">https://arxiv.org/abs/2503.20903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20903">https://arxiv.org/pdf/2503.20903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20903]] Assessing Generative Models for Structured Data(https://arxiv.org/abs/2503.20903)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data generation has emerged as a promising method to address limited data availability and privacy concerns. With the sharp increase in the performance of large language models in recent years, researchers have been interested in applying these models to the generation of tabular data. However, little is known about the quality of the generated tabular data from large language models. The predominant method for assessing the quality of synthetic tabular data is the train-synthetic-test-real approach, where the artificial examples are compared to the original by how well machine learning models, trained separately on the real and synthetic sets, perform in some downstream tasks. This method does not directly measure how closely the distribution of generated data approximates that of the original. This paper introduces rigorous methods for directly assessing synthetic tabular data against real data by looking at inter-column dependencies within the data. We find that large language models (GPT-2), both when queried via few-shot prompting and when fine-tuned, and GAN (CTGAN) models do not produce data with dependencies that mirror the original real data. Results from this study can inform future practice in synthetic data generation to improve data quality.</li>
</ul>

<h3>Title: GatedxLSTM: A Multimodal Affective Computing Approach for Emotion Recognition in Conversations</h3>
<ul>
<li><strong>Authors: </strong>Yupei Li, Qiyang Sun, Sunil Munthumoduku Krishna Murthy, Emran Alturki, Björn W. Schuller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20919">https://arxiv.org/abs/2503.20919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20919">https://arxiv.org/pdf/2503.20919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20919]] GatedxLSTM: A Multimodal Affective Computing Approach for Emotion Recognition in Conversations(https://arxiv.org/abs/2503.20919)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Affective Computing (AC) is essential for advancing Artificial General Intelligence (AGI), with emotion recognition serving as a key component. However, human emotions are inherently dynamic, influenced not only by an individual's expressions but also by interactions with others, and single-modality approaches often fail to capture their full dynamics. Multimodal Emotion Recognition (MER) leverages multiple signals but traditionally relies on utterance-level analysis, overlooking the dynamic nature of emotions in conversations. Emotion Recognition in Conversation (ERC) addresses this limitation, yet existing methods struggle to align multimodal features and explain why emotions evolve within dialogues. To bridge this gap, we propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly considers voice and transcripts of both the speaker and their conversational partner(s) to identify the most influential sentences driving emotional shifts. By integrating Contrastive Language-Audio Pretraining (CLAP) for improved cross-modal alignment and employing a gating mechanism to emphasise emotionally impactful utterances, GatedxLSTM enhances both interpretability and performance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion predictions by modelling contextual dependencies. Experiments on the IEMOCAP dataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA) performance among open-source methods in four-class emotion classification. These results validate its effectiveness for ERC applications and provide an interpretability analysis from a psychological perspective.</li>
</ul>

<h3>Title: Prototype Guided Backdoor Defense</h3>
<ul>
<li><strong>Authors: </strong>Venkat Adithya Amula, Sunayana Samavedam, Saurabh Saini, Avani Gupta, Narayanan P J</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20925">https://arxiv.org/abs/2503.20925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20925">https://arxiv.org/pdf/2503.20925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20925]] Prototype Guided Backdoor Defense(https://arxiv.org/abs/2503.20925)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Deep learning models are susceptible to {\em backdoor attacks} involving malicious attackers perturbing a small subset of training data with a {\em trigger} to causes misclassifications. Various triggers have been used, including semantic triggers that are easily realizable without requiring the attacker to manipulate the image. The emergence of generative AI has eased the generation of varied poisoned samples. Robustness across types of triggers is crucial to effective defense. We propose Prototype Guided Backdoor Defense (PGBD), a robust post-hoc defense that scales across different trigger types, including previously unsolved semantic triggers. PGBD exploits displacements in the geometric spaces of activations to penalize movements toward the trigger. This is done using a novel sanitization loss of a post-hoc fine-tuning step. The geometric approach scales easily to all types of attacks. PGBD achieves better performance across all settings. We also present the first defense against a new semantic attack on celebrity face images. Project page: \hyperlink{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Hacia la interpretabilidad de la detección anticipada de riesgos de depresión utilizando grandes modelos de lenguaje</h3>
<ul>
<li><strong>Authors: </strong>Horacio Thompson, Maximiliano Sapino, Edgardo Ferretti, Marcelo Errecalde</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20939">https://arxiv.org/abs/2503.20939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20939">https://arxiv.org/pdf/2503.20939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20939]] Hacia la interpretabilidad de la detección anticipada de riesgos de depresión utilizando grandes modelos de lenguaje(https://arxiv.org/abs/2503.20939)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Early Detection of Risks (EDR) on the Web involves identifying at-risk users as early as possible. Although Large Language Models (LLMs) have proven to solve various linguistic tasks efficiently, assessing their reasoning ability in specific domains is crucial. In this work, we propose a method for solving depression-related EDR using LLMs on Spanish texts, with responses that can be interpreted by humans. We define a reasoning criterion to analyze users through a specialist, apply in-context learning to the Gemini model, and evaluate its performance both quantitatively and qualitatively. The results show that accurate predictions can be obtained, supported by explanatory reasoning, providing a deeper understanding of the solution. Our approach offers new perspectives for addressing EDR problems by leveraging the power of LLMs.</li>
</ul>

<h3>Title: TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models</h3>
<ul>
<li><strong>Authors: </strong>Caspar Meijer, Jiyue Huang, Shreshtha Sharma, Elena Lazovik, Lydia Y. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20952">https://arxiv.org/abs/2503.20952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20952">https://arxiv.org/pdf/2503.20952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20952]] TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models(https://arxiv.org/abs/2503.20952)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) for time series forecasting (TSF) enables clients with privacy-sensitive time series (TS) data to collaboratively learn accurate forecasting models, for example, in energy load prediction. Unfortunately, privacy risks in FL persist, as servers can potentially reconstruct clients' training data through gradient inversion attacks (GIA). Although GIA is demonstrated for image classification tasks, little is known about time series regression tasks. In this paper, we first conduct an extensive empirical study on inverting TS data across 4 TSF models and 4 datasets, identifying the unique challenges of reconstructing both observations and targets of TS data. We then propose TS-Inverse, a novel GIA that improves the inversion of TS data by (i) learning a gradient inversion model that outputs quantile predictions, (ii) a unique loss function that incorporates periodicity and trend regularization, and (iii) regularization according to the quantile predictions. Our evaluations demonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x improvement in terms of the sMAPE metric over existing GIA methods on TS data. Code repository: this https URL</li>
</ul>

<h3>Title: Sociotechnical Effects of Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Joss Moorkens, Andy Way, Séamus Lankford</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20959">https://arxiv.org/abs/2503.20959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20959">https://arxiv.org/pdf/2503.20959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20959]] Sociotechnical Effects of Machine Translation(https://arxiv.org/abs/2503.20959)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While the previous chapters have shown how machine translation (MT) can be useful, in this chapter we discuss some of the side-effects and risks that are associated, and how they might be mitigated. With the move to neural MT and approaches using Large Language Models (LLMs), there is an associated impact on climate change, as the models built by multinational corporations are massive. They are hugely expensive to train, consume large amounts of electricity, and output huge volumes of kgCO2 to boot. However, smaller models which still perform to a high level of quality can be built with much lower carbon footprints, and tuning pre-trained models saves on the requirement to train from scratch. We also discuss the possible detrimental effects of MT on translators and other users. The topics of copyright and ownership of data are discussed, as well as ethical considerations on data and MT use. Finally, we show how if done properly, using MT in crisis scenarios can save lives, and we provide a method of how this might be done.</li>
</ul>

<h3>Title: Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>David Wong, Bin Wang, Gorkem Durak, Marouane Tliba, Akshay Chaudhari, Aladine Chetouani, Ahmet Enis Cetin, Cagdas Topel, Nicolo Gennaro, Camila Lopes Vendrami, Tugce Agirlar Trabzonlu, Amir Ali Rahsepar, Laetitia Perronne, Matthew Antalek, Onural Ozturk, Gokcan Okur, Andrew C. Gordon, Ayis Pyrros, Frank H. Miller, Amir Borhani, Hatice Savas, Eric Hart, Drew Torigian, Jayaram K. Udupa, Elizabeth Krupinski, Ulas Bagci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20967">https://arxiv.org/abs/2503.20967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20967">https://arxiv.org/pdf/2503.20967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20967]] Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging(https://arxiv.org/abs/2503.20967)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The demand for high-quality synthetic data for model training and augmentation has never been greater in medical imaging. However, current evaluations predominantly rely on computational metrics that fail to align with human expert recognition. This leads to synthetic images that may appear realistic numerically but lack clinical authenticity, posing significant challenges in ensuring the reliability and effectiveness of AI-driven medical tools. To address this gap, we introduce GazeVal, a practical framework that synergizes expert eye-tracking data with direct radiological evaluations to assess the quality of synthetic medical images. GazeVal leverages gaze patterns of radiologists as they provide a deeper understanding of how experts perceive and interact with synthetic data in different tasks (i.e., diagnostic or Turing tests). Experiments with sixteen radiologists revealed that 96.6% of the generated images (by the most recent state-of-the-art AI algorithm) were identified as fake, demonstrating the limitations of generative AI in producing clinically accurate images.</li>
</ul>

<h3>Title: Generator Cost Coefficients Inference Attack via Exploitation of Locational Marginal Prices in Smart Grid</h3>
<ul>
<li><strong>Authors: </strong>Junfei Wang, Pirathayini Srikantha</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20976">https://arxiv.org/abs/2503.20976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20976">https://arxiv.org/pdf/2503.20976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20976]] Generator Cost Coefficients Inference Attack via Exploitation of Locational Marginal Prices in Smart Grid(https://arxiv.org/abs/2503.20976)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Real-time price signals and power generation levels (disaggregated or aggregated) are commonly made available to the public by Independent System Operators (ISOs) to promote efficiency and transparency. However, they may inadvertently reveal crucial private information about the power grid, such as the cost functions of generators. Adversaries can exploit these vulnerabilities for strategic bidding, potentially leading to financial losses for power market participants and consumers. In this paper, we prove the existence of a closed-form solution for recovering coefficients in cost functions when LMPs and disaggregated power generation data are available. Additionally, we establish the convergence conditions for inference the quadratic coefficients of cost functions when LMPs and aggregated generation data are given. Our theoretical analysis provides the conditions under which the algorithm is guaranteed to converge, and our experiments demonstrate the efficacy of this method on IEEE benchmark systems, including 14-bus and 30-bus and 118-bus systems.</li>
</ul>

<h3>Title: ScreenLLM: Stateful Screen Schema for Efficient Action Understanding and Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yiqiao Jin, Stefano Petrangeli, Yu Shen, Gang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20978">https://arxiv.org/abs/2503.20978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20978">https://arxiv.org/pdf/2503.20978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20978]] ScreenLLM: Stateful Screen Schema for Efficient Action Understanding and Prediction(https://arxiv.org/abs/2503.20978)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Graphical User Interface (GUI) agents are autonomous systems that interpret and generate actions, enabling intelligent user assistance and automation. Effective training of these agent presents unique challenges, such as sparsity in supervision signals, scalability for large datasets, and the need for nuanced user understanding. We propose stateful screen schema, an efficient representation of GUI interactions that captures key user actions and intentions over time. Building on this foundation, we introduce ScreenLLM, a set of multimodal large language models (MLLMs) tailored for advanced UI understanding and action prediction. Extensive experiments on both open-source and proprietary models show that ScreenLLM accurately models user behavior and predicts actions. Our work lays the foundation for scalable, robust, and intelligent GUI agents that enhance user interaction in diverse software environments.</li>
</ul>

<h3>Title: Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction</h3>
<ul>
<li><strong>Authors: </strong>Xiaoran Xu, Zhaoqian Xue, Chi Zhang, Jhonatan Medri, Junjie Xiong, Jiayan Zhou, Jin Jin, Yongfeng Zhang, Siyuan Ma, Lingyao Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20981">https://arxiv.org/abs/2503.20981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20981">https://arxiv.org/pdf/2503.20981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20981]] Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction(https://arxiv.org/abs/2503.20981)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Investigating the public experience of urgent care facilities is essential for promoting community healthcare development. Traditional survey methods often fall short due to limited scope, time, and spatial coverage. Crowdsourcing through online reviews or social media offers a valuable approach to gaining such insights. With recent advancements in large language models (LLMs), extracting nuanced perceptions from reviews has become feasible. This study collects Google Maps reviews across the DMV and Florida areas and conducts prompt engineering with the GPT model to analyze the aspect-based sentiment of urgent care. We first analyze the geospatial patterns of various aspects, including interpersonal factors, operational efficiency, technical quality, finances, and facilities. Next, we determine Census Block Group(CBG)-level characteristics underpinning differences in public perception, including population density, median income, GINI Index, rent-to-income ratio, household below poverty rate, no insurance rate, and unemployment rate. Our results show that interpersonal factors and operational efficiency emerge as the strongest determinants of patient satisfaction in urgent care, while technical quality, finances, and facilities show no significant independent effects when adjusted for in multivariate models. Among socioeconomic and demographic factors, only population density demonstrates a significant but modest association with patient ratings, while the remaining factors exhibit no significant correlations. Overall, this study highlights the potential of crowdsourcing to uncover the key factors that matter to residents and provide valuable insights for stakeholders to improve public satisfaction with urgent care.</li>
</ul>

<h3>Title: Cross-Modal State-Space Graph Reasoning for Structured Summarization</h3>
<ul>
<li><strong>Authors: </strong>Hannah Kim, Sofia Martinez, Jason Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20988">https://arxiv.org/abs/2503.20988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20988">https://arxiv.org/pdf/2503.20988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20988]] Cross-Modal State-Space Graph Reasoning for Structured Summarization(https://arxiv.org/abs/2503.20988)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The ability to extract compact, meaningful summaries from large-scale and multimodal data is critical for numerous applications, ranging from video analytics to medical reports. Prior methods in cross-modal summarization have often suffered from high computational overheads and limited interpretability. In this paper, we propose a \textit{Cross-Modal State-Space Graph Reasoning} (\textbf{CSS-GR}) framework that incorporates a state-space model with graph-based message passing, inspired by prior work on efficient state-space models. Unlike existing approaches relying on purely sequential models, our method constructs a graph that captures inter- and intra-modal relationships, allowing more holistic reasoning over both textual and visual streams. We demonstrate that our approach significantly improves summarization quality and interpretability while maintaining computational efficiency, as validated on standard multimodal summarization benchmarks. We also provide a thorough ablation study to highlight the contributions of each component.</li>
</ul>

<h3>Title: MVFNet: Multipurpose Video Forensics Network using Multiple Forms of Forensic Evidence</h3>
<ul>
<li><strong>Authors: </strong>Tai D. Nguyen, Matthew C. Stamm</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20991">https://arxiv.org/abs/2503.20991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20991">https://arxiv.org/pdf/2503.20991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20991]] MVFNet: Multipurpose Video Forensics Network using Multiple Forms of Forensic Evidence(https://arxiv.org/abs/2503.20991)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While videos can be falsified in many different ways, most existing forensic networks are specialized to detect only a single manipulation type (e.g. deepfake, inpainting). This poses a significant issue as the manipulation used to falsify a video is not known a priori. To address this problem, we propose MVFNet - a multipurpose video forensics network capable of detecting multiple types of manipulations including inpainting, deepfakes, splicing, and editing. Our network does this by extracting and jointly analyzing a broad set of forensic feature modalities that capture both spatial and temporal anomalies in falsified videos. To reliably detect and localize fake content of all shapes and sizes, our network employs a novel Multi-Scale Hierarchical Transformer module to identify forensic inconsistencies across multiple spatial scales. Experimental results show that our network obtains state-of-the-art performance in general scenarios where multiple different manipulations are possible, and rivals specialized detectors in targeted scenarios.</li>
</ul>

<h3>Title: Deep Learning for Forensic Identification of Source</h3>
<ul>
<li><strong>Authors: </strong>Cole Patten, Christopher Saunders, Michael Puthawala</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20994">https://arxiv.org/abs/2503.20994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20994">https://arxiv.org/pdf/2503.20994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20994]] Deep Learning for Forensic Identification of Source(https://arxiv.org/abs/2503.20994)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We used contrastive neural networks to learn useful similarity scores between the 144 cartridge casings in the NBIDE dataset, under the common-but-unknown source paradigm. The common-but-unknown source problem is a problem archetype in forensics where the question is whether two objects share a common source (e.g. were two cartridge casings fired from the same firearm). Similarity scores are often used to interpret evidence under this paradigm. We directly compared our results to a state-of-the-art algorithm, Congruent Matching Cells (CMC). When trained on the E3 dataset of 2967 cartridge casings, contrastive learning achieved an ROC AUC of 0.892. The CMC algorithm achieved 0.867. We also conducted an ablation study where we varied the neural network architecture; specifically, the network's width or depth. The ablation study showed that contrastive network performance results are somewhat robust to the network architecture. This work was in part motivated by the use of similarity scores attained via contrastive learning for standard evidence interpretation methods such as score-based likelihood ratios.</li>
</ul>

<h3>Title: Multi-head Reward Aggregation Guided by Entropy</h3>
<ul>
<li><strong>Authors: </strong>Xiaomin Li, Xupeng Chen, Jingxuan Fan, Eric Hanchen Jiang, Mingye Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.20995">https://arxiv.org/abs/2503.20995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.20995">https://arxiv.org/pdf/2503.20995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.20995]] Multi-head Reward Aggregation Guided by Entropy(https://arxiv.org/abs/2503.20995)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with safety guidelines typically involves reinforcement learning from human feedback (RLHF), relying on human-generated preference annotations. However, assigning consistent overall quality ratings is challenging, prompting recent research to shift towards detailed evaluations based on multiple specific safety criteria. This paper uncovers a consistent observation: safety rules characterized by high rating entropy are generally less reliable in identifying responses preferred by humans. Leveraging this finding, we introduce ENCORE, a straightforward entropy-guided approach that composes multi-head rewards by downweighting rules exhibiting high rating entropy. Theoretically, we demonstrate that rules with elevated entropy naturally receive minimal weighting in the Bradley-Terry optimization framework, justifying our entropy-based penalization. Through extensive experiments on RewardBench safety tasks, our method significantly surpasses several competitive baselines, including random weighting, uniform weighting, single-head Bradley-Terry models, and LLM-based judging methods. Our proposed approach is training-free, broadly applicable to various datasets, and maintains interpretability, offering a practical and effective solution for multi-attribute reward modeling.</li>
</ul>

<h3>Title: Forensic Self-Descriptions Are All You Need for Zero-Shot Detection, Open-Set Source Attribution, and Clustering of AI-generated Images</h3>
<ul>
<li><strong>Authors: </strong>Tai D. Nguyen, Aref Azizpour, Matthew C. Stamm</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21003">https://arxiv.org/abs/2503.21003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21003">https://arxiv.org/pdf/2503.21003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21003]] Forensic Self-Descriptions Are All You Need for Zero-Shot Detection, Open-Set Source Attribution, and Clustering of AI-generated Images(https://arxiv.org/abs/2503.21003)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The emergence of advanced AI-based tools to generate realistic images poses significant challenges for forensic detection and source attribution, especially as new generative techniques appear rapidly. Traditional methods often fail to generalize to unseen generators due to reliance on features specific to known sources during training. To address this problem, we propose a novel approach that explicitly models forensic microstructures - subtle, pixel-level patterns unique to the image creation process. Using only real images in a self-supervised manner, we learn a set of diverse predictive filters to extract residuals that capture different aspects of these microstructures. By jointly modeling these residuals across multiple scales, we obtain a compact model whose parameters constitute a unique forensic self-description for each image. This self-description enables us to perform zero-shot detection of synthetic images, open-set source attribution of images, and clustering based on source without prior knowledge. Extensive experiments demonstrate that our method achieves superior accuracy and adaptability compared to competing techniques, advancing the state of the art in synthetic media forensics.</li>
</ul>

<h3>Title: Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Alwakeel, Emory Buck, Jonathan G. Martin, Imran Aslam, Sudarshan Rajagopal, Jian Pei, Mihai V. Podgoreanu, Christopher J. Lindsell, An-Kwok Ian Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21004">https://arxiv.org/abs/2503.21004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21004">https://arxiv.org/pdf/2503.21004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21004]] Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters(https://arxiv.org/abs/2503.21004)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Pulmonary embolism (PE) is a leading cause of cardiovascular mortality, yet our understanding of optimal management remains limited due to heterogeneous and inaccessible radiology documentation. The PERT Consortium registry standardizes PE management data but depends on resource-intensive manual abstraction. Large language models (LLMs) offer a scalable alternative for automating concept extraction from computed tomography PE (CTPE) reports. This study evaluated the accuracy of LLMs in extracting PE-related concepts compared to a human-curated criterion standard. We retrospectively analyzed MIMIC-IV and Duke Health CTPE reports using multiple LLaMA models. Larger models (70B) outperformed smaller ones (8B), achieving kappa values of 0.98 (PE detection), 0.65-0.75 (PE location), 0.48-0.51 (right heart strain), and 0.65-0.70 (image artifacts). Moderate temperature tuning (0.2-0.5) improved accuracy, while excessive in-context examples reduced performance. A dual-model review framework achieved >80-90% precision. LLMs demonstrate strong potential for automating PE registry abstraction, minimizing manual workload while preserving accuracy.</li>
</ul>

<h3>Title: Can Large Language Models Predict Associations Among Human Attitudes?</h3>
<ul>
<li><strong>Authors: </strong>Ana Ma, Derek Powell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21011">https://arxiv.org/abs/2503.21011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21011">https://arxiv.org/pdf/2503.21011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21011]] Can Large Language Models Predict Associations Among Human Attitudes?(https://arxiv.org/abs/2503.21011)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prior work has shown that large language models (LLMs) can predict human attitudes based on other attitudes, but this work has largely focused on predictions from highly similar and interrelated attitudes. In contrast, human attitudes are often strongly associated even across disparate and dissimilar topics. Using a novel dataset of human responses toward diverse attitude statements, we found that a frontier language model (GPT-4o) was able to recreate the pairwise correlations among individual attitudes and to predict individuals' attitudes from one another. Crucially, in an advance over prior work, we tested GPT-4o's ability to predict in the absence of surface-similarity between attitudes, finding that while surface similarity improves prediction accuracy, the model was still highly-capable of generating meaningful social inferences between dissimilar attitudes. Altogether, our findings indicate that LLMs capture crucial aspects of the deeper, latent structure of human belief systems.</li>
</ul>

<h3>Title: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Chi-Hsi Kung, Frangil Ramirez, Juhyung Ha, Yi-Ting Chen, David Crandall, Yi-Hsuan Tsai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21055">https://arxiv.org/abs/2503.21055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21055">https://arxiv.org/pdf/2503.21055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21055]] What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning(https://arxiv.org/abs/2503.21055)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Understanding a procedural activity requires modeling both how action steps transform the scene, and how evolving scene transformations can influence the sequence of action steps, even those that are accidental or erroneous. Existing work has studied procedure-aware video representations by proposing novel approaches such as modeling the temporal order of actions and has not explicitly learned the state changes (scene transformations). In this work, we study procedure-aware video representation learning by incorporating state-change descriptions generated by Large Language Models (LLMs) as supervision signals for video encoders. Moreover, we generate state-change counterfactuals that simulate hypothesized failure outcomes, allowing models to learn by imagining the unseen ``What if'' scenarios. This counterfactual reasoning facilitates the model's ability to understand the cause and effect of each step in an activity. To verify the procedure awareness of our model, we conduct extensive experiments on procedure-aware tasks, including temporal action segmentation and error detection. Our results demonstrate the effectiveness of the proposed state-change descriptions and their counterfactuals and achieve significant improvements on multiple tasks. We will make our source code and data publicly available soon.</li>
</ul>

<h3>Title: Online Reasoning Video Segmentation with Just-in-Time Digital Twins</h3>
<ul>
<li><strong>Authors: </strong>Yiqing Shen, Bohan Liu, Chenjia Li, Lalithkumar Seenivasan, Mathias Unberath</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21056">https://arxiv.org/abs/2503.21056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21056">https://arxiv.org/pdf/2503.21056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21056]] Online Reasoning Video Segmentation with Just-in-Time Digital Twins(https://arxiv.org/abs/2503.21056)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Reasoning segmentation (RS) aims to identify and segment objects of interest based on implicit text queries. As such, RS is a catalyst for embodied AI agents, enabling them to interpret high-level commands without requiring explicit step-by-step guidance. However, current RS approaches rely heavily on the visual perception capabilities of multimodal large language models (LLMs), leading to several major limitations. First, they struggle with queries that require multiple steps of reasoning or those that involve complex spatial/temporal relationships. Second, they necessitate LLM fine-tuning, which may require frequent updates to maintain compatibility with contemporary LLMs and may increase risks of catastrophic forgetting during fine-tuning. Finally, being primarily designed for static images or offline video processing, they scale poorly to online video data. To address these limitations, we propose an agent framework that disentangles perception and reasoning for online video RS without LLM fine-tuning. Our innovation is the introduction of a just-in-time digital twin concept, where -- given an implicit query -- a LLM plans the construction of a low-level scene representation from high-level video using specialist vision models. We refer to this approach to creating a digital twin as "just-in-time" because the LLM planner will anticipate the need for specific information and only request this limited subset instead of always evaluating every specialist model. The LLM then performs reasoning on this digital twin representation to identify target objects. To evaluate our approach, we introduce a new comprehensive video reasoning segmentation benchmark comprising 200 videos with 895 implicit text queries. The benchmark spans three reasoning categories (semantic, spatial, and temporal) with three different reasoning chain complexity.</li>
</ul>

<h3>Title: Efficient Multi-Instance Generation with Janus-Pro-Dirven Prompt Parsing</h3>
<ul>
<li><strong>Authors: </strong>Fan Qi, Yu Duan, Changsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21069">https://arxiv.org/abs/2503.21069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21069">https://arxiv.org/pdf/2503.21069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21069]] Efficient Multi-Instance Generation with Janus-Pro-Dirven Prompt Parsing(https://arxiv.org/abs/2503.21069)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in text-guided diffusion models have revolutionized conditional image generation, yet they struggle to synthesize complex scenes with multiple objects due to imprecise spatial grounding and limited scalability. We address these challenges through two key modules: 1) Janus-Pro-driven Prompt Parsing, a prompt-layout parsing module that bridges text understanding and layout generation via a compact 1B-parameter architecture, and 2) MIGLoRA, a parameter-efficient plug-in integrating Low-Rank Adaptation (LoRA) into UNet (SD1.5) and DiT (SD3) backbones. MIGLoRA is capable of preserving the base model's parameters and ensuring plug-and-play adaptability, minimizing architectural intrusion while enabling efficient fine-tuning. To support a comprehensive evaluation, we create DescripBox and DescripBox-1024, benchmarks that span diverse scenes and resolutions. The proposed method achieves state-of-the-art performance on COCO and LVIS benchmarks while maintaining parameter efficiency, demonstrating superior layout fidelity and scalability for open-world synthesis.</li>
</ul>

<h3>Title: Purifying Approximate Differential Privacy with Randomized Post-processing</h3>
<ul>
<li><strong>Authors: </strong>Yingyu Lin, Erchi Wang, Yi-An Ma, Yu-Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21071">https://arxiv.org/abs/2503.21071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21071">https://arxiv.org/pdf/2503.21071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21071]] Purifying Approximate Differential Privacy with Randomized Post-processing(https://arxiv.org/abs/2503.21071)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We propose a framework to convert $(\varepsilon, \delta)$-approximate Differential Privacy (DP) mechanisms into $(\varepsilon, 0)$-pure DP mechanisms, a process we call ``purification''. This algorithmic technique leverages randomized post-processing with calibrated noise to eliminate the $\delta$ parameter while preserving utility. By combining the tighter utility bounds and computational efficiency of approximate DP mechanisms with the stronger guarantees of pure DP, our approach achieves the best of both worlds. We illustrate the applicability of this framework in various settings, including Differentially Private Empirical Risk Minimization (DP-ERM), data-dependent DP mechanisms such as Propose-Test-Release (PTR), and query release tasks. To the best of our knowledge, this is the first work to provide a systematic method for transforming approximate DP into pure DP while maintaining competitive accuracy and computational efficiency.</li>
</ul>

<h3>Title: Shared Global and Local Geometry of Language Model Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Andrew Lee, Melanie Weber, Fernanda Viégas, Martin Wattenberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21073">https://arxiv.org/abs/2503.21073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21073">https://arxiv.org/pdf/2503.21073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21073]] Shared Global and Local Geometry of Language Model Embeddings(https://arxiv.org/abs/2503.21073)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Researchers have recently suggested that models share common representations. In this work, we find that the token embeddings of language models exhibit common geometric structure. First, we find ``global'' similarities: token embeddings often share similar relative orientations. Next, we characterize local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each token embedding. Our intrinsic dimension measure demonstrates that token embeddings lie on a lower dimensional manifold. We qualitatively show that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Both characterizations allow us to find similarities in the local geometry of token embeddings. Perhaps most surprisingly, we find that alignment in token embeddings persists through the hidden states of language models, allowing us to develop an application for interpretability. Namely, we empirically demonstrate that steering vectors from one language model can be transferred to another, despite the two models having different dimensions.</li>
</ul>

<h3>Title: Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems</h3>
<ul>
<li><strong>Authors: </strong>Ooha Lakkadi Reddy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21074">https://arxiv.org/abs/2503.21074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21074">https://arxiv.org/pdf/2503.21074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21074]] Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems(https://arxiv.org/abs/2503.21074)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This thesis employs a hybrid CNN-Transformer architecture, in conjunction with a detailed anthropological framework, to investigate potential historical connections between the visual morphology of the Indus Valley script and pictographic systems of the Tibetan-Yi Corridor. Through an ensemble methodology of three target scripts across 15 independently trained models, we demonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold higher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze Age Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems. Additionally and contrarily to our current understanding of the networks of the Indus Valley Civilization, the Indus script unexpectedly maps closer to Tibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to the aforementioned contemporaneous West Asian signaries, both of which recorded mean cosine similarities of 0.104 and 0.080 despite their close geographic proximity and evident trade relations. Across various dimensionality reduction practices and clustering methodologies, the Indus script consistently clusters closest to Tibetan-Yi Corridor scripts. Our computational results align with qualitative observations of specific pictorial parallels in numeral systems, gender markers, and key iconographic elements; this is further supported by archaeological evidence of sustained contact networks along the ancient Shu-Shendu road in tandem with the Indus Valley Civilization's decline, providing a plausible transmission pathway. While alternative explanations cannot be ruled out, the specificity and consistency of observed similarities challenge conventional narratives of isolated script development and suggest more complex ancient cultural transmission networks between South and East Asia than previously recognized.</li>
</ul>

<h3>Title: KAC: Kolmogorov-Arnold Classifier for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Yusong Hu, Zichen Liang, Fei Yang, Qibin Hou, Xialei Liu, Ming-Ming Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21076">https://arxiv.org/abs/2503.21076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21076">https://arxiv.org/pdf/2503.21076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21076]] KAC: Kolmogorov-Arnold Classifier for Continual Learning(https://arxiv.org/abs/2503.21076)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Continual learning requires models to train continuously across consecutive tasks without forgetting. Most existing methods utilize linear classifiers, which struggle to maintain a stable classification space while learning new tasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) in preserving learning stability during simple continual regression tasks, we set out to explore their potential in more complex continual learning scenarios. In this paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novel classifier developed for continual learning based on the KAN structure. We delve into the impact of KAN's spline functions and introduce Radial Basis Functions (RBF) for improved compatibility with continual learning. We replace linear classifiers with KAC in several recent approaches and conduct experiments across various continual learning benchmarks, all of which demonstrate performance improvements, highlighting the effectiveness and robustness of KAC in continual learning. The code is available at this https URL.</li>
</ul>

<h3>Title: EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Liu, Yunbo Long</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21080">https://arxiv.org/abs/2503.21080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21080">https://arxiv.org/pdf/2503.21080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21080]] EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues(https://arxiv.org/abs/2503.21080)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language model (LLM)-based chatbots have been applied for effective engagement in credit dialogues, their capacity for dynamic emotional expression remains limited. Current agents primarily rely on passive empathy rather than affective reasoning. For instance, when faced with persistent client negativity, the agent should employ strategic emotional adaptation by expressing measured anger to discourage counterproductive behavior and guide the conversation toward resolution. This context-aware emotional modulation is essential for imitating the nuanced decision-making of human negotiators. This paper introduces an EQ-negotiator that combines emotion sensing from pre-trained language models (PLMs) with emotional reasoning based on Game Theory and Hidden Markov Models. It takes into account both the current and historical emotions of the client to better manage and address negative emotions during interactions. By fine-tuning pre-trained language models (PLMs) on public emotion datasets and validating them on the credit dialogue datasets, our approach enables LLM-based agents to effectively capture shifts in client emotions and dynamically adjust their response tone based on our emotion decision policies in real-world financial negotiations. This EQ-negotiator can also help credit agencies foster positive client relationships, enhancing satisfaction in credit services.</li>
</ul>

<h3>Title: Can Video Diffusion Model Reconstruct 4D Geometry?</h3>
<ul>
<li><strong>Authors: </strong>Jinjie Mai, Wenxuan Zhu, Haozhe Liu, Bing Li, Cheng Zheng, Jürgen Schmidhuber, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21082">https://arxiv.org/abs/2503.21082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21082">https://arxiv.org/pdf/2503.21082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21082]] Can Video Diffusion Model Reconstruct 4D Geometry?(https://arxiv.org/abs/2503.21082)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video is an important yet challenging problem. Conventional multiview geometry-based approaches often struggle with dynamic motion, whereas recent learning-based methods either require specialized 4D representation or sophisticated optimization. In this paper, we present Sora3R, a novel framework that taps into the rich spatiotemporal priors of large-scale video diffusion models to directly infer 4D pointmaps from casual videos. Sora3R follows a two-stage pipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuring compatibility between the geometry and video latent spaces; (2) we finetune a diffusion backbone in combined video and pointmap latent space to generate coherent 4D pointmaps for every frame. Sora3R operates in a fully feedforward manner, requiring no external modules (e.g., depth, optical flow, or segmentation) or iterative global alignment. Extensive experiments demonstrate that Sora3R reliably recovers both camera poses and detailed scene geometry, achieving performance on par with state-of-the-art methods for dynamic 4D reconstruction across diverse scenarios.</li>
</ul>

<h3>Title: ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Haoming Xu, Shuxun Wang, Yanqiu Zhao, Yi Zhong, Ziyan Jiang, Ningyuan Zhao, Shumin Deng, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21088">https://arxiv.org/abs/2503.21088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21088">https://arxiv.org/pdf/2503.21088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21088]] ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging(https://arxiv.org/abs/2503.21088)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at this https URL.</li>
</ul>

<h3>Title: StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yuyin Chen, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Yifei Zhan, Xianpeng Lang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21104">https://arxiv.org/abs/2503.21104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21104">https://arxiv.org/pdf/2503.21104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21104]] StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency(https://arxiv.org/abs/2503.21104)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Urban scene reconstruction requires modeling both static infrastructure and dynamic elements while supporting diverse environmental conditions. We present \textbf{StyledStreets}, a multi-style street simulator that achieves instruction-driven scene editing with guaranteed spatial and temporal consistency. Building on a state-of-the-art Gaussian Splatting framework for street scenarios enhanced by our proposed pose optimization and multi-view training, our method enables photorealistic style transfers across seasons, weather conditions, and camera setups through three key innovations: First, a hybrid embedding scheme disentangles persistent scene geometry from transient style attributes, allowing realistic environmental edits while preserving structural integrity. Second, uncertainty-aware rendering mitigates supervision noise from diffusion priors, enabling robust training across extreme style variations. Third, a unified parametric model prevents geometric drift through regularized updates, maintaining multi-view consistency across seven vehicle-mounted cameras. Our framework preserves the original scene's motion patterns and geometric relationships. Qualitative results demonstrate plausible transitions between diverse conditions (snow, sandstorm, night), while quantitative evaluations show state-of-the-art geometric accuracy under style transfers. The approach establishes new capabilities for urban simulation, with applications in autonomous vehicle testing and augmented reality systems requiring reliable environmental consistency. Codes will be publicly available upon publication.</li>
</ul>

<h3>Title: Function Alignment: A New Theory for Mind and Intelligence, Part I: Foundations</h3>
<ul>
<li><strong>Authors: </strong>Gus G. Xia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21106">https://arxiv.org/abs/2503.21106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21106">https://arxiv.org/pdf/2503.21106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21106]] Function Alignment: A New Theory for Mind and Intelligence, Part I: Foundations(https://arxiv.org/abs/2503.21106)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper introduces function alignment, a novel theory of mind and intelligence that is both intuitively compelling and structurally grounded. It explicitly models how meaning, interpretation, and analogy emerge from interactions among layered representations, forming a coherent framework capable not only of modeling minds but also of serving as a blueprint for building them. One of the key theoretical insights derived from function alignment is bounded interpretability, which provides a unified explanation for previously fragmented ideas in cognitive science, such as bounded rationality, symbol grounding, and analogy-making. Beyond modeling, the function alignment framework bridges disciplines often kept apart, linking computational architecture, psychological theory, and even contemplative traditions such as Zen. Rather than building on any philosophical systems, it offers a structural foundation upon which multiple ways of understanding the mind may be reconstructed.</li>
</ul>

<h3>Title: Leveraging Large Language Models for Risk Assessment in Hyperconnected Logistic Hub Network Deployment</h3>
<ul>
<li><strong>Authors: </strong>Yinzhu Quan, Yujia Xu, Guanlin Chen, Frederick Benaben, Benoit Montreuil</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21115">https://arxiv.org/abs/2503.21115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21115">https://arxiv.org/pdf/2503.21115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21115]] Leveraging Large Language Models for Risk Assessment in Hyperconnected Logistic Hub Network Deployment(https://arxiv.org/abs/2503.21115)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing emphasis on energy efficiency and environmental sustainability in global supply chains introduces new challenges in the deployment of hyperconnected logistic hub networks. In current volatile, uncertain, complex, and ambiguous (VUCA) environments, dynamic risk assessment becomes essential to ensure successful hub deployment. However, traditional methods often struggle to effectively capture and analyze unstructured information. In this paper, we design an Large Language Model (LLM)-driven risk assessment pipeline integrated with multiple analytical tools to evaluate logistic hub deployment. This framework enables LLMs to systematically identify potential risks by analyzing unstructured data, such as geopolitical instability, financial trends, historical storm events, traffic conditions, and emerging risks from news sources. These data are processed through a suite of analytical tools, which are automatically called by LLMs to support a structured and data-driven decision-making process for logistic hub selection. In addition, we design prompts that instruct LLMs to leverage these tools for assessing the feasibility of hub selection by evaluating various risk types and levels. Through risk-based similarity analysis, LLMs cluster logistic hubs with comparable risk profiles, enabling a structured approach to risk assessment. In conclusion, the framework incorporates scalability with long-term memory and enhances decision-making through explanation and interpretation, enabling comprehensive risk assessments for logistic hub deployment in hyperconnected supply chain networks.</li>
</ul>

<h3>Title: One Snapshot is All You Need: A Generalized Method for mmWave Signal Generation</h3>
<ul>
<li><strong>Authors: </strong>Teng Huang, Han Ding, Wenxin Sun, Cui Zhao, Ge Wang, Fei Wang, Kun Zhao, Zhi Wang, Wei Xi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21122">https://arxiv.org/abs/2503.21122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21122">https://arxiv.org/pdf/2503.21122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21122]] One Snapshot is All You Need: A Generalized Method for mmWave Signal Generation(https://arxiv.org/abs/2503.21122)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Wireless sensing systems, particularly those using mmWave technology, offer distinct advantages over traditional vision-based approaches, such as enhanced privacy and effectiveness in poor lighting conditions. These systems, leveraging FMCW signals, have shown success in human-centric applications like localization, gesture recognition, and so on. However, comprehensive mmWave datasets for diverse applications are scarce, often constrained by pre-processed signatures (e.g., point clouds or RA heatmaps) and inconsistent annotation formats. To overcome these limitations, we propose mmGen, a novel and generalized framework tailored for full-scene mmWave signal generation. By constructing physical signal transmission models, mmGen synthesizes human-reflected and environment-reflected mmWave signals from the constructed 3D meshes. Additionally, we incorporate methods to account for material properties, antenna gains, and multipath reflections, enhancing the realism of the synthesized signals. We conduct extensive experiments using a prototype system with commercial mmWave devices and Kinect sensors. The results show that the average similarity of Range-Angle and micro-Doppler signatures between the synthesized and real-captured signals across three different environments exceeds 0.91 and 0.89, respectively, demonstrating the effectiveness and practical applicability of mmGen.</li>
</ul>

<h3>Title: AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction</h3>
<ul>
<li><strong>Authors: </strong>Shuaiyu Zhang, Xun Lin, Rongxiang Zhang, Yu Bai, Yong Xu, Tao Tan, Xunbin Zheng, Zitong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21124">https://arxiv.org/abs/2503.21124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21124">https://arxiv.org/pdf/2503.21124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21124]] AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction(https://arxiv.org/abs/2503.21124)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The integration of pathologic images and genomic data for survival analysis has gained increasing attention with advances in multimodal learning. However, current methods often ignore biological characteristics, such as heterogeneity and sparsity, both within and across modalities, ultimately limiting their adaptability to clinical practice. To address these challenges, we propose AdaMHF: Adaptive Multimodal Hierarchical Fusion, a framework designed for efficient, comprehensive, and tailored feature extraction and fusion. AdaMHF is specifically adapted to the uniqueness of medical data, enabling accurate predictions with minimal resource consumption, even under challenging scenarios with missing modalities. Initially, AdaMHF employs an experts expansion and residual structure to activate specialized experts for extracting heterogeneous and sparse features. Extracted tokens undergo refinement via selection and aggregation, reducing the weight of non-dominant features while preserving comprehensive information. Subsequently, the encoded features are hierarchically fused, allowing multi-grained interactions across modalities to be captured. Furthermore, we introduce a survival prediction benchmark designed to resolve scenarios with missing modalities, mirroring real-world clinical conditions. Extensive experiments on TCGA datasets demonstrate that AdaMHF surpasses current state-of-the-art (SOTA) methods, showcasing exceptional performance in both complete and incomplete modality settings.</li>
</ul>

<h3>Title: Bandwidth-Efficient Two-Server ORAMs with O(1) Client Storage</h3>
<ul>
<li><strong>Authors: </strong>Wei Wang, Xianglong Zhang, Peng Xu, Rongmao Chen, Laurence Tianruo Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21126">https://arxiv.org/abs/2503.21126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21126">https://arxiv.org/pdf/2503.21126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21126]] Bandwidth-Efficient Two-Server ORAMs with O(1) Client Storage(https://arxiv.org/abs/2503.21126)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Oblivious RAM (ORAM) allows a client to securely retrieve elements from outsourced servers without leakage about the accessed elements or their virtual addresses. Two-server ORAM, designed for secure two-party RAM computation, stores data across two non-colluding servers. However, many two-server ORAM schemes suffer from excessive local storage or high bandwidth costs. To serve lightweight clients, it is crucial for ORAM to achieve concretely efficient bandwidth while maintaining O(1) local storage. Hence, this paper presents two new client-friendly two-server ORAM schemes that achieve practical logarithmic bandwidth under O(1) local storage, while incurring linear symmetric key computations. The core design features a hierarchical structure and a pairwise-area setting for the elements and their tags. Accordingly, we specify efficient read-only and write-only private information retrieval (PIR) algorithms in our schemes to ensure obliviousness in accessing two areas respectively, so as to avoid the necessity of costly shuffle techniques in previous works. We empirically evaluate our schemes against LO13 (TCC'13), AFN17 (PKC'17), and KM19 (PKC'19) in terms of both bandwidth and time cost. The results demonstrate that our schemes reduce bandwidth by approximately 2-4x compared to LO13, and by 16-64x compared to AFN17 and KM19. For a database of size 2^14 blocks, our schemes are over 64x faster than KM19, while achieving similar performance to LO13 and AFN17 in the WAN setting, with a latency of around 1 second.</li>
</ul>

<h3>Title: Collaborative Evolution: Multi-Round Learning Between Large and Small Language Models for Emergent Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Zhou, Xiaoming Zhang, Shenghan Tan, Litian Zhang, Chaozhuo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21127">https://arxiv.org/abs/2503.21127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21127">https://arxiv.org/pdf/2503.21127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21127]] Collaborative Evolution: Multi-Round Learning Between Large and Small Language Models for Emergent Fake News Detection(https://arxiv.org/abs/2503.21127)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of fake news on social media platforms has exerted a substantial influence on society, leading to discernible impacts and deleterious consequences. Conventional deep learning methodologies employing small language models (SLMs) suffer from the necessity for extensive supervised training and the challenge of adapting to rapidly evolving circumstances. Large language models (LLMs), despite their robust zero-shot capabilities, have fallen short in effectively identifying fake news due to a lack of pertinent demonstrations and the dynamic nature of knowledge. In this paper, a novel framework Multi-Round Collaboration Detection (MRCD) is proposed to address these aforementioned limitations. The MRCD framework is capable of enjoying the merits from both LLMs and SLMs by integrating their generalization abilities and specialized functionalities, respectively. Our approach features a two-stage retrieval module that selects relevant and up-to-date demonstrations and knowledge, enhancing in-context learning for better detection of emerging news events. We further design a multi-round learning framework to ensure more reliable detection results. Our framework MRCD achieves SOTA results on two real-world datasets Pheme and Twitter16, with accuracy improvements of 7.4\% and 12.8\% compared to using only SLMs, which effectively addresses the limitations of current models and improves the detection of emergent fake news.</li>
</ul>

<h3>Title: MoQa: Rethinking MoE Quantization with Multi-stage Data-model Distribution Awareness</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zheng, Xiuping Cui, Size Zheng, Maoliang Li, Jiayu Chen, Yun (Eric)Liang, Xiang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21135">https://arxiv.org/abs/2503.21135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21135">https://arxiv.org/pdf/2503.21135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21135]] MoQa: Rethinking MoE Quantization with Multi-stage Data-model Distribution Awareness(https://arxiv.org/abs/2503.21135)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the advances in artificial intelligence, Mix-of-Experts (MoE) has become the main form of Large Language Models (LLMs), and its demand for model compression is increasing. Quantization is an effective method that not only compresses the models but also significantly accelerates their performance. Existing quantization methods have gradually shifted the focus from parameter scaling to the analysis of data distributions. However, their analysis is designed for dense LLMs and relies on the simple one-model-all-data mapping, which is unsuitable for MoEs. This paper proposes a new quantization framework called MoQa. MoQa decouples the data-model distribution complexity of MoEs in multiple analysis stages, quantitively revealing the dynamics during sparse data activation, data-parameter mapping, and inter-expert correlations. Based on these, MoQa identifies particular experts' and parameters' significance with optimal data-model distribution awareness and proposes a series of fine-grained mix-quantization strategies adaptive to various data activation and expert combination scenarios. Moreover, MoQa discusses the limitations of existing quantization and analyzes the impact of each stage analysis, showing novel insights for MoE quantization. Experiments show that MoQa achieves a 1.69~2.18 perplexity decrease in language modeling tasks and a 1.58%~8.91% accuracy improvement in zero-shot inference tasks. We believe MoQa will play a role in future MoE construction, optimization, and compression.</li>
</ul>

<h3>Title: ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jinwei Qi, Chaonan Ji, Sheng Xu, Peng Zhang, Bang Zhang, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21144">https://arxiv.org/abs/2503.21144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21144">https://arxiv.org/pdf/2503.21144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21144]] ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model(https://arxiv.org/abs/2503.21144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Real-time interactive video-chat portraits have been increasingly recognized as the future trend, particularly due to the remarkable progress made in text and voice chat technologies. However, existing methods primarily focus on real-time generation of head movements, but struggle to produce synchronized body motions that match these head actions. Additionally, achieving fine-grained control over the speaking style and nuances of facial expressions remains a challenge. To address these limitations, we introduce a novel framework for stylized real-time portrait video generation, enabling expressive and flexible video chat that extends from talking head to upper-body interaction. Our approach consists of the following two stages. The first stage involves efficient hierarchical motion diffusion models, that take both explicit and implicit motion representations into account based on audio inputs, which can generate a diverse range of facial expressions with stylistic control and synchronization between head and body movements. The second stage aims to generate portrait video featuring upper-body movements, including hand gestures. We inject explicit hand control signals into the generator to produce more detailed hand movements, and further perform face refinement to enhance the overall realism and expressiveness of the portrait video. Additionally, our approach supports efficient and continuous generation of upper-body portrait video in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting interactive video-chat in real-time. Experimental results demonstrate the capability of our approach to produce portrait videos with rich expressiveness and natural upper-body movements.</li>
</ul>

<h3>Title: How to Secure Existing C and C++ Software without Memory Safety</h3>
<ul>
<li><strong>Authors: </strong>Úlfar Erlingsson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21145">https://arxiv.org/abs/2503.21145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21145">https://arxiv.org/pdf/2503.21145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21145]] How to Secure Existing C and C++ Software without Memory Safety(https://arxiv.org/abs/2503.21145)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The most important security benefit of software memory safety is easy to state: for C and C++ software, attackers can exploit most bugs and vulnerabilities to gain full, unfettered control of software behavior, whereas this is not true for most bugs in memory-safe software. Fortunately, this security benefit -- most bugs don't give attackers full control -- can be had for unmodified C/C++ software, without per-application effort. This doesn't require trying to establish memory safety; instead, it is sufficient to eliminate most of the combinatorial ways in which software with corrupted memory can execute. To eliminate these interleavings, there already exist practical compiler and runtime mechanisms that incur little overhead and need no special hardware or platform support. Each of the mechanisms described here is already in production use, at scale, on one or more platforms. By supporting their combined use in development toolchains, the security of all C and C++ software against remote code execution attacks can be rapidly, and dramatically, improved.</li>
</ul>

<h3>Title: The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Liu, Yixiong Zou, Yuhua Li, Ruixuan Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21150">https://arxiv.org/abs/2503.21150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21150">https://arxiv.org/pdf/2503.21150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21150]] The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation(https://arxiv.org/abs/2503.21150)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the pixel-level segmentation capabilities learned from large-scale source-domain datasets to downstream target-domain datasets, with only a few annotated images per class. In this paper, we focus on a well-observed but unresolved phenomenon in CDFSS: for target domains, particularly those distant from the source domain, segmentation performance peaks at the very early epochs, and declines sharply as the source-domain training proceeds. We delve into this phenomenon for an interpretation: low-level features are vulnerable to domain shifts, leading to sharper loss landscapes during the source-domain training, which is the devil of CDFSS. Based on this phenomenon and interpretation, we further propose a method that includes two plug-and-play modules: one to flatten the loss landscapes for low-level features during source-domain training as a novel sharpness-aware minimization method, and the other to directly supplement target-domain information to the model during target-domain testing by low-level-based calibration. Extensive experiments on four target datasets validate our rationale and demonstrate that our method surpasses the state-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU in 1-shot and 5-shot scenarios, respectively.</li>
</ul>

<h3>Title: Federated Learning with Differential Privacy: An Utility-Enhanced Approach</h3>
<ul>
<li><strong>Authors: </strong>Kanishka Ranaweera, Dinh C. Nguyen, Pubudu N. Pathirana, David Smith, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21154">https://arxiv.org/abs/2503.21154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21154">https://arxiv.org/pdf/2503.21154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21154]] Federated Learning with Differential Privacy: An Utility-Enhanced Approach(https://arxiv.org/abs/2503.21154)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated learning has emerged as an attractive approach to protect data privacy by eliminating the need for sharing clients' data while reducing communication costs compared with centralized machine learning algorithms. However, recent studies have shown that federated learning alone does not guarantee privacy, as private data may still be inferred from the uploaded parameters to the central server. In order to successfully avoid data leakage, adopting differential privacy (DP) in the local optimization process or in the local update aggregation process has emerged as two feasible ways for achieving sample-level or user-level privacy guarantees respectively, in federated learning models. However, compared to their non-private equivalents, these approaches suffer from a poor utility. To improve the privacy-utility trade-off, we present a modification to these vanilla differentially private algorithms based on a Haar wavelet transformation step and a novel noise injection scheme that significantly lowers the asymptotic bound of the noise variance. We also present a holistic convergence analysis of our proposed algorithm, showing that our method yields better convergence performance than the vanilla DP algorithms. Numerical experiments on real-world datasets demonstrate that our method outperforms existing approaches in model utility while maintaining the same privacy guarantees.</li>
</ul>

<h3>Title: Embedding Domain-Specific Knowledge from LLMs into the Feature Engineering Pipeline</h3>
<ul>
<li><strong>Authors: </strong>João Eduardo Batista</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21155">https://arxiv.org/abs/2503.21155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21155">https://arxiv.org/pdf/2503.21155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21155]] Embedding Domain-Specific Knowledge from LLMs into the Feature Engineering Pipeline(https://arxiv.org/abs/2503.21155)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Feature engineering is mandatory in the machine learning pipeline to obtain robust models. While evolutionary computation is well-known for its great results both in feature selection and feature construction, its methods are computationally expensive due to the large number of evaluations required to induce the final model. Part of the reason why these algorithms require a large number of evaluations is their lack of domain-specific knowledge, resulting in a lot of random guessing during evolution. In this work, we propose using Large Language Models (LLMs) as an initial feature construction step to add knowledge to the dataset. By doing so, our results show that the evolution can converge faster, saving us computational resources. The proposed approach only provides the names of the features in the dataset and the target objective to the LLM, making it usable even when working with datasets containing private data. While consistent improvements to test performance were only observed for one-third of the datasets (CSS, PM, and IM10), possibly due to problems being easily explored by LLMs, this approach only decreased the model performance in 1/77 test cases. Additionally, this work introduces the M6GP feature engineering algorithm to symbolic regression, showing it can improve the results of the random forest regressor and produce competitive results with its predecessor, M3GP.</li>
</ul>

<h3>Title: Integrating Travel Behavior Forecasting and Generative Modeling for Predicting Future Urban Mobility and Spatial Transformations</h3>
<ul>
<li><strong>Authors: </strong>Eugene Denteh, Andrews Danyo, Joshua Kofi Asamoah, Blessing Agyei Kyem, Twitchell Addai, Armstrong Aboah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21158">https://arxiv.org/abs/2503.21158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21158">https://arxiv.org/pdf/2503.21158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21158]] Integrating Travel Behavior Forecasting and Generative Modeling for Predicting Future Urban Mobility and Spatial Transformations(https://arxiv.org/abs/2503.21158)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Transportation planning plays a critical role in shaping urban development, economic mobility, and infrastructure sustainability. However, traditional planning methods often struggle to accurately predict long-term urban growth and transportation demands. This may sometimes result in infrastructure demolition to make room for current transportation planning demands. This study integrates a Temporal Fusion Transformer to predict travel patterns from demographic data with a Generative Adversarial Network to predict future urban settings through satellite imagery. The framework achieved a 0.76 R-square score in travel behavior prediction and generated high-fidelity satellite images with a Structural Similarity Index of 0.81. The results demonstrate that integrating predictive analytics and spatial visualization can significantly improve the decision-making process, fostering more sustainable and efficient urban development. This research highlights the importance of data-driven methodologies in modern transportation planning and presents a step toward optimizing infrastructure placement, capacity, and long-term viability.</li>
</ul>

<h3>Title: Multi-Objective Optimization for Privacy-Utility Balance in Differentially Private Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Kanishka Ranaweera, David Smith, Pubudu N. Pathirana, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21159">https://arxiv.org/abs/2503.21159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21159">https://arxiv.org/pdf/2503.21159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21159]] Multi-Objective Optimization for Privacy-Utility Balance in Differentially Private Federated Learning(https://arxiv.org/abs/2503.21159)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it a promising approach for privacy-preserving machine learning. However, ensuring differential privacy (DP) in FL presents challenges due to the trade-off between model utility and privacy protection. Clipping gradients before aggregation is a common strategy to limit privacy loss, but selecting an optimal clipping norm is non-trivial, as excessively high values compromise privacy, while overly restrictive clipping degrades model performance. In this work, we propose an adaptive clipping mechanism that dynamically adjusts the clipping norm using a multi-objective optimization framework. By integrating privacy and utility considerations into the optimization objective, our approach balances privacy preservation with model accuracy. We theoretically analyze the convergence properties of our method and demonstrate its effectiveness through extensive experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results show that adaptive clipping consistently outperforms fixed-clipping baselines, achieving improved accuracy under the same privacy constraints. This work highlights the potential of dynamic clipping strategies to enhance privacy-utility trade-offs in differentially private federated learning.</li>
</ul>

<h3>Title: A Data Balancing and Ensemble Learning Approach for Credit Card Fraud Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21160">https://arxiv.org/abs/2503.21160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21160">https://arxiv.org/pdf/2503.21160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21160]] A Data Balancing and Ensemble Learning Approach for Credit Card Fraud Detection(https://arxiv.org/abs/2503.21160)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, robust</a></li>
<li><strong>Abstract: </strong>This research introduces an innovative method for identifying credit card fraud by combining the SMOTE-KMEANS technique with an ensemble machine learning model. The proposed model was benchmarked against traditional models such as logistic regression, decision trees, random forests, and support vector machines. Performance was evaluated using metrics, including accuracy, recall, and area under the curve (AUC). The results demonstrated that the proposed model achieved superior performance, with an AUC of 0.96 when combined with the SMOTE-KMEANS algorithm. This indicates a significant improvement in detecting fraudulent transactions while maintaining high precision and recall. The study also explores the application of different oversampling techniques to enhance the performance of various classifiers. The findings suggest that the proposed method is robust and effective for classification tasks on balanced datasets. Future research directions include further optimization of the SMOTE-KMEANS approach and its integration into existing fraud detection systems to enhance financial security and consumer protection.</li>
</ul>

<h3>Title: Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples</h3>
<ul>
<li><strong>Authors: </strong>Samra Irshad, Seungkyu Lee, Nassir Navab, Hong Joo Lee, Seong Tae Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21164">https://arxiv.org/abs/2503.21164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21164">https://arxiv.org/pdf/2503.21164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21164]] Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples(https://arxiv.org/abs/2503.21164)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The presence of adversarial examples in the physical world poses significant challenges to the deployment of Deep Neural Networks in safety-critical applications such as autonomous driving. Most existing methods for crafting physical-world adversarial examples are ad-hoc, relying on temporary modifications like shadows, laser beams, or stickers that are tailored to specific scenarios. In this paper, we introduce a new class of physical-world adversarial examples, AdvWT, which draws inspiration from the naturally occurring phenomenon of `wear and tear', an inherent property of physical objects. Unlike manually crafted perturbations, `wear and tear' emerges organically over time due to environmental degradation, as seen in the gradual deterioration of outdoor signboards. To achieve this, AdvWT follows a two-step approach. First, a GAN-based, unsupervised image-to-image translation network is employed to model these naturally occurring damages, particularly in the context of outdoor signboards. The translation network encodes the characteristics of damaged signs into a latent `damage style code'. In the second step, we introduce adversarial perturbations into the style code, strategically optimizing its transformation process. This manipulation subtly alters the damage style representation, guiding the network to generate adversarial images where the appearance of damages remains perceptually realistic, while simultaneously ensuring their effectiveness in misleading neural networks. Through comprehensive experiments on two traffic sign datasets, we show that AdvWT effectively misleads DNNs in both digital and physical domains. AdvWT achieves an effective attack success rate, greater robustness, and a more natural appearance compared to existing physical-world adversarial examples. Additionally, integrating AdvWT into training enhances a model's generalizability to real-world damaged signs.</li>
</ul>

<h3>Title: VADMamba: Exploring State Space Models for Fast Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Lyu, Minghua Zhao, Jing Hu, Xuewen Huang, Yifei Chen, Shuangli Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21169">https://arxiv.org/abs/2503.21169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21169">https://arxiv.org/pdf/2503.21169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21169]] VADMamba: Exploring State Space Models for Fast Video Anomaly Detection(https://arxiv.org/abs/2503.21169)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Video anomaly detection (VAD) methods are mostly CNN-based or Transformer-based, achieving impressive results, but the focus on detection accuracy often comes at the expense of inference speed. The emergence of state space models in computer vision, exemplified by the Mamba model, demonstrates improved computational efficiency through selective scans and showcases the great potential for long-range modeling. Our study pioneers the application of Mamba to VAD, dubbed VADMamba, which is based on multi-task learning for frame prediction and optical flow reconstruction. Specifically, we propose the VQ-Mamba Unet (VQ-MaU) framework, which incorporates a Vector Quantization (VQ) layer and Mamba-based Non-negative Visual State Space (NVSS) block. Furthermore, two individual VQ-MaU networks separately predict frames and reconstruct corresponding optical flows, further boosting accuracy through a clip-level fusion evaluation strategy. Experimental results validate the efficacy of the proposed VADMamba across three benchmark datasets, demonstrating superior performance in inference speed compared to previous work. Code is available at this https URL.</li>
</ul>

<h3>Title: Model as a Game: On Numerical and Spatial Consistency for Generative Games</h3>
<ul>
<li><strong>Authors: </strong>Jingye Chen, Yuzhong Zhao, Yupan Huang, Lei Cui, Li Dong, Tengchao Lv, Qifeng Chen, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21172">https://arxiv.org/abs/2503.21172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21172">https://arxiv.org/pdf/2503.21172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21172]] Model as a Game: On Numerical and Spatial Consistency for Generative Games(https://arxiv.org/abs/2503.21172)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have significantly impacted game generation. However, despite producing high-quality graphics and adequately receiving player input, existing models often fail to maintain fundamental game properties such as numerical and spatial consistency. Numerical consistency ensures gameplay mechanics correctly reflect score changes and other quantitative elements, while spatial consistency prevents jarring scene transitions, providing seamless player experiences. In this paper, we revisit the paradigm of generative games to explore what truly constitutes a Model as a Game (MaaG) with a well-developed mechanism. We begin with an empirical study on ``Traveler'', a 2D game created by an LLM featuring minimalist rules yet challenging generative models in maintaining consistency. Based on the DiT architecture, we design two specialized modules: (1) a numerical module that integrates a LogicNet to determine event triggers, with calculations processed externally as conditions for image generation; and (2) a spatial module that maintains a map of explored areas, retrieving location-specific information during generation and linking new observations to ensure continuity. Experiments across three games demonstrate that our integrated modules significantly enhance performance on consistency metrics compared to baselines, while incurring minimal time overhead during inference.</li>
</ul>

<h3>Title: DGSUnet: An Improved Unet Model with DINO-Guided SAM2 for Multi-Scale Feature Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Yimin Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21187">https://arxiv.org/abs/2503.21187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21187">https://arxiv.org/pdf/2503.21187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21187]] DGSUnet: An Improved Unet Model with DINO-Guided SAM2 for Multi-Scale Feature Collaboration(https://arxiv.org/abs/2503.21187)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Despite the significant advancements in general image segmentation achieved by large-scale pre-trained foundation models (such as Meta's Segment Any-thing Model (SAM) series and DINOv2), their performance in specialized fields remains limited by two critical issues: the excessive training costs due to large model parameters, and the insufficient ability to represent specific domain characteristics. This paper proposes a multi-scale feature collabora-tion framework guided by DINOv2 for SAM2, with core innovations in three aspects: (1) Establishing a feature collaboration mechanism between DINOv2 and SAM2 backbones, where high-dimensional semantic features extracted by the self-supervised model guide multi-scale feature fusion; (2) Designing lightweight adapter modules and cross-modal, cross-layer feature fusion units to inject cross-domain knowledge while freezing the base model parameters; (3) Constructing a U-shaped network structure based on U-net, which utilizes attention mechanisms to achieve adaptive aggregation decoding of multi-granularity features. This framework surpasses existing state-of-the-art meth-ods in downstream tasks such as camouflage target detection and salient ob-ject detection, without requiring costly training processes. It provides a tech-nical pathway for efficient deployment of visual image segmentation, demon-strating significant application value in a wide range of downstream tasks and specialized fields within image this http URL page: this https URL</li>
</ul>

<h3>Title: Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Erika Mori, Yue Qiu, Hirokatsu Kataoka, Yoshimitsu Aoki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21190">https://arxiv.org/abs/2503.21190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21190">https://arxiv.org/pdf/2503.21190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21190]] Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering(https://arxiv.org/abs/2503.21190)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Social intelligence, the ability to interpret emotions, intentions, and behaviors, is essential for effective communication and adaptive responses. As robots and AI systems become more prevalent in caregiving, healthcare, and education, the demand for AI that can interact naturally with humans grows. However, creating AI that seamlessly integrates multiple modalities, such as vision and speech, remains a challenge. Current video-based methods for social intelligence rely on general video recognition or emotion recognition techniques, often overlook the unique elements inherent in human interactions. To address this, we propose the Looped Video Debating (LVD) framework, which integrates Large Language Models (LLMs) with visual information, such as facial expressions and body movements, to enhance the transparency and reliability of question-answering tasks involving human interaction videos. Our results on the Social-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance without fine-tuning. Furthermore, supplementary human annotations on existing datasets provide insights into the model's accuracy, guiding future improvements in AI-driven social intelligence.</li>
</ul>

<h3>Title: UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongxuan Tang, Hao Liu, Xinyan Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21193">https://arxiv.org/abs/2503.21193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21193">https://arxiv.org/pdf/2503.21193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21193]] UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning(https://arxiv.org/abs/2503.21193)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce UGen, a unified autoregressive multimodal model that demonstrates strong performance across text processing, image understanding, and image generation tasks simultaneously. UGen converts both texts and images into discrete token sequences and utilizes a single transformer to generate them uniformly in an autoregressive manner. To address the challenges associated with unified multimodal learning, UGen is trained using a novel mechanism, namely progressive vocabulary learning. In this process, visual token IDs are incrementally activated and integrated into the training phase, ultimately enhancing the effectiveness of unified multimodal learning. Experiments on comprehensive text and image tasks show that UGen achieves a significant overall performance improvement of 13.3% compared to the vanilla unified autoregressive method, and it also delivers competitive results across all tasks against several task-specific models.</li>
</ul>

<h3>Title: An improved EfficientNetV2 for garbage classification</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Qiu, Chengxin Xie, Jingui Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21208">https://arxiv.org/abs/2503.21208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21208">https://arxiv.org/pdf/2503.21208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21208]] An improved EfficientNetV2 for garbage classification(https://arxiv.org/abs/2503.21208)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper presents an enhanced waste classification framework based on EfficientNetV2 to address challenges in data acquisition cost, generalization, and real-time performance. We propose a Channel-Efficient Attention (CE-Attention) module that mitigates feature loss during global pooling without introducing dimensional scaling, effectively enhancing critical feature extraction. Additionally, a lightweight multi-scale spatial feature extraction module (SAFM) is developed by integrating depthwise separable convolutions, significantly reducing model complexity. Comprehensive data augmentation strategies are further employed to improve generalization. Experiments on the Huawei Cloud waste classification dataset demonstrate that our method achieves a classification accuracy of 95.4\%, surpassing the baseline by 3.2\% and outperforming mainstream models. The results validate the effectiveness of our approach in balancing accuracy and efficiency for practical waste classification scenarios.</li>
</ul>

<h3>Title: FakeReasoning: Towards Generalizable Forgery Detection and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yueying Gao, Dongliang Chang, Bingyao Yu, Haotian Qin, Lei Chen, Kongming Liang, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21210">https://arxiv.org/abs/2503.21210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21210">https://arxiv.org/pdf/2503.21210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21210]] FakeReasoning: Towards Generalizable Forgery Detection and Reasoning(https://arxiv.org/abs/2503.21210)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Accurate and interpretable detection of AI-generated images is essential for mitigating risks associated with AI misuse. However, the substantial domain gap among generative models makes it challenging to develop a generalizable forgery detection model. Moreover, since every pixel in an AI-generated image is synthesized, traditional saliency-based forgery explanation methods are not well suited for this task. To address these challenges, we propose modeling AI-generated image detection and explanation as a Forgery Detection and Reasoning task (FDR-Task), leveraging vision-language models (VLMs) to provide accurate detection through structured and reliable reasoning over forgery attributes. To facilitate this task, we introduce the Multi-Modal Forgery Reasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K images across 10 generative models, with 10 types of forgery reasoning annotations, enabling comprehensive evaluation of FDR-Task. Additionally, we propose FakeReasoning, a forgery detection and reasoning framework with two key components. First, Forgery-Aligned Contrastive Learning enhances VLMs' understanding of forgery-related semantics through both cross-modal and intra-modal contrastive learning between images and forgery attribute reasoning. Second, a Classification Probability Mapper bridges the optimization gap between forgery detection and language modeling by mapping the output logits of VLMs to calibrated binary classification probabilities. Experiments across multiple generative models demonstrate that FakeReasoning not only achieves robust generalization but also outperforms state-of-the-art methods on both detection and reasoning tasks.</li>
</ul>

<h3>Title: Resource-Efficient Federated Fine-Tuning Large Language Models for Heterogeneous Data</h3>
<ul>
<li><strong>Authors: </strong>Jun Liu, Yunming Liao, Hongli Xu, Yang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21213">https://arxiv.org/abs/2503.21213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21213">https://arxiv.org/pdf/2503.21213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21213]] Resource-Efficient Federated Fine-Tuning Large Language Models for Heterogeneous Data(https://arxiv.org/abs/2503.21213)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) via federated learning, i.e., FedLLM, has been proposed to adapt LLMs for various downstream applications in a privacy-preserving way. To reduce the fine-tuning costs on resource-constrained devices, FedLoRA is proposed to fine-tune only a small subset of model parameters by integrating low-rank adaptation (LoRA) into FedLLM. However, apart from resource constraints, there is still another critical challenge, i.e., data heterogeneity, severely hindering the implementation of FedLoRA in practical applications. Herein, inspired by the previous group-based federated learning paradigm, we propose a hierarchical FedLoRA framework, termed HierFedLoRA, to address these challenges. Specifically, HierFedLoRA partitions all devices into multiple near-IID groups and adjusts the intra-group aggregation frequency for each group to eliminate the negative effects of non-IID data. Meanwhile, to reduce the computation and communication cost, HierFedLoRA dynamically assigns diverse and suitable fine-tuning depth (i.e., the number of continuous fine-tuning layers from the output) for each group. HierFedLoRA explores jointly optimizing aggregation frequency and depth upon their coupled relationship to better enhance the performance of FedLoRA. Extensive experiments are conducted on a physical platform with 80 commercial devices. The results show that HierFedLoRA improves the final model accuracy by 1.6% to 4.2%, speeding up the fine-tuning process by at least 2.1$\times$, compared to the strong baselines.</li>
</ul>

<h3>Title: GenFusion: Closing the Loop between Reconstruction and Generation via Videos</h3>
<ul>
<li><strong>Authors: </strong>Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, Anpei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21219">https://arxiv.org/abs/2503.21219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21219">https://arxiv.org/pdf/2503.21219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21219]] GenFusion: Closing the Loop between Reconstruction and Generation via Videos(https://arxiv.org/abs/2503.21219)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recently, 3D reconstruction and generation have demonstrated impressive novel view synthesis results, achieving high fidelity and efficiency. However, a notable conditioning gap can be observed between these two fields, e.g., scalable 3D scene reconstruction often requires densely captured views, whereas 3D generation typically relies on a single or no input view, which significantly limits their applications. We found that the source of this phenomenon lies in the misalignment between 3D constraints and generative priors. To address this problem, we propose a reconstruction-driven video diffusion model that learns to condition video frames on artifact-prone RGB-D renderings. Moreover, we propose a cyclical fusion pipeline that iteratively adds restoration frames from the generative model to the training set, enabling progressive expansion and addressing the viewpoint saturation limitations seen in previous reconstruction and generation pipelines. Our evaluation, including view synthesis from sparse view and masked input, validates the effectiveness of our approach.</li>
</ul>

<h3>Title: Rethinking Graph Structure Learning in the Era of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhihan Zhang, Xunkai Li, Guang Zeng, Hongchao Qin, Ronghua Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21223">https://arxiv.org/abs/2503.21223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21223">https://arxiv.org/pdf/2503.21223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21223]] Rethinking Graph Structure Learning in the Era of LLMs(https://arxiv.org/abs/2503.21223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, the emergence of large language models (LLMs) has prompted researchers to explore the integration of language descriptions into graphs, aiming to enhance model encoding capabilities from a data-centric perspective. This graph representation is called text-attributed graphs (TAGs). A review of prior advancements highlights that graph structure learning (GSL) is a pivotal technique for improving data utility, making it highly relevant to efficient TAG learning. However, most GSL methods are tailored for traditional graphs without textual information, underscoring the necessity of developing a new GSL paradigm. Despite clear motivations, it remains challenging: (1) How can we define a reasonable optimization objective for GSL in the era of LLMs, considering the massive parameters in LLM? (2) How can we design an efficient model architecture that enables seamless integration of LLM for this optimization objective? For Question 1, we reformulate existing GSL optimization objectives as a tree optimization framework, shifting the focus from obtaining a well-trained edge predictor to a language-aware tree sampler. For Question 2, we propose decoupled and training-free model design principles for LLM integration, shifting the focus from computation-intensive fine-tuning to more efficient inference. Based on this, we propose Large Language and Tree Assistant (LLaTA), which leverages tree-based LLM in-context learning to enhance the understanding of topology and text, enabling reliable inference and generating improved graph structure. Extensive experiments on 10 TAG datasets demonstrate that LLaTA enjoys flexibility - incorporated with any backbone; scalability - outperforms other LLM-based GSL methods in terms of running efficiency; effectiveness - achieves SOTA performance.</li>
</ul>

<h3>Title: Efficient Learning for Entropy-regularized Markov Decision Processes via Multilevel Monte Carlo</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Meunier, Christoph Reisinger, Yufei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21224">https://arxiv.org/abs/2503.21224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21224">https://arxiv.org/pdf/2503.21224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21224]] Efficient Learning for Entropy-regularized Markov Decision Processes via Multilevel Monte Carlo(https://arxiv.org/abs/2503.21224)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Designing efficient learning algorithms with complexity guarantees for Markov decision processes (MDPs) with large or continuous state and action spaces remains a fundamental challenge. We address this challenge for entropy-regularized MDPs with Polish state and action spaces, assuming access to a generative model of the environment. We propose a novel family of multilevel Monte Carlo (MLMC) algorithms that integrate fixed-point iteration with MLMC techniques and a generic stochastic approximation of the Bellman operator. We quantify the precise impact of the chosen approximate Bellman operator on the accuracy of the resulting MLMC estimator. Leveraging this error analysis, we show that using a biased plain MC estimate for the Bellman operator results in quasi-polynomial sample complexity, whereas an unbiased randomized multilevel approximation of the Bellman operator achieves polynomial sample complexity in expectation. Notably, these complexity bounds are independent of the dimensions or cardinalities of the state and action spaces, distinguishing our approach from existing algorithms whose complexities scale with the sizes of these spaces. We validate these theoretical performance guarantees through numerical experiments.</li>
</ul>

<h3>Title: Frequency-Aware Gaussian Splatting Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Yishai Lavi, Leo Segre, Shai Avidan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21226">https://arxiv.org/abs/2503.21226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21226">https://arxiv.org/pdf/2503.21226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21226]] Frequency-Aware Gaussian Splatting Decomposition(https://arxiv.org/abs/2503.21226)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3D-GS) has revolutionized novel view synthesis with its efficient, explicit representation. However, it lacks frequency interpretability, making it difficult to separate low-frequency structures from fine details. We introduce a frequency-decomposed 3D-GS framework that groups 3D Gaussians that correspond to subbands in the Laplacian Pyrmaids of the input images. Our approach enforces coherence within each subband (i.e., group of 3D Gaussians) through dedicated regularization, ensuring well-separated frequency components. We extend color values to both positive and negative ranges, allowing higher-frequency layers to add or subtract residual details. To stabilize optimization, we employ a progressive training scheme that refines details in a coarse-to-fine manner. Beyond interpretability, this frequency-aware design unlocks a range of practical benefits. Explicit frequency separation enables advanced 3D editing and stylization, allowing precise manipulation of specific frequency bands. It also supports dynamic level-of-detail control for progressive rendering, streaming, foveated rendering and fast geometry interaction. Through extensive experiments, we demonstrate that our method provides improved control and flexibility for emerging applications in scene editing and interactive rendering. Our code will be made publicly available.</li>
</ul>

<h3>Title: LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, Qingpei Guo, Si Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21227">https://arxiv.org/abs/2503.21227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21227">https://arxiv.org/pdf/2503.21227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21227]] LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models(https://arxiv.org/abs/2503.21227)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although applying Mixture of Experts to large language models for learning new tasks is widely regarded as an effective strategy for continuous learning, there still remain two major challenges: (1) As the number of tasks grows, simple parameter expansion strategies can lead to excessively large models. (2) Modifying the parameters of the existing router results in the erosion of previously acquired knowledge. In this paper, we present an innovative framework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE) architecture without any replay data. Specifically, we have developed a method called Probe-Guided Knowledge Extension (PGKE), which employs probe experts to assess whether additional knowledge is required for a specific layer. This approach enables the model to adaptively expand its network parameters based on task distribution, thereby significantly improving the efficiency of parameter expansion. Additionally, we introduce a hierarchical routing algorithm called Probabilistic Task Locator (PTL), where high-level routing captures inter-task information and low-level routing focuses on intra-task details, ensuring that new task experts do not interfere with existing ones. Our experiments shows that our efficient architecture has substantially improved model performance on the Coin benchmark while maintaining a reasonable parameter count.</li>
</ul>

<h3>Title: Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing</h3>
<ul>
<li><strong>Authors: </strong>Shuai Li, Jie Zhang, Yuang Qi, Kejiang Chen, Tianwei Zhang, Weiming Zhang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21236">https://arxiv.org/abs/2503.21236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21236">https://arxiv.org/pdf/2503.21236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21236]] Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing(https://arxiv.org/abs/2503.21236)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction</a></li>
<li><strong>Abstract: </strong>Large-scale image retrieval using deep hashing has become increasingly popular due to the exponential growth of image data and the remarkable feature extraction capabilities of deep neural networks (DNNs). However, deep hashing methods are vulnerable to malicious attacks, including adversarial and backdoor attacks. It is worth noting that these attacks typically involve altering the query images, which is not a practical concern in real-world scenarios. In this paper, we point out that even clean query images can be dangerous, inducing malicious target retrieval results, like undesired or illegal images. To the best of our knowledge, we are the first to study data \textbf{p}oisoning \textbf{a}ttacks against \textbf{d}eep \textbf{hash}ing \textbf{(\textit{PADHASH})}. Specifically, we first train a surrogate model to simulate the behavior of the target deep hashing model. Then, a strict gradient matching strategy is proposed to generate the poisoned images. Extensive experiments on different models, datasets, hash methods, and hash code lengths demonstrate the effectiveness and generality of our attack method.</li>
</ul>

<h3>Title: Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in Healthcare Data</h3>
<ul>
<li><strong>Authors: </strong>HyeYoung Lee, Pavel Tsoi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21241">https://arxiv.org/abs/2503.21241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21241">https://arxiv.org/pdf/2503.21241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21241]] Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in Healthcare Data(https://arxiv.org/abs/2503.21241)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate patient mortality prediction enables effective risk stratification, leading to personalized treatment plans and improved patient outcomes. However, predicting mortality in healthcare remains a significant challenge, with existing studies often focusing on specific diseases or limited predictor sets. This study evaluates machine learning models for all-cause in-hospital mortality prediction using the MIMIC-III database, employing a comprehensive feature engineering approach. Guided by clinical expertise and literature, we extracted key features such as vital signs (e.g., heart rate, blood pressure), laboratory results (e.g., creatinine, glucose), and demographic information. The Random Forest model achieved the highest performance with an AUC of 0.94, significantly outperforming other machine learning and deep learning approaches. This demonstrates Random Forest's robustness in handling high-dimensional, noisy clinical data and its potential for developing effective clinical decision support tools. Our findings highlight the importance of careful feature engineering for accurate mortality prediction. We conclude by discussing implications for clinical adoption and propose future directions, including enhancing model robustness and tailoring prediction models for specific diseases.</li>
</ul>

<h3>Title: Improving $(α, f)$-Byzantine Resilience in Federated Learning via layerwise aggregation and cosine distance</h3>
<ul>
<li><strong>Authors: </strong>Mario García-Márquez, Nuria Rodríguez-Barroso, M.Victoria Luzón, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21244">https://arxiv.org/abs/2503.21244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21244">https://arxiv.org/pdf/2503.21244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21244]] Improving $(α, f)$-Byzantine Resilience in Federated Learning via layerwise aggregation and cosine distance(https://arxiv.org/abs/2503.21244)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>The rapid development of artificial intelligence systems has amplified societal concerns regarding their usage, necessitating regulatory frameworks that encompass data privacy. Federated Learning (FL) is posed as potential solution to data privacy challenges in distributed machine learning by enabling collaborative model training {without data sharing}. However, FL systems remain vulnerable to Byzantine attacks, where malicious nodes contribute corrupted model updates. While Byzantine Resilient operators have emerged as a widely adopted robust aggregation algorithm to mitigate these attacks, its efficacy diminishes significantly in high-dimensional parameter spaces, sometimes leading to poor performing models. This paper introduces Layerwise Cosine Aggregation, a novel aggregation scheme designed to enhance robustness of these rules in such high-dimensional settings while preserving computational efficiency. A theoretical analysis is presented, demonstrating the superior robustness of the proposed Layerwise Cosine Aggregation compared to original robust aggregation operators. Empirical evaluation across diverse image classification datasets, under varying data distributions and Byzantine attack scenarios, consistently demonstrates the improved performance of Layerwise Cosine Aggregation, achieving up to a 16% increase in model accuracy.</li>
</ul>

<h3>Title: DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Zhao, Zhongang Qi, Cong Wang, Qingping Zheng, Guansong Lu, Fei Chen, Hang Xu, Zuxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21246">https://arxiv.org/abs/2503.21246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21246">https://arxiv.org/pdf/2503.21246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21246]] DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation(https://arxiv.org/abs/2503.21246)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human image animation has recently gained significant attention due to advancements in generative models. However, existing methods still face two major challenges: (1) architectural limitations, most models rely on U-Net, which underperforms compared to the MM-DiT; and (2) the neglect of textual information, which can enhance controllability. In this work, we introduce DynamiCtrl, a novel framework that not only explores different pose-guided control structures in MM-DiT, but also reemphasizes the crucial role of text in this task. Specifically, we employ a Shared VAE encoder for both reference images and driving pose videos, eliminating the need for an additional pose encoder and simplifying the overall framework. To incorporate pose features into the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN), which utilizes adaptive layer normalization to encode sparse pose features. The encoded features are directly added to the visual input, preserving the spatiotemporal consistency of the backbone while effectively introducing pose control into MM-DiT. Furthermore, within the full attention mechanism, we align textual and visual features to enhance controllability. By leveraging text, we not only enable fine-grained control over the generated content, but also, for the first time, achieve simultaneous control over both background and motion. Experimental results verify the superiority of DynamiCtrl on benchmark datasets, demonstrating its strong identity preservation, heterogeneous character driving, background controllability, and high-quality synthesis. The project page is available at this https URL.</li>
</ul>

<h3>Title: ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21248">https://arxiv.org/abs/2503.21248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21248">https://arxiv.org/pdf/2503.21248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21248]] ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition(https://arxiv.org/abs/2503.21248)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.</li>
</ul>

<h3>Title: Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, Yihong Gong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21258">https://arxiv.org/abs/2503.21258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21258">https://arxiv.org/pdf/2503.21258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21258]] Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning(https://arxiv.org/abs/2503.21258)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Few-shot class-incremental Learning (FSCIL) enables models to learn new classes from limited data while retaining performance on previously learned classes. Traditional FSCIL methods often require fine-tuning parameters with limited new class data and suffer from a separation between learning new classes and utilizing old knowledge. Inspired by the analogical learning mechanisms of the human brain, we propose a novel analogical generative method. Our approach includes the Brain-Inspired Analogical Generator (BiAG), which derives new class weights from existing classes without parameter fine-tuning during incremental stages. BiAG consists of three components: Weight Self-Attention Module (WSA), Weight & Prototype Analogical Attention Module (WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory for semantic conversion, WSA supplements new class weights, and WPAA computes analogies to generate new class weights. Experiments on miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our method achieves higher final and average accuracy compared to SOTA methods.</li>
</ul>

<h3>Title: vGamba: Attentive State Space Bottleneck for efficient Long-range Dependencies in Visual Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yunusa Haruna, Adamu Lawan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21262">https://arxiv.org/abs/2503.21262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21262">https://arxiv.org/pdf/2503.21262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21262]] vGamba: Attentive State Space Bottleneck for efficient Long-range Dependencies in Visual Recognition(https://arxiv.org/abs/2503.21262)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Capturing long-range dependencies efficiently is essential for visual recognition tasks, yet existing methods face limitations. Convolutional neural networks (CNNs) struggle with restricted receptive fields, while Vision Transformers (ViTs) achieve global context and long-range modeling at a high computational cost. State-space models (SSMs) offer an alternative, but their application in vision remains underexplored. This work introduces vGamba, a hybrid vision backbone that integrates SSMs with attention mechanisms to enhance efficiency and expressiveness. At its core, the Gamba bottleneck block that includes, Gamba Cell, an adaptation of Mamba for 2D spatial structures, alongside a Multi-Head Self-Attention (MHSA) mechanism and a Gated Fusion Module for effective feature representation. The interplay of these components ensures that vGamba leverages the low computational demands of SSMs while maintaining the accuracy of attention mechanisms for modeling long-range dependencies in vision tasks. Additionally, the Fusion module enables seamless interaction between these components. Extensive experiments on classification, detection, and segmentation tasks demonstrate that vGamba achieves a superior trade-off between accuracy and computational efficiency, outperforming several existing models.</li>
</ul>

<h3>Title: Delving Deep into Semantic Relation Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyi Yan, Kangjun Liu, Qixiang Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21269">https://arxiv.org/abs/2503.21269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21269">https://arxiv.org/pdf/2503.21269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21269]] Delving Deep into Semantic Relation Distillation(https://arxiv.org/abs/2503.21269)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Knowledge distillation has become a cornerstone technique in deep learning, facilitating the transfer of knowledge from complex models to lightweight counterparts. Traditional distillation approaches focus on transferring knowledge at the instance level, but fail to capture nuanced semantic relationships within the data. In response, this paper introduces a novel methodology, Semantics-based Relation Knowledge Distillation (SeRKD), which reimagines knowledge distillation through a semantics-relation lens among each sample. By leveraging semantic components, \ie, superpixels, SeRKD enables a more comprehensive and context-aware transfer of knowledge, which skillfully integrates superpixel-based semantic extraction with relation-based knowledge distillation for a sophisticated model compression and distillation. Particularly, the proposed method is naturally relevant in the domain of Vision Transformers (ViTs), where visual tokens serve as fundamental units of representation. Experimental evaluations on benchmark datasets demonstrate the superiority of SeRKD over existing methods, underscoring its efficacy in enhancing model performance and generalization capabilities.</li>
</ul>

<h3>Title: R-PRM: Reasoning-Driven Process Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, Shujian Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21295">https://arxiv.org/abs/2503.21295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21295">https://arxiv.org/pdf/2503.21295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21295]] R-PRM: Reasoning-Driven Process Reward Modeling(https://arxiv.org/abs/2503.21295)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) inevitably make mistakes when performing step-by-step mathematical reasoning. Process Reward Models (PRMs) have emerged as a promising solution by evaluating each reasoning step. However, existing PRMs typically output evaluation scores directly, limiting both learning efficiency and evaluation accuracy, which is further exacerbated by the scarcity of annotated data. To address these issues, we propose Reasoning-Driven Process Reward Modeling (R-PRM). First, we leverage stronger LLMs to generate seed data from limited annotations, effectively bootstrapping our model's reasoning capabilities and enabling comprehensive step-by-step evaluation. Second, we further enhance performance through preference optimization, without requiring additional annotated data. Third, we introduce inference-time scaling to fully harness the model's reasoning potential. Extensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and PRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores, respectively. When applied to guide mathematical reasoning, R-PRM achieves consistent accuracy improvements of over 8.5 points across six challenging datasets. Further analysis reveals that R-PRM exhibits more comprehensive evaluation and stronger generalization capabilities, thereby highlighting its significant potential.</li>
</ul>

<h3>Title: DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data</h3>
<ul>
<li><strong>Authors: </strong>Dorde Popovic, Amin Sadeghi, Ting Yu, Sanjay Chawla, Issa Khalil</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21305">https://arxiv.org/abs/2503.21305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21305">https://arxiv.org/pdf/2503.21305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21305]] DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data(https://arxiv.org/abs/2503.21305)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>Backdoor attacks are among the most effective, practical, and stealthy attacks in deep learning. In this paper, we consider a practical scenario where a developer obtains a deep model from a third party and uses it as part of a safety-critical system. The developer wants to inspect the model for potential backdoors prior to system deployment. We find that most existing detection techniques make assumptions that are not applicable to this scenario. In this paper, we present a novel framework for detecting backdoors under realistic restrictions. We generate candidate triggers by deductively searching over the space of possible triggers. We construct and optimize a smoothed version of Attack Success Rate as our search objective. Starting from a broad class of template attacks and just using the forward pass of a deep model, we reverse engineer the backdoor attack. We conduct extensive evaluation on a wide range of attacks, models, and datasets, with our technique performing almost perfectly across these settings.</li>
</ul>

<h3>Title: InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression</h3>
<ul>
<li><strong>Authors: </strong>Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, Huo Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21307">https://arxiv.org/abs/2503.21307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21307">https://arxiv.org/pdf/2503.21307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21307]] InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression(https://arxiv.org/abs/2503.21307)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Most multimodal large language models (MLLMs) treat visual tokens as "a sequence of text", integrating them with text tokens into a large language model (LLM). However, a great quantity of visual tokens significantly increases the demand for computational resources and time. In this paper, we propose InternVL-X, which outperforms the InternVL model in both performance and efficiency by incorporating three visual token compression methods. First, we propose a novel vision-language projector, PVTC. This component integrates adjacent visual embeddings to form a local query and utilizes the transformed CLS token as a global query, then performs point-to-region cross-attention through these local and global queries to more effectively convert visual features. Second, we present a layer-wise visual token compression module, LVTC, which compresses tokens in the LLM shallow layers and then expands them through upsampling and residual connections in the deeper layers. This significantly enhances the model computational efficiency. Futhermore, we propose an efficient high resolution slicing method, RVTC, which dynamically adjusts the number of visual tokens based on image area or length filtering. RVTC greatly enhances training efficiency with only a slight reduction in performance. By utilizing 20% or fewer visual tokens, InternVL-X achieves state-of-the-art performance on 7 public MLLM benchmarks, and improves the average metric by 2.34% across 12 tasks.</li>
</ul>

<h3>Title: FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, Haokun Wen, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21309">https://arxiv.org/abs/2503.21309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21309">https://arxiv.org/pdf/2503.21309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21309]] FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval(https://arxiv.org/abs/2503.21309)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Composed Image Retrieval (CIR) facilitates image retrieval through a multimodal query consisting of a reference image and modification text. The reference image defines the retrieval context, while the modification text specifies desired alterations. However, existing CIR datasets predominantly employ coarse-grained modification text (CoarseMT), which inadequately captures fine-grained retrieval intents. This limitation introduces two key challenges: (1) ignoring detailed differences leads to imprecise positive samples, and (2) greater ambiguity arises when retrieving visually similar images. These issues degrade retrieval accuracy, necessitating manual result filtering or repeated queries. To address these limitations, we develop a robust fine-grained CIR data annotation pipeline that minimizes imprecise positive samples and enhances CIR systems' ability to discern modification intents accurately. Using this pipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained CIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR, the first CIR framework explicitly designed to parse the modification text. FineCIR effectively captures fine-grained modification semantics and aligns them with ambiguous visual entities, enhancing retrieval precision. Extensive experiments demonstrate that FineCIR consistently outperforms state-of-the-art CIR baselines on both fine-grained and traditional CIR benchmark datasets. Our FineCIR code and fine-grained CIR datasets are available at this https URL.</li>
</ul>

<h3>Title: HORT: Monocular Hand-held Objects Reconstruction with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zerui Chen, Rolandos Alexandros Potamias, Shizhe Chen, Cordelia Schmid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21313">https://arxiv.org/abs/2503.21313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21313">https://arxiv.org/pdf/2503.21313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21313]] HORT: Monocular Hand-held Objects Reconstruction with Transformers(https://arxiv.org/abs/2503.21313)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Reconstructing hand-held objects in 3D from monocular images remains a significant challenge in computer vision. Most existing approaches rely on implicit 3D representations, which produce overly smooth reconstructions and are time-consuming to generate explicit 3D shapes. While more recent methods directly reconstruct point clouds with diffusion models, the multi-step denoising makes high-resolution reconstruction inefficient. To address these limitations, we propose a transformer-based model to efficiently reconstruct dense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine strategy, first generating a sparse point cloud from the image and progressively refining it into a dense representation using pixel-aligned image features. To enhance reconstruction accuracy, we integrate image features with 3D hand geometry to jointly predict the object point cloud and its pose relative to the hand. Our model is trained end-to-end for optimal performance. Experimental results on both synthetic and real datasets demonstrate that our method achieves state-of-the-art accuracy with much faster inference speed, while generalizing well to in-the-wild images.</li>
</ul>

<h3>Title: Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack</h3>
<ul>
<li><strong>Authors: </strong>Cheng Wang, Yiwei Wang, Yujun Cai, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21315">https://arxiv.org/abs/2503.21315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21315">https://arxiv.org/pdf/2503.21315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21315]] Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack(https://arxiv.org/abs/2503.21315)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) systems enhance large language models by incorporating external knowledge, addressing issues like outdated internal knowledge and hallucination. However, their reliance on external knowledge bases makes them vulnerable to corpus poisoning attacks, where adversarial passages can be injected to manipulate retrieval results. Existing methods for crafting such passages, such as random token replacement or training inversion models, are often slow and computationally expensive, requiring either access to retriever's gradients or large computational resources. To address these limitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an efficient black-box method that leverages two key properties of retrievers: insensitivity to token order and bias towards influential tokens. By focusing on these characteristics, DIGA dynamically adjusts its genetic operations to generate effective adversarial passages with significantly reduced time and memory usage. Our experimental evaluation shows that DIGA achieves superior efficiency and scalability compared to existing methods, while maintaining comparable or better attack success rates across multiple datasets.</li>
</ul>

<h3>Title: DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ling Feng, Tianyu Xie, Wei Ma, Ruijie Fu, Yingxiao Zhang, Jun Li, Bei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21323">https://arxiv.org/abs/2503.21323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21323">https://arxiv.org/pdf/2503.21323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21323]] DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset(https://arxiv.org/abs/2503.21323)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>The modernization of smart farming is a way to improve agricultural production efficiency, and improve the agricultural production environment. Although many large models have achieved high accuracy in the task of object recognition and segmentation, they cannot really be put into use in the farming industry due to their own poor interpretability and limitations in computational volume. In this paper, we built AnYue Shelduck Dateset, which contains a total of 1951 Shelduck datasets, and performed target detection and segmentation annotation with the help of professional annotators. Based on AnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and powerful module for duck identification based on real shelduckfarms. First of all, using the YOLOv8 module designed to divide the mahjong between them, Precision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on the test set. Again using the DuckSegmentation segmentation model, DuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation was used as the teacher model, and through knowledge distillation, Deeplabv3 r50 was used as the student model, and the final student model achieved 94.49% mIoU on the test set. The method provides a new way of thinking in practical sisal duck smart farming.</li>
</ul>

<h3>Title: ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback</h3>
<ul>
<li><strong>Authors: </strong>Taewon Yun, Jihwan Oh, Hyangsuk Min, Yuho Lee, Jihwan Bang, Jason Cai, Hwanjun Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21332">https://arxiv.org/abs/2503.21332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21332">https://arxiv.org/pdf/2503.21332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21332]] ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback(https://arxiv.org/abs/2503.21332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released.</li>
</ul>

<h3>Title: Scalable Expectation Estimation with Subtractive Mixture Models</h3>
<ul>
<li><strong>Authors: </strong>Lena Zellinger, Nicola Branchini, Víctor Elvira, Antonio Vergari</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21346">https://arxiv.org/abs/2503.21346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21346">https://arxiv.org/pdf/2503.21346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21346]] Scalable Expectation Estimation with Subtractive Mixture Models(https://arxiv.org/abs/2503.21346)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many Monte Carlo (MC) and importance sampling (IS) methods use mixture models (MMs) for their simplicity and ability to capture multimodal distributions. Recently, subtractive mixture models (SMMs), i.e. MMs with negative coefficients, have shown greater expressiveness and success in generative modeling. However, their negative parameters complicate sampling, requiring costly auto-regressive techniques or accept-reject algorithms that do not scale in high dimensions. In this work, we use the difference representation of SMMs to construct an unbiased IS estimator ($\Delta\text{Ex}$) that removes the need to sample from the SMM, enabling high-dimensional expectation estimation with SMMs. In our experiments, we show that $\Delta\text{Ex}$ can achieve comparable estimation quality to auto-regressive sampling while being considerably faster in MC estimation. Moreover, we conduct initial experiments with $\Delta\text{Ex}$ using hand-crafted proposals, gaining first insights into how to construct safe proposals for $\Delta\text{Ex}$.</li>
</ul>

<h3>Title: Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization Effectiveness on Cardiology reports and Discharge records</h3>
<ul>
<li><strong>Authors: </strong>Noah Losch, Lucas Plagwitz, Antonius Büscher, Julian Varghese</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21349">https://arxiv.org/abs/2503.21349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21349">https://arxiv.org/pdf/2503.21349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21349]] Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization Effectiveness on Cardiology reports and Discharge records(https://arxiv.org/abs/2503.21349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigate the effectiveness of fine-tuning large language models (LLMs) on small medical datasets for text classification and named entity recognition tasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge dataset, we demonstrate that fine-tuning small LLMs locally on limited training data can improve performance achieving comparable results to larger models. Our experiments show that fine-tuning improves performance on both tasks, with notable gains observed with as few as 200-300 training examples. Overall, the study highlights the potential of task-specific fine-tuning of LLMs for automating clinical workflows and efficiently extracting structured data from unstructured medical text.</li>
</ul>

<h3>Title: Investigating the Duality of Interpretability and Explainability in Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Moncef Garouani, Josiane Mothe, Ayah Barhrhouj, Julien Aligon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21356">https://arxiv.org/abs/2503.21356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21356">https://arxiv.org/pdf/2503.21356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21356]] Investigating the Duality of Interpretability and Explainability in Machine Learning(https://arxiv.org/abs/2503.21356)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The rapid evolution of machine learning (ML) has led to the widespread adoption of complex "black box" models, such as deep neural networks and ensemble methods. These models exhibit exceptional predictive performance, making them invaluable for critical decision-making across diverse domains within society. However, their inherently opaque nature raises concerns about transparency and interpretability, making them untrustworthy decision support systems. To alleviate such a barrier to high-stakes adoption, research community focus has been on developing methods to explain black box models as a means to address the challenges they pose. Efforts are focused on explaining these models instead of developing ones that are inherently interpretable. Designing inherently interpretable models from the outset, however, can pave the path towards responsible and beneficial applications in the field of ML. In this position paper, we clarify the chasm between explaining black boxes and adopting inherently interpretable models. We emphasize the imperative need for model interpretability and, following the purpose of attaining better (i.e., more effective or efficient w.r.t. predictive performance) and trustworthy predictors, provide an experimental evaluation of latest hybrid learning methods that integrates symbolic knowledge into neural network predictors. We demonstrate how interpretable hybrid models could potentially supplant black box ones in different domains.</li>
</ul>

<h3>Title: From User Preferences to Optimization Constraints Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Manuela Sanguinetti, Alessandra Perniciano, Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Maurizio Atzori</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21360">https://arxiv.org/abs/2503.21360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21360">https://arxiv.org/pdf/2503.21360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21360]] From User Preferences to Optimization Constraints Using Large Language Models(https://arxiv.org/abs/2503.21360)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work explores using Large Language Models (LLMs) to translate user preferences into energy optimization constraints for home appliances. We describe a task where natural language user utterances are converted into formal constraints for smart appliances, within the broader context of a renewable energy community (REC) and in the Italian scenario. We evaluate the effectiveness of various LLMs currently available for Italian in translating these preferences resorting to classical zero-shot, one-shot, and few-shot learning settings, using a pilot dataset of Italian user requests paired with corresponding formal constraint representation. Our contributions include establishing a baseline performance for this task, publicly releasing the dataset and code for further research, and providing insights on observed best practices and limitations of LLMs in this particular domain</li>
</ul>

<h3>Title: Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21380">https://arxiv.org/abs/2503.21380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21380">https://arxiv.org/pdf/2503.21380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21380]] Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models(https://arxiv.org/abs/2503.21380)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: this https URL.</li>
</ul>

<h3>Title: Controlling Large Language Model with Latent Actions</h3>
<ul>
<li><strong>Authors: </strong>Chengxing Jia, Ziniu Li, Pengyuan Wang, Yi-Chen Li, Zhenyu Hou, Yuxiao Dong, Yang Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21383">https://arxiv.org/abs/2503.21383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21383">https://arxiv.org/pdf/2503.21383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21383]] Controlling Large Language Model with Latent Actions(https://arxiv.org/abs/2503.21383)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of defining the action space. This paper studies learning a compact latent action space to enhance the controllability and exploration of RL for LLMs. We propose Controlling Large Language Models with Latent Actions (CoLA), a framework that integrates a latent action space into pre-trained LLMs. We apply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that, compared to RL with token-level actions, CoLA's latent action enables greater semantic diversity in text generation. For enhancing downstream tasks, we show that CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing the baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo Tree Search variant. Furthermore, CoLA with RL consistently improves performance on agent-based tasks without degrading the pre-trained LLM's capabilities, unlike the baseline. Finally, CoLA reduces computation time by half in tasks involving enhanced thinking prompts for LLMs by RL. These results highlight CoLA's potential to advance RL-based adaptation of LLMs for downstream applications.</li>
</ul>

<h3>Title: An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses</h3>
<ul>
<li><strong>Authors: </strong>Rohitash Chandra, Aryan Chaudhary, Yeshwanth Rayavarapu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21393">https://arxiv.org/abs/2503.21393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21393">https://arxiv.org/pdf/2503.21393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21393]] An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses(https://arxiv.org/abs/2503.21393)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language models (LLMs) have been prominent for language translation, including low-resource languages. There has been limited study about the assessment of the quality of translations generated by LLMs, including Gemini, GPT and Google Translate. In this study, we address this limitation by using semantic and sentiment analysis of selected LLMs for Indian languages, including Sanskrit, Telugu and Hindi. We select prominent texts that have been well translated by experts and use LLMs to generate their translations to English, and then we provide a comparison with selected expert (human) translations. Our findings suggest that while LLMs have made significant progress in translation accuracy, challenges remain in preserving sentiment and semantic integrity, especially in figurative and philosophical contexts. The sentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving the sentiments for the Bhagavad Gita (Sanskrit-English) translations when compared to Google Translate. We observed a similar trend for the case of Tamas (Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs similarly to GPT-3.5 in the translation in terms of sentiments for the three languages. We found that LLMs are generally better at translation for capturing sentiments when compared to Google Translate.</li>
</ul>

<h3>Title: VALLR: Visual ASR Language Model for Lip Reading</h3>
<ul>
<li><strong>Authors: </strong>Marshall Thomas, Edward Fish, Richard Bowden</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21408">https://arxiv.org/abs/2503.21408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21408">https://arxiv.org/pdf/2503.21408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21408]] VALLR: Visual ASR Language Model for Lip Reading(https://arxiv.org/abs/2503.21408)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach.</li>
</ul>

<h3>Title: Diffusion Image Prior</h3>
<ul>
<li><strong>Authors: </strong>Hamadi Chihaoui, Paolo Favaro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21410">https://arxiv.org/abs/2503.21410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21410">https://arxiv.org/pdf/2503.21410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21410]] Diffusion Image Prior(https://arxiv.org/abs/2503.21410)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Zero-shot image restoration (IR) methods based on pretrained diffusion models have recently achieved significant success. These methods typically require at least a parametric form of the degradation model. However, in real-world scenarios, the degradation may be too complex to define explicitly. To handle this general case, we introduce the Diffusion Image Prior (DIIP). We take inspiration from the Deep Image Prior (DIP)[16], since it can be used to remove artifacts without the need for an explicit degradation model. However, in contrast to DIP, we find that pretrained diffusion models offer a much stronger prior, despite being trained without knowledge from corrupted data. We show that, the optimization process in DIIP first reconstructs a clean version of the image before eventually overfitting to the degraded input, but it does so for a broader range of degradations than DIP. In light of this result, we propose a blind image restoration (IR) method based on early stopping, which does not require prior knowledge of the degradation model. We validate DIIP on various degradation-blind IR tasks, including JPEG artifact removal, waterdrop removal, denoising and super-resolution with state-of-the-art results.</li>
</ul>

<h3>Title: AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram Model</h3>
<ul>
<li><strong>Authors: </strong>Sen Zhang, Qingqing Ye, Haibo Hu, Jianliang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21426">https://arxiv.org/abs/2503.21426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21426">https://arxiv.org/pdf/2503.21426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21426]] AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram Model(https://arxiv.org/abs/2503.21426)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The skip-gram model (SGM), which employs a neural network to generate node vectors, serves as the basis for numerous popular graph embedding techniques. However, since the training datasets contain sensitive linkage information, the parameters of a released SGM may encode private information and pose significant privacy risks. Differential privacy (DP) is a rigorous standard for protecting individual privacy in data analysis. Nevertheless, when applying differential privacy to skip-gram in graphs, it becomes highly challenging due to the complex link relationships, which potentially result in high sensitivity and necessitate substantial noise injection. To tackle this challenge, we present AdvSGM, a differentially private skip-gram for graphs via adversarial training. Our core idea is to leverage adversarial training to privatize skip-gram while improving its utility. Towards this end, we develop a novel adversarial training module by devising two optimizable noise terms that correspond to the parameters of a skip-gram. By fine-tuning the weights between modules within AdvSGM, we can achieve differentially private gradient updates without additional noise injection. Extensive experimental results on six real-world graph datasets show that AdvSGM preserves high data utility across different downstream tasks.</li>
</ul>

<h3>Title: Stochastic Engrams for Efficient Continual Learning with Binarized Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Isabelle Aguilar, Luis Fernando Herbozo Contreras, Omid Kavehei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21436">https://arxiv.org/abs/2503.21436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21436">https://arxiv.org/pdf/2503.21436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21436]] Stochastic Engrams for Efficient Continual Learning with Binarized Neural Networks(https://arxiv.org/abs/2503.21436)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The ability to learn continuously in artificial neural networks (ANNs) is often limited by catastrophic forgetting, a phenomenon in which new knowledge becomes dominant. By taking mechanisms of memory encoding in neuroscience (aka. engrams) as inspiration, we propose a novel approach that integrates stochastically-activated engrams as a gating mechanism for metaplastic binarized neural networks (mBNNs). This method leverages the computational efficiency of mBNNs combined with the robustness of probabilistic memory traces to mitigate forgetting and maintain the model's reliability. Previously validated metaplastic optimization techniques have been incorporated to enhance synaptic stability further. Compared to baseline binarized models and benchmark fully connected continual learning approaches, our method is the only strategy capable of reaching average accuracies over 20% in class-incremental scenarios and achieving comparable domain-incremental results to full precision state-of-the-art methods. Furthermore, we achieve a significant reduction in peak GPU and RAM usage, under 5% and 20%, respectively. Our findings demonstrate (A) an improved stability vs. plasticity trade-off, (B) a reduced memory intensiveness, and (C) an enhanced performance in binarized architectures. By uniting principles of neuroscience and efficient computing, we offer new insights into the design of scalable and robust deep learning systems.</li>
</ul>

<h3>Title: Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid Self-Attention U-Nets in Aerial Imagery</h3>
<ul>
<li><strong>Authors: </strong>Anis Ur Rahman, Einari Heinaro, Mete Ahishali, Samuli Junttila</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21438">https://arxiv.org/abs/2503.21438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21438">https://arxiv.org/pdf/2503.21438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21438]] Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid Self-Attention U-Nets in Aerial Imagery(https://arxiv.org/abs/2503.21438)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Mapping standing dead trees is critical for assessing forest health, monitoring biodiversity, and mitigating wildfire risks, for which aerial imagery has proven useful. However, dense canopy structures, spectral overlaps between living and dead vegetation, and over-segmentation errors limit the reliability of existing methods. This study introduces a hybrid postprocessing framework that refines deep learning-based tree segmentation by integrating watershed algorithms with adaptive filtering, enhancing boundary delineation, and reducing false positives in complex forest environments. Tested on high-resolution aerial imagery from boreal forests, the framework improved instance-level segmentation accuracy by 41.5% and reduced positional errors by 57%, demonstrating robust performance in densely vegetated regions. By balancing detection accuracy and over-segmentation artifacts, the method enabled the precise identification of individual dead trees, which is critical for ecological monitoring. The framework's computational efficiency supports scalable applications, such as wall-to-wall tree mortality mapping over large geographic regions using aerial or satellite imagery. These capabilities directly benefit wildfire risk assessment (identifying fuel accumulations), carbon stock estimation (tracking emissions from decaying biomass), and precision forestry (targeting salvage loggings). By bridging advanced remote sensing techniques with practical forest management needs, this work advances tools for large-scale ecological conservation and climate resilience planning.</li>
</ul>

<h3>Title: Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Lucas Nunes, Rodrigo Marcuzzi, Jens Behley, Cyrill Stachniss</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21449">https://arxiv.org/abs/2503.21449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21449">https://arxiv.org/pdf/2503.21449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21449]] Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving(https://arxiv.org/abs/2503.21449)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic scene understanding is crucial for robotics and computer vision applications. In autonomous driving, 3D semantic segmentation plays an important role for enabling safe navigation. Despite significant advances in the field, the complexity of collecting and annotating 3D data is a bottleneck in this developments. To overcome that data annotation limitation, synthetic simulated data has been used to generate annotated data on demand. There is still however a domain gap between real and simulated data. More recently, diffusion models have been in the spotlight, enabling close-to-real data synthesis. Those generative models have been recently applied to the 3D data domain for generating scene-scale data with semantic annotations. Still, those methods either rely on image projection or decoupled models trained with different resolutions in a coarse-to-fine manner. Such intermediary representations impact the generated data quality due to errors added in those transformations. In this work, we propose a novel approach able to generate 3D semantic scene-scale data without relying on any projection or decoupled trained multi-resolution models, achieving more realistic semantic scene data generation compared to previous state-of-the-art methods. Besides improving 3D semantic scene-scale data synthesis, we thoroughly evaluate the use of the synthetic scene samples as labeled data to train a semantic segmentation network. In our experiments, we show that using the synthetic annotated data generated by our method as training data together with the real semantic segmentation labels, leads to an improvement in the semantic segmentation model performance. Our results show the potential of generated scene-scale point clouds to generate more training data to extend existing datasets, reducing the data annotation effort. Our code is available at this https URL.</li>
</ul>

<h3>Title: FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqin Wang, Xusen Ma, Xianxu Hou, Meidan Ding, Yudong Li, Junliang Chen, Wenting Chen, Xiaoyang Peng, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21457">https://arxiv.org/abs/2503.21457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21457">https://arxiv.org/pdf/2503.21457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21457]] FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs(https://arxiv.org/abs/2503.21457)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in various tasks. However, effectively evaluating these MLLMs on face perception remains largely unexplored. To address this gap, we introduce FaceBench, a dataset featuring hierarchical multi-view and multi-level attributes specifically designed to assess the comprehensive face perception abilities of MLLMs. Initially, we construct a hierarchical facial attribute structure, which encompasses five views with up to three levels of attributes, totaling over 210 attributes and 700 attribute values. Based on the structure, the proposed FaceBench consists of 49,919 visual question-answering (VQA) pairs for evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop a robust face perception MLLM baseline, Face-LLaVA, by training with our proposed face VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVA are conducted to test their face perception ability, with results also compared against human performance. The results reveal that, the existing MLLMs are far from satisfactory in understanding the fine-grained facial attributes, while our Face-LLaVA significantly outperforms existing open-source models with a small amount of training data and is comparable to commercial ones like GPT-4o and Gemini. The dataset will be released at this https URL.</li>
</ul>

<h3>Title: Large Language Model Agent: A Survey on Methodology, Applications and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xian Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, Ming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21460">https://arxiv.org/abs/2503.21460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21460">https://arxiv.org/pdf/2503.21460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21460]] Large Language Model Agent: A Survey on Methodology, Applications and Challenges(https://arxiv.org/abs/2503.21460)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at this https URL.</li>
</ul>

<h3>Title: Unveiling Latent Information in Transaction Hashes: Hypergraph Learning for Ethereum Ponzi Scheme Detection</h3>
<ul>
<li><strong>Authors: </strong>Junhao Wu, Yixin Yang, Chengxiang Jin, Silu Mu, Xiaolei Qian, Jiajun Zhou, Shanqing Yu, Qi Xuan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21463">https://arxiv.org/abs/2503.21463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21463">https://arxiv.org/pdf/2503.21463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21463]] Unveiling Latent Information in Transaction Hashes: Hypergraph Learning for Ethereum Ponzi Scheme Detection(https://arxiv.org/abs/2503.21463)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>With the widespread adoption of Ethereum, financial frauds such as Ponzi schemes have become increasingly rampant in the blockchain ecosystem, posing significant threats to the security of account assets. Existing Ethereum fraud detection methods typically model account transactions as graphs, but this approach primarily focuses on binary transactional relationships between accounts, failing to adequately capture the complex multi-party interaction patterns inherent in Ethereum. To address this, we propose a hypergraph modeling method for the Ponzi scheme detection method in Ethereum, called HyperDet. Specifically, we treat transaction hashes as hyperedges that connect all the relevant accounts involved in a transaction. Additionally, we design a two-step hypergraph sampling strategy to significantly reduce computational complexity. Furthermore, we introduce a dual-channel detection module, including the hypergraph detection channel and the hyper-homo graph detection channel, to be compatible with existing detection methods. Experimental results show that, compared to traditional homogeneous graph-based methods, the hyper-homo graph detection channel achieves significant performance improvements, demonstrating the superiority of hypergraph in Ponzi scheme detection. This research offers innovations for modeling complex relationships in blockchain data.</li>
</ul>

<h3>Title: Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection</h3>
<ul>
<li><strong>Authors: </strong>Ryan Marinelli, Josef Pichlmeier, Tamas Bisztray</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21464">https://arxiv.org/abs/2503.21464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21464">https://arxiv.org/pdf/2503.21464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21464]] Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection(https://arxiv.org/abs/2503.21464)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>In this work, we propose a metric called Number of Thoughts (NofT) to determine the difficulty of tasks pre-prompting and support Large Language Models (LLMs) in production contexts. By setting thresholds based on the number of thoughts, this metric can discern the difficulty of prompts and support more effective prompt routing. A 2% decrease in latency is achieved when routing prompts from the MathInstruct dataset through quantized, distilled versions of Deepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this metric can be used to detect adversarial prompts used in prompt injection attacks with high efficacy. The Number of Thoughts can inform a classifier that achieves 95% accuracy in adversarial prompt detection. Our experiments ad datasets used are available on our GitHub page: this https URL.</li>
</ul>

<h3>Title: Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures</h3>
<ul>
<li><strong>Authors: </strong>Deependra Singh, Saksham Agarwal, Subhankar Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21465">https://arxiv.org/abs/2503.21465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21465">https://arxiv.org/pdf/2503.21465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21465]] Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures(https://arxiv.org/abs/2503.21465)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Our research is motivated by the urgent global issue of a large population affected by retinal diseases, which are evenly distributed but underserved by specialized medical expertise, particularly in non-urban areas. Our primary objective is to bridge this healthcare gap by developing a comprehensive diagnostic system capable of accurately predicting retinal diseases solely from fundus images. However, we faced significant challenges due to limited, diverse datasets and imbalanced class distributions. To overcome these issues, we have devised innovative strategies. Our research introduces novel approaches, utilizing hybrid models combining deeper Convolutional Neural Networks (CNNs), Transformer encoders, and ensemble architectures sequentially and in parallel to classify retinal fundus images into 20 disease labels. Our overarching goal is to assess these advanced models' potential in practical applications, with a strong focus on enhancing retinal disease diagnosis accuracy across a broader spectrum of conditions. Importantly, our efforts have surpassed baseline model results, with the C-Tran ensemble model emerging as the leader, achieving a remarkable model score of 0.9166, surpassing the baseline score of 0.9. Additionally, experiments with the IEViT model showcased equally promising outcomes with improved computational efficiency. We've also demonstrated the effectiveness of dynamic patch extraction and the integration of domain knowledge in computer vision tasks. In summary, our research strives to contribute significantly to retinal disease diagnosis, addressing the critical need for accessible healthcare solutions in underserved regions while aiming for comprehensive and accurate disease prediction.</li>
</ul>

<h3>Title: OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs</h3>
<ul>
<li><strong>Authors: </strong>John Murzaku, Owen Rambow</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21480">https://arxiv.org/abs/2503.21480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21480">https://arxiv.org/pdf/2503.21480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21480]] OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs(https://arxiv.org/abs/2503.21480)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The use of omni-LLMs (large language models that accept any modality as input), particularly for multimodal cognitive state tasks involving speech, is understudied. We present OmniVox, the first systematic evaluation of four omni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely used multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot omni-LLMs outperform or are competitive with fine-tuned audio models. Alongside our audio-only evaluation, we also evaluate omni-LLMs on text only and text and audio. We present acoustic prompting, an audio-specific prompting strategy for omni-LLMs which focuses on acoustic feature analysis, conversation context analysis, and step-by-step reasoning. We compare our acoustic prompting to minimal prompting and full chain-of-thought prompting techniques. We perform a context window analysis on IEMOCAP and MELD, and find that using context helps, especially on IEMOCAP. We conclude with an error analysis on the generated acoustic reasoning outputs from the omni-LLMs.</li>
</ul>

<h3>Title: Invert2Restore: Zero-Shot Degradation-Blind Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Hamadi Chihaoui, Paolo Favaro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21486">https://arxiv.org/abs/2503.21486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21486">https://arxiv.org/pdf/2503.21486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21486]] Invert2Restore: Zero-Shot Degradation-Blind Image Restoration(https://arxiv.org/abs/2503.21486)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Two of the main challenges of image restoration in real-world scenarios are the accurate characterization of an image prior and the precise modeling of the image degradation operator. Pre-trained diffusion models have been very successfully used as image priors in zero-shot image restoration methods. However, how to best handle the degradation operator is still an open problem. In real-world data, methods that rely on specific parametric assumptions about the degradation model often face limitations in their applicability. To address this, we introduce Invert2Restore, a zero-shot, training-free method that operates in both fully blind and partially blind settings -- requiring no prior knowledge of the degradation model or only partial knowledge of its parametric form without known parameters. Despite this, Invert2Restore achieves high-fidelity results and generalizes well across various types of image degradation. It leverages a pre-trained diffusion model as a deterministic mapping between normal samples and undistorted image samples. The key insight is that the input noise mapped by a diffusion model to a degraded image lies in a low-probability density region of the standard normal distribution. Thus, we can restore the degraded image by carefully guiding its input noise toward a higher-density region. We experimentally validate Invert2Restore across several image restoration tasks, demonstrating that it achieves state-of-the-art performance in scenarios where the degradation operator is either unknown or partially known.</li>
</ul>

<h3>Title: Shape Modeling of Longitudinal Medical Images: From Diffeomorphic Metric Mapping to Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Edwin Tay, Nazli Tümer, Amir A. Zadpoor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21489">https://arxiv.org/abs/2503.21489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21489">https://arxiv.org/pdf/2503.21489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21489]] Shape Modeling of Longitudinal Medical Images: From Diffeomorphic Metric Mapping to Deep Learning(https://arxiv.org/abs/2503.21489)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Living biological tissue is a complex system, constantly growing and changing in response to external and internal stimuli. These processes lead to remarkable and intricate changes in shape. Modeling and understanding both natural and pathological (or abnormal) changes in the shape of anatomical structures is highly relevant, with applications in diagnostic, prognostic, and therapeutic healthcare. Nevertheless, modeling the longitudinal shape change of biological tissue is a non-trivial task due to its inherent nonlinear nature. In this review, we highlight several existing methodologies and tools for modeling longitudinal shape change (i.e., spatiotemporal shape modeling). These methods range from diffeomorphic metric mapping to deep-learning based approaches (e.g., autoencoders, generative networks, recurrent neural networks, etc.). We discuss the synergistic combinations of existing technologies and potential directions for future research, underscoring key deficiencies in the current research landscape.</li>
</ul>

<h3>Title: Advancing CAN Network Security through RBM-Based Synthetic Attack Data Generation for Intrusion Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Huacheng Li, Jingyong Su, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21496">https://arxiv.org/abs/2503.21496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21496">https://arxiv.org/pdf/2503.21496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21496]] Advancing CAN Network Security through RBM-Based Synthetic Attack Data Generation for Intrusion Detection Systems(https://arxiv.org/abs/2503.21496)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>The rapid development of network technologies and industrial intelligence has augmented the connectivity and intelligence within the automotive industry. Notably, in the Internet of Vehicles (IoV), the Controller Area Network (CAN), which is crucial for the communication of electronic control units but lacks inbuilt security measures, has become extremely vulnerable to severe cybersecurity threats. Meanwhile, the efficacy of Intrusion Detection Systems (IDS) is hampered by the scarcity of sufficient attack data for robust model training. To overcome this limitation, we introduce a novel methodology leveraging the Restricted Boltzmann Machine (RBM) to generate synthetic CAN attack data, thereby producing training datasets with a more balanced sample distribution. Specifically, we design a CAN Data Processing Module for transforming raw CAN data into an RBM-trainable format, and a Negative Sample Generation Module to generate data reflecting the distribution of CAN data frames denoting network intrusions. Experimental results show the generated data significantly improves IDS performance, with CANet accuracy rising from 0.6477 to 0.9725 and EfficientNet from 0.1067 to 0.1555. Code is available at this https URL.</li>
</ul>

<h3>Title: OpenHuEval: Evaluating Large Language Model on Hungarian Specifics</h3>
<ul>
<li><strong>Authors: </strong>Haote Yang, Xingjian Wei, Jiang Wu, Noémi Ligeti-Nagy, Jiaxing Sun, Yinfan Wang, Zijian Győző Yang, Junyuan Gao, Jingchao Wang, Bowen Jiang, Shasha Wang, Nanjun Yu, Zihao Zhang, Shixin Hong, Hongwei Liu, Wei Li, Songyang Zhang, Dahua Lin, Lijun Wu, Gábor Prószéky, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21500">https://arxiv.org/abs/2503.21500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21500">https://arxiv.org/pdf/2503.21500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21500]] OpenHuEval: Evaluating Large Language Model on Hungarian Specifics(https://arxiv.org/abs/2503.21500)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We introduce OpenHuEval, the first benchmark for LLMs focusing on the Hungarian language and specifics. OpenHuEval is constructed from a vast collection of Hungarian-specific materials sourced from multiple origins. In the construction, we incorporated the latest design principles for evaluating LLMs, such as using real user queries from the internet, emphasizing the assessment of LLMs' generative capabilities, and employing LLM-as-judge to enhance the multidimensionality and accuracy of evaluations. Ultimately, OpenHuEval encompasses eight Hungarian-specific dimensions, featuring five tasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive, in-depth, and scientifically accurate assessment of LLM performance in the context of the Hungarian language and its specifics. We evaluated current mainstream LLMs, including both traditional LLMs and recently developed Large Reasoning Models. The results demonstrate the significant necessity for evaluation and model optimization tailored to the Hungarian language and specifics. We also established the framework for analyzing the thinking processes of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms of these models in non-English languages, with Hungarian serving as a representative example. We will release OpenHuEval at this https URL .</li>
</ul>

<h3>Title: Keyword-Oriented Multimodal Modeling for Euphemism Identification</h3>
<ul>
<li><strong>Authors: </strong>Yuxue Hu, Junsong Li, Meixuan Chen, Dongyu Su, Tongguan Wang, Ying Sha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21504">https://arxiv.org/abs/2503.21504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21504">https://arxiv.org/pdf/2503.21504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21504]] Keyword-Oriented Multimodal Modeling for Euphemism Identification(https://arxiv.org/abs/2503.21504)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Euphemism identification deciphers the true meaning of euphemisms, such as linking "weed" (euphemism) to "marijuana" (target keyword) in illicit texts, aiding content moderation and combating underground markets. While existing methods are primarily text-based, the rise of social media highlights the need for multimodal analysis, incorporating text, images, and audio. However, the lack of multimodal datasets for euphemisms limits further research. To address this, we regard euphemisms and their corresponding target keywords as keywords and first introduce a keyword-oriented multimodal corpus of euphemisms (KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including text, images, and speech. We further propose a keyword-oriented multimodal euphemism identification method (KOM-EI), which uses cross-modal feature alignment and dynamic fusion modules to explicitly utilize the visual and audio features of the keywords for efficient euphemism identification. Extensive experiments demonstrate that KOM-EI outperforms state-of-the-art models and large language models, and show the importance of our multimodal datasets.</li>
</ul>

<h3>Title: Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yue Li, Meng Tian, Zhenyu Lin, Jiangtong Zhu, Dechang Zhu, Haiqiang Liu, Zining Wang, Yueyi Zhang, Zhiwei Xiong, Xinhai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21505">https://arxiv.org/abs/2503.21505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21505">https://arxiv.org/pdf/2503.21505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21505]] Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving(https://arxiv.org/abs/2503.21505)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Existing benchmarks for Vision-Language Model (VLM) on autonomous driving (AD) primarily assess interpretability through open-form visual question answering (QA) within coarse-grained tasks, which remain insufficient to assess capabilities in complex driving scenarios. To this end, we introduce $\textbf{VLADBench}$, a challenging and fine-grained dataset featuring close-form QAs that progress from static foundational knowledge and elements to advanced reasoning for dynamic on-road situations. The elaborate $\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding, General Element Recognition, Traffic Graph Generation, Target Attribute Comprehension, and Ego Decision-Making and Planning. These domains are further broken down into 11 secondary aspects and 29 tertiary tasks for a granular evaluation. A thorough assessment of general and domain-specific (DS) VLMs on this benchmark reveals both their strengths and critical limitations in AD contexts. To further exploit the cognitive and reasoning interactions among the 5 domains for AD understanding, we start from a small-scale VLM and train the DS models on individual domain datasets (collected from 1.4M DS QAs across public sources). The experimental results demonstrate that the proposed benchmark provides a crucial step toward a more comprehensive assessment of VLMs in AD, paving the way for the development of more cognitively sophisticated and reasoning-capable AD systems.</li>
</ul>

<h3>Title: Uncertainty-aware Bayesian machine learning modelling of land cover classification</h3>
<ul>
<li><strong>Authors: </strong>Samuel Bilson, Anna Pustogvar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21510">https://arxiv.org/abs/2503.21510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21510">https://arxiv.org/pdf/2503.21510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21510]] Uncertainty-aware Bayesian machine learning modelling of land cover classification(https://arxiv.org/abs/2503.21510)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Land cover classification involves the production of land cover maps, which determine the type of land through remote sensing imagery. Over recent years, such classification is being performed by machine learning classification models, which can give highly accurate predictions on land cover per pixel using large quantities of input training data. However, such models do not currently take account of input measurement uncertainty, which is vital for traceability in metrology. In this work we propose a Bayesian classification framework using generative modelling to take account of input measurement uncertainty. We take the specific case of Bayesian quadratic discriminant analysis, and apply it to land cover datasets from Copernicus Sentinel-2 in 2020 and 2021. We benchmark the performance of the model against more popular classification models used in land cover maps such as random forests and neural networks. We find that such Bayesian models are more trustworthy, in the sense that they are more interpretable, explicitly model the input measurement uncertainty, and maintain predictive performance of class probability outputs across datasets of different years and sizes, whilst also being computationally efficient.</li>
</ul>

<h3>Title: ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo</h3>
<ul>
<li><strong>Authors: </strong>Yuxi Hu, Jun Zhang, Zhe Zhang, Rafael Weilharter, Yuchen Rao, Kuangyi Chen, Runze Yuan, Friedrich Fraundorfer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21525">https://arxiv.org/abs/2503.21525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21525">https://arxiv.org/pdf/2503.21525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21525]] ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo(https://arxiv.org/abs/2503.21525)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D point clouds from a series of overlapping images. Recent learning-based MVS frameworks overlook the geometric information embedded in features and correlations, leading to weak cost matching. In this paper, we propose ICG-MVSNet, which explicitly integrates intra-view and cross-view relationships for depth estimation. Specifically, we develop an intra-view feature fusion module that leverages the feature coordinate correlations within a single image to enhance robust cost matching. Additionally, we introduce a lightweight cross-view aggregation module that efficiently utilizes the contextual information from volume correlations to guide regularization. Our method is evaluated on the DTU dataset and Tanks and Temples benchmark, consistently achieving competitive performance against state-of-the-art works, while requiring lower computational resources.</li>
</ul>

<h3>Title: Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Umer Butt, Stalin Veranasi, Günter Neumann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21530">https://arxiv.org/abs/2503.21530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21530">https://arxiv.org/pdf/2503.21530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21530]] Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models(https://arxiv.org/abs/2503.21530)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, addressing the needs of low-resource languages remains a significant challenge. Transliteration between Urdu and its Romanized form, Roman Urdu, remains underexplored despite the widespread use of both scripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset showed promising results but suffered from poor domain adaptability and limited evaluation. We propose a transformer-based approach using the m2m100 multilingual translation model, enhanced with masked language modeling (MLM) pretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse Dakshina dataset. To address previous evaluation flaws, we introduce rigorous dataset splits and assess performance using BLEU, character-level BLEU, and CHRF. Our model achieves strong transliteration performance, with Char-BLEU scores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These results outperform both RNN baselines and GPT-4o Mini and demonstrate the effectiveness of multilingual transfer learning for low-resource transliteration tasks.</li>
</ul>

<h3>Title: Exploring the Energy Landscape of RBMs: Reciprocal Space Insights into Bosons, Hierarchical Learning and Symmetry Breaking</h3>
<ul>
<li><strong>Authors: </strong>J. Quetzalcóatl Toledo-Marin, Anindita Maiti, Geoffrey C. Fox, Roger G. Melko</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21536">https://arxiv.org/abs/2503.21536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21536">https://arxiv.org/pdf/2503.21536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21536]] Exploring the Energy Landscape of RBMs: Reciprocal Space Insights into Bosons, Hierarchical Learning and Symmetry Breaking(https://arxiv.org/abs/2503.21536)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models have become ubiquitous due to their ability to learn and sample from complex distributions. Despite the proliferation of various frameworks, the relationships among these models remain largely unexplored, a gap that hinders the development of a unified theory of AI learning. We address two central challenges: clarifying the connections between different deep generative models and deepening our understanding of their learning mechanisms. We focus on Restricted Boltzmann Machines (RBMs), known for their universal approximation capabilities for discrete distributions. By introducing a reciprocal space formulation, we reveal a connection between RBMs, diffusion processes, and coupled Bosons. We show that at initialization, the RBM operates at a saddle point, where the local curvature is determined by the singular values, whose distribution follows the Marcenko-Pastur law and exhibits rotational symmetry. During training, this rotational symmetry is broken due to hierarchical learning, where different degrees of freedom progressively capture features at multiple levels of abstraction. This leads to a symmetry breaking in the energy landscape, reminiscent of Landau theory. This symmetry breaking in the energy landscape is characterized by the singular values and the weight matrix eigenvector matrix. We derive the corresponding free energy in a mean-field approximation. We show that in the limit of infinite size RBM, the reciprocal variables are Gaussian distributed. Our findings indicate that in this regime, there will be some modes for which the diffusion process will not converge to the Boltzmann distribution. To illustrate our results, we trained replicas of RBMs with different hidden layer sizes using the MNIST dataset. Our findings bridge the gap between disparate generative frameworks and also shed light on the processes underpinning learning in generative models.</li>
</ul>

<h3>Title: LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Achint Soni, Meet Soni, Sirisha Rambhatla</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21541">https://arxiv.org/abs/2503.21541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21541">https://arxiv.org/pdf/2503.21541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21541]] LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing(https://arxiv.org/abs/2503.21541)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from cross-attention maps generated from diffusion models to identify the target regions for modification. However, since cross-attention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As a result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances cross-attention maps through a graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. \method consistently and substantially outperforms existing baselines on PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on this https URL</li>
</ul>

<h3>Title: SWI: Speaking with Intent in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuwei Yin, EunJeong Hwang, Giuseppe Carenini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21544">https://arxiv.org/abs/2503.21544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21544">https://arxiv.org/pdf/2503.21544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21544]] SWI: Speaking with Intent in Large Language Models(https://arxiv.org/abs/2503.21544)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMs' reasoning abilities with cognitive notions.</li>
</ul>

<h3>Title: SyncSDE: A Probabilistic Framework for Diffusion Synchronization</h3>
<ul>
<li><strong>Authors: </strong>Hyunjun Lee, Hyunsoo Lee, Sookwan Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21555">https://arxiv.org/abs/2503.21555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21555">https://arxiv.org/pdf/2503.21555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21555]] SyncSDE: A Probabilistic Framework for Diffusion Synchronization(https://arxiv.org/abs/2503.21555)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>There have been many attempts to leverage multiple diffusion models for collaborative generation, extending beyond the original domain. A prominent approach involves synchronizing multiple diffusion trajectories by mixing the estimated scores to artificially correlate the generation processes. However, existing methods rely on naive heuristics, such as averaging, without considering task specificity. These approaches do not clarify why such methods work and often fail when a heuristic suitable for one task is blindly applied to others. In this paper, we present a probabilistic framework for analyzing why diffusion synchronization works and reveal where heuristics should be focused - modeling correlations between multiple trajectories and adapting them to each specific task. We further identify optimal correlation models per task, achieving better results than previous approaches that apply a single heuristic across all tasks without justification.</li>
</ul>

<h3>Title: Consistent Multigroup Low-Rank Approximation</h3>
<ul>
<li><strong>Authors: </strong>Antonis Matakos, Martino Ciaperoni, Heikki Mannila</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21563">https://arxiv.org/abs/2503.21563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21563">https://arxiv.org/pdf/2503.21563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21563]] Consistent Multigroup Low-Rank Approximation(https://arxiv.org/abs/2503.21563)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We consider the problem of consistent low-rank approximation for multigroup data: we ask for a sequence of $k$ basis vectors such that projecting the data onto their spanned subspace treats all groups as equally as possible, by minimizing the maximum error among the groups. Additionally, we require that the sequence of basis vectors satisfies the natural consistency property: when looking for the best $k$ vectors, the first $d<k$ vectors are the best possible solution to the problem of finding $d$ basis vectors. Thus, this multigroup low-rank approximation method naturally generalizes \svd and reduces to \svd for data with a single group. We give an iterative algorithm for this task that sequentially adds to the basis the vector that gives the best rank$-1$ projection according to the min-max criterion, and then projects the data onto the orthogonal complement of that vector. For finding the best rank$-1$ projection, we use primal-dual approaches or semidefinite programming. We analyze the theoretical properties of the algorithms and demonstrate empirically that the proposed methods compare favorably to existing methods for multigroup (or fair) PCA.</li>
</ul>

<h3>Title: AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Liuyue Xie, Jiancong Guo, Ozan Cakmakci, Andre Araujo, Laszlo A. Jeni, Zhiheng Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21581">https://arxiv.org/abs/2503.21581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21581">https://arxiv.org/pdf/2503.21581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21581]] AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion(https://arxiv.org/abs/2503.21581)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate camera calibration is a fundamental task for 3D perception, especially when dealing with real-world, in-the-wild environments where complex optical distortions are common. Existing methods often rely on pre-rectified images or calibration patterns, which limits their applicability and flexibility. In this work, we introduce a novel framework that addresses these challenges by jointly modeling camera intrinsic and extrinsic parameters using a generic ray camera model. Unlike previous approaches, AlignDiff shifts focus from semantic to geometric features, enabling more accurate modeling of local distortions. We propose AlignDiff, a diffusion model conditioned on geometric priors, enabling the simultaneous estimation of camera distortions and scene geometry. To enhance distortion prediction, we incorporate edge-aware attention, focusing the model on geometric features around image edges, rather than semantic content. Furthermore, to enhance generalizability to real-world captures, we incorporate a large database of ray-traced lenses containing over three thousand samples. This database characterizes the distortion inherent in a diverse variety of lens forms. Our experiments demonstrate that the proposed method significantly reduces the angular error of estimated ray bundles by ~8.2 degrees and overall calibration accuracy, outperforming existing approaches on challenging, real-world datasets.</li>
</ul>

<h3>Title: Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yoann Boget, Alexandros Kalousis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21592">https://arxiv.org/abs/2503.21592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21592">https://arxiv.org/pdf/2503.21592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21592]] Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs(https://arxiv.org/abs/2503.21592)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Discrete Diffusion and Flow Matching models have significantly advanced generative modeling for discrete structures, including graphs. However, the time dependencies in the noising process of these models lead to error accumulation and propagation during the backward process. This issue, particularly pronounced in mask diffusion, is a known limitation in sequence modeling and, as we demonstrate, also impacts discrete diffusion models for graphs. To address this problem, we propose a novel framework called Iterative Denoising, which simplifies discrete diffusion and circumvents the issue by assuming conditional independence across time. Additionally, we enhance our model by incorporating a Critic, which during generation selectively retains or corrupts elements in an instance based on their likelihood under the data distribution. Our empirical evaluations demonstrate that the proposed method significantly outperforms existing discrete diffusion baselines in graph generation tasks.</li>
</ul>

<h3>Title: FusionSegReID: Advancing Person Re-Identification with Multimodal Retrieval and Precise Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jincheng Yan, Yun Wang, Xiaoyan Luo, Yu-Wing Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21595">https://arxiv.org/abs/2503.21595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21595">https://arxiv.org/pdf/2503.21595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21595]] FusionSegReID: Advancing Person Re-Identification with Multimodal Retrieval and Precise Segmentation(https://arxiv.org/abs/2503.21595)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Person re-identification (ReID) plays a critical role in applications like security surveillance and criminal investigations by matching individuals across large image galleries captured by non-overlapping cameras. Traditional ReID methods rely on unimodal inputs, typically images, but face limitations due to challenges like occlusions, lighting changes, and pose variations. While advancements in image-based and text-based ReID systems have been made, the integration of both modalities has remained under-explored. This paper presents FusionSegReID, a multimodal model that combines both image and text inputs for enhanced ReID performance. By leveraging the complementary strengths of these modalities, our model improves matching accuracy and robustness, particularly in complex, real-world scenarios where one modality may struggle. Our experiments show significant improvements in Top-1 accuracy and mean Average Precision (mAP) for ReID, as well as better segmentation results in challenging scenarios like occlusion and low-quality images. Ablation studies further confirm that multimodal fusion and segmentation modules contribute to enhanced re-identification and mask accuracy. The results show that FusionSegReID outperforms traditional unimodal models, offering a more robust and flexible solution for real-world person ReID tasks.</li>
</ul>

<h3>Title: Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing</h3>
<ul>
<li><strong>Authors: </strong>Johan Wahréus, Ahmed Hussain, Panos Papadimitratos</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21598">https://arxiv.org/abs/2503.21598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21598">https://arxiv.org/pdf/2503.21598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21598]] Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing(https://arxiv.org/abs/2503.21598)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have transformed task automation and content generation across various domains while incorporating safety filters to prevent misuse. We introduce a novel jailbreaking framework that employs distributed prompt processing combined with iterative refinements to bypass these safety measures, particularly in generating malicious code. Our architecture consists of four key modules: prompt segmentation, parallel processing, response aggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts across 10 cybersecurity categories, the framework achieves a 73.2% Success Rate (SR) in generating malicious code. Notably, our comparative analysis reveals that traditional single-LLM judge evaluation overestimates SRs (93.8%) compared to our LLM jury system (73.2%), with manual verification confirming that single-judge assessments often accept incomplete implementations. Moreover, we demonstrate that our distributed architecture improves SRs by 12% over the non-distributed approach in an ablation study, highlighting both the effectiveness of distributed prompt processing and the importance of robust evaluation methodologies in assessing jailbreak attempts.</li>
</ul>

<h3>Title: Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach</h3>
<ul>
<li><strong>Authors: </strong>Javier Coronado-Blázquez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21613">https://arxiv.org/abs/2503.21613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21613">https://arxiv.org/pdf/2503.21613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21613]] Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach(https://arxiv.org/abs/2503.21613)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>We study the ability of large language models (LLMs) to generate comprehensive and accurate book summaries solely from their internal knowledge, without recourse to the original text. Employing a diverse set of books and multiple LLM architectures, we examine whether these models can synthesize meaningful narratives that align with established human interpretations. Evaluation is performed with a LLM-as-a-judge paradigm: each AI-generated summary is compared against a high-quality, human-written summary via a cross-model assessment, where all participating LLMs evaluate not only their own outputs but also those produced by others. This methodology enables the identification of potential biases, such as the proclivity for models to favor their own summarization style over others. In addition, alignment between the human-crafted and LLM-generated summaries is quantified using ROUGE and BERTScore metrics, assessing the depth of grammatical and semantic correspondence. The results reveal nuanced variations in content representation and stylistic preferences among the models, highlighting both strengths and limitations inherent in relying on internal knowledge for summarization tasks. These findings contribute to a deeper understanding of LLM internal encodings of factual information and the dynamics of cross-model evaluation, with implications for the development of more robust natural language generative systems.</li>
</ul>

<h3>Title: Audio-driven Gesture Generation via Deviation Feature in the Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, Yinong He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21616">https://arxiv.org/abs/2503.21616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21616">https://arxiv.org/pdf/2503.21616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21616]] Audio-driven Gesture Generation via Deviation Feature in the Latent Space(https://arxiv.org/abs/2503.21616)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions. While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations. We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation. Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation. By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production. Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques.</li>
</ul>

<h3>Title: The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Lars Heckler-Kram, Jan-Hendrik Neudeck, Ulla Scheler, Rebecca König, Carsten Steger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21622">https://arxiv.org/abs/2503.21622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21622">https://arxiv.org/pdf/2503.21622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21622]] The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly Detection(https://arxiv.org/abs/2503.21622)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, performance on existing anomaly detection benchmarks like MVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with state-of-the-art models often competing in the range of less than one percentage point. This lack of discriminatory power prevents a meaningful comparison of models and thus hinders progress of the field, especially when considering the inherent stochastic nature of machine learning results. We present MVTec AD 2, a collection of eight anomaly detection scenarios with more than 8000 high-resolution images. It comprises challenging and highly relevant industrial inspection use cases that have not been considered in previous datasets, including transparent and overlapping objects, dark-field and back light illumination, objects with high variance in the normal data, and extremely small defects. We provide comprehensive evaluations of state-of-the-art methods and show that their performance remains below 60% average AU-PRO. Additionally, our dataset provides test scenarios with lighting condition changes to assess the robustness of methods under real-world distribution shifts. We host a publicly accessible evaluation server that holds the pixel-precise ground truth of the test set (this https URL). All image data is available at this https URL.</li>
</ul>

<h3>Title: Provable Reduction in Communication Rounds for Non-Smooth Convex Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Karlo Palenzuela, Ali Dadras, Alp Yurtsever, Tommy Löfstedt</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21627">https://arxiv.org/abs/2503.21627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21627">https://arxiv.org/pdf/2503.21627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21627]] Provable Reduction in Communication Rounds for Non-Smooth Convex Federated Learning(https://arxiv.org/abs/2503.21627)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Multiple local steps are key to communication-efficient federated learning. However, theoretical guarantees for such algorithms, without data heterogeneity-bounding assumptions, have been lacking in general non-smooth convex problems. Leveraging projection-efficient optimization methods, we propose FedMLS, a federated learning algorithm with provable improvements from multiple local steps. FedMLS attains an $\epsilon$-suboptimal solution in $\mathcal{O}(1/\epsilon)$ communication rounds, requiring a total of $\mathcal{O}(1/\epsilon^2)$ stochastic subgradient oracle calls.</li>
</ul>

<h3>Title: When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco</h3>
<ul>
<li><strong>Authors: </strong>Yassir Lairgi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21634">https://arxiv.org/abs/2503.21634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21634">https://arxiv.org/pdf/2503.21634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21634]] When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco(https://arxiv.org/abs/2503.21634)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The accurate determination of the beginning of each Hijri month is essential for religious, cultural, and administrative purposes. Manazel (The code and datasets are available at this https URL) addresses this challenge in Morocco by leveraging 13 years of crescent visibility data to refine the ODEH criterion, a widely used standard for lunar crescent visibility prediction. The study integrates two key features, the Arc of Vision (ARCV) and the total width of the crescent (W), to enhance the accuracy of lunar visibility assessments. A machine learning approach utilizing the Logistic Regression algorithm is employed to classify crescent visibility conditions, achieving a predictive accuracy of 98.83%. This data-driven methodology offers a robust and reliable framework for determining the start of the Hijri month, comparing different data classification tools, and improving the consistency of lunar calendar calculations in Morocco. The findings demonstrate the effectiveness of machine learning in astronomical applications and highlight the potential for further enhancements in the modeling of crescent visibility.</li>
</ul>

<h3>Title: Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base</h3>
<ul>
<li><strong>Authors: </strong>Satvik Verma, Qun Wang, E. Wes Bethel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21674">https://arxiv.org/abs/2503.21674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21674">https://arxiv.org/pdf/2503.21674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21674]] Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base(https://arxiv.org/abs/2503.21674)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Internet of Things (IoT) devices has introduced significant cybersecurity challenges, particularly with the increasing frequency and sophistication of Distributed Denial of Service (DDoS) attacks. Traditional machine learning (ML) techniques often fall short in detecting such attacks due to the complexity of blended and evolving patterns. To address this, we propose a novel framework leveraging On-Device Large Language Models (ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for intelligent IoT network attack detection. By implementing feature ranking techniques and constructing both long and short KBs tailored to model capacities, the proposed framework ensures efficient and accurate detection of DDoS attacks while overcoming computational and privacy limitations. Simulation results demonstrate that the optimized framework achieves superior accuracy across diverse attack types, especially when using compact models in edge computing environments. This work provides a scalable and secure solution for real-time IoT security, advancing the applicability of edge intelligence in cybersecurity.</li>
</ul>

<h3>Title: How do language models learn facts? Dynamics, curricula and hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Zucchet, Jörg Bornschein, Stephanie Chan, Andrew Lampinen, Razvan Pascanu, Soham De</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21676">https://arxiv.org/abs/2503.21676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21676">https://arxiv.org/pdf/2503.21676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21676]] How do language models learn facts? Dynamics, curricula and hallucinations(https://arxiv.org/abs/2503.21676)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.</li>
</ul>

<h3>Title: JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community</h3>
<ul>
<li><strong>Authors: </strong>Yunze Xiao, Tingyu He, Lionel Z. Wang, Yiming Ma, Xingyu Song, Xiaohang Xu, Irene Li, Ka Chung Ng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21679">https://arxiv.org/abs/2503.21679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21679">https://arxiv.org/pdf/2503.21679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21679]] JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community(https://arxiv.org/abs/2503.21679)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces JiraiBench, the first bilingual benchmark for evaluating large language models' effectiveness in detecting self-destructive content across Chinese and Japanese social media communities. Focusing on the transnational "Jirai" (landmine) online subculture that encompasses multiple forms of self-destructive behaviors including drug overdose, eating disorders, and self-harm, we present a comprehensive evaluation framework incorporating both linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese posts and 5,000 Japanese posts with multidimensional annotation along three behavioral categories, achieving substantial inter-annotator agreement. Experimental evaluations across four state-of-the-art models reveal significant performance variations based on instructional language, with Japanese prompts unexpectedly outperforming Chinese prompts when processing Chinese content. This emergent cross-cultural transfer suggests that cultural proximity can sometimes outweigh linguistic similarity in detection tasks. Cross-lingual transfer experiments with fine-tuned models further demonstrate the potential for knowledge transfer between these language systems without explicit target language training. These findings highlight the need for culturally-informed approaches to multilingual content moderation and provide empirical evidence for the importance of cultural context in developing more effective detection systems for vulnerable online communities.</li>
</ul>

<h3>Title: AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiahe Qian, Yaoyu Fang, Jinkui Hao, Bo Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21695">https://arxiv.org/abs/2503.21695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21695">https://arxiv.org/pdf/2503.21695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21695]] AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation(https://arxiv.org/abs/2503.21695)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of cell nuclei in histopathology images is essential for numerous biomedical research and clinical applications. However, existing cell nucleus segmentation methods only consider a single dataset (i.e., primary domain), while neglecting to leverage supplementary data from diverse sources (i.e., auxiliary domains) to reduce overfitting and enhance the performance. Although incorporating multiple datasets could alleviate overfitting, it often exacerbates performance drops caused by domain shifts. In this work, we introduce Adversarial Multi-domain Alignment of Segment Anything Model (AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these obstacles through two key innovations. First, we propose a Conditional Gradient Reversal Layer (CGRL), a multi-domain alignment module that harmonizes features from diverse domains to promote domain-invariant representation learning while preserving crucial discriminative features for the primary dataset. Second, we address SAM's inherent low-resolution output by designing a High-Resolution Decoder (HR-Decoder), which directly produces fine-grained segmentation maps in order to capture intricate nuclei boundaries in high-resolution histology images. To the best of our knowledge, this is the first attempt to adapt SAM for multi-dataset learning with application to histology nuclei segmentation. We validate our method on several publicly available datasets, demonstrating consistent and significant improvements over state-of-the-art approaches.</li>
</ul>

<h3>Title: Learning to Represent Individual Differences for Choice Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Yan-Ying Chen, Yue Weng, Alexandre Filipowicz, Rumen Iliev, Francine Chen, Shabnam Hakimi, Yanxia Zhang, Matthew Lee, Kent Lyons, Charlene Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21704">https://arxiv.org/abs/2503.21704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21704">https://arxiv.org/pdf/2503.21704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21704]] Learning to Represent Individual Differences for Choice Decision Making(https://arxiv.org/abs/2503.21704)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Human decision making can be challenging to predict because decisions are affected by a number of complex factors. Adding to this complexity, decision-making processes can differ considerably between individuals, and methods aimed at predicting human decisions need to take individual differences into account. Behavioral science offers methods by which to measure individual differences (e.g., questionnaires, behavioral models), but these are often narrowed down to low dimensions and not tailored to specific prediction tasks. This paper investigates the use of representation learning to measure individual differences from behavioral experiment data. Representation learning offers a flexible approach to create individual embeddings from data that are both structured (e.g., demographic information) and unstructured (e.g., free text), where the flexibility provides more options for individual difference measures for personalization, e.g., free text responses may allow for open-ended questions that are less privacy-sensitive. In the current paper we use representation learning to characterize individual differences in human performance on an economic decision-making task. We demonstrate that models using representation learning to capture individual differences consistently improve decision predictions over models without representation learning, and even outperform well-known theory-based behavioral models used in these environments. Our results propose that representation learning offers a useful and flexible tool to capture individual differences.</li>
</ul>

<h3>Title: Collab: Controlled Decoding using Mixture of Agents for LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, Sumitra Ganesh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21720">https://arxiv.org/abs/2503.21720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21720">https://arxiv.org/pdf/2503.21720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21720]] Collab: Controlled Decoding using Mixture of Agents for LLM Alignment(https://arxiv.org/abs/2503.21720)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.</li>
</ul>

<h3>Title: Evaluating Text-to-Image Synthesis with a Conditional Fréchet Distance</h3>
<ul>
<li><strong>Authors: </strong>Jaywon Koo, Jefferson Hernandez, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21721">https://arxiv.org/abs/2503.21721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21721">https://arxiv.org/pdf/2503.21721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21721]] Evaluating Text-to-Image Synthesis with a Conditional Fréchet Distance(https://arxiv.org/abs/2503.21721)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Evaluating text-to-image synthesis is challenging due to misalignment between established metrics and human preferences. We propose cFreD, a metric based on the notion of Conditional Fréchet Distance that explicitly accounts for both visual fidelity and text-prompt alignment. Existing metrics such as Inception Score (IS), Fréchet Inception Distance (FID) and CLIPScore assess either image quality or image-text alignment but not both which limits their correlation with human preferences. Scoring models explicitly trained to replicate human preferences require constant updates and may not generalize to novel generation techniques or out-of-domain inputs. Through extensive experiments across multiple recently proposed text-to-image models and diverse prompt datasets, we demonstrate that cFreD exhibits a higher correlation with human judgments compared to statistical metrics, including metrics trained with human preferences. Our findings validate cFreD as a robust, future-proof metric for the systematic evaluation of text-to-image models, standardizing benchmarking in this rapidly evolving field. We release our evaluation toolkit and benchmark in the appendix.</li>
</ul>

<h3>Title: Energy Minimization for Participatory Federated Learning in IoT Analyzed via Game Theory</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Buratto, Elia Guerra, Marco Miozzo, Paolo Dini, Leonardo Badia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21722">https://arxiv.org/abs/2503.21722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21722">https://arxiv.org/pdf/2503.21722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21722]] Energy Minimization for Participatory Federated Learning in IoT Analyzed via Game Theory(https://arxiv.org/abs/2503.21722)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The Internet of Things requires intelligent decision making in many scenarios. To this end, resources available at the individual nodes for sensing or computing, or both, can be leveraged. This results in approaches known as participatory sensing and federated learning, respectively. We investigate the simultaneous implementation of both, through a distributed approach based on empowering local nodes with game theoretic decision making. A global objective of energy minimization is combined with the individual node's optimization of local expenditure for sensing and transmitting data over multiple learning rounds. We present extensive evaluations of this technique, based on both a theoretical framework and experiments in a simulated network scenario with real data. Such a distributed approach can reach a desired level of accuracy for federated learning without a centralized supervision of the data collector. However, depending on the weight attributed to the local costs of the single node, it may also result in a significantly high Price of Anarchy (from 1.28 onwards). Thus, we argue for the need of incentive mechanisms, possibly based on Age of Information of the single nodes.</li>
</ul>

<h3>Title: OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21723">https://arxiv.org/abs/2503.21723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21723">https://arxiv.org/pdf/2503.21723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21723]] OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation(https://arxiv.org/abs/2503.21723)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Occlusion is one of the challenging issues when estimating 3D hand pose. This problem becomes more prominent when hand interacts with an object or two hands are involved. In the past works, much attention has not been given to these occluded regions. But these regions contain important and beneficial information that is vital for 3D hand pose estimation. Thus, in this paper, we propose an occlusion robust and accurate method for the estimation of 3D hand-object pose from the input RGB image. Our method includes first localising the hand joints using a CNN based model and then refining them by extracting contextual information. The self attention transformer then identifies the specific joints along with the hand identity. This helps the model to identify the hand belongingness of a particular joint which helps to detect the joint even in the occluded region. Further, these joints with hand identity are then used to estimate the pose using cross attention mechanism. Thus, by identifying the joints in the occluded region, the obtained network becomes robust to occlusion. Hence, this network achieves state-of-the-art results when evaluated on the InterHand2.6M, HO3D and H$_2$O3D datasets.</li>
</ul>

<h3>Title: ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21729">https://arxiv.org/abs/2503.21729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21729">https://arxiv.org/pdf/2503.21729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21729]] ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation(https://arxiv.org/abs/2503.21729)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).</li>
</ul>

<h3>Title: Effective Skill Unlearning through Intervention and Abstention</h3>
<ul>
<li><strong>Authors: </strong>Yongce Li, Chung-En Sun, Tsui-Wei Weng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21730">https://arxiv.org/abs/2503.21730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21730">https://arxiv.org/pdf/2503.21730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21730]] Effective Skill Unlearning through Intervention and Abstention(https://arxiv.org/abs/2503.21730)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language Models (LLMs) have demonstrated remarkable skills across various domains. Understanding the mechanisms behind their abilities and implementing controls over them is becoming increasingly important for developing better models. In this paper, we focus on skill unlearning in LLMs, specifically unlearning a particular skill while retaining their overall capabilities. We introduce two lightweight, training-free machine skill unlearning techniques for LLMs. First, we observe that the pre-activation distribution of neurons in each Feed-Forward Layer (FFL) differs when the model demonstrates different skills. Additionally, we find that queries triggering the same skill cluster within the FFL key space and can be separated from other queries using a hypercube. Based on these observations, we propose two lightweight, training-free skill unlearning methods via \textit{intervention} and \textit{abstention} respectively: \texttt{Neuron Adjust} and \texttt{Key Space Detection}. We evaluate our methods on unlearning math-solving, Python-coding, and comprehension skills across seven different languages. The results demonstrate their strong unlearning capabilities for the designated skills. Specifically, \texttt{Key Space Detection} achieves over 80\% relative performance drop on the forgetting skill and less than 10\% relative performance drop on other skills and the model's general knowledge (MMLU) for most unlearning tasks. Our code is available at this https URL</li>
</ul>

<h3>Title: SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, Yangguang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21732">https://arxiv.org/abs/2503.21732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21732">https://arxiv.org/pdf/2503.21732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21732]] SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling(https://arxiv.org/abs/2503.21732)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to $1024^3$ directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.</li>
</ul>

<h3>Title: 3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Zhang, Mengchen Zhang, Tong Wu, Tengfei Wang, Gordon Wetzstein, Dahua Lin, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21745">https://arxiv.org/abs/2503.21745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21745">https://arxiv.org/pdf/2503.21745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21745]] 3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models(https://arxiv.org/abs/2503.21745)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D generation is experiencing rapid advancements, while the development of 3D evaluation has not kept pace. How to keep automatic evaluation equitably aligned with human perception has become a well-recognized challenge. Recent advances in the field of language and image generation have explored human preferences and showcased respectable fitting ability. However, the 3D domain still lacks such a comprehensive preference dataset over generative models. To mitigate this absence, we develop 3DGen-Arena, an integrated platform in a battle manner. Then, we carefully design diverse text and image prompts and leverage the arena platform to gather human preferences from both public users and expert annotators, resulting in a large-scale multi-dimension human preference dataset 3DGen-Bench. Using this dataset, we further train a CLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator, 3DGen-Eval. These two models innovatively unify the quality evaluation of text-to-3D and image-to-3D generation, and jointly form our automated evaluation system with their respective strengths. Extensive experiments demonstrate the efficacy of our scoring model in predicting human preferences, exhibiting a superior correlation with human ranks compared to existing metrics. We believe that our 3DGen-Bench dataset and automated evaluation system will foster a more equitable evaluation in the field of 3D generation, further promoting the development of 3D generative models and their downstream applications.</li>
</ul>

<h3>Title: LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Shitian Zhao, Qilong Wu, Xinyue Li, Bo Zhang, Ming Li, Qi Qin, Dongyang Liu, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Peng Gao, Bin Fu, Zhen Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21749">https://arxiv.org/abs/2503.21749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21749">https://arxiv.org/pdf/2503.21749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21749]] LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis(https://arxiv.org/abs/2503.21749)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity. Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024$\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance. To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation. Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%). Our codes, models, datasets, and demo are publicly available.</li>
</ul>

<h3>Title: Reconstructing Humans with a Biomechanically Accurate Skeleton</h3>
<ul>
<li><strong>Authors: </strong>Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21751">https://arxiv.org/abs/2503.21751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21751">https://arxiv.org/pdf/2503.21751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21751]] Reconstructing Humans with a Biomechanically Accurate Skeleton(https://arxiv.org/abs/2503.21751)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: this https URL</li>
</ul>

<h3>Title: VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness</h3>
<ul>
<li><strong>Authors: </strong>Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21755">https://arxiv.org/abs/2503.21755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21755">https://arxiv.org/pdf/2503.21755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21755]] VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness(https://arxiv.org/abs/2503.21755)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real "world models" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive annotations to ensure alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.</li>
</ul>

<h3>Title: A Unified Framework for Diffusion Bridge Problems: Flow Matching and Schrödinger Matching into One</h3>
<ul>
<li><strong>Authors: </strong>Minyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21756">https://arxiv.org/abs/2503.21756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21756">https://arxiv.org/pdf/2503.21756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21756]] A Unified Framework for Diffusion Bridge Problems: Flow Matching and Schrödinger Matching into One(https://arxiv.org/abs/2503.21756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The bridge problem is to find an SDE (or sometimes an ODE) that bridges two given distributions. The application areas of the bridge problem are enormous, among which the recent generative modeling (e.g., conditional or unconditional image generation) is the most popular. Also the famous Schrödinger bridge problem, a widely known problem for a century, is a special instance of the bridge problem. Two most popular algorithms to tackle the bridge problems in the deep learning era are: (conditional) flow matching and iterative fitting algorithms, where the former confined to ODE solutions, and the latter specifically for the Schrödinger bridge problem. The main contribution of this article is in two folds: i) We provide concise reviews of these algorithms with technical details to some extent; ii) We propose a novel unified perspective and framework that subsumes these seemingly unrelated algorithms (and their variants) into one. In particular, we show that our unified framework can instantiate the Flow Matching (FM) algorithm, the (mini-batch) optimal transport FM algorithm, the (mini-batch) Schrödinger bridge FM algorithm, and the deep Schrödinger bridge matching (DSBM) algorithm as its special cases. We believe that this unified framework will be useful for viewing the bridge problems in a more general and flexible perspective, and in turn can help researchers and practitioners to develop new bridge algorithms in their fields.</li>
</ul>

<h3>Title: Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21757">https://arxiv.org/abs/2503.21757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21757">https://arxiv.org/pdf/2503.21757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21757]] Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck(https://arxiv.org/abs/2503.21757)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we aim to compress the vision tokens of a Large Vision Language Model (LVLM) into a representation that is simultaneously suitable for (a) generative and (b) discriminative tasks, (c) is nearly lossless, and (d) is storage-efficient. We propose a novel compression approach, called Fwd2Bot, that uses the LVLM itself to compress the visual information in a task-agnostic manner. At the core of Fwd2bot there exists a "double-forward pass" training strategy, whereby, during the first forward pass, the LLM (of the LVLM) creates a bottleneck by condensing the visual information into a small number of summary tokens. Then, using the same LLM, the second forward pass processes the language instruction(s) alongside the summary tokens, used as a direct replacement for the image ones. The training signal is provided by two losses: an autoregressive one applied after the second pass that provides a direct optimization objective for compression, and a contrastive loss, applied after the first pass, that further boosts the representation strength, especially for discriminative tasks. The training is further enhanced by stage-specific adapters. We accompany the proposed method by an in-depth ablation study. Overall, Fwd2Bot results in highly-informative compressed representations suitable for both generative and discriminative tasks. For generative tasks, we offer a 2x higher compression rate without compromising the generative capabilities, setting a new state-of-the-art result. For discriminative tasks, we set a new state-of-the-art on image retrieval and compositionality.</li>
</ul>

<h3>Title: Lumina-Image 2.0: A Unified and Efficient Image Generative Framework</h3>
<ul>
<li><strong>Authors: </strong>Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Manyuan Zhang, Will Beddow, Erwann Millon, Victor Perez, Wenhai Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu Qiao, Chang Xu, Peng Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21758">https://arxiv.org/abs/2503.21758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21758">https://arxiv.org/pdf/2503.21758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21758]] Lumina-Image 2.0: A Unified and Efficient Image Generative Framework(https://arxiv.org/abs/2503.21758)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at this https URL.</li>
</ul>

<h3>Title: MemInsight: Autonomous Memory Augmentation for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine Benajiba</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21760">https://arxiv.org/abs/2503.21760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21760">https://arxiv.org/pdf/2503.21760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21760]] MemInsight: Autonomous Memory Augmentation for LLM Agents(https://arxiv.org/abs/2503.21760)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.</li>
</ul>

<h3>Title: Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video</h3>
<ul>
<li><strong>Authors: </strong>David Yifan Yao, Albert J. Zhai, Shenlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21761">https://arxiv.org/abs/2503.21761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21761">https://arxiv.org/pdf/2503.21761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21761]] Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video(https://arxiv.org/abs/2503.21761)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents a unified approach to understanding dynamic scenes from casual videos. Large pretrained vision foundation models, such as vision-language, video depth prediction, motion tracking, and segmentation models, offer promising capabilities. However, training a single model for comprehensive 4D understanding remains challenging. We introduce Uni4D, a multi-stage optimization framework that harnesses multiple pretrained models to advance dynamic 3D modeling, including static/dynamic reconstruction, camera pose estimation, and dense 3D motion tracking. Our results show state-of-the-art performance in dynamic 4D modeling with superior visual quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the effectiveness of repurposing visual foundation models for 4D understanding.</li>
</ul>

<h3>Title: Exploring the Evolution of Physics Cognition in Video Generation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, Cunxiang Wang, Zhengrong Zuo, Nong Sang, Siteng Huang, Donglin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21765">https://arxiv.org/abs/2503.21765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21765">https://arxiv.org/pdf/2503.21765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21765]] Exploring the Evolution of Physics Cognition in Video Generation: A Survey(https://arxiv.org/abs/2503.21765)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of ''visual realism but physical absurdity". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of ''visual mimicry'' towards a new phase of ''human-like physical comprehension''.</li>
</ul>

<h3>Title: Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying</h3>
<ul>
<li><strong>Authors: </strong>Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21767">https://arxiv.org/abs/2503.21767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21767">https://arxiv.org/pdf/2503.21767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21767]] Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying(https://arxiv.org/abs/2503.21767)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary querying in 3D Gaussian Splatting aims to identify semantically relevant regions within a 3D Gaussian representation based on a given text query. Prior work, such as LangSplat, addressed this task by retrieving these regions in the form of segmentation masks on 2D renderings. More recently, OpenGaussian introduced point-level querying, which directly selects a subset of 3D Gaussians. In this work, we propose a point-level querying method that builds upon LangSplat's framework. Our approach improves the framework in two key ways: (a) we leverage masklets from the Segment Anything Model 2 (SAM2) to establish semantic consistent ground-truth for distilling the language Gaussians; (b) we introduces a novel two-step querying approach that first retrieves the distilled ground-truth and subsequently uses the ground-truth to query the individual Gaussians. Experimental evaluations on three benchmark datasets demonstrate that the proposed method achieves better performance compared to state-of-the-art approaches. For instance, our method achieves an mIoU improvement of +20.42 on the 3D-OVS dataset.</li>
</ul>

<h3>Title: A Unified Image-Dense Annotation Generation Model for Underwater Scenes</h3>
<ul>
<li><strong>Authors: </strong>Hongkai Lin, Dingkang Liang, Zhenghao Qi, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21771">https://arxiv.org/abs/2503.21771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21771">https://arxiv.org/pdf/2503.21771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21771]] A Unified Image-Dense Annotation Generation Model for Underwater Scenes(https://arxiv.org/abs/2503.21771)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Underwater dense prediction, especially depth estimation and semantic segmentation, is crucial for gaining a comprehensive understanding of underwater scenes. Nevertheless, high-quality and large-scale underwater datasets with dense annotations remain scarce because of the complex environment and the exorbitant data collection costs. This paper proposes a unified Text-to-Image and DEnse annotation generation method (TIDE) for underwater scenes. It relies solely on text as input to simultaneously generate realistic underwater images and multiple highly consistent dense annotations. Specifically, we unify the generation of text-to-image and text-to-dense annotations within a single model. The Implicit Layout Sharing mechanism (ILS) and cross-modal interaction method called Time Adaptive Normalization (TAN) are introduced to jointly optimize the consistency between image and dense annotations. We synthesize a large-scale underwater dataset using TIDE to validate the effectiveness of our method in underwater dense prediction tasks. The results demonstrate that our method effectively improves the performance of existing underwater dense prediction models and mitigates the scarcity of underwater data with dense annotations. We hope our method can offer new perspectives on alleviating data scarcity issues in other fields. The code is available at https: //github.com/HongkLin/TIDE.</li>
</ul>

<h3>Title: Optimal Stepsize for Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Jianning Pei, Han Hu, Shuyang Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21774">https://arxiv.org/abs/2503.21774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21774">https://arxiv.org/pdf/2503.21774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21774]] Optimal Stepsize for Diffusion Sampling(https://arxiv.org/abs/2503.21774)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models achieve remarkable generation quality but suffer from computational intensive sampling due to suboptimal step discretization. While existing works focus on optimizing denoising directions, we address the principled design of stepsize schedules. This paper proposes Optimal Stepsize Distillation, a dynamic programming framework that extracts theoretically optimal schedules by distilling knowledge from reference trajectories. By reformulating stepsize optimization as recursive error minimization, our method guarantees global discretization bounds through optimal substructure exploitation. Crucially, the distilled schedules demonstrate strong robustness across architectures, ODE solvers, and noise schedules. Experiments show 10x accelerated text-to-image generation while preserving 99.4% performance on GenEval. Our code is available at this https URL.</li>
</ul>

<h3>Title: StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Guo, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21775">https://arxiv.org/abs/2503.21775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21775">https://arxiv.org/pdf/2503.21775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21775]] StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion(https://arxiv.org/abs/2503.21775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present StyleMotif, a novel Stylized Motion Latent Diffusion model, generating motion conditioned on both content and style from multiple modalities. Unlike existing approaches that either focus on generating diverse motion content or transferring style from sequences, StyleMotif seamlessly synthesizes motion across a wide range of content while incorporating stylistic cues from multi-modal inputs, including motion, text, image, video, and audio. To achieve this, we introduce a style-content cross fusion mechanism and align a style encoder with a pre-trained multi-modal model, ensuring that the generated motion accurately captures the reference style while preserving realism. Extensive experiments demonstrate that our framework surpasses existing methods in stylized motion generation and exhibits emergent capabilities for multi-modal motion stylization, enabling more nuanced motion synthesis. Source code and pre-trained models will be released upon acceptance. Project Page: this https URL</li>
</ul>

<h3>Title: Video-R1: Reinforcing Video Reasoning in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21776">https://arxiv.org/abs/2503.21776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21776">https://arxiv.org/pdf/2503.21776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21776]] Video-R1: Reinforcing Video Reasoning in MLLMs(https://arxiv.org/abs/2503.21776)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released.</li>
</ul>

<h3>Title: Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Reza Qorbani, Gianluca Villani, Theodoros Panagiotakopoulos, Marc Botet Colomer, Linus Härenstam-Nielsen, Mattia Segu, Pier Luigi Dovesi, Jussi Karlgren, Daniel Cremers, Federico Tombari, Matteo Poggi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21780">https://arxiv.org/abs/2503.21780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21780">https://arxiv.org/pdf/2503.21780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21780]] Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation(https://arxiv.org/abs/2503.21780)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, explainability, segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary semantic segmentation models associate vision and text to label pixels from an undefined set of classes using textual queries, providing versatile performance on novel datasets. However, large shifts between training and test domains degrade their performance, requiring fine-tuning for effective real-world applications. We introduce Semantic Library Adaptation (SemLA), a novel framework for training-free, test-time domain adaptation. SemLA leverages a library of LoRA-based adapters indexed with CLIP embeddings, dynamically merging the most relevant adapters based on proximity to the target domain in the embedding space. This approach constructs an ad-hoc model tailored to each specific input without additional training. Our method scales efficiently, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it ideal for sensitive applications. Comprehensive experiments on a 20-domain benchmark built over 10 standard datasets demonstrate SemLA's superior adaptability and performance across diverse settings, establishing a new standard in domain adaptation for open-vocabulary semantic segmentation.</li>
</ul>

<h3>Title: VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21781">https://arxiv.org/abs/2503.21781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21781">https://arxiv.org/pdf/2503.21781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21781]] VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models(https://arxiv.org/abs/2503.21781)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Customized text-to-video generation aims to produce high-quality videos that incorporate user-specified subject identities or motion patterns. However, existing methods mainly focus on personalizing a single concept, either subject identity or motion pattern, limiting their effectiveness for multiple subjects with the desired motion patterns. To tackle this challenge, we propose a unified framework VideoMage for video customization over both multiple subjects and their interactive motions. VideoMage employs subject and motion LoRAs to capture personalized content from user-provided images and videos, along with an appearance-agnostic motion learning approach to disentangle motion patterns from visual appearance. Furthermore, we develop a spatial-temporal composition scheme to guide interactions among subjects within the desired motion patterns. Extensive experiments demonstrate that VideoMage outperforms existing methods, generating coherent, user-controlled videos with consistent subject identities and interactions.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
