<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-09</h1>
<h3>Title: TokenShapley: Token Level Context Attribution with Shapley Value</h3>
<ul>
<li><strong>Authors: </strong>Yingtai Xiao, Yuqing Zhu, Sirat Samyoun, Wanrong Zhang, Jiachen T. Wang, Jian Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05261">https://arxiv.org/abs/2507.05261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05261">https://arxiv.org/pdf/2507.05261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05261]] TokenShapley: Token Level Context Attribution with Shapley Value(https://arxiv.org/abs/2507.05261)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving an 11-23% improvement in accuracy.</li>
</ul>

<h3>Title: User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sougata Saha, Monojit Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05266">https://arxiv.org/abs/2507.05266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05266">https://arxiv.org/pdf/2507.05266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05266]] User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs(https://arxiv.org/abs/2507.05266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Measuring the generalization ability of Large Language Models (LLMs) is challenging due to data contamination. As models grow and computation becomes cheaper, ensuring tasks and test cases are unseen during training phases will become nearly impossible. We argue that knowledge-retrieval and reasoning tasks are not ideal for measuring generalization, as LLMs are not trained for specific tasks. Instead, we propose user behavior prediction, also a key aspect of personalization, as a theoretically sound, scalable, and robust alternative. We introduce a novel framework for this approach and test it on movie and music recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct. Results align with our framework's predictions, showing GPT-4o outperforms GPT-4o-mini and Llama, though all models have much room for improvement, especially Llama.</li>
</ul>

<h3>Title: An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Zia Ur Rehman, Aditya Shah, Nagendra Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05271">https://arxiv.org/abs/2507.05271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05271">https://arxiv.org/pdf/2507.05271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05271]] An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks(https://arxiv.org/abs/2507.05271)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The global reach of social media has amplified the spread of hateful content, including implicit sexism, which is often overlooked by conventional detection methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning framework for implicit sexism detectioN (ASCEND). A key innovation of our method is the incorporation of threshold-based contrastive learning: by computing cosine similarities between embeddings, we selectively treat only those sample pairs as positive if their similarity exceeds a learnable threshold. This mechanism refines the embedding space by robustly pulling together representations of semantically similar texts while pushing apart dissimilar ones, thus reducing false positives and negatives. The final classification is achieved by jointly optimizing a contrastive loss with a cross-entropy loss. Textual features are enhanced through a word-level attention module. Additionally, we employ sentiment, emotion, and toxicity features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that ASCEND significantly outperforms existing methods, with average Macro F1 improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting its efficacy in capturing the subtle cues of implicit sexist language.</li>
</ul>

<h3>Title: Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Kamal, Niyaz Bin Hashem, Robin Krambroeckers, Nabeel Mohammed, Shafin Rahman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05284">https://arxiv.org/abs/2507.05284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05284">https://arxiv.org/pdf/2507.05284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05284]] Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction(https://arxiv.org/abs/2507.05284)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Although most transformer-based time series forecasting models primarily depend on endogenous inputs, recent state-of-the-art approaches have significantly improved performance by incorporating external information through exogenous inputs. However, these methods face challenges, such as redundancy when endogenous and exogenous inputs originate from the same source and limited ability to capture long-term dependencies due to fixed look-back windows. In this paper, we propose a method that whitens the exogenous input to reduce redundancy that may persist within the data based on global statistics. Additionally, our approach helps the exogenous input to be more aware of patterns and trends over extended periods. By introducing this refined, globally context-aware exogenous input to the endogenous input without increasing the lookback window length, our approach guides the model towards improved forecasting. Our approach achieves state-of-the-art performance in four benchmark datasets, consistently outperforming 11 baseline models. These results establish our method as a robust and effective alternative for using exogenous inputs in time series forecasting.</li>
</ul>

<h3>Title: Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Merchant, Haitz Sáez de Ocáriz Borde, Andrei Cristian Popescu, Carlos Garcia Jurado Suarez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05300">https://arxiv.org/abs/2507.05300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05300">https://arxiv.org/pdf/2507.05300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05300]] Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)(https://arxiv.org/abs/2507.05300)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We argue that generative text-to-image models often struggle with prompt adherence due to the noisy and unstructured nature of large-scale datasets like LAION-5B. This forces users to rely heavily on prompt engineering to elicit desirable outputs. In this work, we propose that enforcing a consistent caption structure during training can significantly improve model controllability and alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part template: subject, setting, aesthetics, and camera details. We fine-tune PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly shuffled captions, and show that structured versions consistently yield higher text-image alignment scores using visual question answering (VQA) models. The dataset is publicly available at this https URL.</li>
</ul>

<h3>Title: CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Binjia Zhou, Hengrui Lou, Lizhe Chen, Haoyuan Li, Dawei Luo, Shuai Chen, Jie Lei, Zunlei Feng, Yijun Bei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05302">https://arxiv.org/abs/2507.05302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05302">https://arxiv.org/pdf/2507.05302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05302]] CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection(https://arxiv.org/abs/2507.05302)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake this http URL techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of this http URL address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias this http URL results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities.</li>
</ul>

<h3>Title: Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces</h3>
<ul>
<li><strong>Authors: </strong>Madina Kojanazarova, Florentin Bieder, Robin Sandkühler, Philippe C. Cattin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05315">https://arxiv.org/abs/2507.05315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05315">https://arxiv.org/pdf/2507.05315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05315]] Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces(https://arxiv.org/abs/2507.05315)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Soft tissue simulation in virtual environments is becoming increasingly important for medical applications. However, the high deformability of soft tissue poses significant challenges. Existing methods rely on segmentation, meshing and estimation of stiffness properties of tissues. In addition, the integration of haptic feedback requires precise force estimation to enable a more immersive experience. We introduce a novel data-driven model, a conditional graph neural network (cGNN) to tackle this complexity. Our model takes surface points and the location of applied forces, and is specifically designed to predict the deformation of the points and the forces exerted on them. We trained our model on experimentally collected surface tracking data of a soft tissue phantom and used transfer learning to overcome the data scarcity by initially training it with mass-spring simulations and fine-tuning it with the experimental data. This approach improves the generalisation capability of the model and enables accurate predictions of tissue deformations and corresponding interaction forces. The results demonstrate that the model can predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces up to 7.5 N. Our data-driven approach presents a promising solution to the intricate challenge of simulating soft tissues within virtual environments. Beyond its applicability in medical simulations, this approach holds the potential to benefit various fields where realistic soft tissue simulations are required.</li>
</ul>

<h3>Title: LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review</h3>
<ul>
<li><strong>Authors: </strong>Cheng Yuan, Xinkai Rui, Yongqi Fan, Yawei Fan, Boyang Zhong, Jiacheng Wang, Weiyan Zhang, Tong Ruan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05319">https://arxiv.org/abs/2507.05319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05319">https://arxiv.org/pdf/2507.05319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05319]] LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review(https://arxiv.org/abs/2507.05319)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository this https URL.</li>
</ul>

<h3>Title: MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Ming Gong, Xucheng Huang, Chenghan Yang, Xianhan Peng, Haoxin Wang, Yang Liu, Ling Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05330">https://arxiv.org/abs/2507.05330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05330">https://arxiv.org/pdf/2507.05330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05330]] MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents(https://arxiv.org/abs/2507.05330)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular "MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.</li>
</ul>

<h3>Title: LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks</h3>
<ul>
<li><strong>Authors: </strong>William Fleshman, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05346">https://arxiv.org/abs/2507.05346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05346">https://arxiv.org/pdf/2507.05346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05346]] LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks(https://arxiv.org/abs/2507.05346)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG).</li>
</ul>

<h3>Title: On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Alberghi, Elizaveta Demyanenko, Luca Biggio, Luca Saglietti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05362">https://arxiv.org/abs/2507.05362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05362">https://arxiv.org/pdf/2507.05362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05362]] On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study(https://arxiv.org/abs/2507.05362)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.</li>
</ul>

<h3>Title: YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries</h3>
<ul>
<li><strong>Authors: </strong>Aquino Joctum, John Kandiri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05376">https://arxiv.org/abs/2507.05376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05376">https://arxiv.org/pdf/2507.05376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05376]] YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries(https://arxiv.org/abs/2507.05376)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous vehicle perception systems require robust pedestrian detection, particularly on geometrically complex roadways like Type-S curved surfaces, where standard RGB camera-based methods face limitations. This paper introduces YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework specifically for this challenge. YOLO-APD integrates several key architectural modifications: a parameter-free SimAM attention mechanism, computationally efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale feature pooling, the Mish activation function for improved optimization, and an Intelligent Gather & Distribute (IGD) module for superior feature fusion in the network's neck. The concept of leveraging vehicle steering dynamics for adaptive region-of-interest processing is also presented. Comprehensive evaluations on a custom CARLA dataset simulating complex scenarios demonstrate that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7% mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly outperforming baseline models, including YOLOv8. Furthermore, it maintains real-time processing capabilities at 100 FPS, showcasing a superior balance between accuracy and efficiency. Ablation studies validate the synergistic contribution of each integrated component. Evaluation on the KITTI dataset confirms the architecture's potential while highlighting the need for domain adaptation. This research advances the development of highly accurate, efficient, and adaptable perception systems based on cost-effective sensors, contributing to enhanced safety and reliability for autonomous navigation in challenging, less-structured driving environments.</li>
</ul>

<h3>Title: Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling</h3>
<ul>
<li><strong>Authors: </strong>Alexandr A. Kalinin, Paula Llanos, Theresa Maria Sommer, Giovanni Sestini, Xinhai Hou, Jonathan Z. Sexton, Xiang Wan, Ivo D. Dinov, Brian D. Athey, Nicolas Rivron, Anne E. Carpenter, Beth Cimini, Shantanu Singh, Matthew J. O'Meara</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05383">https://arxiv.org/abs/2507.05383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05383">https://arxiv.org/pdf/2507.05383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05383]] Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling(https://arxiv.org/abs/2507.05383)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Microscopy enables direct observation of cellular morphology in 3D, with transmitted-light methods offering low-cost, minimally invasive imaging and fluorescence microscopy providing specificity and contrast. Virtual staining combines these strengths by using machine learning to predict fluorescence images from label-free inputs. However, training of existing methods typically relies on loss functions that treat all pixels equally, thus reproducing background noise and artifacts instead of focusing on biologically meaningful signals. We introduce Spotlight, a simple yet powerful virtual staining approach that guides the model to focus on relevant cellular structures. Spotlight uses histogram-based foreground estimation to mask pixel-wise loss and to calculate a Dice loss on soft-thresholded predictions for shape-aware learning. Applied to a 3D benchmark dataset, Spotlight improves morphological representation while preserving pixel-level accuracy, resulting in virtual stains better suited for downstream tasks such as segmentation and profiling.</li>
</ul>

<h3>Title: Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Min Xie, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05386">https://arxiv.org/abs/2507.05386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05386">https://arxiv.org/pdf/2507.05386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05386]] Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training(https://arxiv.org/abs/2507.05386)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis shows that explicit mechanisms, such as KL penalty and chain-of-thought reasoning, are not the primary factors. Instead, we find that the implicit regularization inherent to RFT is a key factor in mitigating forgetting. Finally, we propose a rollout-based instance filtering algorithm to improve the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.</li>
</ul>

<h3>Title: The Generalization Ridge: Information Flow in Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruidi Chang, Chunyuan Deng, Hanjie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05387">https://arxiv.org/abs/2507.05387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05387">https://arxiv.org/pdf/2507.05387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05387]] The Generalization Ridge: Information Flow in Natural Language Generation(https://arxiv.org/abs/2507.05387)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based language models have achieved state-of-the-art performance in natural language generation (NLG) tasks, yet their internal mechanisms for synthesizing task-relevant information remain insufficiently understood. While prior studies suggest that intermediate layers often yield more generalizable representations than final layers, how this generalization ability emerges and propagates across layers during training remains unclear. To address this gap, we propose InfoRidge, an information-theoretic framework, to characterize how predictive information-the mutual information between hidden representations and target outputs-varies across depth. Estimating this quantity enables us to trace the flow of task-relevant information throughout the model during training. Our experiments across various models and datasets reveal a consistent non-monotonic trend: predictive information peaks in upper-middle layers-forming a generalization ridge-before declining in final layers, reflecting a transition between generalization and memorization. To further investigate this phenomenon, we introduce residual scaling coefficients-trainable scalar parameters applied to each residual block-which serve as functional probes for assessing the relative importance of individual transformer layers. These coefficients reveal that, under distribution shift, models downweight final layers and increasingly rely on ridge layers, highlighting their role in generalization. Together, these findings offer new insights into the internal mechanisms of transformers and underscore the critical role of intermediate layers in supporting generalization.</li>
</ul>

<h3>Title: From General to Specialized: The Need for Foundational Models in Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Vishal Nedungadi, Xingguo Xiong, Aike Potze, Ron Van Bree, Tao Lin, Marc Rußwurm, Ioannis N. Athanasiadis</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05390">https://arxiv.org/abs/2507.05390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05390">https://arxiv.org/pdf/2507.05390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05390]] From General to Specialized: The Need for Foundational Models in Agriculture(https://arxiv.org/abs/2507.05390)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Food security remains a global concern as population grows and climate change intensifies, demanding innovative solutions for sustainable agricultural productivity. Recent advances in foundation models have demonstrated remarkable performance in remote sensing and climate sciences, and therefore offer new opportunities for agricultural monitoring. However, their application in challenges related to agriculture-such as crop type mapping, crop phenology estimation, and crop yield estimation-remains under-explored. In this work, we quantitatively evaluate existing foundational models to assess their effectivity for a representative set of agricultural tasks. From an agricultural domain perspective, we describe a requirements framework for an ideal agricultural foundation model (CropFM). We then survey and compare existing general-purpose foundational models in this framework and empirically evaluate two exemplary of them in three representative agriculture specific tasks. Finally, we highlight the need for a dedicated foundational model tailored specifically to agriculture.</li>
</ul>

<h3>Title: Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences</h3>
<ul>
<li><strong>Authors: </strong>Guillem Ramírez, Alexandra Birch, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05391">https://arxiv.org/abs/2507.05391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05391">https://arxiv.org/pdf/2507.05391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05391]] Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences(https://arxiv.org/abs/2507.05391)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are primarily accessed via commercial APIs, but this often requires users to expose their data to service providers. In this paper, we explore how users can stay in control of their data by using privacy profiles: simple natural language instructions that say what should and should not be revealed. We build a framework where a local model uses these instructions to rewrite queries, only hiding details deemed sensitive by the user, before sending them to an external model, thus balancing privacy with performance. To support this research, we introduce PEEP, a multilingual dataset of real user queries annotated to mark private content and paired with synthetic privacy profiles. Our experiments with lightweight LLMs show they can follow these instructions to some extent, but also face consistent challenges, highlighting the need for models that better understand and comply with user-defined privacy preferences.</li>
</ul>

<h3>Title: Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration</h3>
<ul>
<li><strong>Authors: </strong>Jose M. Montero, Jose-Luis Lisani</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05393">https://arxiv.org/abs/2507.05393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05393">https://arxiv.org/pdf/2507.05393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05393]] Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration(https://arxiv.org/abs/2507.05393)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning, particularly neural networks, have significantly impacted a wide range of fields, including the automatic enhancement of underwater images. This paper presents a deep learning-based approach to improving underwater image quality by integrating human subjective assessments into the training process. To this end, we utilize publicly available datasets containing underwater images labeled by experts as either high or low quality. Our method involves first training a classifier network to distinguish between high- and low-quality images. Subsequently, generative adversarial networks (GANs) are trained using various enhancement criteria to refine the low-quality images. The performance of the GAN models is evaluated using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through qualitative analysis. Results demonstrate that the proposed model -- particularly when incorporating criteria such as color fidelity and image sharpness -- achieves substantial improvements in both perceived and measured image quality.</li>
</ul>

<h3>Title: pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sajjad Ghiasvand, Mahnoosh Alizadeh, Ramtin Pedarsani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05394">https://arxiv.org/abs/2507.05394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05394">https://arxiv.org/pdf/2507.05394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05394]] pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models(https://arxiv.org/abs/2507.05394)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) like CLIP have demonstrated remarkable generalization in zero- and few-shot settings, but adapting them efficiently to decentralized, heterogeneous data remains a challenge. While prompt tuning has emerged as a popular parameter-efficient approach in personalized federated learning, existing methods often sacrifice generalization in favor of personalization, struggling particularly on unseen classes or domains. In this work, we propose pFedMMA, the first personalized federated learning framework that leverages multi-modal adapters for vision-language tasks. Each adapter contains modality-specific up- and down-projection layers alongside a globally shared projection that aligns cross-modal features. Our asymmetric optimization strategy allows clients to locally adapt to personalized data distributions while collaboratively training the shared projection to improve global generalization. This design is also communication-efficient, as only the shared component is exchanged during rounds. Through extensive experiments across eleven datasets, including domain- and label-shift scenarios, we show that pFedMMA achieves state-of-the-art trade-offs between personalization and generalization, outperforming recent federated prompt tuning methods. The code is available at this https URL.</li>
</ul>

<h3>Title: Neural-Driven Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Zhou, Jie Xia, Xiaopeng Peng, Wangbo Zhao, Zilong Ye, Zekai Li, Suorong Yang, Jiadong Pan, Yuanxiang Chen, Ziqiao Wang, Kai Wang, Qian Zheng, Xiaojun Chang, Gang Pan, Shurong Dong, Kaipeng Zhang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05397">https://arxiv.org/abs/2507.05397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05397">https://arxiv.org/pdf/2507.05397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05397]] Neural-Driven Image Editing(https://arxiv.org/abs/2507.05397)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area.</li>
</ul>

<h3>Title: Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification</h3>
<ul>
<li><strong>Authors: </strong>Luca Marzari, Ferdinando Cicalese, Alessandro Farinelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05405">https://arxiv.org/abs/2507.05405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05405">https://arxiv.org/pdf/2507.05405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05405]] Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification(https://arxiv.org/abs/2507.05405)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present $\textbf{P}$robabilistically $\textbf{T}$ightened $\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation $\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines over-approximation techniques from LiRPA-based approaches with a sampling-based method to compute tight intermediate reachable sets. In detail, we show that with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the estimated reachable sets, significantly tightens the lower and upper linear bounds of a neural network's output, reducing the computational cost of formal verification tools while providing probabilistic guarantees on verification soundness. Extensive experiments on standard formal verification benchmarks, including the International Verification of Neural Networks Competition, show that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic approach results in a valuable solution for challenging competition entries where state-of-the-art formal verification methods fail, allowing us to provide answers with high confidence (i.e., at least 99%).</li>
</ul>

<h3>Title: Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift</h3>
<ul>
<li><strong>Authors: </strong>Gautam Sreekumar, Vishnu Naresh Boddeti</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05412">https://arxiv.org/abs/2507.05412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05412">https://arxiv.org/pdf/2507.05412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05412]] Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift(https://arxiv.org/abs/2507.05412)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We consider the problem of learning robust discriminative representations of causally-related latent variables. In addition to observational data, the training dataset also includes interventional data obtained through targeted interventions on some of these latent variables to learn representations robust against the resulting interventional distribution shifts. Existing approaches treat interventional data like observational data, even when the underlying causal model is known, and ignore the independence relations that arise from these interventions. Since these approaches do not fully exploit the causal relational information resulting from interventions, they learn representations that produce large disparities in predictive performance on observational and interventional data, which worsens when the number of interventional training samples is limited. In this paper, (1) we first identify a strong correlation between this performance disparity and adherence of the representations to the independence conditions induced by the interventional causal model. (2) For linear models, we derive sufficient conditions on the proportion of interventional data in the training dataset, for which enforcing interventional independence between representations corresponding to the intervened node and its non-descendants lowers the error on interventional data. Combining these insights, (3) we propose RepLIn, a training algorithm to explicitly enforce this statistical independence during interventions. We demonstrate the utility of RepLIn on a synthetic dataset and on real image and text datasets on facial attribute classification and toxicity detection, respectively. Our experiments show that RepLIn is scalable with the number of nodes in the causal graph and is suitable to improve the robust representations against interventional distribution shifts of both continuous and discrete latent variables.</li>
</ul>

<h3>Title: Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the Multiple Privacy Policies and Controls of U.S. Banks</h3>
<ul>
<li><strong>Authors: </strong>Lu Xian, Van Tran, Lauren Lee, Meera Kumar, Yichen Zhang, Florian Schaub</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05415">https://arxiv.org/abs/2507.05415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05415">https://arxiv.org/pdf/2507.05415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05415]] Layered, Overlapping, and Inconsistent: A Large-Scale Analysis of the Multiple Privacy Policies and Controls of U.S. Banks(https://arxiv.org/abs/2507.05415)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Privacy policies are often complex. An exception is the two-page standardized notice that U.S. financial institutions must provide under the Gramm-Leach-Bliley Act (GLBA). However, banks now operate websites, mobile apps, and other services that involve complex data sharing practices that require additional privacy notices and do-not-sell opt-outs. We conducted a large-scale analysis of how U.S. banks implement privacy policies and controls in response to GLBA; other federal privacy policy requirements; and the California Consumer Privacy Act (CCPA), a key example for U.S. state privacy laws. We focused on the disclosure and control of a set of especially privacy-invasive practices: third-party data sharing for marketing-related purposes. We collected privacy policies for the 2,067 largest U.S. banks, 45.3\% of which provided multiple policies. Across disclosures and controls within the \textit{same} bank, we identified frequent, concerning inconsistencies -- such as banks indicating in GLBA notices that they do not share with third parties but disclosing sharing elsewhere, or using third-party marketing/advertising cookies without disclosure. This multiplicity of policies, with the inconsistencies it causes, may create consumer confusion and undermine the transparency goals of the very laws that require them. Our findings call into question whether current policy requirements, such as the GLBA notice, are achieving their intended goals in today's online banking landscape. We discuss potential avenues for reforming and harmonizing privacy policies and control requirements across federal and state laws.</li>
</ul>

<h3>Title: EmissionNet: Air Quality Pollution Forecasting for Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Prady Saligram, Tanvir Bhathal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05416">https://arxiv.org/abs/2507.05416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05416">https://arxiv.org/pdf/2507.05416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05416]] EmissionNet: Air Quality Pollution Forecasting for Agriculture(https://arxiv.org/abs/2507.05416)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Air pollution from agricultural emissions is a significant yet often overlooked contributor to environmental and public health challenges. Traditional air quality forecasting models rely on physics-based approaches, which struggle to capture complex, nonlinear pollutant interactions. In this work, we explore forecasting N$_2$O agricultural emissions through evaluating popular architectures, and proposing two novel deep learning architectures, EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage convolutional and transformer-based architectures to extract spatial-temporal dependencies from high-resolution emissions data</li>
</ul>

<h3>Title: Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05418">https://arxiv.org/abs/2507.05418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05418">https://arxiv.org/pdf/2507.05418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05418]] Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning(https://arxiv.org/abs/2507.05418)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual QA, and code generation, yet their multilingual reasoning capabilities in these tasks remain underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. Current multilingual benchmarks focus only on final answers, overlooking whether models actually reason in the target language. To address this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark with annotated reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. We further propose BRIDGE, a novel training method that guides supervised fine-tuning and test-time reinforcement learning with a language-consistency reward to align reasoning with the input language. Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to assess answer correctness and the quality and language consistency of reasoning traces, enabling nuanced and scalable analysis beyond surface-level metrics. Our results show that BRIDGE significantly enhances multilingual reasoning fidelity, demonstrating that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. this https URL</li>
</ul>

<h3>Title: Motion Generation: A Survey of Generative Approaches and Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Aliasghar Khani, Arianna Rampini, Bruno Roy, Larasika Nadela, Noa Kaplan, Evan Atherton, Derek Cheung, Jacky Bibliowicz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05419">https://arxiv.org/abs/2507.05419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05419">https://arxiv.org/pdf/2507.05419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05419]] Motion Generation: A Survey of Generative Approaches and Benchmarks(https://arxiv.org/abs/2507.05419)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Motion generation, the task of synthesizing realistic motion sequences from various conditioning inputs, has become a central problem in computer vision, computer graphics, and robotics, with applications ranging from animation and virtual agents to human-robot interaction. As the field has rapidly progressed with the introduction of diverse modeling paradigms including GANs, autoencoders, autoregressive models, and diffusion-based techniques, each approach brings its own advantages and limitations. This growing diversity has created a need for a comprehensive and structured review that specifically examines recent developments from the perspective of the generative approach employed. In this survey, we provide an in-depth categorization of motion generation methods based on their underlying generative strategies. Our main focus is on papers published in top-tier venues since 2023, reflecting the most recent advancements in the field. In addition, we analyze architectural principles, conditioning mechanisms, and generation settings, and compile a detailed overview of the evaluation metrics and datasets used across the literature. Our objective is to enable clearer comparisons and identify open challenges, thereby offering a timely and foundational reference for researchers and practitioners navigating the rapidly evolving landscape of motion generation.</li>
</ul>

<h3>Title: "Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yufei Tao, Adam Hiatt, Rahul Seetharaman, Ameeta Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05424">https://arxiv.org/abs/2507.05424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05424">https://arxiv.org/pdf/2507.05424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05424]] "Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models(https://arxiv.org/abs/2507.05424)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are capable of leveraging both contextual and parametric knowledge but how they prioritize and integrate these sources remains underexplored. We introduce CoPE, a novel evaluation framework that systematically measures contextual knowledge (CK) and parametric knowledge (PK) across models and languages. Using our MultiWikiAtomic dataset in English, Spanish, and Danish, we analyze how large language models (LLMs) integrate context, prioritize information, and incorporate PK in open-ended question answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where LLMs tend to overlook or deprioritize information that appears later in a given context, revealing a strong positional bias that affects contextual grounding. We further find that reasoning models, as well as non-reasoning models prompted with chain-of-thought (CoT), use context even less than non-reasoning models without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting, in particular, results in lower recall and shorter responses, leading to degraded contextual grounding. Based on these insights, we design prompt-based methods to effectively leverage input context. A case study applying CoPE to summarization demonstrates that CK-informed prompting improves factual grounding and reduces hallucination.</li>
</ul>

<h3>Title: Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors</h3>
<ul>
<li><strong>Authors: </strong>Lanqing Guo, Yufei Wang, Hezhen Hu, Yan Zheng, Yeying Jin, Siyu Huang, Zhangyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05426">https://arxiv.org/abs/2507.05426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05426">https://arxiv.org/pdf/2507.05426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05426]] Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors(https://arxiv.org/abs/2507.05426)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Many 3D scene editing tasks focus on modifying local regions rather than the entire scene, except for some global applications like style transfer, and in the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a series of Gaussians, this structure allows for precise regional edits, offering enhanced control over specific areas of the scene; however, the challenge lies in the fact that 3D semantic parsing often underperforms compared to its 2D counterpart, making targeted manipulations within 3D spaces more difficult and limiting the fidelity of edits, which we address by leveraging 2D diffusion editing to accurately identify modification regions in each view, followed by inverse rendering for 3D localization, then refining the frontal view and initializing a coarse 3DGS with consistent views and approximate shapes derived from depth maps predicted by a 2D foundation model, thereby supporting an iterative, view-consistent editing process that gradually enhances structural details and textures to ensure coherence across perspectives. Experiments demonstrate that our method achieves state-of-the-art performance while delivering up to a $4\times$ speedup, providing a more efficient and effective approach to 3D scene local editing.</li>
</ul>

<h3>Title: OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts</h3>
<ul>
<li><strong>Authors: </strong>Shiting Xiao, Rishabh Kabra, Yuhang Li, Donghyun Lee, Joao Carreira, Priyadarshini Panda</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05427">https://arxiv.org/abs/2507.05427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05427">https://arxiv.org/pdf/2507.05427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05427]] OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts(https://arxiv.org/abs/2507.05427)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The ability to segment objects based on open-ended language prompts remains a critical challenge, requiring models to ground textual semantics into precise spatial masks while handling diverse and unseen categories. We present OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings extracted from a lightweight vision-language model (VLM). Our approach is guided by four key principles: i) Unified prompting: OpenWorldSAM supports a diverse range of prompts, including category-level and sentence-level language descriptions, providing a flexible interface for various segmentation tasks. ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we train only 4.5 million parameters on the COCO-stuff dataset, achieving remarkable resource efficiency. iii) Instance Awareness: We enhance the model's spatial understanding through novel positional tie-breaker embeddings and cross-attention layers, enabling effective segmentation of multiple instances. iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities, generalizing well on unseen categories and an open vocabulary of concepts without additional training. Extensive experiments demonstrate that OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks, including ADE20k, PASCAL, ScanNet, and SUN-RGBD.</li>
</ul>

<h3>Title: Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Inayat Rasool, Pappu Kumar Yadav, Amee Parmar, Hasan Mirzakhaninafchi, Rikesh Budhathoki, Zain Ul Abideen Usmani, Supriya Paudel, Ivan Perez Olivera, Eric Jone</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05432">https://arxiv.org/abs/2507.05432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05432">https://arxiv.org/pdf/2507.05432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05432]] Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation(https://arxiv.org/abs/2507.05432)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Uniform and excessive herbicide application in modern agriculture contributes to increased input costs, environmental pollution, and the emergence of herbicide resistant weeds. To address these challenges, we developed a vision guided, AI-driven variable rate sprayer system capable of detecting weed presence, estimating canopy size, and dynamically adjusting nozzle activation in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference, and uses an Arduino Uno-based relay interface to control solenoid actuated nozzles based on canopy segmentation results. Indoor trials were conducted using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to simulate a range of weed patch scenarios. The YOLO11n model achieved a mean average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision of 0.55, and recall of 0.52. System performance was validated using water sensitive paper, which showed an average spray coverage of 24.22% in zones where canopy was present. An upward trend in mean spray coverage from 16.22% for small canopies to 21.46% and 21.65% for medium and large canopies, respectively, demonstrated the system's capability to adjust spray output based on canopy size in real time. These results highlight the potential of combining real time deep learning with low-cost embedded hardware for selective herbicide application. Future work will focus on expanding the detection capabilities to include three common weed species in South Dakota: water hemp (Amaranthus tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed by further validation in both indoor and field trials within soybean and corn production systems.</li>
</ul>

<h3>Title: Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack</h3>
<ul>
<li><strong>Authors: </strong>Edward Raff, Karen Kukla, Michel Benaroch, Joseph Comprix</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05441">https://arxiv.org/abs/2507.05441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05441">https://arxiv.org/pdf/2507.05441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05441]] Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack(https://arxiv.org/abs/2507.05441)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Bad actors, primarily distressed firms, have the incentive and desire to manipulate their financial reports to hide their distress and derive personal gains. As attackers, these firms are motivated by potentially millions of dollars and the availability of many publicly disclosed and used financial modeling frameworks. Existing attack methods do not work on this data due to anti-correlated objectives that must both be satisfied for the attacker to succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that adapt the attacker's search direction to find $20\times$ more satisfying attacks compared to standard attacks. The result is that in $\approx50\%$ of cases, a company could inflate their earnings by 100-200%, while simultaneously reducing their fraud scores by 15%. By working with lawyers and professional accountants, we ensure our threat model is realistic to how such frauds are performed in practice.</li>
</ul>

<h3>Title: PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs</h3>
<ul>
<li><strong>Authors: </strong>Sana Kang, Myeongseok Gwon, Su Young Kwon, Jaewook Lee, Andrew Lan, Bhiksha Raj, Rita Singh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05444">https://arxiv.org/abs/2507.05444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05444">https://arxiv.org/pdf/2507.05444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05444]] PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs(https://arxiv.org/abs/2507.05444)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Vocabulary acquisition poses a significant challenge for second-language (L2) learners, especially when learning typologically distant languages such as English and Korean, where phonological and structural mismatches complicate vocabulary learning. Recently, large language models (LLMs) have been used to generate keyword mnemonics by leveraging similar keywords from a learner's first language (L1) to aid in acquiring L2 vocabulary. However, most of this research has focused on native English speakers learning other languages, rather than the reverse. In this paper, we present PhoniTale, a novel cross-lingual mnemonic generation system that retrieves L1 keyword sequence based on phonological similarity and uses LLMs to generate mnemonics. We evaluate PhoniTale using both automated metrics and human evaluations, comparing its output to mnemonics created by humans and by previous automated approaches. To assess practical effectiveness, we also conduct a short-term recall test measuring mnemonic helpfulness. Our findings show that PhoniTale performs comparably to human-authored mnemonics. We also highlight key areas for future improvement in mnemonic quality and methodology.</li>
</ul>

<h3>Title: A Systematization of Security Vulnerabilities in Computer Use Agents</h3>
<ul>
<li><strong>Authors: </strong>Daniel Jones, Giorgio Severi, Martin Pouliot, Gary Lopez, Joris de Gruyter, Santiago Zanella-Beguelin, Justin Song, Blake Bullwinkel, Pamela Cortez, Amanda Minnich</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05445">https://arxiv.org/abs/2507.05445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05445">https://arxiv.org/pdf/2507.05445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05445]] A Systematization of Security Vulnerabilities in Computer Use Agents(https://arxiv.org/abs/2507.05445)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Computer Use Agents (CUAs), autonomous systems that interact with software interfaces via browsers or virtual machines, are rapidly being deployed in consumer and enterprise environments. These agents introduce novel attack surfaces and trust boundaries that are not captured by traditional threat models. Despite their growing capabilities, the security boundaries of CUAs remain poorly understood. In this paper, we conduct a systematic threat analysis and testing of real-world CUAs under adversarial conditions. We identify seven classes of risks unique to the CUA paradigm, and analyze three concrete exploit scenarios in depth: (1) clickjacking via visual overlays that mislead interface-level reasoning, (2) indirect prompt injection that enables Remote Code Execution (RCE) through chained tool use, and (3) CoT exposure attacks that manipulate implicit interface framing to hijack multi-step reasoning. These case studies reveal deeper architectural flaws across current CUA implementations. Namely, a lack of input provenance tracking, weak interface-action binding, and insufficient control over agent memory and delegation. We conclude by proposing a CUA-specific security evaluation framework and design principles for safe deployment in adversarial and high-stakes settings.</li>
</ul>

<h3>Title: On the Semantics of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Martin Schuele</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05448">https://arxiv.org/abs/2507.05448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05448">https://arxiv.org/pdf/2507.05448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05448]] On the Semantics of Large Language Models(https://arxiv.org/abs/2507.05448)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) such as ChatGPT demonstrated the potential to replicate human language abilities through technology, ranging from text generation to engaging in conversations. However, it remains controversial to what extent these systems truly understand language. We examine this issue by narrowing the question down to the semantics of LLMs at the word and sentence level. By examining the inner workings of LLMs and their generated representation of language and by drawing on classical semantic theories by Frege and Russell, we get a more nuanced picture of the potential semantic capabilities of LLMs.</li>
</ul>

<h3>Title: Navigating Sparse Molecular Data with Stein Diffusion Guidance</h3>
<ul>
<li><strong>Authors: </strong>Van Khoa Nguyen, Lionel Blondé, Alexandros Kalousis</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05482">https://arxiv.org/abs/2507.05482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05482">https://arxiv.org/pdf/2507.05482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05482]] Navigating Sparse Molecular Data with Stein Diffusion Guidance(https://arxiv.org/abs/2507.05482)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Stochastic optimal control (SOC) has recently emerged as a principled framework for fine-tuning diffusion models. However, its dependence on computationally intensive simulations makes it impractical for fast sampling. In parallel, a class of training-free approaches has been developed that guides diffusion models using off-the-shelf classifiers on predicted clean samples, bypassing the need to train classifiers on noisy data. These methods can be interpreted as approximate SOC schemes, using Tweedie's formula to estimate diffusion posteriors. In practice, however, such direct approximations can introduce significant errors, leading to unreliable guidance. In this work, we unify the strengths of both paradigms by proposing a novel training-free diffusion guidance framework based on a surrogate stochastic optimal control objective. We derive a new theoretical bound on the value function that reveals the necessity of correcting the approximate posteriors to remain faithful to the true diffusion posterior. To this end, we connect the problem with Stein variational inference, which seeks the steepest descent direction that minimizes the Kullback-Leibler discrepancy between the two posteriors. Our method, which we refer to as Stein Diffusion Guidance (SDG), introduces a principled correction mechanism and incorporates a novel running cost functional to enable effective guidance in low-density regions. Experiments on challenging molecular generation tasks demonstrate that SDG significantly outperforms standard training-free guidance methods, highlighting its potential for broader applications.</li>
</ul>

<h3>Title: Cloud Diffusion Part 1: Theory and Motivation</h3>
<ul>
<li><strong>Authors: </strong>Andrew Randono</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05496">https://arxiv.org/abs/2507.05496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05496">https://arxiv.org/pdf/2507.05496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05496]] Cloud Diffusion Part 1: Theory and Motivation(https://arxiv.org/abs/2507.05496)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models for image generation function by progressively adding noise to an image set and training a model to separate out the signal from the noise. The noise profile used by these models is white noise -- that is, noise based on independent normal distributions at each point whose mean and variance is independent of the scale. By contrast, most natural image sets exhibit a type of scale invariance in their low-order statistical properties characterized by a power-law scaling. Consequently, natural images are closer (in a quantifiable sense) to a different probability distribution that emphasizes large scale correlations and de-emphasizes small scale correlations. These scale invariant noise profiles can be incorporated into diffusion models in place of white noise to form what we will call a ``Cloud Diffusion Model". We argue that these models can lead to faster inference, improved high-frequency details, and greater controllability. In a follow-up paper, we will build and train a Cloud Diffusion Model that uses scale invariance at a fundamental level and compare it to classic, white noise diffusion models.</li>
</ul>

<h3>Title: LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving</h3>
<ul>
<li><strong>Authors: </strong>Giulio Federico, Fabio Carrara, Claudio Gennaro, Giuseppe Amato, Marco Di Benedetto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05499">https://arxiv.org/abs/2507.05499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05499">https://arxiv.org/pdf/2507.05499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05499]] LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving(https://arxiv.org/abs/2507.05499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating consistent multi-view images from a single image remains challenging. Lack of spatial consistency often degrades 3D mesh quality in surface reconstruction. To address this, we propose LoomNet, a novel multi-view diffusion architecture that produces coherent images by applying the same diffusion model multiple times in parallel to collaboratively build and leverage a shared latent space for view consistency. Each viewpoint-specific inference generates an encoding representing its own hypothesis of the novel view from a given camera pose, which is projected onto three orthogonal planes. For each plane, encodings from all views are fused into a single aggregated plane. These aggregated planes are then processed to propagate information and interpolate missing regions, combining the hypotheses into a unified, coherent interpretation. The final latent space is then used to render consistent multi-view images. LoomNet generates 16 high-quality and coherent views in just 15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on both image quality and reconstruction metrics, also showing creativity by producing diverse, plausible novel views from the same input.</li>
</ul>

<h3>Title: Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs</h3>
<ul>
<li><strong>Authors: </strong>Godwin Badu-Marfo, Bilal Farooq</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05507">https://arxiv.org/abs/2507.05507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05507">https://arxiv.org/pdf/2507.05507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05507]] Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs(https://arxiv.org/abs/2507.05507)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, interpretability</a></li>
<li><strong>Abstract: </strong>We present an integrated graph-based neural networks architecture for predicting campus buildings occupancy and inter-buildings movement at dynamic temporal resolution that learns traffic flow patterns from Wi-Fi logs combined with the usage schedules within the buildings. The relative traffic flows are directly estimated from the WiFi data without assuming the occupant behaviour or preferences while maintaining individual privacy. We formulate the problem as a data-driven graph structure represented by a set of nodes (representing buildings), connected through a route of edges or links using a novel Graph Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable success in modelling complex patterns. We describe the formulation, model estimation, interpretability and examine the relative performance of our proposed model. We also present an illustrative architecture of the models and apply on real-world WiFi logs collected at the Toronto Metropolitan University campus. The results of the experiments show that the integrated GCLSTM models significantly outperform traditional pedestrian flow estimators like the Multi Layer Perceptron (MLP) and Linear Regression.</li>
</ul>

<h3>Title: Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice</h3>
<ul>
<li><strong>Authors: </strong>Gehao Zhang, Eugene Bagdasarian, Juan Zhai, Shiqing Ma</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05512">https://arxiv.org/abs/2507.05512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05512">https://arxiv.org/pdf/2507.05512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05512]] Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice(https://arxiv.org/abs/2507.05512)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>Distinguishing AI-generated code from human-written code is becoming crucial for tasks such as authorship attribution, content tracking, and misuse detection. Based on this, N-gram-based watermarking schemes have emerged as prominent, which inject secret watermarks to be detected during the generation. However, their robustness in code content remains insufficiently evaluated. Most claims rely solely on defenses against simple code transformations or code optimizations as a simulation of attack, creating a questionable sense of robustness. In contrast, more sophisticated schemes already exist in the software engineering world, e.g., code obfuscation, which significantly alters code while preserving functionality. Although obfuscation is commonly used to protect intellectual property or evade software scanners, the robustness of code watermarking techniques against such transformations remains largely unexplored. In this work, we formally model the code obfuscation and prove the impossibility of N-gram-based watermarking's robustness with only one intuitive and experimentally verified assumption, distribution consistency, satisfied. Given the original false positive rate of the watermarking detection, the ratio that the detector failed on the watermarked code after obfuscation will increase to 1 - fpr. The experiments have been performed on three SOTA watermarking schemes, two LLMs, two programming languages, four code benchmarks, and four obfuscators. Among them, all watermarking detectors show coin-flipping detection abilities on obfuscated codes (AUROC tightly surrounds 0.5). Among all models, watermarking schemes, and datasets, both programming languages own obfuscators that can achieve attack effects with no detection AUROC higher than 0.6 after the attack. Based on the theoretical and practical observations, we also proposed a potential path of robust code watermarking.</li>
</ul>

<h3>Title: Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications</h3>
<ul>
<li><strong>Authors: </strong>Jean-Philippe Corbeil, Asma Ben Abacha, George Michalopoulos, Phillip Swazinna, Miguel Del-Agua, Jerome Tremblay, Akila Jeeson Daniel, Cari Bader, Kevin Cho, Pooja Krishnan, Nathan Bodenstab, Thomas Lin, Wenxuan Teng, Francois Beaulieu, Paul Vozila</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05517">https://arxiv.org/abs/2507.05517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05517">https://arxiv.org/pdf/2507.05517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05517]] Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications(https://arxiv.org/abs/2507.05517)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong performance on clinical natural language processing (NLP) tasks across multiple medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular reporting from nurse dictations and medical order extraction from doctor-patient consultations - remain underexplored due to data scarcity and sensitivity, despite active industry efforts. Practical solutions to these real-world clinical tasks can significantly reduce the documentation burden on healthcare providers, allowing greater focus on patient care. In this paper, we investigate these two challenging tasks using private and open-source clinical datasets, evaluating the performance of both open- and closed-weight LLMs, and analyzing their respective strengths and limitations. Furthermore, we propose an agentic pipeline for generating realistic, non-sensitive nurse dictations, enabling structured extraction of clinical observations. To support further research in both areas, we release SYNUR and SIMORD, the first open-source datasets for nurse observation extraction and medical order extraction.</li>
</ul>

<h3>Title: PROTEAN: Federated Intrusion Detection in Non-IID Environments through Prototype-Based Knowledge Sharing</h3>
<ul>
<li><strong>Authors: </strong>Sara Chennoufi, Yufei Han, Gregory Blanc, Emiliano De Cristofaro, Christophe Kiennert</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05524">https://arxiv.org/abs/2507.05524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05524">https://arxiv.org/pdf/2507.05524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05524]] PROTEAN: Federated Intrusion Detection in Non-IID Environments through Prototype-Based Knowledge Sharing(https://arxiv.org/abs/2507.05524)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>In distributed networks, participants often face diverse and fast-evolving cyberattacks. This makes techniques based on Federated Learning (FL) a promising mitigation strategy. By only exchanging model updates, FL participants can collaboratively build detection models without revealing sensitive information, e.g., network structures or security postures. However, the effectiveness of FL solutions is often hindered by significant data heterogeneity, as attack patterns often differ drastically across organizations due to varying security policies. To address these challenges, we introduce PROTEAN, a Prototype Learning-based framework geared to facilitate collaborative and privacy-preserving intrusion detection. PROTEAN enables accurate detection in environments with highly non-IID attack distributions and promotes direct knowledge sharing by exchanging class prototypes of different attack types among participants. This allows organizations to better understand attack techniques not present in their data collections. We instantiate PROTEAN on two cyber intrusion datasets collected from IIoT and 5G-connected participants and evaluate its performance in terms of utility and privacy, demonstrating its effectiveness in addressing data heterogeneity while improving cyber attack understanding in federated intrusion detection systems (IDSs).</li>
</ul>

<h3>Title: Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning</h3>
<ul>
<li><strong>Authors: </strong>Anish Dhir, Cristiana Diaconu, Valentinian Mihai Lungu, James Requeima, Richard E. Turner, Mark van der Wilk</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05526">https://arxiv.org/abs/2507.05526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05526">https://arxiv.org/pdf/2507.05526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05526]] Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning(https://arxiv.org/abs/2507.05526)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In scientific domains -- from biology to the social sciences -- many questions boil down to \textit{What effect will we observe if we intervene on a particular variable?} If the causal relationships (e.g.~a causal graph) are known, it is possible to estimate the intervention distributions. In the absence of this domain knowledge, the causal structure must be discovered from the available observational data. However, observational data are often compatible with multiple causal graphs, making methods that commit to a single structure prone to overconfidence. A principled way to manage this structural uncertainty is via Bayesian inference, which averages over a posterior distribution on possible causal structures and functional mechanisms. Unfortunately, the number of causal structures grows super-exponentially with the number of nodes in the graph, making computations intractable. We propose to circumvent these challenges by using meta-learning to create an end-to-end model: the Model-Averaged Causal Estimation Transformer Neural Process (MACE-TNP). The model is trained to predict the Bayesian model-averaged interventional posterior distribution, and its end-to-end nature bypasses the need for expensive calculations. Empirically, we demonstrate that MACE-TNP outperforms strong Bayesian baselines. Our work establishes meta-learning as a flexible and scalable paradigm for approximating complex Bayesian causal inference, that can be scaled to increasingly challenging settings in the future.</li>
</ul>

<h3>Title: Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search</h3>
<ul>
<li><strong>Authors: </strong>Sanaz Kazemi Abharian, Sai Manoj Pudukotai Dinakarrao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05531">https://arxiv.org/abs/2507.05531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05531">https://arxiv.org/pdf/2507.05531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05531]] Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search(https://arxiv.org/abs/2507.05531)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have emerged as a powerful machine learning method for graph-structured data. A plethora of hardware accelerators has been introduced to meet the performance demands of GNNs in real-world applications. However, security challenges of hardware-based attacks have been generally overlooked. In this paper, we investigate the vulnerability of GNN models to hardware-based fault attack, wherein an attacker attempts to misclassify output by modifying trained weight parameters through fault injection in a memory device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware bit-flip fault attack, selecting a vulnerable bit in each selected weight gradually to compromise the GNN's performance by flipping a minimal number of bits. To achieve this, GBFA operates in two steps. First, a Markov model is created to predict the execution sequence of layers based on features extracted from memory access patterns, enabling the launch of the attack within a specific layer. Subsequently, GBFA identifies vulnerable bits within the selected weights using gradient ranking through an in-layer search. We evaluate the effectiveness of the proposed GBFA attack on various GNN models for node classification tasks using the Cora and PubMed datasets. Our findings show that GBFA significantly degrades prediction accuracy, and the variation in its impact across different layers highlights the importance of adopting a layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's prediction accuracy by 17% on the Cora dataset with only a single bit flip in the last layer.</li>
</ul>

<h3>Title: Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception</h3>
<ul>
<li><strong>Authors: </strong>Moseli Mots'oehli, Feimei Chen, Hok Wai Chan, Itumeleng Tlali, Thulani Babeli, Kyungim Baek, Huaijin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05536">https://arxiv.org/abs/2507.05536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05536">https://arxiv.org/pdf/2507.05536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05536]] Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception(https://arxiv.org/abs/2507.05536)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The scarcity of autonomous vehicle datasets from developing regions, particularly across Africa's diverse urban, rural, and unpaved roads, remains a key obstacle to robust perception in low-resource settings. We present a procedural augmentation pipeline that enhances low-cost monocular dashcam footage with realistic refractive distortions and weather-induced artifacts tailored to challenging African driving scenarios. Our refractive module simulates optical effects from low-quality lenses and air turbulence, including lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free (incompressible) warps. The weather module adds homogeneous fog, heterogeneous fog, and lens flare. To establish a benchmark, we provide baseline performance using three image restoration models. To support perception research in underrepresented African contexts, without costly data collection, labeling, or simulation, we release our distortion toolkit, augmented dataset splits, and benchmark results.</li>
</ul>

<h3>Title: Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Chunhui Gu, Mohammad Sadegh Nasr, James P. Long, Kim-Anh Do, Ehsan Irajizad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05540">https://arxiv.org/abs/2507.05540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05540">https://arxiv.org/pdf/2507.05540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05540]] Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge(https://arxiv.org/abs/2507.05540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) often struggle with noisy edges. We propose Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate external "clean" links and guide embeddings of a noisy target graph. We train two encoders--one on the full graph (target plus external edges) and another on a regularization graph excluding the target's potentially noisy links--then penalize discrepancies between their latent representations. This constraint steers the model away from overfitting spurious edges. Experiments on benchmark datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and validate it on a small protein-metabolite network, where metabolite-protein interactions reduce noise in protein co-occurrence data. Our results highlight LSC-GNN's potential to boost predictive performance and interpretability in settings with noisy relational structures.</li>
</ul>

<h3>Title: Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS</h3>
<ul>
<li><strong>Authors: </strong>Alex ZH Dou, Zhongwei Wan, Dongfei Cui, Xin Wang, Jing Xiong, Haokun Lin, Chaofan Tao, Shen Yan, Mi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05557">https://arxiv.org/abs/2507.05557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05557">https://arxiv.org/pdf/2507.05557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05557]] Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS(https://arxiv.org/abs/2507.05557)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Test-time scaling has emerged as a promising paradigm in language modeling, leveraging additional computational resources at inference time to enhance model performance. In this work, we introduce R2-LLMs, a novel and versatile hierarchical retrieval-augmented reasoning framework designed to improve test-time scaling in large language models (LLMs) without requiring distillation from more advanced models to obtain chain-of-thought (CoT) training data. R2-LLMs enhances inference-time generalization by integrating dual-level retrieval-based in-context learning: (1) At the coarse level, our approach extracts abstract templates from complex reasoning problems and retrieves similar problem-answer pairs to facilitate high-level in-context learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs efficiently retrieves analogous intermediate solution steps from reference mathematical problem datasets, refining step-wise reasoning with the aid of a process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical reasoning-augmentation method that enhances in-context-level reasoning while seamlessly integrating with step-level tree search methods. Utilizing PRM, it refines both candidate generation and decision-making for improved reasoning accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO datasets achieve substantial relative improvement with an increase of up to 16% using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of our approach in complex reasoning tasks.</li>
</ul>

<h3>Title: AI Agent Smart Contract Exploit Generation</h3>
<ul>
<li><strong>Authors: </strong>Arthur Gervais, Liyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05558">https://arxiv.org/abs/2507.05558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05558">https://arxiv.org/pdf/2507.05558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05558]] AI Agent Smart Contract Exploit Generation(https://arxiv.org/abs/2507.05558)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>We present A1, an agentic execution driven system that transforms any LLM into an end-to-end exploit generator. A1 has no hand-crafted heuristics and provides the agent with six domain-specific tools that enable autonomous vulnerability discovery. The agent can flexibly leverage these tools to understand smart contract behavior, generate exploit strategies, test them on blockchain states, and refine approaches based on execution feedback. All outputs are concretely validated to eliminate false positives. The evaluation across 36 real-world vulnerable contracts on Ethereum and Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional vulnerable contracts, with 5 cases occurring after the strongest model's training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59 million USD per case and 9.33 million USD total. Through 432 experiments across six LLMs, we analyze iteration-wise performance showing diminishing returns with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations 2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo analysis of 19 historical attacks shows success probabilities of 85.9%-88.8% without detection delays. We investigate whether an attacker or a defender benefits most from deploying A1 as a continuous on-chain scanning system. Our model shows that OpenAI's o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100% vulnerability incidence rates, while faster models require >=1.000% rates to break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability rates, attackers achieve an on-chain scanning profitability at a $6000 exploit value, while defenders require $60000, raising fundamental questions about whether AI agents inevitably favor exploitation over defense.</li>
</ul>

<h3>Title: ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxu Tian, Xuehui Yu, Yaoxing Wang, Pan Wang, Guangqian Guo, Shan Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05568">https://arxiv.org/abs/2507.05568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05568">https://arxiv.org/pdf/2507.05568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05568]] ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models(https://arxiv.org/abs/2507.05568)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Content-aware layout aims to arrange design elements appropriately on a given canvas to convey information effectively. Recently, the trend for this task has been to leverage large language models (LLMs) to generate layouts automatically, achieving remarkable performance. However, existing LLM-based methods fail to adequately interpret spatial relationships among visual themes and design elements, leading to structural and diverse problems in layout generation. To address this issue, we introduce ReLayout, a novel method that leverages relation-CoT to generate more reasonable and aesthetically coherent layouts by fundamentally originating from design concepts. Specifically, we enhance layout annotations by introducing explicit relation definitions, such as region, salient, and margin between elements, with the goal of decomposing the layout into smaller, structured, and recursive layouts, thereby enabling the generation of more structured layouts. Furthermore, based on these defined relationships, we introduce a layout prototype rebalance sampler, which defines layout prototype features across three dimensions and quantifies distinct layout styles. This sampler addresses uniformity issues in generation that arise from data bias in the prototype distribution balance process. Extensive experimental results verify that ReLayout outperforms baselines and can generate structural and diverse layouts that are more aligned with human aesthetics and more explainable.</li>
</ul>

<h3>Title: Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions</h3>
<ul>
<li><strong>Authors: </strong>Jun-Xiong Chong, Fang-Yu Hsu, Ming-Tsung Hsu, Yi-Ting Lin, Kai-Heng Chien, Chiou-Ting Hsu, Pei-Kai Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05575">https://arxiv.org/abs/2507.05575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05575">https://arxiv.org/pdf/2507.05575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05575]] Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions(https://arxiv.org/abs/2507.05575)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, biometric</a></li>
<li><strong>Abstract: </strong>Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by extracting discriminative liveness cues from multiple modalities, such as RGB, infrared (IR), and depth images, to enhance the robustness of biometric authentication systems. However, because data from different modalities are typically captured by various camera sensors and under diverse environmental conditions, multi-modal FAS often exhibits significantly greater distribution discrepancies across training and testing domains compared to single-modal FAS. Furthermore, during the inference stage, multi-modal FAS confronts even greater challenges when one or more modalities are unavailable or inaccessible. In this paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to tackle the challenges in the multi-modal FAS task. Our motivation stems from that, within a single modality, the visual differences between live faces are typically much smaller than those of spoof faces. Additionally, feature transitions across modalities are more consistent for the live class compared to those between live and spoof classes. Upon this insight, we first propose learning consistent cross-modal feature transitions among live samples to construct a generalized feature space. Next, we introduce learning the inconsistent cross-modal feature transitions between live and spoof samples to effectively detect out-of-distribution (OOD) attacks during inference. To further address the issue of missing modalities, we propose learning complementary infrared (IR) and depth features from the RGB modality as auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet outperforms previous two-class multi-modal FAS methods across most protocols.</li>
</ul>

<h3>Title: iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Elahi, Mohamed R. Elshamy, Abdel-Hameed Badawy, Ahmad Patooghy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05576">https://arxiv.org/abs/2507.05576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05576">https://arxiv.org/pdf/2507.05576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05576]] iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips(https://arxiv.org/abs/2507.05576)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Thermal Trojan attacks present a pressing concern for the security and reliability of System-on-Chips (SoCs), especially in mobile applications. The situation becomes more complicated when such attacks are more evasive and operate sporadically to stay hidden from detection mechanisms. In this paper, we introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips' thermal information in a random time-triggered manner. According to our experiments, iThermTroj attack can easily bypass available threshold-based thermal Trojan detection solutions. We investigate SoC vulnerabilities to variations of iThermTroj through an in-depth analysis of Trojan activation and duration scenarios. We also propose a set of tiny Machine Learning classifiers for run-time anomaly detection to protect SoCs against such intermittent thermal Trojan attacks. Compared to existing methods, our approach improves the attack detection rate by 29.4\%, 17.2\%, and 14.3\% in scenarios where iThermTroj manipulates up to 80\%, 60\%, and 40\% of SoC's thermal data, respectively. Additionally, our method increases the full protection resolution to 0.8 degrees Celsius, meaning that any temperature manipulations exceeding $\pm 0.8$ degrees will be detected with 100\% accuracy.</li>
</ul>

<h3>Title: The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Alexander Xiong, Xuandong Zhao, Aneesh Pappu, Dawn Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05578">https://arxiv.org/abs/2507.05578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05578">https://arxiv.org/pdf/2507.05578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05578]] The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation(https://arxiv.org/abs/2507.05578)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they also exhibit memorization of their training data. This phenomenon raises critical questions about model behavior, privacy risks, and the boundary between learning and memorization. Addressing these concerns, this paper synthesizes recent studies and investigates the landscape of memorization, the factors influencing it, and methods for its detection and mitigation. We explore key drivers, including training data duplication, training dynamics, and fine-tuning procedures that influence data memorization. In addition, we examine methodologies such as prefix-based extraction, membership inference, and adversarial prompting, assessing their effectiveness in detecting and measuring memorized content. Beyond technical analysis, we also explore the broader implications of memorization, including the legal and ethical implications. Finally, we discuss mitigation strategies, including data cleaning, differential privacy, and post-training unlearning, while highlighting open challenges in balancing the minimization of harmful memorization with utility. This paper provides a comprehensive overview of the current state of research on LLM memorization across technical, privacy, and performance dimensions, identifying critical directions for future work.</li>
</ul>

<h3>Title: The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction</h3>
<ul>
<li><strong>Authors: </strong>Beibei Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05584">https://arxiv.org/abs/2507.05584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05584">https://arxiv.org/pdf/2507.05584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05584]] The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction(https://arxiv.org/abs/2507.05584)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this work we propose a unified Fourier Spectral Transformer network that integrates the strengths of classical spectral methods and attention based neural architectures. By transforming the original PDEs into spectral ordinary differential equations, we use high precision numerical solvers to generate training data and use a Transformer network to model the evolution of the spectral coefficients. We demonstrate the effectiveness of our approach on the two dimensional incompressible Navier-Stokes equations and the one dimensional Burgers' equation. The results show that our spectral Transformer can achieve highly accurate long term predictions even with limited training data, better than traditional numerical methods and machine learning methods in forecasting future flow dynamics. The proposed framework generalizes well to unseen data, bringing a promising paradigm for real time prediction and control of complex dynamical systems.</li>
</ul>

<h3>Title: Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering</h3>
<ul>
<li><strong>Authors: </strong>Shuai Li, Shihan Chen, Wanru Geng, Zhaohua Xu, Xiaolu Liu, Can Dong, Zhen Tian, Changlin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05588">https://arxiv.org/abs/2507.05588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05588">https://arxiv.org/pdf/2507.05588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05588]] Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering(https://arxiv.org/abs/2507.05588)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>In the realm of industrial quality inspection, defect detection stands as a critical component, particularly in high-precision, safety-critical sectors such as automotive components aerospace, and medical devices. Traditional methods, reliant on manual inspection or early image processing algorithms, suffer from inefficiencies, high costs, and limited robustness. This paper introduces a semi-supervised defect detection framework based on conditional diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a staged joint optimization strategy. The framework utilizes labeled data for initial training and subsequently incorporates unlabeled data through the generation of pseudo-labels. A conditional diffusion model synthesizes multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise filtering mechanism mitigates label contamination. Experimental results on the NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the labeled data required by the original supervised model, showcasing significant advantages in data efficiency. This research provides a high-precision, low-labeling-dependent solution for defect detection in industrial quality inspection scenarios. The work of this article has been open-sourced at this https URL.</li>
</ul>

<h3>Title: PaddleOCR 3.0 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, Yanjun Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05595">https://arxiv.org/abs/2507.05595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05595">https://arxiv.org/pdf/2507.05595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05595]] PaddleOCR 3.0 Technical Report(https://arxiv.org/abs/2507.05595)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This technical report introduces PaddleOCR 3.0, an Apache-licensed open-source toolkit for OCR and document parsing. To address the growing demand for document understanding in the era of large language models, PaddleOCR 3.0 presents three major solutions: (1) PP-OCRv5 for multilingual text recognition, (2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for key information extraction. Compared to mainstream vision-language models (VLMs), these models with fewer than 100 million parameters achieve competitive accuracy and efficiency, rivaling billion-parameter VLMs. In addition to offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient tools for training, inference, and deployment, supports heterogeneous hardware acceleration, and enables developers to easily build intelligent document applications.</li>
</ul>

<h3>Title: Self-Review Framework for Enhancing Instruction Following Capability of LLM</h3>
<ul>
<li><strong>Authors: </strong>Sihyun Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05598">https://arxiv.org/abs/2507.05598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05598">https://arxiv.org/pdf/2507.05598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05598]] Self-Review Framework for Enhancing Instruction Following Capability of LLM(https://arxiv.org/abs/2507.05598)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Various techniques have been proposed to improve large language models (LLMs) adherence to formatting and instruction constraints. One of the most effective approaches involves utilizing high-quality data generated by powerful models. However, such models often fail to fully comply with complex instructions in a single generation. To address this limitation, iterative revision methods have been introduced. Nevertheless, as the number of data points and revision iterations increases, the associated monetary costs grow significantly. As a resource-efficient alternative, methods have been proposed that leverage high-performance evaluation tools to compensate for the limited self-evaluation capabilities of open-source LLMs. However, these approaches often lead to a degradation in output quality due to excessive revision. To overcome these challenges, we propose Re5, a self-evaluation and revision framework designed to enhance instruction-following performance while preserving the quality of the generated content. Re5 extracts task and constraint components from user instructions, performs structural evaluations to prevent error accumulation, and applies fine-grained constraint-specific content evaluations followed by selective revisions. This process ensures precise and quality-preserving improvements. The final high-quality outputs are used for alignment tuning, enabling long-term alignment improvements through a data-centric iterative refinement loop. Experimental results demonstrate that Re5 achieves instruction-following performance comparable to models trained on data generated by GPT-4o-mini, a high-performance model, even with a small amount of data while maintaining response quality with a 64.24%-win rate over the non-revised initial responses. These results validate Re5 as an efficient and effective solution for enhancing instruction adherence with minimal external supervision.</li>
</ul>

<h3>Title: Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yuyang Hu, Kangfu Mei, Mojtaba Sahraee-Ardakan, Ulugbek S. Kamilov, Peyman Milanfar, Mauricio Delbracio</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05604">https://arxiv.org/abs/2507.05604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05604">https://arxiv.org/pdf/2507.05604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05604]] Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration(https://arxiv.org/abs/2507.05604)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models show promise for image restoration, but existing methods often struggle with inconsistent fidelity and undesirable artifacts. To address this, we introduce Kernel Density Steering (KDS), a novel inference-time framework promoting robust, high-fidelity outputs through explicit local mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples, computing patch-wise kernel density estimation gradients from their collective outputs. These gradients steer patches in each particle towards shared, higher-density regions identified within the ensemble. This collective local mode-seeking mechanism, acting as "collective wisdom", steers samples away from spurious modes prone to artifacts, arising from independent sampling or model imperfections, and towards more robust, high-fidelity structures. This allows us to obtain better quality samples at the expense of higher compute by simultaneously sampling multiple particles. As a plug-and-play framework, KDS requires no retraining or external verifiers, seamlessly integrating with various diffusion samplers. Extensive numerical validations demonstrate KDS substantially improves both quantitative and qualitative performance on challenging real-world super-resolution and image inpainting tasks.</li>
</ul>

<h3>Title: Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching</h3>
<ul>
<li><strong>Authors: </strong>Mingzhe Li, Jing Xiang, Qishen Zhang, Kaiyang Wan, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05617">https://arxiv.org/abs/2507.05617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05617">https://arxiv.org/pdf/2507.05617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05617]] Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching(https://arxiv.org/abs/2507.05617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation typically involves transferring knowledge from a Large Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such as text matching, fine-tuned smaller models often yield more effective domain-specific representations, as they focus on optimizing the similarity of input pairs. To leverage both the specialized strengths of small models and the rich semantic understanding of LLMs, we introduce a flipped knowledge distillation paradigm, where LLM learns from SLM. Specifically, we address the architectural gap between decoder-only LLMs and smaller encoder-based models by reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder generates compressed representations, while the decoder maps them to the output space. During training, the encoder produces representations and their similarities, which are then aligned with the similarity scores produced by the teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach. The MCL ensures accurate similarity for both positive and negative pairs, and adaptively handles the internal differences within positive and negative samples. Our paradigm requires only a reasonably good-performing SLM, allowing the LLM to achieve improved performance. Experiments on financial and healthcare benchmarks, as well as real-world applications, confirm its effectiveness, and the model has been fully deployed in an online environment.</li>
</ul>

<h3>Title: Generative Head-Mounted Camera Captures for Photorealistic Avatars</h3>
<ul>
<li><strong>Authors: </strong>Shaojie Bai, Seunghyeon Seo, Yida Wang, Chenghui Li, Owen Wang, Te-Li Wang, Tianyang Ma, Jason Saragih, Shih-En Wei, Nojun Kwak, Hyung Jun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05620">https://arxiv.org/abs/2507.05620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05620">https://arxiv.org/pdf/2507.05620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05620]] Generative Head-Mounted Camera Captures for Photorealistic Avatars(https://arxiv.org/abs/2507.05620)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Enabling photorealistic avatar animations in virtual and augmented reality (VR/AR) has been challenging because of the difficulty of obtaining ground truth state of faces. It is physically impossible to obtain synchronized images from head-mounted cameras (HMC) sensing input, which has partial observations in infrared (IR), and an array of outside-in dome cameras, which have full observations that match avatars' appearance. Prior works relying on analysis-by-synthesis methods could generate accurate ground truth, but suffer from imperfect disentanglement between expression and style in their personalized training. The reliance of extensive paired captures (HMC and dome) for the same subject makes it operationally expensive to collect large-scale datasets, which cannot be reused for different HMC viewpoints and lighting. In this work, we propose a novel generative approach, Generative HMC (GenHMC), that leverages large unpaired HMC captures, which are much easier to collect, to directly generate high-quality synthetic HMC images given any conditioning avatar state from dome captures. We show that our method is able to properly disentangle the input conditioning signal that specifies facial expression and viewpoint, from facial appearance, leading to more accurate ground truth. Furthermore, our method can generalize to unseen identities, removing the reliance on the paired captures. We demonstrate these breakthroughs by both evaluating synthetic HMC images and universal face encoders trained from these new HMC-avatar correspondences, which achieve better data efficiency and state-of-the-art accuracy.</li>
</ul>

<h3>Title: DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</h3>
<ul>
<li><strong>Authors: </strong>Shuo Shao, Yiming Li, Mengren Zheng, Zhiyang Hu, Yukun Chen, Boheng Li, Yu He, Junfeng Guo, Tianwei Zhang, Dacheng Tao, Zhan Qin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05622">https://arxiv.org/abs/2507.05622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05622">https://arxiv.org/pdf/2507.05622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05622]] DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective(https://arxiv.org/abs/2507.05622)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>The widespread application of Deep Learning across diverse domains hinges critically on the quality and composition of training datasets. However, the common lack of disclosure regarding their usage raises significant privacy and copyright concerns. Dataset auditing techniques, which aim to determine if a specific dataset was used to train a given suspicious model, provide promising solutions to addressing these transparency gaps. While prior work has developed various auditing methods, their resilience against dedicated adversarial attacks remains largely unexplored. To bridge the gap, this paper initiates a comprehensive study evaluating dataset auditing from an adversarial perspective. We start with introducing a novel taxonomy, classifying existing methods based on their reliance on internal features (IF) (inherent to the data) versus external features (EF) (artificially introduced for auditing). Subsequently, we formulate two primary attack types: evasion attacks, designed to conceal the use of a dataset, and forgery attacks, intending to falsely implicate an unused dataset. Building on the understanding of existing methods and attack objectives, we further propose systematic attack strategies: decoupling, removal, and detection for evasion; adversarial example-based methods for forgery. These formulations and strategies lead to our new benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9 representative auditing methods. Extensive evaluations using DATABench reveal that none of the evaluated auditing methods are sufficiently robust or distinctive under adversarial settings. These findings underscore the urgent need for developing a more secure and reliable dataset auditing method capable of withstanding sophisticated adversarial manipulation. Code is available at this https URL.</li>
</ul>

<h3>Title: How Not to Detect Prompt Injections with an LLM</h3>
<ul>
<li><strong>Authors: </strong>Sarthak Choudhary, Divyam Anshumaan, Nils Palumbo, Somesh Jha</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05630">https://arxiv.org/abs/2507.05630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05630">https://arxiv.org/pdf/2507.05630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05630]] How Not to Detect Prompt Injections with an LLM(https://arxiv.org/abs/2507.05630)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>LLM-integrated applications and agents are vulnerable to prompt injection attacks, in which adversaries embed malicious instructions within seemingly benign user inputs to manipulate the LLM's intended behavior. Recent defenses based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect performance by using an LLM to classify inputs as clean or contaminated. In this work, we formally characterize the KAD framework and uncover a structural vulnerability in its design that invalidates its core security premise. We design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this fundamental weakness. It consistently evades KAD defenses with detection rates as low as $1.5\%$ while reliably inducing malicious behavior with success rates of up to $88\%$, without needing white-box access to the LLM or any optimization procedures.</li>
</ul>

<h3>Title: OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Chen, Yupeng Hu, Zixu Li, Zhiheng Fu, Xuemeng Song, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05631">https://arxiv.org/abs/2507.05631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05631">https://arxiv.org/pdf/2507.05631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05631]] OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval(https://arxiv.org/abs/2507.05631)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is capable of expressing users' intricate retrieval requirements flexibly. It enables the user to give a multimodal query, comprising a reference image and a modification text, and subsequently retrieve the target image. Notwithstanding the considerable advances made by prevailing methodologies, CIR remains in its nascent stages due to two limitations: 1) inhomogeneity between dominant and noisy portions in visual data is ignored, leading to query feature degradation, and 2) the priority of textual data in the image modification process is overlooked, which leads to a visual focus bias. To address these two limitations, this work presents a focus mapping-based feature extractor, which consists of two modules: dominant portion segmentation and dual focus mapping. It is designed to identify significant dominant portions in images and guide the extraction of visual and textual data features, thereby reducing the impact of noise interference. Subsequently, we propose a textually guided focus revision module, which can utilize the modification requirements implied in the text to perform adaptive focus revision on the reference image, thereby enhancing the perception of the modification focus on the composed features. The aforementioned modules collectively constitute the segmentatiOn-based Focus shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four benchmark datasets substantiate the superiority of our proposed method. The codes and data are available on this https URL</li>
</ul>

<h3>Title: SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression</h3>
<ul>
<li><strong>Authors: </strong>Yiqiao Jin, Kartik Sharma, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, Srijan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05633">https://arxiv.org/abs/2507.05633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05633">https://arxiv.org/pdf/2507.05633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05633]] SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression(https://arxiv.org/abs/2507.05633)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented Generation (RAG) extends large language models (LLMs) with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches reduce input size but often discard fine-grained details essential for factual accuracy. We propose SARA, a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets. SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context efficiency and answer correctness. It represents contexts at two complementary levels: 1) fine-grained natural-language spans that preserve critical entities and numerical values, and 2) compact, interpretable vectors that summarize high-level semantics. An iterative evidence-selection module employs the compression vectors for dynamic reranking of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma), SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53), demonstrating the importance of integrating textual and compressed representations for robust, context-efficient RAG.</li>
</ul>

<h3>Title: Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Feng Xia, Ciyuan Peng, Jing Ren, Falih Gozi Febrinanto, Renqiang Luo, Vidya Saikrishna, Shuo Yu, Xiangjie Kong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05636">https://arxiv.org/abs/2507.05636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05636">https://arxiv.org/pdf/2507.05636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05636]] Graph Learning(https://arxiv.org/abs/2507.05636)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning.</li>
</ul>

<h3>Title: DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning</h3>
<ul>
<li><strong>Authors: </strong>Kaixiang Zhao, Joseph Yousry Attalla, Qian Lou, Yushun Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05649">https://arxiv.org/abs/2507.05649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05649">https://arxiv.org/pdf/2507.05649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05649]] DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning(https://arxiv.org/abs/2507.05649)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph-based learning tasks. However, enabling privacy-preserving GNNs in encrypted domains, such as under Fully Homomorphic Encryption (FHE), typically incurs substantial computational overhead, rendering real-time and privacy-preserving inference impractical. In this work, we propose DESIGN (EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel framework for efficient encrypted GNN inference. DESIGN tackles the critical efficiency limitations of existing FHE GNN approaches, which often overlook input data redundancy and apply uniform computational strategies. Our framework achieves significant performance gains through a hierarchical optimization strategy executed entirely on the server: first, FHE-compatible node importance scores (based on encrypted degree statistics) are computed from the encrypted graph. These scores then guide a homomorphic partitioning process, generating multi-level importance masks directly under FHE. This dynamically generated mask facilitates both input graph pruning (by logically removing unimportant elements) and a novel adaptive polynomial activation scheme, where activation complexity is tailored to node importance levels. Empirical evaluations demonstrate that DESIGN substantially accelerates FHE GNN inference compared to state-of-the-art methods while maintaining competitive model accuracy, presenting a robust solution for secure graph analytics.</li>
</ul>

<h3>Title: TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data</h3>
<ul>
<li><strong>Authors: </strong>Aravind Cheruvu, Shravya Kanchi, Sifat Muhammad Abdullah, Nicholas Kong, Daphne Yao, Murtuza Jadliwala, Bimal Viswanath</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05660">https://arxiv.org/abs/2507.05660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05660">https://arxiv.org/pdf/2507.05660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05660]] TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data(https://arxiv.org/abs/2507.05660)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Recent advances in foundation models, such as LLMs, have revolutionized conversational AI. Chatbots are increasingly being developed by customizing LLMs on specific conversational datasets. However, mitigating toxicity during this customization, especially when dealing with untrusted training data, remains a significant challenge. To address this, we introduce TuneShield, a defense framework designed to mitigate toxicity during chatbot fine-tuning while preserving conversational quality. TuneShield leverages LLM-based toxicity classification, utilizing the instruction-following capabilities and safety alignment of LLMs to effectively identify toxic samples, outperforming industry API services. TuneShield generates synthetic conversation samples, termed 'healing data', based on the identified toxic samples, using them to mitigate toxicity while reinforcing desirable behavior during fine-tuning. It performs an alignment process to further nudge the chatbot towards producing desired responses. Our findings show that TuneShield effectively mitigates toxicity injection attacks while preserving conversational quality, even when the toxicity classifiers are imperfect or biased. TuneShield proves to be resilient against adaptive adversarial and jailbreak attacks. Additionally, TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection attacks during dialog-based learning (DBL).</li>
</ul>

<h3>Title: Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain</h3>
<ul>
<li><strong>Authors: </strong>Junfei Shi, Yu Cheng, Haiyan Jin, Junhuai Li, Zhaolin Xiao, Maoguo Gong, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05666">https://arxiv.org/abs/2507.05666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05666">https://arxiv.org/pdf/2507.05666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05666]] Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain(https://arxiv.org/abs/2507.05666)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated exceptional performance across various domains due to their ability to model and generate complicated data distributions. However, when applied to PolSAR data, traditional real-valued diffusion models face challenges in capturing complex-valued phase this http URL, these models often struggle to preserve fine structural details. To address these limitations, we leverage the Contourlet transform, which provides rich multiscale and multidirectional representations well-suited for PolSAR imagery. We propose a structural knowledge-guided complex diffusion model for PolSAR image classification in the Contourlet domain. Specifically, the complex Contourlet transform is first applied to decompose the data into low- and high-frequency subbands, enabling the extraction of statistical and boundary features. A knowledge-guided complex diffusion network is then designed to model the statistical properties of the low-frequency components. During the process, structural information from high-frequency coefficients is utilized to guide the diffusion process, improving edge preservation. Furthermore, multiscale and multidirectional high-frequency features are jointly learned to further boost classification accuracy. Experimental results on three real-world PolSAR datasets demonstrate that our approach surpasses state-of-the-art methods, particularly in preserving edge details and maintaining region homogeneity in complex terrain.</li>
</ul>

<h3>Title: Modeling and Reversing Brain Lesions Using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Omar Zamzam, Haleh Akrami, Anand Joshi, Richard Leahy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05670">https://arxiv.org/abs/2507.05670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05670">https://arxiv.org/pdf/2507.05670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05670]] Modeling and Reversing Brain Lesions Using Diffusion Models(https://arxiv.org/abs/2507.05670)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Brain lesions are abnormalities or injuries in brain tissue that are often detectable using magnetic resonance imaging (MRI), which reveals structural changes in the affected areas. This broad definition of brain lesions includes areas of the brain that are irreversibly damaged, as well as areas of brain tissue that are deformed as a result of lesion growth or swelling. Despite the importance of differentiating between damaged and deformed tissue, existing lesion segmentation methods overlook this distinction, labeling both of them as a single anomaly. In this work, we introduce a diffusion model-based framework for analyzing and reversing the brain lesion process. Our pipeline first segments abnormal regions in the brain, then estimates and reverses tissue deformations by restoring displaced tissue to its original position, isolating the core lesion area representing the initial damage. Finally, we inpaint the core lesion area to arrive at an estimation of the pre-lesion healthy brain. This proposed framework reverses a forward lesion growth process model that is well-established in biomechanical studies that model brain lesions. Our results demonstrate improved accuracy in lesion segmentation, characterization, and brain labeling compared to traditional methods, offering a robust tool for clinical and research applications in brain lesion analysis. Since pre-lesion healthy versions of abnormal brains are not available in any public dataset for validation of the reverse process, we simulate a forward model to synthesize multiple lesioned brain images.</li>
</ul>

<h3>Title: LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yisu Zhang, Chenjie Cao, Chaohui Yu, Jianke Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05678">https://arxiv.org/abs/2507.05678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05678">https://arxiv.org/pdf/2507.05678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05678]] LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion(https://arxiv.org/abs/2507.05678)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in synthesizing realistic videos by learning from large-scale data. Although vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal movement to driven VDMs with constrained data, achieving precise control over both camera trajectories and object motion remains challenging due to the unstable fusion and non-linear scalability. To address these issues, we propose LiON-LoRA, a novel framework that rethinks LoRA fusion through three core principles: Linear scalability, Orthogonality, and Norm consistency. First, we analyze the orthogonality of LoRA features in shallow VDM layers, enabling decoupled low-level controllability. Second, norm consistency is enforced across layers to stabilize fusion during complex camera motion combinations. Third, a controllable token is integrated into the diffusion transformer (DiT) to linearly adjust motion amplitudes for both cameras and objects with a modified self-attention mechanism to ensure decoupled control. Additionally, we extend LiON-LoRA to temporal generation by leveraging static-camera videos, unifying spatial and temporal controllability. Experiments demonstrate that LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy and motion strength adjustment, achieving superior generalization with minimal training data. Project Page: this https URL</li>
</ul>

<h3>Title: Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach</h3>
<ul>
<li><strong>Authors: </strong>Xiaobing Chen, Boyang Zhang, Xiangwei Zhou, Mingxuan Sun, Shuai Zhang, Songyang Zhang, Geoffrey Ye Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05685">https://arxiv.org/abs/2507.05685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05685">https://arxiv.org/pdf/2507.05685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05685]] Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach(https://arxiv.org/abs/2507.05685)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>The integration of Federated Learning (FL) and Mixture-of-Experts (MoE) presents a compelling pathway for training more powerful, large-scale artificial intelligence models (LAMs) on decentralized data while preserving privacy. However, efficient federated training of these complex MoE-structured LAMs is hindered by significant system-level challenges, particularly in managing the interplay between heterogeneous client resources and the sophisticated coordination required for numerous specialized experts. This article highlights a critical, yet underexplored concept: the absence of robust quantitative strategies for dynamic client-expert alignment that holistically considers varying client capacities and the imperative for system-wise load balancing. Specifically, we propose a conceptual system design for intelligent client-expert alignment that incorporates dynamic fitness scoring, global expert load monitoring, and client capacity profiling. By tackling these systemic issues, we can unlock more scalable, efficient, and robust training mechanisms {with fewer communication rounds for convergence}, paving the way for the widespread deployment of large-scale federated MoE-structured LAMs in edge computing with ultra-high communication efficiency.</li>
</ul>

<h3>Title: Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs</h3>
<ul>
<li><strong>Authors: </strong>SeungWon Ji, Jungyup Lee, Jemin Kim, Sang Park, SeungJae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05686">https://arxiv.org/abs/2507.05686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05686">https://arxiv.org/pdf/2507.05686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05686]] Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs(https://arxiv.org/abs/2507.05686)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual large language models (LLMs) often exhibit language confusion, a tendency to generate responses in a dominant language irrespective of the prompt's language. To address this, we propose Smoothie-Qwen, a lightweight, post-hoc method that mitigates language bias without retraining. This technique selectively adjusts token-level output probabilities to effectively suppress undesired language generation. Applied to the Qwen model, our method reduces unintended Chinese output by over 95% while preserving task accuracy on multilingual benchmarks. This work provides a practical and efficient solution for enhancing the language controllability of LLMs, making them more reliable for global applications.</li>
</ul>

<h3>Title: Agentic-R1: Distilled Dual-Strategy Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05707">https://arxiv.org/abs/2507.05707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05707">https://arxiv.org/pdf/2507.05707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05707]] Agentic-R1: Distilled Dual-Strategy Reasoning(https://arxiv.org/abs/2507.05707)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at this https URL</li>
</ul>

<h3>Title: DRAGON: Dynamic RAG Benchmark On News</h3>
<ul>
<li><strong>Authors: </strong>Fedor Chernogorskii, Sergei Averkiev, Liliya Kudraleeva, Zaven Martirosian, Maria Tikhonova, Valentin Malykh, Alena Fenogenova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05713">https://arxiv.org/abs/2507.05713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05713">https://arxiv.org/pdf/2507.05713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05713]] DRAGON: Dynamic RAG Benchmark On News(https://arxiv.org/abs/2507.05713)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is a widely adopted approach for improving the factuality of large language models (LLMs) by incorporating external knowledge at inference time. Although there exist multiple RAG benchmarks for English, evaluation resources for other languages, including Russian, remain scarce and static, failing to capture the dynamic nature of real-world deployments. In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first dynamic benchmark for evaluating RAG systems in Russian on a changing news corpora. DRAGON is built upon a regularly updated corpus of Russian news and public documents and supports comprehensive evaluation of both the retriever and generator components. Question generation is performed automatically with the use of Knowledge Graph constructed from the corpus and enables the extraction of four core question types aligned with distinct subgraph patterns. We release a complete evaluation framework comprising the pipeline for automatic question generation, evaluation scripts, which are potentially reusable for other languages and multilingual settings, and benchmark data. We also launch a public leaderboard to encourage community participation and comparison.</li>
</ul>

<h3>Title: HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>YiHan Jiao, ZheHao Tan, Dan Yang, DuoLin Sun, Jie Feng, Jian Wang, Peng Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05714">https://arxiv.org/abs/2507.05714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05714">https://arxiv.org/pdf/2507.05714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05714]] HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation(https://arxiv.org/abs/2507.05714)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domain-specific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often \textit{lack a granular focus on RAG task} or \textit{a deeper utilization of chain-of-thought processes}. To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAG-specific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chain-of-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.</li>
</ul>

<h3>Title: Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongbao Li, Ziye Jia, Sijie He, Kun Guo, Qihui Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05722">https://arxiv.org/abs/2507.05722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05722">https://arxiv.org/pdf/2507.05722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05722]] Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning(https://arxiv.org/abs/2507.05722)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the emergence of compute-intensive and delay-sensitive applications in vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising complement for vehicular edge computing due to the high mobility and flexible deployment. However, the existing UAV-assisted offloading strategies are insufficient in coordinating heterogeneous computing resources and adapting to dynamic network conditions. Hence, this paper proposes a dual-layer UAV-assisted edge computing architecture based on partial offloading, composed of the relay capability of high-altitude UAVs and the computing support of low-altitude UAVs. The proposed architecture enables efficient integration and coordination of heterogeneous resources. A joint optimization problem is formulated to minimize the system delay and energy consumption while ensuring the task completion rate. To solve the high-dimensional decision problem, we reformulate the problem as a Markov decision process and propose a hierarchical offloading scheme based on the soft actor-critic algorithm. The method decouples global and local decisions, where the global decisions integrate offloading ratios and trajectory planning into continuous actions, while the local scheduling is handled via designing a priority-based mechanism. Simulations are conducted and demonstrate that the proposed approach outperforms several baselines in task completion rate, system efficiency, and convergence speed, showing strong robustness and applicability in dynamic vehicular environments.</li>
</ul>

<h3>Title: Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05724">https://arxiv.org/abs/2507.05724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05724">https://arxiv.org/pdf/2507.05724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05724]] Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition(https://arxiv.org/abs/2507.05724)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model \emph{Omni-router Transformer}. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.</li>
</ul>

<h3>Title: Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ruofei Wang, Peiqi Duan, Boxin Shi, Renjie Wan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05728">https://arxiv.org/abs/2507.05728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05728">https://arxiv.org/pdf/2507.05728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05728]] Asynchronous Event Error-Minimizing Noise for Safeguarding Event Dataset(https://arxiv.org/abs/2507.05728)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect</a></li>
<li><strong>Abstract: </strong>With more event datasets being released online, safeguarding the event dataset against unauthorized usage has become a serious concern for data owners. Unlearnable Examples are proposed to prevent the unauthorized exploitation of image datasets. However, it's unclear how to create unlearnable asynchronous event streams to prevent event misuse. In this work, we propose the first unlearnable event stream generation method to prevent unauthorized training from event datasets. A new form of asynchronous event error-minimizing noise is proposed to perturb event streams, tricking the unauthorized model into learning embedded noise instead of realistic features. To be compatible with the sparse event, a projection strategy is presented to sparsify the noise to render our unlearnable event streams (UEvs). Extensive experiments demonstrate that our method effectively protects event data from unauthorized exploitation, while preserving their utility for legitimate use. We hope our UEvs contribute to the advancement of secure and trustworthy event dataset sharing. Code is available at: this https URL.</li>
</ul>

<h3>Title: DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Jing Yang Lee, Hamed Bonab, Nasser Zalmout, Ming Zeng, Sanket Lokegaonkar, Colin Lockard, Binxuan Huang, Ritesh Sarkhel, Haodong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05750">https://arxiv.org/abs/2507.05750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05750">https://arxiv.org/pdf/2507.05750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05750]] DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities(https://arxiv.org/abs/2507.05750)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly employed in multi-turn conversational tasks, yet their pre-training data predominantly consists of continuous prose, creating a potential mismatch between required capabilities and training paradigms. We introduce a novel approach to address this discrepancy by synthesizing conversational data from existing text corpora. We present a pipeline that transforms a cluster of multiple related documents into an extended multi-turn, multi-topic information-seeking dialogue. Applying our pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training dialogue corpus consisting of over 730k long conversations. We hypothesize that exposure to such synthesized conversational structures during pre-training can enhance the fundamental multi-turn capabilities of LLMs, such as context memory and understanding. Empirically, we show that incorporating DocTalk during pre-training results in up to 40% gain in context memory and understanding, without compromising base performance. DocTalk is available at this https URL.</li>
</ul>

<h3>Title: SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations</h3>
<ul>
<li><strong>Authors: </strong>Yegyu Han, Taegyoon Yoon, Dayeon Woo, Sojeong Kim, Hyung-Sin Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05751">https://arxiv.org/abs/2507.05751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05751">https://arxiv.org/pdf/2507.05751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05751]] SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations(https://arxiv.org/abs/2507.05751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances on 6D object-pose estimation has achieved high performance on representative benchmarks such as LM-O, YCB-V, and T-Less. However, these datasets were captured under fixed illumination and camera settings, leaving the impact of real-world variations in illumination, exposure, gain or depth-sensor mode - and the potential of test-time sensor control to mitigate such variations - largely unexplored. To bridge this gap, we introduce SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures, 9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels. For three common household objects (spray, pringles, and tincase), we acquire 101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting permutations per object pose. Experiments with state-of-the-art models on our dataset show that applying sensor control during test-time induces greater performance improvement over digital data augmentation, achieving performance comparable to or better than costly increases in real-world training data quantity and diversity. Adapting either RGB or depth sensors individually is effective, while jointly adapting multimodal RGB-D configurations yields even greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from data-centered to sensor-aware robustness, laying a foundation for adaptive, self-tuning perception systems capable of operating robustly in uncertain real-world environments. Our dataset is available at: this http URL Associated scripts can be found at: this http URL</li>
</ul>

<h3>Title: Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy</h3>
<ul>
<li><strong>Authors: </strong>Radoslaw Roszczyk, Artur Krupa, Izabella Antoniuk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05757">https://arxiv.org/abs/2507.05757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05757">https://arxiv.org/pdf/2507.05757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05757]] Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy(https://arxiv.org/abs/2507.05757)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The acquisition of accurately coloured, balanced images in an optical microscope can be a challenge even for experienced microscope operators. This article presents an entirely automatic mechanism for balancing the white level that allows the correction of the microscopic colour images adequately. The results of the algorithm have been confirmed experimentally on a set of two hundred microscopic images. The images contained scans of three microscopic specimens commonly used in pathomorphology. Also, the results achieved were compared with other commonly used white balance algorithms in digital photography. The algorithm applied in this work is more effective than the classical algorithms used in colour photography for microscopic images stained with hematoxylin-phloxine-saffron and for immunohistochemical staining images.</li>
</ul>

<h3>Title: DreamArt: Generating Interactable Articulated Objects from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Ruijie Lu, Yu Liu, Jiaxiang Tang, Junfeng Ni, Yuxiang Wang, Diwen Wan, Gang Zeng, Yixin Chen, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05763">https://arxiv.org/abs/2507.05763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05763">https://arxiv.org/pdf/2507.05763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05763]] DreamArt: Generating Interactable Articulated Objects from a Single Image(https://arxiv.org/abs/2507.05763)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at this https URL.</li>
</ul>

<h3>Title: Flippi: End To End GenAI Assistant for E-Commerce</h3>
<ul>
<li><strong>Authors: </strong>Anand A. Rajasekar, Praveen Tangarajan, Anjali Nainani, Amogh Batwal, Vinay Rao Dandin, Anusua Trivedi, Ozan Ersoy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05788">https://arxiv.org/abs/2507.05788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05788">https://arxiv.org/pdf/2507.05788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05788]] Flippi: End To End GenAI Assistant for E-Commerce(https://arxiv.org/abs/2507.05788)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The emergence of conversational assistants has fundamentally reshaped user interactions with digital platforms. This paper introduces Flippi-a cutting-edge, end-to-end conversational assistant powered by large language models (LLMs) and tailored for the e-commerce sector. Flippi addresses the challenges posed by the vast and often overwhelming product landscape, enabling customers to discover products more efficiently through natural language dialogue. By accommodating both objective and subjective user requirements, Flippi delivers a personalized shopping experience that surpasses traditional search methods. This paper details how Flippi interprets customer queries to provide precise product information, leveraging advanced NLP techniques such as Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG), Named Entity Recognition (NER), and Context Reduction. Flippi's unique capability to identify and present the most attractive offers on an e-commerce site is also explored, demonstrating how it empowers users to make cost-effective decisions. Additionally, the paper discusses Flippi's comparative analysis features, which help users make informed choices by contrasting product features, prices, and other relevant attributes. The system's robust architecture is outlined, emphasizing its adaptability for integration across various e-commerce platforms and the technological choices underpinning its performance and accuracy. Finally, a comprehensive evaluation framework is presented, covering performance metrics, user satisfaction, and the impact on customer engagement and conversion rates. By bridging the convenience of online shopping with the personalized assistance traditionally found in physical stores, Flippi sets a new standard for customer satisfaction and engagement in the digital marketplace.</li>
</ul>

<h3>Title: TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yujie Hu, Xuanyu Zhang, Weiqi Li, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05790">https://arxiv.org/abs/2507.05790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05790">https://arxiv.org/pdf/2507.05790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05790]] TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model(https://arxiv.org/abs/2507.05790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Virtual try-on has made significant progress in recent years. This paper addresses how to achieve multifunctional virtual try-on guided solely by text instructions, including full outfit change and local editing. Previous methods primarily relied on end-to-end networks to perform single try-on tasks, lacking versatility and flexibility. We propose TalkFashion, an intelligent try-on assistant that leverages the powerful comprehension capabilities of large language models to analyze user instructions and determine which task to execute, thereby activating different processing pipelines accordingly. Additionally, we introduce an instruction-based local repainting model that eliminates the need for users to manually provide masks. With the help of multi-modal models, this approach achieves fully automated local editings, enhancing the flexibility of editing tasks. The experimental results demonstrate better semantic consistency and visual quality compared to the current methods.</li>
</ul>

<h3>Title: Automated Reasoning for Vulnerability Management by Design</h3>
<ul>
<li><strong>Authors: </strong>Avi Shaked, Nan Messe</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05794">https://arxiv.org/abs/2507.05794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05794">https://arxiv.org/pdf/2507.05794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05794]] Automated Reasoning for Vulnerability Management by Design(https://arxiv.org/abs/2507.05794)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>For securing systems, it is essential to manage their vulnerability posture and design appropriate security controls. Vulnerability management allows to proactively address vulnerabilities by incorporating pertinent security controls into systems designs. Current vulnerability management approaches do not support systematic reasoning about the vulnerability postures of systems designs. To effectively manage vulnerabilities and design security controls, we propose a formally grounded automated reasoning mechanism. We integrate the mechanism into an open-source security design tool and demonstrate its application through an illustrative example driven by real-world challenges. The automated reasoning mechanism allows system designers to identify vulnerabilities that are applicable to a specific system design, explicitly specify vulnerability mitigation options, declare selected controls, and thus systematically manage vulnerability postures.</li>
</ul>

<h3>Title: SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xin Hu, Ke Qin, Guiduo Duan, Ming Li, Yuan-Fang Li, Tao He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05798">https://arxiv.org/abs/2507.05798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05798">https://arxiv.org/pdf/2507.05798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05798]] SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning(https://arxiv.org/abs/2507.05798)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Panoptic Scene Graph Generation (PSG) integrates instance segmentation with relation understanding to capture pixel-level structural relationships in complex scenes. Although recent approaches leveraging pre-trained vision-language models (VLMs) have significantly improved performance in the open-vocabulary setting, they commonly ignore the inherent limitations of VLMs in spatial relation reasoning, such as difficulty in distinguishing object relative positions, which results in suboptimal relation prediction. Motivated by the denoising diffusion model's inversion process in preserving the spatial structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork) framework -- a novel approach for open-vocabulary PSG. SPADE consists of two key steps: (1) inversion-guided calibration for the UNet adaptation, and (2) spatial-aware context reasoning. In the first step, we calibrate a general pre-trained teacher diffusion model into a PSG-specific denoising network with cross-attention maps derived during inversion through a lightweight LoRA-based fine-tuning strategy. In the second step, we develop a spatial-aware relation graph transformer that captures both local and long-range contextual information, facilitating the generation of high-quality relation queries. Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate that SPADE outperforms state-of-the-art methods in both closed- and open-set scenarios, particularly for spatial relationship prediction.</li>
</ul>

<h3>Title: DREAM: Document Reconstruction via End-to-end Autoregressive Model</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Mingming Gong, Yunfei Wu, Jianxin Dai, Antai Guo, Xinghua Jiang, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05805">https://arxiv.org/abs/2507.05805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05805">https://arxiv.org/pdf/2507.05805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05805]] DREAM: Document Reconstruction via End-to-end Autoregressive Model(https://arxiv.org/abs/2507.05805)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Document reconstruction constitutes a significant facet of document analysis and recognition, a field that has been progressively accruing interest within the scholarly community. A multitude of these researchers employ an array of document understanding models to generate predictions on distinct subtasks, subsequently integrating their results into a holistic document reconstruction format via heuristic principles. Nevertheless, these multi-stage methodologies are hindered by the phenomenon of error propagation, resulting in suboptimal performance. Furthermore, contemporary studies utilize generative models to extract the logical sequence of plain text, tables and mathematical expressions in an end-to-end process. However, this approach is deficient in preserving the information related to element layouts, which are vital for document reconstruction. To surmount these aforementioned limitations, we in this paper present an innovative autoregressive model specifically designed for document reconstruction, referred to as Document Reconstruction via End-to-end Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence of document reconstruction in a comprehensive, end-to-end process, encapsulating a broader spectrum of document element information. In addition, we establish a standardized definition of the document reconstruction task, and introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for assessing the performance of the task. Empirical results substantiate that our methodology attains unparalleled performance in the realm of document reconstruction. Furthermore, the results on a variety of subtasks, encompassing document layout analysis, text recognition, table structure recognition, formula recognition and reading order detection, indicate that our model is competitive and compatible with various tasks.</li>
</ul>

<h3>Title: Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters</h3>
<ul>
<li><strong>Authors: </strong>Marco Roschkowski</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05807">https://arxiv.org/abs/2507.05807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05807">https://arxiv.org/pdf/2507.05807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05807]] Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters(https://arxiv.org/abs/2507.05807)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we tackle two fundamental problems in few-shot domain adaptation of foundation models. First, hyperparameter tuning is often impractical due to the lack of large validation datasets. Second, model robustness under distribution shifts where test time data deviates slightly from training distributions, remains a concern. We show that by training multiple independent adapters and averaging their outputs, the new model has a higher performance and is more robust to distribution shifts compared to any individual adapter. This improvement holds even when the adapters are trained with diverse hyperparameters sampled from a wide range, resulting in varied individual performance. Consequently, our method addresses both of the problems described above. The ensemble is also significantly less sensitive to the residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble can be reparameterized to a single adapter again using a principled concatenation of the parameters, we refer to our method as Soup-Adapter. This is also the first study to explore CLIP adapter-style techniques for DINOv2 and to directly compare them with CLIP in this setting.</li>
</ul>

<h3>Title: Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sofiia Chorna, Kateryna Tarelkina, Eloïse Berthier, Gianni Franchi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05810">https://arxiv.org/abs/2507.05810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05810">https://arxiv.org/pdf/2507.05810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05810]] Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs(https://arxiv.org/abs/2507.05810)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>While concept-based interpretability methods have traditionally focused on local explanations of neural network predictions, we propose a novel framework and interactive tool that extends these methods into the domain of mechanistic interpretability. Our approach enables a global dissection of model behavior by analyzing how high-level semantic attributes (referred to as concepts) emerge, interact, and propagate through internal model components. Unlike prior work that isolates individual neurons or predictions, our framework systematically quantifies how semantic concepts are represented across layers, revealing latent circuits and information flow that underlie model decision-making. A key innovation is our visualization platform that we named BAGEL (for Bias Analysis with a Graph for global Explanation Layers), which presents these insights in a structured knowledge graph, allowing users to explore concept-class relationships, identify spurious correlations, and enhance model trustworthiness. Our framework is model-agnostic, scalable, and contributes to a deeper understanding of how deep learning models generalize (or fail to) in the presence of dataset biases. The demonstration is available at this https URL.</li>
</ul>

<h3>Title: Towards Solar Altitude Guided Scene Illumination</h3>
<ul>
<li><strong>Authors: </strong>Samed Doğan, Maximilian Hoh, Nico Leuze, Nicolas R.-Peña, Alfred Schöttl</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05812">https://arxiv.org/abs/2507.05812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05812">https://arxiv.org/pdf/2507.05812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05812]] Towards Solar Altitude Guided Scene Illumination(https://arxiv.org/abs/2507.05812)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The development of safe and robust autonomous driving functions is heavily dependent on large-scale, high-quality sensor data. However, real-word data acquisition demands intensive human labor and is strongly limited by factors such as labeling cost, driver safety protocols and diverse scenario coverage. Thus, multiple lines of work focus on the conditional generation of synthetic camera sensor data. We identify a significant gap in research regarding daytime variation, presumably caused by the scarcity of available labels. Consequently, we present the solar altitude as global conditioning variable. It is readily computable from latitude-longitude coordinates and local time, eliminating the need for extensive manual labeling. Our work is complemented by a tailored normalization approach, targeting the sensitivity of daylight towards small numeric changes in altitude. We demonstrate its ability to accurately capture lighting characteristics and illumination-dependent image noise in the context of diffusion models.</li>
</ul>

<h3>Title: Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework</h3>
<ul>
<li><strong>Authors: </strong>Wang Wang, Mingyu Shi, Jun Jiang, Wenqian Ma, Chong Liu, Yasutaka Narazaki, Xuguang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05814">https://arxiv.org/abs/2507.05814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05814">https://arxiv.org/pdf/2507.05814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05814]] Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework(https://arxiv.org/abs/2507.05814)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>As critical transportation infrastructure, bridges face escalating challenges from aging and deterioration, while traditional manual inspection methods suffer from low efficiency. Although 3D point cloud technology provides a new data-driven paradigm, its application potential is often constrained by the incompleteness of real-world data, which results from missing labels and scanning occlusions. To overcome the bottleneck of insufficient generalization in existing synthetic data methods, this paper proposes a systematic framework for generating 3D bridge data. This framework can automatically generate complete point clouds featuring component-level instance annotations, high-fidelity color, and precise normal vectors. It can be further extended to simulate the creation of diverse and physically realistic incomplete point clouds, designed to support the training of segmentation and completion networks, respectively. Experiments demonstrate that a PointNet++ model trained with our synthetic data achieves a mean Intersection over Union (mIoU) of 84.2% in real-world bridge semantic segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance on the component completion task. This research offers an innovative methodology and a foundational dataset for the 3D visual analysis of bridge structures, holding significant implications for advancing the automated management and maintenance of infrastructure.</li>
</ul>

<h3>Title: 2D Instance Editing in 3D Space</h3>
<ul>
<li><strong>Authors: </strong>Yuhuan Xie, Aoxuan Pan, Ming-Xian Lin, Wei Huang, Yi-Hua Huang, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05819">https://arxiv.org/abs/2507.05819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05819">https://arxiv.org/pdf/2507.05819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05819]] 2D Instance Editing in 3D Space(https://arxiv.org/abs/2507.05819)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models have achieved significant progress in advancing 2D image editing, demonstrating exceptional precision and realism. However, they often struggle with consistency and object identity preservation due to their inherent pixel-manipulation nature. To address this limitation, we introduce a novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D representation, enabling edits within a physically plausible, rigidity-constrained 3D environment. The edited 3D objects are then reprojected and seamlessly inpainted back into the original 2D image. In contrast to existing 2D editing methods, such as DragGAN and DragDiffusion, our method directly manipulates objects in a 3D environment. Extensive experiments highlight that our framework surpasses previous methods in general performance, delivering highly consistent edits while robustly preserving object identity.</li>
</ul>

<h3>Title: Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>L'ea Dubois, Klaus Schmidt, Chengyu Wang, Ji-Hoon Park, Lin Wang, Santiago Munoz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05822">https://arxiv.org/abs/2507.05822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05822">https://arxiv.org/pdf/2507.05822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05822]] Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models(https://arxiv.org/abs/2507.05822)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current video understanding models excel at recognizing "what" is happening but fall short in high-level cognitive tasks like causal reasoning and future prediction, a limitation rooted in their lack of commonsense world knowledge. To bridge this cognitive gap, we propose a novel framework that synergistically fuses a powerful Vision Foundation Model (VFM) for deep visual perception with a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our key technical innovation is a sophisticated fusion module, inspired by the Q-Former architecture, which distills complex spatiotemporal and object-centric visual features into a concise, language-aligned representation. This enables the LLM to effectively ground its inferential processes in direct visual evidence. The model is trained via a two-stage strategy, beginning with large-scale alignment pre-training on video-text data, followed by targeted instruction fine-tuning on a curated dataset designed to elicit advanced reasoning and prediction skills. Extensive experiments demonstrate that our model achieves state-of-the-art performance on multiple challenging benchmarks. Notably, it exhibits remarkable zero-shot generalization to unseen reasoning tasks, and our in-depth ablation studies validate the critical contribution of each architectural component. This work pushes the boundary of machine perception from simple recognition towards genuine cognitive understanding, paving the way for more intelligent and capable AI systems in robotics, human-computer interaction, and beyond.</li>
</ul>

<h3>Title: Fair Domain Generalization: An Information-Theoretic View</h3>
<ul>
<li><strong>Authors: </strong>Tangzheng Lian, Guanyu Hu, Dimitrios Kollias, Xinyu Yang, Oya Celiktutan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05823">https://arxiv.org/abs/2507.05823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05823">https://arxiv.org/pdf/2507.05823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05823]] Fair Domain Generalization: An Information-Theoretic View(https://arxiv.org/abs/2507.05823)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Domain generalization (DG) and algorithmic fairness are two critical challenges in machine learning. However, most DG methods focus only on minimizing expected risk in the unseen target domain without considering algorithmic fairness. Conversely, fairness methods typically do not account for domain shifts, so the fairness achieved during training may not generalize to unseen test domains. In this work, we bridge these gaps by studying the problem of Fair Domain Generalization (FairDG), which aims to minimize both expected risk and fairness violations in unseen target domains. We derive novel mutual information-based upper bounds for expected risk and fairness violations in multi-class classification tasks with multi-group sensitive attributes. These bounds provide key insights for algorithm design from an information-theoretic perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal Fairness for Domain Generalization), a practical framework that solves the FairDG problem and models the utility-fairness trade-off through Pareto optimization. Experiments on real-world vision and language datasets show that PAFDG achieves superior utility-fairness trade-offs compared to existing methods.</li>
</ul>

<h3>Title: I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ourui Fu, Hangzhou He, Xinliang Zhang, Lei Zhu, Shuang Zeng, ZhaoHeng Xie, Yanye Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05838">https://arxiv.org/abs/2507.05838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05838">https://arxiv.org/pdf/2507.05838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05838]] I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation(https://arxiv.org/abs/2507.05838)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The annotation bottleneck in semantic segmentation has driven significant interest in few-shot segmentation, which aims to develop segmentation models capable of generalizing rapidly to novel classes using minimal exemplars. Conventional training paradigms typically generate query prior maps by extracting masked-area features from support images, followed by making predictions guided by these prior maps. However, current approaches remain constrained by two critical limitations stemming from inter- and intra-image discrepancies, both of which significantly degrade segmentation performance: 1) The semantic gap between support and query images results in mismatched features and inaccurate prior maps; 2) Visually similar yet semantically distinct regions within support or query images lead to false negative or false positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1) Using category-specific high level representations which aggregate global semantic cues from support and query images, enabling more precise inter-image region localization and address the first limitation. 2) Directional masking strategy that suppresses inconsistent support-query pixel pairs, which exhibit high feature similarity but conflicting mask, to mitigate the second issue. Experiments demonstrate that our method outperforms state-of-the-art approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.</li>
</ul>

<h3>Title: USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining</h3>
<ul>
<li><strong>Authors: </strong>Yue Peng, Bing Xiong, Fuqiang Chen, De Eybo, RanRan Zhang, Wanming Hu, Jing Cai, Wenjian Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05843">https://arxiv.org/abs/2507.05843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05843">https://arxiv.org/pdf/2507.05843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05843]] USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining(https://arxiv.org/abs/2507.05843)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Immunohistochemical (IHC) virtual staining is a task that generates virtual IHC images from H\&E images while maintaining pathological semantic consistency with adjacent slices. This task aims to achieve cross-domain mapping between morphological structures and staining patterns through generative models, providing an efficient and cost-effective solution for pathological analysis. However, under weakly paired conditions, spatial heterogeneity between adjacent slices presents significant challenges. This can lead to inaccurate one-to-many mappings and generate results that are inconsistent with the pathological semantics of adjacent slices. To address this issue, we propose a novel unbalanced self-information feature transport for IHC virtual staining, named USIGAN, which extracts global morphological semantics without relying on positional this http URL removing weakly paired terms in the joint marginal distribution, we effectively mitigate the impact of weak pairing on joint distributions, thereby significantly improving the content consistency and pathological semantic consistency of the generated results. Moreover, we design the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation matrices between H\&E and generated IHC in image-level and real IHC and generated IHC image sets in intra-group level.. Experiments conducted on two publicly available datasets demonstrate that our method achieves superior performance across multiple clinically significant metrics, such as IoD and Pearson-R correlation, demonstrating better clinical relevance.</li>
</ul>

<h3>Title: DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction</h3>
<ul>
<li><strong>Authors: </strong>Juli Zhang, Zeyu Yan, Jing Zhang, Qiguang Miao, Quan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05849">https://arxiv.org/abs/2507.05849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05849">https://arxiv.org/pdf/2507.05849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05849]] DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction(https://arxiv.org/abs/2507.05849)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Accurate remote sensing-based crop yield prediction remains a fundamental challenging task due to complex spatial patterns, heterogeneous spectral characteristics, and dynamic agricultural conditions. Existing methods often suffer from limited spatial modeling capacity, weak generalization across crop types and years. To address these challenges, we propose DFYP, a novel Dynamic Fusion framework for crop Yield Prediction, which combines spectral channel attention, edge-adaptive spatial modeling and a learnable fusion mechanism to improve robustness across diverse agricultural scenarios. Specifically, DFYP introduces three key components: (1) a Resolution-aware Channel Attention (RCA) module that enhances spectral representation by adaptively reweighting input channels based on resolution-specific characteristics; (2) an Adaptive Operator Learning Network (AOL-Net) that dynamically selects operators for convolutional kernels to improve edge-sensitive spatial feature extraction under varying crop and temporal conditions; and (3) a dual-branch architecture with a learnable fusion mechanism, which jointly models local spatial details and global contextual information to support cross-resolution and cross-crop generalization. Extensive experiments on multi-year datasets MODIS and multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms current state-of-the-art baselines in RMSE, MAE, and R2 across different spatial resolutions, crop types, and time periods, showcasing its effectiveness and robustness for real-world agricultural monitoring.</li>
</ul>

<h3>Title: Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Samuel Ofosu Mensah, Kerol Djoumessi, Philipp Berens</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05852">https://arxiv.org/abs/2507.05852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05852">https://arxiv.org/pdf/2507.05852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05852]] Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning(https://arxiv.org/abs/2507.05852)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) provides a promising paradigm for collaboratively training machine learning models across distributed data sources while maintaining privacy. Nevertheless, real-world FL often faces major challenges including communication overhead during the transfer of large model parameters and statistical heterogeneity, arising from non-identical independent data distributions across clients. In this work, we propose an FL framework that 1) provides inherent interpretations using prototypes, and 2) tackles statistical heterogeneity by utilising lightweight adapter modules to act as compressed surrogates of local models and guide clients to achieve generalisation despite varying client distribution. Each client locally refines its model by aligning class embeddings toward prototype representations and simultaneously adjust the lightweight adapter. Our approach replaces the need to communicate entire model weights with prototypes and lightweight adapters. This design ensures that each client's model aligns with a globally shared structure while minimising communication load and providing inherent interpretations. Moreover, we conducted our experiments on a real-world retinal fundus image dataset, which provides clinical-site information. We demonstrate inherent interpretable capabilities and perform a classification task, which shows improvements in accuracy over baseline algorithms.</li>
</ul>

<h3>Title: LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential Privacy Protocols and Post-Processing Methods</h3>
<ul>
<li><strong>Authors: </strong>Berkay Kemal Balioglu, Alireza Khodaie, Mehmet Emre Gursoy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05872">https://arxiv.org/abs/2507.05872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05872">https://arxiv.org/pdf/2507.05872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05872]] LDP$^3$: An Extensible and Multi-Threaded Toolkit for Local Differential Privacy Protocols and Post-Processing Methods(https://arxiv.org/abs/2507.05872)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Local differential privacy (LDP) has become a prominent notion for privacy-preserving data collection. While numerous LDP protocols and post-processing (PP) methods have been developed, selecting an optimal combination under different privacy budgets and datasets remains a challenge. Moreover, the lack of a comprehensive and extensible LDP benchmarking toolkit raises difficulties in evaluating new protocols and PP methods. To address these concerns, this paper presents LDP$^3$ (pronounced LDP-Cube), an open-source, extensible, and multi-threaded toolkit for LDP researchers and practitioners. LDP$^3$ contains implementations of several LDP protocols, PP methods, and utility metrics in a modular and extensible design. Its modular design enables developers to conveniently integrate new protocols and PP methods. Furthermore, its multi-threaded nature enables significant reductions in execution times via parallelization. Experimental evaluations demonstrate that: (i) using LDP$^3$ to select a good protocol and post-processing method substantially improves utility compared to a bad or random choice, and (ii) the multi-threaded design of LDP$^3$ brings substantial benefits in terms of efficiency.</li>
</ul>

<h3>Title: Robust Power System State Estimation using Physics-Informed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Solon Falas, Markos Asprou, Charalambos Konstantinou, Maria K. Michael</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05874">https://arxiv.org/abs/2507.05874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05874">https://arxiv.org/pdf/2507.05874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05874]] Robust Power System State Estimation using Physics-Informed Neural Networks(https://arxiv.org/abs/2507.05874)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Modern power systems face significant challenges in state estimation and real-time monitoring, particularly regarding response speed and accuracy under faulty conditions or cyber-attacks. This paper proposes a hybrid approach using physics-informed neural networks (PINNs) to enhance the accuracy and robustness, of power system state estimation. By embedding physical laws into the neural network architecture, PINNs improve estimation accuracy for transmission grid applications under both normal and faulty conditions, while also showing potential in addressing security concerns such as data manipulation attacks. Experimental results show that the proposed approach outperforms traditional machine learning models, achieving up to 83% higher accuracy on unseen subsets of the training dataset and 65% better performance on entirely new, unrelated datasets. Experiments also show that during a data manipulation attack against a critical bus in a system, the PINN can be up to 93% more accurate than an equivalent neural network.</li>
</ul>

<h3>Title: Post-Processing in Local Differential Privacy: An Extensive Evaluation and Benchmark Platform</h3>
<ul>
<li><strong>Authors: </strong>Alireza Khodaie, Berkay Kemal Balioglu, Mehmet Emre Gursoy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05875">https://arxiv.org/abs/2507.05875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05875">https://arxiv.org/pdf/2507.05875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05875]] Post-Processing in Local Differential Privacy: An Extensive Evaluation and Benchmark Platform(https://arxiv.org/abs/2507.05875)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Local differential privacy (LDP) has recently gained prominence as a powerful paradigm for collecting and analyzing sensitive data from users' devices. However, the inherent perturbation added by LDP protocols reduces the utility of the collected data. To mitigate this issue, several post-processing (PP) methods have been developed. Yet, the comparative performance of PP methods under diverse settings remains underexplored. In this paper, we present an extensive benchmark comprising 6 popular LDP protocols, 7 PP methods, 4 utility metrics, and 6 datasets to evaluate the behaviors and optimality of PP methods under diverse conditions. Through extensive experiments, we show that while PP can substantially improve utility when the privacy budget is small (i.e., strict privacy), its benefit diminishes as the privacy budget grows. Moreover, our findings reveal that the optimal PP method depends on multiple factors, including the choice of LDP protocol, privacy budget, data characteristics (such as distribution and domain size), and the specific utility metric. To advance research in this area and assist practitioners in identifying the most suitable PP method for their setting, we introduce LDP$^3$, an open-source benchmark platform. LDP$^3$ contains all methods used in our experimental analysis, and it is designed in a modular, extensible, and multi-threaded way for future use and development.</li>
</ul>

<h3>Title: Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators</h3>
<ul>
<li><strong>Authors: </strong>Sungjib Lim, Woojung Song, Eun-Ju Lee, Yohan Jo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05890">https://arxiv.org/abs/2507.05890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05890">https://arxiv.org/pdf/2507.05890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05890]] Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators(https://arxiv.org/abs/2507.05890)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs replicate human-like behavior. We will publicly release our dataset and code to support future work.</li>
</ul>

<h3>Title: What You Have is What You Track: Adaptive and Robust Multimodal Tracking</h3>
<ul>
<li><strong>Authors: </strong>Yuedong Tan, Jiawei Shao, Eduard Zamfir, Ruanjun Li, Zhaochong An, Chao Ma, Danda Paudel, Luc Van Gool, Radu Timofte, Zongwei Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05899">https://arxiv.org/abs/2507.05899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05899">https://arxiv.org/pdf/2507.05899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05899]] What You Have is What You Track: Adaptive and Robust Multimodal Tracking(https://arxiv.org/abs/2507.05899)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal data is known to be helpful for visual tracking by improving robustness to appearance variations. However, sensor synchronization challenges often compromise data availability, particularly in video settings where shortages can be temporal. Despite its importance, this area remains underexplored. In this paper, we present the first comprehensive study on tracker performance with temporally incomplete multimodal data. Unsurprisingly, under such a circumstance, existing trackers exhibit significant performance degradation, as their rigid architectures lack the adaptability needed to effectively handle missing modalities. To address these limitations, we propose a flexible framework for robust multimodal tracking. We venture that a tracker should dynamically activate computational units based on missing data rates. This is achieved through a novel Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity, coupled with a video-level masking strategy that ensures both temporal consistency and spatial completeness which is critical for effective video tracking. Surprisingly, our model not only adapts to varying missing rates but also adjusts to scene complexity. Extensive experiments show that our model achieves SOTA performance across 9 benchmarks, excelling in both conventional complete and missing modality settings. The code and benchmark will be publicly available at this https URL.</li>
</ul>

<h3>Title: Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Li, Marco Hutter, Andreas Krause</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05906">https://arxiv.org/abs/2507.05906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05906">https://arxiv.org/pdf/2507.05906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05906]] Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why(https://arxiv.org/abs/2507.05906)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This survey provides a comparative analysis of feature-based and GAN-based approaches to learning from demonstrations, with a focus on the structure of reward functions and their implications for policy learning. Feature-based methods offer dense, interpretable rewards that excel at high-fidelity motion imitation, yet often require sophisticated representations of references and struggle with generalization in unstructured settings. GAN-based methods, in contrast, use implicit, distributional supervision that enables scalability and adaptation flexibility, but are prone to training instability and coarse reward signals. Recent advancements in both paradigms converge on the importance of structured motion representations, which enable smoother transitions, controllable synthesis, and improved task integration. We argue that the dichotomy between feature-based and GAN-based methods is increasingly nuanced: rather than one paradigm dominating the other, the choice should be guided by task-specific priorities such as fidelity, diversity, interpretability, and adaptability. This work outlines the algorithmic trade-offs and design considerations that underlie method selection, offering a framework for principled decision-making in learning from demonstrations.</li>
</ul>

<h3>Title: Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data</h3>
<ul>
<li><strong>Authors: </strong>Rui Huang, Shitong Shao, Zikai Zhou, Pukun Zhao, Hangyu Guo, Tian Ye, Lichen Bai, Shuo Yang, Zeke Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05914">https://arxiv.org/abs/2507.05914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05914">https://arxiv.org/pdf/2507.05914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05914]] Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data(https://arxiv.org/abs/2507.05914)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in various generative tasks, but training them remains highly resource-intensive, often requiring millions of images and many days of GPU computation. From a data-centric perspective addressing this limitation, we study diffusion dataset condensation as a new and challenging problem setting. The goal is to construct a "synthetic" sub-dataset with significantly fewer samples than the original dataset, enabling high-quality diffusion model training with greatly reduced cost. To the best of our knowledge, we are the first to formally investigate dataset condensation for diffusion models, whereas prior work focused on training discriminative models. To tackle this new challenge, we propose a novel Diffusion Dataset Condensation (D2C) framework, which consists of two phases: Select and Attach. The Select phase identifies a compact and diverse subset using a diffusion difficulty score and interval sampling. The Attach phase enhances the selected subset by attaching rich semantic and visual representations to strengthen the conditional signals. Extensive experiments across various dataset sizes, model architectures, and resolutions show that our D2C framework enables significantly faster diffusion model training with dramatically fewer data, while preserving high visual quality. Notably, for the SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID score of 4.3 in just 40k steps using only 0.8% of the training data.</li>
</ul>

<h3>Title: On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification</h3>
<ul>
<li><strong>Authors: </strong>Jonas Klotz, Tom Burgert, Begüm Demir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05916">https://arxiv.org/abs/2507.05916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05916">https://arxiv.org/pdf/2507.05916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05916]] On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification(https://arxiv.org/abs/2507.05916)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The development of explainable artificial intelligence (xAI) methods for scene classification problems has attracted great attention in remote sensing (RS). Most xAI methods and the related evaluation metrics in RS are initially developed for natural images considered in computer vision (CV), and their direct usage in RS may not be suitable. To address this issue, in this paper, we investigate the effectiveness of explanation methods and metrics in the context of RS image scene classification. In detail, we methodologically and experimentally analyze ten explanation metrics spanning five categories (faithfulness, robustness, localization, complexity, randomization), applied to five established feature attribution methods (Occlusion, LIME, GradCAM, LRP, and DeepLIFT) across three RS datasets. Our methodological analysis identifies key limitations in both explanation methods and metrics. The performance of perturbation-based methods, such as Occlusion and LIME, heavily depends on perturbation baselines and spatial characteristics of RS scenes. Gradient-based approaches like GradCAM struggle when multiple labels are present in the same image, while some relevance propagation methods (LRP) can distribute relevance disproportionately relative to the spatial extent of classes. Analogously, we find limitations in evaluation metrics. Faithfulness metrics share the same problems as perturbation-based methods. Localization metrics and complexity metrics are unreliable for classes with a large spatial extent. In contrast, robustness metrics and randomization metrics consistently exhibit greater stability. Our experimental results support these methodological findings. Based on our analysis, we provide guidelines for selecting explanation methods, metrics, and hyperparameters in the context of RS image scene classification.</li>
</ul>

<h3>Title: Few-shot text-based emotion detection</h3>
<ul>
<li><strong>Authors: </strong>Teodor-George Marchitan, Claudiu Creanga, Liviu P. Dinu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05918">https://arxiv.org/abs/2507.05918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05918">https://arxiv.org/pdf/2507.05918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05918]] Few-shot text-based emotion detection(https://arxiv.org/abs/2507.05918)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper describes the approach of the Unibuc - NLP team in tackling the SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion Detection. We mainly focused on experiments using large language models (Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With our final system, for the multi-label emotion detection track (track A), we got an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36 teams) for the Portuguese (Mozambican) subset and $0.325$ (\textbf{1}/31 teams) for the Emakhuwa subset.</li>
</ul>

<h3>Title: High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Huang, Yuhao Dong, Weiwei Tian, Bo Li, Rui Feng, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05920">https://arxiv.org/abs/2507.05920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05920">https://arxiv.org/pdf/2507.05920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05920]] High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning(https://arxiv.org/abs/2507.05920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>State-of-the-art large multi-modal models (LMMs) face challenges when processing high-resolution images, as these inputs are converted into enormous visual tokens, many of which are irrelevant to the downstream task. In this paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an end-to-end reinforcement learning (RL) framework that enables LMMs to iteratively focus on key visual regions by automatically cropping sub-images, based on model-predicted grounding coordinates within a multi-turn conversation framework. Compared to supervised fine-tuning (SFT), which requires costly additional grounding annotations, our approach highlights that LMMs can emerge robust grounding abilities during the RL training process, leveraging only a binary reward function derived from the correctness of the final answer. Additionally, we observe that LMMs struggle to autonomously trigger visual grounding during the rollout process. To address this cold start problem, we design a multi-turn conversational template and restrict policy loss computation to model outputs generated across multiple dialogue rounds, thereby promoting stable optimization. Extensive experiments demonstrate that, when trained on standard visual-question-short answering data without grounding annotations, MGPO effectively elicits stronger grounding capabilities compared to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and 5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench. Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at this https URL.</li>
</ul>

<h3>Title: Towards a Principled Evaluation of Knowledge Editors</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Pohl, Max Ploner, Alan Akbik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05937">https://arxiv.org/abs/2507.05937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05937">https://arxiv.org/pdf/2507.05937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05937]] Towards a Principled Evaluation of Knowledge Editors(https://arxiv.org/abs/2507.05937)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Model editing has been gaining increasing attention over the past few years. For Knowledge Editing in particular, more challenging evaluation datasets have recently been released. These datasets use different methodologies to score the success of editors. Yet, it remains under-explored how robust these methodologies are and whether they unfairly favor some editors. Moreover, the disruptive impact of these editors on overall model capabilities remains a constant blind spot. We address both of these problems and show that choosing different metrics and evaluation methodologies as well as different edit batch sizes can lead to a different ranking of knowledge editors. Crucially we demonstrate this effect also on general language understanding tasks evaluated alongside the knowledge editing tasks. Further we include a manual assessment of the string matching based evaluation method for knowledge editing that is favored by recently released datasets, revealing a tendency to produce false positive matches.</li>
</ul>

<h3>Title: Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Quanzhu Niu, Yikang Zhou, Shihao Chen, Tao Zhang, Shunping Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05948">https://arxiv.org/abs/2507.05948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05948">https://arxiv.org/pdf/2507.05948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05948]] Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation(https://arxiv.org/abs/2507.05948)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Video Instance Segmentation (VIS) fundamentally struggles with pervasive challenges including object occlusions, motion blur, and appearance variations during temporal association. To overcome these limitations, this work introduces geometric awareness to enhance VIS robustness by strategically leveraging monocular depth estimation. We systematically investigate three distinct integration paradigms. Expanding Depth Channel (EDC) method concatenates the depth map as input channel to segmentation networks; Sharing ViT (SV) designs a uniform ViT backbone, shared between depth estimation and segmentation branches; Depth Supervision (DS) makes use of depth prediction as an auxiliary training guide for feature learning. Though DS exhibits limited effectiveness, benchmark evaluations demonstrate that EDC and SV significantly enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets 56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work conclusively establishes depth cues as critical enablers for robust video understanding.</li>
</ul>

<h3>Title: Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling</h3>
<ul>
<li><strong>Authors: </strong>Pinar Bisgin, Tom Strube, Niklas Tschorn, Michael Pantförder, Maximilian Fecke, Ingrid Ljungvall, Jens Häggström, Gerhard Wess, Christoph Schummer, Sven Meister, Falk M. Howar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05950">https://arxiv.org/abs/2507.05950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05950">https://arxiv.org/pdf/2507.05950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05950]] Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling(https://arxiv.org/abs/2507.05950)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Noisy labels pose significant challenges for AI model training in veterinary medicine. This study examines expert assessment ambiguity in canine auscultation data, highlights the negative impact of label noise on classification performance, and introduces methods for label noise reduction. To evaluate whether label noise can be minimized by incorporating multiple expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated regarding the intensity of holosystolic heart murmurs caused by Myxomatous Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of 70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging individual heart cycles, the training data was expanded and classification robustness was enhanced. The investigation encompassed training and evaluating three classification algorithms: AdaBoost, XGBoost, and Random Forest. While AdaBoost and Random Forest exhibited reasonable performances, XGBoost demonstrated notable improvements in classification accuracy. All algorithms showed significant improvements in classification accuracy due to the applied label noise reduction, most notably XGBoost. Specifically, for the detection of mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23% to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling category, sensitivity and specificity increased from 58.28% to 95.09% and from 84.84% to 89.69%, respectively. These results highlight the importance of minimizing label noise to improve classification algorithms for the detection of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart sound classification, label noise reduction, machine learning, XGBoost, veterinary cardiology, MMVD.</li>
</ul>

<h3>Title: Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhenghao Zhang, Junchao Liao, Xiangyu Meng, Long Qin, Weizhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05963">https://arxiv.org/abs/2507.05963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05963">https://arxiv.org/pdf/2507.05963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05963]] Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation(https://arxiv.org/abs/2507.05963)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion transformer models for motion-guided video generation, such as Tora, have shown significant progress. In this paper, we present Tora2, an enhanced version of Tora, which introduces several design improvements to expand its capabilities in both appearance and motion customization. Specifically, we introduce a decoupled personalization extractor that generates comprehensive personalization embeddings for multiple open-set entities, better preserving fine-grained visual details compared to previous methods. Building on this, we design a gated self-attention mechanism to integrate trajectory, textual description, and visual information for each entity. This innovation significantly reduces misalignment in multimodal conditioning during training. Moreover, we introduce a contrastive loss that jointly optimizes trajectory dynamics and entity consistency through explicit mapping between motion and personalization embeddings. Tora2 is, to our best knowledge, the first method to achieve simultaneous multi-entity customization of appearance and motion for video generation. Experimental results demonstrate that Tora2 achieves competitive performance with state-of-the-art customization methods while providing advanced motion control capabilities, which marks a critical advancement in multi-condition video generation. Project page: this https URL .</li>
</ul>

<h3>Title: T-LoRA: Single Image Diffusion Model Customization Without Overfitting</h3>
<ul>
<li><strong>Authors: </strong>Vera Soboleva, Aibek Alanov, Andrey Kuznetsov, Konstantin Sobolev</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05964">https://arxiv.org/abs/2507.05964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05964">https://arxiv.org/pdf/2507.05964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05964]] T-LoRA: Single Image Diffusion Model Customization Without Overfitting(https://arxiv.org/abs/2507.05964)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at this https URL.</li>
</ul>

<h3>Title: OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Lucas Fonseca Lage, Simon Ostermann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05965">https://arxiv.org/abs/2507.05965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05965">https://arxiv.org/pdf/2507.05965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05965]] OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation(https://arxiv.org/abs/2507.05965)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce OpenFActScore, an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models (LLMs). FActScore evaluates the factual accuracy of long-form text by using Atomic Fact Generation (AFG) to extract individual factual claims and Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge source. While the original FActScore relies on closed-source and commercial models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. We provide a detailed technical overview of our implementation, highlighting design choices and modifications made to support open models. We evaluate multiple open-source LLMs on both AFG and AFV using the original FActScore benchmark, reporting BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments. OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation, and is available at: this https URL.</li>
</ul>

<h3>Title: Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Haiwen Li, Delong Liu, Zhaohui Hou, Zhicheng Zhao, Fei Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05970">https://arxiv.org/abs/2507.05970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05970">https://arxiv.org/pdf/2507.05970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05970]] Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval(https://arxiv.org/abs/2507.05970)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>As a challenging vision-language (VL) task, Composed Image Retrieval (CIR) aims to retrieve target images using multimodal (image+text) queries. Although many existing CIR methods have attained promising performance, their reliance on costly, manually labeled triplets hinders scalability and zero-shot capability. To address this issue, we propose a scalable pipeline for automatic triplet generation, along with a fully synthetic dataset named Composed Image Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a large language model (LLM) to generate diverse prompts, controlling a text-to-image generative model to produce image pairs with identical elements in each pair, which are then filtered and reorganized to form the CIRHS dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a novel CIR framework, which can accomplish global alignment and local reasoning within a broader context, enabling the model to learn more robust and informative representations. By utilizing the synthetic CIRHS dataset, CoAlign achieves outstanding zero-shot performance on three commonly used benchmarks, demonstrating for the first time the feasibility of training CIR models on a fully synthetic dataset. Furthermore, under supervised training, our method outperforms all the state-of-the-art supervised CIR approaches, validating the effectiveness of our proposed retrieval framework. The code and the CIRHS dataset will be released soon.</li>
</ul>

<h3>Title: RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Chua, Leanne Tan, Ziyu Ge, Roy Ka-Wei Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05980">https://arxiv.org/abs/2507.05980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05980">https://arxiv.org/pdf/2507.05980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05980]] RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages(https://arxiv.org/abs/2507.05980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks. This paper introduces RabakBench, a new multilingual safety benchmark localized to Singapore's unique linguistic context, covering Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages. The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation. RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available.</li>
</ul>

<h3>Title: Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation</h3>
<ul>
<li><strong>Authors: </strong>Haroon Wahab, Hassan Ugail, Lujain Jaleel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05996">https://arxiv.org/abs/2507.05996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05996">https://arxiv.org/pdf/2507.05996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05996]] Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation(https://arxiv.org/abs/2507.05996)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning-based Deepfake detection models have achieved impressive results on benchmark datasets, yet their performance often deteriorates significantly when evaluated on out-of-distribution data. In this work, we investigate an ensemble-based approach for improving the generalization of deepfake detection systems across diverse datasets. Building on a recent open-source benchmark, we combine prediction probabilities from several state-of-the-art asymmetric models proposed at top venues. Our experiments span two distinct out-of-domain datasets and demonstrate that no single model consistently outperforms others across settings. In contrast, ensemble-based predictions provide more stable and reliable performance in all scenarios. Our results suggest that asymmetric ensembling offers a robust and scalable solution for real-world deepfake detection where prior knowledge of forgery type or quality is often unavailable.</li>
</ul>

<h3>Title: DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Popovič, Ashish Kangen, Tim Schopf, Michael Färber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05997">https://arxiv.org/abs/2507.05997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05997">https://arxiv.org/pdf/2507.05997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05997]] DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations(https://arxiv.org/abs/2507.05997)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large, high-quality annotated corpora remain scarce in document-level entity and relation extraction in zero-shot or few-shot settings. In this paper, we present a fully automatic, LLM-based pipeline for synthetic data generation and in-context learning for document-level entity and relation extraction. In contrast to existing approaches that rely on manually annotated demonstrations or direct zero-shot inference, our method combines synthetic data generation with retrieval-based in-context learning, using a reasoning-optimized language model. This allows us to build a high-quality demonstration database without manual annotation and to dynamically retrieve relevant examples at inference time. Based on our approach we produce a synthetic dataset of over $5k$ Wikipedia abstracts with approximately $59k$ entities and $30k$ relation triples. Finally, we evaluate in-context learning performance on the DocIE shared task, extracting entities and relations from long documents in a zero-shot setting. We find that in-context joint entity and relation extraction at document-level remains a challenging task, even for state-of-the-art large language models.</li>
</ul>

<h3>Title: Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Wang, Muhammad Ibrahim, Atif Mansoor, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.05999">https://arxiv.org/abs/2507.05999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.05999">https://arxiv.org/pdf/2507.05999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.05999]] Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS(https://arxiv.org/abs/2507.05999)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate geo-registration of LiDAR point clouds presents significant challenges in GNSS signal denied urban areas with high-rise buildings and bridges. Existing methods typically rely on real-time GNSS and IMU data, that require pre-calibration and assume stable positioning during data collection. However, this assumption often fails in dense urban areas, resulting in localization errors. To address this, we propose a structured geo-registration and spatial correction method that aligns 3D point clouds with satellite images, enabling frame-wise recovery of GNSS information and reconstruction of city scale 3D maps without relying on prior localization. The proposed approach employs a pre-trained Point Transformer model to segment the road points and then extracts the road skeleton and intersection points from the point cloud as well as the target map for alignment. Global rigid alignment of the two is performed using the intersection points, followed by local refinement using radial basis function (RBF) interpolation. Elevation correction is then applied to the point cloud based on terrain information from SRTM dataset to resolve vertical discrepancies. The proposed method was tested on the popular KITTI benchmark and a locally collected Perth (Western Australia) CBD dataset. On the KITTI dataset, our method achieved an average planimetric alignment standard deviation (STD) of 0.84~m across sequences with intersections, representing a 55.3\% improvement over the original dataset. On the Perth dataset, which lacks GNSS information, our method achieved an average STD of 0.96~m compared to the GPS data extracted from Google Maps API. This corresponds to a 77.4\% improvement from the initial alignment. Our method also resulted in elevation correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth dataset.</li>
</ul>

<h3>Title: The Impact of Event Data Partitioning on Privacy-aware Process Discovery</h3>
<ul>
<li><strong>Authors: </strong>Jungeun Lim, Stephan A. Fahrenkrog-Petersen, Xixi Lu, Jan Mendling, Minseok Song</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06008">https://arxiv.org/abs/2507.06008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06008">https://arxiv.org/pdf/2507.06008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06008]] The Impact of Event Data Partitioning on Privacy-aware Process Discovery(https://arxiv.org/abs/2507.06008)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Information systems support the execution of business processes. The event logs of these executions generally contain sensitive information about customers, patients, and employees. The corresponding privacy challenges can be addressed by anonymizing the event logs while still retaining utility for process discovery. However, trading off utility and privacy is difficult: the higher the complexity of event log, the higher the loss of utility by anonymization. In this work, we propose a pipeline that combines anonymization and event data partitioning, where event abstraction is utilized for partitioning. By leveraging event abstraction, event logs can be segmented into multiple parts, allowing each sub-log to be anonymized separately. This pipeline preserves privacy while mitigating the loss of utility. To validate our approach, we study the impact of event partitioning on two anonymization techniques using three real-world event logs and two process discovery techniques. Our results demonstrate that event partitioning can bring improvements in process discovery utility for directly-follows-based anonymization techniques.</li>
</ul>

<h3>Title: KnowIt: Deep Time Series Modeling and Interpretation</h3>
<ul>
<li><strong>Authors: </strong>M.W. Theunissen, R. Rabe, M.H. Davel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06009">https://arxiv.org/abs/2507.06009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06009">https://arxiv.org/pdf/2507.06009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06009]] KnowIt: Deep Time Series Modeling and Interpretation(https://arxiv.org/abs/2507.06009)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>KnowIt (Knowledge discovery in time series data) is a flexible framework for building deep time series models and interpreting them. It is implemented as a Python toolkit, with source code and documentation available from this https URL. It imposes minimal assumptions about task specifications and decouples the definition of dataset, deep neural network architecture, and interpretability technique through well defined interfaces. This ensures the ease of importing new datasets, custom architectures, and the definition of different interpretability paradigms while maintaining on-the-fly modeling and interpretation of different aspects of a user's own time series data. KnowIt aims to provide an environment where users can perform knowledge discovery on their own complex time series data through building powerful deep learning models and explaining their behavior. With ongoing development, collaboration and application our goal is to make this a platform to progress this underexplored field and produce a trusted tool for deep time series modeling.</li>
</ul>

<h3>Title: Kamae: Bridging Spark and Keras for Seamless ML Preprocessing</h3>
<ul>
<li><strong>Authors: </strong>George Barrowclough, Marian Andrecki, James Shinner, Daniele Donghi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06021">https://arxiv.org/abs/2507.06021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06021">https://arxiv.org/pdf/2507.06021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06021]] Kamae: Bridging Spark and Keras for Seamless ML Preprocessing(https://arxiv.org/abs/2507.06021)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In production recommender systems, feature preprocessing must be faithfully replicated across training and inference environments. This often requires duplicating logic between offline and online environments, increasing engineering effort and introducing risks of dataset shift. We present Kamae, an open-source Python library that bridges this gap by translating PySpark preprocessing pipelines into equivalent Keras models. Kamae provides a suite of configurable Spark transformers and estimators, each mapped to a corresponding Keras layer, enabling consistent, end-to-end preprocessing across the ML lifecycle. Framework's utility is illustrated on real-world use cases, including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code is available at this https URL.</li>
</ul>

<h3>Title: TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision</h3>
<ul>
<li><strong>Authors: </strong>Syeda Anshrah Gillani, Mirza Samad Ahmed Baig, Osama Ahmed Khan, Shahid Munir Shah, Umema Mujeeb, Maheen Ali</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06033">https://arxiv.org/abs/2507.06033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06033">https://arxiv.org/pdf/2507.06033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06033]] TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision(https://arxiv.org/abs/2507.06033)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The modern text-to-image diffusion models boom has opened a new era in digital content production as it has proven the previously unseen ability to produce photorealistic and stylistically diverse imagery based on the semantics of natural-language descriptions. However, the consistent disadvantage of these models is that they cannot generate readable, meaningful, and correctly spelled text in generated images, which significantly limits the use of practical purposes like advertising, learning, and creative design. This paper introduces a new framework, namely Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA), using which a typical diffusion backbone is extended by three well-designed modules. To begin with, the model has a dual-stream text encoder that encodes both semantic contextual information and explicit glyph representations, resulting in a character-aware representation of the input text that is rich in nature. Second, an attention mechanism that is aware of the character is proposed with a new attention segregation loss that aims to limit the attention distribution of each character independently in order to avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning phase, where a full text perceptual loss, directly optimises models to be legible and accurately spell. Large scale experiments to benchmark datasets, such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new state-of-the-art on all metrics, with better character based metrics on text rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality on high-fidelity (FID: 14.3).</li>
</ul>

<h3>Title: Enter, Exit, Page Fault, Leak: Testing Isolation Boundaries for Microarchitectural Leaks</h3>
<ul>
<li><strong>Authors: </strong>Oleksii Oleksenko, Flavien Solt, Cédric Fournet, Jana Hofmann, Boris Köpf, Stavros Volos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06039">https://arxiv.org/abs/2507.06039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06039">https://arxiv.org/pdf/2507.06039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06039]] Enter, Exit, Page Fault, Leak: Testing Isolation Boundaries for Microarchitectural Leaks(https://arxiv.org/abs/2507.06039)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>CPUs provide isolation mechanisms like virtualization and privilege levels to protect software. Yet these focus on architectural isolation while typically overlooking microarchitectural side channels, exemplified by Meltdown and Foreshadow. Software must therefore supplement architectural defenses with ad-hoc microarchitectural patches, which are constantly evolving as new attacks emerge and defenses are proposed. Such reactive approach makes ensuring complete isolation a daunting task, and leaves room for errors and oversights. We address this problem by developing a tool that stress tests microarchitectural isolation between security domains such as virtual machines, kernel, and processes, with the goal of detecting flaws in the isolation boundaries. The tool extends model-based relational testing (MRT) methodology to enable detection of cross-domain information leakage. We design a new test case generator and execution sandbox to handle multi-domain execution, new leakage models to encode expected leaks, and new analysis techniques to manage nondeterminism. We use this tool to perform an in-depth testing campaign on six x86-64 CPUs for leakage across different isolation boundaries. The testing campaign exposed four new leaks and corroborated numerous known ones, with only two false positives throughout the entire campaign. These results show critical gaps in current isolation mechanisms as well as validate a robust methodology for detecting microarchitectural flaws. As such, this approach enables a shift from reactive patching to proactive security validation in processor design.</li>
</ul>

<h3>Title: CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations</h3>
<ul>
<li><strong>Authors: </strong>Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06043">https://arxiv.org/abs/2507.06043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06043">https://arxiv.org/pdf/2507.06043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06043]] CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations(https://arxiv.org/abs/2507.06043)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Security alignment enables the Large Language Model (LLM) to gain the protection against malicious queries, but various jailbreak attack methods reveal the vulnerability of this security mechanism. Previous studies have isolated LLM jailbreak attacks and defenses. We analyze the security protection mechanism of the LLM, and propose a framework that combines attack and defense. Our method is based on the linearly separable property of LLM intermediate layer embedding, as well as the essence of jailbreak attack, which aims to embed harmful problems and transfer them to the safe area. We utilize generative adversarial network (GAN) to learn the security judgment boundary inside the LLM to achieve efficient jailbreak attack and defense. The experimental results indicate that our method achieves an average jailbreak success rate of 88.85\% across three popular LLMs, while the defense success rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%. This not only validates the effectiveness of our approach but also sheds light on the internal security mechanisms of LLMs, offering new insights for enhancing model security The code and data are available at this https URL.</li>
</ul>

<h3>Title: Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yizhan Huang, Zhe Yang, Meifang Chen, Jianping Zhang, Michael R. Lyu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06056">https://arxiv.org/abs/2507.06056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06056">https://arxiv.org/pdf/2507.06056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06056]] Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs(https://arxiv.org/abs/2507.06056)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or "gibberish", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI).</li>
</ul>

<h3>Title: Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport</h3>
<ul>
<li><strong>Authors: </strong>Julia Pelzer, Corné Verburg, Alexander Heinlein, Miriam Schulte</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06062">https://arxiv.org/abs/2507.06062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06062">https://arxiv.org/pdf/2507.06062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06062]] Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport(https://arxiv.org/abs/2507.06062)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Machine learning methods often struggle with real-world applications in science and engineering due to limited or low-quality training data. In this work, the example of groundwater flow with heat transport is considered; this corresponds to an advection-diffusion process under heterogeneous flow conditions, that is, spatially distributed material parameters and heat sources. Classical numerical simulations are costly and challenging due to high spatio-temporal resolution requirements and large domains. While often computationally more efficient, purely data-driven surrogate models face difficulties, particularly in predicting the advection process, which is highly sensitive to input variations and involves long-range spatial interactions. Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN) approach is introduced. It combines a lightweight numerical surrogate for the transport process (global) with convolutional neural networks for the groundwater velocity and heat diffusion processes (local). With the LGCNN, a city-wide subsurface temperature field is modeled, involving a heterogeneous groundwater flow field and one hundred groundwater heat pump injection points forming interacting heat plumes over long distances. The model is first systematically analyzed based on random subsurface input fields. Then, the model is trained on a handful of cut-outs from a real-world subsurface map of the Munich region in Germany, and it scales to larger cut-outs without retraining. All datasets, our code, and trained models are published for reproducibility.</li>
</ul>

<h3>Title: MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Tongtong Cheng, Rongzhen Li, Yixin Xiong, Tao Zhang, Jing Wang, Kai Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06072">https://arxiv.org/abs/2507.06072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06072">https://arxiv.org/pdf/2507.06072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06072]] MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding(https://arxiv.org/abs/2507.06072)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate driving behavior recognition and reasoning are critical for autonomous driving video understanding. However, existing methods often tend to dig out the shallow causal, fail to address spurious correlations across modalities, and ignore the ego-vehicle level causality modeling. To overcome these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM) that constructs latent causal structures between visual and language modalities. Firstly, we design a multi-level feature extractor to capture long-range dependencies. Secondly, we design a causal analysis module that dynamically models driving scenarios using a directed acyclic graph (DAG) of driving states. Thirdly, we utilize a vision-language transformer to align critical visual features with their corresponding linguistic expressions. Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM achieves SOTA performance in visual-language causal relationship learning. Furthermore, the model exhibits superior capability in capturing causal characteristics within video sequences, showcasing its effectiveness for autonomous driving applications. The code is available at this https URL.</li>
</ul>

<h3>Title: ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chihan Huang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06078">https://arxiv.org/abs/2507.06078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06078">https://arxiv.org/pdf/2507.06078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06078]] ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models(https://arxiv.org/abs/2507.06078)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures.</li>
</ul>

<h3>Title: QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Siegel, Ming-Jay Yang, Younes Bouhadjar, Maxime Fabre, Emre Neftci, John Paul Strachan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06079">https://arxiv.org/abs/2507.06079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06079">https://arxiv.org/pdf/2507.06079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06079]] QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models(https://arxiv.org/abs/2507.06079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Structured State Space models (SSM) have recently emerged as a new class of deep learning models, particularly well-suited for processing long sequences. Their constant memory footprint, in contrast to the linearly scaling memory demands of Transformers, makes them attractive candidates for deployment on resource-constrained edge-computing devices. While recent works have explored the effect of quantization-aware training (QAT) on SSMs, they typically do not address its implications for specialized edge hardware, for example, analog in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can significantly reduce the complexity of SSMs by up to two orders of magnitude across various performance metrics. We analyze the relation between model size and numerical precision, and show that QAT enhances robustness to analog noise and enables structural pruning. Finally, we integrate these techniques to deploy SSMs on a memristive analog in-memory computing substrate and highlight the resulting benefits in terms of computational efficiency.</li>
</ul>

<h3>Title: A Survey on Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zongqian Li, Yixuan Su, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06085">https://arxiv.org/abs/2507.06085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06085">https://arxiv.org/pdf/2507.06085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06085]] A Survey on Prompt Tuning(https://arxiv.org/abs/2507.06085)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This survey reviews prompt tuning, a parameter-efficient approach for adapting language models by prepending trainable continuous vectors while keeping the model frozen. We classify existing approaches into two categories: direct prompt learning and transfer learning. Direct prompt learning methods include: general optimization approaches, encoder-based methods, decomposition strategies, and mixture-of-experts frameworks. Transfer learning methods consist of: general transfer approaches, encoder-based methods, and decomposition strategies. For each method, we analyze method designs, innovations, insights, advantages, and disadvantages, with illustrative visualizations comparing different frameworks. We identify challenges in computational efficiency and training stability, and discuss future directions in improving training robustness and broadening application scope.</li>
</ul>

<h3>Title: Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Shravya Kanchi, Neal Mangaokar, Aravind Cheruvu, Sifat Muhammad Abdullah, Shirin Nilizadeh, Atul Prakash, Bimal Viswanath</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06092">https://arxiv.org/abs/2507.06092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06092">https://arxiv.org/pdf/2507.06092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06092]] Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI(https://arxiv.org/abs/2507.06092)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative</a></li>
<li><strong>Abstract: </strong>Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks.</li>
</ul>

<h3>Title: Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification</h3>
<ul>
<li><strong>Authors: </strong>Murilo Gustineli, Anthony Miyaguchi, Adrian Cheung, Divyansh Khattak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06093">https://arxiv.org/abs/2507.06093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06093">https://arxiv.org/pdf/2507.06093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06093]] Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification(https://arxiv.org/abs/2507.06093)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on multi-species plant identification in vegetation quadrat images. Our pipeline combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's 518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP + K-Means visual clustering and geolocation filtering. Tile predictions are aggregated by majority vote and re-weighted with cluster-specific Bayesian priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while requiring no additional training. All code, configuration files, and reproducibility scripts are publicly available at this https URL.</li>
</ul>

<h3>Title: Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Mohamad H. Danesh, Maxime Wabartha, Stanley Wu, Joelle Pineau, Hsiu-Chin Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06111">https://arxiv.org/abs/2507.06111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06111">https://arxiv.org/pdf/2507.06111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06111]] Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation(https://arxiv.org/abs/2507.06111)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deploying reinforcement learning (RL) policies in real-world involves significant challenges, including distribution shifts, safety concerns, and the impracticality of direct interactions during policy refinement. Existing methods, such as domain randomization (DR) and off-dynamics RL, enhance policy robustness by direct interaction with the target domain, an inherently unsafe practice. We propose Uncertainty-Aware RL (UARL), a novel framework that prioritizes safety during training by addressing Out-Of-Distribution (OOD) detection and policy adaptation without requiring direct interactions in target domain. UARL employs an ensemble of critics to quantify policy uncertainty and incorporates progressive environmental randomization to prepare the policy for diverse real-world conditions. By iteratively refining over high-uncertainty regions of the state space in simulated environments, UARL enhances robust generalization to the target domain without explicitly training on it. We evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its effectiveness in reliable OOD detection, improved performance, and enhanced sample efficiency compared to baselines.</li>
</ul>

<h3>Title: Fun with flags: How Compilers Break and Fix Constant-Time Code</h3>
<ul>
<li><strong>Authors: </strong>Antoine Geimer, Clementine Maurice</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06112">https://arxiv.org/abs/2507.06112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06112">https://arxiv.org/pdf/2507.06112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06112]] Fun with flags: How Compilers Break and Fix Constant-Time Code(https://arxiv.org/abs/2507.06112)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Developers rely on constant-time programming to prevent timing side-channel attacks. But these efforts can be undone by compilers, whose optimizations may silently reintroduce leaks. While recent works have measured the extent of such leakage, they leave developers without actionable insights: which optimization passes are responsible, and how to disable them without modifying the compiler remains unclear. In this paper, we conduct a qualitative analysis of how compiler optimizations break constant-time code. We construct a dataset of compiler-introduced constant-time violations and analyze the internals of two widely used compilers, GCC and LLVM, to identify the specific optimization passes responsible. Our key insight is that a small set of passes are at the root of most leaks. To the best of our knowledge, we are also the first to characterize how the interactions between these passes contribute to leakage. Based on this analysis, we propose an original and practical mitigation that requires no source code modification or custom compiler: disabling selected optimization passes via compiler flags. We show that this approach significantly reduces leakage with minimal performance overhead, offering an immediately deployable defense for developers.</li>
</ul>

<h3>Title: Omni-Video: Democratizing Unified Video Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Mengping Yang, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06119">https://arxiv.org/abs/2507.06119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06119">https://arxiv.org/pdf/2507.06119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06119]] Omni-Video: Democratizing Unified Video Understanding and Generation(https://arxiv.org/abs/2507.06119)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Notable breakthroughs in unified understanding and generation modeling have led to remarkable advancements in image understanding, reasoning, production and editing, yet current foundational models predominantly focus on processing images, creating a gap in the development of unified models for video understanding and generation. This report presents Omni-Video, an efficient and effective unified framework for video understanding, generation, as well as instruction-based editing. Our key insight is to teach existing multimodal large language models (MLLMs) to produce continuous visual clues that are used as the input of diffusion decoders, which produce high-quality videos conditioned on these visual clues. To fully unlock the potential of our system for unified video modeling, we integrate several technical improvements: 1) a lightweight architectural design that respectively attaches a vision head on the top of MLLMs and a adapter before the input of diffusion decoders, the former produce visual tokens for the latter, which adapts these visual tokens to the conditional space of diffusion decoders; and 2) an efficient multi-stage training scheme that facilitates a fast connection between MLLMs and diffusion decoders with limited data and computational resources. We empirically demonstrate that our model exhibits satisfactory generalization abilities across video generation, editing and understanding tasks.</li>
</ul>

<h3>Title: NeoBabel: A Multilingual Open Tower for Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Mahdi Derakhshani, Dheeraj Varghese, Marzieh Fadaee, Cees G. M. Snoek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06137">https://arxiv.org/abs/2507.06137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06137">https://arxiv.org/pdf/2507.06137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06137]] NeoBabel: A Multilingual Open Tower for Visual Generation(https://arxiv.org/abs/2507.06137)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI.</li>
</ul>

<h3>Title: Coding Triangle: How Does Large Language Model Understand Code?</h3>
<ul>
<li><strong>Authors: </strong>Taolin Zhang, Zihan Ma, Maosong Cao, Junnan Liu, Songyang Zhang, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06138">https://arxiv.org/abs/2507.06138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06138">https://arxiv.org/pdf/2507.06138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06138]] Coding Triangle: How Does Large Language Model Understand Code?(https://arxiv.org/abs/2507.06138)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable progress in code generation, yet their true programming competence remains underexplored. We introduce the Code Triangle framework, which systematically evaluates LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation. Through extensive experiments on competitive programming benchmarks, we reveal that while LLMs can form a self-consistent system across these dimensions, their solutions often lack the diversity and robustness of human programmers. We identify a significant distribution shift between model cognition and human expertise, with model errors tending to cluster due to training data biases and limited reasoning transfer. Our study demonstrates that incorporating human-generated editorials, solutions, and diverse test cases, as well as leveraging model mixtures, can substantially enhance both the performance and robustness of LLMs. Furthermore, we reveal both the consistency and inconsistency in the cognition of LLMs that may facilitate self-reflection and self-improvement, providing a potential direction for developing more powerful coding models.</li>
</ul>

<h3>Title: Topic Modeling and Link-Prediction for Material Property Discovery</h3>
<ul>
<li><strong>Authors: </strong>Ryan C. Barron, Maksim E. Eren, Valentin Stanev, Cynthia Matuszek, Boian S. Alexandrov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06139">https://arxiv.org/abs/2507.06139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06139">https://arxiv.org/pdf/2507.06139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06139]] Topic Modeling and Link-Prediction for Material Property Discovery(https://arxiv.org/abs/2507.06139)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Link prediction infers missing or future relations between graph nodes, based on connection patterns. Scientific literature networks and knowledge graphs are typically large, sparse, and noisy, and often contain missing links between entities. We present an AI-driven hierarchical link prediction framework that integrates matrix factorization to infer hidden associations and steer discovery in complex material domains. Our method combines Hierarchical Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization (BNMFk) with automatic model selection, as well as Logistic matrix factorization (LMF), we use to construct a three-level topic tree from a 46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs). These materials are studied in a variety of physics fields with many current and potential applications. An ensemble BNMFk + LMF approach fuses discrete interpretability with probabilistic scoring. The resulting HNMFk clusters map each material onto coherent topics like superconductivity, energy storage, and tribology. Also, missing or weakly connected links are highlight between topics and materials, suggesting novel hypotheses for cross-disciplinary exploration. We validate our method by removing publications about superconductivity in well-known superconductors, and show the model predicts associations with the superconducting TMD clusters. This shows the method finds hidden connections in a graph of material to latent topic associations built from scientific literature, especially useful when examining a diverse corpus of scientific documents covering the same class of phenomena or materials but originating from distinct communities and perspectives. The inferred links generating new hypotheses, produced by our method, are exposed through an interactive Streamlit dashboard, designed for human-in-the-loop scientific discovery.</li>
</ul>

<h3>Title: Prompt-Free Conditional Diffusion for Multi-object Image Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Wang, Lei Zhang, Wei Wei, Chen Ding, Yanning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06146">https://arxiv.org/abs/2507.06146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06146">https://arxiv.org/pdf/2507.06146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06146]] Prompt-Free Conditional Diffusion for Multi-object Image Augmentation(https://arxiv.org/abs/2507.06146)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models has underpinned much recent advances of dataset augmentation in various computer vision tasks. However, when involving generating multi-object images as real scenarios, most existing methods either rely entirely on text condition, resulting in a deviation between the generated objects and the original data, or rely too much on the original images, resulting in a lack of diversity in the generated images, which is of limited help to downstream tasks. To mitigate both problems with one stone, we propose a prompt-free conditional diffusion framework for multi-object image augmentation. Specifically, we introduce a local-global semantic fusion strategy to extract semantics from images to replace text, and inject knowledge into the diffusion model through LoRA to alleviate the category deviation between the original model and the target dataset. In addition, we design a reward model based counting loss to assist the traditional reconstruction loss for model training. By constraining the object counts of each category instead of pixel-by-pixel constraints, bridging the quantity deviation between the generated data and the original data while improving the diversity of the generated data. Experimental results demonstrate the superiority of the proposed method over several representative state-of-the-art baselines and showcase strong downstream task gain and out-of-domain generalization capabilities. Code is available at \href{this https URL}{here}.</li>
</ul>

<h3>Title: Normalizing Diffusion Kernels with Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Nathan Kessler, Robin Magnet, Jean Feydy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06161">https://arxiv.org/abs/2507.06161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06161">https://arxiv.org/pdf/2507.06161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06161]] Normalizing Diffusion Kernels with Optimal Transport(https://arxiv.org/abs/2507.06161)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Smoothing a signal based on local neighborhoods is a core operation in machine learning and geometry processing. On well-structured domains such as vector spaces and manifolds, the Laplace operator derived from differential geometry offers a principled approach to smoothing via heat diffusion, with strong theoretical guarantees. However, constructing such Laplacians requires a carefully defined domain structure, which is not always available. Most practitioners thus rely on simple convolution kernels and message-passing layers, which are biased against the boundaries of the domain. We bridge this gap by introducing a broad class of smoothing operators, derived from general similarity or adjacency matrices, and demonstrate that they can be normalized into diffusion-like operators that inherit desirable properties from Laplacians. Our approach relies on a symmetric variant of the Sinkhorn algorithm, which rescales positive smoothing operators to match the structural behavior of heat diffusion. This construction enables Laplacian-like smoothing and processing of irregular data such as point clouds, sparse voxel grids or mixture of Gaussians. We show that the resulting operators not only approximate heat diffusion but also retain spectral information from the Laplacian itself, with applications to shape analysis and matching.</li>
</ul>

<h3>Title: OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion</h3>
<ul>
<li><strong>Authors: </strong>Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06165">https://arxiv.org/abs/2507.06165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06165">https://arxiv.org/pdf/2507.06165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06165]] OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion(https://arxiv.org/abs/2507.06165)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The creation of 3D assets with explicit, editable part structures is crucial for advancing interactive applications, yet most generative methods produce only monolithic shapes, limiting their utility. We introduce OmniPart, a novel framework for part-aware 3D object generation designed to achieve high semantic decoupling among components while maintaining robust structural cohesion. OmniPart uniquely decouples this complex task into two synergistic stages: (1) an autoregressive structure planning module generates a controllable, variable-length sequence of 3D part bounding boxes, critically guided by flexible 2D part masks that allow for intuitive control over part decomposition without requiring direct correspondences or semantic labels; and (2) a spatially-conditioned rectified flow model, efficiently adapted from a pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and consistently within the planned layout. Our approach supports user-defined part granularity, precise localization, and enables diverse downstream applications. Extensive experiments demonstrate that OmniPart achieves state-of-the-art performance, paving the way for more interpretable, editable, and versatile 3D content.</li>
</ul>

<h3>Title: Skywork-R1V3 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06167">https://arxiv.org/abs/2507.06167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06167">https://arxiv.org/pdf/2507.06167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06167]] Skywork-R1V3 Technical Report(https://arxiv.org/abs/2507.06167)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We introduce Skywork-R1V3, an advanced, open-source vision-language model (VLM) that pioneers a new approach to visual reasoning. Its key innovation lies in effectively transferring reasoning skills from text-only Large Language Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily stems from our elaborate post-training RL framework, which effectively activates and enhances the model's reasoning ability, without the need for additional continue pre-training. Through this framework, we further uncover the fundamental role of the connector module in achieving robust cross-modal alignment for multimodal reasoning models. In addition, we introduce a unique indicator of reasoning capability, the entropy of critical reasoning tokens, which has proven highly effective for checkpoint selection during RL training. Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%. This performance matches entry-level human capabilities. Remarkably, our RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs. The implementation successfully transfers mathematical reasoning to other subject-related reasoning tasks. We also include an analysis of curriculum learning and reinforcement finetuning strategies, along with a broader discussion on multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities.</li>
</ul>

<h3>Title: DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Heil, Dionne Bang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06189">https://arxiv.org/abs/2507.06189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06189">https://arxiv.org/pdf/2507.06189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06189]] DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation(https://arxiv.org/abs/2507.06189)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents our submission to Task 1, Subjectivity Detection, of the CheckThat! Lab at CLEF 2025. We investigate the effectiveness of transfer-learning and stylistic data augmentation to improve classification of subjective and objective sentences in English news text. Our approach contrasts fine-tuning of pre-trained encoders and transfer-learning of fine-tuned transformer on related tasks. We also introduce a controlled augmentation pipeline using GPT-4o to generate paraphrases in predefined subjectivity styles. To ensure label and style consistency, we employ the same model to correct and refine the generated samples. Results show that transfer-learning of specified encoders outperforms fine-tuning general-purpose ones, and that carefully curated augmentation significantly enhances model robustness, especially in detecting subjective content. Our official submission placed us $16^{th}$ of 24 participants. Overall, our findings underscore the value of combining encoder specialization with label-consistent augmentation for improved subjectivity detection. Our code is available at this https URL.</li>
</ul>

<h3>Title: UQLM: A Python Package for Uncertainty Quantification in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06196">https://arxiv.org/abs/2507.06196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06196">https://arxiv.org/pdf/2507.06196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06196]] UQLM: A Python Package for Uncertainty Quantification in Large Language Models(https://arxiv.org/abs/2507.06196)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs.</li>
</ul>

<h3>Title: A Survey on Latent Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06203">https://arxiv.org/abs/2507.06203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06203">https://arxiv.org/pdf/2507.06203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06203]] A Survey on Latent Reasoning(https://arxiv.org/abs/2507.06203)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the model's continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: this https URL.</li>
</ul>

<h3>Title: Differential Mamba</h3>
<ul>
<li><strong>Authors: </strong>Nadav Schneider, Itamar Zimerman, Eliya Nachmani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06204">https://arxiv.org/abs/2507.06204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06204">https://arxiv.org/pdf/2507.06204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06204]] Differential Mamba(https://arxiv.org/abs/2507.06204)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available.</li>
</ul>

<h3>Title: DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Ayush Parikh, Hoang Thanh Thanh Truong, Jeanette Schofield, Maximilian Heil</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06205">https://arxiv.org/abs/2507.06205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06205">https://arxiv.org/pdf/2507.06205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06205]] DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media(https://arxiv.org/abs/2507.06205)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a Scientific Web Discourse Detection, present the methods we explored for this task. For this multiclass classification task, we determined if a tweet contained a scientific claim, a reference to a scientific study or publication, and/or mentions of scientific entities, such as a university or a scientist. We present 3 modeling approaches for this task: transformer finetuning, few-shot prompting of LLMs, and a combined ensemble model whose design was informed by earlier experiments. Our team placed 7th in the competition, achieving a macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline of 0.8375. Our code is available on Github at this https URL.</li>
</ul>

<h3>Title: CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Huang, Zhiyuan Fan, Zhitao He, Sandeep Polisetty, Wenyan Li, Yi R. Fung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06210">https://arxiv.org/abs/2507.06210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06210">https://arxiv.org/pdf/2507.06210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06210]] CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions(https://arxiv.org/abs/2507.06210)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Pretrained vision-language models (VLMs) such as CLIP excel in multimodal understanding but struggle with contextually relevant fine-grained visual features, making it difficult to distinguish visually similar yet culturally distinct concepts. This limitation stems from the scarcity of high-quality culture-specific datasets, the lack of integrated contextual knowledge, and the absence of hard negatives highlighting subtle distinctions. To address these challenges, we first design a data curation pipeline that leverages open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a synthetic cultural dataset. This dataset consists of paired concept-caption-image triplets, where concepts visually resemble each other but represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to create CultureCLIP, which aligns cultural concepts with contextually enhanced captions and synthetic images through customized contrastive learning, enabling finer cultural differentiation while preserving generalization capabilities. Experiments on culturally relevant benchmarks show that CultureCLIP outperforms the base CLIP, achieving up to a notable 5.49% improvement in fine-grained concept recognition on certain tasks, while preserving CLIP's original generalization ability, validating the effectiveness of our data synthesis and VLM backbone training paradigm in capturing subtle cultural distinctions.</li>
</ul>

<h3>Title: Modern Methods in Associative Memory</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Krotov, Benjamin Hoover, Parikshit Ram, Bao Pham</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06211">https://arxiv.org/abs/2507.06211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06211">https://arxiv.org/pdf/2507.06211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06211]] Modern Methods in Associative Memory(https://arxiv.org/abs/2507.06211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Associative Memories like the famous Hopfield Networks are elegant models for describing fully recurrent neural networks whose fundamental job is to store and retrieve information. In the past few years they experienced a surge of interest due to novel theoretical results pertaining to their information storage capabilities, and their relationship with SOTA AI architectures, such as Transformers and Diffusion Models. These connections open up possibilities for interpreting the computation of traditional AI networks through the theoretical lens of Associative Memories. Additionally, novel Lagrangian formulations of these networks make it possible to design powerful distributed models that learn useful representations and inform the design of novel architectures. This tutorial provides an approachable introduction to Associative Memories, emphasizing the modern language and methods used in this area of research, with practical hands-on mathematical derivations and coding notebooks.</li>
</ul>

<h3>Title: Deep Learning Optimization of Two-State Pinching Antennas Systems</h3>
<ul>
<li><strong>Authors: </strong>Odysseas G. Karagiannidis, Victoria E. Galanopoulou, Panagiotis D. Diamantoulakis, Zhiguo Ding, Octavia Dobre</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06222">https://arxiv.org/abs/2507.06222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06222">https://arxiv.org/pdf/2507.06222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06222]] Deep Learning Optimization of Two-State Pinching Antennas Systems(https://arxiv.org/abs/2507.06222)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The evolution of wireless communication systems requires flexible, energy-efficient, and cost-effective antenna technologies. Pinching antennas (PAs), which can dynamically control electromagnetic wave propagation through binary activation states, have recently emerged as a promising candidate. In this work, we investigate the problem of optimally selecting a subset of fixed-position PAs to activate in a waveguide, when the aim is to maximize the communication rate at a user terminal. Due to the complex interplay between antenna activation, waveguide-induced phase shifts, and power division, this problem is formulated as a combinatorial fractional 0-1 quadratic program. To efficiently solve this challenging problem, we use neural network architectures of varying complexity to learn activation policies directly from data, leveraging spatial features and signal structure. Furthermore, we incorporate user location uncertainty into our training and evaluation pipeline to simulate realistic deployment conditions. Simulation results demonstrate the effectiveness and robustness of the proposed models.</li>
</ul>

<h3>Title: Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao, Yi Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06223">https://arxiv.org/abs/2507.06223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06223">https://arxiv.org/pdf/2507.06223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06223]] Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers(https://arxiv.org/abs/2507.06223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.</li>
</ul>

<h3>Title: Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion</h3>
<ul>
<li><strong>Authors: </strong>Aleksandar Jevtić, Christoph Reich, Felix Wimbauer, Oliver Hahn, Christian Rupprecht, Stefan Roth, Daniel Cremers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06230">https://arxiv.org/abs/2507.06230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06230">https://arxiv.org/pdf/2507.06230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06230]] Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion(https://arxiv.org/abs/2507.06230)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in a feed-forward manner. Through a novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of a current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards a strong foundation for single image 3D scene understanding.</li>
</ul>

<h3>Title: RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Keyan Chen, Chenyang Liu, Bowen Chen, Jiafan Zhang, Zhengxia Zou, Zhenwei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06231">https://arxiv.org/abs/2507.06231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06231">https://arxiv.org/pdf/2507.06231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06231]] RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models(https://arxiv.org/abs/2507.06231)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Referring Remote Sensing Image Segmentation provides a flexible and fine-grained framework for remote sensing scene analysis via vision-language collaborative interpretation. Current approaches predominantly utilize a three-stage pipeline encompassing dual-modal encoding, cross-modal interaction, and pixel decoding. These methods demonstrate significant limitations in managing complex semantic relationships and achieving precise cross-modal alignment, largely due to their coupled processing mechanism that conflates target localization with boundary delineation. This architectural coupling amplifies error propagation under semantic ambiguity while restricting model generalizability and interpretability. To address these issues, we propose RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow into a collaborative dual-stage framework: coarse localization followed by fine segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with SAM's segmentation generalizability through strategic foundation model collaboration. Specifically, CLIP is employed as the dual-modal encoder to activate target features within its pre-aligned semantic space and generate localization prompts. To mitigate CLIP's misactivation challenges in multi-entity scenarios described by referring texts, a cascaded second-order prompter is devised, which enhances precision through implicit reasoning via decomposition of text embeddings into complementary semantic subspaces. These optimized semantic prompts subsequently direct the SAM to generate pixel-level refined masks, thereby completing the semantic transmission pipeline. Extensive experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2 surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex semantic interpretation. Code is available at: this https URL.</li>
</ul>

<h3>Title: Learning to Track Any Points from Human Motion</h3>
<ul>
<li><strong>Authors: </strong>Inès Hyeonsu Kim, Seokju Cho, Jahyeok Koo, Junghyun Park, Jiahui Huang, Joon-Young Lee, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.06233">https://arxiv.org/abs/2507.06233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.06233">https://arxiv.org/pdf/2507.06233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.06233]] Learning to Track Any Points from Human Motion(https://arxiv.org/abs/2507.06233)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human motion, with its inherent complexities, such as non-rigid deformations, articulated movements, clothing distortions, and frequent occlusions caused by limbs or other individuals, provides a rich and challenging source of supervision that is crucial for training robust and generalizable point trackers. Despite the suitability of human motion, acquiring extensive training data for point tracking remains difficult due to laborious manual annotation. Our proposed pipeline, AnthroTAP, addresses this by proposing an automated pipeline to generate pseudo-labeled training data, leveraging the Skinned Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected humans in video frames, project the resulting 3D mesh vertices onto 2D image planes to generate pseudo-trajectories, handle occlusions using ray-casting, and filter out unreliable tracks based on optical flow consistency. A point tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art performance on the TAP-Vid benchmark, surpassing other models trained on real videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to 256 GPUs used in recent state-of-the-art.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
