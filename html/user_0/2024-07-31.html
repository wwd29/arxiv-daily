<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-31</h1>
<h3>Title: Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions</h3>
<ul>
<li><strong>Authors: </strong>Jinsung Yoon, Raj Sinha, Sercan O Arik, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20243">https://arxiv.org/abs/2407.20243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20243">https://arxiv.org/pdf/2407.20243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20243]] Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions(https://arxiv.org/abs/2407.20243)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Embeddings from Large Language Models (LLMs) have emerged as critical components in various applications, particularly for information retrieval. While high-dimensional embeddings generally demonstrate superior performance as they contain more salient information, their practical application is frequently hindered by elevated computational latency and the associated higher cost. To address these challenges, we propose Matryoshka-Adaptor, a novel tuning framework designed for the customization of LLM embeddings. Matryoshka-Adaptor facilitates substantial dimensionality reduction while maintaining comparable performance levels, thereby achieving a significant enhancement in computational efficiency and cost-effectiveness. Our framework directly modifies the embeddings from pre-trained LLMs which is designed to be seamlessly integrated with any LLM architecture, encompassing those accessible exclusively through black-box APIs. Also, it exhibits efficacy in both unsupervised and supervised learning settings. A rigorous evaluation conducted across a diverse corpus of English, multilingual, and multimodal datasets consistently reveals substantial gains with Matryoshka-Adaptor. Notably, with Google and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in dimensionality ranging from two- to twelve-fold without compromising performance across multiple BEIR datasets.</li>
</ul>

<h3>Title: Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies</h3>
<ul>
<li><strong>Authors: </strong>Lachlan McGinness, Peter Baumgartner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20244">https://arxiv.org/abs/2407.20244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20244">https://arxiv.org/pdf/2407.20244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20244]] Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies(https://arxiv.org/abs/2407.20244)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study presents the first examination of the ability of Large Language Models (LLMs) to follow reasoning strategies that are used to guide Automated Theorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and Google's recent Gemini model on problems from a steamroller domain. In addition to determining accuracy we make use of the Natural Language Processing library spaCy to explore new methods of investigating LLM's reasoning capabilities. This led to one alarming result, the low correlation between correct reasoning and correct answers for any of the tested models. We found that the models' performance when using the ATP reasoning strategies was comparable to one-shot chain of thought and observe that attention to uncertainty in the accuracy results is critical when drawing conclusions about model performance. Consistent with previous speculation we confirm that LLMs have a preference for, and are best able to follow, bottom up reasoning processes. However, the reasoning strategies can still be beneficial for deriving small and relevant sets of formulas for external processing by a trusted inference engine.</li>
</ul>

<h3>Title: Utilizing Generative Adversarial Networks for Image Data Augmentation and Classification of Semiconductor Wafer Dicing Induced Defects</h3>
<ul>
<li><strong>Authors: </strong>Zhining Hu, Tobias Schlosser, Michael Friedrich, André Luiz Vieira e Silva, Frederik Beuth, Danny Kowerko</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20268">https://arxiv.org/abs/2407.20268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20268">https://arxiv.org/pdf/2407.20268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20268]] Utilizing Generative Adversarial Networks for Image Data Augmentation and Classification of Semiconductor Wafer Dicing Induced Defects(https://arxiv.org/abs/2407.20268)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In semiconductor manufacturing, the wafer dicing process is central yet vulnerable to defects that significantly impair yield - the proportion of defect-free chips. Deep neural networks are the current state of the art in (semi-)automated visual inspection. However, they are notoriously known to require a particularly large amount of data for model training. To address these challenges, we explore the application of generative adversarial networks (GAN) for image data augmentation and classification of semiconductor wafer dicing induced defects to enhance the variety and balance of training data for visual inspection systems. With this approach, synthetic yet realistic images are generated that mimic real-world dicing defects. We employ three different GAN variants for high-resolution image synthesis: Deep Convolutional GAN (DCGAN), CycleGAN, and StyleGAN3. Our work-in-progress results demonstrate that improved classification accuracies can be obtained, showing an average improvement of up to 23.1 % from 65.1 % (baseline experiment) to 88.2 % (DCGAN experiment) in balanced accuracy, which may enable yield optimization in production.</li>
</ul>

<h3>Title: Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Tang, Ye Liu, Xukai Liu, Kai Zhang, Yanghai Zhang, Qi Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20271">https://arxiv.org/abs/2407.20271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20271">https://arxiv.org/pdf/2407.20271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20271]] Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models(https://arxiv.org/abs/2407.20271)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in machine learning, especially in Natural Language Processing (NLP), have led to the development of sophisticated models trained on vast datasets, but this progress has raised concerns about potential sensitive information leakage. In response, regulatory measures like the EU General Data Protection Regulation (GDPR) have driven the exploration of Machine Unlearning techniques, which aim to enable models to selectively forget certain data entries. While early approaches focused on pre-processing methods, recent research has shifted towards training-based machine unlearning methods. However, many existing methods require access to original training data, posing challenges in scenarios where such data is unavailable. Besides, directly facilitating unlearning may undermine the language model's general expressive ability. To this end, in this paper, we introduce the Iterative Contrastive Unlearning (ICU) framework, which addresses these challenges by incorporating three key components. We propose a Knowledge Unlearning Induction module for unlearning specific target sequences and a Contrastive Learning Enhancement module to prevent degrading in generation capacity. Additionally, an Iterative Unlearning Refinement module is integrated to make the process more adaptive to each target sample respectively. Experimental results demonstrate the efficacy of ICU in maintaining performance while efficiently unlearning sensitive information, offering a promising avenue for privacy-conscious machine learning applications.</li>
</ul>

<h3>Title: An Efficient Inference Framework for Early-exit Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruijie Miao, Yihan Yan, Xinshuo Yao, Tong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20272">https://arxiv.org/abs/2407.20272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20272">https://arxiv.org/pdf/2407.20272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20272]] An Efficient Inference Framework for Early-exit Large Language Models(https://arxiv.org/abs/2407.20272)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Building efficient inference framework has gained increasing interests for research community. Early-exit models, a variant of LLMs, improves the inference efficiency of LLMs by skipping rest layers and directly generate output tokens when they are confident enough. However, there is no work of LLM inference framework that takes early-exit models into consideration. This is non-trivial as prior art on LLM inference cannot be directly applied to early-exit models. In this work, we solves two key challenges in building efficient inference framework for early-exit models: (1) batch inference at iteration-level granularity; and (2) KV cache management. For the former, we propose to process the batch until all sequences surpass the early-exit confidence threshold. For the latter, we propose to fill the KV cache of rest layers before the iteration terminates. Our evaluation shows that, compared with the original vLLM operating at full layers, our solution achieves up to 1.25x speed up.</li>
</ul>

<h3>Title: Exploring the Plausibility of Hate and Counter Speech Detectors with Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Adrian Jaques Böck, Djordje Slijepčević, Matthias Zeppelzauer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20274">https://arxiv.org/abs/2407.20274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20274">https://arxiv.org/pdf/2407.20274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20274]] Exploring the Plausibility of Hate and Counter Speech Detectors with Explainable AI(https://arxiv.org/abs/2407.20274)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>In this paper we investigate the explainability of transformer models and their plausibility for hate speech and counter speech detection. We compare representatives of four different explainability approaches, i.e., gradient-based, perturbation-based, attention-based, and prototype-based approaches, and analyze them quantitatively with an ablation study and qualitatively in a user study. Results show that perturbation-based explainability performs best, followed by gradient-based and attention-based explainability. Prototypebased experiments did not yield useful results. Overall, we observe that explainability strongly supports the users in better understanding the model predictions.</li>
</ul>

<h3>Title: Robust and Efficient Transfer Learning via Supernet Transfer in Warm-started Neural Architecture Search</h3>
<ul>
<li><strong>Authors: </strong>Prabhant Singh, Joaquin Vanschoren</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20279">https://arxiv.org/abs/2407.20279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20279">https://arxiv.org/pdf/2407.20279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20279]] Robust and Efficient Transfer Learning via Supernet Transfer in Warm-started Neural Architecture Search(https://arxiv.org/abs/2407.20279)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hand-designing Neural Networks is a tedious process that requires significant expertise. Neural Architecture Search (NAS) frameworks offer a very useful and popular solution that helps to democratize AI. However, these NAS frameworks are often computationally expensive to run, which limits their applicability and accessibility. In this paper, we propose a novel transfer learning approach, capable of effectively transferring pretrained supernets based on Optimal Transport or multi-dataset pretaining. This method can be generally applied to NAS methods based on Differentiable Architecture Search (DARTS). Through extensive experiments across dozens of image classification tasks, we demonstrate that transferring pretrained supernets in this way can not only drastically speed up the supernet training which then finds optimal models (3 to 5 times faster on average), but even yield that outperform those found when running DARTS methods from scratch. We also observe positive transfer to almost all target datasets, making it very robust. Besides drastically improving the applicability of NAS methods, this also opens up new applications for continual learning and related fields.</li>
</ul>

<h3>Title: From pixels to planning: scale-free active inference</h3>
<ul>
<li><strong>Authors: </strong>Karl Friston, Conor Heins, Tim Verbelen, Lancelot Da Costa, Tommaso Salvatori, Dimitrije Markovic, Alexander Tschantz, Magnus Koudahl, Christopher Buckley, Thomas Parr</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20292">https://arxiv.org/abs/2407.20292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20292">https://arxiv.org/pdf/2407.20292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20292]] From pixels to planning: scale-free active inference(https://arxiv.org/abs/2407.20292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper describes a discrete state-space model -- and accompanying methods -- for generative modelling. This model generalises partially observed Markov decision processes to include paths as latent variables, rendering it suitable for active inference and learning in a dynamic setting. Specifically, we consider deep or hierarchical forms using the renormalisation group. The ensuing renormalising generative models (RGM) can be regarded as discrete homologues of deep convolutional neural networks or continuous state-space models in generalised coordinates of motion. By construction, these scale-invariant models can be used to learn compositionality over space and time, furnishing models of paths or orbits; i.e., events of increasing temporal depth and itinerancy. This technical note illustrates the automatic discovery, learning and deployment of RGMs using a series of applications. We start with image classification and then consider the compression and generation of movies and music. Finally, we apply the same variational principles to the learning of Atari-like games.</li>
</ul>

<h3>Title: A Bayesian Flow Network Framework for Chemistry Tasks</h3>
<ul>
<li><strong>Authors: </strong>Nianze Tao, Minori Abe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20294">https://arxiv.org/abs/2407.20294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20294">https://arxiv.org/pdf/2407.20294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20294]] A Bayesian Flow Network Framework for Chemistry Tasks(https://arxiv.org/abs/2407.20294)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we introduce ChemBFN, a language model that handles chemistry tasks based on Bayesian flow networks working on discrete data. A new accuracy schedule is proposed to improve the sampling quality by significantly reducing the reconstruction loss. We show evidence that our method is appropriate for generating molecules with satisfied diversity even when a smaller number of sampling steps is used. A classifier-free guidance method is adapted for conditional generation. It is also worthwhile to point out that after generative training, our model can be fine-tuned on regression and classification tasks with the state-of-the-art performance, which opens the gate of building all-in-one models in a single module style. Our model has been open sourced at this https URL.</li>
</ul>

<h3>Title: Sun Off, Lights On: Photorealistic Monocular Nighttime Simulation for Robust Semantic Perception</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Tzevelekakis, Shutong Zhang, Luc Van Gool, Christos Sakaridis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20336">https://arxiv.org/abs/2407.20336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20336">https://arxiv.org/pdf/2407.20336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20336]] Sun Off, Lights On: Photorealistic Monocular Nighttime Simulation for Robust Semantic Perception(https://arxiv.org/abs/2407.20336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Nighttime scenes are hard to semantically perceive with learned models and annotate for humans. Thus, realistic synthetic nighttime data become all the more important for learning robust semantic perception at night, thanks to their accurate and cheap semantic annotations. However, existing data-driven or hand-crafted techniques for generating nighttime images from daytime counterparts suffer from poor realism. The reason is the complex interaction of highly spatially varying nighttime illumination, which differs drastically from its daytime counterpart, with objects of spatially varying materials in the scene, happening in 3D and being very hard to capture with such 2D approaches. The above 3D interaction and illumination shift have proven equally hard to model in the literature, as opposed to other conditions such as fog or rain. Our method, named Sun Off, Lights On (SOLO), is the first to perform nighttime simulation on single images in a photorealistic fashion by operating in 3D. It first explicitly estimates the 3D geometry, the materials and the locations of light sources of the scene from the input daytime image and relights the scene by probabilistically instantiating light sources in a way that accounts for their semantics and then running standard ray tracing. Not only is the visual quality and photorealism of our nighttime images superior to competing approaches including diffusion models, but the former images are also proven more beneficial for semantic nighttime segmentation in day-to-night adaptation. Code and data will be made publicly available.</li>
</ul>

<h3>Title: Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Baraldi, Federico Cocchi, Marcella Cornia, Lorenzo Baraldi, Alessandro Nicolosi, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20337">https://arxiv.org/abs/2407.20337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20337">https://arxiv.org/pdf/2407.20337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20337]] Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities(https://arxiv.org/abs/2407.20337)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discerning between authentic content and that generated by advanced AI methods has become increasingly challenging. While previous research primarily addresses the detection of fake faces, the identification of generated natural images has only recently surfaced. This prompted the recent exploration of solutions that employ foundation vision-and-language models, like CLIP. However, the CLIP embedding space is optimized for global image-to-text alignment and is not inherently designed for deepfake detection, neglecting the potential benefits of tailored training and local image features. In this study, we propose CoDE (Contrastive Deepfake Embeddings), a novel embedding space specifically designed for deepfake detection. CoDE is trained via contrastive learning by additionally enforcing global-local similarities. To sustain the training of our model, we generate a comprehensive dataset that focuses on images generated by diffusion models and encompasses a collection of 9.2 million images produced by using four different generators. Experimental results demonstrate that CoDE achieves state-of-the-art accuracy on the newly collected dataset, while also showing excellent generalization capabilities to unseen image generators. Our source code, trained models, and collected dataset are publicly available at: this https URL.</li>
</ul>

<h3>Title: From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection Models against Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Aditya Kulkarni, Vivek Balachandran, Dinil Mon Divakaran, Tamal Das</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20361">https://arxiv.org/abs/2407.20361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20361">https://arxiv.org/pdf/2407.20361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20361]] From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection Models against Adversarial Attacks(https://arxiv.org/abs/2407.20361)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Phishing attacks attempt to deceive users into stealing sensitive information, posing a significant cybersecurity threat. Advances in machine learning (ML) and deep learning (DL) have led to the development of numerous phishing webpage detection solutions, but these models remain vulnerable to adversarial attacks. Evaluating their robustness against adversarial phishing webpages is essential. Existing tools contain datasets of pre-designed phishing webpages for a limited number of brands, and lack diversity in phishing features. To address these challenges, we develop PhishOracle, a tool that generates adversarial phishing webpages by embedding diverse phishing features into legitimate webpages. We evaluate the robustness of two existing models, Stack model and Phishpedia, in classifying PhishOracle-generated adversarial phishing webpages. Additionally, we study a commercial large language model, Gemini Pro Vision, in the context of adversarial attacks. We conduct a user study to determine whether PhishOracle-generated adversarial phishing webpages deceive users. Our findings reveal that many PhishOracle-generated phishing webpages evade current phishing webpage detection models and deceive users, but Gemini Pro Vision is robust to the attack. We also develop the PhishOracle web app, allowing users to input a legitimate URL, select relevant phishing features and generate a corresponding phishing webpage. All resources are publicly available on GitHub.</li>
</ul>

<h3>Title: A Model Generalization Study in Localizing Indoor Cows with COw LOcalization (COLO) dataset</h3>
<ul>
<li><strong>Authors: </strong>Mautushi Das, Gonzalo Ferreira, C. P. James Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20372">https://arxiv.org/abs/2407.20372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20372">https://arxiv.org/pdf/2407.20372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20372]] A Model Generalization Study in Localizing Indoor Cows with COw LOcalization (COLO) dataset(https://arxiv.org/abs/2407.20372)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Precision livestock farming (PLF) increasingly relies on advanced object localization techniques to monitor livestock health and optimize resource management. This study investigates the generalization capabilities of YOLOv8 and YOLOv9 models for cow detection in indoor free-stall barn settings, focusing on varying training data characteristics such as view angles and lighting, and model complexities. Leveraging the newly released public dataset, COws LOcalization (COLO) dataset, we explore three key hypotheses: (1) Model generalization is equally influenced by changes in lighting conditions and camera angles; (2) Higher model complexity guarantees better generalization performance; (3) Fine-tuning with custom initial weights trained on relevant tasks always brings advantages to detection tasks. Our findings reveal considerable challenges in detecting cows in images taken from side views and underscore the importance of including diverse camera angles in building a detection model. Furthermore, our results emphasize that higher model complexity does not necessarily lead to better performance. The optimal model configuration heavily depends on the specific task and dataset. Lastly, while fine-tuning with custom initial weights trained on relevant tasks offers advantages to detection tasks, simpler models do not benefit similarly from this approach. It is more efficient to train a simple model with pre-trained weights without relying on prior relevant information, which can require intensive labor efforts. Future work should focus on adaptive methods and advanced data augmentation to improve generalization and robustness. This study provides practical guidelines for PLF researchers on deploying computer vision models from existing studies, highlights generalization issues, and contributes the COLO dataset containing 1254 images and 11818 cow instances for further research.</li>
</ul>

<h3>Title: What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Navapat Nananukul, Wichayaporn Wongkamjan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20382">https://arxiv.org/abs/2407.20382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20382">https://arxiv.org/pdf/2407.20382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20382]] What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models(https://arxiv.org/abs/2407.20382)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Role-playing games (RPGs) provide players with a rich, interactive world to explore. Dialogue serves as the primary means of communication between developers and players, manifesting in various forms such as guides, NPC interactions, and storytelling. While most games rely on written scripts to define the main story and character personalities, player immersion can be significantly enhanced through casual interactions between characters. With the advent of large language models (LLMs), we introduce a dialogue filler framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic and contextually appropriate character interactions. We test this framework within the environments of Final Fantasy VII Remake and Pokemon, providing qualitative and quantitative evidence that demonstrates GPT-4's capability to act with defined personalities and generate dialogue. However, some flaws remain, such as GPT-4 being overly positive or more subtle personalities, such as maturity, tend to be of lower quality compared to more overt traits like timidity. This study aims to assist developers in crafting more nuanced filler dialogues, thereby enriching player immersion and enhancing the overall RPG experience.</li>
</ul>

<h3>Title: Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Seong Hun Lee, Javier Civera</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20391">https://arxiv.org/abs/2407.20391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20391">https://arxiv.org/pdf/2407.20391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20391]] Alignment Scores: Robust Metrics for Multiview Pose Accuracy Evaluation(https://arxiv.org/abs/2407.20391)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose three novel metrics for evaluating the accuracy of a set of estimated camera poses given the ground truth: Translation Alignment Score (TAS), Rotation Alignment Score (RAS), and Pose Alignment Score (PAS). The TAS evaluates the translation accuracy independently of the rotations, and the RAS evaluates the rotation accuracy independently of the translations. The PAS is the average of the two scores, evaluating the combined accuracy of both translations and rotations. The TAS is computed in four steps: (1) Find the upper quartile of the closest-pair-distances, $d$. (2) Align the estimated trajectory to the ground truth using a robust registration method. (3) Collect all distance errors and obtain the cumulative frequencies for multiple thresholds ranging from $0.01d$ to $d$ with a resolution $0.01d$. (4) Add up these cumulative frequencies and normalize them such that the theoretical maximum is 1. The TAS has practical advantages over the existing metrics in that (1) it is robust to outliers and collinear motion, and (2) there is no need to adjust parameters on different datasets. The RAS is computed in a similar manner to the TAS and is also shown to be more robust against outliers than the existing rotation metrics. We verify our claims through extensive simulations and provide in-depth discussion of the strengths and weaknesses of the proposed metrics.</li>
</ul>

<h3>Title: Dense Self-Supervised Learning for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Maxime Seince, Loic Le Folgoc, Luiz Augusto Facury de Souza, Elsa Angelini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20395">https://arxiv.org/abs/2407.20395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20395">https://arxiv.org/pdf/2407.20395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20395]] Dense Self-Supervised Learning for Medical Image Segmentation(https://arxiv.org/abs/2407.20395)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning has revolutionized medical image segmentation, but it relies heavily on high-quality annotations. The time, cost and expertise required to label images at the pixel-level for each new task has slowed down widespread adoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL) approach for few-shot segmentation, that reduces the manual annotation burden by learning powerful pixel-level representations directly from unlabeled images. Pix2Rep is a novel pixel-level loss and pre-training paradigm for contrastive SSL on whole images. It is applied to generic encoder-decoder deep learning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance of the learned image-level representations under intensity and spatial image augmentations, Pix2Rep enforces equivariance of the pixel-level representations. We demonstrate the framework on a task of cardiac MRI segmentation. Results show improved performance compared to existing semi- and self-supervised approaches; and a 5-fold reduction in the annotation burden for equivalent performance versus a fully supervised U-Net baseline. This includes a 30% (resp. 31%) DICE improvement for one-shot segmentation under linear-probing (resp. fine-tuning). Finally, we also integrate the novel Pix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even better segmentation performance.</li>
</ul>

<h3>Title: Through the Looking Glass, and what Horn Clause Programs Found There</h3>
<ul>
<li><strong>Authors: </strong>Paul Tarau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20413">https://arxiv.org/abs/2407.20413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20413">https://arxiv.org/pdf/2407.20413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20413]] Through the Looking Glass, and what Horn Clause Programs Found There(https://arxiv.org/abs/2407.20413)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative</a></li>
<li><strong>Abstract: </strong>Dual Horn clauses mirror key properties of Horn clauses. This paper explores the ``other side of the looking glass'' to reveal some expected and unexpected symmetries and their practical uses. We revisit Dual Horn clauses as enablers of a form of constructive negation that supports goal-driven forward reasoning and is valid both intuitionistically and classically. In particular, we explore the ability to falsify a counterfactual hypothesis in the context of a background theory expressed as a Dual Horn clause program. With Dual Horn clause programs, by contrast to negation as failure, the variable bindings in their computed answers provide explanations for the reasons why a statement is successfully falsified. Moreover, in the propositional case, by contrast to negation as failure as implemented with stable models semantics in ASP systems, and similarly to Horn clause programs, Dual Horn clause programs have polynomial complexity. After specifying their execution model with a metainterpreter, we devise a compilation scheme from Dual Horn clause programs to Horn clause programs, ensuring their execution with no performance penalty and we design the embedded SymLP language to support combined Horn clause and Dual Horn clause programs. As a (motivating) application, we cast LLM reasoning chains into propositional Horn and Dual Horn clauses that work together to constructively prove and disprove goals and enhance Generative AI with explainability of reasoning chains.</li>
</ul>

<h3>Title: BaseBoostDepth: Exploiting Larger Baselines For Self-supervised Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Kieran Saunders, Luis J. Manso, George Vogiatzis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20437">https://arxiv.org/abs/2407.20437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20437">https://arxiv.org/pdf/2407.20437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20437]] BaseBoostDepth: Exploiting Larger Baselines For Self-supervised Monocular Depth Estimation(https://arxiv.org/abs/2407.20437)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the domain of multi-baseline stereo, the conventional understanding is that, in general, increasing baseline separation substantially enhances the accuracy of depth estimation. However, prevailing self-supervised depth estimation architectures primarily use minimal frame separation and a constrained stereo baseline. Larger frame separations can be employed; however, we show this to result in diminished depth quality due to various factors, including significant changes in brightness, and increased areas of occlusion. In response to these challenges, our proposed method, BaseBoostDepth, incorporates a curriculum learning-inspired optimization strategy to effectively leverage larger frame separations. However, we show that our curriculum learning-inspired strategy alone does not suffice, as larger baselines still cause pose estimation drifts. Therefore, we introduce incremental pose estimation to enhance the accuracy of pose estimations, resulting in significant improvements across all depth metrics. Additionally, to improve the robustness of the model, we introduce error-induced reconstructions, which optimize reconstructions with added error to the pose estimations. Ultimately, our final depth network achieves state-of-the-art performance on KITTI and SYNS-patches datasets across image-based, edge-based, and point cloud-based metrics without increasing computational complexity at test time. The project website can be found at this https URL.</li>
</ul>

<h3>Title: CoMMIT: Coordinated Instruction Tuning for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junda Wu, Xintong Li, Tong Yu, Yu Wang, Xiang Chen, Jiuxiang Gu, Lina Yao, Jingbo Shang, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20454">https://arxiv.org/abs/2407.20454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20454">https://arxiv.org/pdf/2407.20454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20454]] CoMMIT: Coordinated Instruction Tuning for Multimodal Large Language Models(https://arxiv.org/abs/2407.20454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction tuning in multimodal large language models (MLLMs) aims to smoothly integrate a backbone LLM with a pre-trained feature encoder for downstream tasks. The major challenge is how to efficiently find the synergy through cooperative learning where LLMs adapt their reasoning abilities in downstream tasks while feature encoders adjust their encoding to provide more relevant modal information. In this paper, we analyze the MLLM instruction tuning from both theoretical and empirical perspectives, where we find unbalanced learning between the two components, i.e., the feature encoder and the LLM, can cause diminishing learning gradients that slow the model convergence and often lead to sub-optimal results due to insufficient learning. Inspired by our findings, we propose a measurement to quantitatively evaluate the learning balance, based on which we further design a dynamic learning scheduler that better coordinates the learning. In addition, we introduce an auxiliary loss regularization method to promote updating of the generation distribution of MLLMs considering the learning state of each model component, which potentially prevents each component from gradient diminishing and enables a more accurate estimation of the learning balance coefficient. We conduct experiments with multiple LLM backbones and feature encoders, where our techniques are model-agnostic and can be generically integrated with various MLLM backbones. Experiment results on multiple downstream tasks and modalities in vision and audio, demonstrate the proposed method's better efficiency and effectiveness in MLLM instruction tuning.</li>
</ul>

<h3>Title: Learning Feature-Preserving Portrait Editing from Generated Pairs</h3>
<ul>
<li><strong>Authors: </strong>Bowei Chen, Tiancheng Zhi, Peihao Zhu, Shen Sang, Jing Liu, Linjie Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20455">https://arxiv.org/abs/2407.20455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20455">https://arxiv.org/pdf/2407.20455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20455]] Learning Feature-Preserving Portrait Editing from Generated Pairs(https://arxiv.org/abs/2407.20455)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Portrait editing is challenging for existing techniques due to difficulties in preserving subject features like identity. In this paper, we propose a training-based method leveraging auto-generated paired data to learn desired editing while ensuring the preservation of unchanged subject features. Specifically, we design a data generation process to create reasonably good training pairs for desired editing at low cost. Based on these pairs, we introduce a Multi-Conditioned Diffusion Model to effectively learn the editing direction and preserve subject features. During inference, our model produces accurate editing mask that can guide the inference process to further preserve detailed subject features. Experiments on costume editing and cartoon expression editing show that our method achieves state-of-the-art quality, quantitatively and qualitatively.</li>
</ul>

<h3>Title: Excavating Vulnerabilities Lurking in Multi-Factor Authentication Protocols: A Systematic Security Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ang Kok Wee, Eyasu Getahun Chekole, Jianying Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20459">https://arxiv.org/abs/2407.20459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20459">https://arxiv.org/pdf/2407.20459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20459]] Excavating Vulnerabilities Lurking in Multi-Factor Authentication Protocols: A Systematic Security Analysis(https://arxiv.org/abs/2407.20459)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>Nowadays, cyberattacks are growing exponentially, causing havoc to Internet users. In particular, authentication attacks constitute the major attack vector where intruders impersonate legitimate users to maliciously access systems or resources. Traditional single-factor authentication (SFA) protocols are often bypassed by side-channel and other attack techniques, hence they are no longer sufficient to the current authentication requirements. To alleviate this problem, multi-factor authentication (MFA) protocols have been widely adopted recently, which helps to raise the security bar against imposters. Although MFA is generally considered more robust and secure than SFA, it may not always guarantee enhanced security and efficiency. This is because, critical security vulnerabilities and performance problems may still arise due to design or implementation flaws of the protocols. Such vulnerabilities are often left unnoticed until they are exploited by attackers. Therefore, the main objective of this work is identifying such vulnerabilities in existing MFA protocols by systematically analysing their designs and constructions. To this end, we first form a set of security evaluation criteria, encompassing both existing and newly introduced ones, which we believe are very critical for the security of MFA protocols. Then, we thoroughly review several MFA protocols across different domains. Subsequently, we revisit and thoroughly analyze the design and construction of the protocols to identify potential vulnerabilities. Consequently, we manage to identify critical vulnerabilities in ten of the MFA protocols investigated. We thoroughly discuss the identified vulnerabilities in each protocol and devise relevant mitigation strategies. We also consolidate the performance information of those protocols to show the runtime and storage cost when employing varying number of authentication factors.</li>
</ul>

<h3>Title: Uncertainty-Rectified YOLO-SAM for Weakly Supervised ICH Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Pascal Spiegler, Amirhossein Rasoulian, Yiming Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20461">https://arxiv.org/abs/2407.20461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20461">https://arxiv.org/pdf/2407.20461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20461]] Uncertainty-Rectified YOLO-SAM for Weakly Supervised ICH Segmentation(https://arxiv.org/abs/2407.20461)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Intracranial hemorrhage (ICH) is a life-threatening condition that requires rapid and accurate diagnosis to improve treatment outcomes and patient survival rates. Recent advancements in supervised deep learning have greatly improved the analysis of medical images, but often rely on extensive datasets with high-quality annotations, which are costly, time-consuming, and require medical expertise to prepare. To mitigate the need for large amounts of expert-prepared segmentation data, we have developed a novel weakly supervised ICH segmentation method that utilizes the YOLO object detection model and an uncertainty-rectified Segment Anything Model (SAM). In addition, we have proposed a novel point prompt generator for this model to further improve segmentation results with YOLO-predicted bounding box prompts. Our approach achieved a high accuracy of 0.933 and an AUC of 0.796 in ICH detection, along with a mean Dice score of 0.629 for ICH segmentation, outperforming existing weakly supervised and popular supervised (UNet and Swin-UNETR) approaches. Overall, the proposed method provides a robust and accurate alternative to the more commonly used supervised techniques for ICH quantification without requiring refined segmentation ground truths during model training.</li>
</ul>

<h3>Title: Distribution Learning for Molecular Regression</h3>
<ul>
<li><strong>Authors: </strong>Nima Shoghi, Pooya Shoghi, Anuroop Sriram, Abhishek Das</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20475">https://arxiv.org/abs/2407.20475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20475">https://arxiv.org/pdf/2407.20475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20475]] Distribution Learning for Molecular Regression(https://arxiv.org/abs/2407.20475)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Using "soft" targets to improve model performance has been shown to be effective in classification settings, but the usage of soft targets for regression is a much less studied topic in machine learning. The existing literature on the usage of soft targets for regression fails to properly assess the method's limitations, and empirical evaluation is quite limited. In this work, we assess the strengths and drawbacks of existing methods when applied to molecular property regression tasks. Our assessment outlines key biases present in existing methods and proposes methods to address them, evaluated through careful ablation studies. We leverage these insights to propose Distributional Mixture of Experts (DMoE): A model-independent, and data-independent method for regression which trains a model to predict probability distributions of its targets. Our proposed loss function combines the cross entropy between predicted and target distributions and the L1 distance between their expected values to produce a loss function that is robust to the outlined biases. We evaluate the performance of DMoE on different molecular property prediction datasets -- Open Catalyst (OC20), MD17, and QM9 -- across different backbone model architectures -- SchNet, GemNet, and Graphormer. Our results demonstrate that the proposed method is a promising alternative to classical regression for molecular property prediction tasks, showing improvements over baselines on all datasets and architectures.</li>
</ul>

<h3>Title: A2SF: Accumulative Attention Scoring with Forgetting Factor for Token Pruning in Transformer Decoder</h3>
<ul>
<li><strong>Authors: </strong>Hyun Rae Jo, Dong Kun Shin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20485">https://arxiv.org/abs/2407.20485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20485">https://arxiv.org/pdf/2407.20485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20485]] A2SF: Accumulative Attention Scoring with Forgetting Factor for Token Pruning in Transformer Decoder(https://arxiv.org/abs/2407.20485)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLM) based on transformers are facing memory bottleneck issues due to KV cache, especially in long sequence handling. Previous researches proposed KV cache compression techniques that identify insignificant tokens based on Accumulative Attention Scores and removes their items from KV cache, noting that only few tokens play an important role in attention operations. However, we have observed that the existing Accumulative Attention Score is not suitable for the transformer decoder structure. In the decoder model, the number of times the Attention Score accumulates varies depending on the order of token appearance due to the effect of masking, causing an uneven comparison between tokens. To solve this, we propose Accumulative Attention Score with Forgetting Factor (A2SF) technique, which introduces a Forgetting Factor in the Attention Score accumulation process. A2SF applies a penalty to the past Attention Score generated from old tokens by repeatedly multiplying the Forgetting Factor to the Attention Score over time. Therefore, older tokens receive a larger penalty, providing fairness among different ages of tokens. Through the fair comparison among tokens, we can more effectively select important tokens. We have verified the accuracy improvement through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.</li>
</ul>

<h3>Title: A federated large language model for long-term time series forecasting</h3>
<ul>
<li><strong>Authors: </strong>Raed Abdel-Sater, A. Ben Hamza</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20503">https://arxiv.org/abs/2407.20503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20503">https://arxiv.org/pdf/2407.20503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20503]] A federated large language model for long-term time series forecasting(https://arxiv.org/abs/2407.20503)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Long-term time series forecasting in centralized environments poses unique challenges regarding data privacy, communication overhead, and scalability. To address these challenges, we propose FedTime, a federated large language model (LLM) tailored for long-range time series prediction. Specifically, we introduce a federated pre-trained LLM with fine-tuning and alignment strategies. Prior to the learning process, we employ K-means clustering to partition edge devices or clients into distinct clusters, thereby facilitating more focused model training. We also incorporate channel independence and patching to better preserve local semantic information, ensuring that important contextual details are retained while minimizing the risk of information loss. We demonstrate the effectiveness of our FedTime model through extensive experiments on various real-world forecasting benchmarks, showcasing substantial improvements over recent approaches. In addition, we demonstrate the efficiency of FedTime in streamlining resource usage, resulting in reduced communication overhead.</li>
</ul>

<h3>Title: Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language</h3>
<ul>
<li><strong>Authors: </strong>Hossein Rajaby Faghihi, Aliakbar Nafar, Andrzej Uszok, Hamid Karimian, Parisa Kordjamshidi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20513">https://arxiv.org/abs/2407.20513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20513">https://arxiv.org/pdf/2407.20513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20513]] Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language(https://arxiv.org/abs/2407.20513)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a conversational pipeline for crafting domain knowledge for complex neuro-symbolic models through natural language prompts. It leverages large language models to generate declarative programs in the DomiKnowS framework. The programs in this framework express concepts and their relationships as a graph in addition to logical constraints between them. The graph, later, can be connected to trainable neural models according to those specifications. Our proposed pipeline utilizes techniques like dynamic in-context demonstration retrieval, model refinement based on feedback from a symbolic parser, visualization, and user interaction to generate the tasks' structure and formal knowledge representation. This approach empowers domain experts, even those not well-versed in ML/AI, to formally declare their knowledge to be incorporated in customized neural models in the DomiKnowS framework.</li>
</ul>

<h3>Title: Markers Identification for Relative Pose Estimation of an Uncooperative Target</h3>
<ul>
<li><strong>Authors: </strong>Batu Candan, Simone Servadio</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20515">https://arxiv.org/abs/2407.20515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20515">https://arxiv.org/pdf/2407.20515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20515]] Markers Identification for Relative Pose Estimation of an Uncooperative Target(https://arxiv.org/abs/2407.20515)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel method using chaser spacecraft image processing and Convolutional Neural Networks (CNNs) to detect structural markers on the European Space Agency's (ESA) Environmental Satellite (ENVISAT) for safe de-orbiting. Advanced image pre-processing techniques, including noise addition and blurring, are employed to improve marker detection accuracy and robustness. Initial results show promising potential for autonomous space debris removal, supporting proactive strategies for space sustainability. The effectiveness of our approach suggests that our estimation method could significantly enhance the safety and efficiency of debris removal operations by implementing more robust and autonomous systems in actual space missions.</li>
</ul>

<h3>Title: Machine Unlearning in Generative AI: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20516">https://arxiv.org/abs/2407.20516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20516">https://arxiv.org/pdf/2407.20516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20516]] Machine Unlearning in Generative AI: A Survey(https://arxiv.org/abs/2407.20516)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: this https URL.</li>
</ul>

<h3>Title: Can LLMs be Fooled? Investigating Vulnerabilities in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sara Abdali, Jia He, CJ Barberan, Richard Anarfi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20529">https://arxiv.org/abs/2407.20529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20529">https://arxiv.org/pdf/2407.20529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20529]] Can LLMs be Fooled? Investigating Vulnerabilities in LLMs(https://arxiv.org/abs/2407.20529)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, large language model</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has garnered significant popularity and wielded immense power across various domains within Natural Language Processing (NLP). While their capabilities are undeniably impressive, it is crucial to identify and scrutinize their vulnerabilities especially when those vulnerabilities can have costly consequences. One such LLM, trained to provide a concise summarization from medical documents could unequivocally leak personal patient data when prompted surreptitiously. This is just one of many unfortunate examples that have been unveiled and further research is necessary to comprehend the underlying reasons behind such vulnerabilities. In this study, we delve into multiple sections of vulnerabilities which are model-based, training-time, inference-time vulnerabilities, and discuss mitigation strategies including "Model Editing" which aims at modifying LLMs behavior, and "Chroma Teaming" which incorporates synergy of multiple teaming strategies to enhance LLMs' resilience. This paper will synthesize the findings from each vulnerability section and propose new directions of research and development. By understanding the focal points of current vulnerabilities, we can better anticipate and mitigate future risks, paving the road for more robust and secure LLMs.</li>
</ul>

<h3>Title: HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Wencan Cheng, Eunji Kim, Jong Hwan Ko</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20542">https://arxiv.org/abs/2407.20542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20542">https://arxiv.org/pdf/2407.20542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20542]] HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation(https://arxiv.org/abs/2407.20542)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>The extraction of keypoint positions from input hand frames, known as 3D hand pose estimation, is crucial for various human-computer interaction applications. However, current approaches often struggle with the dynamic nature of self-occlusion of hands and intra-occlusion with interacting objects. To address this challenge, this paper proposes the Denoising Adaptive Graph Transformer, HandDAGT, for hand pose estimation. The proposed HandDAGT leverages a transformer structure to thoroughly explore effective geometric features from input patches. Additionally, it incorporates a novel attention mechanism to adaptively weigh the contribution of kinematic correspondence and local geometric features for the estimation of specific keypoints. This attribute enables the model to adaptively employ kinematic and local information based on the occlusion situation, enhancing its robustness and accuracy. Furthermore, we introduce a novel denoising training strategy aimed at improving the model's robust performance in the face of occlusion challenges. Experimental results show that the proposed model significantly outperforms the existing methods on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at this https URL.</li>
</ul>

<h3>Title: Automated Physical Design Watermarking Leveraging Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Ruisi Zhang, Rachel Selina Rajarathnam, David Z. Pan, Farinaz Koushanfar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20544">https://arxiv.org/abs/2407.20544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20544">https://arxiv.org/pdf/2407.20544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20544]] Automated Physical Design Watermarking Leveraging Graph Neural Networks(https://arxiv.org/abs/2407.20544)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, watermark</a></li>
<li><strong>Abstract: </strong>This paper presents AutoMarks, an automated and transferable watermarking framework that leverages graph neural networks to reduce the watermark search overheads during the placement stage. AutoMarks's novel automated watermark search is accomplished by (i) constructing novel graph and node features with physical, semantic, and design constraint-aware representation; (ii) designing a data-efficient sampling strategy for watermarking fidelity label collection; and (iii) leveraging a graph neural network to learn the connectivity between cells and predict the watermarking fidelity on unseen layouts. Extensive evaluations on ISPD'15 and ISPD'19 benchmarks demonstrate that our proposed automated methodology: (i) is capable of finding quality-preserving watermarks in a short time; and (ii) is transferable across various designs, i.e., AutoMarks trained on one layout is generalizable to other benchmark circuits. AutoMarks is also resilient against potential watermark removal and forging attacks</li>
</ul>

<h3>Title: DiffusionCounterfactuals: Inferring High-dimensional Counterfactuals with Guidance of Causal Representations</h3>
<ul>
<li><strong>Authors: </strong>Jiageng Zhu, Hanchen Xie, Jiazhi Li, Wael Abd-Almageed</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20553">https://arxiv.org/abs/2407.20553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20553">https://arxiv.org/pdf/2407.20553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20553]] DiffusionCounterfactuals: Inferring High-dimensional Counterfactuals with Guidance of Causal Representations(https://arxiv.org/abs/2407.20553)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate estimation of counterfactual outcomes in high-dimensional data is crucial for decision-making and understanding causal relationships and intervention outcomes in various domains, including healthcare, economics, and social sciences. However, existing methods often struggle to generate accurate and consistent counterfactuals, particularly when the causal relationships are complex. We propose a novel framework that incorporates causal mechanisms and diffusion models to generate high-quality counterfactual samples guided by causal representation. Our approach introduces a novel, theoretically grounded training and sampling process that enables the model to consistently generate accurate counterfactual high-dimensional data under multiple intervention steps. Experimental results on various synthetic and real benchmarks demonstrate the proposed approach outperforms state-of-the-art methods in generating accurate and high-quality counterfactuals, using different evaluation metrics.</li>
</ul>

<h3>Title: CELLM: An Efficient Communication in Large Language Models Training for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Raja Vavekanand, Kira Sam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20557">https://arxiv.org/abs/2407.20557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20557">https://arxiv.org/pdf/2407.20557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20557]] CELLM: An Efficient Communication in Large Language Models Training for Federated Learning(https://arxiv.org/abs/2407.20557)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a recent model training paradigm in which client devices collaboratively train a model without ever aggregating their data. Crucially, this scheme offers users potential privacy and security benefits by only ever communicating updates to the model weights to a central server as opposed to traditional machine learning (ML) training which directly communicates and aggregates data. However, FL training suffers from statistical heterogeneity as clients may have differing local data distributions. Large language models (LLMs) offer a potential solution to this issue of heterogeneity given that they have consistently been shown to be able to learn on vast amounts of noisy data. While LLMs are a promising development for resolving the consistent issue of non-I.I.D. Clients in federated settings exacerbate two other bottlenecks in FL: limited local computing and expensive communication. This thesis aims to develop efficient training methods for LLMs in FL. To this end, we employ two critical techniques in enabling efficient training. First, we use low-rank adaptation (LoRA) to reduce the computational load of local model training. Second, we communicate sparse updates throughout training to significantly cut down on communication costs. Taken together, our method reduces communication costs by up to 10x over vanilla LoRA and up to 5x over more complex sparse LoRA baselines while achieving greater utility. We emphasize the importance of carefully applying sparsity and picking effective rank and sparsity configurations for federated LLM training.</li>
</ul>

<h3>Title: Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Ruoyue Shen, Nakamasa Inoue, Koichi Shinoda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20563">https://arxiv.org/abs/2407.20563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20563">https://arxiv.org/pdf/2407.20563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20563]] Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question Answering(https://arxiv.org/abs/2407.20563)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual question answering (VQA) is the task of providing accurate answers to natural language questions based on visual input. Programmatic VQA (PVQA) models have been gaining attention recently. These use large language models (LLMs) to formulate executable programs that address questions requiring complex visual reasoning. However, there are challenges in enabling LLMs to comprehend the usage of image processing modules and generate relevant code. To overcome these challenges, this paper introduces PyramidCoder, a novel prompting framework for PVQA models. PyramidCoder consists of three hierarchical levels, each serving a distinct purpose: query rephrasing, code generation, and answer aggregation. Notably, PyramidCoder utilizes a single frozen LLM and pre-defined prompts at each level, eliminating the need for additional training and ensuring flexibility across various LLM architectures. Compared to the state-of-the-art PVQA model, our approach improves accuracy by at least 0.5% on the GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on the NLVR2 dataset.</li>
</ul>

<h3>Title: CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Tianshi Zheng, Jiaxin Bai, Yicheng Wang, Tianqing Fang, Yue Guo, Yauwai Yim, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20564">https://arxiv.org/abs/2407.20564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20564">https://arxiv.org/pdf/2407.20564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20564]] CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge(https://arxiv.org/abs/2407.20564)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have demonstrated impressive capabilities across various natural language processing tasks by acquiring rich factual knowledge from their broad training data, their ability to synthesize and logically reason with this knowledge in complex ways remains underexplored. In this work, we present a systematic evaluation of state-of-the-art LLMs' complex logical reasoning abilities through a novel benchmark of automatically generated complex reasoning questions over general domain and biomedical knowledge graphs. Our extensive experiments, employing diverse in-context learning techniques, reveal that LLMs excel at reasoning over general world knowledge but face significant challenges with specialized domain-specific knowledge. We find that prompting with explicit Chain-of-Thought demonstrations can substantially improve LLM performance on complex logical reasoning tasks with diverse logical operations. Interestingly, our controlled evaluations uncover an asymmetry where LLMs display proficiency at set union operations, but struggle considerably with set intersections - a key building block of logical reasoning. To foster further work, we will publicly release our evaluation benchmark and code.</li>
</ul>

<h3>Title: Comparison of Large Language Models for Generating Contextually Relevant Questions</h3>
<ul>
<li><strong>Authors: </strong>Ivo Lodovico Molina, Valdemar Švábenský, Tsubasa Minematsu, Li Chen, Fumiya Okubo, Atsushi Shimada</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20578">https://arxiv.org/abs/2407.20578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20578">https://arxiv.org/pdf/2407.20578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20578]] Comparison of Large Language Models for Generating Contextually Relevant Questions(https://arxiv.org/abs/2407.20578)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study explores the effectiveness of Large Language Models (LLMs) for Automatic Question Generation in educational settings. Three LLMs are compared in their ability to create questions from university slide text without fine-tuning. Questions were obtained in a two-step pipeline: first, answer phrases were extracted from slides using Llama 2-Chat 13B; then, the three models generated questions for each answer. To analyze whether the questions would be suitable in educational applications for students, a survey was conducted with 46 students who evaluated a total of 246 questions across five metrics: clarity, relevance, difficulty, slide relation, and question-answer alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan T5 XXL by a small margin, particularly in terms of clarity and question-answer alignment. GPT-3.5 especially excels at tailoring questions to match the input answers. The contribution of this research is the analysis of the capacity of LLMs for Automatic Question Generation in education.</li>
</ul>

<h3>Title: Pruning Large Language Models with Semi-Structural Adaptive Sparse Training</h3>
<ul>
<li><strong>Authors: </strong>Weiyu Huang, Guohao Jian, Yuezhou Hu, Jun Zhu, Jianfei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20584">https://arxiv.org/abs/2407.20584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20584">https://arxiv.org/pdf/2407.20584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20584]] Pruning Large Language Models with Semi-Structural Adaptive Sparse Training(https://arxiv.org/abs/2407.20584)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based Large Language Models (LLMs) have demonstrated remarkable success across various challenging tasks. However, the deployment of LLMs is hindered by their substantial parameter count and memory consumption. Recently, numerous studies have attempted to compress LLMs by pruning them using training-free methods. However, these pruned models often experience significant performance degradation on complex tasks. To address this issue, we propose a novel training pipeline for semi-structured sparse models, named Adaptive Sparse Trainer (AST). By distilling the knowledge stored in its dense counterpart, we prevent the sparse model from overfitting and ensure a stable training process. Moreover, AST allows the model to adaptively select better lottery tickets (e.g., masks) during training. Additionally, we discovered that adding extra well-initialized parameters can further enhance model performance with only a small increase in memory footprint. Our method significantly narrows the performance gap between dense and sparse models while maintaining limited computational cost. Furthermore, when combined with existing quantization methods, AST can compress language models by up to 16x compared to dense FP32 precision models with minimal performance loss. AST outperforms previous state-of-the-art methods by reducing the zero-shot accuracy gap between dense and semi-structured sparse models to 1.12% across multiple zero-shot tasks on Llama2-7B, using less than 0.4% of the pretraining tokens.</li>
</ul>

<h3>Title: Enhancing Agricultural Machinery Management through Advanced LLM Integration</h3>
<ul>
<li><strong>Authors: </strong>Emily Johnson, Noah Wilson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20588">https://arxiv.org/abs/2407.20588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20588">https://arxiv.org/pdf/2407.20588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20588]] Enhancing Agricultural Machinery Management through Advanced LLM Integration(https://arxiv.org/abs/2407.20588)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The integration of artificial intelligence into agricultural practices, specifically through Consultation on Intelligent Agricultural Machinery Management (CIAMM), has the potential to revolutionize efficiency and sustainability in farming. This paper introduces a novel approach that leverages large language models (LLMs), particularly GPT-4, combined with multi-round prompt engineering to enhance decision-making processes in agricultural machinery management. We systematically developed and refined prompts to guide the LLMs in generating precise and contextually relevant outputs. Our approach was evaluated using a manually curated dataset from various online sources, and performance was assessed with accuracy and GPT-4 Scores. Comparative experiments were conducted using LLama-2-70B, ChatGPT, and GPT-4 models, alongside baseline and state-of-the-art methods such as Chain of Thought (CoT) and Thought of Thought (ThoT). The results demonstrate that our method significantly outperforms these approaches, achieving higher accuracy and relevance in generated responses. This paper highlights the potential of advanced prompt engineering techniques in improving the robustness and applicability of AI in agricultural contexts.</li>
</ul>

<h3>Title: EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos</h3>
<ul>
<li><strong>Authors: </strong>Aashish Rai, Srinath Sridhar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20592">https://arxiv.org/abs/2407.20592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20592">https://arxiv.org/pdf/2407.20592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20592]] EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos(https://arxiv.org/abs/2407.20592)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce EgoSonics, a method to generate semantically meaningful and synchronized audio tracks conditioned on silent egocentric videos. Generating audio for silent egocentric videos could open new applications in virtual reality, assistive technologies, or for augmenting existing datasets. Existing work has been limited to domains like speech, music, or impact sounds and cannot easily capture the broad range of audio frequencies found in egocentric videos. EgoSonics addresses these limitations by building on the strength of latent diffusion models for conditioned audio synthesis. We first encode and process audio and video data into a form that is suitable for generation. The encoded data is used to train our model to generate audio tracks that capture the semantics of the input video. Our proposed SyncroNet builds on top of ControlNet to provide control signals that enables temporal synchronization to the synthesized audio. Extensive evaluations show that our model outperforms existing work in audio quality, and in our newly proposed synchronization evaluation method. Furthermore, we demonstrate downstream applications of our model in improving video summarization.</li>
</ul>

<h3>Title: Joint Diffusion Processes as an Inductive Bias in Sheaf Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Ferran Hernandez Caralt, Guillermo Bernárdez Gil, Iulia Duta, Pietro Liò, Eduard Alarcón Cot</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20597">https://arxiv.org/abs/2407.20597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20597">https://arxiv.org/pdf/2407.20597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20597]] Joint Diffusion Processes as an Inductive Bias in Sheaf Neural Networks(https://arxiv.org/abs/2407.20597)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Sheaf Neural Networks (SNNs) naturally extend Graph Neural Networks (GNNs) by endowing a cellular sheaf over the graph, equipping nodes and edges with vector spaces and defining linear mappings between them. While the attached geometric structure has proven to be useful in analyzing heterophily and oversmoothing, so far the methods by which the sheaf is computed do not always guarantee a good performance in such settings. In this work, drawing inspiration from opinion dynamics concepts, we propose two novel sheaf learning approaches that (i) provide a more intuitive understanding of the involved structure maps, (ii) introduce a useful inductive bias for heterophily and oversmoothing, and (iii) infer the sheaf in a way that does not scale with the number of features, thus using fewer learnable parameters than existing methods. In our evaluation, we show the limitations of the real-world benchmarks used so far on SNNs, and design a new synthetic task -- leveraging the symmetries of n-dimensional ellipsoids -- that enables us to better assess the strengths and weaknesses of sheaf-based models. Our extensive experimentation on these novel datasets reveals valuable insights into the scenarios and contexts where SNNs in general -- and our proposed approaches in particular -- can be beneficial.</li>
</ul>

<h3>Title: SharkTrack: an accurate, generalisable software for streamlining shark and ray underwater video analysis</h3>
<ul>
<li><strong>Authors: </strong>Filippo Varini, Francesco Ferretti, Jeremy Jenrette, Joel H. Gayford, Mark E. Bond, Matthew J. Witt, Michael R. Heithaus, Sophie Wilday, Ben Glocker</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20623">https://arxiv.org/abs/2407.20623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20623">https://arxiv.org/pdf/2407.20623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20623]] SharkTrack: an accurate, generalisable software for streamlining shark and ray underwater video analysis(https://arxiv.org/abs/2407.20623)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Elasmobranchs (sharks and rays) can be important components of marine ecosystems but are experiencing global population declines. Effective monitoring of these populations is essential to their protection. Baited Remote Underwater Video Stations (BRUVS) have been a key tool for monitoring, but require time-consuming manual analysis. To address these challenges, we developed SharkTrack, an AI-enhanced BRUVS analysis software. SharkTrack uses Convolutional Neural Networks and Multi-Object Tracking to detect and track elasmobranchs and provides an annotation pipeline to manually classify elasmobranch species and compute MaxN, the standard metric of relative abundance. We tested SharkTrack on BRUVS footage from locations unseen by the model during training. SharkTrack computed MaxN with 89% accuracy over 207 hours of footage. The semi-automatic SharkTrack pipeline required two minutes of manual classification per hour of video, a 97% reduction of manual BRUVS analysis time compared to traditional methods, estimated conservatively at one hour per hour of video. Furthermore, we demonstrate SharkTrack application across diverse marine ecosystems and elasmobranch species, an advancement compared to previous models, which were limited to specific species or locations. SharkTrack applications extend beyond BRUVS analysis, facilitating rapid annotation of unlabeled videos, aiding the development of further models to classify elasmobranch species. We provide public access to the software and an unprecedentedly diverse dataset, facilitating future research in an important area of marine conservation.</li>
</ul>

<h3>Title: Spiking-DD: Neuromorphic Event Camera based Driver Distraction Detection with Spiking Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Waseem Shariff, Paul Kielty, Joseph Lemley, Peter Corcoran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20633">https://arxiv.org/abs/2407.20633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20633">https://arxiv.org/pdf/2407.20633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20633]] Spiking-DD: Neuromorphic Event Camera based Driver Distraction Detection with Spiking Neural Network(https://arxiv.org/abs/2407.20633)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Event camera-based driver monitoring is emerging as a pivotal area of research, driven by its significant advantages such as rapid response, low latency, power efficiency, enhanced privacy, and prevention of undersampling. Effective detection of driver distraction is crucial in driver monitoring systems to enhance road safety and reduce accident rates. The integration of an optimized sensor such as Event Camera with an optimized network is essential for maximizing these benefits. This paper introduces the innovative concept of sensing without seeing to detect driver distraction, leveraging computationally efficient spiking neural networks (SNN). To the best of our knowledge, this study is the first to utilize event camera data with spiking neural networks for driver distraction. The proposed Spiking-DD network not only achieve state of the art performance but also exhibit fewer parameters and provides greater accuracy than current event-based methodologies.</li>
</ul>

<h3>Title: Improved Bounds for Pure Private Agnostic Learning: Item-Level and User-Level Privacy</h3>
<ul>
<li><strong>Authors: </strong>Bo Li, Wei Wang, Peng Ye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20640">https://arxiv.org/abs/2407.20640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20640">https://arxiv.org/pdf/2407.20640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20640]] Improved Bounds for Pure Private Agnostic Learning: Item-Level and User-Level Privacy(https://arxiv.org/abs/2407.20640)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Machine Learning has made remarkable progress in a wide range of fields. In many scenarios, learning is performed on datasets involving sensitive information, in which privacy protection is essential for learning algorithms. In this work, we study pure private learning in the agnostic model -- a framework reflecting the learning process in practice. We examine the number of users required under item-level (where each user contributes one example) and user-level (where each user contributes multiple examples) privacy and derive several improved upper bounds. For item-level privacy, our algorithm achieves a near optimal bound for general concept classes. We extend this to the user-level setting, rendering a tighter upper bound than the one proved by Ghazi et al. (2023). Lastly, we consider the problem of learning thresholds under user-level privacy and present an algorithm with a nearly tight user complexity.</li>
</ul>

<h3>Title: Effectively Leveraging CLIP for Generating Situational Summaries of Images and Videos</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Verma, Debaditya Roy, Basura Fernando</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20642">https://arxiv.org/abs/2407.20642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20642">https://arxiv.org/pdf/2407.20642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20642]] Effectively Leveraging CLIP for Generating Situational Summaries of Images and Videos(https://arxiv.org/abs/2407.20642)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Situation recognition refers to the ability of an agent to identify and understand various situations or contexts based on available information and sensory inputs. It involves the cognitive process of interpreting data from the environment to determine what is happening, what factors are involved, and what actions caused those situations. This interpretation of situations is formulated as a semantic role labeling problem in computer vision-based situation recognition. Situations depicted in images and videos hold pivotal information, essential for various applications like image and video captioning, multimedia retrieval, autonomous systems and event monitoring. However, existing methods often struggle with ambiguity and lack of context in generating meaningful and accurate predictions. Leveraging multimodal models such as CLIP, we propose ClipSitu, which sidesteps the need for full fine-tuning and achieves state-of-the-art results in situation recognition and localization tasks. ClipSitu harnesses CLIP-based image, verb, and role embeddings to predict nouns fulfilling all the roles associated with a verb, providing a comprehensive understanding of depicted scenarios. Through a cross-attention Transformer, ClipSitu XTF enhances the connection between semantic role queries and visual token representations, leading to superior performance in situation recognition. We also propose a verb-wise role prediction model with near-perfect accuracy to create an end-to-end framework for producing situational summaries for out-of-domain images. We show that situational summaries empower our ClipSitu models to produce structured descriptions with reduced ambiguity compared to generic captions. Finally, we extend ClipSitu to video situation recognition to showcase its versatility and produce comparable performance to state-of-the-art methods.</li>
</ul>

<h3>Title: No learning rates needed: Introducing SALSA -- Stable Armijo Line Search Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Philip Kenneweg, Tristan Kenneweg, Fabian Fumagalli, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20650">https://arxiv.org/abs/2407.20650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20650">https://arxiv.org/pdf/2407.20650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20650]] No learning rates needed: Introducing SALSA -- Stable Armijo Line Search Adaptation(https://arxiv.org/abs/2407.20650)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In recent studies, line search methods have been demonstrated to significantly enhance the performance of conventional stochastic gradient descent techniques across various datasets and architectures, while making an otherwise critical choice of learning rate schedule superfluous. In this paper, we identify problems of current state-of-the-art of line search methods, propose enhancements, and rigorously assess their effectiveness. Furthermore, we evaluate these methods on orders of magnitude larger datasets and more complex data domains than previously done. More specifically, we enhance the Armijo line search method by speeding up its computation and incorporating a momentum term into the Armijo criterion, making it better suited for stochastic mini-batching. Our optimization approach outperforms both the previous Armijo implementation and a tuned learning rate schedule for the Adam and SGD optimizers. Our evaluation covers a diverse range of architectures, such as Transformers, CNNs, and MLPs, as well as data domains, including NLP and image data. Our work is publicly available as a Python package, which provides a simple Pytorch optimizer.</li>
</ul>

<h3>Title: FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20653">https://arxiv.org/abs/2407.20653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20653">https://arxiv.org/pdf/2407.20653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20653]] FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks(https://arxiv.org/abs/2407.20653)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to generate adversarial examples that are robust in both cross-domain and cross-model settings. With that goal in mind, we propose two modules that are only employed during the training phase: a Frequency-Aware Domain Randomization (FADR) module to randomize domain-variant low- and high-range frequency components and a Frequency-Augmented Contrastive Learning (FACL) module to effectively separate domain-invariant mid-frequency features of clean and perturbed image. We demonstrate strong transferability of our generated adversarial perturbations through extensive cross-domain and cross-model experiments, while keeping the inference time complexity.</li>
</ul>

<h3>Title: Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian</h3>
<ul>
<li><strong>Authors: </strong>Serena Auriemma, Martina Miliani, Mauro Madeddu, Alessandro Bondielli, Lucia Passaro, Alessandro Lenci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20654">https://arxiv.org/abs/2407.20654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20654">https://arxiv.org/pdf/2407.20654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20654]] Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian(https://arxiv.org/abs/2407.20654)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Addressing the challenge of limited annotated data in specialized fields and low-resource languages is crucial for the effective use of Language Models (LMs). While most Large Language Models (LLMs) are trained on general-purpose English corpora, there is a notable gap in models specifically tailored for Italian, particularly for technical and bureaucratic jargon. This paper explores the feasibility of employing smaller, domain-specific encoder LMs alongside prompting techniques to enhance performance in these specialized contexts. Our study concentrates on the Italian bureaucratic and legal language, experimenting with both general-purpose and further pre-trained encoder-only models. We evaluated the models on downstream tasks such as document classification and entity typing and conducted intrinsic evaluations using Pseudo-Log-Likelihood. The results indicate that while further pre-trained models may show diminished robustness in general knowledge, they exhibit superior adaptability for domain-specific tasks, even in a zero-shot setting. Furthermore, the application of calibration techniques and in-domain verbalizers significantly enhances the efficacy of encoder models. These domain-specialized models prove to be particularly advantageous in scenarios where in-domain resources or expertise are scarce. In conclusion, our findings offer new insights into the use of Italian models in specialized contexts, which may have a significant impact on both research and industrial applications in the digital transformation era.</li>
</ul>

<h3>Title: Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20657">https://arxiv.org/abs/2407.20657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20657">https://arxiv.org/pdf/2407.20657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20657]] Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks(https://arxiv.org/abs/2407.20657)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>Recent vision-language foundation models, such as CLIP, have demonstrated superior capabilities in learning representations that can be transferable across diverse range of downstream tasks and domains. With the emergence of such powerful models, it has become crucial to effectively leverage their capabilities in tackling challenging vision tasks. On the other hand, only a few works have focused on devising adversarial examples that transfer well to both unknown domains and model architectures. In this paper, we propose a novel transfer attack method called PDCL-Attack, which leverages the CLIP model to enhance the transferability of adversarial perturbations generated by a generative model-based attack framework. Specifically, we formulate an effective prompt-driven feature guidance by harnessing the semantic representation power of text, particularly from the ground-truth class labels of input images. To the best of our knowledge, we are the first to introduce prompt learning to enhance the transferable generative attacks. Extensive experiments conducted across various cross-domain and cross-model settings empirically validate our approach, demonstrating its superiority over state-of-the-art methods.</li>
</ul>

<h3>Title: What makes for good morphology representations for spatial omics?</h3>
<ul>
<li><strong>Authors: </strong>Eduard Chelebian, Christophe Avenel, Carolina Wählby</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20660">https://arxiv.org/abs/2407.20660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20660">https://arxiv.org/pdf/2407.20660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20660]] What makes for good morphology representations for spatial omics?(https://arxiv.org/abs/2407.20660)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Spatial omics has transformed our understanding of tissue architecture by preserving spatial context of gene expression patterns. Simultaneously, advances in imaging AI have enabled extraction of morphological features describing the tissue. The intersection of spatial omics and imaging AI presents opportunities for a more holistic understanding. In this review we introduce a framework for categorizing spatial omics-morphology combination methods, focusing on how morphological features can be translated or integrated into spatial omics analyses. By translation we mean finding morphological features that spatially correlate with gene expression patterns with the purpose of predicting gene expression. Such features can be used to generate super-resolution gene expression maps or infer genetic information from clinical H&E-stained samples. By integration we mean finding morphological features that spatially complement gene expression patterns with the purpose of enriching information. Such features can be used to define spatial domains, especially where gene expression has preceded morphological changes and where morphology remains after gene expression. We discuss learning strategies and directions for further development of the field.</li>
</ul>

<h3>Title: DocXPand-25k: a large and diverse benchmark dataset for identity documents analysis</h3>
<ul>
<li><strong>Authors: </strong>Julien Lerouge, Guillaume Betmont, Thomas Bres, Evgeny Stepankevich, Alexis Bergès</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20662">https://arxiv.org/abs/2407.20662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20662">https://arxiv.org/pdf/2407.20662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20662]] DocXPand-25k: a large and diverse benchmark dataset for identity documents analysis(https://arxiv.org/abs/2407.20662)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Identity document (ID) image analysis has become essential for many online services, like bank account opening or insurance subscription. In recent years, much research has been conducted on subjects like document localization, text recognition and fraud detection, to achieve a level of accuracy reliable enough to automatize identity verification. However, there are only a few available datasets to benchmark ID analysis methods, mainly because of privacy restrictions, security requirements and legal reasons. In this paper, we present the DocXPand-25k dataset, which consists of 24,994 richly labeled IDs images, generated using custom-made vectorial templates representing nine fictitious ID designs, including four identity cards, two residence permits and three passports designs. These synthetic IDs feature artificially generated personal information (names, dates, identifiers, faces, barcodes, ...), and present a rich diversity in the visual layouts and textual contents. We collected about 5.8k diverse backgrounds coming from real-world photos, scans and screenshots of IDs to guarantee the variety of the backgrounds. The software we wrote to generate these images has been published (this https URL) under the terms of the MIT license, and our dataset has been published (this https URL) under the terms of the CC-BY-NC-SA 4.0 License.</li>
</ul>

<h3>Title: 3D-GRES: Generalized 3D Referring Expression Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Changli Wu, Yihang Liu, Jiayi Ji, Yiwei Ma, Haowei Wang, Gen Luo, Henghui Ding, Xiaoshuai Sun, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20664">https://arxiv.org/abs/2407.20664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20664">https://arxiv.org/pdf/2407.20664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20664]] 3D-GRES: Generalized 3D Referring Expression Segmentation(https://arxiv.org/abs/2407.20664)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D Referring Expression Segmentation (3D-RES) is dedicated to segmenting a specific instance within a 3D space based on a natural language description. However, current approaches are limited to segmenting a single target, restricting the versatility of the task. To overcome this limitation, we introduce Generalized 3D Referring Expression Segmentation (3D-GRES), which extends the capability to segment any number of instances based on natural language instructions. In addressing this broader task, we propose the Multi-Query Decoupled Interaction Network (MDIN), designed to break down multi-object segmentation tasks into simpler, individual segmentations. MDIN comprises two fundamental components: Text-driven Sparse Queries (TSQ) and Multi-object Decoupling Optimization (MDO). TSQ generates sparse point cloud features distributed over key targets as the initialization for queries. Meanwhile, MDO is tasked with assigning each target in multi-object scenarios to different queries while maintaining their semantic consistency. To adapt to this new task, we build a new dataset, namely Multi3DRes. Our comprehensive evaluations on this dataset demonstrate substantial enhancements over existing models, thus charting a new path for intricate multi-object 3D scene comprehension. The benchmark and code are available at this https URL.</li>
</ul>

<h3>Title: Label-Guided Prompt for Multi-label Few-shot Aspect Category Detection</h3>
<ul>
<li><strong>Authors: </strong>ChaoFeng Guan, YaoHui Zhu, Yu Bai, LingYun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20673">https://arxiv.org/abs/2407.20673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20673">https://arxiv.org/pdf/2407.20673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20673]] Label-Guided Prompt for Multi-label Few-shot Aspect Category Detection(https://arxiv.org/abs/2407.20673)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-label few-shot aspect category detection aims at identifying multiple aspect categories from sentences with a limited number of training instances. The representation of sentences and categories is a key issue in this task. Most of current methods extract keywords for the sentence representations and the category representations. Sentences often contain many category-independent words, which leads to suboptimal performance of keyword-based methods. Instead of directly extracting keywords, we propose a label-guided prompt method to represent sentences and categories. To be specific, we design label-specific prompts to represent sentences by combining crucial contextual and semantic information. Further, the label is introduced into a prompt to obtain category descriptions by utilizing a large language model. This kind of category descriptions contain the characteristics of the aspect categories, guiding the construction of discriminative category prototypes. Experimental results on two public datasets show that our method outperforms current state-of-the-art methods with a 3.86% - 4.75% improvement in the Macro-F1 score.</li>
</ul>

<h3>Title: The Susceptibility of Example-Based Explainability Methods to Class Outliers</h3>
<ul>
<li><strong>Authors: </strong>Ikhtiyor Nematov, Dimitris Sacharidis, Tomer Sagi, Katja Hose</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20678">https://arxiv.org/abs/2407.20678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20678">https://arxiv.org/pdf/2407.20678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20678]] The Susceptibility of Example-Based Explainability Methods to Class Outliers(https://arxiv.org/abs/2407.20678)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>This study explores the impact of class outliers on the effectiveness of example-based explainability methods for black-box machine learning models. We reformulate existing explainability evaluation metrics, such as correctness and relevance, specifically for example-based methods, and introduce a new metric, distinguishability. Using these metrics, we highlight the shortcomings of current example-based explainability methods, including those who attempt to suppress class outliers. We conduct experiments on two datasets, a text classification dataset and an image classification dataset, and evaluate the performance of four state-of-the-art explainability methods. Our findings underscore the need for robust techniques to tackle the challenges posed by class outliers.</li>
</ul>

<h3>Title: Detecting Causality in the Frequency Domain with Cross-Mapping Coherence</h3>
<ul>
<li><strong>Authors: </strong>Zsigmond Benkő, Bálint Varga, Marcell Stippinger, Zoltán Somogyvári</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.CD, physics.data-an, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20694">https://arxiv.org/abs/2407.20694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20694">https://arxiv.org/pdf/2407.20694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20694]] Detecting Causality in the Frequency Domain with Cross-Mapping Coherence(https://arxiv.org/abs/2407.20694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding causal relationships within a system is crucial for uncovering its underlying mechanisms. Causal discovery methods, which facilitate the construction of such models from time-series data, hold the potential to significantly advance scientific and engineering fields. This study introduces the Cross-Mapping Coherence (CMC) method, designed to reveal causal connections in the frequency domain between time series. CMC builds upon nonlinear state-space reconstruction and extends the Convergent Cross-Mapping algorithm to the frequency domain by utilizing coherence metrics for evaluation. We tested the Cross-Mapping Coherence method using simulations of logistic maps, Lorenz systems, Kuramoto oscillators, and the Wilson-Cowan model of the visual cortex. CMC accurately identified the direction of causal connections in all simulated scenarios. When applied to the Wilson-Cowan model, CMC yielded consistent results similar to spectral Granger causality. Furthermore, CMC exhibits high sensitivity in detecting weak connections, demonstrates sample efficiency, and maintains robustness in the presence of noise. In conclusion, the capability to determine directed causal influences across different frequency bands allows CMC to provide valuable insights into the dynamics of complex, nonlinear systems.</li>
</ul>

<h3>Title: Time Series Anomaly Detection with CNN for Environmental Sensors in Healthcare-IoT</h3>
<ul>
<li><strong>Authors: </strong>Mirza Akhi Khatun, Mangolika Bhattacharya, Ciarán Eising, Lubna Luxmi Dhirani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20695">https://arxiv.org/abs/2407.20695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20695">https://arxiv.org/pdf/2407.20695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20695]] Time Series Anomaly Detection with CNN for Environmental Sensors in Healthcare-IoT(https://arxiv.org/abs/2407.20695)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This research develops a new method to detect anomalies in time series data using Convolutional Neural Networks (CNNs) in healthcare-IoT. The proposed method creates a Distributed Denial of Service (DDoS) attack using an IoT network simulator, Cooja, which emulates environmental sensors such as temperature and humidity. CNNs detect anomalies in time series data, resulting in a 92\% accuracy in identifying possible attacks.</li>
</ul>

<h3>Title: PIP: Prototypes-Injected Prompt for Federated Class Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Anwar Ma'sum, Mahardhika Pratama, Savitha Ramasamy, Lin Liu, Habibullah Habibullah, Ryszard Kowalczyk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20705">https://arxiv.org/abs/2407.20705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20705">https://arxiv.org/pdf/2407.20705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20705]] PIP: Prototypes-Injected Prompt for Federated Class Incremental Learning(https://arxiv.org/abs/2407.20705)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Class Incremental Learning (FCIL) is a new direction in continual learning (CL) for addressing catastrophic forgetting and non-IID data distribution simultaneously. Existing FCIL methods call for high communication costs and exemplars from previous classes. We propose a novel rehearsal-free method for FCIL named prototypes-injected prompt (PIP) that involves 3 main ideas: a) prototype injection on prompt learning, b) prototype augmentation, and c) weighted Gaussian aggregation on the server side. Our experiment result shows that the proposed method outperforms the current state of the arts (SOTAs) with a significant improvement (up to 33%) in CIFAR100, MiniImageNet and TinyImageNet datasets. Our extensive analysis demonstrates the robustness of PIP in different task sizes, and the advantage of requiring smaller participating local clients, and smaller global rounds. For further study, source codes of PIP, baseline, and experimental logs are shared publicly in this https URL.</li>
</ul>

<h3>Title: SceneTeller: Language-to-3D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Başak Melis Öcal, Maxim Tatarchenko, Sezer Karaoglu, Theo Gevers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20727">https://arxiv.org/abs/2407.20727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20727">https://arxiv.org/pdf/2407.20727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20727]] SceneTeller: Language-to-3D Scene Generation(https://arxiv.org/abs/2407.20727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Designing high-quality indoor 3D scenes is important in many practical applications, such as room planning or game development. Conventionally, this has been a time-consuming process which requires both artistic skill and familiarity with professional software, making it hardly accessible for layman users. However, recent advances in generative AI have established solid foundation for democratizing 3D design. In this paper, we propose a pioneering approach for text-based 3D room design. Given a prompt in natural language describing the object placement in the room, our method produces a high-quality 3D scene corresponding to it. With an additional text prompt the users can change the appearance of the entire scene or of individual objects in it. Built using in-context learning, CAD model retrieval and 3D-Gaussian-Splatting-based stylization, our turnkey pipeline produces state-of-the-art 3D scenes, while being easy to use even for novices. Our project page is available at this https URL.</li>
</ul>

<h3>Title: Neural Fields for Continuous Periodic Motion Estimation in 4D Cardiovascular Imaging</h3>
<ul>
<li><strong>Authors: </strong>Simone Garzia, Patryk Rygiel, Sven Dummer, Filippo Cademartiri, Simona Celi, Jelmer M. Wolterink</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20728">https://arxiv.org/abs/2407.20728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20728">https://arxiv.org/pdf/2407.20728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20728]] Neural Fields for Continuous Periodic Motion Estimation in 4D Cardiovascular Imaging(https://arxiv.org/abs/2407.20728)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Time-resolved three-dimensional flow MRI (4D flow MRI) provides a unique non-invasive solution to visualize and quantify hemodynamics in blood vessels such as the aortic arch. However, most current analysis methods for arterial 4D flow MRI use static artery walls because of the difficulty in obtaining a full cycle segmentation. To overcome this limitation, we propose a neural fields-based method that directly estimates continuous periodic wall deformations throughout the cardiac cycle. For a 3D + time imaging dataset, we optimize an implicit neural representation (INR) that represents a time-dependent velocity vector field (VVF). An ODE solver is used to integrate the VVF into a deformation vector field (DVF), that can deform images, segmentation masks, or meshes over time, thereby visualizing and quantifying local wall motion patterns. To properly reflect the periodic nature of 3D + time cardiovascular data, we impose periodicity in two ways. First, by periodically encoding the time input to the INR, and hence VVF. Second, by regularizing the DVF. We demonstrate the effectiveness of this approach on synthetic data with different periodic patterns, ECG-gated CT, and 4D flow MRI data. The obtained method could be used to improve 4D flow MRI analysis.</li>
</ul>

<h3>Title: Adapting Safe-for-Work Classifier for Malaysian Language Text: Enhancing Alignment in LLM-Ops Framework</h3>
<ul>
<li><strong>Authors: </strong>Aisyah Razak, Ariff Nazhan, Kamarul Adha, Wan Adzhar Faiq Adzlan, Mas Aisyah Ahmad, Ammar Azman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20729">https://arxiv.org/abs/2407.20729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20729">https://arxiv.org/pdf/2407.20729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20729]] Adapting Safe-for-Work Classifier for Malaysian Language Text: Enhancing Alignment in LLM-Ops Framework(https://arxiv.org/abs/2407.20729)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into operational workflows (LLM-Ops), there is a pressing need for effective guardrails to ensure safe and aligned interactions, including the ability to detect potentially unsafe or inappropriate content across languages. However, existing safe-for-work classifiers are primarily focused on English text. To address this gap for the Malaysian language, we present a novel safe-for-work text classifier tailored specifically for Malaysian language content. By curating and annotating a first-of-its-kind dataset of Malaysian text spanning multiple content categories, we trained a classification model capable of identifying potentially unsafe material using state-of-the-art natural language processing techniques. This work represents an important step in enabling safer interactions and content filtering to mitigate potential risks and ensure responsible deployment of LLMs. To maximize accessibility and promote further research towards enhancing alignment in LLM-Ops for the Malaysian context, the model is publicly released at this https URL.</li>
</ul>

<h3>Title: Efficient Pareto Manifold Learning with Low-Rank Structure</h3>
<ul>
<li><strong>Authors: </strong>Weiyu Chen, James T. Kwok</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20734">https://arxiv.org/abs/2407.20734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20734">https://arxiv.org/pdf/2407.20734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20734]] Efficient Pareto Manifold Learning with Low-Rank Structure(https://arxiv.org/abs/2407.20734)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Multi-task learning, which optimizes performance across multiple tasks, is inherently a multi-objective optimization problem. Various algorithms are developed to provide discrete trade-off solutions on the Pareto front. Recently, continuous Pareto front approximations using a linear combination of base networks have emerged as a compelling strategy. However, it suffers from scalability issues when the number of tasks is large. To address this issue, we propose a novel approach that integrates a main network with several low-rank matrices to efficiently learn the Pareto manifold. It significantly reduces the number of parameters and facilitates the extraction of shared features. We also introduce orthogonal regularization to further bolster performance. Extensive experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines, especially on datasets with a large number of tasks.</li>
</ul>

<h3>Title: Meltemi: The first open Large Language Model for Greek</h3>
<ul>
<li><strong>Authors: </strong>Leon Voukoutis, Dimitris Roussis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, Vassilis Katsouros</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20743">https://arxiv.org/abs/2407.20743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20743">https://arxiv.org/pdf/2407.20743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20743]] Meltemi: The first open Large Language Model for Greek(https://arxiv.org/abs/2407.20743)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We describe the development and capabilities of Meltemi 7B, the first open Large Language Model for the Greek language. Meltemi 7B has 7 billion parameters and is trained on a 40 billion token Greek corpus. For the development of Meltemi 7B, we adapt Mistral, by continuous pretraining on the Greek Corpus. Meltemi 7B contains up-to-date information up to September 2023. Furthermore, we have translated and curated a Greek instruction corpus, which has been used for the instruction-tuning of a chat model, named Meltemi 7B Instruct. Special care has been given to the alignment and the removal of toxic content for the Meltemi 7B Instruct. The developed models are evaluated on a broad set of collected evaluation corpora, and examples of prompts and responses are presented. Both Meltemi 7B and Meltemi 7B Instruct are available at this https URL under the Apache 2.0 license.</li>
</ul>

<h3>Title: SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Liu, Hao Liang, Wentao Xiong, Qinhan Yu, Conghui He, Bin Cui, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20756">https://arxiv.org/abs/2407.20756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20756">https://arxiv.org/pdf/2407.20756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20756]] SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models(https://arxiv.org/abs/2407.20756)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recently, with the rise of web images, managing and understanding large-scale image datasets has become increasingly important. Vision Large Language Models (VLLMs) have recently emerged due to their robust vision-understanding capabilities. However, training these models requires vast amounts of data, posing challenges to efficiency, effectiveness, data quality, and privacy. In this paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs. Unlike existing methods that generate captions from images, SynthVLM employs advanced diffusion models and high-quality captions to automatically generate and select high-resolution images from captions, creating precisely aligned image-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA) performance on various vision question answering tasks, maintaining high alignment quality and preserving advanced language abilities. Moreover, SynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in performance while significantly reducing computational overhead. Crucially, our method's reliance on purely generated data ensures the preservation of privacy, achieving SoTA performance with just 100k data points (only 18% of the official dataset size).</li>
</ul>

<h3>Title: HyperMM : Robust Multimodal Learning with Varying-sized Inputs</h3>
<ul>
<li><strong>Authors: </strong>Hava Chaptoukaev, Vincenzo Marcianó, Francesco Galati, Maria A. Zuluaga</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20768">https://arxiv.org/abs/2407.20768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20768">https://arxiv.org/pdf/2407.20768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20768]] HyperMM : Robust Multimodal Learning with Varying-sized Inputs(https://arxiv.org/abs/2407.20768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Combining multiple modalities carrying complementary information through multimodal learning (MML) has shown considerable benefits for diagnosing multiple pathologies. However, the robustness of multimodal models to missing modalities is often overlooked. Most works assume modality completeness in the input data, while in clinical practice, it is common to have incomplete modalities. Existing solutions that address this issue rely on modality imputation strategies before using supervised learning models. These strategies, however, are complex, computationally costly and can strongly impact subsequent prediction models. Hence, they should be used with parsimony in sensitive applications such as healthcare. We propose HyperMM, an end-to-end framework designed for learning with varying-sized inputs. Specifically, we focus on the task of supervised MML with missing imaging modalities without using imputation before training. We introduce a novel strategy for training a universal feature extractor using a conditional hypernetwork, and propose a permutation-invariant neural network that can handle inputs of varying dimensions to process the extracted features, in a two-phase task-agnostic framework. We experimentally demonstrate the advantages of our method in two tasks: Alzheimer's disease detection and breast cancer classification. We demonstrate that our strategy is robust to high rates of missing data and that its flexibility allows it to handle varying-sized datasets beyond the scenario of missing modalities.</li>
</ul>

<h3>Title: Interpretable Pre-Trained Transformers for Heart Time-Series Data</h3>
<ul>
<li><strong>Authors: </strong>Harry J. Davies, James Monsen, Danilo P. Mandic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20775">https://arxiv.org/abs/2407.20775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20775">https://arxiv.org/pdf/2407.20775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20775]] Interpretable Pre-Trained Transformers for Heart Time-Series Data(https://arxiv.org/abs/2407.20775)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Decoder-only transformers are the backbone of the popular generative pre-trained transformer (GPT) series of large language models. In this work, we apply the same framework to periodic heart time-series data to create two pre-trained general purpose cardiac models, namely PPG-PT and ECG-PT. We demonstrate that both such pre-trained models are fully interpretable. This is achieved firstly through aggregate attention maps which show that the model focuses on similar points in previous cardiac cycles in order to make predictions and gradually broadens its attention in deeper layers. Next, tokens with the same value, that occur at different distinct points in the ECG and PPG cycle, form separate clusters in high dimensional space based on their phase as they propagate through the transformer blocks. Finally, we highlight that individual attention heads respond to specific physiologically relevent features, such as the dicrotic notch in PPG and the P-wave in ECG. It is also demonstrated that these pre-trained models can be easily fine-tuned for tasks such as classification of atrial fibrillation. In this specific example, the fine-tuning took 11 minutes of computer time, and achieved a leave-one-subject-out AUCs of 0.99 and 0.93 for ECG and PPG respectively. Importantly, these fine-tuned models are also fully explainable, with attention shifting to regions in the context that are strongly indicative of atrial fibrillation.</li>
</ul>

<h3>Title: Inverse Problems with Diffusion Models: A MAP Estimation Perspective</h3>
<ul>
<li><strong>Authors: </strong>Sai bharath chandra Gutha, Hossein Azizpour, Ricardo Vinuesa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20784">https://arxiv.org/abs/2407.20784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20784">https://arxiv.org/pdf/2407.20784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20784]] Inverse Problems with Diffusion Models: A MAP Estimation Perspective(https://arxiv.org/abs/2407.20784)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inverse problems have many applications in science and engineering. In Computer vision, several image restoration tasks such as inpainting, deblurring, and super-resolution can be formally modeled as inverse problems. Recently, methods have been developed for solving inverse problems that only leverage a pre-trained unconditional diffusion model and do not require additional task-specific training. In such methods, however, the inherent intractability of determining the conditional score function during the reverse diffusion process poses a real challenge, leaving the methods to settle with an approximation instead, which affects their performance in practice. Here, we propose a MAP estimation framework to model the reverse conditional generation process of a continuous time diffusion model as an optimization process of the underlying MAP objective, whose gradient term is tractable. In theory, the proposed framework can be applied to solve general inverse problems using gradient-based optimization methods. However, given the highly non-convex nature of the loss objective, finding a perfect gradient-based optimization algorithm can be quite challenging, nevertheless, our framework offers several potential research directions. We use our proposed formulation and develop empirically effective algorithms for solving noiseless and noisy image inpainting tasks. We validate our proposed algorithms with extensive experiments across diverse mask settings.</li>
</ul>

<h3>Title: Retinex-Diffusion: On Controlling Illumination Conditions in Diffusion Models via Retinex Theory</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Xing, Vincent Tao Hu, Jan Hendrik Metzen, Konrad Groh, Sezer Karaoglu, Theo Gevers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20785">https://arxiv.org/abs/2407.20785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20785">https://arxiv.org/pdf/2407.20785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20785]] Retinex-Diffusion: On Controlling Illumination Conditions in Diffusion Models via Retinex Theory(https://arxiv.org/abs/2407.20785)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to illumination manipulation in diffusion models, addressing the gap in conditional image generation with a focus on lighting conditions. We conceptualize the diffusion model as a black-box image render and strategically decompose its energy function in alignment with the image formation model. Our method effectively separates and controls illumination-related properties during the generative process. It generates images with realistic illumination effects, including cast shadow, soft shadow, and inter-reflections. Remarkably, it achieves this without the necessity for learning intrinsic decomposition, finding directions in latent space, or undergoing additional training with new datasets.</li>
</ul>

<h3>Title: Be aware of overfitting by hyperparameter optimization!</h3>
<ul>
<li><strong>Authors: </strong>Igor V. Tetko, Ruud van Deursen, Guillaume Godin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20786">https://arxiv.org/abs/2407.20786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20786">https://arxiv.org/pdf/2407.20786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20786]] Be aware of overfitting by hyperparameter optimization!(https://arxiv.org/abs/2407.20786)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Hyperparameter optimization is very frequently employed in machine learning. However, an optimization of a large space of parameters could result in overfitting of models. In recent studies on solubility prediction the authors collected seven thermodynamic and kinetic solubility datasets from different data sources. They used state-of-the-art graph-based methods and compared models developed for each dataset using different data cleaning protocols and hyperparameter optimization. In our study we showed that hyperparameter optimization did not always result in better models, possibly due to overfitting when using the same statistical measures. Similar results could be calculated using pre-set hyperparameters, reducing the computational effort by around 10,000 times. We also extended the previous analysis by adding a representation learning method based on Natural Language Processing of smiles called Transformer CNN. We show that across all analyzed sets using exactly the same protocol, Transformer CNN provided better results than graph-based methods for 26 out of 28 pairwise comparisons by using only a tiny fraction of time as compared to other methods. Last but not least we stressed the importance of comparing calculation results using exactly the same statistical measures.</li>
</ul>

<h3>Title: Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Norman Di Palo, Leonard Hasenclever, Jan Humplik, Arunkumar Byravan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20798">https://arxiv.org/abs/2407.20798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20798">https://arxiv.org/pdf/2407.20798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20798]] Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning(https://arxiv.org/abs/2407.20798)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>We introduce Diffusion Augmented Agents (DAAG), a novel framework that leverages large language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning for embodied agents. DAAG hindsight relabels the agent's past experience by using diffusion models to transform videos in a temporally and geometrically consistent way to align with target instructions with a technique we call Hindsight Experience Augmentation. A large language model orchestrates this autonomous process without requiring human supervision, making it well-suited for lifelong learning scenarios. The framework reduces the amount of reward-labeled data needed to 1) finetune a vision language model that acts as a reward detector, and 2) train RL agents on new tasks. We demonstrate the sample efficiency gains of DAAG in simulated robotics environments involving manipulation and navigation. Our results show that DAAG improves learning of reward detectors, transferring past experience, and acquiring new tasks - key abilities for developing efficient lifelong learning agents. Supplementary material and visualizations are available on our website this https URL</li>
</ul>

<h3>Title: SpotFormer: Multi-Scale Spatio-Temporal Transformer for Facial Expression Spotting</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Deng, Hideaki Hayashi, Hajime Nagahara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20799">https://arxiv.org/abs/2407.20799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20799">https://arxiv.org/pdf/2407.20799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20799]] SpotFormer: Multi-Scale Spatio-Temporal Transformer for Facial Expression Spotting(https://arxiv.org/abs/2407.20799)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Facial expression spotting, identifying periods where facial expressions occur in a video, is a significant yet challenging task in facial expression analysis. The issues of irrelevant facial movements and the challenge of detecting subtle motions in micro-expressions remain unresolved, hindering accurate expression spotting. In this paper, we propose an efficient framework for facial expression spotting. First, we propose a Sliding Window-based Multi-Resolution Optical flow (SW-MRO) feature, which calculates multi-resolution optical flow of the input image sequence within compact sliding windows. The window length is tailored to perceive complete micro-expressions and distinguish between general macro- and micro-expressions. SW-MRO can effectively reveal subtle motions while avoiding severe head movement problems. Second, we propose SpotFormer, a multi-scale spatio-temporal Transformer that simultaneously encodes spatio-temporal relationships of the SW-MRO features for accurate frame-level probability estimation. In SpotFormer, our proposed Facial Local Graph Pooling (FLGP) and convolutional layers are applied for multi-scale spatio-temporal feature extraction. We show the validity of the architecture of SpotFormer by comparing it with several model variants. Third, we introduce supervised contrastive learning into SpotFormer to enhance the discriminability between different types of expressions. Extensive experiments on SAMM-LV and CAS(ME)^2 show that our method outperforms state-of-the-art models, particularly in micro-expression spotting.</li>
</ul>

<h3>Title: Robust Load Prediction of Power Network Clusters Based on Cloud-Model-Improved Transformer</h3>
<ul>
<li><strong>Authors: </strong>Cheng Jiang, Gang Lu, Xue Ma, Di Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20817">https://arxiv.org/abs/2407.20817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20817">https://arxiv.org/pdf/2407.20817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20817]] Robust Load Prediction of Power Network Clusters Based on Cloud-Model-Improved Transformer(https://arxiv.org/abs/2407.20817)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Load data from power network clusters indicates economic development in each area, crucial for predicting regional trends and guiding power enterprise decisions. The Transformer model, a leading method for load prediction, faces challenges modeling historical data due to variables like weather, events, festivals, and data volatility. To tackle this, the cloud model's fuzzy feature is utilized to manage uncertainties effectively. Presenting an innovative approach, the Cloud Model Improved Transformer (CMIT) method integrates the Transformer model with the cloud model utilizing the particle swarm optimization algorithm, with the aim of achieving robust and precise power load predictions. Through comparative experiments conducted on 31 real datasets within a power network cluster, it is demonstrated that CMIT significantly surpasses the Transformer model in terms of prediction accuracy, thereby highlighting its effectiveness in enhancing forecasting capabilities within the power network cluster sector.</li>
</ul>

<h3>Title: Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing</h3>
<ul>
<li><strong>Authors: </strong>Eugenio Lomurno, Matteo Matteucci</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20830">https://arxiv.org/abs/2407.20830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20830">https://arxiv.org/pdf/2407.20830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20830]] Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing(https://arxiv.org/abs/2407.20830)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning has emerged as a paradigm for collaborative learning, enabling the development of robust models without the need to centralise sensitive data. However, conventional federated learning techniques have privacy and security vulnerabilities due to the exposure of models, parameters or updates, which can be exploited as an attack surface. This paper presents Federated Knowledge Recycling (FedKR), a cross-silo federated learning approach that uses locally generated synthetic data to facilitate collaboration between institutions. FedKR combines advanced data generation techniques with a dynamic aggregation process to provide greater security against privacy attacks than existing methods, significantly reducing the attack surface. Experimental results on generic and medical datasets show that FedKR achieves competitive performance, with an average improvement in accuracy of 4.24% compared to training models from local data, demonstrating particular effectiveness in data scarcity scenarios.</li>
</ul>

<h3>Title: Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yunfeng Diao, Naixin Zhai, Changtao Miao, Xun Yang, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20836">https://arxiv.org/abs/2407.20836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20836">https://arxiv.org/pdf/2407.20836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20836]] Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks(https://arxiv.org/abs/2407.20836)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of these AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. For the task of AIGI detection, we propose a new attack containing two main parts. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous models, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as frequency-based post-train Bayesian attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario.</li>
</ul>

<h3>Title: DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain Feature Extraction and Interaction Attention</h3>
<ul>
<li><strong>Authors: </strong>Wei Wang, Jixing He, Xin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20843">https://arxiv.org/abs/2407.20843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20843">https://arxiv.org/pdf/2407.20843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20843]] DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain Feature Extraction and Interaction Attention(https://arxiv.org/abs/2407.20843)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>It is helpful in preventing colorectal cancer to detect and treat polyps in the gastrointestinal tract early. However, there have been few studies to date on designing polyp image classification networks that balance efficiency and accuracy. This challenge is mainly attributed to the fact that polyps are similar to other pathologies and have complex features influenced by texture, color, and morphology. In this paper, we propose a novel network DFE-IANet based on both spectral transformation and feature interaction. Firstly, to extract detailed features and multi-scale features, the features are transformed by the multi-scale frequency domain feature extraction (MSFD) block to extract texture details at the fine-grained level in the frequency domain. Secondly, the multi-scale interaction attention (MSIA) block is designed to enhance the network's capability of extracting critical features. This block introduces multi-scale features into self-attention, aiming to adaptively guide the network to concentrate on vital regions. Finally, with a compact parameter of only 4M, DFE-IANet outperforms the latest and classical networks in terms of efficiency. Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results on the challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of 93.94%. This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%, and VMamba by 1.88%. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Hongjia Zhai, Gan Huang, Qirui Hu, Guanglin Li, Hujun Bao, Guofeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20853">https://arxiv.org/abs/2407.20853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20853">https://arxiv.org/pdf/2407.20853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20853]] NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene Understanding(https://arxiv.org/abs/2407.20853)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, the paradigm of neural implicit representations has gained substantial attention in the field of Simultaneous Localization and Mapping (SLAM). However, a notable gap exists in the existing approaches when it comes to scene understanding. In this paper, we introduce NIS-SLAM, an efficient neural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D segmentation network to learn consistent semantic representations. Specifically, for high-fidelity surface reconstruction and spatial consistent scene understanding, we combine high-frequency multi-resolution tetrahedron-based features and low-frequency positional encoding as the implicit scene representations. Besides, to address the inconsistency of 2D segmentation results from multiple views, we propose a fusion strategy that integrates the semantic probabilities from previous non-keyframes into keyframes to achieve consistent semantic learning. Furthermore, we implement a confidence-based pixel sampling and progressive optimization weight function for robust camera tracking. Extensive experimental results on various datasets show the better or more competitive performance of our system when compared to other existing neural dense implicit RGB-D SLAM approaches. Finally, we also show that our approach can be used in augmented reality applications. Project page: \href{this https URL}{this https URL\_slam}.</li>
</ul>

<h3>Title: DeTurb: Atmospheric Turbulence Mitigation with Deformable 3D Convolutions and 3D Swin Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Zou, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20855">https://arxiv.org/abs/2407.20855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20855">https://arxiv.org/pdf/2407.20855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20855]] DeTurb: Atmospheric Turbulence Mitigation with Deformable 3D Convolutions and 3D Swin Transformers(https://arxiv.org/abs/2407.20855)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Atmospheric turbulence in long-range imaging significantly degrades the quality and fidelity of captured scenes due to random variations in both spatial and temporal dimensions. These distortions present a formidable challenge across various applications, from surveillance to astronomy, necessitating robust mitigation strategies. While model-based approaches achieve good results, they are very slow. Deep learning approaches show promise in image and video restoration but have struggled to address these spatiotemporal variant distortions effectively. This paper proposes a new framework that combines geometric restoration with an enhancement module. Random perturbations and geometric distortion are removed using a pyramid architecture with deformable 3D convolutions, resulting in aligned frames. These frames are then used to reconstruct a sharp, clear image via a multi-scale architecture of 3D Swin Transformers. The proposed framework demonstrates superior performance over the state of the art for both synthetic and real atmospheric turbulence effects, with reasonable speed and model size.</li>
</ul>

<h3>Title: Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification</h3>
<ul>
<li><strong>Authors: </strong>Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20859">https://arxiv.org/abs/2407.20859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20859">https://arxiv.org/pdf/2407.20859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20859]] Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification(https://arxiv.org/abs/2407.20859)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Recently, autonomous agents built on large language models (LLMs) have experienced significant development and are being deployed in real-world applications. These agents can extend the base LLM's capabilities in multiple ways. For example, a well-built agent using GPT-3.5-Turbo as its core can outperform the more advanced GPT-4 model by leveraging external components. More importantly, the usage of tools enables these systems to perform actions in the real world, moving from merely generating text to actively interacting with their environment. Given the agents' practical applications and their ability to execute consequential actions, it is crucial to assess potential vulnerabilities. Such autonomous systems can cause more severe damage than a standalone language model if compromised. While some existing research has explored harmful actions by LLM agents, our study approaches the vulnerability from a different perspective. We introduce a new type of attack that causes malfunctions by misleading the agent into executing repetitive or irrelevant actions. We conduct comprehensive evaluations using various attack methods, surfaces, and properties to pinpoint areas of susceptibility. Our experiments reveal that these attacks can induce failure rates exceeding 80\% in multiple scenarios. Through attacks on implemented and deployable agents in multi-agent scenarios, we accentuate the realistic risks associated with these vulnerabilities. To mitigate such attacks, we propose self-examination detection methods. However, our findings indicate these attacks are difficult to detect effectively using LLMs alone, highlighting the substantial risks associated with this vulnerability.</li>
</ul>

<h3>Title: Automatic Die Studies for Ancient Numismatics</h3>
<ul>
<li><strong>Authors: </strong>Clément Cornet, Héloïse Aumaître, Romaric Besançon, Julien Olivier, Thomas Faucher, Hervé Le Borgne</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20876">https://arxiv.org/abs/2407.20876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20876">https://arxiv.org/pdf/2407.20876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20876]] Automatic Die Studies for Ancient Numismatics(https://arxiv.org/abs/2407.20876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Die studies are fundamental to quantifying ancient monetary production, providing insights into the relationship between coinage, politics, and history. The process requires tedious manual work, which limits the size of the corpora that can be studied. Few works have attempted to automate this task, and none have been properly released and evaluated from a computer vision perspective. We propose a fully automatic approach that introduces several innovations compared to previous methods. We rely on fast and robust local descriptors matching that is set automatically. Second, the core of our proposal is a clustering-based approach that uses an intrinsic metric (that does not need the ground truth labels) to determine its critical hyper-parameters. We validate the approach on two corpora of Greek coins, propose an automatic implementation and evaluation of previous baselines, and show that our approach significantly outperforms them.</li>
</ul>

<h3>Title: Effective Black Box Testing of Sentiment Analysis Classification Networks</h3>
<ul>
<li><strong>Authors: </strong>Parsa Karbasizadeh, Fathiyeh Faghih, Pouria Golshanrad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20884">https://arxiv.org/abs/2407.20884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20884">https://arxiv.org/pdf/2407.20884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20884]] Effective Black Box Testing of Sentiment Analysis Classification Networks(https://arxiv.org/abs/2407.20884)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based neural networks have demonstrated remarkable performance in natural language processing tasks such as sentiment analysis. Nevertheless, the issue of ensuring the dependability of these complicated architectures through comprehensive testing is still open. This paper presents a collection of coverage criteria specifically designed to assess test suites created for transformer-based sentiment analysis networks. Our approach utilizes input space partitioning, a black-box method, by considering emotionally relevant linguistic features such as verbs, adjectives, adverbs, and nouns. In order to effectively produce test cases that encompass a wide range of emotional elements, we utilize the k-projection coverage metric. This metric minimizes the complexity of the problem by examining subsets of k features at the same time, hence reducing dimensionality. Large language models are employed to generate sentences that display specific combinations of emotional features. The findings from experiments obtained from a sentiment analysis dataset illustrate that our criteria and generated tests have led to an average increase of 16\% in test coverage. In addition, there is a corresponding average decrease of 6.5\% in model accuracy, showing the ability to identify vulnerabilities. Our work provides a foundation for improving the dependability of transformer-based sentiment analysis systems through comprehensive test evaluation.</li>
</ul>

<h3>Title: Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Bao Gia Doan, Afshar Shamsi, Xiao-Yu Guo, Arash Mohammadi, Hamid Alinejad-Rokny, Dino Sejdinovic, Damith C. Ranasinghe, Ehsan Abbasnejad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20891">https://arxiv.org/abs/2407.20891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20891">https://arxiv.org/pdf/2407.20891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20891]] Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian Neural Networks(https://arxiv.org/abs/2407.20891)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Computational complexity of Bayesian learning is impeding its adoption in practical, large-scale tasks. Despite demonstrations of significant merits such as improved robustness and resilience to unseen or out-of-distribution inputs over their non- Bayesian counterparts, their practical use has faded to near insignificance. In this study, we introduce an innovative framework to mitigate the computational burden of Bayesian neural networks (BNNs). Our approach follows the principle of Bayesian techniques based on deep ensembles, but significantly reduces their cost via multiple low-rank perturbations of parameters arising from a pre-trained neural network. Both vanilla version of ensembles as well as more sophisticated schemes such as Bayesian learning with Stein Variational Gradient Descent (SVGD), previously deemed impractical for large models, can be seamlessly implemented within the proposed framework, called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a dramatic reduction in the number of trainable parameters required to approximate a Bayesian posterior; and ii) it not only maintains, but in some instances, surpasses the performance of conventional Bayesian learning methods and non-Bayesian baselines. Our results with large-scale tasks such as ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the effectiveness and versatility of Bella in building highly scalable and practical Bayesian deep models for real-world applications.</li>
</ul>

<h3>Title: MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with Electrocardiography Using Mamba Capsule Network</h3>
<ul>
<li><strong>Authors: </strong>Yinlong Xu, Xiaoqiang Liu, Zitai Kong, Yixuan Wu, Yue Wang, Yingzhou Lu, Honghao Gao, Jian Wu, Hongxia Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20893">https://arxiv.org/abs/2407.20893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20893">https://arxiv.org/pdf/2407.20893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20893]] MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with Electrocardiography Using Mamba Capsule Network(https://arxiv.org/abs/2407.20893)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, explainability</a></li>
<li><strong>Abstract: </strong>Cardiac arrhythmia, a condition characterized by irregular heartbeats, often serves as an early indication of various heart ailments. With the advent of deep learning, numerous innovative models have been introduced for diagnosing arrhythmias using Electrocardiogram (ECG) signals. However, recent studies solely focus on the performance of models, neglecting the interpretation of their results. This leads to a considerable lack of transparency, posing a significant risk in the actual diagnostic process. To solve this problem, this paper introduces MambaCapsule, a deep neural networks for ECG arrhythmias classification, which increases the explainability of the model while enhancing the accuracy.Our model utilizes Mamba for feature extraction and Capsule networks for prediction, providing not only a confidence score but also signal features. Akin to the processing mechanism of human brain, the model learns signal features and their relationship between them by reconstructing ECG signals in the predicted selection. The model evaluation was conducted on MIT-BIH and PTB dataset, following the AAMI standard. MambaCapsule has achieved a total accuracy of 99.54% and 99.59% on the test sets respectively. These results demonstrate the promising performance of under the standard test protocol.</li>
</ul>

<h3>Title: Automated Review Generation Method Based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Zhi-Jian Zhao, Jinlong Gong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20906">https://arxiv.org/abs/2407.20906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20906">https://arxiv.org/pdf/2407.20906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20906]] Automated Review Generation Method Based on Large Language Models(https://arxiv.org/abs/2407.20906)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Literature research, vital for scientific advancement, is overwhelmed by the vast ocean of available information. Addressing this, we propose an automated review generation method based on Large Language Models (LLMs) to streamline literature processing and reduce cognitive load. In case study on propane dehydrogenation (PDH) catalysts, our method swiftly generated comprehensive reviews from 343 articles, averaging seconds per article per LLM account. Extended analysis of 1041 articles provided deep insights into catalysts' composition, structure, and performance. Recognizing LLMs' hallucinations, we employed a multi-layered quality control strategy, ensuring our method's reliability and effective hallucination mitigation. Expert verification confirms the accuracy and citation integrity of generated reviews, demonstrating LLM hallucination risks reduced to below 0.5% with over 95% confidence. Released Windows application enables one-click review generation, aiding researchers in tracking advancements and recommending literature. This approach showcases LLMs' role in enhancing scientific research productivity and sets the stage for further exploration.</li>
</ul>

<h3>Title: Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering</h3>
<ul>
<li><strong>Authors: </strong>Yanpeng Zhao, Yiwei Hao, Siyu Gao, Yunbo Wang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20908">https://arxiv.org/abs/2407.20908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20908">https://arxiv.org/pdf/2407.20908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20908]] Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering(https://arxiv.org/abs/2407.20908)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.</li>
</ul>

<h3>Title: What Are Good Positional Encodings for Directed Graphs?</h3>
<ul>
<li><strong>Authors: </strong>Yinan Huang, Haoyu Wang, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20912">https://arxiv.org/abs/2407.20912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20912">https://arxiv.org/pdf/2407.20912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20912]] What Are Good Positional Encodings for Directed Graphs?(https://arxiv.org/abs/2407.20912)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Positional encodings (PE) for graphs are essential in constructing powerful and expressive graph neural networks and graph transformers as they effectively capture relative spatial relations between nodes. While PEs for undirected graphs have been extensively studied, those for directed graphs remain largely unexplored, despite the fundamental role of directed graphs in representing entities with strong logical dependencies, such as those in program analysis and circuit designs. This work studies the design of PEs for directed graphs that are expressive to represent desired directed spatial relations. We first propose walk profile, a generalization of walk counting sequence to directed graphs. We identify limitations in existing PE methods, including symmetrized Laplacian PE, Singular Value Decomposition PE, and Magnetic Laplacian PE, in their ability to express walk profiles. To address these limitations, we propose the Multi-q Magnetic Laplacian PE, which extends Magnetic Laplacian PE with multiple potential factors. This simple variant turns out to be capable of provably expressing walk profiles. Furthermore, we generalize previous basis-invariant and stable networks to handle complex-domain PEs decomposed from Magnetic Laplacians. Our numerical experiments demonstrate the effectiveness of Multi-q Magnetic Laplacian PE with a stable neural architecture, outperforming previous PE methods (with stable networks) on predicting directed distances/walk profiles, sorting network satisfiability, and on general circuit benchmarks. Our code is available at this https URL.</li>
</ul>

<h3>Title: SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei, Stan Z. Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20920">https://arxiv.org/abs/2407.20920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20920">https://arxiv.org/pdf/2407.20920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20920]] SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition(https://arxiv.org/abs/2407.20920)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Multi-label image recognition is a fundamental task in computer vision. Recently, Vision-Language Models (VLMs) have made notable advancements in this area. However, previous methods fail to effectively leverage the rich knowledge in language models and often incorporate label semantics into visual features unidirectionally. To overcome these problems, we propose a Split-and-Synthesize Prompting with Gated Alignments (SSPA) framework to amplify the potential of VLMs. Specifically, we develop an in-context learning approach to associate the inherent knowledge from LLMs. Then we propose a novel Split-and-Synthesize Prompting (SSP) strategy to first model the generic knowledge and downstream label semantics individually and then aggregate them carefully through the quaternion network. Moreover, we present Gated Dual-Modal Alignments (GDMA) to bidirectionally interact visual and linguistic modalities while eliminating redundant cross-modal information, enabling more efficient region-level alignments. Rather than making the final prediction by a sharp manner in previous works, we propose a soft aggregator to jointly consider results from all image regions. With the help of flexible prompting and gated alignments, SSPA is generalizable to specific domains. Extensive experiments on nine datasets from three domains (i.e., natural, pedestrian attributes and remote sensing) demonstrate the state-of-the-art performance of SSPA. Further analyses verify the effectiveness of SSP and the interpretability of GDMA. The code will be made public.</li>
</ul>

<h3>Title: Learning Ordinality in Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Rafael Cristino, Ricardo P. M. Cruz, Jaime S. Cardoso</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20959">https://arxiv.org/abs/2407.20959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20959">https://arxiv.org/pdf/2407.20959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20959]] Learning Ordinality in Semantic Segmentation(https://arxiv.org/abs/2407.20959)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation consists of predicting a semantic label for each image pixel. Conventional deep learning models do not take advantage of ordinal relations that might exist in the domain at hand. For example, it is known that the pupil is inside the iris, and the lane markings are inside the road. Such domain knowledge can be employed as constraints to make the model more robust. The current literature on this topic has explored pixel-wise ordinal segmentation methods, which treat each pixel as an independent observation and promote ordinality in its representation. This paper proposes novel spatial ordinal segmentation methods, which take advantage of the structured image space by considering each pixel as an observation dependent on its neighborhood context to also promote ordinal spatial consistency. When evaluated with five biomedical datasets and multiple configurations of autonomous driving datasets, ordinal methods resulted in more ordinally-consistent models, with substantial improvements in ordinal metrics and some increase in the Dice coefficient. It was also shown that the incorporation of ordinal consistency results in models with better generalization abilities.</li>
</ul>

<h3>Title: SoK: Payment Channel Networks</h3>
<ul>
<li><strong>Authors: </strong>Kartick Kolachala, Mohammed Ababneh, Roopa Vishwanathan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20968">https://arxiv.org/abs/2407.20968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20968">https://arxiv.org/pdf/2407.20968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20968]] SoK: Payment Channel Networks(https://arxiv.org/abs/2407.20968)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Payment Channel Networks (PCNs) have been proposed as an alternative solution to the scalability, throughput, and cost overhead associated with on-chain transactions. By facilitating offchain execution of transactions, PCNs significantly reduce the burden on the blockchain, leading to faster transaction processing, reduced transaction fees, and enhanced privacy. Despite these advantages, the current research in PCNs presents a variety of research challenges that require further exploration. In this paper, we survey the recent work in several aspects of PCNs, such as pathfinding and routing, virtual channels, state channels, payment channel hubs and rebalancing. This survey aims to provide the reader with a detailed understanding of the current state-of-the-art in PCN research, highlighting a few important advancements. Additionally, we highlight the various unresolved issues in the area of PCN research. Specifically, this paper seeks to answer the following crucial question: What are the various interesting and non-trivial challenges in PCN research that require immediate attention from the academic and research community? By addressing this question, we aim to identify the most pressing problems and future research directions that interested readers can immediately work on. Through this analysis, we hope to inspire researchers and practitioners to tackle these challenges to make PCNs more secure and versatile</li>
</ul>

<h3>Title: Distributed Symmetric Key Establishment: a Scalable Quantum-Safe Key Distribution Protocol</h3>
<ul>
<li><strong>Authors: </strong>Jie Lin (1 and 2), Hoi-Kwong Lo (1 and 2), Jacob Johannsson (1 and 2), Mattia Montagna (1), Manfred von Willich (1) ((1) Quantum Bridge Technologies Inc., (2) Department of Electrical and Computer Engineering, University of Toronto)</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20969">https://arxiv.org/abs/2407.20969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20969">https://arxiv.org/pdf/2407.20969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20969]] Distributed Symmetric Key Establishment: a Scalable Quantum-Safe Key Distribution Protocol(https://arxiv.org/abs/2407.20969)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Pre-shared keys (PSK) have been widely used in network security. Nonetheless, existing PSK solutions are not scalable. Moreover, whenever a new user joins a network, PSK requires an existing user to get a new key before they are able to communicate with the new user. The key issue is how to distribute the PSK between different users. Here, we solve this problem by proposing a new protocol called Distributed Symmetric Key Establishment (DSKE). DSKE has the advantage of being scalable. Unlike standard public key infrastructure (PKI) which relies on computational assumptions, DSKE provides information-theoretic security in a universally composable security framework. Specifically, we prove the security (correctness and confidentiality) and robustness of this protocol against a computationally unbounded adversary, who additionally may have fully compromised a bounded number of the intermediaries and can eavesdrop on all communication. DSKE also achieves distributed trust through secret sharing. We present several implementations of DSKE in real environments, such as providing client services to link encryptors, network encryptors, and mobile phones, as well as the implementation of intermediaries, called Security Hubs, and associated test data as evidence for its versatility. As DSKE is highly scalable in a network setting with no distance limit, it is expected to be a cost-effective quantum-safe cryptographic solution to the network security threat presented by quantum computers.</li>
</ul>

<h3>Title: MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Chen, Senmiao Wang, Zhihang Lin, Zeyu Qin, Yushun Zhang, Tian Ding, Ruoyu Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20999">https://arxiv.org/abs/2407.20999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20999">https://arxiv.org/pdf/2407.20999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20999]] MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning(https://arxiv.org/abs/2407.20999)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks. Typically, an LLM is pre-trained on large corpora and subsequently fine-tuned on task-specific datasets. However, during finetuning, LLMs may forget the knowledge acquired in the pretraining stage, leading to a decline in general capabilities. To address this issue, we propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO). The key idea of MoFO is to iteratively select and update the model parameters with the largest momentum magnitudes. Compared to full-parameter training, MoFO achieves similar fine-tuning performance while keeping parameters closer to the pre-trained model, thereby mitigating knowledge forgetting. Unlike most existing methods for forgetting mitigation, MoFO combines the following two advantages. First, MoFO does not require access to pre-training data. This makes MoFO particularly suitable for fine-tuning scenarios where pre-training data is unavailable, such as fine-tuning checkpoint-only open-source LLMs. Second, MoFO does not alter the original loss function. This could avoid impairing the model performance on the fine-tuning tasks. We validate MoFO through rigorous convergence analysis and extensive experiments, demonstrating its superiority over existing methods in mitigating forgetting and enhancing fine-tuning performance.</li>
</ul>

<h3>Title: CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuexi Du, Brian Chang, Nicha C. Dvornek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21011">https://arxiv.org/abs/2407.21011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21011">https://arxiv.org/pdf/2407.21011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21011]] CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning(https://arxiv.org/abs/2407.21011)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive GPU resources and prolonged training times due to the considerable size of the model and dataset, making them poor for medical applications, in which large datasets are not always common. Meanwhile, the language model prompts are mainly manually derived from labels tied to images, potentially overlooking the richness of information within training samples. We introduce a novel language-image Contrastive Learning method with an Efficient large language model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of the extensive pre-trained language and visual models. Furthermore, we present an efficient strategy for learning context-based prompts that mitigates the gap between informative clinical diagnostic data and simple class labels. Our method demonstrates state-of-the-art performance on multiple chest X-ray and mammography datasets compared with various baselines. The proposed parameter efficient framework can reduce the total trainable model size by 39% and reduce the trainable language model to only 4% compared with the current BERT encoder.</li>
</ul>

<h3>Title: Add-SD: Rational Generation without Manual Reference</h3>
<ul>
<li><strong>Authors: </strong>Lingfeng Yang, Xinyu Zhang, Xiang Li, Jinwen Chen, Kun Yao, Gang Zhang, Errui Ding, Lingqiao Liu, Jingdong Wang, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21016">https://arxiv.org/abs/2407.21016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21016">https://arxiv.org/pdf/2407.21016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21016]] Add-SD: Rational Generation without Manual Reference(https://arxiv.org/abs/2407.21016)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have exhibited remarkable prowess in visual generalization. Building on this success, we introduce an instruction-based object addition pipeline, named Add-SD, which automatically inserts objects into realistic scenes with rational sizes and positions. Different from layout-conditioned methods, Add-SD is solely conditioned on simple text prompts rather than any other human-costly references like bounding boxes. Our work contributes in three aspects: proposing a dataset containing numerous instructed image pairs; fine-tuning a diffusion model for rational generation; and generating synthetic data to boost downstream tasks. The first aspect involves creating a RemovalDataset consisting of original-edited image pairs with textual instructions, where an object has been removed from the original image while maintaining strong pixel consistency in the background. These data pairs are then used for fine-tuning the Stable Diffusion (SD) model. Subsequently, the pretrained Add-SD model allows for the insertion of expected objects into an image with good rationale. Additionally, we generate synthetic instances for downstream task datasets at scale, particularly for tail classes, to alleviate the long-tailed problem. Downstream tasks benefit from the enriched dataset with enhanced diversity and rationale. Experiments on LVIS val demonstrate that Add-SD yields an improvement of 4.3 mAP on rare classes over the baseline. Code and models are available at this https URL.</li>
</ul>

<h3>Title: Matting by Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, Shin'ichi Satoh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21017">https://arxiv.org/abs/2407.21017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21017">https://arxiv.org/pdf/2407.21017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21017]] Matting by Generation(https://arxiv.org/abs/2407.21017)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge. Our method harnesses the capabilities of latent diffusion models, enriched with extensive pre-trained knowledge, to regularize the matting process. We present novel architectural innovations that empower our model to produce mattes with superior resolution and detail. The proposed method is versatile and can perform both guidance-free and guidance-based image matting, accommodating a variety of additional cues. Our comprehensive evaluation across three benchmark datasets demonstrates the superior performance of our approach, both quantitatively and qualitatively. The results not only reflect our method's robust effectiveness but also highlight its ability to generate visually compelling mattes that approach photorealistic quality. The project page for this paper is available at this https URL</li>
</ul>

<h3>Title: ThinK: Thinner Key Cache by Query-Driven Pruning</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, Doyen Sahoo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21018">https://arxiv.org/abs/2407.21018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21018">https://arxiv.org/pdf/2407.21018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21018]] ThinK: Thinner Key Cache by Query-Driven Pruning(https://arxiv.org/abs/2407.21018)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
