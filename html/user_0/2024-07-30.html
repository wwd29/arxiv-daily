<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-30</h1>
<h3>Title: Optimising Hard Prompts with Few-Shot Meta-Prompting</h3>
<ul>
<li><strong>Authors: </strong>Sayash Raaj Hiraou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18920">https://arxiv.org/abs/2407.18920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18920">https://arxiv.org/pdf/2407.18920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18920]] Optimising Hard Prompts with Few-Shot Meta-Prompting(https://arxiv.org/abs/2407.18920)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompting is a flexible and adaptable way of providing instructions to a Large Language Model (LLM). Contextual prompts include context in the form of a document or dialogue along with the natural language instructions to the LLM, often constraining the LLM to restrict facts to that of the given context while complying with the instructions. Masking the context, it acts as template for prompts. In this paper, we present an iterative method to generate better templates using an LLM from an existing set of prompt templates without revealing the context to the LLM. Multiple methods of optimising prompts using the LLM itself are explored to check the effect of few shot sampling methods on iterative propagation while maintaining linguistic styles and syntax on optimisation of prompt templates, yielding a 103.87% improvement using the best performing method. Comparison of the results of multiple contextual tasks demonstrate the ability of LLMs to maintain syntax while learning to replicate linguistic styles. Additionally, the effect on the output with different methods of prompt template generation is shown.</li>
</ul>

<h3>Title: Towards a Novel Privacy-Preserving Distributed Multiparty Data Outsourcing Scheme for Cloud Computing with Quantum Key Distribution</h3>
<ul>
<li><strong>Authors: </strong>D. Dhinakaran, D. Selvaraj, N. Dharini, S. Edwin Raja, C. Sakthi Lakshmi Priya</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18923">https://arxiv.org/abs/2407.18923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18923">https://arxiv.org/pdf/2407.18923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18923]] Towards a Novel Privacy-Preserving Distributed Multiparty Data Outsourcing Scheme for Cloud Computing with Quantum Key Distribution(https://arxiv.org/abs/2407.18923)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>The intersection of cloud computing, blockchain technology, and the impending era of quantum computing presents a critical juncture for data security. This research addresses the escalating vulnerabilities by proposing a comprehensive framework that integrates Quantum Key Distribution (QKD), CRYSTALS Kyber, and Zero-Knowledge Proofs (ZKPs) for securing data in cloud-based blockchain systems. The primary objective is to fortify data against quantum threats through the implementation of QKD, a quantum-safe cryptographic protocol. We leverage the lattice-based cryptographic mechanism, CRYSTALS Kyber, known for its resilience against quantum attacks. Additionally, ZKPs are introduced to enhance data privacy and verification processes within the cloud and blockchain environment. A significant focus of this research is the performance evaluation of the proposed framework. Rigorous analyses encompass encryption and decryption processes, quantum key generation rates, and overall system efficiency. Practical implications are scrutinized, considering factors such as file size, response time, and computational overhead. The evaluation sheds light on the framework's viability in real-world cloud environments, emphasizing its efficiency in mitigating quantum threats. The findings contribute a robust quantum-safe and ZKP-integrated security framework tailored for cloud-based blockchain storage. By addressing critical gaps in theoretical advancements, this research offers practical insights for organizations seeking to secure their data against quantum threats. The framework's efficiency and scalability underscore its practical feasibility, serving as a guide for implementing enhanced data security in the evolving landscape of quantum computing and blockchain integration within cloud environments.</li>
</ul>

<h3>Title: Predicting Winning Captions for Weekly New Yorker Comics</h3>
<ul>
<li><strong>Authors: </strong>Stanley Cao, Sonny Young</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18949">https://arxiv.org/abs/2407.18949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18949">https://arxiv.org/pdf/2407.18949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18949]] Predicting Winning Captions for Weekly New Yorker Comics(https://arxiv.org/abs/2407.18949)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Image captioning using Vision Transformers (ViTs) represents a pivotal convergence of computer vision and natural language processing, offering the potential to enhance user experiences, improve accessibility, and provide textual representations of visual data. This paper explores the application of image captioning techniques to New Yorker cartoons, aiming to generate captions that emulate the wit and humor of winning entries in the New Yorker Cartoon Caption Contest. This task necessitates sophisticated visual and linguistic processing, along with an understanding of cultural nuances and humor. We propose several new baselines for using vision transformer encoder-decoder models to generate captions for the New Yorker cartoon caption contest.</li>
</ul>

<h3>Title: Region Guided Attention Network for Retinal Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Syed Javed, Tariq M. Khan, Abdul Qayyum, Arcot Sowmya, Imran Razzak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18970">https://arxiv.org/abs/2407.18970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18970">https://arxiv.org/pdf/2407.18970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18970]] Region Guided Attention Network for Retinal Vessel Segmentation(https://arxiv.org/abs/2407.18970)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Retinal imaging has emerged as a promising method of addressing this challenge, taking advantage of the unique structure of the retina. The retina is an embryonic extension of the central nervous system, providing a direct in vivo window into neurological health. Recent studies have shown that specific structural changes in retinal vessels can not only serve as early indicators of various diseases but also help to understand disease progression. In this work, we present a lightweight retinal vessel segmentation network based on the encoder-decoder mechanism with region-guided attention. We introduce inverse addition attention blocks with region guided attention to focus on the foreground regions and improve the segmentation of regions of interest. To further boost the model's performance on retinal vessel segmentation, we employ a weighted dice loss. This choice is particularly effective in addressing the class imbalance issues frequently encountered in retinal vessel segmentation tasks. Dice loss penalises false positives and false negatives equally, encouraging the model to generate more accurate segmentation with improved object boundary delineation and reduced fragmentation. Extensive experiments on a benchmark dataset show better performance (0.8285, 0.8098, 0.9677, and 0.8166 recall, precision, accuracy and F1 score respectively) compared to state-of-the-art methods.</li>
</ul>

<h3>Title: Prompt Injection Attacks on Large Language Models in Oncology</h3>
<ul>
<li><strong>Authors: </strong>Jan Clusmann, Dyke Ferber, Isabella C. Wiest, Carolin V. Schneider, Titus J. Brinker, Sebastian Foersch, Daniel Truhn, Jakob N. Kather</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18981">https://arxiv.org/abs/2407.18981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18981">https://arxiv.org/pdf/2407.18981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18981]] Prompt Injection Attacks on Large Language Models in Oncology(https://arxiv.org/abs/2407.18981)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be attacked by prompt injection attacks, which can be used to output harmful information just by interacting with the VLM, without any access to its parameters. We performed a quantitative study to evaluate the vulnerabilities to these attacks in four state of the art VLMs which have been proposed to be of utility in healthcare: Claude 3 Opus, Claude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show that all of these models are susceptible. Specifically, we show that embedding sub-visual prompts in medical imaging data can cause the model to provide harmful output, and that these prompts are non-obvious to human observers. Thus, our study demonstrates a key vulnerability in medical VLMs which should be mitigated before widespread clinical adoption.</li>
</ul>

<h3>Title: Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC</h3>
<ul>
<li><strong>Authors: </strong>Ke Lin, Yasir Glani, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18982">https://arxiv.org/abs/2407.18982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18982">https://arxiv.org/pdf/2407.18982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18982]] Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC(https://arxiv.org/abs/2407.18982)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Secure multi-party computation (MPC) facilitates privacy-preserving computation between multiple parties without leaking private information. While most secure deep learning techniques utilize MPC operations to achieve feasible privacy-preserving machine learning on downstream tasks, the overhead of the computation and communication still hampers their practical application. This work proposes a low-latency secret-sharing-based MPC design that reduces unnecessary communication rounds during the execution of MPC protocols. We also present a method for improving the computation of commonly used nonlinear functions in deep learning by integrating multivariate multiplication and coalescing different packets into one to maximize network utilization. Our experimental results indicate that our method is effective in a variety of settings, with a speedup in communication latency of $10\sim20\%$.</li>
</ul>

<h3>Title: Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications</h3>
<ul>
<li><strong>Authors: </strong>Alon Halfon, Shai Gretz, Ofir Arviv, Artem Spector, Orith Toledo-Ronen, Yoav Katz, Liat Ein-Dor, Michal Shmueli-Scheuer, Noam Slonim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18990">https://arxiv.org/abs/2407.18990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18990">https://arxiv.org/pdf/2407.18990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18990]] Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications(https://arxiv.org/abs/2407.18990)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) is an effective method to enhance their performance on downstream tasks. However, choosing the appropriate setting of tuning hyperparameters (HPs) is a labor-intensive and computationally expensive process. Here, we provide recommended HP configurations for practical use-cases that represent a better starting point for practitioners, when considering two SOTA LLMs and two commonly used tuning methods. We describe Coverage-based Search (CBS), a process for ranking HP configurations based on an offline extensive grid search, such that the top ranked configurations collectively provide a practical robust recommendation for a wide range of datasets and domains. We focus our experiments on Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a total of > 10,000 tuning experiments. Our results suggest that, in general, Llama-3-8B and LoRA should be preferred, when possible. Moreover, we show that for both models and tuning methods, exploring only a few HP configurations, as recommended by our analysis, can provide excellent results in practice, making this work a valuable resource for practitioners.</li>
</ul>

<h3>Title: SWIFT: Semantic Watermarking for Image Forgery Thwarting</h3>
<ul>
<li><strong>Authors: </strong>Gautier Evennou, Vivien Chappelier, Ewa Kijak, Teddy Furon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18995">https://arxiv.org/abs/2407.18995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18995">https://arxiv.org/pdf/2407.18995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18995]] SWIFT: Semantic Watermarking for Image Forgery Thwarting(https://arxiv.org/abs/2407.18995)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel approach towards image authentication and tampering detection by using watermarking as a communication channel for semantic information. We modify the HiDDeN deep-learning watermarking architecture to embed and extract high-dimensional real vectors representing image captions. Our method improves significantly robustness on both malign and benign edits. We also introduce a local confidence metric correlated with Message Recovery Rate, enhancing the method's practical applicability. This approach bridges the gap between traditional watermarking and passive forensic methods, offering a robust solution for image integrity verification.</li>
</ul>

<h3>Title: Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Baao Xie, Qiuyu Chen, Yunnan Wang, Zequn Zhang, Xin Jin, Wenjun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.18999">https://arxiv.org/abs/2407.18999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.18999">https://arxiv.org/pdf/2407.18999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.18999]] Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models(https://arxiv.org/abs/2407.18999)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Disentangled representation learning (DRL) aims to identify and decompose underlying factors behind observations, thus facilitating data perception and generation. However, current DRL approaches often rely on the unrealistic assumption that semantic factors are statistically independent. In reality, these factors may exhibit correlations, which off-the-shelf solutions have yet to properly address. To tackle this challenge, we introduce a bidirectional weighted graph-based framework, to learn factorized attributes and their interrelations within complex data. Specifically, we propose a $\beta$-VAE based module to extract factors as the initial nodes of the graph, and leverage the multimodal large language model (MLLM) to discover and rank latent correlations, thereby updating the weighted edges. By integrating these complementary modules, our model successfully achieves fine-grained, practical and unsupervised disentanglement. Experiments demonstrate our method's superior performance in disentanglement and reconstruction. Furthermore, the model inherits enhanced interpretability and generalizability from MLLMs.</li>
</ul>

<h3>Title: Sparse Refinement for Efficient High-Resolution Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhijian Liu, Zhuoyang Zhang, Samir Khaki, Shang Yang, Haotian Tang, Chenfeng Xu, Kurt Keutzer, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19014">https://arxiv.org/abs/2407.19014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19014">https://arxiv.org/pdf/2407.19014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19014]] Sparse Refinement for Efficient High-Resolution Semantic Segmentation(https://arxiv.org/abs/2407.19014)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation empowers numerous real-world applications, such as autonomous driving and augmented/mixed reality. These applications often operate on high-resolution images (e.g., 8 megapixels) to capture the fine details. However, this comes at the cost of considerable computational complexity, hindering the deployment in latency-sensitive scenarios. In this paper, we introduce SparseRefine, a novel approach that enhances dense low-resolution predictions with sparse high-resolution refinements. Based on coarse low-resolution outputs, SparseRefine first uses an entropy selector to identify a sparse set of pixels with high entropy. It then employs a sparse feature extractor to efficiently generate the refinements for those pixels of interest. Finally, it leverages a gated ensembler to apply these sparse refinements to the initial coarse predictions. SparseRefine can be seamlessly integrated into any existing semantic segmentation model, regardless of CNN- or ViT-based. SparseRefine achieves significant speedup: 1.5 to 3.7 times when applied to HRNet-W48, SegFormer-B5, Mask2Former-T/L and SegNeXt-L on Cityscapes, with negligible to no loss of accuracy. Our "dense+sparse" paradigm paves the way for efficient high-resolution visual computing.</li>
</ul>

<h3>Title: ScalingGaussian: Enhancing 3D Content Creation with Generative Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Shen Chen, Jiale Zhou, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19035">https://arxiv.org/abs/2407.19035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19035">https://arxiv.org/pdf/2407.19035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19035]] ScalingGaussian: Enhancing 3D Content Creation with Generative Gaussian Splatting(https://arxiv.org/abs/2407.19035)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The creation of high-quality 3D assets is paramount for applications in digital heritage preservation, entertainment, and robotics. Traditionally, this process necessitates skilled professionals and specialized software for the modeling, texturing, and rendering of 3D objects. However, the rising demand for 3D assets in gaming and virtual reality (VR) has led to the creation of accessible image-to-3D technologies, allowing non-professionals to produce 3D content and decreasing dependence on expert input. Existing methods for 3D content generation struggle to simultaneously achieve detailed textures and strong geometric consistency. We introduce a novel 3D content creation framework, ScalingGaussian, which combines 3D and 2D diffusion models to achieve detailed textures and geometric consistency in generated 3D assets. Initially, a 3D diffusion model generates point clouds, which are then densified through a process of selecting local regions, introducing Gaussian noise, followed by using local density-weighted selection. To refine the 3D gaussians, we utilize a 2D diffusion model with Score Distillation Sampling (SDS) loss, guiding the 3D Gaussians to clone and split. Finally, the 3D Gaussians are converted into meshes, and the surface textures are optimized using Mean Square Error(MSE) and Gradient Profile Prior(GPP) losses. Our method addresses the common issue of sparse point clouds in 3D diffusion, resulting in improved geometric structure and detailed textures. Experiments on image-to-3D tasks demonstrate that our approach efficiently generates high-quality 3D assets.</li>
</ul>

<h3>Title: Advancing Neural Network Performance through Emergence-Promoting Initialization Scheme</h3>
<ul>
<li><strong>Authors: </strong>Johnny Jingze Li, Vivek Kurien George, Gabriel A. Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19044">https://arxiv.org/abs/2407.19044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19044">https://arxiv.org/pdf/2407.19044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19044]] Advancing Neural Network Performance through Emergence-Promoting Initialization Scheme(https://arxiv.org/abs/2407.19044)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce a novel yet straightforward neural network initialization scheme that modifies conventional methods like Xavier and Kaiming initialization. Inspired by the concept of emergence and leveraging the emergence measures proposed by Li (2023), our method adjusts the layer-wise weight scaling factors to achieve higher emergence values. This enhancement is easy to implement, requiring no additional optimization steps for initialization compared to GradInit. We evaluate our approach across various architectures, including MLP and convolutional architectures for image recognition, and transformers for machine translation. We demonstrate substantial improvements in both model accuracy and training speed, with and without batch normalization. The simplicity, theoretical innovation, and demonstrable empirical advantages of our method make it a potent enhancement to neural network initialization practices. These results suggest a promising direction for leveraging emergence to improve neural network training methodologies. Code is available at: this https URL.</li>
</ul>

<h3>Title: OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation</h3>
<ul>
<li><strong>Authors: </strong>Zilong Wang, Yuedong Cui, Li Zhong, Zimin Zhang, Da Yin, Bill Yuchen Lin, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19056">https://arxiv.org/abs/2407.19056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19056">https://arxiv.org/pdf/2407.19056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19056]] OfficeBench: Benchmarking Language Agents across Multiple Applications for Office Automation(https://arxiv.org/abs/2407.19056)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Office automation significantly enhances human productivity by automatically finishing routine tasks in the workflow. Beyond the basic information extraction studied in much of the prior document AI literature, the office automation research should be extended to more realistic office tasks which require to integrate various information sources in the office system and produce outputs through a series of decision-making processes. We introduce OfficeBench, one of the first office automation benchmarks for evaluating current LLM agents' capability to address office tasks in realistic office workflows. OfficeBench requires LLM agents to perform feasible long-horizon planning, proficiently switch between applications in a timely manner, and accurately ground their actions within a large combined action space, based on the contextual demands of the workflow. Applying our customized evaluation methods on each task, we find that GPT-4 Omni achieves the highest pass rate of 47.00%, demonstrating a decent performance in handling office tasks. However, this is still far below the human performance and accuracy standards required by real-world office workflows. We further observe that most issues are related to operation redundancy and hallucinations, as well as limitations in switching between multiple applications, which may provide valuable insights for developing effective agent frameworks for office automation.</li>
</ul>

<h3>Title: Configural processing as an optimized strategy for robust object recognition in neural networks</h3>
<ul>
<li><strong>Authors: </strong>Hojin Jang, Pawan Sinha, Xavier Boix</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19072">https://arxiv.org/abs/2407.19072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19072">https://arxiv.org/pdf/2407.19072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19072]] Configural processing as an optimized strategy for robust object recognition in neural networks(https://arxiv.org/abs/2407.19072)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Configural processing, the perception of spatial relationships among an object's components, is crucial for object recognition. However, the teleology and underlying neurocomputational mechanisms of such processing are still elusive, notwithstanding decades of research. We hypothesized that processing objects via configural cues provides a more robust means to recognizing them relative to local featural cues. We evaluated this hypothesis by devising identification tasks with composite letter stimuli and comparing different neural network models trained with either only local or configural cues available. We found that configural cues yielded more robust performance to geometric transformations such as rotation or scaling. Furthermore, when both features were simultaneously available, configural cues were favored over local featural cues. Layerwise analysis revealed that the sensitivity to configural cues emerged later relative to local feature cues, possibly contributing to the robustness to pixel-level transformations. Notably, this configural processing occurred in a purely feedforward manner, without the need for recurrent computations. Our findings with letter stimuli were successfully extended to naturalistic face images. Thus, our study provides neurocomputational evidence that configural processing emerges in a naïve network based on task contingencies, and is beneficial for robust object processing under varying viewing conditions.</li>
</ul>

<h3>Title: UniForensics: Face Forgery Detection via General Facial Representation</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Fang, Hanqing Zhao, Tianyi Wei, Wenbo Zhou, Ming Wan, Zhanyi Wang, Weiming Zhang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19079">https://arxiv.org/abs/2407.19079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19079">https://arxiv.org/pdf/2407.19079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19079]] UniForensics: Face Forgery Detection via General Facial Representation(https://arxiv.org/abs/2407.19079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Previous deepfake detection methods mostly depend on low-level textural features vulnerable to perturbations and fall short of detecting unseen forgery methods. In contrast, high-level semantic features are less susceptible to perturbations and not limited to forgery-specific artifacts, thus having stronger generalization. Motivated by this, we propose a detection method that utilizes high-level semantic features of faces to identify inconsistencies in temporal domain. We introduce UniForensics, a novel deepfake detection framework that leverages a transformer-based video classification network, initialized with a meta-functional face encoder for enriched facial representation. In this way, we can take advantage of both the powerful spatio-temporal model and the high-level semantic information of faces. Furthermore, to leverage easily accessible real face data and guide the model in focusing on spatio-temporal features, we design a Dynamic Video Self-Blending (DVSB) method to efficiently generate training samples with diverse spatio-temporal forgery traces using real facial videos. Based on this, we advance our framework with a two-stage training approach: The first stage employs a novel self-supervised contrastive learning, where we encourage the network to focus on forgery traces by impelling videos generated by the same forgery process to have similar representations. On the basis of the representation learned in the first stage, the second stage involves fine-tuning on face forgery detection dataset to build a deepfake detector. Extensive experiments validates that UniForensics outperforms existing face forgery methods in generalization ability and robustness. In particular, our method achieves 95.3\% and 77.2\% cross dataset AUC on the challenging Celeb-DFv2 and DFDC respectively.</li>
</ul>

<h3>Title: Many-Shot In-Context Learning for Molecular Inverse Design</h3>
<ul>
<li><strong>Authors: </strong>Saeed Moayedpour, Alejandro Corrochano-Navarro, Faryad Sahneh, Shahriar Noroozizadeh, Alexander Koetter, Jiri Vymetal, Lorenzo Kogler-Anele, Pablo Mas, Yasser Jangjou, Sizhen Li, Michael Bailey, Marc Bianciotto, Hans Matter, Christoph Grebner, Gerhard Hessler, Ziv Bar-Joseph, Sven Jager</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19089">https://arxiv.org/abs/2407.19089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19089">https://arxiv.org/pdf/2407.19089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19089]] Many-Shot In-Context Learning for Molecular Inverse Design(https://arxiv.org/abs/2407.19089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated great performance in few-shot In-Context Learning (ICL) for a variety of generative and discriminative chemical design tasks. The newly expanded context windows of LLMs can further improve ICL capabilities for molecular inverse design and lead optimization. To take full advantage of these capabilities we developed a new semi-supervised learning method that overcomes the lack of experimental data available for many-shot ICL. Our approach involves iterative inclusion of LLM generated molecules with high predicted performance, along with experimental data. We further integrated our method in a multi-modal LLM which allows for the interactive modification of generated molecular structures using text instructions. As we show, the new method greatly improves upon existing ICL methods for molecular design while being accessible and easy to use for scientists.</li>
</ul>

<h3>Title: FedAR: Addressing Client Unavailability in Federated Learning with Local Update Approximation and Rectification</h3>
<ul>
<li><strong>Authors: </strong>Chutian Jiang, Hansong Zhou, Xiaonan Zhang, Shayok Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19103">https://arxiv.org/abs/2407.19103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19103">https://arxiv.org/pdf/2407.19103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19103]] FedAR: Addressing Client Unavailability in Federated Learning with Local Update Approximation and Rectification(https://arxiv.org/abs/2407.19103)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables clients to collaboratively train machine learning models under the coordination of a server in a privacy-preserving manner. One of the main challenges in FL is that the server may not receive local updates from each client in each round due to client resource limitations and intermittent network connectivity. The existence of unavailable clients severely deteriorates the overall FL performance. In this paper, we propose , a novel client update Approximation and Rectification algorithm for FL to address the client unavailability issue. FedAR can get all clients involved in the global model update to achieve a high-quality global model on the server, which also furnishes accurate predictions for each client. To this end, the server uses the latest update from each client as a surrogate for its current update. It then assigns a different weight to each client's surrogate update to derive the global model, in order to guarantee contributions from both available and unavailable clients. Our theoretical analysis proves that FedAR achieves optimal convergence rates on non-IID datasets for both convex and non-convex smooth loss functions. Extensive empirical studies show that FedAR comprehensively outperforms state-of-the-art FL baselines including FedAvg, MIFA, FedVARP and Scaffold in terms of the training loss, test accuracy, and bias mitigation. Moreover, FedAR also depicts impressive performance in the presence of a large number of clients with severe client unavailability.</li>
</ul>

<h3>Title: ObjectCarver: Semi-automatic segmentation, reconstruction and separation of 3D objects</h3>
<ul>
<li><strong>Authors: </strong>Gemmechu Hassena, Jonathan Moon, Ryan Fujii, Andrew Yuen, Noah Snavely, Steve Marschner, Bharath Hariharan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19108">https://arxiv.org/abs/2407.19108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19108">https://arxiv.org/pdf/2407.19108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19108]] ObjectCarver: Semi-automatic segmentation, reconstruction and separation of 3D objects(https://arxiv.org/abs/2407.19108)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Implicit neural fields have made remarkable progress in reconstructing 3D surfaces from multiple images; however, they encounter challenges when it comes to separating individual objects within a scene. Previous work has attempted to tackle this problem by introducing a framework to train separate signed distance fields (SDFs) simultaneously for each of N objects and using a regularization term to prevent objects from overlapping. However, all of these methods require segmentation masks to be provided, which are not always readily available. We introduce our method, ObjectCarver, to tackle the problem of object separation from just click input in a single view. Given posed multi-view images and a set of user-input clicks to prompt segmentation of the individual objects, our method decomposes the scene into separate objects and reconstructs a high-quality 3D surface for each one. We introduce a loss function that prevents floaters and avoids inappropriate carving-out due to occlusion. In addition, we introduce a novel scene initialization method that significantly speeds up the process while preserving geometric details compared to previous approaches. Despite requiring neither ground truth masks nor monocular cues, our method outperforms baselines both qualitatively and quantitatively. In addition, we introduce a new benchmark dataset for evaluation.</li>
</ul>

<h3>Title: To which reference class do you belong? Measuring racial fairness of reference classes with normative modeling</h3>
<ul>
<li><strong>Authors: </strong>Saige Rutherford, Thomas Wolfers, Charlotte Fraza, Nathaniel G. Harrnet, Christian F. Beckmann, Henricus G. Ruhe, Andre F. Marquand</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19114">https://arxiv.org/abs/2407.19114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19114">https://arxiv.org/pdf/2407.19114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19114]] To which reference class do you belong? Measuring racial fairness of reference classes with normative modeling(https://arxiv.org/abs/2407.19114)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Reference classes in healthcare establish healthy norms, such as pediatric growth charts of height and weight, and are used to chart deviations from these norms which represent potential clinical risk. How the demographics of the reference class influence clinical interpretation of deviations is unknown. Using normative modeling, a method for building reference classes, we evaluate the fairness (racial bias) in reference models of structural brain images that are widely used in psychiatry and neurology. We test whether including race in the model creates fairer models. We predict self-reported race using the deviation scores from three different reference class normative models, to better understand bias in an integrated, multivariate sense. Across all of these tasks, we uncover racial disparities that are not easily addressed with existing data or commonly used modeling techniques. Our work suggests that deviations from the norm could be due to demographic mismatch with the reference class, and assigning clinical meaning to these deviations should be done with caution. Our approach also suggests that acquiring more representative samples is an urgent research priority.</li>
</ul>

<h3>Title: Towards Scalable and Stable Parallelization of Nonlinear RNNs</h3>
<ul>
<li><strong>Authors: </strong>Xavier Gonzalez, Andrew Warrington, Jimmy T.H. Smith, Scott W. Linderman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19115">https://arxiv.org/abs/2407.19115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19115">https://arxiv.org/pdf/2407.19115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19115]] Towards Scalable and Stable Parallelization of Nonlinear RNNs(https://arxiv.org/abs/2407.19115)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Conventional nonlinear RNNs are not naturally parallelizable across the sequence length, whereas transformers and linear RNNs are. Lim et al. [2024] therefore tackle parallelized evaluation of nonlinear RNNs by posing it as a fixed point problem, solved with Newton's method. By deriving and applying a parallelized form of Newton's method, they achieve huge speedups over sequential evaluation. However, their approach inherits cubic computational complexity and numerical instability. We tackle these weaknesses. To reduce the computational complexity, we apply quasi-Newton approximations and show they converge comparably to full-Newton, use less memory, and are faster. To stabilize Newton's method, we leverage a connection between Newton's method damped with trust regions and Kalman smoothing. This connection allows us to stabilize Newtons method, per the trust region, while using efficient parallelized Kalman algorithms to retain performance. We compare these methods empirically, and highlight the use cases where each algorithm excels.</li>
</ul>

<h3>Title: Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Sayyed Farid Ahamed, Soumya Banerjee, Sandip Roy, Devin Quinn, Marc Vucovich, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, Sachin Shetty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19119">https://arxiv.org/abs/2407.19119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19119">https://arxiv.org/pdf/2407.19119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19119]] Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning(https://arxiv.org/abs/2407.19119)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, federate</a></li>
<li><strong>Abstract: </strong>Over the last few years, federated learning (FL) has emerged as a prominent method in machine learning, emphasizing privacy preservation by allowing multiple clients to collaboratively build a model while keeping their training data private. Despite this focus on privacy, FL models are susceptible to various attacks, including membership inference attacks (MIAs), posing a serious threat to data confidentiality. In a recent study, Rezaei \textit{et al.} revealed the existence of an accuracy-privacy trade-off in deep ensembles and proposed a few fusion strategies to overcome it. In this paper, we aim to explore the relationship between deep ensembles and FL. Specifically, we investigate whether confidence-based metrics derived from deep ensembles apply to FL and whether there is a trade-off between accuracy and privacy in FL with respect to MIA. Empirical investigations illustrate a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. By experimenting with different numbers of federated clients, datasets, and confidence-metric-based fusion strategies, we identify and analytically justify the clear existence of the accuracy-privacy trade-off.</li>
</ul>

<h3>Title: Task Offloading in Fog Computing with Deep Reinforcement Learning: Future Research Directions Based on Security and Efficiency Enhancements</h3>
<ul>
<li><strong>Authors: </strong>Amir Pakmehr</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19121">https://arxiv.org/abs/2407.19121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19121">https://arxiv.org/pdf/2407.19121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19121]] Task Offloading in Fog Computing with Deep Reinforcement Learning: Future Research Directions Based on Security and Efficiency Enhancements(https://arxiv.org/abs/2407.19121)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The surge in Internet of Things (IoT) devices and data generation highlights the limitations of traditional cloud computing in meeting demands for immediacy, Quality of Service, and location-aware services. Fog computing emerges as a solution, bringing computation, storage, and networking closer to data sources. This study explores the role of Deep Reinforcement Learning in enhancing fog computing's task offloading, aiming for operational efficiency and robust security. By reviewing current strategies and proposing future research directions, the paper shows the potential of Deep Reinforcement Learning in optimizing resource use, speeding up responses, and securing against vulnerabilities. It suggests advancing Deep Reinforcement Learning for fog computing, exploring blockchain for better security, and seeking energy-efficient models to improve the Internet of Things ecosystem. Incorporating artificial intelligence, our results indicate potential improvements in key metrics, such as task completion time, energy consumption, and security incident reduction. These findings provide a concrete foundation for future research and practical applications in optimizing fog computing architectures.</li>
</ul>

<h3>Title: Few-Shot Medical Image Segmentation with Large Kernel Attention</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxiao Wu, Xiaowei Chen, Zhenguo Gao, Shulei Qu, Yuanyuan Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19148">https://arxiv.org/abs/2407.19148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19148">https://arxiv.org/pdf/2407.19148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19148]] Few-Shot Medical Image Segmentation with Large Kernel Attention(https://arxiv.org/abs/2407.19148)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation has witnessed significant advancements with the emergence of deep learning. However, the reliance of most neural network models on a substantial amount of annotated data remains a challenge for medical image segmentation. To address this issue, few-shot segmentation methods based on meta-learning have been employed. Presently, the methods primarily focus on aligning the support set and query set to enhance performance, but this approach hinders further improvement of the model's effectiveness. In this paper, our objective is to propose a few-shot medical segmentation model that acquire comprehensive feature representation capabilities, which will boost segmentation accuracy by capturing both local and long-range features. To achieve this, we introduce a plug-and-play attention module that dynamically enhances both query and support features, thereby improving the representativeness of the extracted features. Our model comprises four key modules: a dual-path feature extractor, an attention module, an adaptive prototype prediction module, and a multi-scale prediction fusion module. Specifically, the dual-path feature extractor acquires multi-scale features by obtaining features of 32{\times}32 size and 64{\times}64 size. The attention module follows the feature extractor and captures local and long-range information. The adaptive prototype prediction module automatically adjusts the anomaly score threshold to predict prototypes, while the multi-scale fusion prediction module integrates prediction masks of various scales to produce the final segmentation result. We conducted experiments on publicly available MRI datasets, namely CHAOS and CMR, and compared our method with other advanced techniques. The results demonstrate that our method achieves state-of-the-art performance.</li>
</ul>

<h3>Title: A Survey of Malware Detection Using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Bensaoud, Jugal Kalita, Mahmoud Bensaoud</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19153">https://arxiv.org/abs/2407.19153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19153">https://arxiv.org/pdf/2407.19153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19153]] A Survey of Malware Detection Using Deep Learning(https://arxiv.org/abs/2407.19153)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The problem of malicious software (malware) detection and classification is a complex task, and there is no perfect approach. There is still a lot of work to be done. Unlike most other research areas, standard benchmarks are difficult to find for malware detection. This paper aims to investigate recent advances in malware detection on MacOS, Windows, iOS, Android, and Linux using deep learning (DL) by investigating DL in text and image classification, the use of pre-trained and multi-task learning models for malware detection approaches to obtain high accuracy and which the best approach if we have a standard benchmark dataset. We discuss the issues and the challenges in malware detection using DL classifiers by reviewing the effectiveness of these DL classifiers and their inability to explain their decisions and actions to DL developers presenting the need to use Explainable Machine Learning (XAI) or Interpretable Machine Learning (IML) programs. Additionally, we discuss the impact of adversarial attacks on deep learning models, negatively affecting their generalization capabilities and resulting in poor performance on unseen data. We believe there is a need to train and test the effectiveness and efficiency of the current state-of-the-art deep learning models on different malware datasets. We examine eight popular DL approaches on various datasets. This survey will help researchers develop a general understanding of malware recognition using deep learning.</li>
</ul>

<h3>Title: Debiased Graph Poisoning Attack via Contrastive Surrogate Objective</h3>
<ul>
<li><strong>Authors: </strong>Kanghoon Yoon, Yeonjun In, Namkyeong Lee, Kibum Kim, Chanyoung Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19155">https://arxiv.org/abs/2407.19155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19155">https://arxiv.org/pdf/2407.19155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19155]] Debiased Graph Poisoning Attack via Contrastive Surrogate Objective(https://arxiv.org/abs/2407.19155)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNN) are vulnerable to adversarial attacks, which aim to degrade the performance of GNNs through imperceptible changes on the graph. However, we find that in fact the prevalent meta-gradient-based attacks, which utilizes the gradient of the loss w.r.t the adjacency matrix, are biased towards training nodes. That is, their meta-gradient is determined by a training procedure of the surrogate model, which is solely trained on the training nodes. This bias manifests as an uneven perturbation, connecting two nodes when at least one of them is a labeled node, i.e., training node, while it is unlikely to connect two unlabeled nodes. However, these biased attack approaches are sub-optimal as they do not consider flipping edges between two unlabeled nodes at all. This means that they miss the potential attacked edges between unlabeled nodes that significantly alter the representation of a node. In this paper, we investigate the meta-gradients to uncover the root cause of the uneven perturbations of existing attacks. Based on our analysis, we propose a Meta-gradient-based attack method using contrastive surrogate objective (Metacon), which alleviates the bias in meta-gradient using a new surrogate loss. We conduct extensive experiments to show that Metacon outperforms existing meta gradient-based attack methods through benchmark datasets, while showing that alleviating the bias towards training nodes is effective in attacking the graph structure.</li>
</ul>

<h3>Title: Robust Multimodal 3D Object Detection via Modality-Agnostic Decoding and Proximity-based Modality Ensemble</h3>
<ul>
<li><strong>Authors: </strong>Juhan Cha, Minseok Joo, Jihwan Park, Sanghyeok Lee, Injae Kim, Hyunwoo J. Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19156">https://arxiv.org/abs/2407.19156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19156">https://arxiv.org/pdf/2407.19156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19156]] Robust Multimodal 3D Object Detection via Modality-Agnostic Decoding and Proximity-based Modality Ensemble(https://arxiv.org/abs/2407.19156)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in 3D object detection have benefited from multi-modal information from the multi-view cameras and LiDAR sensors. However, the inherent disparities between the modalities pose substantial challenges. We observe that existing multi-modal 3D object detection methods heavily rely on the LiDAR sensor, treating the camera as an auxiliary modality for augmenting semantic details. This often leads to not only underutilization of camera data but also significant performance degradation in scenarios where LiDAR data is unavailable. Additionally, existing fusion methods overlook the detrimental impact of sensor noise induced by environmental changes, on detection performance. In this paper, we propose MEFormer to address the LiDAR over-reliance problem by harnessing critical information for 3D object detection from every available modality while concurrently safeguarding against corrupted signals during the fusion process. Specifically, we introduce Modality Agnostic Decoding (MOAD) that extracts geometric and semantic features with a shared transformer decoder regardless of input modalities and provides promising improvement with a single modality as well as multi-modality. Additionally, our Proximity-based Modality Ensemble (PME) module adaptively utilizes the strengths of each modality depending on the environment while mitigating the effects of a noisy sensor. Our MEFormer achieves state-of-the-art performance of 73.9% NDS and 71.5% mAP in the nuScenes validation set. Extensive analyses validate that our MEFormer improves robustness against challenging conditions such as sensor malfunctions or environmental changes. The source code is available at this https URL</li>
</ul>

<h3>Title: Addressing Topic Leakage in Cross-Topic Evaluation for Authorship Verification</h3>
<ul>
<li><strong>Authors: </strong>Jitkapat Sawatphol, Can Udomcharoenchaikit, Sarana Nutanong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19164">https://arxiv.org/abs/2407.19164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19164">https://arxiv.org/pdf/2407.19164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19164]] Addressing Topic Leakage in Cross-Topic Evaluation for Authorship Verification(https://arxiv.org/abs/2407.19164)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Authorship verification (AV) aims to identify whether a pair of texts has the same author. We address the challenge of evaluating AV models' robustness against topic shifts. The conventional evaluation assumes minimal topic overlap between training and test data. However, we argue that there can still be topic leakage in test data, causing misleading model performance and unstable rankings. To address this, we propose an evaluation method called Heterogeneity-Informed Topic Sampling (HITS), which creates a smaller dataset with a heterogeneously distributed topic set. Our experimental results demonstrate that HITS-sampled datasets yield a more stable ranking of models across random seeds and evaluation splits. Our contributions include: 1. An analysis of causes and effects of topic leakage. 2. A demonstration of the HITS in reducing the effects of topic leakage, and 3. The Robust Authorship Verification bENchmark (RAVEN) that allows topic shortcut test to uncover AV models' reliance on topic-specific features.</li>
</ul>

<h3>Title: FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity Measurement of Persian Social Networks Informal Texts</h3>
<ul>
<li><strong>Authors: </strong>Seyed Mojtaba Sadjadi, Zeinab Rajabi, Leila Rabiei, Mohammad-Shahram Moin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19173">https://arxiv.org/abs/2407.19173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19173">https://arxiv.org/pdf/2407.19173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19173]] FarSSiBERT: A Novel Transformer-based Model for Semantic Similarity Measurement of Persian Social Networks Informal Texts(https://arxiv.org/abs/2407.19173)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>One fundamental task for NLP is to determine the similarity between two texts and evaluate the extent of their likeness. The previous methods for the Persian language have low accuracy and are unable to comprehend the structure and meaning of texts effectively. Additionally, these methods primarily focus on formal texts, but in real-world applications of text processing, there is a need for robust methods that can handle colloquial texts. This requires algorithms that consider the structure and significance of words based on context, rather than just the frequency of words. The lack of a proper dataset for this task in the Persian language makes it important to develop such algorithms and construct a dataset for Persian text. This paper introduces a new transformer-based model to measure semantic similarity between Persian informal short texts from social networks. In addition, a Persian dataset named FarSSiM has been constructed for this purpose, using real data from social networks and manually annotated and verified by a linguistic expert team. The proposed model involves training a large language model using the BERT architecture from scratch. This model, called FarSSiBERT, is pre-trained on approximately 104 million Persian informal short texts from social networks, making it one of a kind in the Persian language. Moreover, a novel specialized informal language tokenizer is provided that not only performs tokenization on formal texts well but also accurately identifies tokens that other Persian tokenizers are unable to recognize. It has been demonstrated that our proposed model outperforms ParsBERT, laBSE, and multilingual BERT in the Pearson and Spearman's coefficient criteria. Additionally, the pre-trained large language model has great potential for use in other NLP tasks on colloquial text and as a tokenizer for less-known informal words.</li>
</ul>

<h3>Title: Reducing Spurious Correlation for Federated Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Shuran Ma, Weiying Xie, Daixun Li, Haowei Li, Yunsong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19174">https://arxiv.org/abs/2407.19174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19174">https://arxiv.org/pdf/2407.19174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19174]] Reducing Spurious Correlation for Federated Domain Generalization(https://arxiv.org/abs/2407.19174)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The rapid development of multimedia has provided a large amount of data with different distributions for visual tasks, forming different domains. Federated Learning (FL) can efficiently use this diverse data distributed on different client media in a decentralized manner through model sharing. However, in open-world scenarios, there is a challenge: global models may struggle to predict well on entirely new domain data captured by certain media, which were not encountered during training. Existing methods still rely on strong statistical correlations between samples and labels to address this issue, which can be misleading, as some features may establish spurious short-cut correlations with the predictions. To comprehensively address this challenge, we introduce FedCD (Cross-Domain Invariant Federated Learning), an overall optimization framework at both the local and global levels. We introduce the Spurious Correlation Intervener (SCI), which employs invariance theory to locally generate interventers for features in a self-supervised manner to reduce the model's susceptibility to spurious correlated features. Our approach requires no sharing of data or features, only the gradients related to the model. Additionally, we develop the simple yet effective Risk Extrapolation Aggregation strategy (REA), determining aggregation coefficients through mathematical optimization to facilitate global causal invariant predictions. Extensive experiments and ablation studies highlight the effectiveness of our approach. In both classification and object detection generalization tasks, our method outperforms the baselines by an average of at least 1.45% in Acc, 4.8% and 1.27% in mAP50.</li>
</ul>

<h3>Title: Data Processing Techniques for Modern Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Yinheng Li, Han Ding, Hang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19180">https://arxiv.org/abs/2407.19180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19180">https://arxiv.org/pdf/2407.19180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19180]] Data Processing Techniques for Modern Multimodal Models(https://arxiv.org/abs/2407.19180)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Data processing plays an significant role in current multimodal model training. In this paper. we provide an comprehensive review of common data processing techniques used in modern multimodal model training with a focus on diffusion models and multimodal large language models (MLLMs). We summarized all techniques into four categories: data quality, data quantity, data distribution and data safety. We further present our findings in the choice of data process methods in different type of models. This study aims to provide guidance to multimodal models developers with effective data processing techniques.</li>
</ul>

<h3>Title: LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruiyi Zhang, Yufan Zhou, Jian Chen, Jiuxiang Gu, Changyou Chen, Tong Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19185">https://arxiv.org/abs/2407.19185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19185">https://arxiv.org/pdf/2407.19185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19185]] LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models(https://arxiv.org/abs/2407.19185)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large multimodal language models have demonstrated impressive capabilities in understanding and manipulating images. However, many of these models struggle with comprehending intensive textual contents embedded within the images, primarily due to the limited text recognition and layout understanding ability. To understand the sources of these limitations, we perform an exploratory analysis showing the drawbacks of classical visual encoders on visual text understanding. Hence, we present LLaVA-Read, a multimodal large language model that utilizes dual visual encoders along with a visual text encoder. Our model surpasses existing state-of-the-art models in various text-rich image understanding tasks, showcasing enhanced comprehension of textual content within images. Together, our research suggests visual text understanding remains an open challenge and an efficient visual text encoder is crucial for future successful multimodal systems.</li>
</ul>

<h3>Title: A collaborative ensemble construction method for federated random forest</h3>
<ul>
<li><strong>Authors: </strong>Penjan Antonio Eng Lim, Cheong Hee Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19193">https://arxiv.org/abs/2407.19193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19193">https://arxiv.org/pdf/2407.19193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19193]] A collaborative ensemble construction method for federated random forest(https://arxiv.org/abs/2407.19193)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Random forests are considered a cornerstone in machine learning for their robustness and versatility. Despite these strengths, their conventional centralized training is ill-suited for the modern landscape of data that is often distributed, sensitive, and subject to privacy concerns. Federated learning (FL) provides a compelling solution to this problem, enabling models to be trained across a group of clients while maintaining the privacy of each client's data. However, adapting tree-based methods like random forests to federated settings introduces significant challenges, particularly when it comes to non-identically distributed (non-IID) data across clients, which is a common scenario in real-world applications. This paper presents a federated random forest approach that employs a novel ensemble construction method aimed at improving performance under non-IID data. Instead of growing trees independently in each client, our approach ensures each decision tree in the ensemble is iteratively and collectively grown across clients. To preserve the privacy of the client's data, we confine the information stored in the leaf nodes to the majority class label identified from the samples of the client's local data that reach each node. This limited disclosure preserves the confidentiality of the underlying data distribution of clients, thereby enhancing the privacy of the federated learning process. Furthermore, our collaborative ensemble construction strategy allows the ensemble to better reflect the data's heterogeneity across different clients, enhancing its performance on non-IID data, as our experimental results confirm.</li>
</ul>

<h3>Title: On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nitay Calderon, Roi Reichart</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19200">https://arxiv.org/abs/2407.19200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19200">https://arxiv.org/pdf/2407.19200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19200]] On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs(https://arxiv.org/abs/2407.19200)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Recent advancements in NLP systems, particularly with the introduction of LLMs, have led to widespread adoption of these systems by a broad spectrum of users across various domains, impacting decision-making, the job market, society, and scientific research. This surge in usage has led to an explosion in NLP model interpretability and analysis research, accompanied by numerous technical surveys. Yet, these surveys often overlook the needs and perspectives of explanation stakeholders. In this paper, we address three fundamental questions: Why do we need interpretability, what are we interpreting, and how? By exploring these questions, we examine existing interpretability paradigms, their properties, and their relevance to different stakeholders. We further explore the practical implications of these paradigms by analyzing trends from the past decade across multiple research fields. To this end, we retrieved thousands of papers and employed an LLM to characterize them. Our analysis reveals significant disparities between NLP developers and non-developer users, as well as between research fields, underscoring the diverse needs of stakeholders. For example, explanations of internal model components are rarely used outside the NLP field. We hope this paper informs the future design, development, and application of methods that align with the objectives and requirements of various stakeholders.</li>
</ul>

<h3>Title: Towards Clean-Label Backdoor Attacks in the Physical World</h3>
<ul>
<li><strong>Authors: </strong>Thinh Dao, Cuong Chi Le, Khoa D Doan, Kok-Seng Wong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19203">https://arxiv.org/abs/2407.19203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19203">https://arxiv.org/pdf/2407.19203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19203]] Towards Clean-Label Backdoor Attacks in the Physical World(https://arxiv.org/abs/2407.19203)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) are vulnerable to backdoor poisoning attacks, with most research focusing on digital triggers, special patterns digitally added to test-time inputs to induce targeted misclassification. In contrast, physical triggers, which are natural objects within a physical scene, have emerged as a desirable alternative since they enable real-time backdoor activations without digital manipulation. However, current physical attacks require that poisoned inputs have incorrect labels, making them easily detectable upon human inspection. In this paper, we collect a facial dataset of 21,238 images with 7 common accessories as triggers and use it to study the threat of clean-label backdoor attacks in the physical world. Our study reveals two findings. First, the success of physical attacks depends on the poisoning algorithm, physical trigger, and the pair of source-target classes. Second, although clean-label poisoned samples preserve ground-truth labels, their perceptual quality could be seriously degraded due to conspicuous artifacts in the images. Such samples are also vulnerable to statistical filtering methods because they deviate from the distribution of clean samples in the feature space. To address these issues, we propose replacing the standard $\ell_\infty$ regularization with a novel pixel regularization and feature regularization that could enhance the imperceptibility of poisoned samples without compromising attack performance. Our study highlights accidental backdoor activations as a key limitation of clean-label physical backdoor attacks. This happens when unintended objects or classes accidentally cause the model to misclassify as the target class.</li>
</ul>

<h3>Title: Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's Impact on Spatio-Temporal Cross-Attentions</h3>
<ul>
<li><strong>Authors: </strong>Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Zinuo Li, Hamid Laga, Farid Boussaid</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19205">https://arxiv.org/abs/2407.19205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19205">https://arxiv.org/pdf/2407.19205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19205]] Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's Impact on Spatio-Temporal Cross-Attentions(https://arxiv.org/abs/2407.19205)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper investigates the role of CLIP image embeddings within the Stable Video Diffusion (SVD) framework, focusing on their impact on video generation quality and computational efficiency. Our findings indicate that CLIP embeddings, while crucial for aesthetic quality, do not significantly contribute towards the subject and background consistency of video outputs. Moreover, the computationally expensive cross-attention mechanism can be effectively replaced by a simpler linear layer. This layer is computed only once at the first diffusion inference step, and its output is then cached and reused throughout the inference process, thereby enhancing efficiency while maintaining high-quality outputs. Building on these insights, we introduce the VCUT, a training-free approach optimized for efficiency within the SVD architecture. VCUT eliminates temporal cross-attention and replaces spatial cross-attention with a one-time computed linear layer, significantly reducing computational load. The implementation of VCUT leads to a reduction of up to 322T Multiple-Accumulate Operations (MACs) per video and a decrease in model parameters by up to 50M, achieving a 20% reduction in latency compared to the baseline. Our approach demonstrates that conditioning during the Semantic Binding stage is sufficient, eliminating the need for continuous computation across all inference steps and setting a new standard for efficient video generation.</li>
</ul>

<h3>Title: Collaborative CP-NIZKs: Modular, Composable Proofs for Distributed Secrets</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Alghazwi, Tariq Bontekoe, Leon Visscher, Fatih Turkmen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19212">https://arxiv.org/abs/2407.19212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19212">https://arxiv.org/pdf/2407.19212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19212]] Collaborative CP-NIZKs: Modular, Composable Proofs for Distributed Secrets(https://arxiv.org/abs/2407.19212)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Non-interactive zero-knowledge (NIZK) proofs of knowledge have proven to be highly relevant for securely realizing a wide array of applications that rely on both privacy and correctness. They enable a prover to convince any party of the correctness of a public statement for a secret witness. However, most NIZKs do not natively support proving knowledge of a secret witness that is distributed over multiple provers. Previously, collaborative proofs [51] have been proposed to overcome this limitation. We investigate the notion of composability in this setting, following the Commit-and-Prove design of LegoSNARK [17]. Composability allows users to combine different, specialized NIZKs (e.g., one arithmetic circuit, one boolean circuit, and one for range proofs) with the aim of reducing the prove generation time. Moreover, it opens the door to efficient realizations of many applications in the collaborative setting such as mutually exclusive prover groups, combining collaborative and single-party proofs and efficiently implementing publicly auditable MPC (PA-MPC). We present the first, general definition for collaborative commit-and-prove NIZK (CP-NIZK) proofs of knowledge and construct distributed protocols to enable their realization. We implement our protocols for two commonly used NIZKs, Groth16 and Bulletproofs, and evaluate their practicality in a variety of computational settings. Our findings indicate that composability adds only minor overhead, especially for large circuits. We experimented with our construction in an application setting, and when compared to prior works, our protocols reduce latency by 18-55x while requiring only a fraction (0.2%) of the communication.</li>
</ul>

<h3>Title: EaTVul: ChatGPT-based Evasion Attack Against Software Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Shigang Liu, Di Cao, Junae Kim, Tamas Abraham, Paul Montague, Seyit Camtepe, Jun Zhang, Yang Xiang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19216">https://arxiv.org/abs/2407.19216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19216">https://arxiv.org/pdf/2407.19216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19216]] EaTVul: ChatGPT-based Evasion Attack Against Software Vulnerability Detection(https://arxiv.org/abs/2407.19216)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Recently, deep learning has demonstrated promising results in enhancing the accuracy of vulnerability detection and identifying vulnerabilities in software. However, these techniques are still vulnerable to attacks. Adversarial examples can exploit vulnerabilities within deep neural networks, posing a significant threat to system security. This study showcases the susceptibility of deep learning models to adversarial attacks, which can achieve 100% attack success rate (refer to Table 5). The proposed method, EaTVul, encompasses six stages: identification of important samples using support vector machines, identification of important features using the attention mechanism, generation of adversarial data based on these features using ChatGPT, preparation of an adversarial attack pool, selection of seed data using a fuzzy genetic algorithm, and the execution of an evasion attack. Extensive experiments demonstrate the effectiveness of EaTVul, achieving an attack success rate of more than 83% when the snippet size is greater than 2. Furthermore, in most cases with a snippet size of 4, EaTVul achieves a 100% attack success rate. The findings of this research emphasize the necessity of robust defenses against adversarial attacks in software vulnerability detection.</li>
</ul>

<h3>Title: Enhancing cybersecurity defenses: a multicriteria decision-making approach to MITRE ATT&CK mitigation strategy</h3>
<ul>
<li><strong>Authors: </strong>Ihab Mohamed, Hesham A. Hefny, Nagy R. Darwish</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19222">https://arxiv.org/abs/2407.19222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19222">https://arxiv.org/pdf/2407.19222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19222]] Enhancing cybersecurity defenses: a multicriteria decision-making approach to MITRE ATT&CK mitigation strategy(https://arxiv.org/abs/2407.19222)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Cybersecurity is a big challenge as hackers are always trying to find new methods to attack and exploit system vulnerabilities. Cybersecurity threats and risks have increased in recent years, due to the increasing number of devices and networks connected. This has led to the development of new cyberattack patterns, such as ransomware, data breaches, and advanced persistent threats (APT). Consequently, defending such complicated attacks needs to stay up to date with the latest system vulnerabilities and weaknesses to set a proper cybersecurity defense strategy. This paper aims to propose a defense strategy for the presented security threats by determining and prioritizing which security control to put in place based on combining the MITRE ATT&CK framework with multi-criteria decision-making (MCDM) techniques. This approach helps organizations achieve a more robust and resilient cybersecurity posture.</li>
</ul>

<h3>Title: Radio Frequency Signal based Human Silhouette Segmentation: A Sequential Diffusion Approach</h3>
<ul>
<li><strong>Authors: </strong>Penghui Wen, Kun Hu, Dong Yuan, Zhiyuan Ning, Changyang Li, Zhiyong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19244">https://arxiv.org/abs/2407.19244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19244">https://arxiv.org/pdf/2407.19244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19244]] Radio Frequency Signal based Human Silhouette Segmentation: A Sequential Diffusion Approach(https://arxiv.org/abs/2407.19244)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Radio frequency (RF) signals have been proved to be flexible for human silhouette segmentation (HSS) under complex environments. Existing studies are mainly based on a one-shot approach, which lacks a coherent projection ability from the RF domain. Additionally, the spatio-temporal patterns have not been fully explored for human motion dynamics in HSS. Therefore, we propose a two-stage Sequential Diffusion Model (SDM) to progressively synthesize high-quality segmentation jointly with the considerations on motion dynamics. Cross-view transformation blocks are devised to guide the diffusion model in a multi-scale manner for comprehensively characterizing human related patterns in an individual frame such as directional projection from signal planes. Moreover, spatio-temporal blocks are devised to fine-tune the frame-level model to incorporate spatio-temporal contexts and motion dynamics, enhancing the consistency of the segmentation maps. Comprehensive experiments on a public benchmark -- HIBER demonstrate the state-of-the-art performance of our method with an IoU 0.732. Our code is available at this https URL.</li>
</ul>

<h3>Title: Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yansheng Li, Tingzhu Wang, Kang Wu, Linlin Wang, Xin Guo, Wenbin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19259">https://arxiv.org/abs/2407.19259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19259">https://arxiv.org/pdf/2407.19259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19259]] Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction(https://arxiv.org/abs/2407.19259)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Scene Graph Generation (SGG) aims to explore the relationships between objects in images and obtain scene summary graphs, thereby better serving downstream tasks. However, the long-tailed problem has adversely affected the scene graph's quality. The predictions are dominated by coarse-grained relationships, lacking more informative fine-grained ones. The union region of one object pair (i.e., one sample) contains rich and dedicated contextual information, enabling the prediction of the sample-specific bias for refining the original relationship prediction. Therefore, we propose a novel Sample-Level Bias Prediction (SBP) method for fine-grained SGG (SBG). Firstly, we train a classic SGG model and construct a correction bias set by calculating the margin between the ground truth label and the predicted label with one classic SGG model. Then, we devise a Bias-Oriented Generative Adversarial Network (BGAN) that learns to predict the constructed correction biases, which can be utilized to correct the original predictions from coarse-grained relationships to fine-grained ones. The extensive experimental results on VG, GQA, and VG-1800 datasets demonstrate that our SBG outperforms the state-of-the-art methods in terms of Average@K across three mainstream SGG models: Motif, VCtree, and Transformer. Compared to dataset-level correction methods on VG, SBG shows a significant average improvement of 5.6%, 3.9%, and 3.2% on Average@K for tasks PredCls, SGCls, and SGDet, respectively. The code will be available at this https URL.</li>
</ul>

<h3>Title: Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications</h3>
<ul>
<li><strong>Authors: </strong>Till Speicher, Mohammad Aflah Khan, Qinyuan Wu, Vedant Nanda, Soumi Das, Bishwamittra Ghosh, Krishna P. Gummadi, Evimaria Terzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19262">https://arxiv.org/abs/2407.19262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19262">https://arxiv.org/pdf/2407.19262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19262]] Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications(https://arxiv.org/abs/2407.19262)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Understanding whether and to what extent large language models (LLMs) have memorised training data has important implications for the reliability of their output and the privacy of their training data. In order to cleanly measure and disentangle memorisation from other phenomena (e.g. in-context learning), we create an experimental framework that is based on repeatedly exposing LLMs to random strings. Our framework allows us to better understand the dynamics, i.e., the behaviour of the model, when repeatedly exposing it to random strings. Using our framework, we make several striking observations: (a) we find consistent phases of the dynamics across families of models (Pythia, Phi and Llama2), (b) we identify factors that make some strings easier to memorise than others, and (c) we identify the role of local prefixes and global context in memorisation. We also show that sequential exposition to different random strings has a significant effect on memorisation. Our results, often surprising, have significant downstream implications in the study and usage of LLMs.</li>
</ul>

<h3>Title: Sewer Image Super-Resolution with Depth Priors and Its Lightweight Network</h3>
<ul>
<li><strong>Authors: </strong>Gang Pan, Chen Wang, Zhijie Sui, Shuai Guo, Yaozhi Lv, Honglie Li, Di Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19271">https://arxiv.org/abs/2407.19271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19271">https://arxiv.org/pdf/2407.19271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19271]] Sewer Image Super-Resolution with Depth Priors and Its Lightweight Network(https://arxiv.org/abs/2407.19271)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>The Quick-view (QV) technique serves as a primary method for detecting defects within sewerage systems. However, the effectiveness of QV is impeded by the limited visual range of its hardware, resulting in suboptimal image quality for distant portions of the sewer network. Image super-resolution is an effective way to improve image quality and has been applied in a variety of scenes. However, research on super-resolution for sewer images remains considerably unexplored. In response, this study leverages the inherent depth relationships present within QV images and introduces a novel Depth-guided, Reference-based Super-Resolution framework denoted as DSRNet. It comprises two core components: a depth extraction module and a depth information matching module (DMM). DSRNet utilizes the adjacent frames of the low-resolution image as reference images and helps them recover texture information based on the correlation. By combining these modules, the integration of depth priors significantly enhances both visual quality and performance benchmarks. Besides, in pursuit of computational efficiency and compactness, our paper introduces a super-resolution knowledge distillation model based on an attention mechanism. This mechanism facilitates the acquisition of feature similarity between a more complex teacher model and a streamlined student model, the latter being a lightweight version of DSRNet. Experimental results demonstrate that DSRNet significantly improves PSNR and SSIM compared with other methods. This study also conducts experiments on sewer defect semantic segmentation, object detection, and classification on the Pipe dataset and Sewer-ML dataset. Experiments show that the method can improve the performance of low-resolution sewer images in these tasks.</li>
</ul>

<h3>Title: Mamba? Catch The Hype Or Rethink What Really Helps for Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Bailiang Jian, Jiazhen Pan, Morteza Ghahremani, Daniel Rueckert, Christian Wachinger, Benedikt Wiestler</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19274">https://arxiv.org/abs/2407.19274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19274">https://arxiv.org/pdf/2407.19274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19274]] Mamba? Catch The Hype Or Rethink What Really Helps for Image Registration(https://arxiv.org/abs/2407.19274)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Our findings indicate that adopting "advanced" computational elements fails to significantly improve registration accuracy. Instead, well-established registration-specific designs offer fair improvements, enhancing results by a marginal 1.5\% over the baseline. Our findings emphasize the importance of rigorous, unbiased evaluation and contribution disentanglement of all low- and high-level registration components, rather than simply following the computer vision trends with "more advanced" computational blocks. We advocate for simpler yet effective solutions and novel evaluation metrics that go beyond conventional registration accuracy, warranting further research across diverse organs and modalities. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: On Joint Noise Scaling in Differentially Private Federated Learning with Multiple Local Steps</h3>
<ul>
<li><strong>Authors: </strong>Mikko A. Heikkilä</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19286">https://arxiv.org/abs/2407.19286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19286">https://arxiv.org/pdf/2407.19286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19286]] On Joint Noise Scaling in Differentially Private Federated Learning with Multiple Local Steps(https://arxiv.org/abs/2407.19286)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated learning is a distributed learning setting where the main aim is to train machine learning models without having to share raw data but only what is required for learning. To guarantee training data privacy and high-utility models, differential privacy and secure aggregation techniques are often combined with federated learning. However, with fine-grained protection granularities the currently existing techniques require the parties to communicate for each local optimisation step, if they want to fully benefit from the secure aggregation in terms of the resulting formal privacy guarantees. In this paper, we show how a simple new analysis allows the parties to perform multiple local optimisation steps while still benefiting from joint noise scaling when using secure aggregation. We show that our analysis enables higher utility models with guaranteed privacy protection under limited number of communication rounds.</li>
</ul>

<h3>Title: Rethinking Attention Module Design for Point Cloud Analysis</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Wu, Kaige Wang, Zeyun Zhong, Hao Fu, Junwei Zheng, Jiaming Zhang, Julius Pfrommer, Jürgen Beyerer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19294">https://arxiv.org/abs/2407.19294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19294">https://arxiv.org/pdf/2407.19294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19294]] Rethinking Attention Module Design for Point Cloud Analysis(https://arxiv.org/abs/2407.19294)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, there have been significant advancements in applying attention mechanisms to point cloud analysis. However, attention module variants featured in various research papers often operate under diverse settings and tasks, incorporating potential training strategies. This heterogeneity poses challenges in establishing a fair comparison among these attention module variants. In this paper, we address this issue by rethinking and exploring attention module design within a consistent base framework and settings. Both global-based and local-based attention methods are studied, with a focus on the selection basis and scales of neighbors for local-based attention. Different combinations of aggregated local features and computation methods for attention scores are evaluated, ranging from the initial addition/concatenation-based approach to the widely adopted dot product-based method and the recently proposed vector attention technique. Various position encoding methods are also investigated. Our extensive experimental analysis reveals that there is no universally optimal design across diverse point cloud tasks. Instead, drawing from best practices, we propose tailored attention modules for specific tasks, leading to superior performance on point cloud classification and segmentation benchmarks.</li>
</ul>

<h3>Title: The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Dung Le, Ti Ti Nguyen, Vu Nguyen Ha</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19299">https://arxiv.org/abs/2407.19299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19299">https://arxiv.org/pdf/2407.19299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19299]] The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations(https://arxiv.org/abs/2407.19299)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) for clinical Natural Language Processing (NLP) poses significant challenges due to the domain gap and limited data availability. This study investigates the effectiveness of various adapter techniques, equivalent to Low-Rank Adaptation (LoRA), for fine-tuning LLMs in a resource-constrained hospital environment. We experimented with four structures-Adapter, Lightweight, TinyAttention, and Gated Residual Network (GRN)-as final layers for clinical notes classification. We fine-tuned biomedical pre-trained models, including CamemBERT-bio, AliBERT, and DrBERT, alongside two Transformer-based models. Our extensive experimental results indicate that i) employing adapter structures does not yield significant improvements in fine-tuning biomedical pre-trained LLMs, and ii) simpler Transformer-based models, trained from scratch, perform better under resource constraints. Among the adapter structures, GRN demonstrated superior performance with accuracy, precision, recall, and an F1 score of 0.88. Moreover, the total training time for LLMs exceeded 1000 hours, compared to under 6 hours for simpler transformer-based models, highlighting that LLMs are more suitable for environments with extensive computational resources and larger datasets. Consequently, this study demonstrates that simpler Transformer-based models can be effectively trained from scratch, providing a viable solution for clinical NLP tasks in low-resource environments with limited data availability. By identifying the GRN as the most effective adapter structure, we offer a practical approach to enhance clinical note classification without requiring extensive computational resources.</li>
</ul>

<h3>Title: CoLiDR: Concept Learning using Aggregated Disentangled Representations</h3>
<ul>
<li><strong>Authors: </strong>Sanchit Sinha, Guangzhi Xiong, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19300">https://arxiv.org/abs/2407.19300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19300">https://arxiv.org/pdf/2407.19300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19300]] CoLiDR: Concept Learning using Aggregated Disentangled Representations(https://arxiv.org/abs/2407.19300)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Interpretability of Deep Neural Networks using concept-based models offers a promising way to explain model behavior through human-understandable concepts. A parallel line of research focuses on disentangling the data distribution into its underlying generative factors, in turn explaining the data generation process. While both directions have received extensive attention, little work has been done on explaining concepts in terms of generative factors to unify mathematically disentangled representations and human-understandable concepts as an explanation for downstream tasks. In this paper, we propose a novel method CoLiDR - which utilizes a disentangled representation learning setup for learning mutually independent generative factors and subsequently learns to aggregate the said representations into human-understandable concepts using a novel aggregation/decomposition module. Experiments are conducted on datasets with both known and unknown latent generative factors. Our method successfully aggregates disentangled generative factors into concepts while maintaining parity with state-of-the-art concept-based approaches. Quantitative and visual analysis of the learned aggregation procedure demonstrates the advantages of our work compared to commonly used concept-based models over four challenging datasets. Lastly, our work is generalizable to an arbitrary number of concepts and generative factors - making it flexible enough to be suitable for various types of data.</li>
</ul>

<h3>Title: IBMEA: Exploring Variational Information Bottleneck for Multi-modal Entity Alignment</h3>
<ul>
<li><strong>Authors: </strong>Taoyu Su, Jiawei Sheng, Shicheng Wang, Xinghua Zhang, Hongbo Xu, Tingwen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19302">https://arxiv.org/abs/2407.19302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19302">https://arxiv.org/pdf/2407.19302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19302]] IBMEA: Exploring Variational Information Bottleneck for Multi-modal Entity Alignment(https://arxiv.org/abs/2407.19302)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal entity alignment (MMEA) aims to identify equivalent entities between multi-modal knowledge graphs (MMKGs), where the entities can be associated with related images. Most existing studies integrate multi-modal information heavily relying on the automatically-learned fusion module, rarely suppressing the redundant information for MMEA explicitly. To this end, we explore variational information bottleneck for multi-modal entity alignment (IBMEA), which emphasizes the alignment-relevant information and suppresses the alignment-irrelevant information in generating entity representations. Specifically, we devise multi-modal variational encoders to generate modal-specific entity representations as probability distributions. Then, we propose four modal-specific information bottleneck regularizers, limiting the misleading clues in refining modal-specific entity representations. Finally, we propose a modal-hybrid information contrastive regularizer to integrate all the refined modal-specific representations, enhancing the entity similarity between MMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and three bilingual MMEA datasets. Experimental results demonstrate that our model consistently outperforms previous state-of-the-art methods, and also shows promising and robust performance in low-resource and high-noise data scenarios.</li>
</ul>

<h3>Title: Symmetrical Joint Learning Support-query Prototypes for Few-shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qun Li, Baoquan Sun, Fu Xiao, Yonggang Qi, Bir Bhanu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19306">https://arxiv.org/abs/2407.19306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19306">https://arxiv.org/pdf/2407.19306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19306]] Symmetrical Joint Learning Support-query Prototypes for Few-shot Segmentation(https://arxiv.org/abs/2407.19306)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We propose Sym-Net, a novel framework for Few-Shot Segmentation (FSS) that addresses the critical issue of intra-class variation by jointly learning both query and support prototypes in a symmetrical manner. Unlike previous methods that generate query prototypes solely by matching query features to support prototypes, which is a form of bias learning towards the few-shot support samples, Sym-Net leverages a balanced symmetrical learning approach for both query and support prototypes, ensuring that the learning process does not favor one set (support or query) over the other. One of main modules of Sym-Net is the visual-text alignment-based prototype aggregation module, which is not just query-guided prototype refinement, it is a jointly learning from both support and query samples, which makes the model beneficial for handling intra-class discrepancies and allows it to generalize better to new, unseen classes. Specifically, a parameter-free prior mask generation module is designed to accurately localize both local and global regions of the query object by using sliding windows of different sizes and a self-activation kernel to suppress incorrect background matches. Additionally, to address the information loss caused by spatial pooling during prototype learning, a top-down hyper-correlation module is integrated to capture multi-scale spatial relationships between support and query images. This approach is further jointly optimized by implementing a co-optimized hard triplet mining strategy. Experimental results show that the proposed Sym-Net outperforms state-of-the-art models, which demonstrates that jointly learning support-query prototypes in a symmetrical manner for FSS offers a promising direction to enhance segmentation performance with limited annotated data.</li>
</ul>

<h3>Title: Comprehensive Attribution: Inherently Explainable Vision Model with Feature Detector</h3>
<ul>
<li><strong>Authors: </strong>Xianren Zhang, Dongwon Lee, Suhang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19308">https://arxiv.org/abs/2407.19308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19308">https://arxiv.org/pdf/2407.19308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19308]] Comprehensive Attribution: Inherently Explainable Vision Model with Feature Detector(https://arxiv.org/abs/2407.19308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As deep vision models' popularity rapidly increases, there is a growing emphasis on explanations for model predictions. The inherently explainable attribution method aims to enhance the understanding of model behavior by identifying the important regions in images that significantly contribute to predictions. It is achieved by cooperatively training a selector (generating an attribution map to identify important features) and a predictor (making predictions using the identified features). Despite many advancements, existing methods suffer from the incompleteness problem, where discriminative features are masked out, and the interlocking problem, where the non-optimized selector initially selects noise, causing the predictor to fit on this noise and perpetuate the cycle. To address these problems, we introduce a new objective that discourages the presence of discriminative features in the masked-out regions thus enhancing the comprehensiveness of feature selection. A pre-trained detector is introduced to detect discriminative features in the masked-out region. If the selector selects noise instead of discriminative features, the detector can observe and break the interlocking situation by penalizing the selector. Extensive experiments show that our model makes accurate predictions with higher accuracy than the regular black-box model, and produces attribution maps with high feature coverage, localization ability, fidelity and robustness. Our code will be available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Ensembling convolutional neural networks for human skin segmentation</h3>
<ul>
<li><strong>Authors: </strong>Patryk Kuban, Michal Kawulok</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19310">https://arxiv.org/abs/2407.19310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19310">https://arxiv.org/pdf/2407.19310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19310]] Ensembling convolutional neural networks for human skin segmentation(https://arxiv.org/abs/2407.19310)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Detecting and segmenting human skin regions in digital images is an intensively explored topic of computer vision with a variety of approaches proposed over the years that have been found useful in numerous practical applications. The first methods were based on pixel-wise skin color modeling and they were later enhanced with context-based analysis to include the textural and geometrical features, recently extracted using deep convolutional neural networks. It has been also demonstrated that skin regions can be segmented from grayscale images without using color information at all. However, the possibility to combine these two sources of information has not been explored so far and we address this research gap with the contribution reported in this paper. We propose to train a convolutional network using the datasets focused on different features to create an ensemble whose individual outcomes are effectively combined using yet another convolutional network trained to produce the final segmentation map. The experimental results clearly indicate that the proposed approach outperforms the basic classifiers, as well as an ensemble based on the voting scheme. We expect that this study will help in developing new ensemble-based techniques that will improve the performance of semantic segmentation systems, reaching beyond the problem of detecting human skin.</li>
</ul>

<h3>Title: MSP-MVS: Multi-granularity Segmentation Prior Guided Multi-View Stereo</h3>
<ul>
<li><strong>Authors: </strong>Zhenlong Yuan, Cong Liu, Fei Shen, Zhaoxin Li, Tianlu Mao, Zhaoqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19323">https://arxiv.org/abs/2407.19323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19323">https://arxiv.org/pdf/2407.19323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19323]] MSP-MVS: Multi-granularity Segmentation Prior Guided Multi-View Stereo(https://arxiv.org/abs/2407.19323)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Reconstructing textureless areas in MVS poses challenges due to the absence of reliable pixel correspondences within fixed patch. Although certain methods employ patch deformation to expand the receptive field, their patches mistakenly skip depth edges to calculate areas with depth discontinuity, thereby causing ambiguity. Consequently, we introduce Multi-granularity Segmentation Prior Multi-View Stereo (MSP-MVS). Specifically, we first propose multi-granularity segmentation prior by integrating multi-granularity depth edges to restrict patch deformation within homogeneous areas. Moreover, we present anchor equidistribution that bring deformed patches with more uniformly distributed anchors to ensure an adequate coverage of their own homogeneous areas. Furthermore, we introduce iterative local search optimization to represent larger patch with sparse representative candidates, significantly boosting the expressive capacity for each patch. The state-of-the-art results on ETH3D and Tanks & Temples benchmarks demonstrate the effectiveness and robust generalization ability of our proposed method.</li>
</ul>

<h3>Title: Enhancing Group Fairness in Federated Learning through Personalization</h3>
<ul>
<li><strong>Authors: </strong>Yifan Yang, Ali Payani, Parinaz Naghizadeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19331">https://arxiv.org/abs/2407.19331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19331">https://arxiv.org/pdf/2407.19331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19331]] Enhancing Group Fairness in Federated Learning through Personalization(https://arxiv.org/abs/2407.19331)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Personalized Federated Learning (FL) algorithms collaboratively train customized models for each client, enhancing the accuracy of the learned models on the client's local data (e.g., by clustering similar clients, or by fine-tuning models locally). In this paper, we investigate the impact of such personalization techniques on the group fairness of the learned models, and show that personalization can also lead to improved (local) fairness as an unintended benefit. We begin by illustrating these benefits of personalization through numerical experiments comparing two classes of personalized FL algorithms (clustering and fine-tuning) against a baseline FedAvg algorithm, elaborating on the reasons behind improved fairness using personalized FL, and then providing analytical support. Motivated by these, we further propose a new, Fairness-aware Federated Clustering Algorithm, Fair-FCA, in which clients can be clustered to obtain a (tuneable) fairness-accuracy tradeoff. Through numerical experiments, we demonstrate the ability of Fair-FCA to strike a balance between accuracy and fairness at the client level.</li>
</ul>

<h3>Title: Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification</h3>
<ul>
<li><strong>Authors: </strong>Santosh V. Patapati</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19340">https://arxiv.org/abs/2407.19340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19340">https://arxiv.org/pdf/2407.19340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19340]] Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification(https://arxiv.org/abs/2407.19340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide. This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings. The proposed architecture incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses a two-shot learning based GPT-4 model to process text data. This is the first work to incorporate large language models into a multi-modal architecture for this task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge cross-validation split and Leave-One-Subject-Out cross-validation split, surpassing all baseline models and multiple state-of-the-art models. In Leave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score of 85.95%, a precision of 80%, and a recall of 92.86%.</li>
</ul>

<h3>Title: Inference-Time Selective Debiasing</h3>
<ul>
<li><strong>Authors: </strong>Gleb Kuzmin, Nemeesh Yadav, Ivan Smirnov, Timothy Baldwin, Artem Shelmanov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19345">https://arxiv.org/abs/2407.19345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19345">https://arxiv.org/pdf/2407.19345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19345]] Inference-Time Selective Debiasing(https://arxiv.org/abs/2407.19345)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We propose selective debiasing -- an inference-time safety mechanism that aims to increase the overall quality of models in terms of prediction performance and fairness in the situation when re-training a model is prohibitive. The method is inspired by selective prediction, where some predictions that are considered low quality are discarded at inference time. In our approach, we identify the potentially biased model predictions and, instead of discarding them, we debias them using LEACE -- a post-processing debiasing method. To select problematic predictions, we propose a bias quantification approach based on KL divergence, which achieves better results than standard UQ methods. Experiments with text classification datasets demonstrate that selective debiasing helps to close the performance gap between post-processing methods and at-training and pre-processing debiasing techniques.</li>
</ul>

<h3>Title: Polynomial Regression as a Task for Understanding In-context Learning Through Finetuning and Alignment</h3>
<ul>
<li><strong>Authors: </strong>Max Wilcoxson, Morten Svendgård, Ria Doshi, Dylan Davis, Reya Vir, Anant Sahai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19346">https://arxiv.org/abs/2407.19346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19346">https://arxiv.org/pdf/2407.19346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19346]] Polynomial Regression as a Task for Understanding In-context Learning Through Finetuning and Alignment(https://arxiv.org/abs/2407.19346)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Simple function classes have emerged as toy problems to better understand in-context-learning in transformer-based architectures used for large language models. But previously proposed simple function classes like linear regression or multi-layer-perceptrons lack the structure required to explore things like prompting and alignment within models capable of in-context-learning. We propose univariate polynomial regression as a function class that is just rich enough to study prompting and alignment, while allowing us to visualize and understand what is going on clearly.</li>
</ul>

<h3>Title: The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies</h3>
<ul>
<li><strong>Authors: </strong>Feng He, Tianqing Zhu, Dayong Ye, Bo Liu, Wanlei Zhou, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19354">https://arxiv.org/abs/2407.19354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19354">https://arxiv.org/pdf/2407.19354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19354]] The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies(https://arxiv.org/abs/2407.19354)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks. LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks. The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities. At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed. This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents. We begin by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats. We then discuss the impacts of these threats on humans, environment, and other agents. Subsequently, we review existing defensive strategies, and finally explore future trends. Additionally, the survey incorporates diverse case studies to facilitate a more accessible understanding. By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications.</li>
</ul>

<h3>Title: Seamless Website Fingerprinting in Multiple Environments</h3>
<ul>
<li><strong>Authors: </strong>Chuxu Song, Zining Fan, Hao Wang, Richard Martin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19365">https://arxiv.org/abs/2407.19365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19365">https://arxiv.org/pdf/2407.19365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19365]] Seamless Website Fingerprinting in Multiple Environments(https://arxiv.org/abs/2407.19365)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Website fingerprinting (WF) attacks identify the websites visited over anonymized connections by analyzing patterns in network traffic flows, such as packet sizes, directions, or interval times using a machine learning classifier. Previous studies showed WF attacks achieve high classification accuracy. However, several issues call into question whether existing WF approaches are realizable in practice and thus motivate a re-exploration. Due to Tor's performance issues and resulting poor browsing experience, the vast majority of users opt for Virtual Private Networking (VPN) despite VPNs weaker privacy protections. Many other past assumptions are increasingly unrealistic as web technology advances. Our work addresses several key limitations of prior art. First, we introduce a new approach that classifies entire websites rather than individual web pages. Site-level classification uses traffic from all site components, including advertisements, multimedia, and single-page applications. Second, our Convolutional Neural Network (CNN) uses only the jitter and size of 500 contiguous packets from any point in a TCP stream, in contrast to prior work requiring heuristics to find page boundaries. Our seamless approach makes eavesdropper attack models realistic. Using traces from a controlled browser, we show our CNN matches observed traffic to a website with over 90% accuracy. We found the training traffic quality is critical as classification accuracy is significantly reduced when the training data lacks variability in network location, performance, and clients' computational capability. We enhanced the base CNN's efficacy using domain adaptation, allowing it to discount irrelevant features, such as network location. Lastly, we evaluate several defensive strategies against seamless WF attacks.</li>
</ul>

<h3>Title: ClickDiff: Click to Induce Semantic Contact Map for Controllable Grasp Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Peiming Li, Ziyi Wang, Mengyuan Liu, Hong Liu, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19370">https://arxiv.org/abs/2407.19370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19370">https://arxiv.org/pdf/2407.19370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19370]] ClickDiff: Click to Induce Semantic Contact Map for Controllable Grasp Generation with Diffusion Models(https://arxiv.org/abs/2407.19370)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Grasp generation aims to create complex hand-object interactions with a specified object. While traditional approaches for hand generation have primarily focused on visibility and diversity under scene constraints, they tend to overlook the fine-grained hand-object interactions such as contacts, resulting in inaccurate and undesired grasps. To address these challenges, we propose a controllable grasp generation task and introduce ClickDiff, a controllable conditional generation model that leverages a fine-grained Semantic Contact Map (SCM). Particularly when synthesizing interactive grasps, the method enables the precise control of grasp synthesis through either user-specified or algorithmically predicted Semantic Contact Map. Specifically, to optimally utilize contact supervision constraints and to accurately model the complex physical structure of hands, we propose a Dual Generation Framework. Within this framework, the Semantic Conditional Module generates reasonable contact maps based on fine-grained contact information, while the Contact Conditional Module utilizes contact maps alongside object point clouds to generate realistic grasps. We evaluate the evaluation criteria applicable to controllable grasp generation. Both unimanual and bimanual generation experiments on GRAB and ARCTIC datasets verify the validity of our proposed method, demonstrating the efficacy and robustness of ClickDiff, even with previously unseen objects. Our code is available at this https URL.</li>
</ul>

<h3>Title: Deep State-Space Generative Model For Correlated Time-to-Event Predictions</h3>
<ul>
<li><strong>Authors: </strong>Yuan Xue, Denny Zhou, Nan Du, Andrew M. Dai, Zhen Xu, Kun Zhang, Claire Cui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19371">https://arxiv.org/abs/2407.19371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19371">https://arxiv.org/pdf/2407.19371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19371]] Deep State-Space Generative Model For Correlated Time-to-Event Predictions(https://arxiv.org/abs/2407.19371)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Capturing the inter-dependencies among multiple types of clinically-critical events is critical not only to accurate future event prediction, but also to better treatment planning. In this work, we propose a deep latent state-space generative model to capture the interactions among different types of correlated clinical events (e.g., kidney failure, mortality) by explicitly modeling the temporal dynamics of patients' latent states. Based on these learned patient states, we further develop a new general discrete-time formulation of the hazard rate function to estimate the survival distribution of patients with significantly improved accuracy. Extensive evaluations over real EMR data show that our proposed model compares favorably to various state-of-the-art baselines. Furthermore, our method also uncovers meaningful insights about the latent correlations among mortality and different types of organ failures.</li>
</ul>

<h3>Title: Empowering Clinicians with Medical Decision Transformers: A Framework for Sepsis Treatment</h3>
<ul>
<li><strong>Authors: </strong>Aamer Abdul Rahman, Pranav Agarwal, Rita Noumeir, Philippe Jouvet, Vincent Michalski, Samira Ebrahimi Kahou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19380">https://arxiv.org/abs/2407.19380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19380">https://arxiv.org/pdf/2407.19380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19380]] Empowering Clinicians with Medical Decision Transformers: A Framework for Sepsis Treatment(https://arxiv.org/abs/2407.19380)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning has shown promise for solving tasks in safety-critical settings, such as clinical decision support. Its application, however, has been limited by the lack of interpretability and interactivity for clinicians. To address these challenges, we propose the medical decision transformer (MeDT), a novel and versatile framework based on the goal-conditioned reinforcement learning paradigm for sepsis treatment recommendation. MeDT uses the decision transformer architecture to learn a policy for drug dosage recommendation. During offline training, MeDT utilizes collected treatment trajectories to predict administered treatments for each time step, incorporating known treatment outcomes, target acuity scores, past treatment decisions, and current and past medical states. This analysis enables MeDT to capture complex dependencies among a patient's medical history, treatment decisions, outcomes, and short-term effects on stability. Our proposed conditioning uses acuity scores to address sparse reward issues and to facilitate clinician-model interactions, enhancing decision-making. Following training, MeDT can generate tailored treatment recommendations by conditioning on the desired positive outcome (survival) and user-specified short-term stability improvements. We carry out rigorous experiments on data from the MIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT recommends interventions that outperform or are competitive with existing offline reinforcement learning methods while enabling a more interpretable, personalized and clinician-directed approach.</li>
</ul>

<h3>Title: Multi-modal Imaging Genomics Transformer: Attentive Integration of Imaging with Genomic Biomarkers for Schizophrenia Classification</h3>
<ul>
<li><strong>Authors: </strong>Nagur Shareef Shaik, Teja Krishna Cherukuri, Vince D. Calhoun, Dong Hye Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19385">https://arxiv.org/abs/2407.19385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19385">https://arxiv.org/pdf/2407.19385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19385]] Multi-modal Imaging Genomics Transformer: Attentive Integration of Imaging with Genomic Biomarkers for Schizophrenia Classification(https://arxiv.org/abs/2407.19385)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Schizophrenia (SZ) is a severe brain disorder marked by diverse cognitive impairments, abnormalities in brain structure, function, and genetic factors. Its complex symptoms and overlap with other psychiatric conditions challenge traditional diagnostic methods, necessitating advanced systems to improve precision. Existing research studies have mostly focused on imaging data, such as structural and functional MRI, for SZ diagnosis. There has been less focus on the integration of genomic features despite their potential in identifying heritable SZ traits. In this study, we introduce a Multi-modal Imaging Genomics Transformer (MIGTrans), that attentively integrates genomics with structural and functional imaging data to capture SZ-related neuroanatomical and connectome abnormalities. MIGTrans demonstrated improved SZ classification performance with an accuracy of 86.05% (+/- 0.02), offering clear interpretations and identifying significant genomic locations and brain morphological/connectivity patterns associated with SZ.</li>
</ul>

<h3>Title: Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets</h3>
<ul>
<li><strong>Authors: </strong>Tianxiao Zhang, Wenju Xu, Bo Luo, Guanghui Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19394">https://arxiv.org/abs/2407.19394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19394">https://arxiv.org/pdf/2407.19394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19394]] Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets(https://arxiv.org/abs/2407.19394)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The Vision Transformer (ViT) leverages the Transformer's encoder to capture global information by dividing images into patches and achieves superior performance across various computer vision tasks. However, the self-attention mechanism of ViT captures the global context from the outset, overlooking the inherent relationships between neighboring pixels in images or videos. Transformers mainly focus on global information while ignoring the fine-grained local details. Consequently, ViT lacks inductive bias during image or video dataset training. In contrast, convolutional neural networks (CNNs), with their reliance on local filters, possess an inherent inductive bias, making them more efficient and quicker to converge than ViT with less data. In this paper, we present a lightweight Depth-Wise Convolution module as a shortcut in ViT models, bypassing entire Transformer blocks to ensure the models capture both local and global information with minimal overhead. Additionally, we introduce two architecture variants, allowing the Depth-Wise Convolution modules to be applied to multiple Transformer blocks for parameter savings, and incorporating independent parallel Depth-Wise Convolution modules with different kernels to enhance the acquisition of local information. The proposed approach significantly boosts the performance of ViT models on image classification, object detection and instance segmentation by a large margin, especially on small datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet for image classification, and COCO for object detection and instance segmentation. The source code can be accessed at this https URL.</li>
</ul>

<h3>Title: IDEA: A Flexible Framework of Certified Unlearning for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, Jundong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19398">https://arxiv.org/abs/2407.19398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19398">https://arxiv.org/pdf/2407.19398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19398]] IDEA: A Flexible Framework of Certified Unlearning for Graph Neural Networks(https://arxiv.org/abs/2407.19398)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have been increasingly deployed in a plethora of applications. However, the graph data used for training may contain sensitive personal information of the involved individuals. Once trained, GNNs typically encode such information in their learnable parameters. As a consequence, privacy leakage may happen when the trained GNNs are deployed and exposed to potential attackers. Facing such a threat, machine unlearning for GNNs has become an emerging technique that aims to remove certain personal information from a trained GNN. Among these techniques, certified unlearning stands out, as it provides a solid theoretical guarantee of the information removal effectiveness. Nevertheless, most of the existing certified unlearning methods for GNNs are only designed to handle node and edge unlearning requests. Meanwhile, these approaches are usually tailored for either a specific design of GNN or a specially designed training objective. These disadvantages significantly jeopardize their flexibility. In this paper, we propose a principled framework named IDEA to achieve flexible and certified unlearning for GNNs. Specifically, we first instantiate four types of unlearning requests on graphs, and then we propose an approximation approach to flexibly handle these unlearning requests over diverse GNNs. We further provide theoretical guarantee of the effectiveness for the proposed approach as a certification. Different from existing alternatives, IDEA is not designed for any specific GNNs or optimization objectives to perform certified unlearning, and thus can be easily generalized. Extensive experiments on real-world datasets demonstrate the superiority of IDEA in multiple key perspectives.</li>
</ul>

<h3>Title: Word Segmentation for Asian Languages: Chinese, Korean, and Japanese</h3>
<ul>
<li><strong>Authors: </strong>Matthew Rho, Yexin Tian, Qin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19400">https://arxiv.org/abs/2407.19400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19400">https://arxiv.org/pdf/2407.19400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19400]] Word Segmentation for Asian Languages: Chinese, Korean, and Japanese(https://arxiv.org/abs/2407.19400)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We provide a detailed overview of various approaches to word segmentation of Asian Languages, specifically Chinese, Korean, and Japanese languages. For each language, approaches to deal with word segmentation differs. We also include our analysis about certain advantages and disadvantages to each method. In addition, there is room for future work in this field.</li>
</ul>

<h3>Title: Complete Security and Privacy for AI Inference in Decentralized Systems</h3>
<ul>
<li><strong>Authors: </strong>Hongyang Zhang, Yue Zhao, Claudio Angione, Harry Yang, James Buban, Ahmad Farhan, Fielding Johnston, Patrick Colangelo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19401">https://arxiv.org/abs/2407.19401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19401">https://arxiv.org/pdf/2407.19401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19401]] Complete Security and Privacy for AI Inference in Decentralized Systems(https://arxiv.org/abs/2407.19401)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>The need for data security and model integrity has been accentuated by the rapid adoption of AI and ML in data-driven domains including healthcare, finance, and security. Large models are crucial for tasks like diagnosing diseases and forecasting finances but tend to be delicate and not very scalable. Decentralized systems solve this issue by distributing the workload and reducing central points of failure. Yet, data and processes spread across different nodes can be at risk of unauthorized access, especially when they involve sensitive information. Nesa solves these challenges with a comprehensive framework using multiple techniques to protect data and model outputs. This includes zero-knowledge proofs for secure model verification. The framework also introduces consensus-based verification checks for consistent outputs across nodes and confirms model integrity. Split Learning divides models into segments processed by different nodes for data privacy by preventing full data access at any single point. For hardware-based security, trusted execution environments are used to protect data and computations within secure zones. Nesa's state-of-the-art proofs and principles demonstrate the framework's effectiveness, making it a promising approach for securely democratizing artificial intelligence.</li>
</ul>

<h3>Title: NVC-1B: A Large Neural Video Coding Model</h3>
<ul>
<li><strong>Authors: </strong>Xihua Sheng, Chuanbo Tang, Li Li, Dong Liu, Feng Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19402">https://arxiv.org/abs/2407.19402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19402">https://arxiv.org/pdf/2407.19402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19402]] NVC-1B: A Large Neural Video Coding Model(https://arxiv.org/abs/2407.19402)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The emerging large models have achieved notable progress in the fields of natural language processing and computer vision. However, large models for neural video coding are still unexplored. In this paper, we try to explore how to build a large neural video coding model. Based on a small baseline model, we gradually scale up the model sizes of its different coding parts, including the motion encoder-decoder, motion entropy model, contextual encoder-decoder, contextual entropy model, and temporal context mining module, and analyze the influence of model sizes on video compression performance. Then, we explore to use different architectures, including CNN, mixed CNN-Transformer, and Transformer architectures, to implement the neural video coding model and analyze the influence of model architectures on video compression performance. Based on our exploration results, we design the first neural video coding model with more than 1 billion parameters -- NVC-1B. Experimental results show that our proposed large model achieves a significant video compression performance improvement over the small baseline model, and represents the state-of-the-art compression efficiency. We anticipate large models may bring up the video coding technologies to the next level.</li>
</ul>

<h3>Title: LLAVADI: What Matters For Multimodal Large Language Models Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shilin Xu, Xiangtai Li, Haobo Yuan, Lu Qi, Yunhai Tong, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19409">https://arxiv.org/abs/2407.19409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19409">https://arxiv.org/pdf/2407.19409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19409]] LLAVADI: What Matters For Multimodal Large Language Models Distillation(https://arxiv.org/abs/2407.19409)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent surge in Multimodal Large Language Models (MLLMs) has showcased their remarkable potential for achieving generalized intelligence by integrating visual understanding into Large Language Models.Nevertheless, the sheer model size of MLLMs leads to substantial memory and computational demands that hinder their widespread deployment. In this work, we do not propose a new efficient model structure or train small-scale MLLMs from scratch. Instead, we focus on what matters for training small-scale MLLMs through knowledge distillation, which is the first step from the multimodal distillation perspective. Our extensive studies involve training strategies, model choices, and distillation algorithms in the knowledge distillation process. These results show that joint alignment for both tokens and logit alignment plays critical roles in teacher-student frameworks. In addition, we draw a series of intriguing observations from this study. By evaluating different benchmarks and proper strategy, even a 2.7B small-scale model can perform on par with larger models with 7B or 13B parameters. Our code and models will be publicly available for further research.</li>
</ul>

<h3>Title: UniGAP: A Universal and Adaptive Graph Upsampling Approach to Mitigate Over-Smoothing in Node Classification Tasks</h3>
<ul>
<li><strong>Authors: </strong>Xiaotang Wang, Yun Zhu, Haizhou Shi, Yongchao Liu, Chuntao Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19420">https://arxiv.org/abs/2407.19420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19420">https://arxiv.org/pdf/2407.19420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19420]] UniGAP: A Universal and Adaptive Graph Upsampling Approach to Mitigate Over-Smoothing in Node Classification Tasks(https://arxiv.org/abs/2407.19420)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In the graph domain, deep graph networks based on Message Passing Neural Networks (MPNNs) or Graph Transformers often cause over-smoothing of node features, limiting their expressive capacity. Many upsampling techniques involving node and edge manipulation have been proposed to mitigate this issue. However, these methods often require extensive manual labor, resulting in suboptimal performance and lacking a universal integration strategy. In this study, we introduce UniGAP, a universal and adaptive graph upsampling technique for graph data. It provides a universal framework for graph upsampling, encompassing most current methods as variants. Moreover, UniGAP serves as a plug-in component that can be seamlessly and adaptively integrated with existing GNNs to enhance performance and mitigate the over-smoothing problem. Through extensive experiments, UniGAP demonstrates significant improvements over heuristic data augmentation methods across various datasets and metrics. We analyze how graph structure evolves with UniGAP, identifying key bottlenecks where over-smoothing occurs, and providing insights into how UniGAP addresses this issue. Lastly, we show the potential of combining UniGAP with large language models (LLMs) to further improve downstream performance. Our code is available at: this https URL</li>
</ul>

<h3>Title: Reputation-Driven Asynchronous Federated Learning for Enhanced Trajectory Prediction with Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Weiliang Chen, Li Jia, Yang Zhou, Qianqian Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19428">https://arxiv.org/abs/2407.19428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19428">https://arxiv.org/pdf/2407.19428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19428]] Reputation-Driven Asynchronous Federated Learning for Enhanced Trajectory Prediction with Blockchain(https://arxiv.org/abs/2407.19428)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning combined with blockchain empowers secure data sharing in autonomous driving applications. Nevertheless, with the increasing granularity and complexity of vehicle-generated data, the lack of data quality audits raises concerns about multi-party mistrust in trajectory prediction tasks. In response, this paper proposes an asynchronous federated learning data sharing method based on an interpretable reputation quantization mechanism utilizing graph neural network tools. Data providers share data structures under differential privacy constraints to ensure security while reducing redundant data. We implement deep reinforcement learning to categorize vehicles by reputation level, which optimizes the aggregation efficiency of federated learning. Experimental results demonstrate that the proposed data sharing scheme not only reinforces the security of the trajectory prediction task but also enhances prediction accuracy.</li>
</ul>

<h3>Title: ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhen Chen, Zongming Zhang, Wenwu Guo, Xingjian Luo, Long Bai, Jinlin Wu, Hongliang Ren, Hongbin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.HC, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19435">https://arxiv.org/abs/2407.19435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19435">https://arxiv.org/pdf/2407.19435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19435]] ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon Intention Understanding(https://arxiv.org/abs/2407.19435)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Surgical instrument segmentation is crucial in surgical scene understanding, thereby facilitating surgical safety. Existing algorithms directly detected all instruments of pre-defined categories in the input image, lacking the capability to segment specific instruments according to the surgeon's intention. During different stages of surgery, surgeons exhibit varying preferences and focus toward different surgical instruments. Therefore, an instrument segmentation algorithm that adheres to the surgeon's intention can minimize distractions from irrelevant instruments and assist surgeons to a great extent. The recent Segment Anything Model (SAM) reveals the capability to segment objects following prompts, but the manual annotations for prompts are impractical during the surgery. To address these limitations in operating rooms, we propose an audio-driven surgical instrument segmentation framework, named ASI-Seg, to accurately segment the required surgical instruments by parsing the audio commands of surgeons. Specifically, we propose an intention-oriented multimodal fusion to interpret the segmentation intention from audio commands and retrieve relevant instrument details to facilitate segmentation. Moreover, to guide our ASI-Seg segment of the required surgical instruments, we devise a contrastive learning prompt encoder to effectively distinguish the required instruments from the irrelevant ones. Therefore, our ASI-Seg promotes the workflow in the operating rooms, thereby providing targeted support and reducing the cognitive load on surgeons. Extensive experiments are performed to validate the ASI-Seg framework, which reveals remarkable advantages over classical state-of-the-art and medical SAMs in both semantic segmentation and intention-oriented segmentation. The source code is available at this https URL.</li>
</ul>

<h3>Title: X-Fake: Juggling Utility Evaluation and Explanation of Simulated SAR Images</h3>
<ul>
<li><strong>Authors: </strong>Zhongling Huang, Yihan Zhuang, Zipei Zhong, Feng Xu, Gong Cheng, Junwei Han</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19436">https://arxiv.org/abs/2407.19436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19436">https://arxiv.org/pdf/2407.19436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19436]] X-Fake: Juggling Utility Evaluation and Explanation of Simulated SAR Images(https://arxiv.org/abs/2407.19436)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>SAR image simulation has attracted much attention due to its great potential to supplement the scarce training data for deep learning algorithms. Consequently, evaluating the quality of the simulated SAR image is crucial for practical applications. The current literature primarily uses image quality assessment techniques for evaluation that rely on human observers' perceptions. However, because of the unique imaging mechanism of SAR, these techniques may produce evaluation results that are not entirely valid. The distribution inconsistency between real and simulated data is the main obstacle that influences the utility of simulated SAR images. To this end, we propose a novel trustworthy utility evaluation framework with a counterfactual explanation for simulated SAR images for the first time, denoted as X-Fake. It unifies a probabilistic evaluator and a causal explainer to achieve a trustworthy utility assessment. We construct the evaluator using a probabilistic Bayesian deep model to learn the posterior distribution, conditioned on real data. Quantitatively, the predicted uncertainty of simulated data can reflect the distribution discrepancy. We build the causal explainer with an introspective variational auto-encoder to generate high-resolution counterfactuals. The latent code of IntroVAE is finally optimized with evaluation indicators and prior information to generate the counterfactual explanation, thus revealing the inauthentic details of simulated data explicitly. The proposed framework is validated on four simulated SAR image datasets obtained from electromagnetic models and generative artificial intelligence approaches. The results demonstrate the proposed X-Fake framework outperforms other IQA methods in terms of utility. Furthermore, the results illustrate that the generated counterfactual explanations are trustworthy, and can further improve the data utility in applications.</li>
</ul>

<h3>Title: \textsc{Perm}: A Parametric Representation for Multi-Style 3D Hair Modeling</h3>
<ul>
<li><strong>Authors: </strong>Chengan He, Xin Sun, Zhixin Shu, Fujun Luan, Sören Pirk, Jorge Alejandro Amador Herrera, Dominik L. Michels, Tuanfeng Y. Wang, Meng Zhang, Holly Rushmeier, Yi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19451">https://arxiv.org/abs/2407.19451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19451">https://arxiv.org/pdf/2407.19451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19451]] \textsc{Perm}: A Parametric Representation for Multi-Style 3D Hair Modeling(https://arxiv.org/abs/2407.19451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present \textsc{Perm}, a learned parametric model of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair shape and local strand details, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair modeling process. We conduct extensive experiments to validate the architecture design of \textsc{Perm}, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as 3D hair parameterization, hairstyle interpolation, single-view hair reconstruction, and hair-conditioned image generation. Our code and data will be available at: \url{this https URL}.</li>
</ul>

<h3>Title: FIND: Fine-tuning Initial Noise Distribution with Policy Optimization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Changgu Chen, Libing Yang, Xiaoyan Yang, Lianggangxu Chen, Gaoqi He, CHangbo Wang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19453">https://arxiv.org/abs/2407.19453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19453">https://arxiv.org/pdf/2407.19453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19453]] FIND: Fine-tuning Initial Noise Distribution with Policy Optimization for Diffusion Models(https://arxiv.org/abs/2407.19453)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, large-scale pre-trained diffusion models have demonstrated their outstanding capabilities in image and video generation tasks. However, existing models tend to produce visual objects commonly found in the training dataset, which diverges from user input prompts. The underlying reason behind the inaccurate generated results lies in the model's difficulty in sampling from specific intervals of the initial noise distribution corresponding to the prompt. Moreover, it is challenging to directly optimize the initial distribution, given that the diffusion process involves multiple denoising steps. In this paper, we introduce a Fine-tuning Initial Noise Distribution (FIND) framework with policy optimization, which unleashes the powerful potential of pre-trained diffusion networks by directly optimizing the initial distribution to align the generated contents with user-input prompts. To this end, we first reformulate the diffusion denoising procedure as a one-step Markov decision process and employ policy optimization to directly optimize the initial distribution. In addition, a dynamic reward calibration module is proposed to ensure training stability during optimization. Furthermore, we introduce a ratio clipping algorithm to utilize historical data for network training and prevent the optimized distribution from deviating too far from the original policy to restrain excessive optimization magnitudes. Extensive experiments demonstrate the effectiveness of our method in both text-to-image and text-to-video tasks, surpassing SOTA methods in achieving consistency between prompts and the generated content. Our method achieves 10 times faster than the SOTA approach. Our homepage is available at \url{this https URL}.</li>
</ul>

<h3>Title: An Alternative to Multi-Factor Authentication with a Triple-Identity Authentication Scheme</h3>
<ul>
<li><strong>Authors: </strong>Suyun Borjigin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET, cs.HC, cs.NI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19459">https://arxiv.org/abs/2407.19459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19459">https://arxiv.org/pdf/2407.19459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19459]] An Alternative to Multi-Factor Authentication with a Triple-Identity Authentication Scheme(https://arxiv.org/abs/2407.19459)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Every user authentication scheme involves three login credentials, i.e. a username, a password and a hash value, but only one of them is associated with a user identity. However, this identity is actually not robust enough to protect the whole system, while the login entries (i.e., the username and password forms) have not been effectively protected. Furthermore, the extra factor in a system adding multi-factor authentication is transmitted in cyberspace and operated by users. If more identities can be employed for the two login forms to associate with all login credentials, and if the corresponding identifiers are not transmitted in cyberspace and operated by users, such a system can be more robust even without relying on a third-party service. To this end, a triple-identity authentication scheme is designed within a dual-password login-authentication system, which defines identities for the username and the login password, respectively. Therefore, in addition to the traditional server verification, the system can verify the identifiers at the username and password forms in succession. In the triple-identity authentication, the identifiers are entirely managed by the system without involvement of users or a third-party service, and they are concealed, incommunicable, inaccessible and independent of personal information. Thus, they are useless in online attacks.</li>
</ul>

<h3>Title: White Matter Geometry-Guided Score-Based Diffusion Model for Tissue Microstructure Imputation in Tractography Imaging</h3>
<ul>
<li><strong>Authors: </strong>Yui Lo, Yuqian Chen, Fan Zhang, Dongnan Liu, Leo Zekelman, Suheyla Cetin-Karayumak, Yogesh Rathi, Weidong Cai, Lauren J. O'Donnell</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19460">https://arxiv.org/abs/2407.19460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19460">https://arxiv.org/pdf/2407.19460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19460]] White Matter Geometry-Guided Score-Based Diffusion Model for Tissue Microstructure Imputation in Tractography Imaging(https://arxiv.org/abs/2407.19460)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Parcellation of white matter tractography provides anatomical features for disease prediction, anatomical tract segmentation, surgical brain mapping, and non-imaging phenotype classifications. However, parcellation does not always reach 100% accuracy due to various factors, including inter-individual anatomical variability and the quality of neuroimaging scan data. The failure to identify parcels causes a problem of missing microstructure data values, which is especially challenging for downstream tasks that analyze large brain datasets. In this work, we propose a novel deep-learning model to impute tissue microstructure: the White Matter Geometry-guided Diffusion (WMG-Diff) model. Specifically, we first propose a deep score-based guided diffusion model to impute tissue microstructure for diffusion magnetic resonance imaging (dMRI) tractography fiber clusters. Second, we propose a white matter atlas geometric relationship-guided denoising function to guide the reverse denoising process at the subject-specific level. Third, we train and evaluate our model on a large dataset with 9342 subjects. Comprehensive experiments for tissue microstructure imputation and a downstream non-imaging phenotype prediction task demonstrate that our proposed WMG-Diff outperforms state-of-the-art methods.</li>
</ul>

<h3>Title: MVPbev: Multi-view Perspective Image Generation from BEV with Test-time Controllability and Generalizability</h3>
<ul>
<li><strong>Authors: </strong>Buyu Liu, Kai Wang, Yansong Liu, Jun Bao, Tingting Han, Jun Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19468">https://arxiv.org/abs/2407.19468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19468">https://arxiv.org/pdf/2407.19468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19468]] MVPbev: Multi-view Perspective Image Generation from BEV with Test-time Controllability and Generalizability(https://arxiv.org/abs/2407.19468)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work aims to address the multi-view perspective RGB generation from text prompts given Bird-Eye-View(BEV) semantics. Unlike prior methods that neglect layout consistency, lack the ability to handle detailed text prompts, or are incapable of generalizing to unseen view points, MVPbev simultaneously generates cross-view consistent images of different perspective views with a two-stage design, allowing object-level control and novel view generation at test-time. Specifically, MVPbev firstly projects given BEV semantics to perspective view with camera parameters, empowering the model to generalize to unseen view points. Then we introduce a multi-view attention module where special initialization and de-noising processes are introduced to explicitly enforce local consistency among overlapping views w.r.t. cross-view homography. Last but not least, MVPbev further allows test-time instance-level controllability by refining a pre-trained text-to-image diffusion model. Our extensive experiments on NuScenes demonstrate that our method is capable of generating high-resolution photorealistic images from text descriptions with thousands of training samples, surpassing the state-of-the-art methods under various evaluation metrics. We further demonstrate the advances of our method in terms of generalizability and controllability with the help of novel evaluation metrics and comprehensive human analysis. Our code, data, and model can be found in \url{this https URL}.</li>
</ul>

<h3>Title: Combined CNN and ViT features off-the-shelf: Another astounding baseline for recognition</h3>
<ul>
<li><strong>Authors: </strong>Fernando Alonso-Fernandez, Kevin Hernandez-Diaz, Prayag Tiwari, Josef Bigun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19472">https://arxiv.org/abs/2407.19472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19472">https://arxiv.org/pdf/2407.19472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19472]] Combined CNN and ViT features off-the-shelf: Another astounding baseline for recognition(https://arxiv.org/abs/2407.19472)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We apply pre-trained architectures, originally developed for the ImageNet Large Scale Visual Recognition Challenge, for periocular recognition. These architectures have demonstrated significant success in various computer vision tasks beyond the ones for which they were designed. This work builds on our previous study using off-the-shelf Convolutional Neural Network (CNN) and extends it to include the more recently proposed Vision Transformers (ViT). Despite being trained for generic object classification, middle-layer features from CNNs and ViTs are a suitable way to recognize individuals based on periocular images. We also demonstrate that CNNs and ViTs are highly complementary since their combination results in boosted accuracy. In addition, we show that a small portion of these pre-trained models can achieve good accuracy, resulting in thinner models with fewer parameters, suitable for resource-limited environments such as mobiles. This efficiency improves if traditional handcrafted features are added as well.</li>
</ul>

<h3>Title: Breaking the Balance of Power: Commitment Attacks on Ethereum's Reward Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Roozbeh Sarenche, Ertem Nusret Tas, Barnabe Monnot, Caspar Schwarz-Schilling, Bart Preneel</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19479">https://arxiv.org/abs/2407.19479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19479">https://arxiv.org/pdf/2407.19479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19479]] Breaking the Balance of Power: Commitment Attacks on Ethereum's Reward Mechanism(https://arxiv.org/abs/2407.19479)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, fair</a></li>
<li><strong>Abstract: </strong>Validators in permissionless, large-scale blockchains (e.g., Ethereum) are typically payoff-maximizing, rational actors. Ethereum relies on in-protocol incentives, like rewards for validators delivering correct and timely votes, to induce honest behavior and secure the blockchain. However, external incentives, such as the block proposer's opportunity to capture maximal extractable value (MEV), may tempt validators to deviate from honest protocol participation. We show a series of commitment attacks on LMD GHOST, a core part of Ethereum's consensus mechanism. We demonstrate how a single adversarial block proposer can orchestrate long-range chain reorganizations by manipulating Ethereum's reward system for timely votes. These attacks disrupt the intended balance of power between proposers and voters: by leveraging credible threats, the adversarial proposer can coerce voters from previous slots into supporting blocks that conflict with the honest chain, enabling a chain reorganization at no cost to the adversary. In response, we introduce a novel reward mechanism that restores the voters' role as a check against proposer power. Our proposed mitigation is fairer and more decentralized -- not only in the context of these attacks -- but also practical for implementation in Ethereum.</li>
</ul>

<h3>Title: Large-scale cervical precancerous screening via AI-assisted cytology whole slide image analysis</h3>
<ul>
<li><strong>Authors: </strong>Honglin Li, Yusuan Sun, Chenglu Zhu, Yunlong Zhang, Shichuan Zhang, Zhongyi Shui, Pingyi Chen, Jingxiong Li, Sunyi Zheng, Can Cui, Lin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19512">https://arxiv.org/abs/2407.19512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19512">https://arxiv.org/pdf/2407.19512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19512]] Large-scale cervical precancerous screening via AI-assisted cytology whole slide image analysis(https://arxiv.org/abs/2407.19512)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Cervical Cancer continues to be the leading gynecological malignancy, posing a persistent threat to women's health on a global scale. Early screening via cytology Whole Slide Image (WSI) diagnosis is critical to prevent this Cancer progression and improve survival rate, but pathologist's single test suffers inevitable false negative due to the immense number of cells that need to be reviewed within a WSI. Though computer-aided automated diagnostic models can serve as strong complement for pathologists, their effectiveness is hampered by the paucity of extensive and detailed annotations, coupled with the limited interpretability and robustness. These factors significantly hinder their practical applicability and reliability in clinical settings. To tackle these challenges, we develop an AI approach, which is a Scalable Technology for Robust and Interpretable Diagnosis built on Extensive data (STRIDE) of cervical cytology. STRIDE addresses the bottleneck of limited annotations by integrating patient-level labels with a small portion of cell-level labels through an end-to-end training strategy, facilitating scalable learning across extensive datasets. To further improve the robustness to real-world domain shifts of cytology slide-making and imaging, STRIDE employs color adversarial samples training that mimic staining and imaging variations. Lastly, to achieve pathologist-level interpretability for the trustworthiness in clinical settings, STRIDE can generate explanatory textual descriptions that simulates pathologists' diagnostic processes by cell image feature and textual description alignment. Conducting extensive experiments and evaluations in 183 medical centers with a dataset of 341,889 WSIs and 0.1 billion cells from cervical cytology patients, STRIDE has demonstrated a remarkable superiority over previous state-of-the-art techniques.</li>
</ul>

<h3>Title: Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation</h3>
<ul>
<li><strong>Authors: </strong>Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19523">https://arxiv.org/abs/2407.19523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19523">https://arxiv.org/pdf/2407.19523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19523]] Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation(https://arxiv.org/abs/2407.19523)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Meta-learning is a practical learning paradigm to transfer skills across tasks from a few examples. Nevertheless, the existence of task distribution shifts tends to weaken meta-learners' generalization capability, particularly when the task distribution is naively hand-crafted or based on simple priors that fail to cover typical scenarios sufficiently. Here, we consider explicitly generative modeling task distributions placed over task identifiers and propose robustifying fast adaptation from adversarial training. Our approach, which can be interpreted as a model of a Stackelberg game, not only uncovers the task structure during problem-solving from an explicit generative model but also theoretically increases the adaptation robustness in worst cases. This work has practical implications, particularly in dealing with task distribution shifts in meta-learning, and contributes to theoretical insights in the field. Our method demonstrates its robustness in the presence of task subpopulation shifts and improved performance over SOTA baselines in extensive experiments. The project is available at this https URL.</li>
</ul>

<h3>Title: VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary</h3>
<ul>
<li><strong>Authors: </strong>Hanjun Luo, Ziye Deng, Haoyu Huang, Xuecheng Liu, Ruizhe Chen, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19524">https://arxiv.org/abs/2407.19524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19524">https://arxiv.org/pdf/2407.19524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19524]] VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via SLM-Based Prompt Engineering and Generative Adversary(https://arxiv.org/abs/2407.19524)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, generative</a></li>
<li><strong>Abstract: </strong>With the rapid development of Text-to-Image models, biases in human image generation against demographic groups social attract more and more concerns. Existing methods are designed based on certain models with fixed prompts, unable to accommodate the trend of high-speed updating of Text-to-Image (T2I) models and variable prompts in practical scenes. Additionally, they fail to consider the possibility of hallucinations, leading to deviations between expected and actual results. To address this issue, we introduce VersusDebias, a novel and universal debiasing framework for biases in T2I models, consisting of one generative adversarial mechanism (GAM) and one debiasing generation mechanism using a small language model (SLM). The self-adaptive GAM generates specialized attribute arrays for each prompts for diminishing the influence of hallucinations from T2I models. The SLM uses prompt engineering to generate debiased prompts for the T2I model, providing zero-shot debiasing ability and custom optimization for different models. Extensive experiments demonstrate VersusDebias's capability to rectify biases on arbitrary models across multiple protected attributes simultaneously, including gender, race, and age. Furthermore, VersusDebias outperforms existing methods in both zero-shot and few-shot situations, illustrating its extraordinary utility. Our work is openly accessible to the research community to ensure the reproducibility.</li>
</ul>

<h3>Title: Impact of Decoding Methods on Human Alignment of Conversational LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shaz Furniturewala, Kokil Jaidka, Yashvardhan Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19526">https://arxiv.org/abs/2407.19526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19526">https://arxiv.org/pdf/2407.19526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19526]] Impact of Decoding Methods on Human Alignment of Conversational LLMs(https://arxiv.org/abs/2407.19526)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To be included into chatbot systems, Large language models (LLMs) must be aligned with human conversational conventions. However, being trained mainly on web-scraped data gives existing LLMs a voice closer to informational text than actual human speech. In this paper, we examine the effect of decoding methods on the alignment between LLM-generated and human conversations, including Beam Search, Top K Sampling, and Nucleus Sampling. We present new measures of alignment in substance, style, and psychometric orientation, and experiment with two conversation datasets. Our results provide subtle insights: better alignment is attributed to fewer beams in Beam Search and lower values of P in Nucleus Sampling. We also find that task-oriented and open-ended datasets perform differently in terms of alignment, indicating the significance of taking into account the context of the interaction.</li>
</ul>

<h3>Title: Motamot: A Dataset for Revealing the Supremacy of Large Language Models over Transformer Models in Bengali Political Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Fatema Tuj Johora Faria, Mukaffi Bin Moin, Rabeya Islam Mumu, Md Mahabubul Alam Abir, Abrar Nawar Alfy, Mohammad Shafiul Alam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19528">https://arxiv.org/abs/2407.19528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19528">https://arxiv.org/pdf/2407.19528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19528]] Motamot: A Dataset for Revealing the Supremacy of Large Language Models over Transformer Models in Bengali Political Sentiment Analysis(https://arxiv.org/abs/2407.19528)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. Analyzing political sentiment is critical for understanding the complexities of public opinion processes, especially during election seasons. It gives significant information on voter preferences, attitudes, and current trends. In this study, we investigate political sentiment analysis during Bangladeshi elections, specifically examining how effectively Pre-trained Language Models (PLMs) and Large Language Models (LLMs) capture complex sentiment characteristics. Our study centers on the creation of the "Motamot" dataset, comprising 7,058 instances annotated with positive and negative sentiments, sourced from diverse online newspaper portals, forming a comprehensive resource for political sentiment analysis. We meticulously evaluate the performance of various PLMs including BanglaBERT, Bangla BERT Base, XLM-RoBERTa, mBERT, and sahajBERT, alongside LLMs such as Gemini 1.5 Pro and GPT 3.5 Turbo. Moreover, we explore zero-shot and few-shot learning strategies to enhance our understanding of political sentiment analysis methodologies. Our findings underscore BanglaBERT's commendable accuracy of 88.10% among PLMs. However, the exploration into LLMs reveals even more promising results. Through the adept application of Few-Shot learning techniques, Gemini 1.5 Pro achieves an impressive accuracy of 96.33%, surpassing the remarkable performance of GPT 3.5 Turbo, which stands at 94%. This underscores Gemini 1.5 Pro's status as the superior performer in this comparison.</li>
</ul>

<h3>Title: Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction via Knowledge Distillation and Random Data Erasing</h3>
<ul>
<li><strong>Authors: </strong>Heejoon Koo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19540">https://arxiv.org/abs/2407.19540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19540">https://arxiv.org/pdf/2407.19540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19540]] Overcoming Uncertain Incompleteness for Robust Multimodal Sequential Diagnosis Prediction via Knowledge Distillation and Random Data Erasing(https://arxiv.org/abs/2407.19540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we present NECHO v2, a novel framework designed to enhance the predictive accuracy of multimodal sequential patient diagnoses under uncertain missing visit sequences, a common challenge in clinical settings. Firstly, we modify NECHO to handle uncertain modality representation dominance under the imperfect data. Next, we develop a systematic knowledge distillation by employing the modified NECHO as both teacher and student. It encompasses a modality-wise contrastive and hierarchical distillation, transformer representation random distillation, along with other distillations to align representations tightly and effectively. We also utilise random erasing on individual data points within sequences during both training and distillation of teacher to lightly simulate scenario with missing visit information to foster effective knowledge transfer. As a result, NECHO v2 verifies itself by showing superiority in multimodal sequential diagnosis prediction on both balanced and imbalanced incomplete settings on multimodal healthcare data.</li>
</ul>

<h3>Title: Temporal Feature Matters: A Framework for Diffusion Model Quantization</h3>
<ul>
<li><strong>Authors: </strong>Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen Lu, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19547">https://arxiv.org/abs/2407.19547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19547">https://arxiv.org/pdf/2407.19547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19547]] Temporal Feature Matters: A Framework for Diffusion Model Quantization(https://arxiv.org/abs/2407.19547)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The Diffusion models, widely used for image generation, face significant challenges related to their broad applicability due to prolonged inference times and high memory demands. Efficient Post-Training Quantization (PTQ) is crucial to address these issues in traditional models. Unlike those models, diffusion models critically rely on the time-step $t$ for effective multi-round denoising. Typically, $t$ from the finite set $\{1, \ldots, T\}$ is encoded into a hypersensitive temporal feature by several modules, entirely independent of the sampling data. However, existing PTQ methods do not optimize these modules individually. Instead, they employ unsuitable reconstruction objectives and complex calibration methods, leading to significant disturbances in the temporal feature and denoising trajectory. To address these challenges, we introduce a novel quantization framework: 1)~TIB-based Maintenance: Based on our innovative Temporal Information Block~(TIB) definition, Temporal Information-aware Reconstruction~(TIAR) and Finite Set Calibration~(FSC) are developed to efficiently align full precision temporal features. 2)~Cache-based Maintenance: Instead of indirect and complex optimization for the related modules, pre-computing and caching quantized counterparts of temporal features are developed to minimize errors. 3)~Disturbance-aware Selection: Employ temporal feature errors to guide a fine-grained selection for superior maintenance. This framework preserves most of the temporal information and ensures high-quality end-to-end generation. Extensive testing on various datasets and diffusion models confirms our superior results. Notably, our approach closely matches the performance of the full-precision model under 4-bit quantization. Furthermore, the quantized SD-XL model achieves hardware acceleration of 2.20$\times$ on CPU and 5.76$\times$ on GPU demonstrating its efficiency.</li>
</ul>

<h3>Title: Cycle3D: High-quality and Consistent Image-to-3D Generation via Generation-Reconstruction Cycle</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19548">https://arxiv.org/abs/2407.19548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19548">https://arxiv.org/pdf/2407.19548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19548]] Cycle3D: High-quality and Consistent Image-to-3D Generation via Generation-Reconstruction Cycle(https://arxiv.org/abs/2407.19548)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent 3D large reconstruction models typically employ a two-stage process, including first generate multi-view images by a multi-view diffusion model, and then utilize a feed-forward model to reconstruct images to 3D content.However, multi-view diffusion models often produce low-quality and inconsistent images, adversely affecting the quality of the final 3D reconstruction. To address this issue, we propose a unified 3D generation framework called Cycle3D, which cyclically utilizes a 2D diffusion-based generation module and a feed-forward 3D reconstruction module during the multi-step diffusion process. Concretely, 2D diffusion model is applied for generating high-quality texture, and the reconstruction model guarantees multi-view consistency.Moreover, 2D diffusion model can further control the generated content and inject reference-view information for unseen views, thereby enhancing the diversity and texture consistency of 3D generation during the denoising process. Extensive experiments demonstrate the superior ability of our method to create 3D content with high-quality and consistency compared with state-of-the-art baselines.</li>
</ul>

<h3>Title: Exploring the Adversarial Robustness of CLIP for AI-generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Vincenzo De Rosa, Fabrizio Guillaro, Giovanni Poggi, Davide Cozzolino, Luisa Verdoliva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19553">https://arxiv.org/abs/2407.19553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19553">https://arxiv.org/pdf/2407.19553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19553]] Exploring the Adversarial Robustness of CLIP for AI-generated Image Detection(https://arxiv.org/abs/2407.19553)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>In recent years, many forensic detectors have been proposed to detect AI-generated images and prevent their use for malicious purposes. Convolutional neural networks (CNNs) have long been the dominant architecture in this field and have been the subject of intense study. However, recently proposed Transformer-based detectors have been shown to match or even outperform CNN-based detectors, especially in terms of generalization. In this paper, we study the adversarial robustness of AI-generated image detectors, focusing on Contrastive Language-Image Pretraining (CLIP)-based methods that rely on Visual Transformer backbones and comparing their performance with CNN-based methods. We study the robustness to different adversarial attacks under a variety of conditions and analyze both numerical results and frequency-domain patterns. CLIP-based detectors are found to be vulnerable to white-box attacks just like CNN-based detectors. However, attacks do not easily transfer between CNN-based and CLIP-based methods. This is also confirmed by the different distribution of the adversarial noise patterns in the frequency domain. Overall, this analysis provides new insights into the properties of forensic detectors that can help to develop more effective strategies.</li>
</ul>

<h3>Title: Diffie-Hellman Picture Show: Key Exchange Stories from Commercial VoWiFi Deployments</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Karl Gegenhuber, Florian Holzbauer, Philipp Frenzel, Edgar Weippl, Adrian Dabrowski</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19556">https://arxiv.org/abs/2407.19556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19556">https://arxiv.org/pdf/2407.19556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19556]] Diffie-Hellman Picture Show: Key Exchange Stories from Commercial VoWiFi Deployments(https://arxiv.org/abs/2407.19556)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>Voice over Wi-Fi (VoWiFi) uses a series of IPsec tunnels to deliver IP-based telephony from the subscriber's phone (User Equipment, UE) into the Mobile Network Operator's (MNO) core network via an Internet-facing endpoint, the Evolved Packet Data Gateway (ePDG). IPsec tunnels are set up in phases. The first phase negotiates the cryptographic algorithm and parameters and performs a key exchange via the Internet Key Exchange protocol, while the second phase (protected by the above-established encryption) performs the authentication. An insecure key exchange would jeopardize the later stages and the data's security and confidentiality. In this paper, we analyze the phase 1 settings and implementations as they are found in phones as well as in commercially deployed networks worldwide. On the UE side, we identified a recent 5G baseband chipset from a major manufacturer that allows for fallback to weak, unannounced modes and verified it experimentally. On the MNO side -- among others -- we identified 13 operators (totaling an estimated 140 million subscribers) on three continents that all use the same globally static set of ten private keys, serving them at random. Those not-so-private keys allow the decryption of the shared keys of every VoWiFi user of all those operators. All these operators deployed their core network from one common manufacturer.</li>
</ul>

<h3>Title: Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models</h3>
<ul>
<li><strong>Authors: </strong>Jifeng Wang, Kaouther Messaoud, Yuejiang Liu, Juergen Gall, Alexandre Alahi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19564">https://arxiv.org/abs/2407.19564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19564">https://arxiv.org/pdf/2407.19564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19564]] Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models(https://arxiv.org/abs/2407.19564)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent progress in motion forecasting has been substantially driven by self-supervised pre-training. However, adapting pre-trained models for specific downstream tasks, especially motion prediction, through extensive fine-tuning is often inefficient. This inefficiency arises because motion prediction closely aligns with the masked pre-training tasks, and traditional full fine-tuning methods fail to fully leverage this alignment. To address this, we introduce Forecast-PEFT, a fine-tuning strategy that freezes the majority of the model's parameters, focusing adjustments on newly introduced prompts and adapters. This approach not only preserves the pre-learned representations but also significantly reduces the number of parameters that need retraining, thereby enhancing efficiency. This tailored strategy, supplemented by our method's capability to efficiently adapt to different datasets, enhances model efficiency and ensures robust performance across datasets without the need for extensive retraining. Our experiments show that Forecast-PEFT outperforms traditional full fine-tuning methods in motion prediction tasks, achieving higher accuracy with only 17% of the trainable parameters typically required. Moreover, our comprehensive adaptation, Forecast-FT, further improves prediction performance, evidencing up to a 9.6% enhancement over conventional baseline methods. Code will be available at this https URL.</li>
</ul>

<h3>Title: Are LLMs Good Annotators for Discourse-level Event Relation Extraction?</h3>
<ul>
<li><strong>Authors: </strong>Kangda Wei, Aayush Gautam, Ruihong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19568">https://arxiv.org/abs/2407.19568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19568">https://arxiv.org/pdf/2407.19568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19568]] Are LLMs Good Annotators for Discourse-level Event Relation Extraction?(https://arxiv.org/abs/2407.19568)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated proficiency in a wide array of natural language processing tasks. However, its effectiveness over discourse-level event relation extraction (ERE) tasks remains unexplored. In this paper, we assess the effectiveness of LLMs in addressing discourse-level ERE tasks characterized by lengthy documents and intricate relations encompassing coreference, temporal, causal, and subevent types. Evaluation is conducted using an commercial model, GPT-3.5, and an open-source model, LLaMA-2. Our study reveals a notable underperformance of LLMs compared to the baseline established through supervised learning. Although Supervised Fine-Tuning (SFT) can improve LLMs performance, it does not scale well compared to the smaller supervised baseline model. Our quantitative and qualitative analysis shows that LLMs have several weaknesses when applied for extracting event relations, including a tendency to fabricate event mentions, and failures to capture transitivity rules among relations, detect long distance relations, or comprehend contexts with dense event mentions.</li>
</ul>

<h3>Title: Maximal Extractable Value Mitigation Approaches in Ethereum and Layer-2 Chains: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Zeinab Alipanahloo, Abdelhakim Senhaji Hafid, Kaiwen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19572">https://arxiv.org/abs/2407.19572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19572">https://arxiv.org/pdf/2407.19572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19572]] Maximal Extractable Value Mitigation Approaches in Ethereum and Layer-2 Chains: A Comprehensive Survey(https://arxiv.org/abs/2407.19572)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, fair</a></li>
<li><strong>Abstract: </strong>Maximal Extractable Value (MEV) represents a pivotal challenge within the Ethereum ecosystem; it impacts the fairness, security, and efficiency of both Layer 1 (L1) and Layer 2 (L2) networks. MEV arises when miners or validators manipulate transaction ordering to extract additional value, often at the expense of other network participants. This not only affects user experience by introducing unpredictability and potential financial losses but also threatens the underlying principles of decentralization and trust. Given the growing complexity of blockchain applications, particularly with the increase of Decentralized Finance (DeFi) protocols, addressing MEV is crucial. This paper presents a comprehensive survey of MEV mitigation techniques as applied to both Ethereums L1 and various L2 solutions. We provide a novel categorization of mitigation strategies; we also describe the challenges, ranging from transaction sequencing and cryptographic methods to reconfiguring decentralized applications (DApps) to reduce front-running opportunities. We investigate their effectiveness, implementation challenges, and impact on network performance. By synthesizing current research, real-world applications, and emerging trends, this paper aims to provide a detailed roadmap for researchers, developers, and policymakers to understand and combat MEV in an evolving blockchain landscape.</li>
</ul>

<h3>Title: Memory-efficient Training of LLMs with Larger Mini-batches</h3>
<ul>
<li><strong>Authors: </strong>Dang Nguyen, Wenhan Yang, Rathul Anand, Yu Yang, Baharan Mirzasoleiman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19580">https://arxiv.org/abs/2407.19580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19580">https://arxiv.org/pdf/2407.19580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19580]] Memory-efficient Training of LLMs with Larger Mini-batches(https://arxiv.org/abs/2407.19580)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training with larger mini-batches improves the performance and convergence rate of training machine learning models. However, training with large mini-batches becomes prohibitive for Large Language Models (LLMs) with billions of parameters, due to the large GPU memory requirement. To address this problem, we propose finding small mini-batches that simulate the dynamics of training with larger mini-batches. Specifically, we formulate selecting smaller mini-batches of examples that closely capture gradients of large mini-batches as a submodular maximization problem. Nevertheless, the very large dimensionality of the gradients makes the problem very challenging to solve. To address this, we leverage ideas from zeroth-order optimization and neural network pruning to find lower-dimensional gradient estimates that allow finding high-quality subsets effectively with a limited amount of memory. We prove the superior convergence rate of training on the small mini-batches found by our method and empirically show its effectiveness. Our method can effectively reduce the memory requirement by 2x and speed up training by 1.3x, as we confirm for fine-tuning Phi-2 on MathInstruct. Our method can be easily stacked with LoRA and other memory-efficient methods to further reduce the memory requirements of training LLMs.</li>
</ul>

<h3>Title: SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain</h3>
<ul>
<li><strong>Authors: </strong>Pierre Colombo, Telmo Pires, Malik Boudiaf, Rui Melo, Dominic Culver, Sofia Morgado, Etienne Malaboeuf, Gabriel Hautreux, Johanne Charpentier, Michael Desa</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19584">https://arxiv.org/abs/2407.19584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19584">https://arxiv.org/pdf/2407.19584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19584]] SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain(https://arxiv.org/abs/2407.19584)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce SaulLM-54B and SaulLM-141B, two large language models (LLMs) tailored for the legal sector. These models, which feature architectures of 54 billion and 141 billion parameters, respectively, are based on the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is guided by large-scale domain adaptation, divided into three strategies: (1) the exploitation of continued pretraining involving a base corpus that includes over 540 billion of legal tokens, (2) the implementation of a specialized legal instruction-following protocol, and (3) the alignment of model outputs with human preferences in legal interpretations. The integration of synthetically generated data in the second and third steps enhances the models' capabilities in interpreting and processing legal texts, effectively reaching state-of-the-art performance and outperforming previous open-source models on LegalBench-Instruct. This work explores the trade-offs involved in domain-specific adaptation at this scale, offering insights that may inform future studies on domain adaptation using strong decoder models. Building upon SaulLM-7B, this study refines the approach to produce an LLM better equipped for legal tasks. We are releasing base, instruct, and aligned versions on top of SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and collaborative research.</li>
</ul>

<h3>Title: Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture</h3>
<ul>
<li><strong>Authors: </strong>ShahRukh Athar, Shunsuke Saito, Zhengyu Yang, Stanislav Pidhorsky, Chen Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19593">https://arxiv.org/abs/2407.19593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19593">https://arxiv.org/pdf/2407.19593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19593]] Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture(https://arxiv.org/abs/2407.19593)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Creating photorealistic avatars for individuals traditionally involves extensive capture sessions with complex and expensive studio devices like the LightStage system. While recent strides in neural representations have enabled the generation of photorealistic and animatable 3D avatars from quick phone scans, they have the capture-time lighting baked-in, lack facial details and have missing regions in areas such as the back of the ears. Thus, they lag in quality compared to studio-captured avatars. In this paper, we propose a method that bridges this gap by generating studio-like illuminated texture maps from short, monocular phone captures. We do this by parameterizing the phone texture maps using the $W^+$ space of a StyleGAN2, enabling near-perfect reconstruction. Then, we finetune a StyleGAN2 by sampling in the $W^+$ parameterized space using a very small set of studio-captured textures as an adversarial training signal. To further enhance the realism and accuracy of facial details, we super-resolve the output of the StyleGAN2 using carefully designed diffusion model that is guided by image gradients of the phone-captured texture map. Once trained, our method excels at producing studio-like facial texture maps from casual monocular smartphone videos. Demonstrating its capabilities, we showcase the generation of photorealistic, uniformly lit, complete avatars from monocular phone captures. \href{this http URL}{The project page can be found here.}</li>
</ul>

<h3>Title: Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19594">https://arxiv.org/abs/2407.19594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19594">https://arxiv.org/pdf/2407.19594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19594]] Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge(https://arxiv.org/abs/2407.19594)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, we introduce a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision.</li>
</ul>

<h3>Title: Look Hear: Gaze Prediction for Speech-directed Human Attention</h3>
<ul>
<li><strong>Authors: </strong>Sounak Mondal, Seoyoung Ahn, Zhibo Yang, Niranjan Balasubramanian, Dimitris Samaras, Gregory Zelinsky, Minh Hoai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19605">https://arxiv.org/abs/2407.19605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19605">https://arxiv.org/pdf/2407.19605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19605]] Look Hear: Gaze Prediction for Speech-directed Human Attention(https://arxiv.org/abs/2407.19605)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>For computer systems to effectively interact with humans using spoken language, they need to understand how the words being generated affect the users' moment-by-moment attention. Our study focuses on the incremental prediction of attention as a person is seeing an image and hearing a referring expression defining the object in the scene that should be fixated by gaze. To predict the gaze scanpaths in this incremental object referral task, we developed the Attention in Referral Transformer model or ART, which predicts the human fixations spurred by each word in a referring expression. ART uses a multimodal transformer encoder to jointly learn gaze behavior and its underlying grounding tasks, and an autoregressive transformer decoder to predict, for each word, a variable number of fixations based on fixation history. To train ART, we created RefCOCO-Gaze, a large-scale dataset of 19,738 human gaze scanpaths, corresponding to 2,094 unique image-expression pairs, from 220 participants performing our referral task. In our quantitative and qualitative analyses, ART not only outperforms existing methods in scanpath prediction, but also appears to capture several human attention patterns, such as waiting, scanning, and verification.</li>
</ul>

<h3>Title: TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Selma Wanna, Ryan Barron, Nick Solovyev, Maksim E. Eren, Manish Bhattarai, Kim Rasmussen, Boian S. Alexandrov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19616">https://arxiv.org/abs/2407.19616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19616">https://arxiv.org/pdf/2407.19616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19616]] TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs(https://arxiv.org/abs/2407.19616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Topic modeling is a technique for organizing and extracting themes from large collections of unstructured text. Non-negative matrix factorization (NMF) is a common unsupervised approach that decomposes a term frequency-inverse document frequency (TF-IDF) matrix to uncover latent topics and segment the dataset accordingly. While useful for highlighting patterns and clustering documents, NMF does not provide explicit topic labels, necessitating subject matter experts (SMEs) to assign labels manually. We present a methodology for automating topic labeling in documents clustered via NMF with automatic model determination (NMFk). By leveraging the output of NMFk and employing prompt engineering, we utilize large language models (LLMs) to generate accurate topic labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs demonstrates the effectiveness of our method in enhancing knowledge management and document organization.</li>
</ul>

<h3>Title: AgEval: A Benchmark for Zero-Shot and Few-Shot Plant Stress Phenotyping with Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Arbab Arshad, Talukder Zaki Jubery, Tirtho Roy, Rim Nassiri, Asheesh K. Singh, Arti Singh, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19617">https://arxiv.org/abs/2407.19617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19617">https://arxiv.org/pdf/2407.19617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19617]] AgEval: A Benchmark for Zero-Shot and Few-Shot Plant Stress Phenotyping with Multimodal LLMs(https://arxiv.org/abs/2407.19617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Plant stress phenotyping traditionally relies on expert assessments and specialized models, limiting scalability in agriculture. Recent advances in multimodal large language models (LLMs) offer potential solutions to this challenge. We present AgEval, a benchmark comprising 12 diverse plant stress phenotyping tasks, to evaluate these models' capabilities. Our study assesses zero-shot and few-shot in-context learning performance of state-of-the-art models, including Claude, GPT, Gemini, and LLaVA. Results show significant performance improvements with few-shot learning, with F1 scores increasing from 46.24% to 73.37% in 8-shot identification for the best-performing model. Few-shot examples from other classes in the dataset have negligible or negative impacts, although having the exact category example helps to increase performance by 15.38%. We also quantify the consistency of model performance across different classes within each task, finding that the coefficient of variance (CV) ranges from 26.02% to 58.03% across models, implying that subject matter expertise is needed - of 'difficult' classes - to achieve reliability in performance. AgEval establishes baseline metrics for multimodal LLMs in agricultural applications, offering insights into their promise for enhancing plant stress phenotyping at scale. Benchmark and code can be accessed at: https://anonymous.4open.science/r/AgEval/</li>
</ul>

<h3>Title: Text2LiDAR: Text-guided LiDAR Point Cloud Generation via Equirectangular Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yang Wu, Kaihua Zhang, Jianjun Qian, Jin Xie, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19628">https://arxiv.org/abs/2407.19628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19628">https://arxiv.org/pdf/2407.19628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19628]] Text2LiDAR: Text-guided LiDAR Point Cloud Generation via Equirectangular Transformer(https://arxiv.org/abs/2407.19628)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The complex traffic environment and various weather conditions make the collection of LiDAR data expensive and challenging. Achieving high-quality and controllable LiDAR data generation is urgently needed, controlling with text is a common practice, but there is little research in this field. To this end, we propose Text2LiDAR, the first efficient, diverse, and text-controllable LiDAR data generation model. Specifically, we design an equirectangular transformer architecture, utilizing the designed equirectangular attention to capture LiDAR features in a manner with data characteristics. Then, we design a control-signal embedding injector to efficiently integrate control signals through the global-to-focused attention mechanism. Additionally, we devise a frequency modulator to assist the model in recovering high-frequency details, ensuring the clarity of the generated point cloud. To foster development in the field and optimize text-controlled generation performance, we construct nuLiDARtext which offers diverse text descriptors for 34,149 LiDAR point clouds from 850 scenes. Experiments on uncontrolled and text-controlled generation in various forms on KITTI-360 and nuScenes datasets demonstrate the superiority of our approach.</li>
</ul>

<h3>Title: From Pre-training Corpora to Large Language Models: What Factors Influence LLM Performance in Causal Discovery Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Tao Feng, Lizhen Qu, Niket Tandon, Zhuang Li, Xiaoxi Kang, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19638">https://arxiv.org/abs/2407.19638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19638">https://arxiv.org/pdf/2407.19638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19638]] From Pre-training Corpora to Large Language Models: What Factors Influence LLM Performance in Causal Discovery Tasks?(https://arxiv.org/abs/2407.19638)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in artificial intelligence have seen Large Language Models (LLMs) demonstrate notable proficiency in causal discovery tasks. This study explores the factors influencing the performance of LLMs in causal discovery tasks. Utilizing open-source LLMs, we examine how the frequency of causal relations within their pre-training corpora affects their ability to accurately respond to causal discovery queries. Our findings reveal that a higher frequency of causal mentions correlates with better model performance, suggesting that extensive exposure to causal information during training enhances the models' causal discovery capabilities. Additionally, we investigate the impact of context on the validity of causal relations. Our results indicate that LLMs might exhibit divergent predictions for identical causal relations when presented in different contexts. This paper provides the first comprehensive analysis of how different factors contribute to LLM performance in causal discovery tasks.</li>
</ul>

<h3>Title: Segmented Private Data Aggregation in the Multi-message Shuffle Model</h3>
<ul>
<li><strong>Authors: </strong>Shaowei Wang, Ruilin Yang, Sufen Zeng, Kaiqi Yu, Rundong Mei, Shaozheng Huang, Wei Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19639">https://arxiv.org/abs/2407.19639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19639">https://arxiv.org/pdf/2407.19639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19639]] Segmented Private Data Aggregation in the Multi-message Shuffle Model(https://arxiv.org/abs/2407.19639)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The shuffle model of differential privacy (DP) offers compelling privacy-utility trade-offs in decentralized settings (e.g., internet of things, mobile edge networks). Particularly, the multi-message shuffle model, where each user may contribute multiple messages, has shown that accuracy can approach that of the central model of DP. However, existing studies typically assume a uniform privacy protection level for all users, which may deter conservative users from participating and prevent liberal users from contributing more information, thereby reducing the overall data utility, such as the accuracy of aggregated statistics. In this work, we pioneer the study of segmented private data aggregation within the multi-message shuffle model of DP, introducing flexible privacy protection for users and enhanced utility for the aggregation server. Our framework not only protects users' data but also anonymizes their privacy level choices to prevent potential data leakage from these choices. To optimize the privacy-utility-communication trade-offs, we explore approximately optimal configurations for the number of blanket messages and conduct almost tight privacy amplification analyses within the shuffle model. Through extensive experiments, we demonstrate that our segmented multi-message shuffle framework achieves a reduction of about 50\% in estimation error compared to existing approaches, significantly enhancing both privacy and utility.</li>
</ul>

<h3>Title: ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck</h3>
<ul>
<li><strong>Authors: </strong>Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19651">https://arxiv.org/abs/2407.19651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19651">https://arxiv.org/pdf/2407.19651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19651]] ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck(https://arxiv.org/abs/2407.19651)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents the first-ever study of adapting compressed image latents to suit the needs of downstream vision tasks that adopt Multimodal Large Language Models (MLLMs). MLLMs have extended the success of large language models to modalities (e.g. images) beyond text, but their billion scale hinders deployment on resource-constrained end devices. While cloud-hosted MLLMs could be available, transmitting raw, uncompressed images captured by end devices to the cloud requires an efficient image compression system. To address this, we focus on emerging neural image compression and propose a novel framework with a lightweight transform-neck and a surrogate loss to adapt compressed image latents for MLLM-based vision tasks. The proposed framework is generic and applicable to multiple application scenarios, where the neural image codec can be (1) pre-trained for human perception without updating, (2) fully updated for joint human and machine perception, or (3) fully updated for only machine perception. The transform-neck trained with the surrogate loss is universal, for it can serve various downstream vision tasks enabled by a variety of MLLMs that share the same visual encoder. Our framework has the striking feature of excluding the downstream MLLMs from training the transform-neck, and potentially the neural image codec as well. This stands out from most existing coding for machine approaches that involve downstream networks in training and thus could be impractical when the networks are MLLMs. Extensive experiments on different neural image codecs and various MLLM-based vision tasks show that our method achieves great rate-accuracy performance with much less complexity, demonstrating its effectiveness.</li>
</ul>

<h3>Title: Towards Detecting IoT Event Spoofing Attacks Using Time-Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Uzma Maroof, Gustavo Batista, Arash Shaghaghi, Sanjay Jha</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19662">https://arxiv.org/abs/2407.19662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19662">https://arxiv.org/pdf/2407.19662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19662]] Towards Detecting IoT Event Spoofing Attacks Using Time-Series Classification(https://arxiv.org/abs/2407.19662)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Internet of Things (IoT) devices have grown in popularity since they can directly interact with the real world. Home automation systems automate these interactions. IoT events are crucial to these systems' decision-making but are often unreliable. Security vulnerabilities allow attackers to impersonate events. Using statistical machine learning, IoT event fingerprints from deployed sensors have been used to detect spoofed events. Multivariate temporal data from these sensors has structural and temporal properties that statistical machine learning cannot learn. These schemes' accuracy depends on the knowledge base; the larger, the more accurate. However, the lack of huge datasets with enough samples of each IoT event in the nascent field of IoT can be a bottleneck. In this work, we deployed advanced machine learning to detect event-spoofing assaults. The temporal nature of sensor data lets us discover important patterns with fewer events. Our rigorous investigation of a publicly available real-world dataset indicates that our time-series-based solution technique learns temporal features from sensor data faster than earlier work, even with a 100- or 500-fold smaller training sample, making it a realistic IoT solution.</li>
</ul>

<h3>Title: Adaptive Soft Error Protection for Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinghua Xue, Cheng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19664">https://arxiv.org/abs/2407.19664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19664">https://arxiv.org/pdf/2407.19664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19664]] Adaptive Soft Error Protection for Deep Learning(https://arxiv.org/abs/2407.19664)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>The rising incidence of soft errors in hardware systems represents a considerable risk to the reliability of deep learning systems and can precipitate severe malfunctions. Although essential, soft error mitigation can impose substantial costs on deep learning systems that are inherently demanding in terms of computation and memory. Previous research has primarily explored variations in vulnerability among different components of computing engines or neural networks, aiming for selective protection to minimize protection overhead. Our approach diverges from these studies by recognizing that the susceptibility of deep learning tasks to soft errors is heavily input-dependent. Notably, some inputs are simpler for deep learning models and inherently exhibit greater tolerance to soft errors. Conversely, more complex inputs are prone to soft error impact. Based on these insights, we introduce an adaptive soft error protection strategy that tailors protection to the computational demands of individual inputs. To implement this strategy, we develop a metric for assessing the complexity of inputs and deploy a lightweight machine learning algorithm to gauge input difficulty. Subsequently, we employ robust protection for challenging inputs and minimal protection for simpler ones. Our experimental evaluation across diverse datasets and deep learning tasks reveals that our adaptive strategy reduces the soft error protection overhead by an average of 46.9%, without compromising system reliability.</li>
</ul>

<h3>Title: SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Zhang, Hou Pong Chan, Yiran Zhao, Mahani Aljunied, Jianyu Wang, Chaoqun Liu, Yue Deng, Zhiqiang Hu, Weiwen Xu, Yew Ken Chia, Xin Li, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19672">https://arxiv.org/abs/2407.19672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19672">https://arxiv.org/pdf/2407.19672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19672]] SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages(https://arxiv.org/abs/2407.19672)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable abilities across various tasks, yet their development has predominantly centered on high-resource languages like English and Chinese, leaving low-resource languages underserved. To address this disparity, we present SeaLLMs 3, the latest iteration of the SeaLLMs model family, tailored for Southeast Asian languages. This region, characterized by its rich linguistic diversity, has lacked adequate language technology support. SeaLLMs 3 aims to bridge this gap by covering a comprehensive range of languages spoken in this region, including English, Chinese, Indonesian, Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao, Tamil, and Javanese. Leveraging efficient language enhancement techniques and a specially constructed instruction tuning dataset, SeaLLMs 3 significantly reduces training costs while maintaining high performance and versatility. Our model excels in tasks such as world knowledge, mathematical reasoning, translation, and instruction following, achieving state-of-the-art performance among similarly sized models. Additionally, we prioritized safety and reliability by addressing both general and culture-specific considerations and incorporated mechanisms to reduce hallucinations. This work underscores the importance of inclusive AI, showing that advanced LLM capabilities can benefit underserved linguistic and cultural communities.</li>
</ul>

<h3>Title: Harnessing Large Vision and Language Models in Agriculture: A Review</h3>
<ul>
<li><strong>Authors: </strong>Hongyan Zhu, Shuai Qin, Min Su, Chengzhi Lin, Anjie Li, Junfeng Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19679">https://arxiv.org/abs/2407.19679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19679">https://arxiv.org/pdf/2407.19679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19679]] Harnessing Large Vision and Language Models in Agriculture: A Review(https://arxiv.org/abs/2407.19679)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large models can play important roles in many domains. Agriculture is another key factor affecting the lives of people around the world. It provides food, fabric, and coal for humanity. However, facing many challenges such as pests and diseases, soil degradation, global warming, and food security, how to steadily increase the yield in the agricultural sector is a problem that humans still need to solve. Large models can help farmers improve production efficiency and harvest by detecting a series of agricultural production tasks such as pests and diseases, soil quality, and seed quality. It can also help farmers make wise decisions through a variety of information, such as images, text, etc. Herein, we delve into the potential applications of large models in agriculture, from large language model (LLM) and large vision model (LVM) to large vision-language models (LVLM). After gaining a deeper understanding of multimodal large language models (MLLM), it can be recognized that problems such as agricultural image processing, agricultural question answering systems, and agricultural machine automation can all be solved by large models. Large models have great potential in the field of agriculture. We outline the current applications of agricultural large models, and aims to emphasize the importance of large models in the domain of agriculture. In the end, we envisage a future in which famers use MLLM to accomplish many tasks in agriculture, which can greatly improve agricultural production efficiency and yield.</li>
</ul>

<h3>Title: Revisiting the robustness of post-hoc interpretability methods</h3>
<ul>
<li><strong>Authors: </strong>Jiawen Wei, Hugues Turbé, Gianmarco Mengaldo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19683">https://arxiv.org/abs/2407.19683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19683">https://arxiv.org/pdf/2407.19683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19683]] Revisiting the robustness of post-hoc interpretability methods(https://arxiv.org/abs/2407.19683)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Post-hoc interpretability methods play a critical role in explainable artificial intelligence (XAI), as they pinpoint portions of data that a trained deep learning model deemed important to make a decision. However, different post-hoc interpretability methods often provide different results, casting doubts on their accuracy. For this reason, several evaluation strategies have been proposed to understand the accuracy of post-hoc interpretability. Many of these evaluation strategies provide a coarse-grained assessment -- i.e., they evaluate how the performance of the model degrades on average by corrupting different data points across multiple samples. While these strategies are effective in selecting the post-hoc interpretability method that is most reliable on average, they fail to provide a sample-level, also referred to as fine-grained, assessment. In other words, they do not measure the robustness of post-hoc interpretability methods. We propose an approach and two new metrics to provide a fine-grained assessment of post-hoc interpretability methods. We show that the robustness is generally linked to its coarse-grained performance.</li>
</ul>

<h3>Title: Efficiently and Effectively: A Two-stage Approach to Balance Plaintext and Encrypted Text for Traffic Classification</h3>
<ul>
<li><strong>Authors: </strong>Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19687">https://arxiv.org/abs/2407.19687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19687">https://arxiv.org/pdf/2407.19687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19687]] Efficiently and Effectively: A Two-stage Approach to Balance Plaintext and Encrypted Text for Traffic Classification(https://arxiv.org/abs/2407.19687)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Encrypted traffic classification is the task of identifying the application or service associated with encrypted network traffic. One effective approach for this task is to use deep learning methods to encode the raw traffic bytes directly and automatically extract features for classification (byte-based models). However, current byte-based models input raw traffic bytes, whether plaintext or encrypted text, for automated feature extraction, neglecting the distinct impacts of plaintext and encrypted text on downstream tasks. Additionally, these models primarily focus on improving classification accuracy, with little emphasis on the efficiency of models. In this paper, for the first time, we analyze the impact of plaintext and encrypted text on the model's effectiveness and efficiency. Based on our observations and findings, we propose a two-phase approach to balance the trade-off between plaintext and encrypted text in traffic classification. Specifically, Stage one is to Determine whether the Plain text is enough to be accurately Classified (DPC) using the proposed DPC Selector. This stage quickly identifies samples that can be classified using plaintext, leveraging explicit byte features in plaintext to enhance model's efficiency. Stage two aims to adaptively make a classification with the result from stage one. This stage incorporates encrypted text information for samples that cannot be classified using plaintext alone, ensuring the model's effectiveness on traffic classification tasks. Experiments on two datasets demonstrate that our proposed model achieves state-of-the-art results in both effectiveness and efficiency.</li>
</ul>

<h3>Title: Causal Interventional Prediction System for Robust and Explainable Effect Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhixuan Chu, Hui Ding, Guang Zeng, Shiyu Wang, Yiming Li</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19688">https://arxiv.org/abs/2407.19688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19688">https://arxiv.org/pdf/2407.19688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19688]] Causal Interventional Prediction System for Robust and Explainable Effect Forecasting(https://arxiv.org/abs/2407.19688)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Although the widespread use of AI systems in today's world is growing, many current AI systems are found vulnerable due to hidden bias and missing information, especially in the most commonly used forecasting system. In this work, we explore the robustness and explainability of AI-based forecasting systems. We provide an in-depth analysis of the underlying causality involved in the effect prediction task and further establish a causal graph based on treatment, adjustment variable, confounder, and outcome. Correspondingly, we design a causal interventional prediction system (CIPS) based on a variational autoencoder and fully conditional specification of multiple imputations. Extensive results demonstrate the superiority of our system over state-of-the-art methods and show remarkable versatility and extensibility in practice.</li>
</ul>

<h3>Title: Structural damage detection via hierarchical damage information with volumetric assessment</h3>
<ul>
<li><strong>Authors: </strong>Isaac Osei Agyemang, Jianwen Chen, Liaoyuan Zeng, Isaac Adjei-Mensah, Daniel Acheampong, Gordon Owusu Boateng, Adu Asare Baffour</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19694">https://arxiv.org/abs/2407.19694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19694">https://arxiv.org/pdf/2407.19694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19694]] Structural damage detection via hierarchical damage information with volumetric assessment(https://arxiv.org/abs/2407.19694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Image environments and noisy labels hinder deep learning-based inference models in structural damage detection. Post-detection, there is the challenge of reliance on manual assessments of detected damages. As a result, Guided-DetNet, characterized by Generative Attention Module (GAM), Hierarchical Elimination Algorithm (HEA), and Volumetric Contour Visual Assessment (VCVA), is proposed to mitigate complex image environments, noisy labeling, and post-detection manual assessment of structural damages. GAM leverages cross-horizontal and cross-vertical patch merging and cross foreground-background feature fusion to generate varied features to mitigate complex image environments. HEA addresses noisy labeling using hierarchical relationships among classes to refine instances given an image by eliminating unlikely class categories. VCVA assesses the severity of detected damages via volumetric representation and quantification leveraging the Dirac delta distribution. A comprehensive quantitative study, two robustness tests, and an application scenario based on the PEER Hub Image-Net dataset substantiate Guided-DetNet's promising performances. Guided-DetNet outperformed the best-compared models in a triple classification task by a difference of not less than 3% and not less than 2% in a dual detection task under varying metrics.</li>
</ul>

<h3>Title: Cross-Layer Feature Pyramid Transformer for Small Object Detection in Aerial Images</h3>
<ul>
<li><strong>Authors: </strong>Zewen Du, Zhenjiang Hu, Guiyu Zhao, Ying Jin, Hongbin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19696">https://arxiv.org/abs/2407.19696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19696">https://arxiv.org/pdf/2407.19696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19696]] Cross-Layer Feature Pyramid Transformer for Small Object Detection in Aerial Images(https://arxiv.org/abs/2407.19696)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Object detection in aerial images has always been a challenging task due to the generally small size of the objects. Most current detectors prioritize novel detection frameworks, often overlooking research on fundamental components such as feature pyramid networks. In this paper, we introduce the Cross-Layer Feature Pyramid Transformer (CFPT), a novel upsampler-free feature pyramid network designed specifically for small object detection in aerial images. CFPT incorporates two meticulously designed attention blocks with linear computational complexity: the Cross-Layer Channel-Wise Attention (CCA) and the Cross-Layer Spatial-Wise Attention (CSA). CCA achieves cross-layer interaction by dividing channel-wise token groups to perceive cross-layer global information along the spatial dimension, while CSA completes cross-layer interaction by dividing spatial-wise token groups to perceive cross-layer global information along the channel dimension. By integrating these modules, CFPT enables cross-layer interaction in one step, thereby avoiding the semantic gap and information loss associated with element-wise summation and layer-by-layer transmission. Furthermore, CFPT incorporates global contextual information, which enhances detection performance for small objects. To further enhance location awareness during cross-layer interaction, we propose the Cross-Layer Consistent Relative Positional Encoding (CCPE) based on inter-layer mutual receptive fields. We evaluate the effectiveness of CFPT on two challenging object detection datasets in aerial images, namely VisDrone2019-DET and TinyPerson. Extensive experiments demonstrate the effectiveness of CFPT, which outperforms state-of-the-art feature pyramid networks while incurring lower computational costs. The code will be released at this https URL.</li>
</ul>

<h3>Title: Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Wang, Zhixuan Chu, Yinbo Sun, Yu Liu, Yuliang Guo, Yang Chen, Huiyang Jian, Lintao Ma, Xingyu Lu, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19697">https://arxiv.org/abs/2407.19697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19697">https://arxiv.org/pdf/2407.19697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19697]] Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting(https://arxiv.org/abs/2407.19697)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate workload forecasting is critical for efficient resource management in cloud computing systems, enabling effective scheduling and autoscaling. Despite recent advances with transformer-based forecasting models, challenges remain due to the non-stationary, nonlinear characteristics of workload time series and the long-term dependencies. In particular, inconsistent performance between long-term history and near-term forecasts hinders long-range predictions. This paper proposes a novel framework leveraging self-supervised multiscale representation learning to capture both long-term and near-term workload patterns. The long-term history is encoded through multiscale representations while the near-term observations are modeled via temporal flow fusion. These representations of different scales are fused using an attention mechanism and characterized with normalizing flows to handle non-Gaussian/non-linear distributions of time series. Extensive experiments on 9 benchmarks demonstrate superiority over existing methods.</li>
</ul>

<h3>Title: Efficient Byzantine-Robust and Provably Privacy-Preserving Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenfei Nie (1), Qiang Li (1), Yuxin Yang (1 and 2), Yuede Ji (3), Binghui Wang (2) ((1) College of Computer Science and Technology, Jilin University, (2) Department of Computer Science, Illinois Institute of Technology, (3) Department of Computer Science and Engineering, University of Texas at Arlington)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19703">https://arxiv.org/abs/2407.19703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19703">https://arxiv.org/pdf/2407.19703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19703]] Efficient Byzantine-Robust and Provably Privacy-Preserving Federated Learning(https://arxiv.org/abs/2407.19703)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is an emerging distributed learning paradigm without sharing participating clients' private data. However, existing works show that FL is vulnerable to both Byzantine (security) attacks and data reconstruction (privacy) attacks. Almost all the existing FL defenses only address one of the two attacks. A few defenses address the two attacks, but they are not efficient and effective enough. We propose BPFL, an efficient Byzantine-robust and provably privacy-preserving FL method that addresses all the issues. Specifically, we draw on state-of-the-art Byzantine-robust FL methods and use similarity metrics to measure the robustness of each participating client in FL. The validity of clients are formulated as circuit constraints on similarity metrics and verified via a zero-knowledge proof. Moreover, the client models are masked by a shared random vector, which is generated based on homomorphic encryption. In doing so, the server receives the masked client models rather than the true ones, which are proven to be private. BPFL is also efficient due to the usage of non-interactive zero-knowledge proof. Experimental results on various datasets show that our BPFL is efficient, Byzantine-robust, and privacy-preserving.</li>
</ul>

<h3>Title: CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Zhu, Minghuan Tan, Min Yang, Ruixue Li, Hamid Alinejad-Rokny</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19705">https://arxiv.org/abs/2407.19705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19705">https://arxiv.org/pdf/2407.19705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19705]] CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare(https://arxiv.org/abs/2407.19705)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid progress in Large Language Models (LLMs) has prompted the creation of numerous benchmarks to evaluate their capabilities.This study focuses on the Comprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset diversity and distribution in supervised fine-tuning (SFT) may enhance LLM performance.Remarkably, We successfully trained a smaller base model to achieve scores comparable to larger models, indicating that a diverse and well-distributed dataset can optimize performance regardless of model size.This study suggests that even smaller models may reach high performance levels with carefully curated and varied this http URL integrating a wide range of instructional content, our approach addresses potential issues such as data quality inconsistencies. Our results imply that a broader spectrum of training data may enhance a model's ability to generalize and perform effectively across different medical scenarios, highlighting the importance of dataset quality and diversity in fine-tuning processes.</li>
</ul>

<h3>Title: ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Ezequiel Perez-Zarate, Oscar Ramos-Soto, Diego Oliva, Marco Perez-Cisneros</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19708">https://arxiv.org/abs/2407.19708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19708">https://arxiv.org/pdf/2407.19708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19708]] ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image Enhancement(https://arxiv.org/abs/2407.19708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement is an important task in computer vision, essential for improving the visibility and quality of images captured in non-optimal lighting conditions. Inadequate illumination can lead to significant information loss and poor image quality, impacting various applications such as surveillance. photography, or even autonomous driving. In this regard, automated methods have been developed to automatically adjust illumination in the image for a better visual perception. Current enhancement techniques often use specific datasets to enhance low-light images, but still present challenges when adapting to diverse real-world conditions, where illumination degradation may be localized to specific regions. To address this challenge, the Adaptive Light Enhancement Network (ALEN) is introduced, whose main approach is the use of a classification mechanism to determine whether local or global illumination enhancement is required. Subsequently, estimator networks adjust illumination based on this classification and simultaneously enhance color fidelity. ALEN integrates the Light Classification Network (LCNet) for illuminance categorization, complemented by the Single-Channel Network (SCNet), and Multi-Channel Network (MCNet) for precise estimation of illumination and color, respectively. Extensive experiments on publicly available datasets for low-light conditions were carried out to underscore ALEN's robust generalization capabilities, demonstrating superior performance in both quantitative metrics and qualitative assessments when compared to recent state-of-the-art methods. The ALEN not only enhances image quality in terms of visual perception but also represents an advancement in high-level vision tasks, such as semantic segmentation, as presented in this work. The code of this method is available at this https URL.</li>
</ul>

<h3>Title: Rethinking RGB-D Fusion for Semantic Segmentation in Surgical Datasets</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Abdullah Jamal, Omid Mohareri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19714">https://arxiv.org/abs/2407.19714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19714">https://arxiv.org/pdf/2407.19714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19714]] Rethinking RGB-D Fusion for Semantic Segmentation in Surgical Datasets(https://arxiv.org/abs/2407.19714)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Surgical scene understanding is a key technical component for enabling intelligent and context aware systems that can transform various aspects of surgical interventions. In this work, we focus on the semantic segmentation task, propose a simple yet effective multi-modal (RGB and depth) training framework called SurgDepth, and show state-of-the-art (SOTA) results on all publicly available datasets applicable for this task. Unlike previous approaches, which either fine-tune SOTA segmentation models trained on natural images, or encode RGB or RGB-D information using RGB only pre-trained backbones, SurgDepth, which is built on top of Vision Transformers (ViTs), is designed to encode both RGB and depth information through a simple fusion mechanism. We conduct extensive experiments on benchmark datasets including EndoVis2022, AutoLapro, LapI2I and EndoVis2017 to verify the efficacy of SurgDepth. Specifically, SurgDepth achieves a new SOTA IoU of 0.86 on EndoVis 2022 SAR-RARP50 challenge and outperforms the current best method by at least 4%, using a shallow and compute efficient decoder consisting of ConvNeXt blocks.</li>
</ul>

<h3>Title: Revolutionizing Urban Safety Perception Assessments: Integrating Multimodal Large Language Models with Street View Images</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Zhanga, Yunqin Lia, Tomohiro Fukudab, Bowen Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19719">https://arxiv.org/abs/2407.19719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19719">https://arxiv.org/pdf/2407.19719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19719]] Revolutionizing Urban Safety Perception Assessments: Integrating Multimodal Large Language Models with Street View Images(https://arxiv.org/abs/2407.19719)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Measuring urban safety perception is an important and complex task that traditionally relies heavily on human resources. This process often involves extensive field surveys, manual data collection, and subjective assessments, which can be time-consuming, costly, and sometimes inconsistent. Street View Images (SVIs), along with deep learning methods, provide a way to realize large-scale urban safety detection. However, achieving this goal often requires extensive human annotation to train safety ranking models, and the architectural differences between cities hinder the transferability of these models. Thus, a fully automated method for conducting safety evaluations is essential. Recent advances in multimodal large language models (MLLMs) have demonstrated powerful reasoning and analytical capabilities. Cutting-edge models, e.g., GPT-4 have shown surprising performance in many tasks. We employed these models for urban safety ranking on a human-annotated anchor set and validated that the results from MLLMs align closely with human perceptions. Additionally, we proposed a method based on the pre-trained Contrastive Language-Image Pre-training (CLIP) feature and K-Nearest Neighbors (K-NN) retrieval to quickly assess the safety index of the entire city. Experimental results show that our method outperforms existing training needed deep learning approaches, achieving efficient and accurate urban safety evaluations. The proposed automation for urban safety perception assessment is a valuable tool for city planners, policymakers, and researchers aiming to improve urban environments.</li>
</ul>

<h3>Title: Do Text-to-Vis Benchmarks Test Real Use of Visualisations?</h3>
<ul>
<li><strong>Authors: </strong>Hy Nguyen, Xuefei He, Andrew Reeson, Cecile Paris, Josiah Poon, Jonathan K. Kummerfeld</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19726">https://arxiv.org/abs/2407.19726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19726">https://arxiv.org/pdf/2407.19726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19726]] Do Text-to-Vis Benchmarks Test Real Use of Visualisations?(https://arxiv.org/abs/2407.19726)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are able to generate code for visualisations in response to user requests. This is a useful application, and an appealing one for NLP research because plots of data provide grounding for language. However, there are relatively few benchmarks, and it is unknown whether those that exist are representative of what people do in practice. This paper aims to answer that question through an empirical study comparing benchmark datasets and code from public repositories. Our findings reveal a substantial gap in datasets, with evaluations not testing the same distribution of chart types, attributes, and the number of actions. The only representative dataset requires modification to become an end-to-end and practical benchmark. This shows that new, more benchmarks are needed to support the development of systems that truly address users' visualisation needs. These observations will guide future data creation, highlighting which features hold genuine significance for users.</li>
</ul>

<h3>Title: Sensor Selection via GFlowNets: A Deep Generative Modeling Framework to Navigate Combinatorial Complexity</h3>
<ul>
<li><strong>Authors: </strong>Spilios Evmorfos, Zhaoyi Xu, Athina Petropulu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19736">https://arxiv.org/abs/2407.19736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19736">https://arxiv.org/pdf/2407.19736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19736]] Sensor Selection via GFlowNets: A Deep Generative Modeling Framework to Navigate Combinatorial Complexity(https://arxiv.org/abs/2407.19736)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The performance of sensor arrays in sensing and wireless communications improves with more elements, but this comes at the cost of increased energy consumption and hardware expense. This work addresses the challenge of selecting $k$ sensor elements from a set of $m$ to optimize a generic Quality-of-Service metric. Evaluating all $\binom{m}{k}$ possible sensor subsets is impractical, leading to prior solutions using convex relaxations, greedy algorithms, and supervised learning approaches. The current paper proposes a new framework that employs deep generative modeling, treating sensor selection as a deterministic Markov Decision Process where sensor subsets of size $k$ arise as terminal states. Generative Flow Networks (GFlowNets) are employed to model an action distribution conditioned on the state. Sampling actions from the aforementioned distribution ensures that the probability of arriving at a terminal state is proportional to the performance of the corresponding subset. Applied to a standard sensor selection scenario, the developed approach outperforms popular methods which are based on convex optimization and greedy algorithms. Finally, a multiobjective formulation of the proposed approach is adopted and applied on the sparse antenna array design for Integrated Sensing and Communication (ISAC) systems. The multiobjective variation is shown to perform well in managing the trade-off between radar and communication performance.</li>
</ul>

<h3>Title: PredIN: Towards Open-Set Gesture Recognition via Prediction Inconsistency</h3>
<ul>
<li><strong>Authors: </strong>Chen Liu, Can Han, Chengfeng Zhou, Crystal Cai, Dahong Qian</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19753">https://arxiv.org/abs/2407.19753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19753">https://arxiv.org/pdf/2407.19753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19753]] PredIN: Towards Open-Set Gesture Recognition via Prediction Inconsistency(https://arxiv.org/abs/2407.19753)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gesture recognition based on surface electromyography (sEMG) has achieved significant progress in human-machine interaction (HMI). However, accurately recognizing predefined gestures within a closed set is still inadequate in practice; a robust open-set system needs to effectively reject unknown gestures while correctly classifying known ones. To handle this challenge, we first report prediction inconsistency discovered for unknown classes due to ensemble diversity, which can significantly facilitate the detection of unknown classes. Based on this insight, we propose an ensemble learning approach, PredIN, to explicitly magnify the prediction inconsistency by enhancing ensemble diversity. Specifically, PredIN maximizes the class feature distribution inconsistency among ensemble members to enhance diversity. Meanwhile, it optimizes inter-class separability within an individual ensemble member to maintain individual performance. Comprehensive experiments on various benchmark datasets demonstrate that the PredIN outperforms state-of-the-art methods by a clear margin.Our proposed method simultaneously achieves accurate closed-set classification for predefined gestures and effective rejection for unknown gestures, exhibiting its efficacy and superiority in open-set gesture recognition based on sEMG.</li>
</ul>

<h3>Title: Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional Principles in Complex Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Camilla Bignotti, Carolina Camassa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19760">https://arxiv.org/abs/2407.19760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19760">https://arxiv.org/pdf/2407.19760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19760]] Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional Principles in Complex Scenarios(https://arxiv.org/abs/2407.19760)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we conduct an empirical analysis of how large language models (LLMs), specifically GPT-4, interpret constitutional principles in complex decision-making scenarios. We examine rulings from the Italian Constitutional Court on bioethics issues that involve trade-offs between competing values and compare model-generated legal arguments on these issues to those presented by the State, the Court, and the applicants. Our results indicate that GPT-4 consistently aligns more closely with progressive interpretations of the Constitution, often overlooking competing values and mirroring the applicants' views rather than the more conservative perspectives of the State or the Court's moderate positions. Our experiments reveal a distinct tendency of GPT-4 to favor progressive legal interpretations, underscoring the influence of underlying data biases. We thus underscore the importance of testing alignment in real-world scenarios and considering the implications of deploying LLMs in decision-making processes.</li>
</ul>

<h3>Title: Efficient Face Super-Resolution via Wavelet-based Feature Enhancement Network</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Li, Heng Guo, Xuannan Liu, Kongming Liang, Jiani Hu, Zhanyu Ma, Jun Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19768">https://arxiv.org/abs/2407.19768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19768">https://arxiv.org/pdf/2407.19768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19768]] Efficient Face Super-Resolution via Wavelet-based Feature Enhancement Network(https://arxiv.org/abs/2407.19768)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Face super-resolution aims to reconstruct a high-resolution face image from a low-resolution face image. Previous methods typically employ an encoder-decoder structure to extract facial structural features, where the direct downsampling inevitably introduces distortions, especially to high-frequency features such as edges. To address this issue, we propose a wavelet-based feature enhancement network, which mitigates feature distortion by losslessly decomposing the input feature into high and low-frequency components using the wavelet transform and processing them separately. To improve the efficiency of facial feature extraction, a full domain Transformer is further proposed to enhance local, regional, and global facial features. Such designs allow our method to perform better without stacking many modules as previous methods did. Experiments show that our method effectively balances performance, model size, and speed. Code link: this https URL.</li>
</ul>

<h3>Title: Synthesizing Scientific Summaries: An Extractive and Abstractive Approach</h3>
<ul>
<li><strong>Authors: </strong>Grishma Sharma, Aditi Paretkar, Deepak Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19779">https://arxiv.org/abs/2407.19779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19779">https://arxiv.org/pdf/2407.19779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19779]] Synthesizing Scientific Summaries: An Extractive and Abstractive Approach(https://arxiv.org/abs/2407.19779)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>The availability of a vast array of research papers in any area of study, necessitates the need of automated summarisation systems that can present the key research conducted and their corresponding findings. Scientific paper summarisation is a challenging task for various reasons including token length limits in modern transformer models and corresponding memory and compute requirements for long text. A significant amount of work has been conducted in this area, with approaches that modify the attention mechanisms of existing transformer models and others that utilise discourse information to capture long range dependencies in research papers. In this paper, we propose a hybrid methodology for research paper summarisation which incorporates an extractive and abstractive approach. We use the extractive approach to capture the key findings of research, and pair it with the introduction of the paper which captures the motivation for research. We use two models based on unsupervised learning for the extraction stage and two transformer language models, resulting in four combinations for our hybrid approach. The performances of the models are evaluated on three metrics and we present our findings in this paper. We find that using certain combinations of hyper parameters, it is possible for automated summarisation systems to exceed the abstractiveness of summaries written by humans. Finally, we state our future scope of research in extending this methodology to summarisation of generalised long documents.</li>
</ul>

<h3>Title: Survey and Taxonomy: The Role of Data-Centric AI in Transformer-Based Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Jingjing Xu, Caesar Wu, Yuan-Fang Li, Gregoire Danoy, Pascal Bouvry</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19784">https://arxiv.org/abs/2407.19784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19784">https://arxiv.org/pdf/2407.19784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19784]] Survey and Taxonomy: The Role of Data-Centric AI in Transformer-Based Time Series Forecasting(https://arxiv.org/abs/2407.19784)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Alongside the continuous process of improving AI performance through the development of more sophisticated models, researchers have also focused their attention to the emerging concept of data-centric AI, which emphasizes the important role of data in a systematic machine learning training process. Nonetheless, the development of models has also continued apace. One result of this progress is the development of the Transformer Architecture, which possesses a high level of capability in multiple domains such as Natural Language Processing (NLP), Computer Vision (CV) and Time Series Forecasting (TSF). Its performance is, however, heavily dependent on input data preprocessing and output data evaluation, justifying a data-centric approach to future research. We argue that data-centric AI is essential for training AI models, particularly for transformer-based TSF models efficiently. However, there is a gap regarding the integration of transformer-based TSF and data-centric AI. This survey aims to pin down this gap via the extensive literature review based on the proposed taxonomy. We review the previous research works from a data-centric AI perspective and we intend to lay the foundation work for the future development of transformer-based architecture and data-centric AI.</li>
</ul>

<h3>Title: Interpreting Low-level Vision Models with Causal Effect Maps</h3>
<ul>
<li><strong>Authors: </strong>Jinfan Hu, Jinjin Gu, Shiyao Yu, Fanghua Yu, Zheyuan Li, Zhiyuan You, Chaochao Lu, Chao Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19789">https://arxiv.org/abs/2407.19789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19789">https://arxiv.org/pdf/2407.19789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19789]] Interpreting Low-level Vision Models with Causal Effect Maps(https://arxiv.org/abs/2407.19789)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep neural networks have significantly improved the performance of low-level vision tasks but also increased the difficulty of interpretability. A deep understanding of deep models is beneficial for both network design and practical reliability. To take up this challenge, we introduce causality theory to interpret low-level vision models and propose a model-/task-agnostic method called Causal Effect Map (CEM). With CEM, we can visualize and quantify the input-output relationships on either positive or negative effects. After analyzing various low-level vision tasks with CEM, we have reached several interesting insights, such as: (1) Using more information of input images (e.g., larger receptive field) does NOT always yield positive outcomes. (2) Attempting to incorporate mechanisms with a global receptive field (e.g., channel attention) into image denoising may prove futile. (3) Integrating multiple tasks to train a general model could encourage the network to prioritize local information over global context. Based on the causal effect theory, the proposed diagnostic tool can refresh our common knowledge and bring a deeper understanding of low-level vision models. Codes are available at this https URL.</li>
</ul>

<h3>Title: Introducing a new hyper-parameter for RAG: Context Window Utilization</h3>
<ul>
<li><strong>Authors: </strong>Kush Juvekar, Anupam Purwar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19794">https://arxiv.org/abs/2407.19794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19794">https://arxiv.org/pdf/2407.19794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19794]] Introducing a new hyper-parameter for RAG: Context Window Utilization(https://arxiv.org/abs/2407.19794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a new hyper-parameter for Retrieval-Augmented Generation (RAG) systems called Context Window Utilization. RAG systems enhance generative models by incorporating relevant information retrieved from external knowledge bases, improving the factual accuracy and contextual relevance of generated responses. The size of the text chunks retrieved and processed is a critical factor influencing RAG performance. This study aims to identify the optimal chunk size that maximizes answer generation quality. Through systematic experimentation, we analyze the effects of varying chunk sizes on the efficiency and effectiveness of RAG frameworks. Our findings reveal that an optimal chunk size balances the trade-off between providing sufficient context and minimizing irrelevant information. These insights are crucial for enhancing the design and implementation of RAG systems, underscoring the importance of selecting an appropriate chunk size to achieve superior performance.</li>
</ul>

<h3>Title: VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks</h3>
<ul>
<li><strong>Authors: </strong>Juhwan Choi, Junehyoung Kwon, JungMin Yun, Seunguk Yu, YoungBin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19795">https://arxiv.org/abs/2407.19795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19795">https://arxiv.org/pdf/2407.19795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19795]] VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks(https://arxiv.org/abs/2407.19795)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Domain generalizability is a crucial aspect of a deep learning model since it determines the capability of the model to perform well on data from unseen domains. However, research on the domain generalizability of deep learning models for vision-language tasks remains limited, primarily because of the lack of required datasets. To address these challenges, we propose VolDoGer: Vision-Language Dataset for Domain Generalization, a dedicated dataset designed for domain generalization that addresses three vision-language tasks: image captioning, visual question answering, and visual entailment. We constructed VolDoGer by extending LLM-based data annotation techniques to vision-language tasks, thereby alleviating the burden of recruiting human annotators. We evaluated the domain generalizability of various models, ranging from fine-tuned models to a recent multimodal large language model, through VolDoGer.</li>
</ul>

<h3>Title: Teaching LLMs at Charles University: Assignments and Activities</h3>
<ul>
<li><strong>Authors: </strong>Jindřich Helcl, Zdeněk Kasner, Ondřej Dušek, Tomasz Limisiewicz, Dominik Macháček, Tomáš Musil, Jindřich Libovický</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19798">https://arxiv.org/abs/2407.19798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19798">https://arxiv.org/pdf/2407.19798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19798]] Teaching LLMs at Charles University: Assignments and Activities(https://arxiv.org/abs/2407.19798)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents teaching materials, particularly assignments and ideas for classroom activities, from a new course on large language models (LLMs) taught at Charles University. The assignments include experiments with LLM inference for weather report generation and machine translation. The classroom activities include class quizzes, focused research on downstream tasks and datasets, and an interactive "best paper" session aimed at reading and comprehension of research papers.</li>
</ul>

<h3>Title: Cool-Fusion: Fuse Large Language Models without Training</h3>
<ul>
<li><strong>Authors: </strong>Cong Liu, Xiaojun Quan, Yan Pan, Liang Lin, Weigang Wu, Xu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19807">https://arxiv.org/abs/2407.19807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19807">https://arxiv.org/pdf/2407.19807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19807]] Cool-Fusion: Fuse Large Language Models without Training(https://arxiv.org/abs/2407.19807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We focus on the problem of fusing two or more heterogeneous large language models (LLMs) to facilitate their complementary strengths. One of the challenges on model fusion is high computational load, i.e. to fine-tune or to align vocabularies via combinatorial optimization. To this end, we propose \emph{Cool-Fusion}, a simple yet effective approach that fuses the knowledge of heterogeneous source LLMs to leverage their complementary strengths. \emph{Cool-Fusion} is the first method that does not require any type of training like the ensemble approaches. But unlike ensemble methods, it is applicable to any set of source LLMs that have different vocabularies. The basic idea is to have each source LLM individually generate tokens until the tokens can be decoded into a text segment that ends at word boundaries common to all source LLMs. Then, the source LLMs jointly rerank the generated text segment and select the best one, which is the fused text generation in one step. Extensive experiments are conducted across a variety of benchmark datasets. On \emph{GSM8K}, \emph{Cool-Fusion} increases accuracy from three strong source LLMs by a significant 8\%-17.8\%.</li>
</ul>

<h3>Title: Segmentation en phrases : ouvrez les guillemets sans perdre le fil</h3>
<ul>
<li><strong>Authors: </strong>Sandrine Ollinger (ATILF), Denis Maurel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19808">https://arxiv.org/abs/2407.19808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19808">https://arxiv.org/pdf/2407.19808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19808]] Segmentation en phrases : ouvrez les guillemets sans perdre le fil(https://arxiv.org/abs/2407.19808)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents a graph cascade for sentence segmentation of XML documents. Our proposal offers sentences inside sentences for cases introduced by quotation marks and hyphens, and also pays particular attention to situations involving incises introduced by parentheses and lists introduced by colons. We present how the tool works and compare the results obtained with those available in 2019 on the same dataset, together with an evaluation of the system's performance on a test corpus</li>
</ul>

<h3>Title: Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for Multimodal Automatic Pain Assessment using Facial Videos and fNIRS</h3>
<ul>
<li><strong>Authors: </strong>Stefanos Gkikas, Manolis Tsiknakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19809">https://arxiv.org/abs/2407.19809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19809">https://arxiv.org/pdf/2407.19809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19809]] Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for Multimodal Automatic Pain Assessment using Facial Videos and fNIRS(https://arxiv.org/abs/2407.19809)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Automatic pain assessment plays a critical role for advancing healthcare and optimizing pain management strategies. This study has been submitted to the First Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN). The proposed multimodal framework utilizes facial videos and fNIRS and presents a modality-agnostic approach, alleviating the need for domain-specific models. Employing a dual ViT configuration and adopting waveform representations for the fNIRS, as well as for the extracted embeddings from the two modalities, demonstrate the efficacy of the proposed method, achieving an accuracy of 46.76% in the multilevel pain assessment task.</li>
</ul>

<h3>Title: Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-MLP Architecture</h3>
<ul>
<li><strong>Authors: </strong>Stefanos Gkikas, Manolis Tsiknakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19811">https://arxiv.org/abs/2407.19811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19811">https://arxiv.org/pdf/2407.19811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19811]] Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-MLP Architecture(https://arxiv.org/abs/2407.19811)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Pain assessment is essential in developing optimal pain management protocols to alleviate suffering and prevent functional decline in patients. Consequently, reliable and accurate automatic pain assessment systems are essential for continuous and effective patient monitoring. This study presents synthetic thermal videos generated by Generative Adversarial Networks integrated into the pain recognition pipeline and evaluates their efficacy. A framework consisting of a Vision-MLP and a Transformer-based module is utilized, employing RGB and synthetic thermal videos in unimodal and multimodal settings. Experiments conducted on facial videos from the BioVid database demonstrate the effectiveness of synthetic thermal videos and underline the potential advantages of it.</li>
</ul>

<h3>Title: Image-text matching for large-scale book collections</h3>
<ul>
<li><strong>Authors: </strong>Artemis Llabrés, Arka Ujjal Dey, Dimosthenis Karatzas, Ernest Valveny</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19812">https://arxiv.org/abs/2407.19812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19812">https://arxiv.org/pdf/2407.19812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19812]] Image-text matching for large-scale book collections(https://arxiv.org/abs/2407.19812)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We address the problem of detecting and mapping all books in a collection of images to entries in a given book catalogue. Instead of performing independent retrieval for each book detected, we treat the image-text mapping problem as a many-to-many matching process, looking for the best overall match between the two sets. We combine a state-of-the-art segmentation method (SAM) to detect book spines and extract book information using a commercial OCR. We then propose a two-stage approach for text-image matching, where CLIP embeddings are used first for fast matching, followed by a second slower stage to refine the matching, employing either the Hungarian Algorithm or a BERT-based model trained to cope with noisy OCR input and partial text matches. To evaluate our approach, we publish a new dataset of annotated bookshelf images that covers the whole book collection of a public library in Spain. In addition, we provide two target lists of book metadata, a closed-set of 15k book titles that corresponds to the known library inventory, and an open-set of 2.3M book titles to simulate an open-world scenario. We report results on two settings, on one hand on a matching-only task, where the book segments and OCR is given and the objective is to perform many-to-many matching against the target lists, and a combined detection and matching task, where books must be first detected and recognised before they are matched to the target list entries. We show that both the Hungarian Matching and the proposed BERT-based model outperform a fuzzy string matching baseline, and we highlight inherent limitations of the matching algorithms as the target increases in size, and when either of the two sets (detected books or target book list) is incomplete. The dataset and code are available at this https URL</li>
</ul>

<h3>Title: Improving Retrieval Augmented Language Model with Self-Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, Haifeng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19813">https://arxiv.org/abs/2407.19813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19813">https://arxiv.org/pdf/2407.19813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19813]] Improving Retrieval Augmented Language Model with Self-Reasoning(https://arxiv.org/abs/2407.19813)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Retrieval-Augmented Language Model (RALM) has shown remarkable performance on knowledge-intensive tasks by incorporating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly concerning their reliability and traceability. To be specific, the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs, while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reason trajectories with three processes: a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We have evaluated our framework across four public datasets (two short-form QA datasets, one long-form QA dataset, and one fact verification dataset) to demonstrate the superiority of our method, which can outperform existing state-of-art models and can achieve comparable performance with GPT-4, while only using 2,000 training samples.</li>
</ul>

<h3>Title: Comparative Analysis of Encoder-Based NER and Large Language Models for Skill Extraction from Russian Job Vacancies</h3>
<ul>
<li><strong>Authors: </strong>Nikita Matkin, Aleksei Smirnov, Mikhail Usanin, Egor Ivanov, Kirill Sobyanin, Sofiia Paklina, Petr Parshakov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19816">https://arxiv.org/abs/2407.19816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19816">https://arxiv.org/pdf/2407.19816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19816]] Comparative Analysis of Encoder-Based NER and Large Language Models for Skill Extraction from Russian Job Vacancies(https://arxiv.org/abs/2407.19816)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The labor market is undergoing rapid changes, with increasing demands on job seekers and a surge in job openings. Identifying essential skills and competencies from job descriptions is challenging due to varying employer requirements and the omission of key skills. This study addresses these challenges by comparing traditional Named Entity Recognition (NER) methods based on encoders with Large Language Models (LLMs) for extracting skills from Russian job vacancies. Using a labeled dataset of 4,000 job vacancies for training and 1,472 for testing, the performance of both approaches is evaluated. Results indicate that traditional NER models, especially DeepPavlov RuBERT NER tuned, outperform LLMs across various metrics including accuracy, precision, recall, and inference time. The findings suggest that traditional NER models provide more effective and efficient solutions for skill extraction, enhancing job requirement clarity and aiding job seekers in aligning their qualifications with employer expectations. This research contributes to the field of natural language processing (NLP) and its application in the labor market, particularly in non-English contexts.</li>
</ul>

<h3>Title: Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost</h3>
<ul>
<li><strong>Authors: </strong>Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, Fabrizio Giacomelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19825">https://arxiv.org/abs/2407.19825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19825">https://arxiv.org/pdf/2407.19825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19825]] Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost(https://arxiv.org/abs/2407.19825)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Today's large language models (LLMs) can solve challenging question-answering tasks, and prompt engineering techniques, such as chain-of-thought (CoT), have gained attention for enhancing the explanation and correctness of outputs. Nevertheless, models require significant time to generate answers augmented with lengthy reasoning details. To address this issue, this paper analyzes the impact of output lengths on LLM inference pipelines and proposes novel metrics to evaluate them in terms of \textit{correct conciseness}. It also examines the impact of controlling output length through a refined prompt engineering strategy, Constrained-CoT (CCoT), which encourages the model to limit output length. Experiments on pre-trained LLMs demonstrated the benefit of the proposed metrics and the effectiveness of CCoT across different models. For instance, constraining the reasoning of LLaMA2-70b to 100 words improves the accuracy from 36.01\% (CoT) to 41.07\% (CCoT) on the GSM8K dataset, while reducing the average output length by 28 words.</li>
</ul>

<h3>Title: Federated Learning based Latent Factorization of Tensors for Privacy-Preserving QoS Prediction</h3>
<ul>
<li><strong>Authors: </strong>Shuai Zhong, Zengtong Tang, Di Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19828">https://arxiv.org/abs/2407.19828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19828">https://arxiv.org/pdf/2407.19828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19828]] Federated Learning based Latent Factorization of Tensors for Privacy-Preserving QoS Prediction(https://arxiv.org/abs/2407.19828)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>In applications related to big data and service computing, dynamic connections tend to be encountered, especially the dynamic data of user-perspective quality of service (QoS) in Web services. They are transformed into high-dimensional and incomplete (HDI) tensors which include abundant temporal pattern information. Latent factorization of tensors (LFT) is an extremely efficient and typical approach for extracting such patterns from an HDI tensor. However, current LFT models require the QoS data to be maintained in a central place (e.g., a central server), which is impossible for increasingly privacy-sensitive users. To address this problem, this article creatively designs a federated learning based on latent factorization of tensors (FL-LFT). It builds a data-density -oriented federated learning model to enable isolated users to collaboratively train a global LFT model while protecting user's privacy. Extensive experiments on a QoS dataset collected from the real world verify that FL-LFT shows a remarkable increase in prediction accuracy when compared to state-of-the-art federated learning (FL) approaches.</li>
</ul>

<h3>Title: ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Huang, Jianguo Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19832">https://arxiv.org/abs/2407.19832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19832">https://arxiv.org/pdf/2407.19832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19832]] ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2(https://arxiv.org/abs/2407.19832)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have attracted much attention due to their multifunctionality. However, traditional Transformer architectures incur significant overhead due to their secondary computational complexity. To address this issue, we introduce ML-Mamba, a multimodal language model that utilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known for its linear extension and fast processing of long sequences. We replace the Transformer based backbone with a pre-trained Mamba-2 model and explore methods for integrating 2D visual selective scanning mechanisms into multimodal learning. We also try various visual encoders and Mamba-2 model variants. Our extensive experiments conducted in various multimodal benchmark tests have demonstrated the competitive performance of ML-Mamba and highlighted the potential of state space models in multimodal tasks. The experimental results show that: (1) ML-Mamba achieves performance comparable to state-of-the-art methods such as TinyLaVA and MobileVLM v2 through its linear sequential modeling, while also having faster inference speed; (2) ML-Mamba performs well in visual hallucinations and spatial relationship judgment in closed set benchmark tests; (3) ML-Mamba achieves performance comparable to LLaVA while reducing the number of parameters by 40\%.(4) Compared to the multimodal model using the original Mamba model, the Mamba-2 based large-scale multimodal language model has stronger inference performance and effectiveness.</li>
</ul>

<h3>Title: ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Khalil, Mohammed Sabry</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19835">https://arxiv.org/abs/2407.19835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19835">https://arxiv.org/pdf/2407.19835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19835]] ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation(https://arxiv.org/abs/2407.19835)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Classical Arabic represents a significant era, encompassing the golden age of Arab culture, philosophy, and scientific literature. With a broad consensus on the importance of translating these literatures to enrich knowledge dissemination across communities, the advent of large language models (LLMs) and translation systems offers promising tools to facilitate this goal. However, we have identified a scarcity of translation datasets in Classical Arabic, which are often limited in scope and topics, hindering the development of high-quality translation systems. In response, we present the ATHAR dataset, comprising 66,000 high-quality Classical Arabic to English translation samples that cover a wide array of subjects including science, culture, and philosophy. Furthermore, we assess the performance of current state-of-the-art LLMs under various settings, concluding that there is a need for such datasets in current systems. Our findings highlight how models can benefit from fine-tuning or incorporating this dataset into their pretraining pipelines. The dataset is publicly available on the HuggingFace Data Hub at \url{this https URL}.</li>
</ul>

<h3>Title: Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Jorge García-Carrasco, Alejandro Maté, Juan Trujillo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19842">https://arxiv.org/abs/2407.19842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19842">https://arxiv.org/pdf/2407.19842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19842]] Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability(https://arxiv.org/abs/2407.19842)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), characterized by being trained on broad amounts of data in a self-supervised manner, have shown impressive performance across a wide range of tasks. Indeed, their generative abilities have aroused interest on the application of LLMs across a wide range of contexts. However, neural networks in general, and LLMs in particular, are known to be vulnerable to adversarial attacks, where an imperceptible change to the input can mislead the output of the model. This is a serious concern that impedes the use of LLMs on high-stakes applications, such as healthcare, where a wrong prediction can imply serious consequences. Even though there are many efforts on making LLMs more robust to adversarial attacks, there are almost no works that study \emph{how} and \emph{where} these vulnerabilities that make LLMs prone to adversarial attacks happen. Motivated by these facts, we explore how to localize and understand vulnerabilities, and propose a method, based on Mechanistic Interpretability (MI) techniques, to guide this process. Specifically, this method enables us to detect vulnerabilities related to a concrete task by (i) obtaining the subset of the model that is responsible for that task, (ii) generating adversarial samples for that task, and (iii) using MI techniques together with the previous samples to discover and understand the possible vulnerabilities. We showcase our method on a pretrained GPT-2 Small model carrying out the task of predicting 3-letter acronyms to demonstrate its effectiveness on locating and understanding concrete vulnerabilities of the model.</li>
</ul>

<h3>Title: BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning</h3>
<ul>
<li><strong>Authors: </strong>Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19845">https://arxiv.org/abs/2407.19845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19845">https://arxiv.org/pdf/2407.19845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19845]] BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning(https://arxiv.org/abs/2407.19845)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, fair</a></li>
<li><strong>Abstract: </strong>As an emerging approach to explore the vulnerability of deep neural networks (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons or unreliable conclusions (e.g., misleading, biased or even false conclusions). Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. Our benchmark makes three valuable contributions to the research community. 1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 20 attack and 32 defense algorithms), based on an extensible modular-based codebase. 2) We conduct comprehensive evaluations with 5 poisoning ratios, based on 4 models and 4 datasets, leading to 11,492 pairs of attack-against-defense evaluations in total. 3) Based on above evaluations, we present abundant analysis from 10 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning. Finally, we have created a user-friendly website at this http URL, which collects all important information of BackdoorBench, including codebase, docs, leaderboard, and model Zoo.</li>
</ul>

<h3>Title: Fast Private Location-based Information Retrieval Over the Torus</h3>
<ul>
<li><strong>Authors: </strong>Joon Soo Yoo, Mi Yeon Hong, Ji Won Heo, Kang Hoon Lee, Ji Won Yoon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19871">https://arxiv.org/abs/2407.19871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19871">https://arxiv.org/pdf/2407.19871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19871]] Fast Private Location-based Information Retrieval Over the Torus(https://arxiv.org/abs/2407.19871)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Location-based services offer immense utility, but also pose significant privacy risks. In response, we propose LocPIR, a novel framework using homomorphic encryption (HE), specifically the TFHE scheme, to preserve user location privacy when retrieving data from public clouds. Our system employs TFHE's expertise in non-polynomial evaluations, crucial for comparison operations. LocPIR showcases minimal client-server interaction, reduced memory overhead, and efficient throughput. Performance tests confirm its computational speed, making it a viable solution for practical scenarios, demonstrated via application to a COVID-19 alert model. Thus, LocPIR effectively addresses privacy concerns in location-based services, enabling secure data sharing from the public cloud.</li>
</ul>

<h3>Title: Exploring Robust Face-Voice Matching in Multilingual Environments</h3>
<ul>
<li><strong>Authors: </strong>Jiehui Tang, Xiaofei Wang, Zhen Xiao, Jiayi Liu, Xueliang Liu, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19875">https://arxiv.org/abs/2407.19875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19875">https://arxiv.org/pdf/2407.19875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19875]] Exploring Robust Face-Voice Matching in Multilingual Environments(https://arxiv.org/abs/2407.19875)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents Team Xaiofei's innovative approach to exploring Face-Voice Association in Multilingual Environments (FAME) at ACM Multimedia 2024. We focus on the impact of different languages in face-voice matching by building upon Fusion and Orthogonal Projection (FOP), introducing four key components: a dual-branch structure, dynamic sample pair weighting, robust data augmentation, and score polarization strategy. Our dual-branch structure serves as an auxiliary mechanism to better integrate and provide more comprehensive information. We also introduce a dynamic weighting mechanism for various sample pairs to optimize learning. Data augmentation techniques are employed to enhance the model's generalization across diverse conditions. Additionally, score polarization strategy based on age and gender matching confidence clarifies and accentuates the final results. Our methods demonstrate significant effectiveness, achieving an equal error rate (EER) of 20.07 on the V2-EH dataset and 21.76 on the V1-EU dataset.</li>
</ul>

<h3>Title: Yucca: A Deep Learning Framework For Medical Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Nørgaard Llambias, Julia Machnio, Asbjørn Munk, Jakob Ambsdorf, Mads Nielsen, Mostafa Mehdipour Ghazi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19888">https://arxiv.org/abs/2407.19888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19888">https://arxiv.org/pdf/2407.19888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19888]] Yucca: A Deep Learning Framework For Medical Image Analysis(https://arxiv.org/abs/2407.19888)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image analysis using deep learning frameworks has advanced healthcare by automating complex tasks, but many existing frameworks lack flexibility, modularity, and user-friendliness. To address these challenges, we introduce Yucca, an open-source AI framework available at this https URL, designed specifically for medical imaging applications and built on PyTorch and PyTorch Lightning. Yucca features a three-tiered architecture: Functional, Modules, and Pipeline, providing a comprehensive and customizable solution. Evaluated across diverse tasks such as cerebral microbleeds detection, white matter hyperintensity segmentation, and hippocampus segmentation, Yucca achieves state-of-the-art results, demonstrating its robustness and versatility. Yucca offers a powerful, flexible, and user-friendly platform for medical image analysis, inviting community contributions to advance its capabilities and impact.</li>
</ul>

<h3>Title: Self-Supervised Learning for Text Recognition: A Critical Survey</h3>
<ul>
<li><strong>Authors: </strong>Carlos Penarrubia, Jose J. Valero-Mas, Jorge Calvo-Zaragoza</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19889">https://arxiv.org/abs/2407.19889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19889">https://arxiv.org/pdf/2407.19889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19889]] Self-Supervised Learning for Text Recognition: A Critical Survey(https://arxiv.org/abs/2407.19889)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Text Recognition (TR) refers to the research area that focuses on retrieving textual information from images, a topic that has seen significant advancements in the last decade due to the use of Deep Neural Networks (DNN). However, these solutions often necessitate vast amounts of manually labeled or synthetic data. Addressing this challenge, Self-Supervised Learning (SSL) has gained attention by utilizing large datasets of unlabeled data to train DNN, thereby generating meaningful and robust representations. Although SSL was initially overlooked in TR because of its unique characteristics, recent years have witnessed a surge in the development of SSL methods specifically for this field. This rapid development, however, has led to many methods being explored independently, without taking previous efforts in methodology or comparison into account, thereby hindering progress in the field of research. This paper, therefore, seeks to consolidate the use of SSL in the field of TR, offering a critical and comprehensive overview of the current state of the art. We will review and analyze the existing methods, compare their results, and highlight inconsistencies in the current literature. This thorough analysis aims to provide general insights into the field, propose standardizations, identify new research directions, and foster its proper development.</li>
</ul>

<h3>Title: BEExAI: Benchmark to Evaluate Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Samuel Sithakoul, Sara Meftah, Clément Feutry</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19897">https://arxiv.org/abs/2407.19897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19897">https://arxiv.org/pdf/2407.19897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19897]] BEExAI: Benchmark to Evaluate Explainable AI(https://arxiv.org/abs/2407.19897)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Recent research in explainability has given rise to numerous post-hoc attribution methods aimed at enhancing our comprehension of the outputs of black-box machine learning models. However, evaluating the quality of explanations lacks a cohesive approach and a consensus on the methodology for deriving quantitative metrics that gauge the efficacy of explainability post-hoc attribution methods. Furthermore, with the development of increasingly complex deep learning models for diverse data applications, the need for a reliable way of measuring the quality and correctness of explanations is becoming critical. We address this by proposing BEExAI, a benchmark tool that allows large-scale comparison of different post-hoc XAI methods, employing a set of selected evaluation metrics.</li>
</ul>

<h3>Title: Cell Culture Assistive Application for Precipitation Image Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Takato Yasuno</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19913">https://arxiv.org/abs/2407.19913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19913">https://arxiv.org/pdf/2407.19913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19913]] Cell Culture Assistive Application for Precipitation Image Diagnosis(https://arxiv.org/abs/2407.19913)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In regenerative medicine research, we experimentally design the composition of chemical medium. We add different components to 384-well plates and culture the biological cells. We monitor the condition of the cells and take time-lapse bioimages for morphological assay. In particular, precipitation can appear as artefacts in the image and contaminate the noise in the imaging assay. Inspecting precipitates is a tedious task for the observer, and differences in experience can lead to variations in judgement from person to person. The machine learning approach will remove the burden of human inspection and provide consistent inspection. In addition, precipitation features are as small as 10-20 {\mu}m. A 1200 pixel square well image resized under a resolution of 2.82 {\mu}m/pixel will result in a reduction in precipitation features. Dividing the well images into 240-pixel squares and learning without resizing preserves the resolution of the original image. In this study, we developed an application to automatically detect precipitation on 384-well plates utilising optical microscope images. We apply MN-pair contrastive clustering to extract precipitation classes from approximately 20,000 patch images. To detect precipitation features, we compare deeper FCDDs detectors with optional backbones and build a machine learning pipeline to detect precipitation from the maximum score of quadruplet well images using isolation Forest algorithm, where the anomaly score is ranged from zero to one. Furthermore, using this application we can visualise precipitation situ heatmap on a 384-well plate.</li>
</ul>

<h3>Title: Sentiment Analysis of Lithuanian Online Reviews Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Brigita Vileikytė, Mantas Lukoševičius, Lukas Stankevičius</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19914">https://arxiv.org/abs/2407.19914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19914">https://arxiv.org/pdf/2407.19914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19914]] Sentiment Analysis of Lithuanian Online Reviews Using Large Language Models(https://arxiv.org/abs/2407.19914)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Sentiment analysis is a widely researched area within Natural Language Processing (NLP), attracting significant interest due to the advent of automated solutions. Despite this, the task remains challenging because of the inherent complexity of languages and the subjective nature of sentiments. It is even more challenging for less-studied and less-resourced languages such as Lithuanian. Our review of existing Lithuanian NLP research reveals that traditional machine learning methods and classification algorithms have limited effectiveness for the task. In this work, we address sentiment analysis of Lithuanian five-star-based online reviews from multiple domains that we collect and clean. We apply transformer models to this task for the first time, exploring the capabilities of pre-trained multilingual Large Language Models (LLMs), specifically focusing on fine-tuning BERT and T5 models. Given the inherent difficulty of the task, the fine-tuned models perform quite well, especially when the sentiments themselves are less ambiguous: 80.74% and 89.61% testing recognition accuracy of the most popular one- and five-star reviews respectively. They significantly outperform current commercial state-of-the-art general-purpose LLM GPT-4. We openly share our fine-tuned LLMs online.</li>
</ul>

<h3>Title: FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention</h3>
<ul>
<li><strong>Authors: </strong>Yu Lu, Yuanzhi Liang, Linchao Zhu, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19918">https://arxiv.org/abs/2407.19918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19918">https://arxiv.org/pdf/2407.19918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19918]] FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention(https://arxiv.org/abs/2407.19918)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video diffusion models have made substantial progress in various video generation applications. However, training models for long video generation tasks require significant computational and data resources, posing a challenge to developing long video diffusion models. This paper investigates a straightforward and training-free approach to extend an existing short video diffusion model (e.g. pre-trained on 16-frame videos) for consistent long video generation (e.g. 128 frames). Our preliminary observation has found that directly applying the short video diffusion model to generate long videos can lead to severe video quality degradation. Further investigation reveals that this degradation is primarily due to the distortion of high-frequency components in long videos, characterized by a decrease in spatial high-frequency components and an increase in temporal high-frequency components. Motivated by this, we propose a novel solution named FreeLong to balance the frequency distribution of long video features during the denoising process. FreeLong blends the low-frequency components of global video features, which encapsulate the entire video sequence, with the high-frequency components of local video features that focus on shorter subsequences of frames. This approach maintains global consistency while incorporating diverse and high-quality spatiotemporal details from local videos, enhancing both the consistency and fidelity of long video generation. We evaluated FreeLong on multiple base video diffusion models and observed significant improvements. Additionally, our method supports coherent multi-prompt generation, ensuring both visual coherence and seamless transitions between scenes.</li>
</ul>

<h3>Title: Lightweight Dataset for Decoy Development to Improve IoT Security</h3>
<ul>
<li><strong>Authors: </strong>David Weissman, Anura P. Jayasumana</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19926">https://arxiv.org/abs/2407.19926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19926">https://arxiv.org/pdf/2407.19926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19926]] Lightweight Dataset for Decoy Development to Improve IoT Security(https://arxiv.org/abs/2407.19926)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>In this paper, the authors introduce a lightweight dataset to interpret IoT (Internet of Things) activity in preparation to create decoys by replicating known data traffic patterns. The dataset comprises different scenarios in a real network setting. This paper also surveys information related to other IoT datasets along with the characteristics that make our data valuable. Many of the datasets available are synthesized (simulated) or often address industrial applications, while the IoT dataset we present is based on likely smart home scenarios. Further, there are only a limited number of IoT datasets that contain both normal operation and attack scenarios. A discussion of the network configuration and the steps taken to prepare this dataset are presented as we prepare to create replicative patterns for decoy purposes. The dataset, which we refer to as IoT Flex Data, consists of four categories, namely, IoT benign idle, IoT benign active, IoT setup, and malicious (attack) traffic associating the IoT devices with the scenarios under consideration.</li>
</ul>

<h3>Title: Robust Conformal Volume Estimation in 3D Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Lambert, Florence Forbes, Senan Doyle, Michel Dojat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19938">https://arxiv.org/abs/2407.19938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19938">https://arxiv.org/pdf/2407.19938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19938]] Robust Conformal Volume Estimation in 3D Medical Images(https://arxiv.org/abs/2407.19938)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Volumetry is one of the principal downstream applications of 3D medical image segmentation, for example, to detect abnormal tissue growth or for surgery planning. Conformal Prediction is a promising framework for uncertainty quantification, providing calibrated predictive intervals associated with automatic volume measurements. However, this methodology is based on the hypothesis that calibration and test samples are exchangeable, an assumption that is in practice often violated in medical image applications. A weighted formulation of Conformal Prediction can be framed to mitigate this issue, but its empirical investigation in the medical domain is still lacking. A potential reason is that it relies on the estimation of the density ratio between the calibration and test distributions, which is likely to be intractable in scenarios involving high-dimensional data. To circumvent this, we propose an efficient approach for density ratio estimation relying on the compressed latent representations generated by the segmentation model. Our experiments demonstrate the efficiency of our approach to reduce the coverage error in the presence of covariate shifts, in both synthetic and real-world settings. Our implementation is available at this https URL</li>
</ul>

<h3>Title: Practical and Robust Safety Guarantees for Advanced Counterfactual Learning to Rank</h3>
<ul>
<li><strong>Authors: </strong>Shashank Gupta, Harrie Oosterhuis, Maarten de Rijke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19943">https://arxiv.org/abs/2407.19943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19943">https://arxiv.org/pdf/2407.19943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19943]] Practical and Robust Safety Guarantees for Advanced Counterfactual Learning to Rank(https://arxiv.org/abs/2407.19943)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Counterfactual learning to rank (CLTR ) can be risky; various circumstances can cause it to produce sub-optimal models that hurt performance when deployed. Safe CLTR was introduced to mitigate these risks when using inverse propensity scoring to correct for position bias. However, the existing safety measure for CLTR is not applicable to state-of-the-art CLTR, it cannot handle trust bias, and its guarantees rely on specific assumptions about user behavior. Our contributions are two-fold. First, we generalize the existing safe CLTR approach to make it applicable to state-of-the-art doubly robust (DR) CLTR and trust bias. Second, we propose a novel approach, proximal ranking policy optimization (PRPO ), that provides safety in deployment without assumptions about user behavior. PRPO removes incentives for learning ranking behavior that is too dissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how much learned models can degrade performance metrics, without relying on any specific user assumptions. Our experiments show that both our novel safe doubly robust method and PRPO provide higher performance than the existing safe inverse propensity scoring approach. However, when circumstances are unexpected, the safe doubly robust approach can become unsafe and bring detrimental performance. In contrast, PRPO always maintains safety, even in maximally adversarial situations. By avoiding assumptions, PRPO is the first method with unconditional safety in deployment that translates to robust safety for real-world applications.</li>
</ul>

<h3>Title: Inference acceleration for large language models using "stairs" assisted greedy generation</h3>
<ul>
<li><strong>Authors: </strong>Domas Grigaliūnas, Mantas Lukoševičius</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19947">https://arxiv.org/abs/2407.19947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19947">https://arxiv.org/pdf/2407.19947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19947]] Inference acceleration for large language models using "stairs" assisted greedy generation(https://arxiv.org/abs/2407.19947)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) with billions of parameters are known for their impressive predicting capabilities but require lots of resources to run. With their massive rise in popularity, even a small reduction in required resources could have an impact on environment. On the other hand, smaller models require fewer resources but may sacrifice accuracy. In this work, we are proposing an implementation of ``stairs'' assisted greedy generation. It is a modified assisted generation methodology that makes use of a smaller model's fast generation, large model's batch prediction, and "stairs" validation in order to achieve a speed up in prediction generation. Results show between 9.58 and 17.24 percent inference time reduction compared to a stand-alone large LLM prediction in a text generation task without a loss in accuracy.</li>
</ul>

<h3>Title: Can I trust my anomaly detection system? A case study based on explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Rashid, Elvio Amparore, Enrico Ferrari, Damiano Verda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19951">https://arxiv.org/abs/2407.19951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19951">https://arxiv.org/pdf/2407.19951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19951]] Can I trust my anomaly detection system? A case study based on explainable AI(https://arxiv.org/abs/2407.19951)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Generative models based on variational autoencoders are a popular technique for detecting anomalies in images in a semi-supervised context. A common approach employs the anomaly score to detect the presence of anomalies, and it is known to reach high level of accuracy on benchmark datasets. However, since anomaly scores are computed from reconstruction disparities, they often obscure the detection of various spurious features, raising concerns regarding their actual efficacy. This case study explores the robustness of an anomaly detection system based on variational autoencoder generative models through the use of eXplainable AI methods. The goal is to get a different perspective on the real performances of anomaly detectors that use reconstruction differences. In our case study we discovered that, in many cases, samples are detected as anomalous for the wrong or misleading factors.</li>
</ul>

<h3>Title: FedDEO: Description-Enhanced One-Shot Federated Learning with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19953">https://arxiv.org/abs/2407.19953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19953">https://arxiv.org/pdf/2407.19953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19953]] FedDEO: Description-Enhanced One-Shot Federated Learning with Diffusion Models(https://arxiv.org/abs/2407.19953)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, the attention towards One-Shot Federated Learning (OSFL) has been driven by its capacity to minimize communication. With the development of the diffusion model (DM), several methods employ the DM for OSFL, utilizing model parameters, image features, or textual prompts as mediums to transfer the local client knowledge to the server. However, these mediums often require public datasets or the uniform feature extractor, significantly limiting their practicality. In this paper, we propose FedDEO, a Description-Enhanced One-Shot Federated Learning Method with DMs, offering a novel exploration of utilizing the DM in OSFL. The core idea of our method involves training local descriptions on the clients, serving as the medium to transfer the knowledge of the distributed clients to the server. Firstly, we train local descriptions on the client data to capture the characteristics of client distributions, which are then uploaded to the server. On the server, the descriptions are used as conditions to guide the DM in generating synthetic datasets that comply with the distributions of various clients, enabling the training of the aggregated model. Theoretical analyses and sufficient quantitation and visualization experiments on three large-scale real-world datasets demonstrate that through the training of local descriptions, the server is capable of generating synthetic datasets with high quality and diversity. Consequently, with advantages in communication and privacy protection, the aggregated model outperforms compared FL or diffusion-based OSFL methods and, on some clients, outperforms the performance ceiling of centralized training.</li>
</ul>

<h3>Title: Integrated Communications and Security: RIS-Assisted Simultaneous Transmission and Generation of Secret Keys</h3>
<ul>
<li><strong>Authors: </strong>Ning Gao, Yuze Yao, Shi Jin, Cen Li, Michail Matthaiou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19960">https://arxiv.org/abs/2407.19960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19960">https://arxiv.org/pdf/2407.19960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19960]] Integrated Communications and Security: RIS-Assisted Simultaneous Transmission and Generation of Secret Keys(https://arxiv.org/abs/2407.19960)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>We develop a new integrated communications and security (ICAS) design paradigm by leveraging the concept of reconfigurable intelligent surfaces (RISs). In particular, we propose RIS-assisted simultaneous transmission and secret key generation by sharing the RIS for these two tasks. Specifically, the legitimate transceivers intend to jointly optimize the data transmission rate and the key generation rate by configuring the phase-shift of the RIS in the presence of a smart attacker. We first derive the key generation rate of the RIS-assisted physical layer key generation (PLKG). Then, to obtain the optimal RIS configuration, we formulate the problem as a secure transmission (ST) game and prove the existence of the Nash equilibrium (NE), and then derive the NE point of the static game. For the dynamic ST game, we model the problem as a finite Markov decision process and propose a model-free reinforcement learning approach to obtain the NE point. Particularly, considering that the legitimate transceivers cannot obtain the channel state information (CSI) of the attacker in real-world conditions, we develop a deep recurrent Q-network (DRQN) based dynamic ST strategy to learn the optimal RIS configuration. The details of the algorithm are provided, and then, the system complexity is analyzed. Our simulation results show that the proposed DRQN based dynamic ST strategy has a better performance than the benchmarks even with a partial observation information, and achieves "one time pad" communication by allocating a suitable weight factor for data transmission and PLKG.</li>
</ul>

<h3>Title: Prichain II: CloudGuardian Cloud Security Proposal with Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Craveiro Rodrigues, Pedro Miguel Calhau Mateus, Valderi Reis Quietinho Leithardt</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19961">https://arxiv.org/abs/2407.19961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19961">https://arxiv.org/pdf/2407.19961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19961]] Prichain II: CloudGuardian Cloud Security Proposal with Blockchain(https://arxiv.org/abs/2407.19961)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>With the advancement of cloud computing, data storage, and security have become crucial. The growing adoption of cloud services by companies, accompanied by increased threats from cybersecurity, highlights the importance of privacy and ownership of user data. Between 2022 and 2023, there has been an increase of around 48% in cloud security threats, emphasizing the urgent need for strong security solutions. To face these challenges, in this project, we propose integrating the Ethereum network's blockchain technology with a database located in the PostgreSQL cloud. The proposed solution aims to provide bidirectional data synchronization and strict control of access mechanisms. Blockchain technology ensures immutability and transparency of transactions, while PostgreSQL provides efficient and scalable storage. Through rigorous testing in an adaptive traffic control scenario, the results obtained indicate that this solution offers a significantly high level of security due to the decentralization of data, confirming that this solution is effective, and making it a powerful new option to improve security in cloud environments. In conclusion, the solution proposed in this project not only increases information security but also demonstrates the practical feasibility of integrating blockchain with cloud relational databases. This two-way alignment improves protection against cyberattacks and ensures that user data is protected from unauthorized access and malicious changes.</li>
</ul>

<h3>Title: A Temporal Psycholinguistics Approach to Identity Resolution of Social Media Users</h3>
<ul>
<li><strong>Authors: </strong>Md Touhidul Islam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19967">https://arxiv.org/abs/2407.19967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19967">https://arxiv.org/pdf/2407.19967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19967]] A Temporal Psycholinguistics Approach to Identity Resolution of Social Media Users(https://arxiv.org/abs/2407.19967)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this thesis, we propose an approach to identity resolution across social media platforms using the topics, sentiments, and timings of the posts on the platforms. After collecting the public posts of around 5000 profiles from Disqus and Twitter, we analyze their posts to match their profiles across the two platforms. We pursue both temporal and non-temporal methods in our analysis. While neither approach proves definitively superior, the temporal approach generally performs better. We found that the temporal window size influences results more than the shifting amount. On the other hand, our sentiment analysis shows that the inclusion of sentiment makes little difference, probably due to flawed data extraction methods. We also experimented with a distance-based reward-and-punishment-focused scoring model, which achieved an accuracy of 24.198% and an average rank of 158.217 out of 2525 in our collected corpus. Future work includes refining sentiment analysis by evaluating sentiments per topic, extending temporal analysis with additional phases, and improving the scoring model through weight adjustments and modified rewards.</li>
</ul>

<h3>Title: Private and Secure Fuzzy Name Matching</h3>
<ul>
<li><strong>Authors: </strong>Harsh Kasyap, Ugur Ilker Atmaca, Carsten Maple, Graham Cormode, Jiancong He</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19979">https://arxiv.org/abs/2407.19979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19979">https://arxiv.org/pdf/2407.19979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19979]] Private and Secure Fuzzy Name Matching(https://arxiv.org/abs/2407.19979)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Modern financial institutions rely on data for many operations, including a need to drive efficiency, enhance services and prevent financial crime. Data sharing across an organisation or between institutions can facilitate rapid, evidence-based decision making, including identifying money laundering and fraud. However, data privacy regulations impose restrictions on data sharing. Privacy-enhancing technologies are being increasingly employed to allow organisations to derive shared intelligence while ensuring regulatory compliance. This paper examines the case in which regulatory restrictions mean a party cannot share data on accounts of interest with another (internal or external) party to identify people that hold an account in each dataset. We observe that the names of account holders may be recorded differently in each data set. We introduce a novel privacy-preserving approach for fuzzy name matching across institutions, employing fully homomorphic encryption with locality-sensitive hashing. The efficiency of the approach is enhanced using a clustering mechanism. The practicality and effectiveness of the proposed approach are evaluated using different datasets. Experimental results demonstrate it takes around 100 and 1000 seconds to search 1000 names from 10k and 100k names, respectively. Moreover, the proposed approach exhibits significant improvement in reducing communication overhead by 30-300 times, using clustering.</li>
</ul>

<h3>Title: Adversarial Robustness in RGB-Skeleton Action Recognition: Leveraging Attention Modality Reweighter</h3>
<ul>
<li><strong>Authors: </strong>Chao Liu, Xin Liu, Zitong Yu, Yonghong Hou, Huanjing Yue, Jingyu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19981">https://arxiv.org/abs/2407.19981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19981">https://arxiv.org/pdf/2407.19981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19981]] Adversarial Robustness in RGB-Skeleton Action Recognition: Leveraging Attention Modality Reweighter(https://arxiv.org/abs/2407.19981)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) have been applied in many computer vision tasks and achieved state-of-the-art (SOTA) performance. However, misclassification will occur when DNNs predict adversarial examples which are created by adding human-imperceptible adversarial noise to natural examples. This limits the application of DNN in security-critical fields. In order to enhance the robustness of models, previous research has primarily focused on the unimodal domain, such as image recognition and video understanding. Although multi-modal learning has achieved advanced performance in various tasks, such as action recognition, research on the robustness of RGB-skeleton action recognition models is scarce. In this paper, we systematically investigate how to improve the robustness of RGB-skeleton action recognition models. We initially conducted empirical analysis on the robustness of different modalities and observed that the skeleton modality is more robust than the RGB modality. Motivated by this observation, we propose the \formatword{A}ttention-based \formatword{M}odality \formatword{R}eweighter (\formatword{AMR}), which utilizes an attention layer to re-weight the two modalities, enabling the model to learn more robust features. Our AMR is plug-and-play, allowing easy integration with multimodal models. To demonstrate the effectiveness of AMR, we conducted extensive experiments on various datasets. For example, compared to the SOTA methods, AMR exhibits a 43.77\% improvement against PGD20 attacks on the NTU-RGB+D 60 dataset. Furthermore, it effectively balances the differences in robustness between different modalities.</li>
</ul>

<h3>Title: Mixture of Nested Experts: Adaptive Processing of Visual Tokens</h3>
<ul>
<li><strong>Authors: </strong>Gagan Jain, Nidhi Hegde, Aditya Kusupati, Arsha Nagrani, Shyamal Buch, Prateek Jain, Anurag Arnab, Sujoy Paul</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19985">https://arxiv.org/abs/2407.19985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19985">https://arxiv.org/pdf/2407.19985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19985]] Mixture of Nested Experts: Adaptive Processing of Visual Tokens(https://arxiv.org/abs/2407.19985)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. We present Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, we achieve equivalent performance as the baseline models, while reducing inference time compute by over two-fold. We validate our approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2. We further highlight MoNE$'$s adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model.</li>
</ul>

<h3>Title: Reproducibility Study of "ITI-GEN: Inclusive Text-to-Image Generation"</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gallo Fernández, Răzvan-Andrei Matisan, Alejandro Monroy Muñoz, Janusz Partyka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19996">https://arxiv.org/abs/2407.19996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19996">https://arxiv.org/pdf/2407.19996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19996]] Reproducibility Study of "ITI-GEN: Inclusive Text-to-Image Generation"(https://arxiv.org/abs/2407.19996)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image generative models often present issues regarding fairness with respect to certain sensitive attributes, such as gender or skin tone. This study aims to reproduce the results presented in "ITI-GEN: Inclusive Text-to-Image Generation" by Zhang et al. (2023a), which introduces a model to improve inclusiveness in these kinds of models. We show that most of the claims made by the authors about ITI-GEN hold: it improves the diversity and quality of generated images, it is scalable to different domains, it has plug-and-play capabilities, and it is efficient from a computational point of view. However, ITI-GEN sometimes uses undesired attributes as proxy features and it is unable to disentangle some pairs of (correlated) attributes such as gender and baldness. In addition, when the number of considered attributes increases, the training time grows exponentially and ITI-GEN struggles to generate inclusive images for all elements in the joint distribution. To solve these issues, we propose using Hard Prompt Search with negative prompting, a method that does not require training and that handles negation better than vanilla Hard Prompt Search. Nonetheless, Hard Prompt Search (with or without negative prompting) cannot be used for continuous attributes that are hard to express in natural language, an area where ITI-GEN excels as it is guided by images during training. Finally, we propose combining ITI-GEN and Hard Prompt Search with negative prompting.</li>
</ul>

<h3>Title: Do LLMs Really Adapt to Domains? An Ontology Learning Perspective</h3>
<ul>
<li><strong>Authors: </strong>Huu Tan Mai, Cuong Xuan Chu, Heiko Paulheim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.19998">https://arxiv.org/abs/2407.19998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.19998">https://arxiv.org/pdf/2407.19998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.19998]] Do LLMs Really Adapt to Domains? An Ontology Learning Perspective(https://arxiv.org/abs/2407.19998)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated unprecedented prowess across various natural language processing tasks in various application domains. Recent studies show that LLMs can be leveraged to perform lexical semantic tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL). However, it has not effectively been verified whether their success is due to their ability to reason over unstructured or semi-structured data, or their effective learning of linguistic patterns and senses alone. This unresolved question is particularly crucial when dealing with domain-specific data, where the lexical senses and their meaning can completely differ from what a LLM has learned during its training stage. This paper investigates the following question: Do LLMs really adapt to domains and remain consistent in the extraction of structured knowledge, or do they only learn lexical senses instead of reasoning? To answer this question and, we devise a controlled experiment setup that uses WordNet to synthesize parallel corpora, with English and gibberish terms. We examine the differences in the outputs of LLMs for each corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical results show that, while adapting to the gibberish corpora, off-the-shelf LLMs do not consistently reason over semantic relationships between concepts, and instead leverage senses and their frame. However, fine-tuning improves the performance of LLMs on lexical semantic tasks even when the domain-specific terms are arbitrary and unseen during pre-training, hinting at the applicability of pre-trained LLMs for OL.</li>
</ul>

<h3>Title: ImagiNet: A Multi-Content Dataset for Generalizable Synthetic Image Detection via Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Delyan Boychev, Radostin Cholakov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20020">https://arxiv.org/abs/2407.20020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20020">https://arxiv.org/pdf/2407.20020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20020]] ImagiNet: A Multi-Content Dataset for Generalizable Synthetic Image Detection via Contrastive Learning(https://arxiv.org/abs/2407.20020)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models, such as diffusion models (DMs), variational autoencoders (VAEs), and generative adversarial networks (GANs), produce images with a level of authenticity that makes them nearly indistinguishable from real photos and artwork. While this capability is beneficial for many industries, the difficulty of identifying synthetic images leaves online media platforms vulnerable to impersonation and misinformation attempts. To support the development of defensive methods, we introduce ImagiNet, a high-resolution and balanced dataset for synthetic image detection, designed to mitigate potential biases in existing resources. It contains 200K examples, spanning four content categories: photos, paintings, faces, and uncategorized. Synthetic images are produced with open-source and proprietary generators, whereas real counterparts of the same content type are collected from public datasets. The structure of ImagiNet allows for a two-track evaluation system: i) classification as real or synthetic and ii) identification of the generative model. To establish a baseline, we train a ResNet-50 model using a self-supervised contrastive objective (SelfCon) for each track. The model demonstrates state-of-the-art performance and high inference speed across established benchmarks, achieving an AUC of up to 0.99 and balanced accuracy ranging from 86% to 95%, even under social network conditions that involve compression and resizing. Our data and code are available at this https URL.</li>
</ul>

<h3>Title: MimiQ: Low-Bit Data-Free Quantization of Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Kanghyun Choi, Hye Yoon Lee, Dain Kwon, SunJong Park, Kyuyeun Kim, Noseong Park, Jinho Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20021">https://arxiv.org/abs/2407.20021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20021">https://arxiv.org/pdf/2407.20021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20021]] MimiQ: Low-Bit Data-Free Quantization of Vision Transformers(https://arxiv.org/abs/2407.20021)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, transformer</a></li>
<li><strong>Abstract: </strong>Data-free quantization (DFQ) is a technique that creates a lightweight network from its full-precision counterpart without the original training data, often through a synthetic dataset. Although several DFQ methods have been proposed for vision transformer (ViT) architectures, they fail to achieve efficacy in low-bit settings. Examining the existing methods, we identify that their synthetic data produce misaligned attention maps, while those of the real samples are highly aligned. From the observation of aligned attention, we find that aligning attention maps of synthetic data helps to improve the overall performance of quantized ViTs. Motivated by this finding, we devise \aname, a novel DFQ method designed for ViTs that focuses on inter-head attention similarity. First, we generate synthetic data by aligning head-wise attention responses in relation to spatial query patches. Then, we apply head-wise structural attention distillation to align the attention maps of the quantized network to those of the full-precision teacher. The experimental results show that the proposed method significantly outperforms baselines, setting a new state-of-the-art performance for data-free ViT quantization.</li>
</ul>

<h3>Title: Aircraft Trajectory Segmentation-based Contrastive Coding: A Framework for Self-supervised Trajectory Representation</h3>
<ul>
<li><strong>Authors: </strong>Thaweerath Phisannupawong, Joshua Julian Damanik, Han-Lim Choi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20028">https://arxiv.org/abs/2407.20028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20028">https://arxiv.org/pdf/2407.20028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20028]] Aircraft Trajectory Segmentation-based Contrastive Coding: A Framework for Self-supervised Trajectory Representation(https://arxiv.org/abs/2407.20028)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Air traffic trajectory recognition has gained significant interest within the air traffic management community, particularly for fundamental tasks such as classification and clustering. This paper introduces Aircraft Trajectory Segmentation-based Contrastive Coding (ATSCC), a novel self-supervised time series representation learning framework designed to capture semantic information in air traffic trajectory data. The framework leverages the segmentable characteristic of trajectories and ensures consistency within the self-assigned segments. Intensive experiments were conducted on datasets from three different airports, totaling four datasets, comparing the learned representation's performance of downstream classification and clustering with other state-of-the-art representation learning techniques. The results show that ATSCC outperforms these methods by aligning with the labels defined by aeronautical procedures. ATSCC is adaptable to various airport configurations and scalable to incomplete trajectories. This research has expanded upon existing capabilities, achieving these improvements independently without predefined inputs such as airport configurations, maneuvering procedures, or labeled data.</li>
</ul>

<h3>Title: MaskInversion: Localized Embeddings via Optimization of Explainability Maps</h3>
<ul>
<li><strong>Authors: </strong>Walid Bousselham, Sofian Chaybouti, Christian Rupprecht, Vittorio Ferrari, Hilde Kuehne</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20034">https://arxiv.org/abs/2407.20034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20034">https://arxiv.org/pdf/2407.20034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20034]] MaskInversion: Localized Embeddings via Optimization of Explainability Maps(https://arxiv.org/abs/2407.20034)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Vision-language foundation models such as CLIP have achieved tremendous results in global vision-language alignment, but still show some limitations in creating representations for specific image regions. % To address this problem, we propose MaskInversion, a method that leverages the feature representations of pre-trained foundation models, such as CLIP, to generate a context-aware embedding for a query image region specified by a mask at test time. MaskInversion starts with initializing an embedding token and compares its explainability map, derived from the foundation model, to the query mask. The embedding token is then subsequently refined to approximate the query region by minimizing the discrepancy between its explainability map and the query mask. During this process, only the embedding vector is updated, while the underlying foundation model is kept frozen allowing to use MaskInversion with any pre-trained model. As deriving the explainability map involves computing its gradient, which can be expensive, we propose a gradient decomposition strategy that simplifies this computation. The learned region representation can be used for a broad range of tasks, including open-vocabulary class retrieval, referring expression comprehension, as well as for localized captioning and image generation. We evaluate the proposed method on all those tasks on several datasets such as PascalVOC, MSCOCO, RefCOCO, and OpenImagesV7 and show its capabilities compared to other SOTA approaches.</li>
</ul>

<h3>Title: Exploring Large Language Models to generate Easy to Read content</h3>
<ul>
<li><strong>Authors: </strong>Paloma Martínez, Lourdes Moreno, Alberto Ramos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20046">https://arxiv.org/abs/2407.20046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20046">https://arxiv.org/pdf/2407.20046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20046]] Exploring Large Language Models to generate Easy to Read content(https://arxiv.org/abs/2407.20046)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring text accessibility and understandability are essential goals, particularly for individuals with cognitive impairments and intellectual disabilities, who encounter challenges in accessing information across various mediums such as web pages, newspapers, administrative tasks, or health documents. Initiatives like Easy to Read and Plain Language guidelines aim to simplify complex texts; however, standardizing these guidelines remains challenging and often involves manual processes. This work presents an exploratory investigation into leveraging Artificial Intelligence (AI) and Natural Language Processing (NLP) approaches to systematically simplify Spanish texts into Easy to Read formats, with a focus on utilizing Large Language Models (LLMs) for simplifying texts, especially in generating Easy to Read content. The study contributes a parallel corpus of Spanish adapted for Easy To Read format, which serves as a valuable resource for training and testing text simplification systems. Additionally, several text simplification experiments using LLMs and the collected corpus are conducted, involving fine-tuning and testing a Llama2 model to generate Easy to Read content. A qualitative evaluation, guided by an expert in text adaptation for Easy to Read content, is carried out to assess the automatically simplified texts. This research contributes to advancing text accessibility for individuals with cognitive impairments, highlighting promising strategies for leveraging LLMs while responsibly managing energy usage.</li>
</ul>

<h3>Title: Denoising ESG: quantifying data uncertainty from missing data with Machine Learning and prediction intervals</h3>
<ul>
<li><strong>Authors: </strong>Sergio Caprioli, Jacopo Foschi, Riccardo Crupi, Alessandro Sabatino</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20047">https://arxiv.org/abs/2407.20047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20047">https://arxiv.org/pdf/2407.20047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20047]] Denoising ESG: quantifying data uncertainty from missing data with Machine Learning and prediction intervals(https://arxiv.org/abs/2407.20047)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Environmental, Social, and Governance (ESG) datasets are frequently plagued by significant data gaps, leading to inconsistencies in ESG ratings due to varying imputation methods. This paper explores the application of established machine learning techniques for imputing missing data in a real-world ESG dataset, emphasizing the quantification of uncertainty through prediction intervals. By employing multiple imputation strategies, this study assesses the robustness of imputation methods and quantifies the uncertainty associated with missing data. The findings highlight the importance of probabilistic machine learning models in providing better understanding of ESG scores, thereby addressing the inherent risks of wrong ratings due to incomplete data. This approach improves imputation practices to enhance the reliability of ESG ratings.</li>
</ul>

<h3>Title: Orca: Ocean Significant Wave Height Estimation with Spatio-temporally Aware Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhe Li, Ronghui Xu, Jilin Hu, Zhong Peng, Xi Lu, Chenjuan Guo, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20053">https://arxiv.org/abs/2407.20053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20053">https://arxiv.org/pdf/2407.20053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20053]] Orca: Ocean Significant Wave Height Estimation with Spatio-temporally Aware Large Language Models(https://arxiv.org/abs/2407.20053)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Significant wave height (SWH) is a vital metric in marine science, and accurate SWH estimation is crucial for various applications, e.g., marine energy development, fishery, early warning systems for potential risks, etc. Traditional SWH estimation methods that are based on numerical models and physical theories are hindered by computational inefficiencies. Recently, machine learning has emerged as an appealing alternative to improve accuracy and reduce computational time. However, due to limited observational technology and high costs, the scarcity of real-world data restricts the potential of machine learning models. To overcome these limitations, we propose an ocean SWH estimation framework, namely Orca. Specifically, Orca enhances the limited spatio-temporal reasoning abilities of classic LLMs with a novel spatiotemporal aware encoding module. By segmenting the limited buoy observational data temporally, encoding the buoys' locations spatially, and designing prompt templates, Orca capitalizes on the robust generalization ability of LLMs to estimate significant wave height effectively with limited data. Experimental results on the Gulf of Mexico demonstrate that Orca achieves state-of-the-art performance in SWH estimation.</li>
</ul>

<h3>Title: xAI-Drop: Don't Use What You Cannot Explain</h3>
<ul>
<li><strong>Authors: </strong>Vincenzo Marco De Luca, Antonio Longa, Andrea Passerini, Pietro Liò</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20067">https://arxiv.org/abs/2407.20067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20067">https://arxiv.org/pdf/2407.20067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20067]] xAI-Drop: Don't Use What You Cannot Explain(https://arxiv.org/abs/2407.20067)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have emerged as the predominant paradigm for learning from graph-structured data, offering a wide range of applications from social network analysis to bioinformatics. Despite their versatility, GNNs face challenges such as oversmoothing, lack of generalization and poor interpretability, which hinder their wider adoption and reliability in critical applications. Dropping has emerged as an effective paradigm for reducing noise during training and improving robustness of GNNs. However, existing approaches often rely on random or heuristic-based selection criteria, lacking a principled method to identify and exclude nodes that contribute to noise and over-complexity in the model. In this work, we argue that explainability should be a key indicator of a model's robustness throughout its training phase. To this end, we introduce xAI-Drop, a novel topological-level dropping regularizer that leverages explainability to pinpoint noisy network elements to be excluded from the GNN propagation mechanism. An empirical evaluation on diverse real-world datasets demonstrates that our method outperforms current state-of-the-art dropping approaches in accuracy, effectively reduces over-smoothing, and improves explanation quality.</li>
</ul>

<h3>Title: Unleash the Power of Ellipsis: Accuracy-enhanced Sparse Vector Technique with Exponential Noise</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Liu, Sheng Wang, Yixuan Liu, Feifei Li, Hong Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20068">https://arxiv.org/abs/2407.20068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20068">https://arxiv.org/pdf/2407.20068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20068]] Unleash the Power of Ellipsis: Accuracy-enhanced Sparse Vector Technique with Exponential Noise(https://arxiv.org/abs/2407.20068)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The Sparse Vector Technique (SVT) is one of the most fundamental tools in differential privacy (DP). It works as a backbone for adaptive data analysis by answering a sequence of queries on a given dataset, and gleaning useful information in a privacy-preserving manner. Unlike the typical private query releases that directly publicize the noisy query results, SVT is less informative -- it keeps the noisy query results to itself and only reveals a binary bit for each query, indicating whether the query result surpasses a predefined threshold. To provide a rigorous DP guarantee for SVT, prior works in the literature adopt a conservative privacy analysis by assuming the direct disclosure of noisy query results as in typical private query releases. This approach, however, hinders SVT from achieving higher query accuracy due to an overestimation of the privacy risks, which further leads to an excessive noise injection using the Laplacian or Gaussian noise for perturbation. Motivated by this, we provide a new privacy analysis for SVT by considering its less informative nature. Our analysis results not only broaden the range of applicable noise types for perturbation in SVT, but also identify the exponential noise as optimal among all evaluated noises (which, however, is usually deemed non-applicable in prior works). The main challenge in applying exponential noise to SVT is mitigating the sub-optimal performance due to the bias introduced by noise distributions. To address this, we develop a utility-oriented optimal threshold correction method and an appending strategy, which enhances the performance of SVT by increasing the precision and recall, respectively. The effectiveness of our proposed methods is substantiated both theoretically and empirically, demonstrating significant improvements up to $50\%$ across evaluated metrics.</li>
</ul>

<h3>Title: An Interpretable Rule Creation Method for Black-Box Models based on Surrogate Trees -- SRules</h3>
<ul>
<li><strong>Authors: </strong>Mario Parrón Verdasco, Esteban García-Cuesta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20070">https://arxiv.org/abs/2407.20070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20070">https://arxiv.org/pdf/2407.20070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20070]] An Interpretable Rule Creation Method for Black-Box Models based on Surrogate Trees -- SRules(https://arxiv.org/abs/2407.20070)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>As artificial intelligence (AI) systems become increasingly integrated into critical decision-making processes, the need for transparent and interpretable models has become paramount. In this article we present a new ruleset creation method based on surrogate decision trees (SRules), designed to improve the interpretability of black-box machine learning models. SRules balances the accuracy, coverage, and interpretability of machine learning models by recursively creating surrogate interpretable decision tree models that approximate the decision boundaries of a complex model. We propose a systematic framework for generating concise and meaningful rules from these surrogate models, allowing stakeholders to understand and trust the AI system's decision-making process. Our approach not only provides interpretable rules, but also quantifies the confidence and coverage of these rules. The proposed model allows to adjust its parameters to counteract the lack of interpretability by precision and coverage by allowing a near perfect fit and high interpretability of some parts of the model . The results show that SRules improves on other state-of-the-art techniques and introduces the possibility of creating highly interpretable specific rules for specific sub-parts of the model.</li>
</ul>

<h3>Title: Investigating the Impact of Semi-Supervised Methods with Data Augmentation on Offensive Language Detection in Romanian Language</h3>
<ul>
<li><strong>Authors: </strong>Elena Beatrice Nicola, Dumitru Clementin Cercel, Florin Pop</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20076">https://arxiv.org/abs/2407.20076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20076">https://arxiv.org/pdf/2407.20076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20076]] Investigating the Impact of Semi-Supervised Methods with Data Augmentation on Offensive Language Detection in Romanian Language(https://arxiv.org/abs/2407.20076)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Offensive language detection is a crucial task in today's digital landscape, where online platforms grapple with maintaining a respectful and inclusive environment. However, building robust offensive language detection models requires large amounts of labeled data, which can be expensive and time-consuming to obtain. Semi-supervised learning offers a feasible solution by utilizing labeled and unlabeled data to create more accurate and robust models. In this paper, we explore a few different semi-supervised methods, as well as data augmentation techniques. Concretely, we implemented eight semi-supervised methods and ran experiments for them using only the available data in the RO-Offense dataset and applying five augmentation techniques before feeding the data to the models. Experimental results demonstrate that some of them benefit more from augmentations than others.</li>
</ul>

<h3>Title: Background Semantics Matter: Cross-Task Feature Exchange Network for Clustered Infrared Small Target Detection With Sky-Annotated Dataset</h3>
<ul>
<li><strong>Authors: </strong>Yimian Dai, Mengxuan Xiao, Yiming Zhu, Huan Wang, Kehua Guo, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20078">https://arxiv.org/abs/2407.20078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20078">https://arxiv.org/pdf/2407.20078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20078]] Background Semantics Matter: Cross-Task Feature Exchange Network for Clustered Infrared Small Target Detection With Sky-Annotated Dataset(https://arxiv.org/abs/2407.20078)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Infrared small target detection poses unique challenges due to the scarcity of intrinsic target features and the abundance of similar background distractors. We argue that background semantics play a pivotal role in distinguishing visually similar objects for this task. To address this, we introduce a new task -- clustered infrared small target detection, and present DenseSIRST, a novel benchmark dataset that provides per-pixel semantic annotations for background regions, enabling the transition from sparse to dense target detection. Leveraging this dataset, we propose the Background-Aware Feature Exchange Network (BAFE-Net), which transforms the detection paradigm from a single task focused on the foreground to a multi-task architecture that jointly performs target detection and background semantic segmentation. BAFE-Net introduces a cross-task feature hard-exchange mechanism to embed target and background semantics between the two tasks. Furthermore, we propose the Background-Aware Gaussian Copy-Paste (BAG-CP) method, which selectively pastes small targets into sky regions during training, avoiding the creation of false alarm targets in complex non-sky backgrounds. Extensive experiments validate the effectiveness of BAG-CP and BAFE-Net in improving target detection accuracy while reducing false alarms. The DenseSIRST dataset, code, and trained models are available at this https URL.</li>
</ul>

<h3>Title: Infrared Small Target Detection based on Adjustable Sensitivity Strategy and Multi-Scale Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jinmiao Zhao, Zelin Shi, Chuang Yu, Yunpeng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20090">https://arxiv.org/abs/2407.20090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20090">https://arxiv.org/pdf/2407.20090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20090]] Infrared Small Target Detection based on Adjustable Sensitivity Strategy and Multi-Scale Fusion(https://arxiv.org/abs/2407.20090)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recently, deep learning-based single-frame infrared small target (SIRST) detection technology has made significant progress. However, existing infrared small target detection methods are often optimized for a fixed image resolution, a single wavelength, or a specific imaging system, limiting their breadth and flexibility in practical applications. Therefore, we propose a refined infrared small target detection scheme based on an adjustable sensitivity (AS) strategy and multi-scale fusion. Specifically, a multi-scale model fusion framework based on multi-scale direction-aware network (MSDA-Net) is constructed, which uses input images of multiple scales to train multiple models and fuses them. Multi-scale fusion helps characterize the shape, edge, and texture features of the target from different scales, making the model more accurate and reliable in locating the target. At the same time, we fully consider the characteristics of the infrared small target detection task and construct an edge enhancement difficulty mining (EEDM) loss. The EEDM loss helps alleviate the problem of category imbalance and guides the network to pay more attention to difficult target areas and edge features during training. In addition, we propose an adjustable sensitivity strategy for post-processing. This strategy significantly improves the detection rate of infrared small targets while ensuring segmentation accuracy. Extensive experimental results show that the proposed scheme achieves the best performance. Notably, this scheme won the first prize in the PRCV 2024 wide-area infrared small target detection competition.</li>
</ul>

<h3>Title: RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding</h3>
<ul>
<li><strong>Authors: </strong>Keming Wu, Man Yao, Yuhong Chou, Xuerui Qiu, Rui Yang, Bo Xu, Guoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20099">https://arxiv.org/abs/2407.20099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20099">https://arxiv.org/pdf/2407.20099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20099]] RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding(https://arxiv.org/abs/2407.20099)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) have received widespread attention due to their unique neuronal dynamics and low-power nature. Previous research empirically shows that SNNs with Poisson coding are more robust than Artificial Neural Networks (ANNs) on small-scale datasets. However, it is still unclear in theory how the adversarial robustness of SNNs is derived, and whether SNNs can still maintain its adversarial robustness advantage on large-scale dataset tasks. This work theoretically demonstrates that SNN's inherent adversarial robustness stems from its Poisson coding. We reveal the conceptual equivalence of Poisson coding and randomized smoothing in defense strategies, and analyze in depth the trade-off between accuracy and adversarial robustness in SNNs via the proposed Randomized Smoothing Coding (RSC) method. Experiments demonstrate that the proposed RSC-SNNs show remarkable adversarial robustness, surpassing ANNs and achieving state-of-the-art robustness results on large-scale dataset ImageNet. Our open-source implementation code is available at this https URL: this https URL.</li>
</ul>

<h3>Title: F-KANs: Federated Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Engin Zeydan, Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, Marius Caus, Abdullah Aydeger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20100">https://arxiv.org/abs/2407.20100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20100">https://arxiv.org/pdf/2407.20100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20100]] F-KANs: Federated Kolmogorov-Arnold Networks(https://arxiv.org/abs/2407.20100)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>In this paper, we present an innovative federated learning (FL) approach that utilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. By utilizing the adaptive activation capabilities of KANs in a federated framework, we aim to improve classification capabilities while preserving privacy. The study evaluates the performance of federated KANs (F- KANs) compared to traditional Multi-Layer Perceptrons (MLPs) on classification task. The results show that the F-KANs model significantly outperforms the federated MLP model in terms of accuracy, precision, recall, F1 score and stability, and achieves better performance, paving the way for more efficient and privacy-preserving predictive analytics.</li>
</ul>

<h3>Title: Strong Copyright Protection for Language Models via Adaptive Model Fusion</h3>
<ul>
<li><strong>Authors: </strong>Javier Abad, Konstantin Donhauser, Francesco Pinto, Fanny Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20105">https://arxiv.org/abs/2407.20105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20105">https://arxiv.org/pdf/2407.20105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20105]] Strong Copyright Protection for Language Models via Adaptive Model Fusion(https://arxiv.org/abs/2407.20105)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>The risk of language models unintentionally reproducing copyrighted material from their training data has led to the development of various protective measures. In this paper, we propose model fusion as an effective solution to safeguard against copyright infringement. In particular, we introduce Copyright-Protecting Fusion (CP-Fuse), an algorithm that adaptively combines language models to minimize the reproduction of protected materials. CP-Fuse is inspired by the recently proposed Near-Access Free (NAF) framework and additionally incorporates a desirable balancing property that we demonstrate prevents the reproduction of memorized training data. Our results show that CP-Fuse significantly reduces the memorization of copyrighted content while maintaining high-quality text and code generation. Furthermore, we demonstrate how CP-Fuse can be integrated with other techniques for enhanced protection.</li>
</ul>

<h3>Title: Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Liyuan Mao, Haoran Xu, Weinan Zhang, Xianyuan Zhan, Amy Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20109">https://arxiv.org/abs/2407.20109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20109">https://arxiv.org/pdf/2407.20109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20109]] Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning(https://arxiv.org/abs/2407.20109)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>One important property of DIstribution Correction Estimation (DICE) methods is that the solution is the optimal stationary distribution ratio between the optimized and data collection policy. In this work, we show that DICE-based methods can be viewed as a transformation from the behavior distribution to the optimal policy distribution. Based on this, we propose a novel approach, Diffusion-DICE, that directly performs this transformation using diffusion models. We find that the optimal policy's score function can be decomposed into two terms: the behavior policy's score function and the gradient of a guidance term which depends on the optimal distribution ratio. The first term can be obtained from a diffusion model trained on the dataset and we propose an in-sample learning objective to learn the second term. Due to the multi-modality contained in the optimal policy distribution, the transformation in Diffusion-DICE may guide towards those local-optimal modes. We thus generate a few candidate actions and carefully select from them to approach global-optimum. Different from all other diffusion-based offline RL methods, the guide-then-select paradigm in Diffusion-DICE only uses in-sample actions for training and brings minimal error exploitation in the value function. We use a didatic toycase example to show how previous diffusion-based methods fail to generate optimal actions due to leveraging these errors and how Diffusion-DICE successfully avoids that. We then conduct extensive experiments on benchmark datasets to show the strong performance of Diffusion-DICE.</li>
</ul>

<h3>Title: Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number</h3>
<ul>
<li><strong>Authors: </strong>Chen-Lu Ding, Jiancan Wu, Wei Lin, Shiyang Shen, Xiang Wang, Yancheng Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20119">https://arxiv.org/abs/2407.20119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20119">https://arxiv.org/pdf/2407.20119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20119]] Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number(https://arxiv.org/abs/2407.20119)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce a novel self-supervised deep clustering approach tailored for unstructured data without requiring prior knowledge of the number of clusters, termed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRC adaptively learns the graph structure and edge weights to capture both local and global structural information. The obtained graph enables us to learn clustering-friendly feature representations by an enhanced graph auto-encoder with contrastive learning technique. It further leverages the clustering results adaptively obtained by robust continuous clustering (RCC) to generate prototypes for negative sampling, which can further contribute to promoting consistency among positive pairs and enlarging the gap between positive and negative samples. ASRC obtains the final clustering results by applying RCC to the learned feature representations with their consistent graph structure and edge weights. Extensive experiments conducted on seven benchmark datasets demonstrate the efficacy of ASRC, demonstrating its superior performance over other popular clustering models. Notably, ASRC even outperforms methods that rely on prior knowledge of the number of clusters, highlighting its effectiveness in addressing the challenges of clustering unstructured data.</li>
</ul>

<h3>Title: DDAP: Dual-Domain Anti-Personalization against Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jing Yang, Runping Xi, Yingxin Lai, Xun Lin, Zitong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20141">https://arxiv.org/abs/2407.20141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20141">https://arxiv.org/pdf/2407.20141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20141]] DDAP: Dual-Domain Anti-Personalization against Text-to-Image Diffusion Models(https://arxiv.org/abs/2407.20141)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, steal, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based personalized visual content generation technologies have achieved significant breakthroughs, allowing for the creation of specific objects by just learning from a few reference photos. However, when misused to fabricate fake news or unsettling content targeting individuals, these technologies could cause considerable societal harm. To address this problem, current methods generate adversarial samples by adversarially maximizing the training loss, thereby disrupting the output of any personalized generation model trained with these samples. However, the existing methods fail to achieve effective defense and maintain stealthiness, as they overlook the intrinsic properties of diffusion models. In this paper, we introduce a novel Dual-Domain Anti-Personalization framework (DDAP). Specifically, we have developed Spatial Perturbation Learning (SPL) by exploiting the fixed and perturbation-sensitive nature of the image encoder in personalized generation. Subsequently, we have designed a Frequency Perturbation Learning (FPL) method that utilizes the characteristics of diffusion models in the frequency domain. The SPL disrupts the overall texture of the generated images, while the FPL focuses on image details. By alternating between these two methods, we construct the DDAP framework, effectively harnessing the strengths of both domains. To further enhance the visual quality of the adversarial samples, we design a localization module to accurately capture attentive areas while ensuring the effectiveness of the attack and avoiding unnecessary disturbances in the background. Extensive experiments on facial benchmarks have shown that the proposed DDAP enhances the disruption of personalized generation models while also maintaining high quality in adversarial samples, making it more effective in protecting privacy in practical applications.</li>
</ul>

<h3>Title: Diffusion Feedback Helps CLIP See Better</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, Xinlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20171">https://arxiv.org/abs/2407.20171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20171">https://arxiv.org/pdf/2407.20171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20171]] Diffusion Feedback Helps CLIP See Better(https://arxiv.org/abs/2407.20171)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedback from text-to-image diffusion models to optimize CLIP representations, with only images (without corresponding text). We demonstrate that DIVA improves CLIP's performance on the challenging MMVP-VLM benchmark which assesses fine-grained visual abilities to a large extent (e.g., 3-7%), and enhances the performance of MLLMs and vision models on multimodal understanding and segmentation tasks. Extensive evaluation on 29 image classification and retrieval benchmarks confirms that our framework preserves CLIP's strong zero-shot capabilities. The code will be available at this https URL.</li>
</ul>

<h3>Title: Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xingchen Zeng, Haichuan Lin, Yilin Ye, Wei Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20174">https://arxiv.org/abs/2407.20174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20174">https://arxiv.org/pdf/2407.20174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20174]] Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning(https://arxiv.org/abs/2407.20174)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emerging multimodal large language models (MLLMs) exhibit great potential for chart question answering (CQA). Recent efforts primarily focus on scaling up training datasets (i.e., charts, data tables, and question-answer (QA) pairs) through data collection and synthesis. However, our empirical study on existing MLLMs and CQA datasets reveals notable gaps. First, current data collection and synthesis focus on data volume and lack consideration of fine-grained visual encodings and QA tasks, resulting in unbalanced data distribution divergent from practical CQA scenarios. Second, existing work follows the training recipe of the base MLLMs initially designed for natural images, under-exploring the adaptation to unique chart characteristics, such as rich text elements. To fill the gap, we propose a visualization-referenced instruction tuning approach to guide the training dataset enhancement and model development. Specifically, we propose a novel data engine to effectively filter diverse and high-quality data from existing datasets and subsequently refine and augment the data using LLM-based generation techniques to better align with practical QA tasks and visual encodings. Then, to facilitate the adaptation to chart characteristics, we utilize the enriched data to train an MLLM by unfreezing the vision encoder and incorporating a mixture-of-resolution adaptation strategy for enhanced fine-grained recognition. Experimental results validate the effectiveness of our approach. Even with fewer training examples, our model consistently outperforms state-of-the-art CQA models on established benchmarks. We also contribute a dataset split as a benchmark for future research. Source codes and datasets of this paper are available at this https URL.</li>
</ul>

<h3>Title: Towards Localized Fine-Grained Control for Facial Expression Generation</h3>
<ul>
<li><strong>Authors: </strong>Tuomas Varanka, Huai-Qian Khor, Yante Li, Mengting Wei, Hanwei Kung, Nicu Sebe, Guoying Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20175">https://arxiv.org/abs/2407.20175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20175">https://arxiv.org/pdf/2407.20175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20175]] Towards Localized Fine-Grained Control for Facial Expression Generation(https://arxiv.org/abs/2407.20175)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have surged in popularity recently due to their ability to produce high-quality images and video. However, steering these models to produce images with specific attributes and precise control remains challenging. Humans, particularly their faces, are central to content generation due to their ability to convey rich expressions and intent. Current generative models mostly generate flat neutral expressions and characterless smiles without authenticity. Other basic expressions like anger are possible, but are limited to the stereotypical expression, while other unconventional facial expressions like doubtful are difficult to reliably generate. In this work, we propose the use of AUs (action units) for facial expression control in face generation. AUs describe individual facial muscle movements based on facial anatomy, allowing precise and localized control over the intensity of facial movements. By combining different action units, we unlock the ability to create unconventional facial expressions that go beyond typical emotional models, enabling nuanced and authentic reactions reflective of real-world expressions. The proposed method can be seamlessly integrated with both text and image prompts using adapters, offering precise and intuitive control of the generated results. Code and dataset are available in {this https URL}.</li>
</ul>

<h3>Title: Blockchain for Large Language Model Security and Safety: A Holistic Survey</h3>
<ul>
<li><strong>Authors: </strong>Caleb Geren, Amanda Board, Gaby G. Dagher, Tim Andersen, Jun Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20181">https://arxiv.org/abs/2407.20181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20181">https://arxiv.org/pdf/2407.20181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20181]] Blockchain for Large Language Model Security and Safety: A Holistic Survey(https://arxiv.org/abs/2407.20181)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>With the advent of accessible interfaces for interacting with large language models, there has been an associated explosion in both their commercial and academic interest. Consequently, there has also been an sudden burst of novel attacks associated with large language models, jeopardizing user data on a massive scale. Situated at a comparable crossroads in its development, and equally prolific to LLMs in its rampant growth, blockchain has emerged in recent years as a disruptive technology with the potential to redefine how we approach data handling. In particular, and due to its strong guarantees about data immutability and irrefutability as well as inherent data provenance assurances, blockchain has attracted significant attention as a means to better defend against the array of attacks affecting LLMs and further improve the quality of their responses. In this survey, we holistically evaluate current research on how blockchains are being used to help protect against LLM vulnerabilities, as well as analyze how they may further be used in novel applications. To better serve these ends, we introduce a taxonomy of blockchain for large language models (BC4LLM) and also develop various definitions to precisely capture the nature of different bodies of research in these areas. Moreover, throughout the paper, we present frameworks to contextualize broader research efforts, and in order to motivate the field further, we identify future research goals as well as challenges present in the blockchain for large language model (BC4LLM) space.</li>
</ul>

<h3>Title: MindSearch: Mimicking Human Minds Elicits Deep AI Searcher</h3>
<ul>
<li><strong>Authors: </strong>Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20183">https://arxiv.org/abs/2407.20183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20183">https://arxiv.org/pdf/2407.20183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20183]] MindSearch: Mimicking Human Minds Elicits Deep AI Searcher(https://arxiv.org/abs/2407.20183)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and this http URL applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.</li>
</ul>

<h3>Title: QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Hongming Tan, Shaoxiong Zhan, Hai Lin, Hai-Tao Zheng, Wai Kin (Victor)Chan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20207">https://arxiv.org/abs/2407.20207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20207">https://arxiv.org/pdf/2407.20207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20207]] QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval(https://arxiv.org/abs/2407.20207)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In dense retrieval, embedding long texts into dense vectors can result in information loss, leading to inaccurate query-text matching. Additionally, low-quality texts with excessive noise or sparse key information are unlikely to align well with relevant queries. Recent studies mainly focus on improving the sentence embedding model or retrieval process. In this work, we introduce a novel text augmentation framework for dense retrieval. This framework transforms raw documents into information-dense text formats, which supplement the original texts to effectively address the aforementioned issues without modifying embedding or retrieval methodologies. Two text representations are generated via large language models (LLMs) zero-shot prompting: question-answer pairs and element-driven events. We term this approach QAEA-DR: unifying question-answer generation and event extraction in a text augmentation framework for dense retrieval. To further enhance the quality of generated texts, a scoring-based evaluation and regeneration mechanism is introduced in LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval, supported by both theoretical analysis and empirical experiments.</li>
</ul>

<h3>Title: SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction</h3>
<ul>
<li><strong>Authors: </strong>Çağhan Köksal, Ghazal Ghazaei, Felix Holm, Azade Farshad, Nassir Navab</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20214">https://arxiv.org/abs/2407.20214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20214">https://arxiv.org/pdf/2407.20214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20214]] SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction(https://arxiv.org/abs/2407.20214)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Graph-based holistic scene representations facilitate surgical workflow understanding and have recently demonstrated significant success. However, this task is often hindered by the limited availability of densely annotated surgical scene data. In this work, we introduce an end-to-end framework for the generation and optimization of surgical scene graphs on a downstream task. Our approach leverages the flexibility of graph-based spectral clustering and the generalization capability of foundation models to generate unsupervised scene graphs with learnable properties. We reinforce the initial spatial graph with sparse temporal connections using local matches between consecutive frames to predict temporally consistent clusters across a temporal neighborhood. By jointly optimizing the spatiotemporal relations and node features of the dynamic scene graph with the downstream task of phase segmentation, we address the costly and annotation-burdensome task of semantic scene comprehension and scene graph generation in surgical videos using only weak surgical phase labels. Further, by incorporating effective intermediate scene representation disentanglement steps within the pipeline, our solution outperforms the SOTA on the CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflow recognition</li>
</ul>

<h3>Title: Global Structure-from-Motion Revisited</h3>
<ul>
<li><strong>Authors: </strong>Linfei Pan, Dániel Baráth, Marc Pollefeys, Johannes L. Schönberger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20219">https://arxiv.org/abs/2407.20219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20219">https://arxiv.org/pdf/2407.20219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20219]] Global Structure-from-Motion Revisited(https://arxiv.org/abs/2407.20219)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recovering 3D structure and camera motion from images has been a long-standing focus of computer vision research and is known as Structure-from-Motion (SfM). Solutions to this problem are categorized into incremental and global approaches. Until now, the most popular systems follow the incremental paradigm due to its superior accuracy and robustness, while global approaches are drastically more scalable and efficient. With this work, we revisit the problem of global SfM and propose GLOMAP as a new general-purpose system that outperforms the state of the art in global SfM. In terms of accuracy and robustness, we achieve results on-par or superior to COLMAP, the most widely used incremental SfM, while being orders of magnitude faster. We share our system as an open-source implementation at {this https URL}.</li>
</ul>

<h3>Title: Correspondence-Free SE(3) Point Cloud Registration in RKHS via Unsupervised Equivariant Learning</h3>
<ul>
<li><strong>Authors: </strong>Ray Zhang, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Cheng-Hao Kuo, Ryan Eustice, Maani Ghaffari, Arnie Sen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20223">https://arxiv.org/abs/2407.20223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20223">https://arxiv.org/pdf/2407.20223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20223]] Correspondence-Free SE(3) Point Cloud Registration in RKHS via Unsupervised Equivariant Learning(https://arxiv.org/abs/2407.20223)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces a robust unsupervised SE(3) point cloud registration method that operates without requiring point correspondences. The method frames point clouds as functions in a reproducing kernel Hilbert space (RKHS), leveraging SE(3)-equivariant features for direct feature space registration. A novel RKHS distance metric is proposed, offering reliable performance amidst noise, outliers, and asymmetrical data. An unsupervised training approach is introduced to effectively handle limited ground truth data, facilitating adaptation to real datasets. The proposed method outperforms classical and supervised methods in terms of registration accuracy on both synthetic (ModelNet40) and real-world (ETH3D) noisy, outlier-rich datasets. To our best knowledge, this marks the first instance of successful real RGB-D odometry data registration using an equivariant method. The code is available at {this https URL}</li>
</ul>

<h3>Title: Can Editing LLMs Inject Harm?</h3>
<ul>
<li><strong>Authors: </strong>Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen, Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, Jindong Gu, Huaxiu Yao, Chaowei Xiao, Xifeng Yan, William Yang Wang, Philip Torr, Dawn Song, Kai Shu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20224">https://arxiv.org/abs/2407.20224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20224">https://arxiv.org/pdf/2407.20224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20224]] Can Editing LLMs Inject Harm?(https://arxiv.org/abs/2407.20224)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, fair, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing techniques have been increasingly adopted to efficiently correct the false or outdated knowledge in Large Language Models (LLMs), due to the high cost of retraining from scratch. Meanwhile, one critical but under-explored question is: can knowledge editing be used to inject harm into LLMs? In this paper, we propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and conduct a systematic investigation with a newly constructed dataset EditAttack. Specifically, we focus on two typical safety risks of Editing Attack including Misinformation Injection and Bias Injection. For the risk of misinformation injection, we first categorize it into commonsense misinformation injection and long-tail misinformation injection. Then, we find that editing attacks can inject both types of misinformation into LLMs, and the effectiveness is particularly high for commonsense misinformation injection. For the risk of bias injection, we discover that not only can biased sentences be injected into LLMs with high effectiveness, but also one single biased sentence injection can cause a high bias increase in general outputs of LLMs, which are even highly irrelevant to the injected sentence, indicating a catastrophic impact on the overall fairness of LLMs. Then, we further illustrate the high stealthiness of editing attacks, measured by their impact on the general knowledge and reasoning capacities of LLMs, and show the hardness of defending editing attacks with empirical evidence. Our discoveries demonstrate the emerging misuse risks of knowledge editing techniques on compromising the safety alignment of LLMs.</li>
</ul>

<h3>Title: Improving 2D Feature Representations by 3D-Aware Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuanwen Yue, Anurag Das, Francis Engelmann, Siyu Tang, Jan Eric Lenssen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20229">https://arxiv.org/abs/2407.20229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20229">https://arxiv.org/pdf/2407.20229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20229]] Improving 2D Feature Representations by 3D-Aware Fine-Tuning(https://arxiv.org/abs/2407.20229)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Current visual foundation models are trained purely on unstructured 2D data, limiting their understanding of 3D structure of objects and scenes. In this work, we show that fine-tuning on 3D-aware data improves the quality of emerging semantic features. We design a method to lift semantic 2D features into an efficient 3D Gaussian representation, which allows us to re-render them for arbitrary views. Using the rendered 3D-aware features, we design a fine-tuning strategy to transfer such 3D awareness into a 2D foundation model. We demonstrate that models fine-tuned in that way produce features that readily improve downstream task performance in semantic segmentation and depth estimation through simple linear probing. Notably, though fined-tuned on a single indoor dataset, the improvement is transferable to a variety of indoor datasets and out-of-domain datasets. We hope our study encourages the community to consider injecting 3D awareness when training 2D foundation models. Project page: this https URL.</li>
</ul>

<h3>Title: Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Ekaterina Iakovleva, Fabio Pizzati, Philip Torr, Stéphane Lathuilière</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20232">https://arxiv.org/abs/2407.20232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20232">https://arxiv.org/pdf/2407.20232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20232]] Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing(https://arxiv.org/abs/2407.20232)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Text-based editing diffusion models exhibit limited performance when the user's input instruction is ambiguous. To solve this problem, we propose $\textit{Specify ANd Edit}$ (SANE), a zero-shot inference pipeline for diffusion-based editing systems. We use a large language model (LLM) to decompose the input instruction into specific instructions, i.e. well-defined interventions to apply to the input image to satisfy the user's request. We benefit from the LLM-derived instructions along the original one, thanks to a novel denoising guidance strategy specifically designed for the task. Our experiments with three baselines and on two datasets demonstrate the benefits of SANE in all setups. Moreover, our pipeline improves the interpretability of editing models, and boosts the output diversity. We also demonstrate that our approach can be applied to any edit, whether ambiguous or not. Our code is public at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
