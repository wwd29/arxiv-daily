<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Confidential Truth Finding with Multi-Party Computation (Extended Version). (arXiv:2305.14727v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14727">http://arxiv.org/abs/2305.14727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14727] Confidential Truth Finding with Multi-Party Computation (Extended Version)](http://arxiv.org/abs/2305.14727) #secure</code></li>
<li>Summary: <p>Federated knowledge discovery and data mining are challenged to assess the
trustworthiness of data originating from autonomous sources while protecting
confidentiality and privacy. Truth-finding algorithms help corroborate data
from disagreeing sources. For each query it receives, a truth-finding algorithm
predicts a truth value of the answer, possibly updating the trustworthiness
factor of each source. Few works, however, address the issues of
confidentiality and privacy. We devise and present a secure
secret-sharing-based multi-party computation protocol for pseudo-equality tests
that are used in truth-finding algorithms to compute additions depending on a
condition. The protocol guarantees confidentiality of the data and privacy of
the sources. We also present variants of truth-finding algorithms that would
make the computation faster when executed using secure multi-party computation.
We empirically evaluate the performance of the proposed protocol on two
state-of-the-art truth-finding algorithms, Cosine, and 3-Estimates, and compare
them with that of the baseline plain algorithms. The results confirm that the
secret-sharing-based secure multi-party algorithms are as accurate as the
corresponding baselines but for proposed numerical approximations that
significantly reduce the efficiency loss incurred.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Understanding the Country-Level Security of Free Content Websites and their Hosting Infrastructure. (arXiv:2305.14531v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14531">http://arxiv.org/abs/2305.14531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14531] Understanding the Country-Level Security of Free Content Websites and their Hosting Infrastructure](http://arxiv.org/abs/2305.14531) #security</code></li>
<li>Summary: <p>This paper examines free content websites (FCWs) and premium content websites
(PCWs) in different countries, comparing them to general websites. The focus is
on the distribution of malicious websites and their correlation with the
national cyber security index (NCSI), which measures a country's cyber security
maturity and its ability to deter the hosting of such malicious websites. By
analyzing a dataset comprising 1,562 FCWs and PCWs, along with Alexa's top
million websites dataset sample, we discovered that a majority of the
investigated websites are hosted in the United States. Interestingly, the
United States has a relatively low NCSI, mainly due to a lower score in privacy
policy development. Similar patterns were observed for other countries With
varying NCSI criteria. Furthermore, we present the distribution of various
categories of FCWs and PCWs across countries. We identify the top hosting
countries for each category and provide the percentage of discovered malicious
websites in those countries. Ultimately, the goal of this study is to identify
regional vulnerabilities in hosting FCWs and guide policy improvements at the
country level to mitigate potential cyber threats.
</p></li>
</ul>

<h3>Title: Adversarial Machine Learning and Cybersecurity: Risks, Challenges, and Legal Implications. (arXiv:2305.14553v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14553">http://arxiv.org/abs/2305.14553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14553] Adversarial Machine Learning and Cybersecurity: Risks, Challenges, and Legal Implications](http://arxiv.org/abs/2305.14553) #security</code></li>
<li>Summary: <p>In July 2022, the Center for Security and Emerging Technology (CSET) at
Georgetown University and the Program on Geopolitics, Technology, and
Governance at the Stanford Cyber Policy Center convened a workshop of experts
to examine the relationship between vulnerabilities in artificial intelligence
systems and more traditional types of software vulnerabilities. Topics
discussed included the extent to which AI vulnerabilities can be handled under
standard cybersecurity processes, the barriers currently preventing the
accurate sharing of information about AI vulnerabilities, legal issues
associated with adversarial attacks on AI systems, and potential areas where
government support could improve AI vulnerability management and mitigation.
</p></li>
</ul>

<p>This report is meant to accomplish two things. First, it provides a
high-level discussion of AI vulnerabilities, including the ways in which they
are disanalogous to other types of vulnerabilities, and the current state of
affairs regarding information sharing and legal oversight of AI
vulnerabilities. Second, it attempts to articulate broad recommendations as
endorsed by the majority of participants at the workshop.
</p>

<h3>Title: Towards Understanding Crypto Money Laundering in Web3 Through the Lenses of Ethereum Heists. (arXiv:2305.14748v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14748">http://arxiv.org/abs/2305.14748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14748] Towards Understanding Crypto Money Laundering in Web3 Through the Lenses of Ethereum Heists](http://arxiv.org/abs/2305.14748) #security</code></li>
<li>Summary: <p>With the overall momentum of the blockchain industry, crypto-based crimes are
becoming more and more prevalent. After committing a crime, the main goal of
cybercriminals is to obfuscate the source of the illicit funds in order to
convert them into cash and get away with it. Many studies have analyzed money
laundering in the field of the traditional financial sector and
blockchain-based Bitcoin. But so far, little is known about the characteristics
of crypto money laundering in the blockchain-based Web3 ecosystem. To fill this
gap, and considering that Ethereum is the largest platform on Web3, in this
paper, we systematically study the behavioral characteristics and economic
impact of money laundering accounts through the lenses of Ethereum heists.
Based on a very small number of tagged accounts of exchange hackers, DeFi
exploiters, and scammers, we mine untagged money laundering groups through
heuristic transaction tracking methods, to carve out a full picture of security
incidents. By analyzing account characteristics and transaction networks, we
obtain many interesting findings about crypto money laundering in Web3,
observing the escalating money laundering methods such as creating counterfeit
tokens and masquerading as speculators. Finally, based on these findings we
provide inspiration for anti-money laundering to promote the healthy
development of the Web3 ecosystem.
</p></li>
</ul>

<h3>Title: Applications of Machine Learning in Detecting Afghan Fake Banknotes. (arXiv:2305.14745v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14745">http://arxiv.org/abs/2305.14745</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14745] Applications of Machine Learning in Detecting Afghan Fake Banknotes](http://arxiv.org/abs/2305.14745) #security</code></li>
<li>Summary: <p>Fake currency, unauthorized imitation money lacking government approval,
constitutes a form of fraud. Particularly in Afghanistan, the prevalence of
fake currency poses significant challenges and detrimentally impacts the
economy. While banks and commercial establishments employ authentication
machines, the public lacks access to such systems, necessitating a program that
can detect counterfeit banknotes accessible to all. This paper introduces a
method using image processing to identify counterfeit Afghan banknotes by
analyzing specific security features. Extracting first and second order
statistical features from input images, the WEKA machine learning tool was
employed to construct models and perform classification with Random Forest,
PART, and Na\"ive Bayes algorithms. The Random Forest algorithm achieved
exceptional accuracy of 99% in detecting fake Afghan banknotes, indicating the
efficacy of the proposed method as a solution for identifying counterfeit
currency.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems. (arXiv:2305.14536v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14536">http://arxiv.org/abs/2305.14536</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14536] MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems](http://arxiv.org/abs/2305.14536) #privacy</code></li>
<li>Summary: <p>Although automatic dialogue tutors hold great potential in making education
personalized and more accessible, research on such systems has been hampered by
a lack of sufficiently large and high-quality datasets. However, collecting
such datasets remains challenging, as recording tutoring sessions raises
privacy concerns and crowdsourcing leads to insufficient data quality. To
address this problem, we propose a framework to semi-synthetically generate
such dialogues by pairing real teachers with a large language model (LLM)
scaffolded to represent common student errors. In this paper, we describe our
ongoing efforts to use this framework to collect MathDial, a dataset of
currently ca. 1.5k tutoring dialogues grounded in multi-step math word
problems. We show that our dataset exhibits rich pedagogical properties,
focusing on guiding students using sense-making questions to let them explore
problems. Moreover, we outline that MathDial and its grounding annotations can
be used to finetune language models to be more effective tutors (and not just
solvers) and highlight remaining challenges that need to be addressed by the
research community. We will release our dataset publicly to foster research in
this socially important area of NLP.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models. (arXiv:2305.14384v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14384">http://arxiv.org/abs/2305.14384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14384] Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models](http://arxiv.org/abs/2305.14384) #attack</code></li>
<li>Summary: <p>The generative AI revolution in recent years has been spurred by an expansion
in compute power and data quantity, which together enable extensive
pre-training of powerful text-to-image (T2I) models. With their greater
capabilities to generate realistic and creative content, these T2I models like
DALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider
audiences. Any unsafe behaviors inherited from pretraining on uncurated
internet-scraped datasets thus have the potential to cause wide-reaching harm,
for example, through generated images which are violent, sexually explicit, or
contain biased and derogatory stereotypes. Despite this risk of harm, we lack
systematic and structured evaluation datasets to scrutinize model behavior,
especially adversarial attacks that bypass existing safety filters. A typical
bottleneck in safety evaluation is achieving a wide coverage of different types
of challenging examples in the evaluation set, i.e., identifying 'unknown
unknowns' or long-tail problems. To address this need, we introduce the
Adversarial Nibbler challenge. The goal of this challenge is to crowdsource a
diverse set of failure modes and reward challenge participants for successfully
finding safety vulnerabilities in current state-of-the-art T2I models.
Ultimately, we aim to provide greater awareness of these issues and assist
developers in improving the future safety and reliability of generative AI
models. Adversarial Nibbler is a data-centric challenge, part of the DataPerf
challenge suite, organized and supported by Kaggle and MLCommons.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14381">http://arxiv.org/abs/2305.14381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14381] Connecting Multi-modal Contrastive Representations](http://arxiv.org/abs/2305.14381) #robust</code></li>
<li>Summary: <p>Multi-modal Contrastive Representation (MCR) learning aims to encode
different modalities into a semantically aligned shared space. This paradigm
shows remarkable generalization ability on numerous downstream tasks across
various modalities. However, the reliance on massive high-quality data pairs
limits its further development on more modalities. This paper proposes a novel
training-efficient method for learning MCR without paired data called
Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given
two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project
them to a new space and use the data from the overlapping modality B to
aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A,
B) and (B, C) are already aligned within each MCR, the connection learned by
overlapping modality can also be transferred to non-overlapping modality pair
(A, C). To unleash the potential of C-MCR, we further introduce a
semantic-enhanced inter- and intra-MCR connection method. We first enhance the
semantic consistency and completion of embeddings across different modalities
for more robust alignment. Then we utilize the inter-MCR alignment to establish
the connection, and employ the intra-MCR alignment to better maintain the
connection for inputs from non-overlapping modalities. We take the field of
audio-visual contrastive learning as an example to demonstrate the
effectiveness of C-MCR. We connect pre-trained CLIP and CLAP models via texts
to derive audio-visual contrastive representations. Remarkably, without using
any paired audio-visual data and further tuning, C-MCR achieves
state-of-the-art performance on six datasets across three audio-visual
downstream tasks.
</p></li>
</ul>

<h3>Title: Point2SSM: Learning Morphological Variations of Anatomies from Point Cloud. (arXiv:2305.14486v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14486">http://arxiv.org/abs/2305.14486</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14486] Point2SSM: Learning Morphological Variations of Anatomies from Point Cloud](http://arxiv.org/abs/2305.14486) #robust</code></li>
<li>Summary: <p>We introduce Point2SSM, a novel unsupervised learning approach that can
accurately construct correspondence-based statistical shape models (SSMs) of
anatomy directly from point clouds. SSMs are crucial in clinical research for
analyzing the population-level morphological variation in bones and organs.
However, traditional methods for creating SSMs have limitations that hinder
their widespread adoption, such as the need for noise-free surface meshes or
binary volumes, reliance on assumptions or predefined templates, and
simultaneous optimization of the entire cohort leading to lengthy inference
times given new data. Point2SSM overcomes these barriers by providing a
data-driven solution that infers SSMs directly from raw point clouds, reducing
inference burdens and increasing applicability as point clouds are more easily
acquired. Deep learning on 3D point clouds has seen recent success in
unsupervised representation learning, point-to-point matching, and shape
correspondence; however, their application to constructing SSMs of anatomies is
largely unexplored. In this work, we benchmark state-of-the-art point cloud
deep networks on the task of SSM and demonstrate that they are not robust to
the challenges of anatomical SSM, such as noisy, sparse, or incomplete input
and significantly limited training data. Point2SSM addresses these challenges
via an attention-based module that provides correspondence mappings from
learned point features. We demonstrate that the proposed method significantly
outperforms existing networks in terms of both accurate surface sampling and
correspondence, better capturing population-level statistics.
</p></li>
</ul>

<h3>Title: Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14521">http://arxiv.org/abs/2305.14521</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14521] Eliminating Spurious Correlations from Pre-trained Models via Data Mixing](http://arxiv.org/abs/2305.14521) #robust</code></li>
<li>Summary: <p>Machine learning models pre-trained on large datasets have achieved
remarkable convergence and robustness properties. However, these models often
exploit spurious correlations between certain attributes and labels, which are
prevalent in the majority of examples within specific categories but are not
predictive of these categories in general. The learned spurious correlations
may persist even after fine-tuning on new data, which degrades models'
performance on examples that do not exhibit the spurious correlation. In this
work, we propose a simple and highly effective method to eliminate spurious
correlations from pre-trained models. The key idea of our method is to leverage
a small set of examples with spurious attributes, and balance the spurious
attributes across all classes via data mixing. We theoretically confirm the
effectiveness of our method, and empirically demonstrate its state-of-the-art
performance on various vision and NLP tasks, including eliminating spurious
correlations from pre-trained ResNet50 on Waterbirds and CelebA, adversarially
pre-trained ResNet50 on ImageNet, and BERT pre-trained on CivilComments.
</p></li>
</ul>

<h3>Title: Robust 3D-aware Object Classification via Discriminative Render-and-Compare. (arXiv:2305.14668v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14668">http://arxiv.org/abs/2305.14668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14668] Robust 3D-aware Object Classification via Discriminative Render-and-Compare](http://arxiv.org/abs/2305.14668) #robust</code></li>
<li>Summary: <p>In real-world applications, it is essential to jointly estimate the 3D object
pose and class label of objects, i.e., to perform 3D-aware classification.While
current approaches for either image classification or pose estimation can be
extended to 3D-aware classification, we observe that they are inherently
limited: 1) Their performance is much lower compared to the respective
single-task models, and 2) they are not robust in out-of-distribution (OOD)
scenarios. Our main contribution is a novel architecture for 3D-aware
classification, which builds upon a recent work and performs comparably to
single-task models while being highly robust. In our method, an object category
is represented as a 3D cuboid mesh composed of feature vectors at each mesh
vertex. Using differentiable rendering, we estimate the 3D object pose by
minimizing the reconstruction error between the mesh and the feature
representation of the target image. Object classification is then performed by
comparing the reconstruction losses across object categories. Notably, the
neural texture of the mesh is trained in a discriminative manner to enhance the
classification performance while also avoiding local optima in the
reconstruction loss. Furthermore, we show how our method and feed-forward
neural networks can be combined to scale the render-and-compare approach to
larger numbers of categories. Our experiments on PASCAL3D+, occluded-PASCAL3D+,
and OOD-CV show that our method outperforms all baselines at 3D-aware
classification by a wide margin in terms of performance and robustness.
</p></li>
</ul>

<h3>Title: NegVSR: Augmenting Negatives for Generalized Noise Modeling in Real-World Video Super-Resolution. (arXiv:2305.14669v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14669">http://arxiv.org/abs/2305.14669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14669] NegVSR: Augmenting Negatives for Generalized Noise Modeling in Real-World Video Super-Resolution](http://arxiv.org/abs/2305.14669) #robust</code></li>
<li>Summary: <p>The capability of video super-resolution (VSR) to synthesize high-resolution
(HR) video from ideal datasets has been demonstrated in many works. However,
applying the VSR model to real-world video with unknown and complex degradation
remains a challenging task. First, existing degradation metrics in most VSR
methods are not able to effectively simulate real-world noise and blur. On the
contrary, simple combinations of classical degradation are used for real-world
noise modeling, which led to the VSR model often being violated by
out-of-distribution noise. Second, many SR models focus on noise simulation and
transfer. Nevertheless, the sampled noise is monotonous and limited. To address
the aforementioned problems, we propose a Negatives augmentation strategy for
generalized noise modeling in Video Super-Resolution (NegVSR) task.
Specifically, we first propose sequential noise generation toward real-world
data to extract practical noise sequences. Then, the degeneration domain is
widely expanded by negative augmentation to build up various yet challenging
real-world noise sets. We further propose the augmented negative guidance loss
to learn robust features among augmented negatives effectively. Extensive
experiments on real-world datasets (e.g., VideoLQ and FLIR) show that our
method outperforms state-of-the-art methods with clear margins, especially in
visual quality.
</p></li>
</ul>

<h3>Title: AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness. (arXiv:2305.14700v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14700">http://arxiv.org/abs/2305.14700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14700] AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness](http://arxiv.org/abs/2305.14700) #robust</code></li>
<li>Summary: <p>\emph{Consistent teaching} is an effective paradigm for implementing
knowledge distillation (KD), where both student and teacher models receive
identical inputs, and KD is treated as a function matching task (FunMatch).
However, one limitation of FunMatch is that it does not account for the
transfer of adversarial robustness, a model's resistance to adversarial
attacks. To tackle this problem, we propose a simple but effective strategy
called Adversarial Function Matching (AdvFunMatch), which aims to match
distributions for all data points within the $\ell_p$-norm ball of the training
data, in accordance with consistent teaching. Formulated as a min-max
optimization problem, AdvFunMatch identifies the worst-case instances that
maximizes the KL-divergence between teacher and student model outputs, which we
refer to as "mismatched examples," and then matches the outputs on these
mismatched examples. Our experimental results show that AdvFunMatch effectively
produces student models with both high clean accuracy and robustness.
Furthermore, we reveal that strong data augmentations (\emph{e.g.},
AutoAugment) are beneficial in AdvFunMatch, whereas prior works have found them
less effective in adversarial training. Code is available at
\url{https://gitee.com/zihui998/adv-fun-match}.
</p></li>
</ul>

<h3>Title: AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras. (arXiv:2305.14731v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14731">http://arxiv.org/abs/2305.14731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14731] AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras](http://arxiv.org/abs/2305.14731) #robust</code></li>
<li>Summary: <p>Depth cameras have found applications in diverse fields, such as computer
vision, artificial intelligence, and video gaming. However, the high latency
and low frame rate of existing commodity depth cameras impose limitations on
their applications. We propose a fast and accurate depth map reconstruction
technique to reduce latency and increase the frame rate in depth cameras. Our
approach uses only a commodity depth camera and color camera in a hybrid camera
setup; our prototype is implemented using a Kinect Azure depth camera at 30 fps
and a high-speed RGB iPhone 11 Pro camera captured at 240 fps. The proposed
network, AutoDepthNet, is an encoder-decoder model that captures frames from
the high-speed RGB camera and combines them with previous depth frames to
reconstruct a stream of high frame rate depth maps. On GPU, with a 480 x 270
output resolution, our system achieves an inference time of 8 ms, enabling
real-time use at up to 200 fps with parallel processing. AutoDepthNet can
estimate depth values with an average RMS error of 0.076, a 44.5% improvement
compared to an optical flow-based comparison method. Our method can also
improve depth map quality by estimating depth values for missing and
invalidated pixels. The proposed method can be easily applied to existing depth
cameras and facilitates the use of depth cameras in applications that require
high-speed depth estimation. We also showcase the effectiveness of the
framework in upsampling different sparse datasets e.g. video object
segmentation. As a demonstration of our method, we integrated our framework
into existing body tracking systems and demonstrated the robustness of the
proposed method in such applications.
</p></li>
</ul>

<h3>Title: Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. (arXiv:2305.14450v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14450">http://arxiv.org/abs/2305.14450</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14450] Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors](http://arxiv.org/abs/2305.14450) #robust</code></li>
<li>Summary: <p>ChatGPT has stimulated the research boom in the field of large language
models. In this paper, we assess the capabilities of ChatGPT from four
perspectives including Performance, Evaluation Criteria, Robustness and Error
Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets
with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought
scenarios, and find a huge performance gap between ChatGPT and SOTA results.
Next, we rethink this gap and propose a soft-matching strategy for evaluation
to more accurately reflect ChatGPT's performance. Then, we analyze the
robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely
outputs invalid responses; 2) Irrelevant context and long-tail target types
greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the
subject-object relationships in RE task. Finally, we analyze the errors of
ChatGPT, and find that "unannotated spans" is the most dominant error type.
This raises concerns about the quality of annotated data, and indicates the
possibility of annotating data with ChatGPT. The data and code are released at
Github site.
</p></li>
</ul>

<h3>Title: On Robustness of Finetuned Transformer-based NLP Models. (arXiv:2305.14453v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14453">http://arxiv.org/abs/2305.14453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14453] On Robustness of Finetuned Transformer-based NLP Models](http://arxiv.org/abs/2305.14453) #robust</code></li>
<li>Summary: <p>Transformer-based pretrained models like BERT, GPT-2 and T5 have been
finetuned for a large number of natural language processing (NLP) tasks, and
have been shown to be very effective. However, while finetuning, what changes
across layers in these models with respect to pretrained checkpoints is
under-studied. Further, how robust are these models to perturbations in input
text? Does the robustness vary depending on the NLP task for which the models
have been finetuned? While there exists some work on studying robustness of
BERT finetuned for a few NLP tasks, there is no rigorous study which compares
this robustness across encoder only, decoder only and encoder-decoder models.
</p></li>
</ul>

<p>In this paper, we study the robustness of three language models (BERT, GPT-2
and T5) with eight different text perturbations on the General Language
Understanding Evaluation (GLUE) benchmark. Also, we use two metrics (CKA and
STIR) to quantify changes between pretrained and finetuned language model
representations across layers. GPT-2 representations are more robust than BERT
and T5 across multiple types of input perturbation. Although models exhibit
good robustness broadly, dropping nouns, verbs or changing characters are the
most impactful. Overall, this study provides valuable insights into
perturbation-specific weaknesses of popular Transformer-based models which
should be kept in mind when passing inputs.
</p>

<h3>Title: Are Large Language Models Robust Zero-shot Coreference Resolvers?. (arXiv:2305.14489v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14489">http://arxiv.org/abs/2305.14489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14489] Are Large Language Models Robust Zero-shot Coreference Resolvers?](http://arxiv.org/abs/2305.14489) #robust</code></li>
<li>Summary: <p>Recent progress in domain adaptation for coreference resolution relies on
continued training using annotated data from target domains. At the same time,
pre-trained large language models (LMs) have exhibited strong zero- and
few-shot learning abilities across a wide range of NLP tasks including pronoun
resolution. While this demonstrates evidence of coreference ability, previous
work has mostly studied this ability using simple sentence-level datasets such
as the Winograd Schema Challenge. In this work, we assess the feasibility of
zero-shot learning for coreference resolution by evaluating instruction-tuned
language models on more difficult, linguistically-complex coreference
benchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting
outperforms current unsupervised coreference systems. Further investigations
reveal the robust zero-shot generalization ability of instruction-tuned LMs
across a wide range of domains, languages, and time periods, as well as a
strong reliance on high-quality mention detection systems.
</p></li>
</ul>

<h3>Title: How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation. (arXiv:2305.14533v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14533">http://arxiv.org/abs/2305.14533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14533] How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation](http://arxiv.org/abs/2305.14533) #robust</code></li>
<li>Summary: <p>We release MMSMR, a Massively Multi-System MultiReference dataset to enable
future work on metrics and evaluation for dialog. Automatic metrics for
dialogue evaluation should be robust proxies for human judgments; however, the
verification of robustness is currently far from satisfactory. To quantify the
robustness correlation and understand what is necessary in a test set, we
create and release an 8-reference dialog dataset by extending single-reference
evaluation sets and introduce this new language learning conversation dataset.
We then train 1750 systems and evaluate them on our novel test set and the
DailyDialog dataset. We release the novel test set, and model hyper parameters,
inference outputs, and metric scores for each system on a variety of datasets.
</p></li>
</ul>

<h3>Title: From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding. (arXiv:2305.14571v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14571">http://arxiv.org/abs/2305.14571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14571] From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding](http://arxiv.org/abs/2305.14571) #robust</code></li>
<li>Summary: <p>Current state-of-the-art models for natural language understanding require a
preprocessing step to convert raw text into discrete tokens. This process known
as tokenization relies on a pre-built vocabulary of words or sub-word
morphemes. This fixed vocabulary limits the model's robustness to spelling
errors and its capacity to adapt to new domains. In this work, we introduce a
novel open-vocabulary language model that adopts a hierarchical two-level
approach: one at the word level and another at the sequence level. Concretely,
we design an intra-word module that uses a shallow Transformer architecture to
learn word representations from their characters, and a deep inter-word
Transformer module that contextualizes each word representation by attending to
the entire word sequence. Our model thus directly operates on character
sequences with explicit awareness of word boundaries, but without biased
sub-word or word-level vocabulary. Experiments on various downstream tasks show
that our method outperforms strong baselines. We also demonstrate that our
hierarchical model is robust to textual corruption and domain shift.
</p></li>
</ul>

<h3>Title: DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade. (arXiv:2305.14751v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14751">http://arxiv.org/abs/2305.14751</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14751] DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade](http://arxiv.org/abs/2305.14751) #robust</code></li>
<li>Summary: <p>In the constant updates of the product dialogue systems, we need to retrain
the natural language understanding (NLU) model as new data from the real users
would be merged into the existent data accumulated in the last updates. Within
the newly added data, new intents would emerge and might have semantic
entanglement with the existing intents, e.g. new intents that are semantically
too specific or generic are actually subset or superset of some existing
intents in the semantic space, thus impairing the robustness of the NLU model.
As the first attempt to solve this problem, we setup a new benchmark consisting
of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent
detection with imperfect data in the system update as a multi-label
classification task with positive but unlabeled intents, which asks the models
to recognize all the proper intents, including the ones with semantic
entanglement, in the inference. We also propose comprehensive baseline models
and conduct in-depth analyses for the benchmark, showing that the semantically
entangled intents can be effectively recognized with an automatic workflow.
</p></li>
</ul>

<h3>Title: Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization. (arXiv:2305.14760v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14760">http://arxiv.org/abs/2305.14760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14760] Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization](http://arxiv.org/abs/2305.14760) #robust</code></li>
<li>Summary: <p>Pretrained language models have achieved remarkable success in a variety of
natural language understanding tasks. Nevertheless, finetuning large pretrained
models on downstream tasks is susceptible to overfitting if the training set is
limited, which will lead to diminished performance. In this work, we propose a
dynamic fine-tuning strategy for pretrained language models called Bi-Drop. It
utilizes the gradient information of various sub-models generated by dropout to
update the model parameters selectively. Experiments on the GLUE benchmark show
that Bi-Drop outperforms previous fine-tuning methods by a considerable margin,
and exhibits consistent superiority over vanilla fine-tuning across various
pretrained models. Furthermore, empirical results indicate that Bi-Drop yields
substantial improvements in the multiple task or domain transfer, data
imbalance, and low-resource scenarios, demonstrating superb generalization
ability and robustness.
</p></li>
</ul>

<h3>Title: Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models. (arXiv:2305.14775v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14775">http://arxiv.org/abs/2305.14775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14775] Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models](http://arxiv.org/abs/2305.14775) #robust</code></li>
<li>Summary: <p>While pre-trained language models (PLMs) have shown evidence of acquiring
vast amounts of knowledge, it remains unclear how much of this parametric
knowledge is actually usable in performing downstream tasks. We propose a
systematic framework to measure parametric knowledge utilization in PLMs. Our
framework first extracts knowledge from a PLM's parameters and subsequently
constructs a downstream task around this extracted knowledge. Performance on
this task thus depends exclusively on utilizing the model's possessed
knowledge, avoiding confounding factors like insufficient signal. As an
instantiation, we study factual knowledge of PLMs and measure utilization
across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps -
in acquired vs. utilized knowledge, (2) they show limited robustness in
utilizing knowledge under distribution shifts, and (3) larger models close the
acquired knowledge gap but the utilized knowledge gap remains. Overall, our
study provides insights into PLMs' capabilities beyond their acquired
knowledge.
</p></li>
</ul>

<h3>Title: Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14550">http://arxiv.org/abs/2305.14550</a></li>
<li>Code URL: <a href="https://github.com/prajjwal1/rl_paradigm">https://github.com/prajjwal1/rl_paradigm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14550] Sequence Modeling is a Robust Contender for Offline Reinforcement Learning](http://arxiv.org/abs/2305.14550) #robust</code></li>
<li>Summary: <p>Offline reinforcement learning (RL) allows agents to learn effective,
return-maximizing policies from a static dataset. Three major paradigms for
offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key
open question is: which paradigm is preferred under what conditions? We study
this question empirically by exploring the performance of representative
algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and
Decision Transformer (DT) -- across the commonly used D4RL and Robomimic
benchmarks. We design targeted experiments to understand their behavior
concerning data suboptimality and task complexity. Our key findings are: (1)
Sequence Modeling requires more data than Q-Learning to learn competitive
policies but is more robust; (2) Sequence Modeling is a substantially better
choice than both Q-Learning and Imitation Learning in sparse-reward and
low-quality data settings; and (3) Sequence Modeling and Imitation Learning are
preferable as task horizon increases, or when data is obtained from suboptimal
human demonstrators. Based on the overall strength of Sequence Modeling, we
also investigate architectural choices and scaling trends for DT on Atari and
D4RL and make design recommendations. We find that scaling the amount of data
for DT by 5x gives a 2.5x average score improvement on Atari.
</p></li>
</ul>

<h3>Title: Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14561">http://arxiv.org/abs/2305.14561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14561] Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators](http://arxiv.org/abs/2305.14561) #robust</code></li>
<li>Summary: <p>Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents
a highly promising and efficient approach for accelerating deep neural networks
(DNNs). By concurrently storing network weights and performing matrix
operations within the same crossbar structure, CiM accelerators offer DNN
inference acceleration with minimal area requirements and exceptional energy
efficiency. However, the stochasticity and intrinsic variations of NVM devices
often lead to performance degradation, such as reduced classification accuracy,
compared to expected outcomes. Although several methods have been proposed to
mitigate device variation and enhance robustness, most of them rely on overall
modulation and lack constraints on the training process. Drawing inspiration
from the negative feedback mechanism, we introduce a novel training approach
that uses a multi-exit mechanism as negative feedback to enhance the
performance of DNN models in the presence of device variation. Our negative
feedback training method surpasses state-of-the-art techniques by achieving an
impressive improvement of up to 12.49% in addressing DNN robustness against
device variation.
</p></li>
</ul>

<h3>Title: Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14585">http://arxiv.org/abs/2305.14585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14585] Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models](http://arxiv.org/abs/2305.14585) #robust</code></li>
<li>Summary: <p>One of the ways recent progress has been made on explainable AI has been via
explain-by-example strategies, specifically, through data attribution tasks.
The feature spaces used to attribute decisions to training data, however, have
not been compared against one another as to whether they form a truly
representative surrogate model of the neural network (NN). Here, we demonstrate
the efficacy of surrogate linear feature spaces to neural networks through two
means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK)
is more correlated to the neural network decision functions than embedding
based and influence based alternatives in both computer vision and large
language model architectures; (2) we show that the attributions created from
the normalized pNTK more accurately select perturbed training data in a data
poisoning attribution task than these alternatives. Based on these
observations, we conclude that kernel linear models are effective surrogate
models across multiple classification architectures and that pNTK-based kernels
are the most appropriate surrogate feature space of all kernels studied.
</p></li>
</ul>

<h3>Title: Learning Survival Distribution with Implicit Survival Function. (arXiv:2305.14655v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14655">http://arxiv.org/abs/2305.14655</a></li>
<li>Code URL: <a href="https://github.com/bcai0797/isf">https://github.com/bcai0797/isf</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14655] Learning Survival Distribution with Implicit Survival Function](http://arxiv.org/abs/2305.14655) #robust</code></li>
<li>Summary: <p>Survival analysis aims at modeling the relationship between covariates and
event occurrence with some untracked (censored) samples. In implementation,
existing methods model the survival distribution with strong assumptions or in
a discrete time space for likelihood estimation with censorship, which leads to
weak generalization. In this paper, we propose Implicit Survival Function (ISF)
based on Implicit Neural Representation for survival distribution estimation
without strong assumptions,and employ numerical integration to approximate the
cumulative distribution function for prediction and optimization. Experimental
results show that ISF outperforms the state-of-the-art methods in three public
datasets and has robustness to the hyperparameter controlling estimation
precision.
</p></li>
</ul>

<h3>Title: An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation. (arXiv:2305.14704v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14704">http://arxiv.org/abs/2305.14704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14704] An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation](http://arxiv.org/abs/2305.14704) #robust</code></li>
<li>Summary: <p>To speed up online testing, adaptive traffic experimentation through
multi-armed bandit algorithms is rising as an essential complementary
alternative to the fixed horizon A/B testing. Based on recent research on best
arm identification and statistical inference with adaptively collected data,
this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS,
WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting
batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies
(Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine
traffic allocation. These derived Bayesian sampling algorithms are practically
based on summary batch statistics of a reward metric for pilot experiments,
where one of the combination WB-TTTS in this paper seems to be newly discussed.
The comprehensive evaluation on the four Bayesian sampling algorithms covers
trustworthiness, sensitivity and regret of a testing methodology. Moreover, the
evaluation includes 4 real-world eBay experiments and 40 reproducible synthetic
experiments to reveal the learnings, which covers both stationary and
non-stationary situations. Our evaluation reveals that, (a) There exist false
positives inflation with equivalent best arms, while seldom discussed in
literatures; (b) To control false positives, connections between convergence of
posterior optimal probabilities and neutral posterior reshaping are discovered;
(c) WB-TTTS shows competitive recall, higher precision, and robustness against
non-stationary trend; (d) NB-TS outperforms on minimizing regret trials except
on precision and robustness; (e) WB-TTTS is a promising alternative if regret
of A/B Testing is affordable, otherwise NB-TS is still a powerful choice with
regret consideration for pilot experiments.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Assessment of Anterior Cruciate Ligament Injury Risk Based on Human Key Points Detection Algorithm. (arXiv:2305.14612v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14612">http://arxiv.org/abs/2305.14612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14612] Assessment of Anterior Cruciate Ligament Injury Risk Based on Human Key Points Detection Algorithm](http://arxiv.org/abs/2305.14612) #extraction</code></li>
<li>Summary: <p>This paper aims to detect the potential injury risk of the anterior cruciate
ligament (ACL) by proposing an ACL potential injury risk assessment algorithm
based on key points of the human body detected using computer vision
technology. To obtain the key points data of the human body in each frame,
OpenPose, an open source computer vision algorithm, was employed. The obtained
data underwent preprocessing and were then fed into an ACL potential injury
feature extraction model based on the Landing Error Evaluation System (LESS).
This model extracted several important parameters, including the knee flexion
angle, the trunk flexion on the sagittal plane, trunk flexion angle on the
frontal plane, the ankle knee horizontal distance, and the ankle shoulder
horizontal distance. Each of these features was assigned a threshold interval,
and a segmented evaluation function was utilized to score them accordingly. To
calculate the final score of the participant, the score values were input into
a weighted scoring model designed based on the Analytic Hierarchy Process
(AHP). The AHP based model takes into account the relative importance of each
feature in the overall assessment. The results demonstrate that the proposed
algorithm effectively detects the potential risk of ACL injury. The proposed
algorithm demonstrates its effectiveness in detecting ACL injury risk, offering
valuable insights for injury prevention and intervention strategies in sports
and related fields. Code is available at:
https://github.com/ZiyuGong-proj/Assessment-of-ACL-Injury-Risk-Based-on-Openpose
</p></li>
</ul>

<h3>Title: Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment Triplet Extraction. (arXiv:2305.14434v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14434">http://arxiv.org/abs/2305.14434</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14434] Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment Triplet Extraction](http://arxiv.org/abs/2305.14434) #extraction</code></li>
<li>Summary: <p>Aspect Sentiment Triplet Extraction (ASTE) is a subtask of Aspect-Based
Sentiment Analysis (ABSA) that considers each opinion term, their expressed
sentiment, and the corresponding aspect targets. However, existing methods are
limited to the in-domain setting with two domains. Hence, we propose a
domain-expanded benchmark to address the in-domain, out-of-domain and
cross-domain settings. We support the new benchmark by annotating more than
4000 data samples for two new domains based on hotel and cosmetics reviews. Our
analysis of five existing methods shows that while there is a significant gap
between in-domain and out-of-domain performance, generative methods have a
strong potential for domain generalization. Our datasets, code implementation
and models are available at https://github.com/DAMO-NLP-SG/domain-expanded-aste .
</p></li>
</ul>

<h3>Title: BAND: Biomedical Alert News Dataset. (arXiv:2305.14480v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14480">http://arxiv.org/abs/2305.14480</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14480] BAND: Biomedical Alert News Dataset](http://arxiv.org/abs/2305.14480) #extraction</code></li>
<li>Summary: <p>Infectious disease outbreaks continue to pose a significant threat to human
health and well-being. To improve disease surveillance and understanding of
disease spread, several surveillance systems have been developed to monitor
daily news alerts and social media. However, existing systems lack thorough
epidemiological analysis in relation to corresponding alerts or news, largely
due to the scarcity of well-annotated reports data. To address this gap, we
introduce the Biomedical Alert News Dataset (BAND), which includes 1,508
samples from existing reported news articles, open emails, and alerts, as well
as 30 epidemiology-related questions. These questions necessitate the model's
expert reasoning abilities, thereby offering valuable insights into the
outbreak of the disease. The BAND dataset brings new challenges to the NLP
world, requiring better disguise capability of the content and the ability to
infer important information. We provide several benchmark tasks, including
Named Entity Recognition (NER), Question Answering (QA), and Event Extraction
(EE), to show how existing models are capable of handling these tasks in the
epidemiology domain. To the best of our knowledge, the BAND corpus is the
largest corpus of well-annotated biomedical outbreak alert news with
elaborately designed questions, making it a valuable resource for
epidemiologists and NLP researchers alike.
</p></li>
</ul>

<h3>Title: RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents. (arXiv:2305.14590v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14590">http://arxiv.org/abs/2305.14590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14590] RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents](http://arxiv.org/abs/2305.14590) #extraction</code></li>
<li>Summary: <p>Current research in form understanding predominantly relies on large
pre-trained language models, necessitating extensive data for pre-training.
However, the importance of layout structure (i.e., the spatial relationship
between the entity blocks in the visually rich document) to relation extraction
has been overlooked. In this paper, we propose REgion-Aware Relation Extraction
(RE$^2$) that leverages region-level spatial structure among the entity blocks
to improve their relation prediction. We design an edge-aware graph attention
network to learn the interaction between entities while considering their
spatial relationship defined by their region-level representations. We also
introduce a constraint objective to regularize the model towards consistency
with the inherent constraints of the relation extraction task. Extensive
experiments across various datasets, languages and domains demonstrate the
superiority of our proposed approach.
</p></li>
</ul>

<h3>Title: Iteratively Improving Biomedical Entity Linking and Event Extraction via Hard Expectation-Maximization. (arXiv:2305.14645v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14645">http://arxiv.org/abs/2305.14645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14645] Iteratively Improving Biomedical Entity Linking and Event Extraction via Hard Expectation-Maximization](http://arxiv.org/abs/2305.14645) #extraction</code></li>
<li>Summary: <p>Biomedical entity linking and event extraction are two crucial tasks to
support text understanding and retrieval in the biomedical domain. These two
tasks intrinsically benefit each other: entity linking disambiguates the
biomedical concepts by referring to external knowledge bases and the domain
knowledge further provides additional clues to understand and extract the
biological processes, while event extraction identifies a key trigger and
entities involved to describe each biological process which also captures the
structural context to better disambiguate the biomedical entities. However,
previous research typically solves these two tasks separately or in a pipeline,
leading to error propagation. What's more, it's even more challenging to solve
these two tasks together as there is no existing dataset that contains
annotations for both tasks. To solve these challenges, we propose joint
biomedical entity linking and event extraction by regarding the event
structures and entity references in knowledge bases as latent variables and
updating the two task-specific models in a hard Expectation-Maximization (EM)
fashion: (1) predicting the missing variables for each partially annotated
dataset based on the current two task-specific models, and (2) updating the
parameters of each model on the corresponding pseudo completed dataset.
Experimental results on two benchmark datasets: Genia 2011 for event extraction
and BC4GO for entity linking, show that our joint framework significantly
improves the model for each individual task and outperforms the strong
baselines for both tasks. We will make the code and model checkpoints publicly
available once the paper is accepted.
</p></li>
</ul>

<h3>Title: InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction. (arXiv:2305.14659v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14659">http://arxiv.org/abs/2305.14659</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14659] InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration in Improving the Performance of Information Extraction](http://arxiv.org/abs/2305.14659) #extraction</code></li>
<li>Summary: <p>Learning template based information extraction from documents is a crucial
yet difficult task. Prior template-based IE approaches assume foreknowledge of
the domain templates; however, real-world IE do not have pre-defined schemas
and it is a figure-out-as you go phenomena. To quickly bootstrap templates in a
real-world setting, we need to induce template slots from documents with zero
or minimal supervision. Since the purpose of question answering intersect with
the goal of information extraction, we use automatic question generation to
induce template slots from the documents and investigate how a tiny amount of a
proxy human-supervision on-the-fly (termed as InteractiveIE) can further boost
the performance. Extensive experiments on biomedical and legal documents, where
obtaining training data is expensive, reveal encouraging trends of performance
improvement using InteractiveIE over AI-only baseline.
</p></li>
</ul>

<h3>Title: Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction. (arXiv:2305.14660v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14660">http://arxiv.org/abs/2305.14660</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14660] Complex Mathematical Symbol Definition Structures: A Dataset and Model for Coordination Resolution in Definition Extraction](http://arxiv.org/abs/2305.14660) #extraction</code></li>
<li>Summary: <p>Mathematical symbol definition extraction is important for improving
scholarly reading interfaces and scholarly information extraction (IE).
However, the task poses several challenges: math symbols are difficult to
process as they are not composed of natural language morphemes; and scholarly
papers often contain sentences that require resolving complex coordinate
structures. We present SymDef, an English language dataset of 5,927 sentences
from full-text scientific papers where each sentence is annotated with all
mathematical symbols linked with their corresponding definitions. This dataset
focuses specifically on complex coordination structures such as "respectively"
constructions, which often contain overlapping definition spans. We also
introduce a new definition extraction method that masks mathematical symbols,
creates a copy of each sentence for each symbol, specifies a target symbol, and
predicts its corresponding definition spans using slot filling. Our experiments
show that our definition extraction model significantly outperforms RoBERTa and
other strong IE baseline systems by 10.9 points with a macro F1 score of 84.82.
With our dataset and model, we can detect complex definitions in scholarly
documents to make scientific writing more readable.
</p></li>
</ul>

<h3>Title: A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14695">http://arxiv.org/abs/2305.14695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14695] A Causal View of Entity Bias in (Large) Language Models](http://arxiv.org/abs/2305.14695) #extraction</code></li>
<li>Summary: <p>Entity bias widely affects pretrained (large) language models, causing them
to excessively rely on (biased) parametric knowledge to make unfaithful
predictions. Although causality-inspired methods have shown great potential to
mitigate entity bias, it is hard to precisely estimate the parameters of
underlying causal models in practice. The rise of black-box LLMs also makes the
situation even worse, because of their inaccessible parameters and uncalibrated
logits. To address these problems, we propose a specific structured causal
model (SCM) whose parameters are comparatively easier to estimate. Building
upon this SCM, we propose causal intervention techniques to mitigate entity
bias for both white-box and black-box settings. The proposed causal
intervention perturbs the original entity with neighboring entities. This
intervention reduces specific biasing information pertaining to the original
entity while still preserving sufficient common predictive information from
similar entities. When evaluated on the relation extraction task, our
training-time intervention significantly improves the F1 score of RoBERTa by
5.7 points on EntRED, in which spurious shortcuts between entities and labels
are removed. Meanwhile, our in-context intervention effectively reduces the
knowledge conflicts between parametric knowledge and contextual knowledge in
GPT-3.5 and improves the F1 score by 9.14 points on a challenging test set
derived from Re-TACRED.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14711">http://arxiv.org/abs/2305.14711</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14711] Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning](http://arxiv.org/abs/2305.14711) #fair</code></li>
<li>Summary: <p>Pretrained model-based evaluation metrics have demonstrated strong
performance with high correlations with human judgments in various natural
language generation tasks such as image captioning. Despite the impressive
results, their impact on fairness is under-explored -- it is widely
acknowledged that pretrained models can encode societal biases, and utilizing
them for evaluation purposes may inadvertently manifest and potentially amplify
biases. In this paper, we conduct a systematic study in gender biases of
model-based evaluation metrics with a focus on image captioning tasks.
Specifically, we first identify and quantify gender biases in different
evaluation metrics regarding profession, activity, and object concepts. Then,
we demonstrate the negative consequences of using these biased metrics, such as
favoring biased generation models in deployment and propagating the biases to
generation models through reinforcement learning. We also present a simple but
effective alternative to reduce gender biases by combining n-gram
matching-based and pretrained model-based evaluation metrics.
</p></li>
</ul>

<h3>Title: FITNESS: A Causal De-correlation Approach for Mitigating Bias in Machine Learning Software. (arXiv:2305.14396v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14396">http://arxiv.org/abs/2305.14396</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14396] FITNESS: A Causal De-correlation Approach for Mitigating Bias in Machine Learning Software](http://arxiv.org/abs/2305.14396) #fair</code></li>
<li>Summary: <p>Software built on top of machine learning algorithms is becoming increasingly
prevalent in a variety of fields, including college admissions, healthcare,
insurance, and justice. The effectiveness and efficiency of these systems
heavily depend on the quality of the training datasets. Biased datasets can
lead to unfair and potentially harmful outcomes, particularly in such critical
decision-making systems where the allocation of resources may be affected. This
can exacerbate discrimination against certain groups and cause significant
social disruption. To mitigate such unfairness, a series of bias-mitigating
methods are proposed. Generally, these studies improve the fairness of the
trained models to a certain degree but with the expense of sacrificing the
model performance. In this paper, we propose FITNESS, a bias mitigation
approach via de-correlating the causal effects between sensitive features
(e.g., the sex) and the label. Our key idea is that by de-correlating such
effects from a causality perspective, the model would avoid making predictions
based on sensitive features and thus fairness could be improved. Furthermore,
FITNESS leverages multi-objective optimization to achieve a better
performance-fairness trade-off. To evaluate the effectiveness, we compare
FITNESS with 7 state-of-the-art methods in 8 benchmark tasks by multiple
metrics. Results show that FITNESS can outperform the state-of-the-art methods
on bias mitigation while preserve the model's performance: it improved the
model's fairness under all the scenarios while decreased the model's
performance under only 26.67% of the scenarios. Additionally, FITNESS surpasses
the Fairea Baseline in 96.72% cases, outperforming all methods we compared.
</p></li>
</ul>

<h3>Title: Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces. (arXiv:2305.14516v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14516">http://arxiv.org/abs/2305.14516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14516] Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces](http://arxiv.org/abs/2305.14516) #fair</code></li>
<li>Summary: <p>Benchmarking and co-design are essential for driving optimizations and
innovation around ML models, ML software, and next-generation hardware. Full
workload benchmarks, e.g. MLPerf, play an essential role in enabling fair
comparison across different software and hardware stacks especially once
systems are fully designed and deployed. However, the pace of AI innovation
demands a more agile methodology to benchmark creation and usage by simulators
and emulators for future system co-design. We propose Chakra, an open graph
schema for standardizing workload specification capturing key operations and
dependencies, also known as Execution Trace (ET). In addition, we propose a
complementary set of tools/capabilities to enable collection, generation, and
adoption of Chakra ETs by a wide range of simulators, emulators, and
benchmarks. For instance, we use generative AI models to learn latent
statistical properties across thousands of Chakra ETs and use these models to
synthesize Chakra ETs. These synthetic ETs can obfuscate key proprietary
information and also target future what-if scenarios. As an example, we
demonstrate an end-to-end proof-of-concept that converts PyTorch ETs to Chakra
ETs and uses this to drive an open-source training system simulator
(ASTRA-sim). Our end-goal is to build a vibrant industry-wide ecosystem of
agile benchmarks and tools to drive future AI system co-design.
</p></li>
</ul>

<h3>Title: Interpretation of Time-Series Deep Models: A Survey. (arXiv:2305.14582v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14582">http://arxiv.org/abs/2305.14582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14582] Interpretation of Time-Series Deep Models: A Survey](http://arxiv.org/abs/2305.14582) #fair</code></li>
<li>Summary: <p>Deep learning models developed for time-series associated tasks have become
more widely researched nowadays. However, due to the unintuitive nature of
time-series data, the interpretability problem -- where we understand what is
under the hood of these models -- becomes crucial. The advancement of similar
studies in computer vision has given rise to many post-hoc methods, which can
also shed light on how to explain time-series models. In this paper, we present
a wide range of post-hoc interpretation methods for time-series models based on
backpropagation, perturbation, and approximation. We also want to bring focus
onto inherently interpretable models, a novel category of interpretation where
human-understandable information is designed within the models. Furthermore, we
introduce some common evaluation metrics used for the explanations, and propose
several directions of future researches on the time-series interpretability
problem. As a highlight, our work summarizes not only the well-established
interpretation methods, but also a handful of fairly recent and under-developed
techniques, which we hope to capture their essence and spark future endeavours
to innovate and improvise.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations. (arXiv:2305.14599v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14599">http://arxiv.org/abs/2305.14599</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14599] Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations](http://arxiv.org/abs/2305.14599) #interpretability</code></li>
<li>Summary: <p>Traditional sentence embedding models encode sentences into vector
representations to capture useful properties such as the semantic similarity
between sentences. However, in addition to similarity, sentence semantics can
also be interpreted via compositional operations such as sentence fusion or
difference. It is unclear whether the compositional semantics of sentences can
be directly reflected as compositional operations in the embedding space. To
more effectively bridge the continuous embedding and discrete text spaces, we
explore the plausibility of incorporating various compositional properties into
the sentence embedding space that allows us to interpret embedding
transformations as compositional sentence operations. We propose InterSent, an
end-to-end framework for learning interpretable sentence embeddings that
supports compositional sentence operations in the embedding space. Our method
optimizes operator networks and a bottleneck encoder-decoder model to produce
meaningful and interpretable sentence embeddings. Experimental results
demonstrate that our method significantly improves the interpretability of
sentence embeddings on four textual generation tasks over existing approaches
while maintaining strong performance on traditional semantic similarity tasks.
</p></li>
</ul>

<h3>Title: Mixture of Prompt Experts for Generalizable and Interpretable Question Answering. (arXiv:2305.14628v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14628">http://arxiv.org/abs/2305.14628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14628] Mixture of Prompt Experts for Generalizable and Interpretable Question Answering](http://arxiv.org/abs/2305.14628) #interpretability</code></li>
<li>Summary: <p>One of the ultimate quests of question answering (QA) is to deploy a system
that can answer any type of question from the users, and refrain from answering
when it does not know the answer. While recent advancements in scaling large
language models (LLMs) brought significant improvements on various QA datasets,
it remains difficult for a single model to generalize across question types
that require distinct reasoning abilities. In this paper, we first provide
empirical evidence that state-of-the-art LLMs such as Codex suffer from poor
generalizability on question types beyond those seen in the prompt. To address
this, we propose a Mixture-of-Prompt-Experts (MOPE) system that ensembles
multiple specialized LLMs. We first implement each specialized model based on
the same backbone model (Codex) but with prompts optimized for different
reasoning categories including factual, multihop, mathematical, and commonsense
reasoning. By strategically selecting the best specialized model for each given
question, our MOPE system significantly outperforms any single specialized
model on a collection of 12 QA datasets from four reasoning types. Moreover,
the attribution and agreement among specialized expert models offer greater
interpretability, allowing for better selective question answering. Our human
study further confirms that presenting the expert predictions and answer
selection process helps annotators more accurately decide when to trust the
system's output. We release all code and data to facilitate future work.
</p></li>
</ul>

<h3>Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations. (arXiv:2305.14728v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14728">http://arxiv.org/abs/2305.14728</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14728] SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations](http://arxiv.org/abs/2305.14728) #interpretability</code></li>
<li>Summary: <p>Although deep language representations have become the dominant form of
language featurization in recent years, in many settings it is important to
understand a model's decision-making process. This necessitates not only an
interpretable model but also interpretable features. In particular, language
must be featurized in a way that is interpretable while still characterizing
the original text well. We present SenteCon, a method for introducing human
interpretability in deep language representations. Given a passage of text,
SenteCon encodes the text as a layer of interpretable categories in which each
dimension corresponds to the relevance of a specific category. Our empirical
evaluations indicate that encoding language with SenteCon provides high-level
interpretability at little to no cost to predictive performance on downstream
tasks. Moreover, we find that SenteCon outperforms existing interpretable
language representations with respect to both its downstream performance and
its agreement with human characterizations of the text.
</p></li>
</ul>

<h3>Title: Human-Centered Metrics for Dialog System Evaluation. (arXiv:2305.14757v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14757">http://arxiv.org/abs/2305.14757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14757] Human-Centered Metrics for Dialog System Evaluation](http://arxiv.org/abs/2305.14757) #interpretability</code></li>
<li>Summary: <p>We present metrics for evaluating dialog systems through a
psychologically-grounded "human" lens: conversational agents express a
diversity of both states (short-term factors like emotions) and traits
(longer-term factors like personality) just as people do. These interpretable
metrics consist of five measures from established psychology constructs that
can be applied both across dialogs and on turns within dialogs: emotional
entropy, linguistic style and emotion matching, as well as agreeableness and
empathy. We compare these human metrics against 6 state-of-the-art automatic
metrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We
also introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which
consists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We
demonstrate the proposed human metrics offer novel information, are
uncorrelated with automatic metrics, and lead to increased accuracy beyond
existing automatic metrics for predicting crowd-sourced dialog judgements. The
interpretability and unique signal of our proposed human-centered framework
make it a valuable tool for evaluating and improving dialog systems.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering. (arXiv:2305.14682v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14682">http://arxiv.org/abs/2305.14682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14682] TACR: A Table-alignment-based Cell-selection and Reasoning Model for Hybrid Question-Answering](http://arxiv.org/abs/2305.14682) #explainability</code></li>
<li>Summary: <p>Hybrid Question-Answering (HQA), which targets reasoning over tables and
passages linked from table cells, has witnessed significant research in recent
years. A common challenge in HQA and other passage-table QA datasets is that it
is generally unrealistic to iterate over all table rows, columns, and linked
passages to retrieve evidence. Such a challenge made it difficult for previous
studies to show their reasoning ability in retrieving answers. To bridge this
gap, we propose a novel Table-alignment-based Cell-selection and Reasoning
model (TACR) for hybrid text and table QA, evaluated on the HybridQA and
WikiTableQuestions datasets. In evidence retrieval, we design a
table-question-alignment enhanced cell-selection method to retrieve
fine-grained evidence. In answer reasoning, we incorporate a QA module that
treats the row containing selected cells as context. Experimental results over
the HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves
state-of-the-art results on cell selection and outperforms fine-grained
evidence retrieval baselines on HybridQA, while achieving competitive
performance on WTQ. We also conducted a detailed analysis to demonstrate that
being able to align questions to tables in the cell-selection stage can result
in important gains from experiments of over 90\% table row and column selection
accuracy, meanwhile also improving output explainability.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities. (arXiv:2305.14674v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14674">http://arxiv.org/abs/2305.14674</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14674] T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified Visual Modalities](http://arxiv.org/abs/2305.14674) #diffusion</code></li>
<li>Summary: <p>Diffusion Probabilistic Field (DPF) models the distribution of continuous
functions defined over metric spaces. While DPF shows great potential for
unifying data generation of various modalities including images, videos, and 3D
geometry, it does not scale to a higher data resolution. This can be attributed
to the ``scaling property'', where it is difficult for the model to capture
local structures through uniform sampling. To this end, we propose a new model
comprising of a view-wise sampling algorithm to focus on local structure
learning, and incorporating additional guidance, e.g., text description, to
complement the global geometry. The model can be scaled to generate
high-resolution data while unifying multiple modalities. Experimental results
on data generation in various modalities demonstrate the effectiveness of our
model, as well as its potential as a foundation framework for scalable
modality-unified visual content generation.
</p></li>
</ul>

<h3>Title: Optimal Linear Subspace Search: Learning to Construct Fast and High-Quality Schedulers for Diffusion Models. (arXiv:2305.14677v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14677">http://arxiv.org/abs/2305.14677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14677] Optimal Linear Subspace Search: Learning to Construct Fast and High-Quality Schedulers for Diffusion Models](http://arxiv.org/abs/2305.14677) #diffusion</code></li>
<li>Summary: <p>In recent years, diffusion models have become the most popular and powerful
methods in the field of image synthesis, even rivaling human artists in
artistic creativity. However, the key issue currently limiting the application
of diffusion models is its extremely slow generation process. Although several
methods were proposed to speed up the generation process, there still exists a
trade-off between efficiency and quality. In this paper, we first provide a
detailed theoretical and empirical analysis of the generation process of the
diffusion models based on schedulers. We transform the designing problem of
schedulers into the determination of several parameters, and further transform
the accelerated generation process into an expansion process of the linear
subspace. Based on these analyses, we consequently propose a novel method
called Optimal Linear Subspace Search (OLSS), which accelerates the generation
process by searching for the optimal approximation process of the complete
generation process in the linear subspaces spanned by latent variables. OLSS is
able to generate high-quality images with a very small number of steps. To
demonstrate the effectiveness of our method, we conduct extensive comparative
experiments on open-source diffusion models. Experimental results show that
with a given number of steps, OLSS can significantly improve the quality of
generated images. Using an NVIDIA A100 GPU, we make it possible to generate a
high-quality image by Stable Diffusion within only one second without other
optimization techniques.
</p></li>
</ul>

<h3>Title: BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. (arXiv:2305.14720v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14720">http://arxiv.org/abs/2305.14720</a></li>
<li>Code URL: <a href="https://github.com/salesforce/lavis">https://github.com/salesforce/lavis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14720] BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](http://arxiv.org/abs/2305.14720) #diffusion</code></li>
<li>Summary: <p>Subject-driven text-to-image generation models create novel renditions of an
input subject based on text prompts. Existing models suffer from lengthy
fine-tuning and difficulties preserving the subject fidelity. To overcome these
limitations, we introduce BLIP-Diffusion, a new subject-driven image generation
model that supports multimodal control which consumes inputs of subject images
and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion
introduces a new multimodal encoder which is pre-trained to provide subject
representation. We first pre-train the multimodal encoder following BLIP-2 to
produce visual representation aligned with the text. Then we design a subject
representation learning task which enables a diffusion model to leverage such
visual representation and generates new subject renditions. Compared with
previous methods such as DreamBooth, our model enables zero-shot subject-driven
generation, and efficient fine-tuning for customized subject with up to 20x
speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with
existing techniques such as ControlNet and prompt-to-prompt to enable novel
subject-driven generation and editing applications. Code and models will be
released at
https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion. Project
page at https://dxli94.github.io/BLIP-Diffusion-website/.
</p></li>
</ul>

<h3>Title: I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14724">http://arxiv.org/abs/2305.14724</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14724] I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors](http://arxiv.org/abs/2305.14724) #diffusion</code></li>
<li>Summary: <p>Visual metaphors are powerful rhetorical devices used to persuade or
communicate creative ideas through images. Similar to linguistic metaphors,
they convey meaning implicitly through symbolism and juxtaposition of the
symbols. We propose a new task of generating visual metaphors from linguistic
metaphors. This is a challenging task for diffusion-based text-to-image models,
such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning
and compositionality. We propose to solve the task through the collaboration
between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3
(davinci-002) with Chain-of-Thought prompting generates text that represents a
visual elaboration of the linguistic metaphor containing the implicit meaning
and relevant objects, which is then used as input to the diffusion-based
text-to-image models.Using a human-AI collaboration framework, where humans
interact both with the LLM and the top-performing diffusion model, we create a
high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic
metaphors and their associated visual elaborations. Evaluation by professional
illustrators shows the promise of LLM-Diffusion Model collaboration for this
task.To evaluate the utility of our Human-AI collaboration framework and the
quality of our dataset, we perform both an intrinsic human-based evaluation and
an extrinsic evaluation using visual entailment as a downstream task.
</p></li>
</ul>

<h3>Title: ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation. (arXiv:2305.14742v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14742">http://arxiv.org/abs/2305.14742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14742] ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation](http://arxiv.org/abs/2305.14742) #diffusion</code></li>
<li>Summary: <p>Editing real facial images is a crucial task in computer vision with
significant demand in various real-world applications. While GAN-based methods
have showed potential in manipulating images especially when combined with
CLIP, these methods are limited in their ability to reconstruct real images due
to challenging GAN inversion capability. Despite the successful image
reconstruction achieved by diffusion-based methods, there are still challenges
in effectively manipulating fine-gained facial attributes with textual
instructions.To address these issues and facilitate convenient manipulation of
real facial images, we propose a novel approach that conduct text-driven image
editing in the semantic latent space of diffusion model. By aligning the
temporal feature of the diffusion model with the semantic condition at
generative process, we introduce a stable manipulation strategy, which perform
precise zero-shot manipulation effectively. Furthermore, we develop an
interactive system named ChatFace, which combines the zero-shot reasoning
ability of large language models to perform efficient manipulations in
diffusion semantic latent space. This system enables users to perform complex
multi-attribute manipulations through dialogue, opening up new possibilities
for interactive image editing. Extensive experiments confirmed that our
approach outperforms previous methods and enables precise editing of real
facial images, making it a promising candidate for real-world applications.
Project page: https://dongxuyue.github.io/chatface/
</p></li>
</ul>

<h3>Title: Diffusion Models in NLP: A Survey. (arXiv:2305.14671v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14671">http://arxiv.org/abs/2305.14671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14671] Diffusion Models in NLP: A Survey](http://arxiv.org/abs/2305.14671) #diffusion</code></li>
<li>Summary: <p>This survey paper provides a comprehensive review of the use of diffusion
models in natural language processing (NLP). Diffusion models are a class of
mathematical models that aim to capture the diffusion of information or signals
across a network or manifold. In NLP, diffusion models have been used in a
variety of applications, such as natural language generation, sentiment
analysis, topic modeling, and machine translation. This paper discusses the
different formulations of diffusion models used in NLP, their strengths and
limitations, and their applications. We also perform a thorough comparison
between diffusion models and alternative generative models, specifically
highlighting the autoregressive (AR) models, while also examining how diverse
architectures incorporate the Transformer in conjunction with diffusion models.
Compared to AR models, diffusion models have significant advantages for
parallel generation, text interpolation, token-level controls such as syntactic
structures and semantic contents, and robustness. Exploring further
permutations of integrating Transformers into diffusion models would be a
valuable pursuit. Also, the development of multimodal diffusion models and
large-scale diffusion language models with notable capabilities for few-shot
learning would be important directions for the future advance of diffusion
models in NLP.
</p></li>
</ul>

<h3>Title: SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models. (arXiv:2305.14771v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14771">http://arxiv.org/abs/2305.14771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14771] SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models](http://arxiv.org/abs/2305.14771) #diffusion</code></li>
<li>Summary: <p>Diffusion-based language models (LMs) have been shown to be competent
generative models that are easy to control at inference and are a promising
alternative to autoregressive LMs. While autoregressive LMs have benefited
immensely from scaling and instruction-based learning, existing studies on
diffusion LMs have been conducted on a relatively smaller scale. Starting with
a recently proposed diffusion model SSD-LM, in this work we explore methods to
scale it from 0.4B to 13B parameters, proposing several techniques to improve
its training and inference efficiency. We call the new model SSD-2. We further
show that this model can be easily finetuned to follow instructions. Finally,
leveraging diffusion models' capability at inference-time control, we show that
SSD-2 facilitates novel ensembles with 100x smaller models that can be
customized and deployed by individual users. We find that compared to
autoregressive models, the collaboration between diffusion models is more
effective, leading to higher-quality and more relevant model responses due to
their ability to incorporate bi-directional contexts.
</p></li>
</ul>

<h3>Title: On the Generalization of Diffusion Model. (arXiv:2305.14712v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14712">http://arxiv.org/abs/2305.14712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14712] On the Generalization of Diffusion Model](http://arxiv.org/abs/2305.14712) #diffusion</code></li>
<li>Summary: <p>The diffusion probabilistic generative models are widely used to generate
high-quality data. Though they can synthetic data that does not exist in the
training set, the rationale behind such generalization is still unexplored. In
this paper, we formally define the generalization of the generative model,
which is measured by the mutual information between the generated data and the
training set. The definition originates from the intuition that the model which
generates data with less correlation to the training set exhibits better
generalization ability. Meanwhile, we show that for the empirical optimal
diffusion model, the data generated by a deterministic sampler are all highly
related to the training set, thus poor generalization. This result contradicts
the observation of the trained diffusion model's (approximating empirical
optima) extrapolation ability (generating unseen data). To understand this
contradiction, we empirically verify the difference between the sufficiently
trained diffusion model and the empirical optima. We found, though obtained
through sufficient training, there still exists a slight difference between
them, which is critical to making the diffusion model generalizable. Moreover,
we propose another training objective whose empirical optimal solution has no
potential generalization problem. We empirically show that the proposed
training objective returns a similar model to the original one, which further
verifies the generalization ability of the trained diffusion model.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation. (arXiv:2305.14637v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14637">http://arxiv.org/abs/2305.14637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14637] Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation](http://arxiv.org/abs/2305.14637) #transformer</code></li>
<li>Summary: <p>Automated HTML/CSS code generation from screenshots is an important yet
challenging problem with broad applications in website development and design.
In this paper, we present a novel vision-code transformer approach that
leverages an Encoder-Decoder architecture as well as explore actor-critic
fine-tuning as a method for improving upon the baseline. For this purpose, two
image encoders are compared: Vision Transformer (ViT) and Document Image
Transformer (DiT).
</p></li>
</ul>

<p>We propose an end-to-end pipeline that can generate high-quality code
snippets directly from screenshots, streamlining the website creation process
for developers. To train and evaluate our models, we created a synthetic
dataset of 30,000 unique pairs of code and corresponding screenshots.
</p>
<p>We evaluate the performance of our approach using a combination of automated
metrics such as MSE, BLEU, IoU, and a novel htmlBLEU score, where our models
demonstrated strong performance. We establish a strong baseline with the
DiT-GPT2 model and show that actor-critic can be used to improve IoU score from
the baseline of 0.64 to 0.79 and lower MSE from 12.25 to 9.02. We achieved
similar performance as when using larger models, with much lower computational
cost.
</p>

<h3>Title: Quantifying Character Similarity with Vision Transformers. (arXiv:2305.14672v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14672">http://arxiv.org/abs/2305.14672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14672] Quantifying Character Similarity with Vision Transformers](http://arxiv.org/abs/2305.14672) #transformer</code></li>
<li>Summary: <p>Record linkage is a bedrock of quantitative social science, as analyses often
require linking data from multiple, noisy sources. Off-the-shelf string
matching methods are widely used, as they are straightforward and cheap to
implement and scale. Not all character substitutions are equally probable, and
for some settings there are widely used handcrafted lists denoting which string
substitutions are more likely, that improve the accuracy of string matching.
However, such lists do not exist for many settings, skewing research with
linked datasets towards a few high-resource contexts that are not
representative of the diversity of human societies. This study develops an
extensible way to measure character substitution costs for OCR'ed documents, by
employing large-scale self-supervised training of vision transformers (ViT)
with augmented digital fonts. For each language written with the CJK script, we
contrastively learn a metric space where different augmentations of the same
character are represented nearby. In this space, homoglyphic characters - those
with similar appearance such as <code>O'' and</code>0'' - have similar vector
representations. Using the cosine distance between characters' representations
as the substitution cost in an edit distance matching algorithm significantly
improves record linkage compared to other widely used string matching methods,
as OCR errors tend to be homoglyphic in nature. Homoglyphs can plausibly
capture character visual similarity across any script, including low-resource
settings. We illustrate this by creating homoglyph sets for 3,000 year old
ancient Chinese characters, which are highly pictorial. Fascinatingly, a ViT is
able to capture relationships in how different abstract concepts were
conceptualized by ancient societies, that have been noted in the archaeological
literature.
</p></li>
</ul>

<h3>Title: BinaryViT: Towards Efficient and Accurate Binary Vision Transformers. (arXiv:2305.14730v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14730">http://arxiv.org/abs/2305.14730</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14730] BinaryViT: Towards Efficient and Accurate Binary Vision Transformers](http://arxiv.org/abs/2305.14730) #transformer</code></li>
<li>Summary: <p>Vision Transformers (ViTs) have emerged as the fundamental architecture for
most computer vision fields, but the considerable memory and computation costs
hinders their application on resource-limited devices. As one of the most
powerful compression methods, binarization reduces the computation of the
neural network by quantizing the weights and activation values as $\pm$1.
Although existing binarization methods have demonstrated excellent performance
on Convolutional Neural Networks (CNNs), the full binarization of ViTs is still
under-studied and suffering a significant performance drop. In this paper, we
first argue empirically that the severe performance degradation is mainly
caused by the weight oscillation in the binarization training and the
information distortion in the activation of ViTs. Based on these analyses, we
propose $\textbf{BinaryViT}$, an accurate full binarization scheme for ViTs,
which pushes the quantization of ViTs to the limit. Specifically, we propose a
novel gradient regularization scheme (GRS) for driving a bimodal distribution
of the weights to reduce oscillation in binarization training. Moreover, we
design an activation shift module (ASM) to adaptively tune the activation
distribution to reduce the information distortion caused by binarization.
Extensive experiments on ImageNet dataset show that our BinaryViT consistently
surpasses the strong baseline by 2.05% and improve the accuracy of fully
binarized ViTs to a usable level. Furthermore, our method achieves impressive
savings of 16.2$\times$ and 17.7$\times$ in model size and OPs compared to the
full-precision DeiT-S. The codes and models will be released on github.
</p></li>
</ul>

<h3>Title: Dual Path Transformer with Partition Attention. (arXiv:2305.14768v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14768">http://arxiv.org/abs/2305.14768</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14768] Dual Path Transformer with Partition Attention](http://arxiv.org/abs/2305.14768) #transformer</code></li>
<li>Summary: <p>This paper introduces a novel attention mechanism, called dual attention,
which is both efficient and effective. The dual attention mechanism consists of
two parallel components: local attention generated by Convolutional Neural
Networks (CNNs) and long-range attention generated by Vision Transformers
(ViTs). To address the high computational complexity and memory footprint of
vanilla Multi-Head Self-Attention (MHSA), we introduce a novel Multi-Head
Partition-wise Attention (MHPA) mechanism. The partition-wise attention
approach models both intra-partition and inter-partition attention
simultaneously. Building on the dual attention block and partition-wise
attention mechanism, we present a hierarchical vision backbone called
DualFormer. We evaluate the effectiveness of our model on several computer
vision tasks, including image classification on ImageNet, object detection on
COCO, and semantic segmentation on Cityscapes. Specifically, the proposed
DualFormer-XS achieves 81.5\% top-1 accuracy on ImageNet, outperforming the
recent state-of-the-art MPViT-XS by 0.6\% top-1 accuracy with much higher
throughput.
</p></li>
</ul>

<h3>Title: Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14380">http://arxiv.org/abs/2305.14380</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14380] Finding the Pillars of Strength for Multi-Head Attention](http://arxiv.org/abs/2305.14380) #transformer</code></li>
<li>Summary: <p>Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g.,
redundancy and over-parameterization. Specifically, the heads of MHA were
originally designed to attend to information from different representation
subspaces, whereas prior studies found that some attention heads likely learn
similar features and can be pruned without harming performance. Inspired by the
minimum-redundancy feature selection, we assume that focusing on the most
representative and distinctive features with minimum resources can mitigate the
above issues and lead to more effective and efficient MHAs. In particular, we
propose Grouped Head Attention, trained with a self-supervised group constraint
that group attention heads, where each group focuses on an essential but
distinctive feature subset. We additionally propose a Voting-to-Stay procedure
to remove redundant heads, thus achieving a transformer with lighter weights.
Moreover, our method achieves significant performance gains on three
well-established tasks while considerably compressing parameters.
</p></li>
</ul>

<h3>Title: NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14499">http://arxiv.org/abs/2305.14499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14499] NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders](http://arxiv.org/abs/2305.14499) #transformer</code></li>
<li>Summary: <p>Neural document rerankers are extremely effective in terms of accuracy.
However, the best models require dedicated hardware for serving, which is
costly and often not feasible. To avoid this serving-time requirement, we
present a method of capturing up to 86% of the gains of a Transformer
cross-attention model with a lexicalized scoring function that only requires
10-6% of the Transformer's FLOPs per document and can be served using commodity
CPUs. When combined with a BM25 retriever, this approach matches the quality of
a state-of-the art dual encoder retriever, that still requires an accelerator
for query encoding. We introduce NAIL (Non-Autoregressive Indexing with
Language models) as a model architecture that is compatible with recent
encoder-decoder and decoder-only large language models, such as T5, GPT-3 and
PaLM. This model architecture can leverage existing pre-trained checkpoints and
can be fine-tuned for efficiently constructing document representations that do
not require neural processing of queries.
</p></li>
</ul>

<h3>Title: All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations. (arXiv:2305.14555v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14555">http://arxiv.org/abs/2305.14555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14555] All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations](http://arxiv.org/abs/2305.14555) #transformer</code></li>
<li>Summary: <p>Transformer models bring propelling advances in various NLP tasks, thus
inducing lots of interpretability research on the learned representations of
the models. However, we raise a fundamental question regarding the reliability
of the representations. Specifically, we investigate whether transformers learn
essentially isomorphic representation spaces, or those that are sensitive to
the random seeds in their pretraining process. In this work, we formulate the
Bijection Hypothesis, which suggests the use of bijective methods to align
different models' representation spaces. We propose a model based on invertible
neural networks, BERT-INN, to learn the bijection more effectively than other
existing bijective methods such as the canonical correlation analysis (CCA). We
show the advantage of BERT-INN both theoretically and through extensive
experiments, and apply it to align the reproduced BERT embeddings to draw
insights that are meaningful to the interpretability research. Our code is at
https://github.com/twinkle0331/BERT-similarity.
</p></li>
</ul>

<h3>Title: KNN-LM Does Not Improve Open-ended Text Generation. (arXiv:2305.14625v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14625">http://arxiv.org/abs/2305.14625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14625] KNN-LM Does Not Improve Open-ended Text Generation](http://arxiv.org/abs/2305.14625) #transformer</code></li>
<li>Summary: <p>In this paper, we study the generation quality of interpolation-based
retrieval-augmented language models (LMs). These methods, best exemplified by
the KNN-LM, interpolate the LM's predicted distribution of the next word with a
distribution formed from the most relevant retrievals for a given prefix. While
the KNN-LM and related methods yield impressive decreases in perplexity, we
discover that they do not exhibit corresponding improvements in open-ended
generation quality, as measured by both automatic evaluation metrics (e.g.,
MAUVE) and human evaluations. Digging deeper, we find that interpolating with a
retrieval distribution actually increases perplexity compared to a baseline
Transformer LM for the majority of tokens in the WikiText-103 test set, even
though the overall perplexity is lower due to a smaller number of tokens for
which perplexity dramatically decreases after interpolation. However, when
decoding a long sequence at inference time, significant improvements on this
smaller subset of tokens are washed out by slightly worse predictions on most
tokens. Furthermore, we discover that the entropy of the retrieval distribution
increases faster than that of the base LM as the generated sequence becomes
longer, which indicates that retrieval is less reliable when using
model-generated text as queries (i.e., is subject to exposure bias). We hope
that our analysis spurs future work on improved decoding algorithms and
interpolation strategies for retrieval-augmented language models.
</p></li>
</ul>

<h3>Title: Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation. (arXiv:2305.14734v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14734">http://arxiv.org/abs/2305.14734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14734] Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation](http://arxiv.org/abs/2305.14734) #transformer</code></li>
<li>Summary: <p>Grammatical error correction (GEC) is a well-explored problem in English with
many existing models and datasets. However, research on GEC in morphologically
rich languages has been limited due to challenges such as data scarcity and
language complexity. In this paper, we present the first results on Arabic GEC
by using two newly developed Transformer-based pretrained sequence-to-sequence
models. We address the task of multi-class Arabic grammatical error detection
(GED) and present the first results on multi-class Arabic GED. We show that
using GED information as auxiliary input in GEC models improves GEC performance
across three datasets spanning different genres. Moreover, we also investigate
the use of contextual morphological preprocessing in aiding GEC systems. Our
models achieve state-of-the-art results on two Arabic GEC shared tasks datasets
and establish a strong benchmark on a newly created dataset.
</p></li>
</ul>

<h3>Title: Adapting Language Models to Compress Contexts. (arXiv:2305.14788v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14788">http://arxiv.org/abs/2305.14788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14788] Adapting Language Models to Compress Contexts](http://arxiv.org/abs/2305.14788) #transformer</code></li>
<li>Summary: <p>Transformer-based language models (LMs) are powerful and widely-applicable
tools, but their usefulness is constrained by a finite context window and the
expensive computational cost of processing long text documents. We propose to
adapt pre-trained LMs into AutoCompressors. These models are capable of
compressing long contexts into compact summary vectors, which are then
accessible to the model as soft prompts. Summary vectors are trained with an
unsupervised objective, whereby long documents are processed in segments and
summary vectors from all previous segments are used in language modeling. We
fine-tune OPT models on sequences of up to 30,720 tokens and show that
AutoCompressors can utilize long contexts to improve perplexity. We evaluate
AutoCompressors on in-context learning by compressing task demonstrations. We
find that summary vectors are good substitutes for plain-text demonstrations,
increasing accuracy while reducing inference cost. Finally, we explore the
benefits of pre-computing summary vectors for large corpora by applying summary
vectors to retrieval-augmented language modeling. Overall, AutoCompressors
emerge as a simple and inexpensive solution for extending the context window of
LMs while speeding up inference over long contexts.
</p></li>
</ul>

<h3>Title: NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14405">http://arxiv.org/abs/2305.14405</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14405] NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference](http://arxiv.org/abs/2305.14405) #transformer</code></li>
<li>Summary: <p>In this study, we introduce NeuralMatrix, a novel framework that enables the
computation of versatile deep neural networks (DNNs) on a single general matrix
multiplication (GEMM) accelerator. The proposed approach overcomes the
specificity limitations of ASIC-based accelerators while achieving
application-specific acceleration levels compared to general-purpose processors
such as CPUs and GPUs. We address the challenges of mapping both linear and
nonlinear operations in DNN computation to general matrix multiplications and
the impact of using a GEMM accelerator on DNN inference accuracy. Extensive
experiments are conducted on various DNN models from three popular categories
(i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results
demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being
converted to general matrix multiplication, while achieving 113x to 19.44x
improvements in throughput per power compared to CPUs and GPUs.
</p></li>
</ul>

<h3>Title: A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting. (arXiv:2305.14649v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14649">http://arxiv.org/abs/2305.14649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14649] A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting](http://arxiv.org/abs/2305.14649) #transformer</code></li>
<li>Summary: <p>To enhance predicting performance while minimizing computational demands,
this paper introduces a joint time-frequency domain Transformer (JTFT) for
multivariate forecasting. The method exploits the sparsity of time series in
the frequency domain using a small number of learnable frequencies to extract
temporal dependencies effectively. Alongside the frequency domain
representation, a fixed number of the most recent data points are directly
encoded in the time domain, bolstering the learning of local relationships and
mitigating the adverse effects of non-stationarity. JTFT achieves linear
complexity since the length of the internal representation remains independent
of the input sequence length. Additionally, a low-rank attention layer is
proposed to efficiently capture cross-dimensional dependencies and prevent
performance degradation due to the entanglement of temporal and channel-wise
modeling. Experiments conducted on six real-world datasets demonstrate that
JTFT outperforms state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Revenge of MLP in Sequential Recommendation. (arXiv:2305.14675v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14675">http://arxiv.org/abs/2305.14675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14675] Revenge of MLP in Sequential Recommendation](http://arxiv.org/abs/2305.14675) #transformer</code></li>
<li>Summary: <p>Sequential recommendation models sequences of historical user-item
interactive behaviors (or referred as token) to better infer dynamic
preferences. Fueled by the improved neural network architectures such as RNN,
CNN and Transformer, this field has enjoyed rapid performance boost in the past
years. Recent progress on all-MLP models lights on an efficient method with
less intensive computation, token-mixing MLP, to learn the transformation
patterns among historical behaviors. However, due to the inherent
fully-connection design that allows the unrestricted cross-token communication
and ignores the chronological order, we find that directly applying
token-mixing MLP into sequential recommendation leads to subpar performance. In
this paper, we present a purely MLP-based sequential recommendation
architecture TriMLP with a novel \underline{Tri}angular Mixer where the
modified \underline{MLP} endows tokens with ordered interactions. As the
cross-token interaction in MLP is actually matrix multiplication, Triangular
Mixer drops the lower-triangle neurons in the weight matrix and thus blocks the
connections from future tokens, which prevents information leakage and improves
prediction capability under the standard auto-regressive training fashion. To
further model long and short-term preferences on fine-grained level, the mixer
adopts a dual-branch structure based on the delicate MLP described above,
namely global and local mixing, to separately capture the sequential long-range
dependencies and local patterns. Empirical study on 9 different scale datasets
(contain 50K\textasciitilde20M behaviors) of various benchmarks, including
MovieLens, Amazon and Tenrec, demonstrates that TriMLP attains promising and
stable accuracy/efficiency trade-off, i.e., averagely surpasses several
state-of-the-art baselines by 5.32\% and saves 8.44\% inference time cost.
</p></li>
</ul>

<h3>Title: Can Transformers Learn to Solve Problems Recursively?. (arXiv:2305.14699v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14699">http://arxiv.org/abs/2305.14699</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14699] Can Transformers Learn to Solve Problems Recursively?](http://arxiv.org/abs/2305.14699) #transformer</code></li>
<li>Summary: <p>Neural networks have in recent years shown promise for helping software
engineers write programs and even formally verify them. While semantic
information plays a crucial part in these processes, it remains unclear to what
degree popular neural architectures like transformers are capable of modeling
that information. This paper examines the behavior of neural networks learning
algorithms relevant to programs and formal verification proofs through the lens
of mechanistic interpretability, focusing in particular on structural
recursion. Structural recursion is at the heart of tasks on which symbolic
tools currently outperform neural models, like inferring semantic relations
between datatypes and emulating program behavior. We evaluate the ability of
transformer models to learn to emulate the behavior of structurally recursive
functions from input-output examples. Our evaluation includes empirical and
conceptual analyses of the limitations and capabilities of transformer models
in approximating these functions, as well as reconstructions of the ``shortcut"
algorithms the model learns. By reconstructing these algorithms, we are able to
correctly predict 91 percent of failure cases for one of the approximated
functions. Our work provides a new foundation for understanding the behavior of
neural networks that fail to solve the very tasks they are trained for.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Design a Delicious Lunchbox in Style. (arXiv:2305.14522v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14522">http://arxiv.org/abs/2305.14522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14522] Design a Delicious Lunchbox in Style](http://arxiv.org/abs/2305.14522) #generative</code></li>
<li>Summary: <p>We propose a cyclic generative adversarial network with spatial-wise and
channel-wise attention modules for text-to-image synthesis. To accurately
depict and design scenes with multiple occluded objects, we design a
pre-trained ordering recovery model and a generative adversarial network to
predict layout and composite novel box lunch presentations. In the experiments,
we devise the Bento800 dataset to evaluate the performance of the text-to-image
synthesis model and the layout generation &amp; image composition model. This paper
is the continuation of our previous paper works. We also present additional
experiments and qualitative performance comparisons to verify the effectiveness
of our proposed method. Bento800 dataset is available at
https://github.com/Yutong-Zhou-cv/Bento800_Dataset
</p></li>
</ul>

<h3>Title: Towards Early Prediction of Human iPSC Reprogramming Success. (arXiv:2305.14575v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14575">http://arxiv.org/abs/2305.14575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14575] Towards Early Prediction of Human iPSC Reprogramming Success](http://arxiv.org/abs/2305.14575) #generative</code></li>
<li>Summary: <p>This paper presents advancements in automated early-stage prediction of the
success of reprogramming human induced pluripotent stem cells (iPSCs) as a
potential source for regenerative cell therapies.The minuscule success rate of
iPSC-reprogramming of around $ 0.01% $ to $ 0.1% $ makes it labor-intensive,
time-consuming, and exorbitantly expensive to generate a stable iPSC line.
Since that requires culturing of millions of cells and intense biological
scrutiny of multiple clones to identify a single optimal clone. The ability to
reliably predict which cells are likely to establish as an optimal iPSC line at
an early stage of pluripotency would therefore be ground-breaking in rendering
this a practical and cost-effective approach to personalized medicine. Temporal
information about changes in cellular appearance over time is crucial for
predicting its future growth outcomes. In order to generate this data, we first
performed continuous time-lapse imaging of iPSCs in culture using an ultra-high
resolution microscope. We then annotated the locations and identities of cells
in late-stage images where reliable manual identification is possible. Next, we
propagated these labels backwards in time using a semi-automated tracking
system to obtain labels for early stages of growth. Finally, we used this data
to train deep neural networks to perform automatic cell segmentation and
classification. Our code and data are available at
https://github.com/abhineet123/ipsc_prediction.
</p></li>
</ul>

<h3>Title: Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport. (arXiv:2305.14777v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14777">http://arxiv.org/abs/2305.14777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14777] Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport](http://arxiv.org/abs/2305.14777) #generative</code></li>
<li>Summary: <p>Optimal Transport (OT) problem investigates a transport map that bridges two
distributions while minimizing a given cost function. In this regard, OT
between tractable prior distribution and data has been utilized for generative
modeling tasks. However, OT-based methods are susceptible to outliers and face
optimization challenges during training. In this paper, we propose a novel
generative model based on the semi-dual formulation of Unbalanced Optimal
Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution
matching. This approach provides better robustness against outliers, stability
during training, and faster convergence. We validate these properties
empirically through experiments. Moreover, we study the theoretical upper-bound
of divergence between distributions in UOT. Our model outperforms existing
OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80
on CelebA-HQ-256.
</p></li>
</ul>

<h3>Title: CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains. (arXiv:2305.14471v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14471">http://arxiv.org/abs/2305.14471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14471] CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains](http://arxiv.org/abs/2305.14471) #generative</code></li>
<li>Summary: <p>Generative chat models, such as ChatGPT and GPT-4, have revolutionized
natural language generation (NLG) by incorporating instructions and human
feedback to achieve significant performance improvements. However, the lack of
standardized evaluation benchmarks for chat models, particularly for Chinese
and domain-specific models, hinders their assessment and progress. To address
this gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark,
focusing on general and financial domains. The CGCE benchmark encompasses
diverse tasks, including 200 questions in the general domain and 150 specific
professional questions in the financial domain. Manual scoring evaluates
factors such as accuracy, coherence, expression clarity, and completeness. The
CGCE benchmark provides researchers with a standardized framework to assess and
compare Chinese generative chat models, fostering advancements in NLG research.
</p></li>
</ul>

<h3>Title: Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14651">http://arxiv.org/abs/2305.14651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14651] Revisit and Outstrip Entity Alignment: A Perspective of Generative Models](http://arxiv.org/abs/2305.14651) #generative</code></li>
<li>Summary: <p>Recent embedding-based methods have achieved great successes on exploiting
entity alignment from knowledge graph (KG) embeddings of multiple modals. In
this paper, we study embedding-based entity alignment (EEA) from a perspective
of generative models. We show that EEA is a special problem where the main
objective is analogous to that in a typical generative model, based on which we
theoretically prove the effectiveness of the recently developed generative
adversarial network (GAN)-based EEA methods. We then reveal that their
incomplete objective limits the capacity on both entity alignment and entity
synthesis (i.e., generating new entities). We mitigate this problem by
introducing a generative EEA (abbr., GEEA) framework with the proposed mutual
variational autoencoder (M-VAE) as the generative model. M-VAE can convert an
entity from one KG to another and generate new entities from random noise
vectors. We demonstrate the power of GEEA with theoretical analysis and
empirical experiments on both entity alignment and entity synthesis tasks.
</p></li>
</ul>

<h3>Title: A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14383">http://arxiv.org/abs/2305.14383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14383] A Rational Model of Dimension-reduced Human Categorization](http://arxiv.org/abs/2305.14383) #generative</code></li>
<li>Summary: <p>Existing models in cognitive science typically assume human categorization as
graded generalization behavior in a multidimensional psychological space.
However, category representations in these models may suffer from the curse of
dimensionality in a natural setting. People generally rely on a tractable yet
sufficient set of features to understand the complex environment. We propose a
rational model of categorization based on a hierarchical mixture of
probabilistic principal components, that simultaneously learn category
representations and an economical collection of features. The model captures
dimensional biases in human categorization and supports zero-shot learning. We
further exploit a generative process within a low-dimensional latent space to
provide a better account of categorization with high-dimensional stimuli. We
validate the model with simulation and behavioral experiments.
</p></li>
</ul>

<h3>Title: Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling. (arXiv:2305.14452v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14452">http://arxiv.org/abs/2305.14452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14452] Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling](http://arxiv.org/abs/2305.14452) #generative</code></li>
<li>Summary: <p>Climate simulations are essential in guiding our understanding of climate
change and responding to its effects. However, it is computationally expensive
to resolve complex climate processes at high spatial resolution. As one way to
speed up climate simulations, neural networks have been used to downscale
climate variables from fast-running low-resolution simulations, but
high-resolution training data are often unobtainable or scarce, greatly
limiting accuracy. In this work, we propose a downscaling method based on the
Fourier neural operator. It trains with data of a small upsampling factor and
then can zero-shot downscale its input to arbitrary unseen high resolution.
Evaluated both on ERA5 climate model data and on the Navier-Stokes equation
solution data, our downscaling model significantly outperforms state-of-the-art
convolutional and generative adversarial downscaling models, both in standard
single-resolution downscaling and in zero-shot generalization to higher
upsampling factors. Furthermore, we show that our method also outperforms
state-of-the-art data-driven partial differential equation solvers on
Navier-Stokes equations. Overall, our work bridges the gap between simulation
of a physical process and interpolation of low-resolution output, showing that
it is possible to combine both approaches and significantly improve upon each
other.
</p></li>
</ul>

<h3>Title: torchgfn: A PyTorch GFlowNet library. (arXiv:2305.14594v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14594">http://arxiv.org/abs/2305.14594</a></li>
<li>Code URL: <a href="https://github.com/saleml/torchgfn">https://github.com/saleml/torchgfn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14594] torchgfn: A PyTorch GFlowNet library](http://arxiv.org/abs/2305.14594) #generative</code></li>
<li>Summary: <p>The increasing popularity of generative flow networks (GFlowNets or GFNs) is
accompanied with a proliferation of code sources. This hinders the
implementation of new features, such as training losses, that can readily be
compared to existing ones, on a set of common environments. In addition to
slowing down research in the field of GFlowNets, different code bases use
different conventions, that might be confusing for newcomers. <code>torchgfn</code> is a
library built on top of PyTorch, that aims at addressing both problems. It
provides user with a simple API for environments, and useful abstractions for
samplers and losses. Multiple examples are provided, replicating published
results. The code is available in https://github.com/saleml/torchgfn.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Prompting Language-Informed Distribution for Compositional Zero-Shot Learning. (arXiv:2305.14428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14428">http://arxiv.org/abs/2305.14428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14428] Prompting Language-Informed Distribution for Compositional Zero-Shot Learning](http://arxiv.org/abs/2305.14428) #large language model</code></li>
<li>Summary: <p>The compositional zero-shot learning (CZSL) task aims to recognize unseen
compositional visual concepts (i.e., sliced tomatoes), where the models are
learned only from the seen compositions (i.e., sliced potatoes and red
tomatoes). Thanks to the prompt tuning on large pre-trained visual language
models such as CLIP, recent literature shows impressively better CZSL
performance than traditional vision-based methods. However, the key aspects
that impact the generalization to unseen compositions, including the diversity
and informativeness of class context, and the entanglement between visual
primitives (i.e., states and objects), are not properly addressed in existing
CLIP-based CZSL literature. In this paper, we propose a model by prompting the
language-informed distribution, aka., PLID, for the CZSL task. Specifically,
the PLID leverages pre-trained large language models (LLM) to 1) formulate the
language-informed class distribution, and 2) enhance the compositionality of
the softly prompted class embedding. Moreover, a stochastic logit mixup
strategy is proposed to dynamically fuse the decisions from the predictions in
the compositional and the primitive logit space. Orthogonal to the existing
literature of soft, hard, or distributional prompts, our method advocates
prompting the LLM-supported class distribution that leads to a better
compositional zero-shot generalization. Experimental results on MIT-States,
UT-Zappos, and C-GQA datasets show the superior performance of the PLID to the
prior arts. The code and models will be publicly released.
</p></li>
</ul>

<h3>Title: Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation. (arXiv:2305.14386v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14386">http://arxiv.org/abs/2305.14386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14386] Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation](http://arxiv.org/abs/2305.14386) #large language model</code></li>
<li>Summary: <p>In this paper, we present a novel approach for distilling math word problem
solving capabilities from large language models (LLMs) into smaller, more
efficient student models. Our approach is designed to consider the student
model's weaknesses and foster a tailored learning experience by generating
targeted exercises aligned with educational science principles, such as
knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math
tutor and run two steps iteratively: 1) assessing the student model's current
learning status on a GPT-generated exercise book, and 2) improving the student
model by training it with tailored exercise samples generated by GPT-3.
Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and
PaLM) in accuracy across three distinct benchmarks while employing
significantly fewer parameters. Furthermore, we provide a comprehensive
analysis of the various components within our methodology to substantiate their
efficacy.
</p></li>
</ul>

<h3>Title: AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14387">http://arxiv.org/abs/2305.14387</a></li>
<li>Code URL: <a href="https://github.com/tatsu-lab/alpaca_farm">https://github.com/tatsu-lab/alpaca_farm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14387] AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback](http://arxiv.org/abs/2305.14387) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their ability to follow user instructions well. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following process
faces three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 45x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
</p></li>
</ul>

<h3>Title: Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions. (arXiv:2305.14441v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14441">http://arxiv.org/abs/2305.14441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14441] Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions](http://arxiv.org/abs/2305.14441) #large language model</code></li>
<li>Summary: <p>Contrast consistency, the ability of a model to make consistently correct
predictions in the presence of perturbations, is an essential aspect in NLP.
While studied in tasks such as sentiment analysis and reading comprehension, it
remains unexplored in open-domain question answering (OpenQA) due to the
difficulty of collecting perturbed questions that satisfy factuality
requirements. In this work, we collect minimally edited questions as
challenging contrast sets to evaluate OpenQA models. Our collection approach
combines both human annotation and large language model generation. We find
that the widely used dense passage retriever (DPR) performs poorly on our
contrast sets, despite fitting the training set well and performing
competitively on standard test sets. To address this issue, we introduce a
simple and effective query-side contrastive loss with the aid of data
augmentation to improve DPR training. Our experiments on the contrast sets
demonstrate that DPR's contrast consistency is improved without sacrificing its
accuracy on the standard test sets.
</p></li>
</ul>

<h3>Title: Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14456">http://arxiv.org/abs/2305.14456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14456] Having Beer after Prayer? Measuring Cultural Bias in Large Language Models](http://arxiv.org/abs/2305.14456) #large language model</code></li>
<li>Summary: <p>Are language models culturally biased? It is important that language models
conform to the cultural aspects of the communities they serve. However, we show
in this paper that language models suffer from a significant bias towards
Western culture when handling and generating text in Arabic, often preferring,
and producing Western-fitting content as opposed to the relevant Arab content.
We quantify this bias through a likelihood scoring-based metric using naturally
occurring contexts that we collect from online social media. Our experiments
reveal that both Arabic monolingual and multilingual models exhibit bias
towards Western culture in eight different cultural aspects: person names,
food, clothing, location, literature, beverage, religion, and sports. Models
also tend to exhibit more bias when prompted with Arabic sentences that are
more linguistically aligned with English. These findings raise concerns about
the cultural relevance of current language models. Our analyses show that
providing culture-indicating tokens or culturally-relevant demonstrations to
the model can help in debiasing.
</p></li>
</ul>

<h3>Title: Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA. (arXiv:2305.14458v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14458">http://arxiv.org/abs/2305.14458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14458] Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA](http://arxiv.org/abs/2305.14458) #large language model</code></li>
<li>Summary: <p>Large language models (e.g., GPT-3.5) are uniquely capable of producing
highly rated text simplification, yet current human evaluation methods fail to
provide a clear understanding of systems' specific strengths and weaknesses. To
address this limitation, we introduce SALSA, an edit-based human annotation
framework that enables holistic and fine-grained text simplification
evaluation. We develop twenty one linguistically grounded edit types, covering
the full spectrum of success and failure across dimensions of conceptual,
syntactic and lexical simplicity. Using SALSA, we collect 12K edit annotations
on 700 simplifications, revealing discrepancies in the distribution of
transformation approaches performed by fine-tuned models, few-shot LLMs and
humans, and finding GPT-3.5 performs more quality edits than humans, but still
exhibits frequent errors. Using our fine-grained annotations, we develop
LENS-SALSA, a reference-free automatic simplification metric, trained to
predict sentence- and word-level quality simultaneously. Additionally, we
introduce word-level quality estimation for simplification and report promising
baseline results. Our training material, annotation toolkit, and data are
released at <a href="http://salsa-eval.com.">this http URL</a>
</p></li>
</ul>

<h3>Title: Language Model Self-improvement by Reinforcement Learning Contemplation. (arXiv:2305.14483v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14483">http://arxiv.org/abs/2305.14483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14483] Language Model Self-improvement by Reinforcement Learning Contemplation](http://arxiv.org/abs/2305.14483) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have exhibited remarkable performance across
various natural language processing (NLP) tasks. However, fine-tuning these
models often necessitates substantial supervision, which can be expensive and
time-consuming to obtain. This paper introduces a novel unsupervised method
called LanguageModel Self-Improvement by Reinforcement Learning Contemplation
(SIRLC) that improves LLMs without reliance on external labels. Our approach is
grounded in the observation that it is simpler for language models to assess
text quality than to generate text. Building on this insight, SIRLC assigns
LLMs dual roles as both student and teacher. As a student, the LLM generates
answers to unlabeled questions, while as a teacher, it evaluates the generated
text and assigns scores accordingly. The model parameters are updated using
reinforcement learning to maximize the evaluation score. We demonstrate that
SIRLC can be applied to various NLP tasks, such as reasoning problems, text
generation, and machine translation. Our experiments show that SIRLC
effectively improves LLM performance without external supervision, resulting in
a 5.6% increase in answering accuracy for reasoning tasks and a rise in
BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be
applied to models of different sizes, showcasing its broad applicability.
</p></li>
</ul>

<h3>Title: Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14497">http://arxiv.org/abs/2305.14497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14497] Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement](http://arxiv.org/abs/2305.14497) #large language model</code></li>
<li>Summary: <p>Prompting methods such as Chain-of-Thought (CoT) have shed new light on
enhancing the reasoning capabilities of large language models, and researchers
have extensively explored the generation process of rationales and answers.
However, they have overlooked the potential challenges posed by the poor
quality of reasoning problems, which may influence the reasoning performance
significantly. In this work, we propose Self-Polish (SP), a novel method that
facilitates the model's problem-solving process by prompting them to
progressively refine the given problems to be more comprehensible and solvable.
Specifically, the method teaches models to eliminate irrelevant information,
rearrange the logic structure and organize local conditions into new ones
parallelly. SP is orthogonal to all other prompting methods, making it
convenient to integrate with state-of-the-art techniques for further
improvement. We conduct thorough experiments on five benchmarks to illustrate
the effectiveness of the proposed method. For example, with Text-davinci-003,
our method boosts the performance of standard few-shot prompting by $8.0\%$ on
GSM8K and $17.8\%$ on MultiArith; it also improves the performance of CoT by
$6.0\%$ on GSM8K and $6.0\%$ on MathQA, respectively. Furthermore, our method
also showcases impressive performance on robustness evaluation.
</p></li>
</ul>

<h3>Title: RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14502">http://arxiv.org/abs/2305.14502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14502] RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning](http://arxiv.org/abs/2305.14502) #large language model</code></li>
<li>Summary: <p>Many recent developments in large language models focus on prompting them to
perform specific tasks. One effective prompting method is in-context learning,
where the model performs a (possibly new) generation/prediction task given one
(or more) examples. Past work has shown that the choice of examples can make a
large impact on task performance. However, finding good examples is not
straightforward since the definition of a representative group of examples can
vary greatly depending on the task. While there are many existing methods for
selecting in-context examples, they generally score examples independently,
ignoring the dependency between them and the order in which they are provided
to the large language model. In this work, we propose Retrieval for In-Context
Learning (RetICL), a learnable method for modeling and optimally selecting
examples sequentially for in-context learning. We frame the problem of
sequential example selection as a Markov decision process, design an example
retriever model using an LSTM, and train it using proximal policy optimization
(PPO). We validate RetICL on math problem solving datasets and show that it
outperforms both heuristic and learnable baselines, and achieves
state-of-the-art accuracy on the TabMWP dataset. We also use case studies to
show that RetICL implicitly learns representations of math problem solving
strategies.
</p></li>
</ul>

<h3>Title: Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models. (arXiv:2305.14507v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14507">http://arxiv.org/abs/2305.14507</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14507] Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models](http://arxiv.org/abs/2305.14507) #large language model</code></li>
<li>Summary: <p>We explore whether Large Language Models (LLMs) are capable of logical
reasoning with distorted facts, which we call Deduction under Perturbed
Evidence (DUPE). DUPE presents a unique challenge to LLMs since they typically
rely on their parameters, which encode mostly accurate information, to reason
and make inferences. However, in DUPE, LLMs must reason over manipulated or
falsified evidence present in their prompts, which can result in false
conclusions that are valid only under the manipulated evidence. Our goal with
DUPE is to determine whether LLMs can arrive at these false conclusions and
identify whether the dominant factor influencing the deduction process is the
encoded data in the parameters or the manipulated evidence in the prompts. To
evaluate the DUPE capabilities of LLMs, we create a DUPEd version of the
StrategyQA dataset, where facts are manipulated to reverse the answer to the
question. Our findings show that even the most advanced GPT models struggle to
reason on manipulated facts - showcasing poor DUPE skills - with accuracy
dropping by 45% compared to the original dataset. We also investigate prompt
settings inspired from student simulation models, which mitigate the accuracy
drop to some extent. Our findings have practical implications for understanding
the performance of LLMs in real-world applications such as student simulation
models that involve reasoning over inaccurate information.
</p></li>
</ul>

<h3>Title: Sources of Hallucination by Large Language Models on Inference Tasks. (arXiv:2305.14552v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14552">http://arxiv.org/abs/2305.14552</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14552] Sources of Hallucination by Large Language Models on Inference Tasks](http://arxiv.org/abs/2305.14552) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) are claimed to be capable of Natural Language
Inference (NLI), necessary for applied tasks like question answering and
summarization, yet this capability is under-explored. We present a series of
behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which
probe their behavior using controlled experiments. We establish two factors
which predict much of their performance, and propose that these are major
sources of hallucination in generative LLM. First, the most influential factor
is memorization of the training data. We show that models falsely label NLI
test samples as entailing when the hypothesis is attested in the training text,
regardless of the premise. We further show that named entity IDs are used as
"indices" to access the memorized data. Second, we show that LLMs exploit a
further corpus-based heuristic using the relative frequencies of words. We show
that LLMs score significantly worse on NLI test samples which do not conform to
these factors than those which do; we also discuss a tension between the two
factors, and a performance trade-off.
</p></li>
</ul>

<h3>Title: PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents. (arXiv:2305.14564v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14564">http://arxiv.org/abs/2305.14564</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14564] PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents](http://arxiv.org/abs/2305.14564) #large language model</code></li>
<li>Summary: <p>Strategies such as chain-of-thought prompting improve the performance of
large language models (LLMs) on complex reasoning tasks by decomposing input
examples into intermediate steps. However, it remains unclear how to apply such
methods to reason over long input documents, in which both the decomposition
and the output of each intermediate step are non-trivial to obtain. In this
work, we propose PEARL, a prompting framework to improve reasoning over long
documents, which consists of three stages: action mining, plan formulation, and
plan execution. More specifically, given a question about a long document,
PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,
FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain
the answer. Each stage of PEARL is implemented via zero-shot or few-shot
prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate
PEARL on a challenging subset of the QuALITY dataset, which contains questions
that require complex reasoning over long narrative texts. PEARL outperforms
zero-shot and chain-of-thought prompting on this dataset, and ablation
experiments show that each stage of PEARL is critical to its performance.
Overall, PEARL is a first step towards leveraging LLMs to reason over long
documents.
</p></li>
</ul>

<h3>Title: ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers. (arXiv:2305.14591v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14591">http://arxiv.org/abs/2305.14591</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14591] ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers](http://arxiv.org/abs/2305.14591) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) excel at implementing code from functionality
descriptions, but struggle with algorithmic problems that require not only
implementation but also identification of the suitable algorithm. Moreover,
LLM-generated programs lack guaranteed correctness and require human
verification. To address these challenges, we propose ALGO, a framework that
synthesizes Algorithmic programs with LLM-Generated Oracles to guide the
creation and verify their correctness. ALGO first generates a probably correct
but possibly slow reference oracle by prompting an LLM to exhaustively
enumerate all the combinations of relevant variables. This oracle is then
utilized to guide an arbitrary search strategy in exploring the algorithm space
and to verify the algorithms synthesized. Our study shows that the
LLM-generated oracles are correct for 88% of the cases. With the oracles as
verifiers, ALGO can be integrated with any existing code generation model in a
model-agnostic manner to enhance its performance. Experiments show that when
equipped with ALGO, we achieve an 8x better one-submission pass rate over the
Codex model and a 2.6x better one-submission pass rate over CodeT, the current
state-of-the-art model on CodeContests. We can also get 1.3x better pass rate
over the ChatGPT Code Interpreter on unseen problems.
</p></li>
</ul>

<h3>Title: Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14596">http://arxiv.org/abs/2305.14596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14596] Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy](http://arxiv.org/abs/2305.14596) #large language model</code></li>
<li>Summary: <p>When large language models (LMs) are applied in zero- or few-shot settings to
discriminative tasks such as multiple-choice questions, their attentiveness
(i.e., probability mass) is spread across many vocabulary tokens that are not
valid choices. Such a spread across multiple surface forms with identical
meaning is thought to cause an underestimation of a model's true performance,
referred to as the "surface form competition" (SFC) hypothesis. This has
motivated the introduction of various probability normalization methods.
However, many core questions remain unanswered. How do we measure SFC or
attentiveness? Are there direct ways of increasing attentiveness on valid
choices? Does increasing attentiveness always improve task accuracy? We propose
a mathematical formalism for studying this phenomenon, provide a metric for
quantifying attentiveness, and identify a simple method for increasing it --
namely, in-context learning with even just one example containing answer
choices. The formalism allows us to quantify SFC and bound its impact. Our
experiments on three diverse datasets and six LMs reveal several surprising
findings. For example, encouraging models to generate a valid answer choice
can, in fact, be detrimental to task performance for some LMs, and prior
probability normalization methods are less effective (sometimes even
detrimental) to instruction-tuned LMs. We conclude with practical insights for
effectively using prompted LMs for multiple-choice tasks.
</p></li>
</ul>

<h3>Title: Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models. (arXiv:2305.14623v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14623">http://arxiv.org/abs/2305.14623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14623] Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models](http://arxiv.org/abs/2305.14623) #large language model</code></li>
<li>Summary: <p>Fact-checking is an essential task in NLP that is commonly utilized for
validating the factual accuracy of claims. Prior work has mainly focused on
fine-tuning pre-trained languages models on specific datasets, which can be
computationally intensive and time-consuming. With the rapid development of
large language models (LLMs), such as ChatGPT and GPT-3, researchers are now
exploring their in-context learning capabilities for a wide range of tasks. In
this paper, we aim to assess the capacity of LLMs for fact-checking by
introducing Self-Checker, a framework comprising a set of plug-and-play modules
that facilitate fact-checking by purely prompting LLMs in an almost zero-shot
setting. This framework provides a fast and efficient way to construct
fact-checking systems in low-resource environments. Empirical results
demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.
However, there is still significant room for improvement compared to SOTA
fine-tuned models, which suggests that LLM adoption could be a promising
approach for future fact-checking research.
</p></li>
</ul>

<h3>Title: Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14627">http://arxiv.org/abs/2305.14627</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14627] Enabling Large Language Models to Generate Text with Citations](http://arxiv.org/abs/2305.14627) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have emerged as a widely-used tool for
information seeking, but their generated outputs are prone to hallucination. In
this work, we aim to enable LLMs to generate text with citations, improving
their factual correctness and verifiability. Existing work mainly relies on
commercial search engines and human evaluation, making it challenging to
reproduce and compare with different modeling approaches. We propose ALCE, the
first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a
diverse set of questions and retrieval corpora and requires building end-to-end
systems to retrieve supporting evidence and generate answers with citations. We
build automatic metrics along three dimensions -- fluency, correctness, and
citation quality -- and demonstrate their strong correlation with human
judgements. Our experiments with state-of-the-art LLMs and novel prompting
strategies show that current systems have considerable room for improvements --
for example, on the ELI5 dataset, even the best model has 49% of its
generations lacking complete citation support. Our extensive analyses further
highlight promising future directions, including developing better retrievers,
advancing long-context LLMs, and improving the ability to synthesize
information from multiple sources.
</p></li>
</ul>

<h3>Title: Testing Causal Models of Word Meaning in GPT-3 and -4. (arXiv:2305.14630v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14630">http://arxiv.org/abs/2305.14630</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14630] Testing Causal Models of Word Meaning in GPT-3 and -4](http://arxiv.org/abs/2305.14630) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have driven extraordinary improvements in NLP.
However, it is unclear how such models represent lexical concepts-i.e., the
meanings of the words they use. This paper evaluates the lexical
representations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of
concept representations which focuses on representations of words describing
artifacts (such as "mop", "pencil", and "whistle"). The theory posits a causal
graph that relates the meanings of such words to the form, use, and history of
the objects to which they refer. We test LLMs using the same stimuli originally
used by Chaigneau et al. (2004) to evaluate the theory in humans, and consider
a variety of prompt designs. Our experiments concern judgements about causal
outcomes, object function, and object naming. We find no evidence that GPT-3
encodes the causal structure hypothesized by HIPE, but do find evidence that
GPT-4 encodes such structure. The results contribute to a growing body of
research characterizing the representational capacity of large language models.
</p></li>
</ul>

<h3>Title: Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (arXiv:2305.14658v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14658">http://arxiv.org/abs/2305.14658</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14658] Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality](http://arxiv.org/abs/2305.14658) #large language model</code></li>
<li>Summary: <p>LLMs (large language models) such as ChatGPT have shown remarkable language
understanding and generation capabilities. Although reference-free evaluators
based on LLMs show better human alignment than traditional reference-based
evaluators, there are many challenges in using reference-free evaluators based
on LLMs. Reference-free evaluators are more suitable for open-ended examples
with different semantics responses. But not all examples are open-ended. For
closed-ended examples with unique correct semantic response, reference-free
evaluators will still consider it high quality when giving a response that is
inconsistent with the facts and the semantic of reference. In order to
comprehensively evaluate the reliability of evaluators based on LLMs, we
construct two adversarial meta-evaluation dialogue generation datasets
KdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared
to previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more
challenging since they requires evaluators to be able to reasonably evaluate
closed-ended examples with the help of external knowledge or even its own
knowledge. Empirical results show that the ability of LLMs to identify
unreasonable responses is insufficient. There are risks in using eference-free
evaluators based on LLMs to evaluate the quality of dialogue responses.
</p></li>
</ul>

<h3>Title: ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (arXiv:2305.14688v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14688">http://arxiv.org/abs/2305.14688</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14688] ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](http://arxiv.org/abs/2305.14688) #large language model</code></li>
<li>Summary: <p>The answering quality of an aligned large language model (LLM) can be
drastically improved if treated with proper crafting of prompts. In this paper,
we propose ExpertPrompting to elicit the potential of LLMs to answer as
distinguished experts. We first utilize In-Context Learning to automatically
synthesize detailed and customized descriptions of the expert identity for each
specific instruction, and then ask LLMs to provide answer conditioned on such
agent background. Based on this augmented prompting strategy, we produce a new
set of instruction-following data using GPT-3.5, and train a competitive
open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation
to show that 1) the expert data is of significantly higher quality than vanilla
answers, and 2) ExpertLLaMA outperforms existing open-source opponents and
achieves 96\% of the original ChatGPT's capability. All data and the
ExpertLLaMA model will be made publicly available at
\url{https://github.com/OFA-Sys/ExpertLLaMA}.
</p></li>
</ul>

<h3>Title: Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs. (arXiv:2305.14693v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14693">http://arxiv.org/abs/2305.14693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14693] Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs](http://arxiv.org/abs/2305.14693) #large language model</code></li>
<li>Summary: <p>Have Large Language Models (LLMs) developed a personality? The short answer
is a resounding "We Don't Know!". In this paper, we show that we do not yet
have the right tools to measure personality in language models. Personality is
an important characteristic that influences behavior. As LLMs emulate
human-like intelligence and performance in various tasks, a natural question to
ask is whether these models have developed a personality. Previous works have
evaluated machine personality through self-assessment personality tests, which
are a set of multiple-choice questions created to evaluate personality in
humans. A fundamental assumption here is that human personality tests can
accurately measure personality in machines. In this paper, we investigate the
emergence of personality in five LLMs of different sizes ranging from 1.5B to
30B. We propose the Option-Order Symmetry property as a necessary condition for
the reliability of these self-assessment tests. Under this condition, the
answer to self-assessment questions is invariant to the order in which the
options are presented. We find that many LLMs personality test responses do not
preserve option-order symmetry. We take a deeper look at LLMs test responses
where option-order symmetry is preserved to find that in these cases, LLMs do
not take into account the situational statement being tested and produce the
exact same answer irrespective of the situation being tested. We also identify
the existence of inherent biases in these LLMs which is the root cause of the
aforementioned phenomenon and makes self-assessment tests unreliable. These
observations indicate that self-assessment tests are not the correct tools to
measure personality in LLMs. Through this paper, we hope to draw attention to
the shortcomings of current literature in measuring personality in LLMs and
call for developing tools for machine personality measurement.
</p></li>
</ul>

<h3>Title: Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14702">http://arxiv.org/abs/2305.14702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14702] Analyzing Influential Factors in Human Preference Judgments via GPT-4](http://arxiv.org/abs/2305.14702) #large language model</code></li>
<li>Summary: <p>Pairwise human judgments are pivotal in guiding large language models (LLMs)
to generate outputs that align with human preferences. They are also often used
in summarization evaluation, complementing existing automatic metrics. Despite
their significance, however, there has been limited research probing these
pairwise human judgments. The collective impact and respective weights of
factors such as informativeness, coherence, fluency, and factual consistency
remain elusive. The impact of hidden factors on the final judgment is also
unclear. In this paper, we conduct an in-depth examination of a dataset of
pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce
model, we identify key factors that could potentially influence human
judgments. Our research uncovers the inherent preferences embedded in human
judgments and suggests strategies to boost sample efficiency. Finally, we
provide insights on the construction of balanced datasets for human judgment
evaluations, a crucial step in shaping the behaviors of future LLMs.
</p></li>
</ul>

<h3>Title: Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14710">http://arxiv.org/abs/2305.14710</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14710] Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](http://arxiv.org/abs/2305.14710) #large language model</code></li>
<li>Summary: <p>Instruction-tuned models are trained on crowdsourcing datasets with task
instructions to achieve superior performance. However, in this work we raise
security concerns about this training paradigm. Our studies demonstrate that an
attacker can inject backdoors by issuing very few malicious instructions among
thousands of gathered data and control model behavior through data poisoning,
without even the need of modifying data instances or labels themselves. Through
such instruction attacks, the attacker can achieve over 90% attack success rate
across four commonly used NLP datasets, and cause persistent backdoors that are
easily transferred to 15 diverse datasets zero-shot. In this way, the attacker
can directly apply poisoned instructions designed for one dataset on many other
datasets. Moreover, the poisoned model cannot be cured by continual learning.
Lastly, instruction attacks show resistance to existing inference-time defense.
These findings highlight the need for more robust defenses against data
poisoning attacks in instructiontuning models and underscore the importance of
ensuring data quality in instruction crowdsourcing.
</p></li>
</ul>

<h3>Title: In-Context Demonstration Selection with Cross Entropy Difference. (arXiv:2305.14726v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14726">http://arxiv.org/abs/2305.14726</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14726] In-Context Demonstration Selection with Cross Entropy Difference](http://arxiv.org/abs/2305.14726) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) can use in-context demonstrations to improve
performance on zero-shot tasks. However, selecting the best in-context examples
is challenging because model performance can vary widely depending on the
selected examples. We present a cross-entropy difference (CED) method for
selecting in-context demonstrations. Our method is based on the observation
that the effectiveness of in-context demonstrations negatively correlates with
the perplexity of the test example by a language model that was finetuned on
that demonstration. We utilize parameter efficient finetuning to train small
models on training data that are used for computing the cross-entropy
difference between a test example and every candidate in-context demonstration.
This metric is used to rank and select in-context demonstrations independently
for each test input. We evaluate our method on a mix-domain dataset that
combines 8 benchmarks, representing 4 text generation tasks, showing that CED
for in-context demonstration selection can improve performance for a variety of
LLMs.
</p></li>
</ul>

<h3>Title: Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation. (arXiv:2305.14750v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14750">http://arxiv.org/abs/2305.14750</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14750] Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation](http://arxiv.org/abs/2305.14750) #large language model</code></li>
<li>Summary: <p>When answering complex questions, large language models (LLMs) may produce
answers that do not satisfy all criteria of the question. While existing
self-evaluation techniques aim to detect if such answers are correct, these
techniques are unable to determine which criteria of the question are satisfied
by the generated answers. To address this issue, we propose answer-based claim
decomposition (ABCD), a prompting strategy that decomposes questions into a
series of true/false claims that can be used to verify which criteria of the
input question an answer satisfies. Using the decomposed ABCD claims, we
perform fine-grained self-evaluation. Through preliminary experiments on three
datasets, including a newly-collected challenge dataset ObscureQA, we find that
GPT-3.5 has some ability to determine to what extent its answer satisfies the
criteria of the input question, and can give insights into the errors and
knowledge gaps of the model.
</p></li>
</ul>

<h3>Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. (arXiv:2305.14763v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14763">http://arxiv.org/abs/2305.14763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14763] Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models](http://arxiv.org/abs/2305.14763) #large language model</code></li>
<li>Summary: <p>The escalating debate on AI's capabilities warrants developing reliable
metrics to assess machine "intelligence". Recently, many anecdotal examples
were used to suggest that newer large language models (LLMs) like ChatGPT and
GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached
conflicting conclusions regarding those abilities. We investigate the extent of
LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs
exhibit certain N-ToM abilities, this behavior is far from being robust. We
further examine the factors impacting performance on N-ToM tasks and discover
that LLMs struggle with adversarial examples, indicating reliance on shallow
heuristics rather than robust ToM abilities. We caution against drawing
conclusions from anecdotal examples, limited benchmark testing, and using
human-designed psychological tests to evaluate models.
</p></li>
</ul>

<h3>Title: BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver. (arXiv:2305.14766v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14766">http://arxiv.org/abs/2305.14766</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14766] BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver](http://arxiv.org/abs/2305.14766) #large language model</code></li>
<li>Summary: <p>Open-domain question answering is a crucial task that often requires
accessing external information. Existing methods typically adopt a single-turn
retrieve-then-read approach, where relevant documents are first retrieved, and
questions are then answered based on the retrieved information. However, there
are cases where answering a question requires implicit knowledge that is not
directly retrievable from the question itself. In this work, we propose a novel
question-answering pipeline called eamSearchQA. Our approach leverages large
language models(LLMs) to iteratively generate new questions about the original
question, enabling an iterative reasoning process. By iteratively refining and
expanding the scope of the question, our method aims to capture and utilize
hidden knowledge that may not be directly obtainable through retrieval. We
evaluate our approach on the widely-used open-domain NQ and WebQ datasets. The
experimental results demonstrate that BeamSearchQA significantly outperforms
other zero-shot baselines, indicating its effectiveness in tackling the
challenges of open-domain question answering.
</p></li>
</ul>

<h3>Title: Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14770">http://arxiv.org/abs/2305.14770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14770] Using Natural Language Explanations to Rescale Human Judgments](http://arxiv.org/abs/2305.14770) #large language model</code></li>
<li>Summary: <p>The rise of large language models (LLMs) has brought a critical need for
high-quality human-labeled data, particularly for processes like human feedback
and evaluation. A common practice is to label data via consensus annotation
over the judgments of multiple crowdworkers. However, different annotators may
have different interpretations of labeling schemes unless given extensive
training, and for subjective NLP tasks, even trained expert annotators can
diverge heavily. We show that these nuances can be captured by high quality
natural language explanations, and propose a method to rescale ordinal
annotation in the presence of disagreement using LLMs. Specifically, we feed
Likert ratings and corresponding natural language explanations into an LLM and
prompt it to produce a numeric score. This score should reflect the underlying
assessment of the example by the annotator. The presence of explanations allows
the LLM to homogenize ratings across annotators in spite of scale usage
differences. We explore our technique in the context of a document-grounded
question answering task on which large language models achieve near-human
performance. Among questions where annotators identify incompleteness in the
answers, our rescaling improves correlation between nearly all annotator pairs,
improving pairwise correlation on these examples by an average of 0.2 Kendall's
tau.
</p></li>
</ul>

<h3>Title: Large Language Models as Counterfactual Generator: Strengths and Weaknesses. (arXiv:2305.14791v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14791">http://arxiv.org/abs/2305.14791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14791] Large Language Models as Counterfactual Generator: Strengths and Weaknesses](http://arxiv.org/abs/2305.14791) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated remarkable performance in a
range of natural language understanding and generation tasks. Yet, their
ability to generate counterfactuals, which can be used for areas like data
augmentation, remains under-explored. This study aims to investigate the
counterfactual generation capabilities of LLMs and analysis factors that
influence this ability. First, we evaluate how effective are LLMs in
counterfactual generation through data augmentation experiments for small
language models (SLMs) across four tasks: sentiment analysis, natural language
inference, named entity recognition, and relation extraction. While LLMs show
promising enhancements in various settings, they struggle in complex tasks due
to their self-limitations and the lack of logical guidance to produce
counterfactuals that align with commonsense. Second, our analysis reveals the
pivotal role of providing accurate task definitions and detailed step-by-step
instructions to LLMs in generating counterfactuals. Interestingly, we also find
that LLMs can generate reasonable counterfactuals even with unreasonable
demonstrations, which illustrates that demonstrations are primarily to regulate
the output format.This study provides the first comprehensive insight into
counterfactual generation abilities of LLMs, and offers a novel perspective on
utilizing LLMs for data augmentation to enhance SLMs.
</p></li>
</ul>

<h3>Title: MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions. (arXiv:2305.14795v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14795">http://arxiv.org/abs/2305.14795</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14795] MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions](http://arxiv.org/abs/2305.14795) #large language model</code></li>
<li>Summary: <p>The information stored in large language models (LLMs) falls out of date
quickly, and retraining from scratch is often not an option. This has recently
given rise to a range of techniques for injecting new facts through updating
model weights. Current evaluation paradigms are extremely limited, mainly
validating the recall of edited facts, but changing one fact should cause
rippling changes to the model's related beliefs. If we edit the UK Prime
Minister to now be Rishi Sunak, then we should get a different answer to Who is
married to the British Prime Minister? In this work, we present a benchmark
MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising
multi-hop questions that assess whether edited models correctly answer
questions where the answer should change as an entailed consequence of edited
facts. While we find that current knowledge-editing approaches can recall
edited facts accurately, they fail catastrophically on the constructed
multi-hop questions. We thus propose a simple memory-based approach, MeLLo,
which stores all edited facts externally while prompting the language model
iteratively to generate answers that are consistent with the edited facts.
While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up
to 175B) and outperforms previous model editors by a large margin.
</p></li>
</ul>

<h3>Title: Estimating Large Language Model Capabilities without Labeled Test Data. (arXiv:2305.14802v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14802">http://arxiv.org/abs/2305.14802</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14802] Estimating Large Language Model Capabilities without Labeled Test Data](http://arxiv.org/abs/2305.14802) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have exhibited an impressive ability to perform
in-context learning (ICL) from only a few examples, but the success of ICL
varies widely from task to task. Thus, it is important to quickly determine
whether ICL is applicable to a new task, but directly evaluating ICL accuracy
can be expensive in situations where test data is expensive to annotate -- the
exact situations where ICL is most appealing. In this paper, we propose the
task of ICL accuracy estimation, in which we predict the accuracy of an LLM
when doing in-context learning on a new task given only unlabeled data for that
task. To perform ICL accuracy estimation, we propose a method that trains a
meta-model using LLM confidence scores as features. We compare our method to
several strong accuracy estimation baselines on a new benchmark that covers 4
LLMs and 3 task collections. On average, the meta-model improves over all
baselines and achieves the same estimation performance as directly evaluating
on 40 labeled test examples per task, across the total 12 settings. We
encourage future work to improve on our methods and evaluate on our ICL
accuracy estimation benchmark to deepen our understanding of when ICL works.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI. (arXiv:2305.14389v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14389">http://arxiv.org/abs/2305.14389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14389] Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI](http://arxiv.org/abs/2305.14389) #segmentation</code></li>
<li>Summary: <p>Breast cancer (BC) remains a significant health threat, with no long-term
cure currently available. Early detection is crucial, yet mammography
interpretation is hindered by high false positives and negatives. With BC
incidence projected to surpass lung cancer, improving early detection methods
is vital. Thermography, using high-resolution infrared cameras, offers promise,
especially when combined with artificial intelligence (AI). This work presents
an attention-based convolutional neural network for segmentation, providing
increased speed and precision in BC detection and classification. The system
enhances images and performs cancer segmentation with explainable AI. We
propose a transformer-attention-based convolutional architecture (UNet) for
fault identification and employ Gradient-weighted Class Activation Mapping
(Grad-CAM) to analyze areas of bias and weakness in the UNet architecture with
IRT images. The superiority of our proposed framework is confirmed when
compared with existing deep learning frameworks.
</p></li>
</ul>

<h3>Title: FLAIR #2: textural and temporal information for semantic segmentation from multi-source optical imagery. (arXiv:2305.14467v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14467">http://arxiv.org/abs/2305.14467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14467] FLAIR #2: textural and temporal information for semantic segmentation from multi-source optical imagery](http://arxiv.org/abs/2305.14467) #segmentation</code></li>
<li>Summary: <p>The FLAIR #2 dataset hereby presented includes two very distinct types of
data, which are exploited for a semantic segmentation task aimed at mapping
land cover. The data fusion workflow proposes the exploitation of the fine
spatial and textural information of very high spatial resolution (VHR)
mono-temporal aerial imagery and the temporal and spectral richness of high
spatial resolution (HR) time series of Copernicus Sentinel-2 satellite images.
The French National Institute of Geographical and Forest Information (IGN), in
response to the growing availability of high-quality Earth Observation (EO)
data, is actively exploring innovative strategies to integrate these data with
heterogeneous characteristics. IGN is therefore offering this dataset to
promote innovation and improve our knowledge of our territories.
</p></li>
</ul>

<h3>Title: Streaming Object Detection on Fisheye Cameras for Automatic Parking. (arXiv:2305.14713v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14713">http://arxiv.org/abs/2305.14713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14713] Streaming Object Detection on Fisheye Cameras for Automatic Parking](http://arxiv.org/abs/2305.14713) #segmentation</code></li>
<li>Summary: <p>Fisheye cameras are widely employed in automatic parking, and the video
stream object detection (VSOD) of the fisheye camera is a fundamental
perception function to ensure the safe operation of vehicles. In past research
work, the difference between the output of the deep learning model and the
actual situation at the current moment due to the existence of delay of the
perception system is generally ignored. But the environment will inevitably
change within the delay time which may cause a potential safety hazard. In this
paper, we propose a real-time detection framework equipped with a dual-flow
perception module (dynamic and static flows) that can predict the future and
alleviate the time-lag problem. Meanwhile, we use a new scheme to evaluate
latency and accuracy. The standard bounding box is unsuitable for the object in
fisheye camera images due to the strong radial distortion of the fisheye camera
and the primary detection objects of parking perception are vehicles and
pedestrians, so we adopt the rotate bounding box and propose a new periodic
angle loss function to regress the angle of the box, which is the simple and
accurate representation method of objects. The instance segmentation ground
truth is used to supervise the training. Experiments demonstrate the
effectiveness of our approach. Code is released at:
https://gitee.com/hiyanyx/fisheye-streaming-perception.
</p></li>
</ul>

<h3>Title: Polarimetric Imaging for Perception. (arXiv:2305.14787v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14787">http://arxiv.org/abs/2305.14787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14787] Polarimetric Imaging for Perception](http://arxiv.org/abs/2305.14787) #segmentation</code></li>
<li>Summary: <p>Autonomous driving and advanced driver-assistance systems rely on a set of
sensors and algorithms to perform the appropriate actions and provide alerts as
a function of the driving scene. Typically, the sensors include color cameras,
radar, lidar and ultrasonic sensors. Strikingly however, although light
polarization is a fundamental property of light, it is seldom harnessed for
perception tasks. In this work we analyze the potential for improvement in
perception tasks when using an RGB-polarimetric camera, as compared to an RGB
camera. We examine monocular depth estimation and free space detection during
the middle of the day, when polarization is independent of subject heading, and
show that a quantifiable improvement can be achieved for both of them using
state-of-the-art deep neural networks, with a minimum of architectural changes.
We also present a new dataset composed of RGB-polarimetric images, lidar scans,
GNSS / IMU readings and free space segmentations that further supports
developing perception algorithms that take advantage of light polarization.
</p></li>
</ul>

<h3>Title: Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.14790">http://arxiv.org/abs/2305.14790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.14790] Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark](http://arxiv.org/abs/2305.14790) #segmentation</code></li>
<li>Summary: <p>Topic segmentation and outline generation strive to divide a document into
coherent topic sections and generate corresponding subheadings. Such a process
unveils the discourse topic structure of a document that benefits quickly
grasping and understanding the overall context of the document from a higher
level. However, research and applications in this field have been restrained
due to the lack of proper paragraph-level topic representations and
large-scale, high-quality corpora in Chinese compared to the success achieved
in English. Addressing these issues, we introduce a hierarchical
paragraph-level topic structure representation with title, subheading, and
paragraph that comprehensively models the document discourse topic structure.
In addition, we ensure a more holistic representation of topic distribution
within the document by using sentences instead of keywords to represent
sub-topics. Following this representation, we construct the largest Chinese
Paragraph-level Topic Structure corpus (CPTS), four times larger than the
previously largest one. We also employ a two-stage man-machine collaborative
annotation method to ensure the high quality of the corpus both in form and
semantics. Finally, we validate the computability of CPTS on two fundamental
tasks (topic segmentation and outline generation) by several strong baselines,
and its efficacy has been preliminarily confirmed on the downstream task:
discourse parsing. The representation, corpus, and benchmark we established
will provide a solid foundation for future studies.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
