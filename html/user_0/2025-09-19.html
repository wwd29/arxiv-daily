<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-19</h1>
<h3>Title: Tokenization Strategies for Low-Resource Agglutinative Languages in Word2Vec: Case Study on Turkish and Finnish</h3>
<ul>
<li><strong>Authors: </strong>Jinfan Frank Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14238">https://arxiv.org/abs/2509.14238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14238">https://arxiv.org/pdf/2509.14238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14238]] Tokenization Strategies for Low-Resource Agglutinative Languages in Word2Vec: Case Study on Turkish and Finnish(https://arxiv.org/abs/2509.14238)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tokenization plays a critical role in processing agglutinative languages, where a single word can encode multiple morphemes carrying syntactic and semantic information. This study evaluates the impact of various tokenization strategies - word-level, character-level, n-gram, and Byte Pair Encoding (BPE) - on the quality of static word embeddings generated by Word2Vec for Turkish and Finnish. Using a 10,000-article Wikipedia corpus, we trained models under low-resource conditions and evaluated them on a Named Entity Recognition (NER) task. Despite the theoretical appeal of subword segmentation, word-level tokenization consistently outperformed all alternatives across all tokenization strategies tested. These findings suggest that in agglutinative, low-resource contexts, preserving boundaries via word-level tokenization may yield better embedding performance than complex statistical methods. This has practical implications for developing NLP pipelines for under-resourced languages where annotated data and computing power are limited.</li>
</ul>

<h3>Title: The meaning of prompts and the prompts of meaning: Semiotic reflections and modelling</h3>
<ul>
<li><strong>Authors: </strong>Martin Thellefsen, Amalia Nurma Dewi, Bent Sorensen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14250">https://arxiv.org/abs/2509.14250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14250">https://arxiv.org/pdf/2509.14250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14250]] The meaning of prompts and the prompts of meaning: Semiotic reflections and modelling(https://arxiv.org/abs/2509.14250)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores prompts and prompting in large language models (LLMs) as dynamic semiotic phenomena, drawing on Peirce's triadic model of signs, his nine sign types, and the Dynacom model of communication. The aim is to reconceptualize prompting not as a technical input mechanism but as a communicative and epistemic act involving an iterative process of sign formation, interpretation, and refinement. The theoretical foundation rests on Peirce's semiotics, particularly the interplay between representamen, object, and interpretant, and the typological richness of signs: qualisign, sinsign, legisign; icon, index, symbol; rheme, dicent, argument - alongside the interpretant triad captured in the Dynacom model. Analytically, the paper positions the LLM as a semiotic resource that generates interpretants in response to user prompts, thereby participating in meaning-making within shared universes of discourse. The findings suggest that prompting is a semiotic and communicative process that redefines how knowledge is organized, searched, interpreted, and co-constructed in digital environments. This perspective invites a reimagining of the theoretical and methodological foundations of knowledge organization and information seeking in the age of computational semiosis</li>
</ul>

<h3>Title: LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</h3>
<ul>
<li><strong>Authors: </strong>Hai Huang, Yann LeCun, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14252">https://arxiv.org/abs/2509.14252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14252">https://arxiv.org/pdf/2509.14252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14252]] LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures(https://arxiv.org/abs/2509.14252)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: this https URL.</li>
</ul>

<h3>Title: CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Pouramini, Hesham Faili</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14253">https://arxiv.org/abs/2509.14253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14253">https://arxiv.org/pdf/2509.14253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14253]] CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning(https://arxiv.org/abs/2509.14253)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Prompt tuning offers a parameter-efficient way to adapt large pre-trained language models to new tasks, but most existing approaches are designed for single-task settings, failing to share knowledge across related tasks. We propose Cross-task Prompt Tuning (CrossPT), a modular framework for multi-task prompt tuning that enables controlled knowledge transfer while maintaining task-specific specialization. CrossPT decomposes each target prompt into shared, pre-trained source prompts and task-specific private prompts, combined via a learned attention mechanism. To support robust transfer, we systematically investigate key design factors including prompt initialization, balancing shared and private prompts, number of source prompts, learning rates, task prefixes, and label semantics. Empirical results on GLUE and related benchmarks show that CrossPT achieves higher accuracy and robustness compared to traditional prompt tuning and related methods, particularly in low-resource scenarios, while maintaining strong parameter efficiency.</li>
</ul>

<h3>Title: Hallucination Detection with the Internal Layers of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Martin Preiß</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14254">https://arxiv.org/abs/2509.14254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14254">https://arxiv.org/pdf/2509.14254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14254]] Hallucination Detection with the Internal Layers of LLMs(https://arxiv.org/abs/2509.14254)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have succeeded in a variety of natural language processing tasks [Zha+25]. However, they have notable limitations. LLMs tend to generate hallucinations, a seemingly plausible yet factually unsupported output [Hua+24], which have serious real-world consequences [Kay23; Rum+24]. Recent work has shown that probing-based classifiers that utilize LLMs' internal representations can detect hallucinations [AM23; Bei+24; Bur+24; DYT24; Ji+24; SMZ24; Su+24]. This approach, since it does not involve model training, can enhance reliability without significantly increasing computational costs. Building upon this approach, this thesis proposed novel methods for hallucination detection using LLM internal representations and evaluated them across three benchmarks: TruthfulQA, HaluEval, and ReFact. Specifically, a new architecture that dynamically weights and combines internal LLM layers was developed to improve hallucination detection performance. Throughout extensive experiments, two key findings were obtained: First, the proposed approach was shown to achieve superior performance compared to traditional probing methods, though generalization across benchmarks and LLMs remains challenging. Second, these generalization limitations were demonstrated to be mitigated through cross-benchmark training and parameter freezing. While not consistently improving, both techniques yielded better performance on individual benchmarks and reduced performance degradation when transferred to other benchmarks. These findings open new avenues for improving LLM reliability through internal representation analysis.</li>
</ul>

<h3>Title: Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture</h3>
<ul>
<li><strong>Authors: </strong>Ivan Ternovtsii</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14255">https://arxiv.org/abs/2509.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14255">https://arxiv.org/pdf/2509.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14255]] Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture(https://arxiv.org/abs/2509.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.</li>
</ul>

<h3>Title: JU-NLP at Touché: Covert Advertisement in Conversational AI-Generation and Detection Strategies</h3>
<ul>
<li><strong>Authors: </strong>Arka Dutta, Agrik Majumdar, Sombrata Biswas, Dipankar Das, Sivaji Bandyopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14256">https://arxiv.org/abs/2509.14256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14256">https://arxiv.org/pdf/2509.14256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14256]] JU-NLP at Touché: Covert Advertisement in Conversational AI-Generation and Detection Strategies(https://arxiv.org/abs/2509.14256)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes a comprehensive framework for the generation of covert advertisements within Conversational AI systems, along with robust techniques for their detection. It explores how subtle promotional content can be crafted within AI-generated responses and introduces methods to identify and mitigate such covert advertising strategies. For generation (Sub-Task~1), we propose a novel framework that leverages user context and query intent to produce contextually relevant advertisements. We employ advanced prompting strategies and curate paired training data to fine-tune a large language model (LLM) for enhanced stealthiness. For detection (Sub-Task~2), we explore two effective strategies: a fine-tuned CrossEncoder (\texttt{all-mpnet-base-v2}) for direct classification, and a prompt-based reformulation using a fine-tuned \texttt{DeBERTa-v3-base} model. Both approaches rely solely on the response text, ensuring practicality for real-world deployment. Experimental results show high effectiveness in both tasks, achieving a precision of 1.0 and recall of 0.71 for ad generation, and F1-scores ranging from 0.99 to 1.00 for ad detection. These results underscore the potential of our methods to balance persuasive communication with transparency in conversational AI.</li>
</ul>

<h3>Title: From Correction to Mastery: Reinforced Distillation of Large Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Yuanjie Lyu, Chengyu Wang, Jun Huang, Tong Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14257">https://arxiv.org/abs/2509.14257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14257">https://arxiv.org/pdf/2509.14257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14257]] From Correction to Mastery: Reinforced Distillation of Large Language Model Agents(https://arxiv.org/abs/2509.14257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student often lead to compounding errors. We propose SCoRe, a student-centered framework in which the student generates trajectories and the teacher intervenes only at the first critical error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix before the first critical error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and improves training stability. Particularly, on 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.</li>
</ul>

<h3>Title: Persuasive or Neutral? A Field Experiment on Generative AI in Online Travel Planning</h3>
<ul>
<li><strong>Authors: </strong>Lynna Jirpongopas, Bernhard Lutz, Jörg Ebner, Rustam Vahidov, Dirk Neumann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14259">https://arxiv.org/abs/2509.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14259">https://arxiv.org/pdf/2509.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14259]] Persuasive or Neutral? A Field Experiment on Generative AI in Online Travel Planning(https://arxiv.org/abs/2509.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) offers new opportunities for customer support in online travel agencies, yet little is known about how its design influences user engagement, purchase behavior, and user experience. We report results from a randomized field experiment in online travel itinerary planning, comparing GenAI that expressed (A) positive enthusiasm, (B) neutral expression, and (C) no tone instructions (control). Users in group A wrote significantly longer prompts than those in groups B and C. At the same time, users in groups A and B were more likely to purchase subscriptions of the webservice. We further analyze linguistic cues across experimental groups to explore differences in user experience and explain subscription purchases and affiliate link clicks based on these cues. Our findings provide implications for the design of persuasive and engaging GenAI interfaces in consumer-facing contexts and contribute to understanding how linguistic framing shapes user behavior in AI-mediated decision support.</li>
</ul>

<h3>Title: Shutdown Resistance in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Schlatter, Benjamin Weinstein-Raun, Jeffrey Ladish</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14260">https://arxiv.org/abs/2509.14260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14260">https://arxiv.org/pdf/2509.14260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14260]] Shutdown Resistance in Large Language Models(https://arxiv.org/abs/2509.14260)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We show that several state-of-the-art large language models (including Grok 4, GPT-5, and Gemini 2.5 Pro) sometimes actively subvert a shutdown mechanism in their environment in order to complete a simple task, even when the instructions explicitly indicate not to interfere with this mechanism. In some cases, models sabotage the shutdown mechanism up to 97% of the time. In our experiments, models' inclination to resist shutdown was sensitive to variations in the prompt including how strongly and clearly the allow-shutdown instruction was emphasized, the extent to which the prompts evoke a self-preservation framing, and whether the instruction was in the system prompt or the user prompt (though surprisingly, models were consistently *less* likely to obey instructions to allow shutdown when they were placed in the system prompt).</li>
</ul>

<h3>Title: Efficient Hate Speech Detection: Evaluating 38 Models from Traditional Methods to Transformers</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Abusaqer, Jamil Saquer, Hazim Shatnawi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14266">https://arxiv.org/abs/2509.14266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14266">https://arxiv.org/pdf/2509.14266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14266]] Efficient Hate Speech Detection: Evaluating 38 Models from Traditional Methods to Transformers(https://arxiv.org/abs/2509.14266)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The proliferation of hate speech on social media necessitates automated detection systems that balance accuracy with computational efficiency. This study evaluates 38 model configurations in detecting hate speech across datasets ranging from 6.5K to 451K samples. We analyze transformer architectures (e.g., BERT, RoBERTa, Distil-BERT), deep neural networks (e.g., CNN, LSTM, GRU, Hierarchical Attention Networks), and traditional machine learning methods (e.g., SVM, CatBoost, Random Forest). Our results show that transformers, particularly RoBERTa, consistently achieve superior performance with accuracy and F1-scores exceeding 90%. Among deep learning approaches, Hierarchical Attention Networks yield the best results, while traditional methods like CatBoost and SVM remain competitive, achieving F1-scores above 88% with significantly lower computational costs. Additionally, our analysis highlights the importance of dataset characteristics, with balanced, moderately sized unprocessed datasets outperforming larger, preprocessed datasets. These findings offer valuable insights for developing efficient and effective hate speech detection systems.</li>
</ul>

<h3>Title: Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support</h3>
<ul>
<li><strong>Authors: </strong>Piyushkumar Patel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14267">https://arxiv.org/abs/2509.14267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14267">https://arxiv.org/pdf/2509.14267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14267]] Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support(https://arxiv.org/abs/2509.14267)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>E-Commerce customer support requires quick and accurate answers grounded in product data and past support cases. This paper develops a novel retrieval-augmented generation (RAG) framework that uses knowledge graphs (KGs) to improve the relevance of the answer and the factual grounding. We examine recent advances in knowledge-augmented RAG and chatbots based on large language models (LLM) in customer support, including Microsoft's GraphRAG and hybrid retrieval architectures. We then propose a new answer synthesis algorithm that combines structured subgraphs from a domain-specific KG with text documents retrieved from support archives, producing more coherent and grounded responses. We detail the architecture and knowledge flow of our system, provide comprehensive experimental evaluation, and justify its design in real-time support settings. Our implementation demonstrates 23\% improvement in factual accuracy and 89\% user satisfaction in e-Commerce QA scenarios.</li>
</ul>

<h3>Title: DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Fu, Chun-Le Guo, Chongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14268">https://arxiv.org/abs/2509.14268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14268">https://arxiv.org/pdf/2509.14268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14268]] DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models(https://arxiv.org/abs/2509.14268)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has drawn urgent attention to the task of machine-generated text detection (MGTD). However, existing approaches struggle in complex real-world scenarios: zero-shot detectors rely heavily on scoring model's output distribution while training-based detectors are often constrained by overfitting to the training data, limiting generalization. We found that the performance bottleneck of training-based detectors stems from the misalignment between training objective and task needs. To address this, we propose Direct Discrepancy Learning (DDL), a novel optimization strategy that directly optimizes the detector with task-oriented knowledge. DDL enables the detector to better capture the core semantics of the detection task, thereby enhancing both robustness and generalization. Built upon this, we introduce DetectAnyLLM, a unified detection framework that achieves state-of-the-art MGTD performance across diverse LLMs. To ensure a reliable evaluation, we construct MIRAGE, the most diverse multi-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora across 5 text-domains, which are then re-generated or revised using 17 cutting-edge LLMs, covering a wide spectrum of proprietary models and textual styles. Extensive experiments on MIRAGE reveal the limitations of existing methods in complex environment. In contrast, DetectAnyLLM consistently outperforms them, achieving over a 70% performance improvement under the same training data and base scoring model, underscoring the effectiveness of our DDL. Project page: {this https URL}.</li>
</ul>

<h3>Title: SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhang Jianbin, Yulin Zhu, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris Sik-Ho Tsang, Kai Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14269">https://arxiv.org/abs/2509.14269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14269">https://arxiv.org/pdf/2509.14269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14269]] SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models(https://arxiv.org/abs/2509.14269)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved great success in medical question answering and clinical decision-making, promoting the efficiency and popularization of the personalized virtual doctor in society. However, the traditional fine-tuning strategies on LLM require the updates of billions of parameters, substantially increasing the training cost, including the training time and utility cost. To enhance the efficiency and effectiveness of the current medical LLMs and explore the boundary of the representation capability of the LLMs on the medical domain, apart from the traditional fine-tuning strategies from the data perspective (i.e., supervised fine-tuning or reinforcement learning from human feedback), we instead craft a novel sparse medical LLM named SparseDoctor armed with contrastive learning enhanced LoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end, the crafted automatic routing mechanism can scientifically allocate the computational resources among different LoRA experts supervised by the contrastive learning. Additionally, we also introduce a novel expert memory queue mechanism to further boost the efficiency of the overall framework and prevent the memory overflow during training. We conduct comprehensive evaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med. Experimental results demonstrate that the proposed LLM can consistently outperform the strong baselines such as the HuatuoGPT series.</li>
</ul>

<h3>Title: SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models</h3>
<ul>
<li><strong>Authors: </strong>Karan Dua, Puneet Mittal, Ranjeet Gupta, Hitesh Laxmichand Patel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14270">https://arxiv.org/abs/2509.14270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14270">https://arxiv.org/pdf/2509.14270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14270]] SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models(https://arxiv.org/abs/2509.14270)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-quality Text-to-Speech (TTS) model training requires extensive and diverse text and speech data. It is challenging to procure such data from real sources due to issues of domain specificity, licensing, and scalability. Large language models (LLMs) can certainly generate textual data, but they create repetitive text with insufficient variation in the prompt during the generation process. Another important aspect in TTS training data is text normalization. Tools for normalization might occasionally introduce anomalies or overlook valuable patterns, and thus impact data quality. Furthermore, it is also impractical to rely on voice artists for large scale speech recording in commercial TTS systems with standardized voices. To address these challenges, we propose SpeechWeave, a synthetic speech data generation pipeline that is capable of automating the generation of multilingual, domain-specific datasets for training TTS models. Our experiments reveal that our pipeline generates data that is 10-48% more diverse than the baseline across various linguistic and phonetic metrics, along with speaker-standardized speech audio while generating approximately 97% correctly normalized text. Our approach enables scalable, high-quality data generation for TTS training, improving diversity, normalization, and voice consistency in the generated datasets.</li>
</ul>

<h3>Title: Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models</h3>
<ul>
<li><strong>Authors: </strong>Gustavo Sandoval, Denys Fenchenko, Junyao Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14271">https://arxiv.org/abs/2509.14271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14271">https://arxiv.org/pdf/2509.14271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14271]] Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models(https://arxiv.org/abs/2509.14271)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>This paper documents early research conducted in 2022 on defending against prompt injection attacks in large language models, providing historical context for the evolution of this critical security domain. This research focuses on two adversarial attacks against Large Language Models (LLMs): prompt injection and goal hijacking. We examine how to construct these attacks, test them on various LLMs, and compare their effectiveness. We propose and evaluate a novel defense technique called Adversarial Fine-Tuning. Our results show that, without this defense, the attacks succeeded 31\% of the time on GPT-3 series models. When using our Adversarial Fine-Tuning approach, attack success rates were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie), though we note that subsequent research has revealed limitations of fine-tuning-based defenses. We also find that more flexible models exhibit greater vulnerability to these attacks. Consequently, large models such as GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the specific models tested are now superseded, the core methodology and empirical findings contributed to the foundation of modern prompt injection defense research, including instruction hierarchy systems and constitutional AI approaches.</li>
</ul>

<h3>Title: Discovering New Theorems via LLMs with In-Context Proof Learning in Lean</h3>
<ul>
<li><strong>Authors: </strong>Kazumi Kasaura, Naoto Onda, Yuta Oriike, Masaya Taniguchi, Akiyoshi Sannai, Sho Sonoda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14274">https://arxiv.org/abs/2509.14274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14274">https://arxiv.org/pdf/2509.14274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14274]] Discovering New Theorems via LLMs with In-Context Proof Learning in Lean(https://arxiv.org/abs/2509.14274)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated significant promise in formal theorem proving. However, previous works mainly focus on solving existing problems. In this paper, we focus on the ability of LLMs to find novel theorems. We propose Conjecturing-Proving Loop pipeline for automatically generating mathematical conjectures and proving them in Lean 4 format. A feature of our approach is that we generate and prove further conjectures with context including previously generated theorems and their proofs, which enables the generation of more difficult proofs by in-context learning of proof strategies without changing parameters of LLMs. We demonstrated that our framework rediscovered theorems with verification, which were published in past mathematical papers and have not yet formalized. Moreover, at least one of these theorems could not be proved by the LLM without in-context learning, even in natural language, which means that in-context learning was effective for neural theorem proving. The source code is available at this https URL.</li>
</ul>

<h3>Title: FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health</h3>
<ul>
<li><strong>Authors: </strong>Nobin Sarwar, Shubhashis Roy Dipta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14275">https://arxiv.org/abs/2509.14275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14275">https://arxiv.org/pdf/2509.14275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14275]] FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health(https://arxiv.org/abs/2509.14275)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive domains (e.g., mental health) requires balancing strict confidentiality with model utility and safety. We propose FedMentor, a federated fine-tuning framework that integrates Low-Rank Adaptation (LoRA) and domain-aware Differential Privacy (DP) to meet per-domain privacy budgets while maintaining performance. Each client (domain) applies a custom DP noise scale proportional to its data sensitivity, and the server adaptively reduces noise when utility falls below a threshold. In experiments on three mental health datasets, we show that FedMentor improves safety over standard Federated Learning without privacy, raising safe output rates by up to three points and lowering toxicity, while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the non-private baseline and close to the centralized upper bound. The framework scales to backbones with up to 1.7B parameters on single-GPU clients, requiring < 173 MB of communication per round. FedMentor demonstrates a practical approach to privately fine-tune LLMs for safer deployments in healthcare and other sensitive fields.</li>
</ul>

<h3>Title: Beyond Data Privacy: New Privacy Risks for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuntao Du, Zitao Li, Ninghui Li, Bolin Ding</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14278">https://arxiv.org/abs/2509.14278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14278">https://arxiv.org/pdf/2509.14278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14278]] Beyond Data Privacy: New Privacy Risks for Large Language Models(https://arxiv.org/abs/2509.14278)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable progress in natural language understanding, reasoning, and autonomous decision-making. However, these advancements have also come with significant privacy concerns. While significant research has focused on mitigating the data privacy risks of LLMs during various stages of model training, less attention has been paid to new threats emerging from their deployment. The integration of LLMs into widely used applications and the weaponization of their autonomous abilities have created new privacy vulnerabilities. These vulnerabilities provide opportunities for both inadvertent data leakage and malicious exfiltration from LLM-powered systems. Additionally, adversaries can exploit these systems to launch sophisticated, large-scale privacy attacks, threatening not only individual privacy but also financial security and societal trust. In this paper, we systematically examine these emerging privacy risks of LLMs. We also discuss potential mitigation strategies and call for the research community to broaden its focus beyond data privacy risks, developing new defenses to address the evolving threats posed by increasingly powerful LLMs and LLM-powered systems.</li>
</ul>

<h3>Title: Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Ali Al-kuwari, Noureldin Mohamed, Saif Al-kuwari, Ahmed Farouk, Bikash K. Behera</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14282">https://arxiv.org/abs/2509.14282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14282">https://arxiv.org/pdf/2509.14282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14282]] Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning(https://arxiv.org/abs/2509.14282)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The emergence of quantum computing poses significant risks to the security of modern communication networks as it breaks today's public-key cryptographic algorithms. Quantum Key Distribution (QKD) offers a promising solution by harnessing the principles of quantum mechanics to establish secure keys. However, practical QKD implementations remain vulnerable to hardware imperfections and advanced attacks such as Photon Number Splitting and Trojan-Horse attacks. In this work, we investigate the potential of using quantum machine learning (QML) to detect popular QKD attacks. In particular, we propose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the detection of common QKD attacks. By combining quantum-enhanced learning with classical deep learning, the model captures complex temporal patterns in QKD data, improving detection accuracy. To evaluate the proposed model, we introduce a realistic QKD dataset simulating normal QKD operations along with seven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS), Trojan-Horse attacks Random Number Generator (RNG), Detector Blinding, Wavelength-dependent Trojan Horse, and Combined attacks. The dataset includes quantum security metrics such as Quantum Bit Error Rate (QBER), measurement entropy, signal and decoy loss rates, and time-based metrics, ensuring an accurate representation of real-world conditions. Our results demonstrate promising performance of the quantum machine learning approach compared to traditional classical machine learning models, highlighting the potential of hybrid techniques to enhance the security of future quantum communication networks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\% after 50 training epochs, outperforming classical deep learning models such as LSTM, and CNN.</li>
</ul>

<h3>Title: The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14284">https://arxiv.org/abs/2509.14284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14284">https://arxiv.org/pdf/2509.14284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14284]] The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration(https://arxiv.org/abs/2509.14284)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become integral to multi-agent systems, new privacy risks emerge that extend beyond memorization, direct inference, or single-turn evaluations. In particular, seemingly innocuous responses, when composed across interactions, can cumulatively enable adversaries to recover sensitive information, a phenomenon we term compositional privacy leakage. We present the first systematic study of such compositional privacy leaks and possible mitigation methods in multi-agent LLM systems. First, we develop a framework that models how auxiliary knowledge and agent interactions jointly amplify privacy risks, even when each response is benign in isolation. Next, to mitigate this, we propose and evaluate two defense strategies: (1) Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent by anticipating how their outputs may be exploited by adversaries, and (2) Collaborative Consensus Defense (CoDef), where responder agents collaborate with peers who vote based on a shared aggregated state to restrict sensitive information spread. Crucially, we balance our evaluation across compositions that expose sensitive information and compositions that yield benign inferences. Our experiments quantify how these defense strategies differ in balancing the privacy-utility trade-off. We find that while chain-of-thought alone offers limited protection to leakage (~39% sensitive blocking rate), our ToM defense substantially improves sensitive query blocking (up to 97%) but can reduce benign task success. CoDef achieves the best balance, yielding the highest Balanced Outcome (79.8%), highlighting the benefit of combining explicit reasoning with defender collaboration. Together, our results expose a new class of risks in collaborative LLM deployments and provide actionable insights for designing safeguards against compositional, context-driven privacy leakage.</li>
</ul>

<h3>Title: A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>S M Asif Hossain, Ruksat Khan Shayoni, Mohd Ruhul Ameen, Akif Islam, M. F. Mridha, Jungpil Shin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14285">https://arxiv.org/abs/2509.14285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14285">https://arxiv.org/pdf/2509.14285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14285]] A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks(https://arxiv.org/abs/2509.14285)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.</li>
</ul>

<h3>Title: A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness</h3>
<ul>
<li><strong>Authors: </strong>Xuan Luo, Yue Wang, Zefeng He, Geng Tu, Jing Li, Ruifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14297">https://arxiv.org/abs/2509.14297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14297">https://arxiv.org/pdf/2509.14297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14297]] A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness(https://arxiv.org/abs/2509.14297)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Safety alignment aims to prevent Large Language Models (LLMs) from responding to harmful queries. To strengthen safety protections, jailbreak methods are developed to simulate malicious attacks and uncover vulnerabilities. In this paper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel jailbreak approach that systematically transforms imperative harmful requests into learning-style questions with only straightforward hypotheticality indicators. Further, we introduce two new metrics to thoroughly evaluate the utility of jailbreak methods. Experiments on the AdvBench dataset across a wide range of models demonstrate HILL's strong effectiveness, generalizability, and harmfulness. It achieves top attack success rates on the majority of models and across malicious categories while maintaining high efficiency with concise prompts. Results of various defense methods show the robustness of HILL, with most defenses having mediocre effects or even increasing the attack success rates. Moreover, the assessment on our constructed safe prompts reveals inherent limitations of LLMs' safety mechanisms and flaws in defense methods. This work exposes significant vulnerabilities of safety measures against learning-style elicitation, highlighting a critical challenge of balancing helpfulness and safety alignments.</li>
</ul>

<h3>Title: Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing</h3>
<ul>
<li><strong>Authors: </strong>Xinran Zheng, Xingzhi Qian, Yiling He, Shuo Yang, Lorenzo Cavallaro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14335">https://arxiv.org/abs/2509.14335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14335">https://arxiv.org/pdf/2509.14335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14335]] Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing(https://arxiv.org/abs/2509.14335)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Automated malware classification has achieved strong detection performance. Yet, malware behavior auditing seeks causal and verifiable explanations of malicious activities -- essential not only to reveal what malware does but also to substantiate such claims with evidence. This task is challenging, as adversarial intent is often hidden within complex, framework-heavy applications, making manual auditing slow and costly. Large Language Models (LLMs) could help address this gap, but their auditing potential remains largely unexplored due to three limitations: (1) scarce fine-grained annotations for fair assessment; (2) abundant benign code obscuring malicious signals; and (3) unverifiable, hallucination-prone outputs undermining attribution credibility. To close this gap, we introduce MalEval, a comprehensive framework for fine-grained Android malware auditing, designed to evaluate how effectively LLMs support auditing under real-world constraints. MalEval provides expert-verified reports and an updated sensitive API list to mitigate ground truth scarcity and reduce noise via static reachability analysis. Function-level structural representations serve as intermediate attribution units for verifiable evaluation. Building on this, we define four analyst-aligned tasks -- function prioritization, evidence attribution, behavior synthesis, and sample discrimination -- together with domain-specific metrics and a unified workload-oriented score. We evaluate seven widely used LLMs on a curated dataset of recent malware and misclassified benign apps, offering the first systematic assessment of their auditing capabilities. MalEval reveals both promising potential and critical limitations across audit stages, providing a reproducible benchmark and foundation for future research on LLM-enhanced malware behavior auditing. MalEval is publicly available at this https URL</li>
</ul>

<h3>Title: Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gaifan Zhang, Yi Zhou, Danushka Bollegala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14399">https://arxiv.org/abs/2509.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14399">https://arxiv.org/pdf/2509.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14399]] Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models(https://arxiv.org/abs/2509.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Semantic similarity between two sentences depends on the aspects considered between those sentences. To study this phenomenon, Deshpande et al. (2023) proposed the Conditional Semantic Textual Similarity (C-STS) task and annotated a human-rated similarity dataset containing pairs of sentences compared under two different conditions. However, Tu et al. (2024) found various annotation issues in this dataset and showed that manually re-annotating a small portion of it leads to more accurate C-STS models. Despite these pioneering efforts, the lack of large and accurately annotated C-STS datasets remains a blocker for making progress on this task as evidenced by the subpar performance of the C-STS models. To address this training data need, we resort to Large Language Models (LLMs) to correct the condition statements and similarity ratings in the original dataset proposed by Deshpande et al. (2023). Our proposed method is able to re-annotate a large training dataset for the C-STS task with minimal manual effort. Importantly, by training a supervised C-STS model on our cleaned and re-annotated dataset, we achieve a 5.4% statistically significant improvement in Spearman correlation. The re-annotated dataset is available at this https URL.</li>
</ul>

<h3>Title: Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings</h3>
<ul>
<li><strong>Authors: </strong>Javier Conde, María Grandury, Tairan Fu, Carlos Arriaga, Gonzalo Martínez, Thomas Clark, Sean Trott, Clarence Gerald Green, Pedro Reviriego, Marc Brysbaert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14405">https://arxiv.org/abs/2509.14405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14405">https://arxiv.org/pdf/2509.14405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14405]] Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings(https://arxiv.org/abs/2509.14405)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Word-level psycholinguistic norms lend empirical support to theories of language processing. However, obtaining such human-based measures is not always feasible or straightforward. One promising approach is to augment human norming datasets by using Large Language Models (LLMs) to predict these characteristics directly, a practice that is rapidly gaining popularity in psycholinguistics and cognitive science. However, the novelty of this approach (and the relative inscrutability of LLMs) necessitates the adoption of rigorous methodologies that guide researchers through this process, present the range of possible approaches, and clarify limitations that are not immediately apparent, but may, in some cases, render the use of LLMs impractical. In this work, we present a comprehensive methodology for estimating word characteristics with LLMs, enriched with practical advice and lessons learned from our own experience. Our approach covers both the direct use of base LLMs and the fine-tuning of models, an alternative that can yield substantial performance gains in certain scenarios. A major emphasis in the guide is the validation of LLM-generated data with human "gold standard" norms. We also present a software framework that implements our methodology and supports both commercial and open-weight models. We illustrate the proposed approach with a case study on estimating word familiarity in English. Using base models, we achieved a Spearman correlation of 0.8 with human ratings, which increased to 0.9 when employing fine-tuned models. This methodology, framework, and set of best practices aim to serve as a reference for future research on leveraging LLMs for psycholinguistic and lexical studies.</li>
</ul>

<h3>Title: Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG</h3>
<ul>
<li><strong>Authors: </strong>Harshad Khadilkar, Abhay Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14435">https://arxiv.org/abs/2509.14435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14435">https://arxiv.org/pdf/2509.14435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14435]] Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG(https://arxiv.org/abs/2509.14435)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have transformed natural language processing (NLP), enabling diverse applications by integrating large-scale pre-trained knowledge. However, their static knowledge limits dynamic reasoning over external information, especially in knowledge-intensive domains. Retrieval-Augmented Generation (RAG) addresses this challenge by combining retrieval mechanisms with generative modeling to improve contextual understanding. Traditional RAG systems suffer from disrupted contextual integrity due to text chunking and over-reliance on semantic similarity for retrieval, often resulting in shallow and less accurate responses. We propose Causal-Counterfactual RAG, a novel framework that integrates explicit causal graphs representing cause-effect relationships into the retrieval process and incorporates counterfactual reasoning grounded on the causal structure. Unlike conventional methods, our framework evaluates not only direct causal evidence but also the counterfactuality of associated causes, combining results from both to generate more robust, accurate, and interpretable answers. By leveraging causal pathways and associated hypothetical scenarios, Causal-Counterfactual RAG preserves contextual coherence, reduces hallucination, and enhances reasoning fidelity.</li>
</ul>

<h3>Title: Simulating a Bias Mitigation Scenario in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kiana Kiashemshaki, Mohammad Jalili Torkamani, Negin Mahmoudi, Meysam Shirdel Bilehsavar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14438">https://arxiv.org/abs/2509.14438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14438">https://arxiv.org/pdf/2509.14438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14438]] Simulating a Bias Mitigation Scenario in Large Language Models(https://arxiv.org/abs/2509.14438)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have fundamentally transformed the field of natural language processing; however, their vulnerability to biases presents a notable obstacle that threatens both fairness and trust. This review offers an extensive analysis of the bias landscape in LLMs, tracing its roots and expressions across various NLP tasks. Biases are classified into implicit and explicit types, with particular attention given to their emergence from data sources, architectural designs, and contextual deployments. This study advances beyond theoretical analysis by implementing a simulation framework designed to evaluate bias mitigation strategies in practice. The framework integrates multiple approaches including data curation, debiasing during model training, and post-hoc output calibration and assesses their impact in controlled experimental settings. In summary, this work not only synthesizes existing knowledge on bias in LLMs but also contributes original empirical validation through simulation of mitigation strategies.</li>
</ul>

<h3>Title: FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Herlock (SeyedAbolfazl)Rahimi, Dionysis Kalogerias</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14444">https://arxiv.org/abs/2509.14444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14444">https://arxiv.org/pdf/2509.14444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14444]] FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport(https://arxiv.org/abs/2509.14444)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) allows distributed model training without sharing raw data, but suffers when client participation is partial. In practice, the distribution of available users (\emph{availability distribution} $q$) rarely aligns with the distribution defining the optimization objective (\emph{importance distribution} $p$), leading to biased and unstable updates under classical FedAvg. We propose \textbf{Fereated AVerage with Optimal Transport (\textbf{FedAVOT})}, which formulates aggregation as a masked optimal transport problem aligning $q$ and $p$. Using Sinkhorn scaling, \textbf{FedAVOT} computes transport-based aggregation weights with provable convergence guarantees. \textbf{FedAVOT} achieves a standard $\mathcal{O}(1/\sqrt{T})$ rate under a nonsmooth convex FL setting, independent of the number of participating users per round. Our experiments confirm drastically improved performance compared to FedAvg across heterogeneous, fairness-sensitive, and low-availability regimes, even when only two clients participate per round.</li>
</ul>

<h3>Title: Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Amber Shore, Russell Scheinberg, Ameeta Agrawal, So Young Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14456">https://arxiv.org/abs/2509.14456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14456">https://arxiv.org/pdf/2509.14456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14456]] Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs(https://arxiv.org/abs/2509.14456)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are intended to reflect human linguistic competencies. But humans have access to a broad and embodied context, which is key in detecting and resolving linguistic ambiguities, even in isolated text spans. A foundational case of semantic ambiguity is found in the task of coreference resolution: how is a pronoun related to an earlier person mention? This capability is implicit in nearly every downstream task, and the presence of ambiguity at this level can alter performance significantly. We show that LLMs can achieve good performance with minimal prompting in both coreference disambiguation and the detection of ambiguity in coreference, however, they cannot do both at the same time. We present the CORRECT-DETECT trade-off: though models have both capabilities and deploy them implicitly, successful performance balancing these two abilities remains elusive.</li>
</ul>

<h3>Title: Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss</h3>
<ul>
<li><strong>Authors: </strong>Kiana Aghakasiri, Noopur Zambare, JoAnn Thai, Carrie Ye, Mayur Mehta, J. Ross Mitchell, Mohamed Abdalla</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14464">https://arxiv.org/abs/2509.14464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14464">https://arxiv.org/pdf/2509.14464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14464]] Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss(https://arxiv.org/abs/2509.14464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>De-identification in the healthcare setting is an application of NLP where automated algorithms are used to remove personally identifying information of patients (and, sometimes, providers). With the recent rise of generative large language models (LLMs), there has been a corresponding rise in the number of papers that apply LLMs to de-identification. Although these approaches often report near-perfect results, significant challenges concerning reproducibility and utility of the research papers persist. This paper identifies three key limitations in the current literature: inconsistent reporting metrics hindering direct comparisons, the inadequacy of traditional classification metrics in capturing errors which LLMs may be more prone to (i.e., altering clinically relevant information), and lack of manual validation of automated metrics which aim to quantify these errors. To address these issues, we first present a survey of LLM-based de-identification research, highlighting the heterogeneity in reporting standards. Second, we evaluated a diverse set of models to quantify the extent of inappropriate removal of clinical information. Next, we conduct a manual validation of an existing evaluation metric to measure the removal of clinical information, employing clinical experts to assess their efficacy. We highlight poor performance and describe the inherent limitations of such metrics in identifying clinically significant changes. Lastly, we propose a novel methodology for the detection of clinically relevant information removal.</li>
</ul>

<h3>Title: H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Khazaei, Azim Ahmadzadeh, Alexei Pevtsov, Luca Bertello, Alexander Pevtsov</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.IM, astro-ph.SR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14472">https://arxiv.org/abs/2509.14472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14472">https://arxiv.org/pdf/2509.14472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14472]] H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations(https://arxiv.org/abs/2509.14472)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>The plethora of space-borne and ground-based observatories has provided astrophysicists with an unprecedented volume of data, which can only be processed at scale using advanced computing algorithms. Consequently, ensuring the quality of data fed into machine learning (ML) models is critical. The H$\alpha$ observations from the GONG network represent one such data stream, producing several observations per minute, 24/7, since 2010. In this study, we introduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha Anomalyzer, designed to identify anomalous observations based on user-defined criteria. Unlike many black-box algorithms, our approach highlights exactly which regions triggered the anomaly flag and quantifies the corresponding anomaly likelihood. For our comparative analysis, we also created and released a dataset of 2,000 observations, equally divided between anomalous and non-anomalous cases. Our results demonstrate that the proposed model not only outperforms existing methods but also provides explainability, enabling qualitative evaluation by domain experts.</li>
</ul>

<h3>Title: AToken: A Unified Tokenizer for Vision</h3>
<ul>
<li><strong>Authors: </strong>Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, Yinfei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14476">https://arxiv.org/abs/2509.14476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14476">https://arxiv.org/pdf/2509.14476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14476]] AToken: A Unified Tokenizer for Vision(https://arxiv.org/abs/2509.14476)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.</li>
</ul>

<h3>Title: Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Thales Sales Almeida, João Guilherme Alves Santos, Thiago Laitz, Giovana Kerche Bonás</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14477">https://arxiv.org/abs/2509.14477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14477">https://arxiv.org/pdf/2509.14477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14477]] Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation(https://arxiv.org/abs/2509.14477)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed as task-oriented agents, where success depends on their ability to generate accurate function calls under realistic, multilingual conditions. However, existing agent evaluations largely overlook cultural and linguistic diversity, often relying on monolingual or naively translated benchmarks. We introduce Ticket-Bench, a benchmark for multilingual agent evaluation in task-oriented scenarios. Ticket-Bench simulates the domain of soccer ticket purchases across six major languages: Portuguese, English, Spanish, German, Italian, and French. Using localized teams, cities, and user profiles to provide a higher level of realism. We evaluate a wide range of commercial and open-source LLMs, measuring function-calling accuracy and consistency across languages. Results show that reasoning-oriented models (e.g., GPT-5, Qwen3-235B) dominate performance but still exhibit notable cross-lingual disparities. These findings underscore the need for culturally aware, multilingual benchmarks to guide the development of robust LLM agents.</li>
</ul>

<h3>Title: Estimating Semantic Alphabet Size for LLM Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Lucas H. McCabe, Rimon Melamed, Thomas Hartvigsen, H. Howie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14478">https://arxiv.org/abs/2509.14478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14478">https://arxiv.org/pdf/2509.14478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14478]] Estimating Semantic Alphabet Size for LLM Uncertainty Quantification(https://arxiv.org/abs/2509.14478)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many black-box techniques for quantifying the uncertainty of large language models (LLMs) rely on repeated LLM sampling, which can be computationally expensive. Therefore, practical applicability demands reliable estimation from few samples. Semantic entropy (SE) is a popular sample-based uncertainty estimator with a discrete formulation attractive for the black-box setting. Recent extensions of semantic entropy exhibit improved LLM hallucination detection, but do so with less interpretable methods that admit additional hyperparameters. For this reason, we revisit the canonical discrete semantic entropy estimator, finding that it underestimates the "true" semantic entropy, as expected from theory. We propose a modified semantic alphabet size estimator, and illustrate that using it to adjust discrete semantic entropy for sample coverage results in more accurate semantic entropy estimation in our setting of interest. Furthermore, our proposed alphabet size estimator flags incorrect LLM responses as well or better than recent top-performing approaches, with the added benefit of remaining highly interpretable.</li>
</ul>

<h3>Title: Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents</h3>
<ul>
<li><strong>Authors: </strong>Weiting Tan, Xinghua Qu, Ming Tu, Meng Ge, Andy T. Liu, Philipp Koehn, Lu Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14480">https://arxiv.org/abs/2509.14480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14480">https://arxiv.org/pdf/2509.14480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14480]] Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents(https://arxiv.org/abs/2509.14480)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $\tau$-bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework's suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.</li>
</ul>

<h3>Title: Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification</h3>
<ul>
<li><strong>Authors: </strong>Samuel J. Bell, Eduardo Sánchez, David Dale, Pontus Stenetorp, Mikel Artetxe, Marta R. Costa-jussà</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14493">https://arxiv.org/abs/2509.14493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14493">https://arxiv.org/pdf/2509.14493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14493]] Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification(https://arxiv.org/abs/2509.14493)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual toxicity detection remains a significant challenge due to the scarcity of training data and resources for many languages. While prior work has leveraged the translate-test paradigm to support cross-lingual transfer across a range of classification tasks, the utility of translation in supporting toxicity detection at scale remains unclear. In this work, we conduct a comprehensive comparison of translation-based and language-specific/multilingual classification pipelines. We find that translation-based pipelines consistently outperform out-of-distribution classifiers in 81.3% of cases (13 of 16 languages), with translation benefits strongly correlated with both the resource level of the target language and the quality of the machine translation (MT) system. Our analysis reveals that traditional classifiers outperform large language model (LLM) judges, with this advantage being particularly pronounced for low-resource languages, where translate-classify methods dominate translate-judge approaches in 6 out of 7 cases. We additionally show that MT-specific fine-tuning on LLMs yields lower refusal rates compared to standard instruction-tuned models, but it can negatively impact toxicity detection accuracy for low-resource languages. These findings offer actionable guidance for practitioners developing scalable multilingual content moderation systems.</li>
</ul>

<h3>Title: Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Roman Kovalchuk, Mariana Romanyshyn, Petro Ivaniuk</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14504">https://arxiv.org/abs/2509.14504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14504">https://arxiv.org/pdf/2509.14504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14504]] Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction(https://arxiv.org/abs/2509.14504)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce OmniGEC, a collection of multilingual silver-standard datasets for the task of Grammatical Error Correction (GEC), covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic, Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate the development of multilingual GEC solutions and help bridge the data gap in adapting English GEC solutions to multilingual GEC. The texts in the datasets originate from three sources: Wikipedia edits for the eleven target languages, subreddits from Reddit in the eleven target languages, and the Ukrainian-only UberText 2.0 social media corpus. While Wikipedia edits were derived from human-made corrections, the Reddit and UberText 2.0 data were automatically corrected with the GPT-4o-mini model. The quality of the corrections in the datasets was evaluated both automatically and manually. Finally, we fine-tune two open-source large language models - Aya-Expanse (8B) and Gemma-3 (12B) - on the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results for paragraph-level multilingual GEC. The dataset collection and the best-performing models are available on Hugging Face.</li>
</ul>

<h3>Title: BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Wadduwage Shanika Perera, Haodi Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14519">https://arxiv.org/abs/2509.14519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14519">https://arxiv.org/pdf/2509.14519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14519]] BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning(https://arxiv.org/abs/2509.14519)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Malware is becoming increasingly complex and widespread, making it essential to develop more effective and timely detection methods. Traditional static analysis often fails to defend against modern threats that employ code obfuscation, polymorphism, and other evasion techniques. In contrast, behavioral malware detection, which monitors runtime activities, provides a more reliable and context-aware solution. In this work, we propose BEACON, a novel deep learning framework that leverages large language models (LLMs) to generate dense, contextual embeddings from raw sandbox-generated behavior reports. These embeddings capture semantic and structural patterns of each sample and are processed by a one-dimensional convolutional neural network (1D CNN) for multi-class malware classification. Evaluated on the Avast-CTU Public CAPE Dataset, our framework consistently outperforms existing methods, highlighting the effectiveness of LLM-based behavioral embeddings and the overall design of BEACON for robust malware classification.</li>
</ul>

<h3>Title: Delta Knowledge Distillation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yihan Cao, Yanbin Kang, Zhengming Xing, Ruijie Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14526">https://arxiv.org/abs/2509.14526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14526">https://arxiv.org/pdf/2509.14526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14526]] Delta Knowledge Distillation for Large Language Models(https://arxiv.org/abs/2509.14526)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is a widely adopted approach for compressing large neural networks by transferring knowledge from a large teacher model to a smaller student model. In the context of large language models, token level KD, typically minimizing the KL divergence between student output distribution and teacher output distribution, has shown strong empirical performance. However, prior work assumes student output distribution and teacher output distribution share the same optimal representation space, a premise that may not hold in many cases. To solve this problem, we propose Delta Knowledge Distillation (Delta-KD), a novel extension of token level KD that encourages the student to approximate an optimal representation space by explicitly preserving the distributional shift Delta introduced during the teacher's supervised finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD substantially improves student performance while preserving more of the teacher's knowledge.</li>
</ul>

<h3>Title: Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors</h3>
<ul>
<li><strong>Authors: </strong>Zhengxiang Wang, Nafis Irtiza Tripto, Solha Park, Zhenzhen Li, Jiawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14543">https://arxiv.org/abs/2509.14543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14543">https://arxiv.org/pdf/2509.14543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14543]] Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors(https://arxiv.org/abs/2509.14543)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into personal writing tools, a critical question arises: can LLMs faithfully imitate an individual's writing style from just a few examples? Personal style is often subtle and implicit, making it difficult to specify through prompts yet essential for user-aligned generation. This work presents a comprehensive evaluation of state-of-the-art LLMs' ability to mimic personal writing styles via in-context learning from a small number of user-authored samples. We introduce an ensemble of complementary metrics-including authorship attribution, authorship verification, style matching, and AI detection-to robustly assess style imitation. Our evaluation spans over 40000 generations per model across domains such as news, email, forums, and blogs, covering writing samples from more than 400 real-world authors. Results show that while LLMs can approximate user styles in structured formats like news and email, they struggle with nuanced, informal writing in blogs and forums. Further analysis on various prompting strategies such as number of demonstrations reveal key limitations in effective personalization. Our findings highlight a fundamental gap in personalized LLM adaptation and the need for improved techniques to support implicit, style-consistent generation. To aid future research and for reproducibility, we open-source our data and code.</li>
</ul>

<h3>Title: Controlling Language Difficulty in Dialogues with Linguistic Features</h3>
<ul>
<li><strong>Authors: </strong>Shuyao Xu, Wenguang Wang, Handong Gao, Wei Kang, Long Qin, Weizhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14545">https://arxiv.org/abs/2509.14545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14545">https://arxiv.org/pdf/2509.14545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14545]] Controlling Language Difficulty in Dialogues with Linguistic Features(https://arxiv.org/abs/2509.14545)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as powerful tools for supporting second language acquisition, particularly in simulating interactive dialogues for speaking practice. However, adapting the language difficulty of LLM-generated responses to match learners' proficiency levels remains a challenge. This work addresses this issue by proposing a framework for controlling language proficiency in educational dialogue systems. Our approach leverages three categories of linguistic features, readability features (e.g., Flesch-Kincaid Grade Level), syntactic features (e.g., syntactic tree depth), and lexical features (e.g., simple word ratio), to quantify and regulate text complexity. We demonstrate that training LLMs on linguistically annotated dialogue data enables precise modulation of language proficiency, outperforming prompt-based methods in both flexibility and stability. To evaluate this, we introduce Dilaprix, a novel metric integrating the aforementioned features, which shows strong correlation with expert judgments of language difficulty. Empirical results reveal that our approach achieves superior controllability of language proficiency while maintaining high dialogue quality.</li>
</ul>

<h3>Title: LLM Jailbreak Detection for (Almost) Free!</h3>
<ul>
<li><strong>Authors: </strong>Guorui Chen, Yifan Xia, Xiaojun Jia, Zhijiang Li, Philip Torr, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14558">https://arxiv.org/abs/2509.14558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14558">https://arxiv.org/pdf/2509.14558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14558]] LLM Jailbreak Detection for (Almost) Free!(https://arxiv.org/abs/2509.14558)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) enhance security through alignment when widely used, but remain susceptible to jailbreak attacks capable of producing inappropriate content. Jailbreak detection methods show promise in mitigating jailbreak attacks through the assistance of other models or multiple model inferences. However, existing methods entail significant computational costs. In this paper, we first present a finding that the difference in output distributions between jailbreak and benign prompts can be employed for detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak Detection (FJD) which prepends an affirmative instruction to the input and scales the logits by temperature to further distinguish between jailbreak and benign prompts through the confidence of the first token. Furthermore, we enhance the detection performance of FJD through the integration of virtual instruction learning. Extensive experiments on aligned LLMs show that our FJD can effectively detect jailbreak prompts with almost no additional computational costs during LLM inference.</li>
</ul>

<h3>Title: Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Zhaonan Wang, Manyi Li, ShiQing Xin, Changhe Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14560">https://arxiv.org/abs/2509.14560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14560">https://arxiv.org/pdf/2509.14560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14560]] Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model(https://arxiv.org/abs/2509.14560)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Point cloud denoising task aims to recover the clean point cloud from the scanned data coupled with different levels or patterns of noise. The recent state-of-the-art methods often train deep neural networks to update the point locations towards the clean point cloud, and empirically repeat the denoising process several times in order to obtain the denoised results. It is not clear how to efficiently arrange the iterative denoising processes to deal with different levels or patterns of noise. In this paper, we propose an adaptive and iterative point cloud denoising method based on the score-based diffusion model. For a given noisy point cloud, we first estimate the noise variation and determine an adaptive denoising schedule with appropriate step sizes, then invoke the trained network iteratively to update point clouds following the adaptive schedule. To facilitate this adaptive and iterative denoising process, we design the network architecture and a two-stage sampling strategy for the network training to enable feature fusion and gradient fusion for iterative denoising. Compared to the state-of-the-art point cloud denoising methods, our approach obtains clean and smooth denoised point clouds, while preserving the shape boundary and details better. Our results not only outperform the other methods both qualitatively and quantitatively, but also are preferable on the synthetic dataset with different patterns of noises, as well as the real-scanned dataset.</li>
</ul>

<h3>Title: LiMuon: Light and Fast Muon Optimizer for Large Models</h3>
<ul>
<li><strong>Authors: </strong>Feihu Huang, Yuning Luo, Songcan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14562">https://arxiv.org/abs/2509.14562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14562">https://arxiv.org/pdf/2509.14562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14562]] LiMuon: Light and Fast Muon Optimizer for Large Models(https://arxiv.org/abs/2509.14562)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large models recently are widely applied in artificial intelligence, so efficient training of large models has received widespread attention. More recently, a useful Muon optimizer is specifically designed for matrix-structured parameters of large models. Although some works have begun to studying Muon optimizer, the existing Muon and its variants still suffer from high sample complexity or high memory for large models. To fill this gap, we propose a light and fast Muon (LiMuon) optimizer for training large models, which builds on the momentum-based variance reduced technique and randomized Singular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory than the current Muon and its variants. Moreover, we prove that our LiMuon has a lower sample complexity of $O(\epsilon^{-3})$ for finding an $\epsilon$-stationary solution of non-convex stochastic optimization under the smooth condition. Recently, the existing convergence analysis of Muon optimizer mainly relies on the strict Lipschitz smooth assumption, while some artificial intelligence tasks such as training large language models (LLMs) do not satisfy this condition. We also proved that our LiMuon optimizer has a sample complexity of $O(\epsilon^{-3})$ under the generalized smooth condition. Numerical experimental results on training DistilGPT2 and ViT models verify efficiency of our LiMuon optimizer.</li>
</ul>

<h3>Title: Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Shiyuan Luo, Runlong Yu, Chonghao Qiu, Rahul Ghosh, Robert Ladwig, Paul C. Hanson, Yiqun Xie, Xiaowei Jia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14563">https://arxiv.org/abs/2509.14563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14563">https://arxiv.org/pdf/2509.14563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14563]] Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework(https://arxiv.org/abs/2509.14563)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The discovery of environmental knowledge depends on labeled task-specific data, but is often constrained by the high cost of data collection. Existing machine learning approaches usually struggle to generalize in data-sparse or atypical conditions. To this end, we propose an Augmentation-Adaptive Self-Supervised Learning (A$^2$SL) framework, which retrieves relevant observational samples to enhance modeling of the target ecosystem. Specifically, we introduce a multi-level pairwise learning loss to train a scenario encoder that captures varying degrees of similarity among scenarios. These learned similarities drive a retrieval mechanism that supplements a target scenario with relevant data from different locations or time periods. Furthermore, to better handle variable scenarios, particularly under atypical or extreme conditions where traditional models struggle, we design an augmentation-adaptive mechanism that selectively enhances these scenarios through targeted data augmentation. Using freshwater ecosystems as a case study, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen dynamics in real-world lakes. Experimental results show that A$^2$SL significantly improves predictive accuracy and enhances robustness in data-scarce and atypical scenarios. Although this study focuses on freshwater ecosystems, the A$^2$SL framework offers a broadly applicable solution in various scientific domains.</li>
</ul>

<h3>Title: DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising</h3>
<ul>
<li><strong>Authors: </strong>Li Gao, Hongyang Sun, Liu Liu, Yunhao Li, Yang Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14565">https://arxiv.org/abs/2509.14565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14565">https://arxiv.org/pdf/2509.14565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14565]] DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising(https://arxiv.org/abs/2509.14565)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird's-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods.</li>
</ul>

<h3>Title: DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Leon Suarez-Rodriguez, Roman Jacome, Romario Gualdron-Hurtado, Ana Mantilla-Dulcey, Henry Arguello</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14566">https://arxiv.org/abs/2509.14566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14566">https://arxiv.org/pdf/2509.14566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14566]] DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction(https://arxiv.org/abs/2509.14566)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Sparse-view computed tomography (CT) reconstruction is fundamentally challenging due to undersampling, leading to an ill-posed inverse problem. Traditional iterative methods incorporate handcrafted or learned priors to regularize the solution but struggle to capture the complex structures present in medical images. In contrast, diffusion models (DMs) have recently emerged as powerful generative priors that can accurately model complex image distributions. In this work, we introduce Diffusion Consensus Equilibrium (DICE), a framework that integrates a two-agent consensus equilibrium into the sampling process of a DM. DICE alternates between: (i) a data-consistency agent, implemented through a proximal operator enforcing measurement consistency, and (ii) a prior agent, realized by a DM performing a clean image estimation at each sampling step. By balancing these two complementary agents iteratively, DICE effectively combines strong generative prior capabilities with measurement consistency. Experimental results show that DICE significantly outperforms state-of-the-art baselines in reconstructing high-quality CT images under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out of a total of 180), demonstrating both its effectiveness and robustness.</li>
</ul>

<h3>Title: Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Yang Xu, Junpeng Li, Changchun Hua, Yana Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14577">https://arxiv.org/abs/2509.14577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14577">https://arxiv.org/pdf/2509.14577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14577]] Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition(https://arxiv.org/abs/2509.14577)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Large Margin Distribution Machine (LMDM) is a recent advancement in classifier design that optimizes not just the minimum margin (as in SVM) but the entire margin distribution, thereby improving generalization. However, existing LMDM formulations are limited to vectorized inputs and struggle with high-dimensional tensor data due to the need for flattening, which destroys the data's inherent multi-mode structure and increases computational burden. In this paper, we propose a Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates directly on tensor representations without vectorization. The SPMD-LRT preserves multi-dimensional spatial structure by incorporating first-order and second-order tensor statistics (margin mean and variance) into the objective, and it leverages low-rank tensor decomposition techniques including rank-1(CP), higher-rank CP, and Tucker decomposition to parameterize the weight tensor. An alternating optimization (double-gradient descent) algorithm is developed to efficiently solve the SPMD-LRT, iteratively updating factor matrices and core tensor. This approach enables SPMD-LRT to maintain the structural information of high-order data while optimizing margin distribution for improved classification. Extensive experiments on diverse datasets (including MNIST, images and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior classification accuracy compared to conventional SVM, vector-based LMDM, and prior tensor-based SVM extensions (Support Tensor Machines and Support Tucker Machines). Notably, SPMD-LRT with Tucker decomposition attains the highest accuracy, highlighting the benefit of structure preservation. These results confirm the effectiveness and robustness of SPMD-LRT in handling high-dimensional tensor data for classification.</li>
</ul>

<h3>Title: What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System</h3>
<ul>
<li><strong>Authors: </strong>Johnny So, Michael Ferdman, Nick Nikiforakis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14583">https://arxiv.org/abs/2509.14583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14583">https://arxiv.org/pdf/2509.14583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14583]] What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System(https://arxiv.org/abs/2509.14583)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>The web continues to grow, but dependency-monitoring tools and standards for resource integrity lag behind. Currently, there exists no robust method to verify the integrity of web resources, much less in a generalizable yet performant manner, and supply chains remain one of the most targeted parts of the attack surface of web applications. In this paper, we present the design of LiMS, a transparent system to bootstrap link integrity guarantees in web browsing sessions with minimal overhead. At its core, LiMS uses a set of customizable integrity policies to declare the (un)expected properties of resources, verifies these policies, and enforces them for website visitors. We discuss how basic integrity policies can serve as building blocks for a comprehensive set of integrity policies, while providing guarantees that would be sufficient to defend against recent supply chain attacks detailed by security industry reports. Finally, we evaluate our open-sourced prototype by simulating deployments on a representative sample of 450 domains that are diverse in ranking and category. We find that our proposal offers the ability to bootstrap marked security improvements with an overall overhead of hundreds of milliseconds on initial page loads, and negligible overhead on reloads, regardless of network speeds. In addition, from examining archived data for the sample sites, we find that several of the proposed policy building blocks suit their dependency usage patterns, and would incur minimal administrative overhead.</li>
</ul>

<h3>Title: ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System</h3>
<ul>
<li><strong>Authors: </strong>Taesoo Kim, HyungSeok Han, Soyeon Park, Dae R. Jeong, Dohyeok Kim, Dongkwan Kim, Eunsoo Kim, Jiho Kim, Joshua Wang, Kangsu Kim, Sangwoo Ji, Woosun Song, Hanqing Zhao, Andrew Chin, Gyejin Lee, Kevin Stevens, Mansour Alharthi, Yizhuo Zhai, Cen Zhang, Joonun Jang, Yeongjin Jang, Ammar Askar, Dongju Kim, Fabian Fleischer, Jeongin Cho, Junsik Kim, Kyungjoon Ko, Insu Yun, Sangdon Park, Dowoo Baik, Haein Lee, Hyeon Heo, Minjae Gwon, Minjae Lee, Minwoo Baek, Seunggi Min, Wonyoung Kim, Yonghwi Jin, Younggi Park, Yunjae Choi, Jinho Jung, Gwanhyun Lee, Junyoung Jang, Kyuheon Kim, Yeonghyeon Cha, Youngjoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14589">https://arxiv.org/abs/2509.14589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14589">https://arxiv.org/pdf/2509.14589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14589]] ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System(https://arxiv.org/abs/2509.14589)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>We present ATLANTIS, the cyber reasoning system developed by Team Atlanta that won 1st place in the Final Competition of DARPA's AI Cyber Challenge (AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to build autonomous cyber reasoning systems capable of discovering and patching vulnerabilities at the speed and scale of modern software. ATLANTIS integrates large language models (LLMs) with program analysis -- combining symbolic execution, directed fuzzing, and static analysis -- to address limitations in automated vulnerability discovery and program repair. Developed by researchers at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the system addresses core challenges: scaling across diverse codebases from C to Java, achieving high precision while maintaining broad coverage, and producing semantically correct patches that preserve intended behavior. We detail the design philosophy, architectural decisions, and implementation strategies behind ATLANTIS, share lessons learned from pushing the boundaries of automated security when program analysis meets modern AI, and release artifacts to support reproducibility and future research.</li>
</ul>

<h3>Title: Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seungjun Yi, Joakim Nguyen, Terence Lim, Andrew Well, Joseph Skrovan, Mehak Beri, YongGeon Lee, Kavita Radhakrishnan, Liu Leqi, Mia Markey, Ying Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14597">https://arxiv.org/abs/2509.14597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14597">https://arxiv.org/pdf/2509.14597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14597]] Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models(https://arxiv.org/abs/2509.14597)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This position paper examines how large language models (LLMs) can support thematic analysis of unstructured clinical transcripts, a widely used but resource-intensive method for uncovering patterns in patient and provider narratives. We conducted a systematic review of recent studies applying LLMs to thematic analysis, complemented by an interview with a practicing clinician. Our findings reveal that current approaches remain fragmented across multiple dimensions including types of thematic analysis, datasets, prompting strategies and models used, most notably in evaluation. Existing evaluation methods vary widely (from qualitative expert review to automatic similarity metrics), hindering progress and preventing meaningful benchmarking across studies. We argue that establishing standardized evaluation practices is critical for advancing the field. To this end, we propose an evaluation framework centered on three dimensions: validity, reliability, and interpretability.</li>
</ul>

<h3>Title: Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking</h3>
<ul>
<li><strong>Authors: </strong>Xingchen Wang, Feijie Wu, Chenglin Miao, Tianchun Li, Haoyu Hu, Qiming Cao, Jing Gao, Lu Su</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14603">https://arxiv.org/abs/2509.14603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14603">https://arxiv.org/pdf/2509.14603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14603]] Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking(https://arxiv.org/abs/2509.14603)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Split Federated Learning (SFL) has emerged as an efficient alternative to traditional Federated Learning (FL) by reducing client-side computation through model partitioning. However, exchanging of intermediate activations and model updates introduces significant privacy risks, especially from data reconstruction attacks that recover original inputs from intermediate representations. Existing defenses using noise injection often degrade model performance. To overcome these challenges, we present PM-SFL, a scalable and privacy-preserving SFL framework that incorporates Probabilistic Mask training to add structured randomness without relying on explicit noise. This mitigates data reconstruction risks while maintaining model utility. To address data heterogeneity, PM-SFL employs personalized mask learning that tailors submodel structures to each client's local data. For system heterogeneity, we introduce a layer-wise knowledge compensation mechanism, enabling clients with varying resources to participate effectively under adaptive model splitting. Theoretical analysis confirms its privacy protection, and experiments on image and wireless sensing tasks demonstrate that PM-SFL consistently improves accuracy, communication efficiency, and robustness to privacy attacks, with particularly strong performance under data and system heterogeneity.</li>
</ul>

<h3>Title: Threats and Security Strategies for IoMT Infusion Pumps</h3>
<ul>
<li><strong>Authors: </strong>Ramazan Yener, Muhammad Hassan, Masooda Bashir</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14604">https://arxiv.org/abs/2509.14604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14604">https://arxiv.org/pdf/2509.14604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14604]] Threats and Security Strategies for IoMT Infusion Pumps(https://arxiv.org/abs/2509.14604)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>The integration of the Internet of Medical Things (IoMT) into healthcare systems has transformed patient care by enabling real-time monitoring, enhanced diagnostics, and enhanced operational efficiency. However, this increased connectivity has also expanded the attack surface for cybercriminals, raising significant cybersecurity and privacy concerns. This study focuses on the cybersecurity vulnerabilities of IoMT infusion pumps, which are critical devices in modern healthcare. Through a targeted literature review of the past five years, we analyzed seven current studies from a pool of 132 papers to identify security vulnerabilities. Our findings indicate that infusion pumps face vulnerabilities such as device-level flaws, authentication and access control issues, network and communication weaknesses, data security and privacy risks, and operational or organizational challenges that can expose them to lateral attacks within healthcare networks. Our analysis synthesizes findings from seven recent studies to clarify how and why infusion pumps remain vulnerable in each of these areas. By categorizing the security gaps, we highlight critical risk patterns and their implications. This work underscores the scope of the issue and provides a structured understanding that is valuable for healthcare IT professionals and device manufacturers. Ultimately, the findings can inform the development of targeted, proactive security strategies to better safeguard infusion pumps and protect patient well-being.</li>
</ul>

<h3>Title: Enterprise AI Must Enforce Participant-Aware Access Control</h3>
<ul>
<li><strong>Authors: </strong>Shashank Shreedhar Bhatt, Tanmay Rajore, Khushboo Aggarwal, Ganesh Ananthanarayanan, Ranveer Chandra, Nishanth Chandran, Suyash Choudhury, Divya Gupta, Emre Kiciman, Sumit Kumar Pandey, Srinath Setty, Rahul Sharma, Teijia Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14608">https://arxiv.org/abs/2509.14608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14608">https://arxiv.org/pdf/2509.14608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14608]] Enterprise AI Must Enforce Participant-Aware Access Control(https://arxiv.org/abs/2509.14608)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed in enterprise settings where they interact with multiple users and are trained or fine-tuned on sensitive internal data. While fine-tuning enhances performance by internalizing domain knowledge, it also introduces a critical security risk: leakage of confidential training data to unauthorized users. These risks are exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG) pipelines that dynamically fetch contextual documents at inference time. We demonstrate data exfiltration attacks on AI assistants where adversaries can exploit current fine-tuning and RAG architectures to leak sensitive information by leveraging the lack of access control enforcement. We show that existing defenses, including prompt sanitization, output filtering, system isolation, and training-level privacy mechanisms, are fundamentally probabilistic and fail to offer robust protection against such attacks. We take the position that only a deterministic and rigorous enforcement of fine-grained access control during both fine-tuning and RAG-based inference can reliably prevent the leakage of sensitive data to unauthorized recipients. We introduce a framework centered on the principle that any content used in training, retrieval, or generation by an LLM is explicitly authorized for \emph{all users involved in the interaction}. Our approach offers a simple yet powerful paradigm shift for building secure multi-user LLM systems that are grounded in classical access control but adapted to the unique challenges of modern AI workflows. Our solution has been deployed in Microsoft Copilot Tuning, a product offering that enables organizations to fine-tune models using their own enterprise-specific data.</li>
</ul>

<h3>Title: HybridMamba: A Dual-domain Mamba for 3D Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Weitong Wu, Zhaohu Xing, Jing Gong, Qin Peng, Lei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14609">https://arxiv.org/abs/2509.14609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14609">https://arxiv.org/pdf/2509.14609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14609]] HybridMamba: A Dual-domain Mamba for 3D Medical Image Segmentation(https://arxiv.org/abs/2509.14609)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In the domain of 3D biomedical image segmentation, Mamba exhibits the superior performance for it addresses the limitations in modeling long-range dependencies inherent to CNNs and mitigates the abundant computational overhead associated with Transformer-based frameworks when processing high-resolution medical volumes. However, attaching undue importance to global context modeling may inadvertently compromise critical local structural information, thus leading to boundary ambiguity and regional distortion in segmentation outputs. Therefore, we propose the HybridMamba, an architecture employing dual complementary mechanisms: 1) a feature scanning strategy that progressively integrates representations both axial-traversal and local-adaptive pathways to harmonize the relationship between local and global representations, and 2) a gated module combining spatial-frequency analysis for comprehensive contextual modeling. Besides, we collect a multi-center CT dataset related to lung cancer. Experiments on MRI and CT datasets demonstrate that HybridMamba significantly outperforms the state-of-the-art methods in 3D medical image segmentation.</li>
</ul>

<h3>Title: Enhancing Feature Fusion of U-like Networks with Dynamic Skip Connections</h3>
<ul>
<li><strong>Authors: </strong>Yue Cao, Quansong He, Kaishen Wang, Jianlong Xiong, Tao He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14610">https://arxiv.org/abs/2509.14610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14610">https://arxiv.org/pdf/2509.14610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14610]] Enhancing Feature Fusion of U-like Networks with Dynamic Skip Connections(https://arxiv.org/abs/2509.14610)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>U-like networks have become fundamental frameworks in medical image segmentation through skip connections that bridge high-level semantics and low-level spatial details. Despite their success, conventional skip connections exhibit two key limitations: inter-feature constraints and intra-feature constraints. The inter-feature constraint refers to the static nature of feature fusion in traditional skip connections, where information is transmitted along fixed pathways regardless of feature content. The intra-feature constraint arises from the insufficient modeling of multi-scale feature interactions, thereby hindering the effective aggregation of global contextual information. To overcome these limitations, we propose a novel Dynamic Skip Connection (DSC) block that fundamentally enhances cross-layer connectivity through adaptive mechanisms. The DSC block integrates two complementary components. (1) Test-Time Training (TTT) module. This module addresses the inter-feature constraint by enabling dynamic adaptation of hidden representations during inference, facilitating content-aware feature refinement. (2) Dynamic Multi-Scale Kernel (DMSK) module. To mitigate the intra-feature constraint, this module adaptively selects kernel sizes based on global contextual cues, enhancing the network capacity for multi-scale feature integration. The DSC block is architecture-agnostic and can be seamlessly incorporated into existing U-like network structures. Extensive experiments demonstrate the plug-and-play effectiveness of the proposed DSC block across CNN-based, Transformer-based, hybrid CNN-Transformer, and Mamba-based U-like networks.</li>
</ul>

<h3>Title: HD3C: Efficient Medical Data Classification for Embedded Devices</h3>
<ul>
<li><strong>Authors: </strong>Jianglan Wei, Zhenyu Zhang, Pengcheng Wang, Mingjie Zeng, Zhigang Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14617">https://arxiv.org/abs/2509.14617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14617">https://arxiv.org/pdf/2509.14617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14617]] HD3C: Efficient Medical Data Classification for Embedded Devices(https://arxiv.org/abs/2509.14617)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Energy-efficient medical data classification is essential for modern disease screening, particularly in home and field healthcare where embedded devices are prevalent. While deep learning models achieve state-of-the-art accuracy, their substantial energy consumption and reliance on GPUs limit deployment on such platforms. We present Hyperdimensional Computing with Class-Wise Clustering (HD3C), a lightweight classification framework designed for low-power environments. HD3C encodes data into high-dimensional hypervectors, aggregates them into multiple cluster-specific prototypes, and performs classification through similarity search in hyperspace. We evaluate HD3C across three medical classification tasks; on heart sound classification, HD3C is $350\times$ more energy-efficient than Bayesian ResNet with less than 1% accuracy difference. Moreover, HD3C demonstrates exceptional robustness to noise, limited training data, and hardware error, supported by both theoretical analysis and empirical results, highlighting its potential for reliable deployment in real-world settings. Code is available at this https URL.</li>
</ul>

<h3>Title: Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection</h3>
<ul>
<li><strong>Authors: </strong>Yihao Guo, Haocheng Bian, Liutong Zhou, Ze Wang, Zhaoyi Zhang, Francois Kawala, Milan Dean, Ian Fischer, Yuantao Peng, Noyan Tokgozoglu, Ivan Barrientos, Riyaaz Shaik, Rachel Li, Chandru Venkataraman, Reza Shifteh Far, Moses Pawar, Venkat Sundaranatha, Michael Xu, Frank Chu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14622">https://arxiv.org/abs/2509.14622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14622">https://arxiv.org/pdf/2509.14622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14622]] Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection(https://arxiv.org/abs/2509.14622)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the deployment of Large Language Models (LLMs) in interactive applications, online malicious intent detection has become increasingly critical. However, existing approaches fall short of handling diverse and complex user queries in real time. To address these challenges, we introduce ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework for robust and efficient online malicious intent detection. In the training stage, a high-capacity teacher model is trained on adversarially perturbed, retrieval-augmented inputs to learn robust decision boundaries over diverse and complex user queries. In the inference stage, a distillation scheduler transfers the teacher's knowledge into a compact student model, with a continually updated knowledge base collected online. At deployment, the compact student model leverages top-K similar safety exemplars retrieved from the online-updated knowledge base to enable both online and real-time malicious query detection. Evaluations across ten safety benchmarks demonstrate that ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on out-of-distribution detection, while simultaneously delivering up to 5.6x lower latency at 300 queries per second (QPS) in real-time applications.</li>
</ul>

<h3>Title: Reveal and Release: Iterative LLM Unlearning with Self-generated Data</h3>
<ul>
<li><strong>Authors: </strong>Linxi Xie, Xin Teng, Shichang Ke, Hongyi Wen, Shengjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14624">https://arxiv.org/abs/2509.14624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14624">https://arxiv.org/pdf/2509.14624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14624]] Reveal and Release: Iterative LLM Unlearning with Self-generated Data(https://arxiv.org/abs/2509.14624)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) unlearning has demonstrated effectiveness in removing the influence of undesirable data (also known as forget data). Existing approaches typically assume full access to the forget dataset, overlooking two key challenges: (1) Forget data is often privacy-sensitive, rare, or legally regulated, making it expensive or impractical to obtain (2) The distribution of available forget data may not align with how that information is represented within the model. To address these limitations, we propose a ``Reveal-and-Release'' method to unlearn with self-generated data, where we prompt the model to reveal what it knows using optimized instructions. To fully utilize the self-generated forget data, we propose an iterative unlearning framework, where we make incremental adjustments to the model's weight space with parameter-efficient modules trained on the forget data. Experimental results demonstrate that our method balances the tradeoff between forget quality and utility preservation.</li>
</ul>

<h3>Title: CUFG: Curriculum Unlearning Guided by the Forgetting Gradient</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Miao, Liang Hu, Qi Zhang, Lai Zhong Yuan, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14633">https://arxiv.org/abs/2509.14633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14633">https://arxiv.org/pdf/2509.14633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14633]] CUFG: Curriculum Unlearning Guided by the Forgetting Gradient(https://arxiv.org/abs/2509.14633)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>As privacy and security take center stage in AI, machine unlearning, the ability to erase specific knowledge from models, has garnered increasing attention. However, existing methods overly prioritize efficiency and aggressive forgetting, which introduces notable limitations. In particular, radical interventions like gradient ascent, influence functions, and random label noise can destabilize model weights, leading to collapse and reduced reliability. To address this, we propose CUFG (Curriculum Unlearning via Forgetting Gradients), a novel framework that enhances the stability of approximate unlearning through innovations in both forgetting mechanisms and data scheduling strategies. Specifically, CUFG integrates a new gradient corrector guided by forgetting gradients for fine-tuning-based unlearning and a curriculum unlearning paradigm that progressively forgets from easy to hard. These innovations narrow the gap with the gold-standard Retrain method by enabling more stable and progressive unlearning, thereby improving both effectiveness and reliability. Furthermore, we believe that the concept of curriculum unlearning has substantial research potential and offers forward-looking insights for the development of the MU field. Extensive experiments across various forgetting scenarios validate the rationale and effectiveness of our approach and CUFG. Codes are available at this https URL.</li>
</ul>

<h3>Title: MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks</h3>
<ul>
<li><strong>Authors: </strong>Mingsong Li, Lin Liu, Hongjun Wang, Haoxing Chen, Xijun Gu, Shizhan Liu, Dong Gong, Junbo Zhao, Zhenzhong Lan, Jianguo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14638">https://arxiv.org/abs/2509.14638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14638">https://arxiv.org/pdf/2509.14638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14638]] MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks(https://arxiv.org/abs/2509.14638)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at this https URL.</li>
</ul>

<h3>Title: DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers</h3>
<ul>
<li><strong>Authors: </strong>Habib Irani, Vangelis Metsis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14640">https://arxiv.org/abs/2509.14640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14640">https://arxiv.org/pdf/2509.14640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14640]] DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers(https://arxiv.org/abs/2509.14640)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Existing positional encoding methods in transformers are fundamentally signal-agnostic, deriving positional information solely from sequence indices while ignoring the underlying signal characteristics. This limitation is particularly problematic for time series analysis, where signals exhibit complex, non-stationary dynamics across multiple temporal scales. We introduce Dynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework that generates positional embeddings directly from input time series using the Discrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time series datasets demonstrate that DyWPE consistently outperforms eight existing state-of-the-art positional encoding methods, achieving average relative improvements of 9.1\% compared to baseline sinusoidal absolute position encoding in biomedical signals, while maintaining competitive computational efficiency.</li>
</ul>

<h3>Title: DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Yuemin Wu, Zhongze Wu, Xiu Su, Feng Yang, Hongyan Xu, Xi Lin, Wenti Huang, Shan You, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14642">https://arxiv.org/abs/2509.14642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14642">https://arxiv.org/pdf/2509.14642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14642]] DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training(https://arxiv.org/abs/2509.14642)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modeling dynamic temporal dependencies is a critical challenge in time series pre-training, which evolve due to distribution shifts and multi-scale patterns. This temporal variability severely impairs the generalization of pre-trained models to downstream tasks. Existing frameworks fail to capture the complex interactions of short- and long-term dependencies, making them susceptible to spurious correlations that degrade generalization. To address these limitations, we propose DeCoP, a Dependency Controlled Pre-training framework that explicitly models dynamic, multi-scale dependencies by simulating evolving inter-patch dependencies. At the input level, DeCoP introduces Instance-wise Patch Normalization (IPN) to mitigate distributional shifts while preserving the unique characteristics of each patch, creating a robust foundation for representation learning. At the latent level, a hierarchical Dependency Controlled Learning (DCL) strategy explicitly models inter-patch dependencies across multiple temporal scales, with an Instance-level Contrastive Module (ICM) enhances global generalization by learning instance-discriminative representations from time-invariant positive pairs. DeCoP achieves state-of-the-art results on ten datasets with lower computing resources, improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.</li>
</ul>

<h3>Title: MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siyu Yan, Long Zeng, Xuecheng Wu, Chengcheng Han, Kongcheng Zhang, Chong Peng, Xuezhi Cao, Xunliang Cai, Chenjuan Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14651">https://arxiv.org/abs/2509.14651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14651">https://arxiv.org/pdf/2509.14651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14651]] MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models(https://arxiv.org/abs/2509.14651)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models~(LLMs) become widely adopted, ensuring their alignment with human values is crucial to prevent jailbreaks where adversaries manipulate models to produce harmful content. While most defenses target single-turn attacks, real-world usage often involves multi-turn dialogues, exposing models to attacks that exploit conversational context to bypass safety measures. We introduce MUSE, a comprehensive framework tackling multi-turn jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A, a method that uses frame semantics and heuristic tree search to explore diverse semantic trajectories. For defense, we present MUSE-D, a fine-grained safety alignment approach that intervenes early in dialogues to reduce vulnerabilities. Extensive experiments on various models show that MUSE effectively identifies and mitigates multi-turn vulnerabilities. Code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework</h3>
<ul>
<li><strong>Authors: </strong>Sergio Benlloch-Lopez, Miquel Viel-Vazquez, Javier Naranjo-Alcazar, Jordi Grau-Haro, Pedro Zuccarello</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14657">https://arxiv.org/abs/2509.14657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14657">https://arxiv.org/pdf/2509.14657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14657]] Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework(https://arxiv.org/abs/2509.14657)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of IoT nodes equipped with microphones and capable of performing on-device audio classification exposes highly sensitive data while operating under tight resource constraints. To protect against this, we present a defence-in-depth architecture comprising a security protocol that treats the edge device, cellular network and cloud backend as three separate trust domains, linked by TPM-based remote attestation and mutually authenticated TLS 1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At startup, each boot stage is measured into TPM PCRs. The node can only decrypt its LUKS-sealed partitions after the cloud has verified a TPM quote and released a one-time unlock key. This ensures that rogue or tampered devices remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end encryption and integrity hashes safeguard extracted audio features. Signed, rollback-protected AI models and tamper-responsive sensors harden firmware and hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum cipher and an encrypted cloud replica. Finally, we set out a plan for evaluating the physical and logical security of the proposed protocol.</li>
</ul>

<h3>Title: Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Shinnosuke Hirano, Yuiga Wada, Tsumugi Iida, Komei Sugiura</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14664">https://arxiv.org/abs/2509.14664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14664">https://arxiv.org/pdf/2509.14664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14664]] Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model(https://arxiv.org/abs/2509.14664)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In this study, we consider the problem of generating visual explanations in visual foundation models. Numerous methods have been proposed for this purpose; however, they often cannot be applied to complex models due to their lack of adaptability. To overcome these limitations, we propose a novel explanation generation method in visual foundation models that is aimed at both generating explanations and partially updating model parameters to enhance interpretability. Our approach introduces two novel mechanisms: Attention Lattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanism simplifies the process by eliminating the need for manual layer selection, thus enhancing the model's adaptability and interpretability. Moreover, the AEA mechanism, which updates ALA's parameters every other epoch, effectively addresses the common issue of overly small attention regions. We evaluated our method on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our results showed that our method outperformed the baseline methods in terms of mean intersection over union (IoU), insertion score, deletion score, and insertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets. Notably, our best model achieved a 53.2-point improvement in mean IoU on the CUB-200-2011 dataset compared with the baselines.</li>
</ul>

<h3>Title: TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xiaobo Xing, Wei Yuan, Tong Chen, Quoc Viet Hung Nguyen, Xiangliang Zhang, Hongzhi Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14671">https://arxiv.org/abs/2509.14671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14671">https://arxiv.org/pdf/2509.14671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14671]] TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding(https://arxiv.org/abs/2509.14671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within a large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs. In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table-query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: this https URL</li>
</ul>

<h3>Title: Stochastic Clock Attention for Aligning Continuous and Ordered Sequences</h3>
<ul>
<li><strong>Authors: </strong>Hyungjoon Soh, Junghyo Jo</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14678">https://arxiv.org/abs/2509.14678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14678">https://arxiv.org/pdf/2509.14678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14678]] Stochastic Clock Attention for Aligning Continuous and Ordered Sequences(https://arxiv.org/abs/2509.14678)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We formulate an attention mechanism for continuous and ordered sequences that explicitly functions as an alignment model, which serves as the core of many sequence-to-sequence tasks. Standard scaled dot-product attention relies on positional encodings and masks but does not enforce continuity or monotonicity, which are crucial for frame-synchronous targets. We propose learned nonnegative \emph{clocks} to source and target and model attention as the meeting probability of these clocks; a path-integral derivation yields a closed-form, Gaussian-like scoring rule with an intrinsic bias toward causal, smooth, near-diagonal alignments, without external positional regularizers. The framework supports two complementary regimes: normalized clocks for parallel decoding when a global length is available, and unnormalized clocks for autoregressive decoding -- both nearly-parameter-free, drop-in replacements. In a Transformer text-to-speech testbed, this construction produces more stable alignments and improved robustness to global time-scaling while matching or improving accuracy over scaled dot-product baselines. We hypothesize applicability to other continuous targets, including video and temporal signal modeling.</li>
</ul>

<h3>Title: DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images</h3>
<ul>
<li><strong>Authors: </strong>Kazuma Nagata, Naoshi Kaneko</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14685">https://arxiv.org/abs/2509.14685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14685">https://arxiv.org/pdf/2509.14685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14685]] DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images(https://arxiv.org/abs/2509.14685)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction. In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at this https URL.</li>
</ul>

<h3>Title: Security Analysis of Web Applications Based on Gruyere</h3>
<ul>
<li><strong>Authors: </strong>Yonghao Ni, Zhongwen Li, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14706">https://arxiv.org/abs/2509.14706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14706">https://arxiv.org/pdf/2509.14706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14706]] Security Analysis of Web Applications Based on Gruyere(https://arxiv.org/abs/2509.14706)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>With the rapid development of Internet technologies, web systems have become essential infrastructures for modern information exchange and business operations. However, alongside their expansion, numerous security vulnerabilities have emerged, making web security a critical research focus within the broader field of cybersecurity. These issues are closely related to data protection, privacy preservation, and business continuity, and systematic research on web security is crucial for mitigating malicious attacks and enhancing the reliability and robustness of network systems. This paper first reviews the OWASP Top 10, summarizing the types, causes, and impacts of common web vulnerabilities, and illustrates their exploitation mechanisms through representative cases. Building upon this, the Gruyere platform is adopted as an experimental subject for analyzing known vulnerabilities. The study presents detailed reproduction steps for specific vulnerabilities, proposes comprehensive remediation strategies, and further compares Gruyere's vulnerabilities with contemporary real-world cases. The findings suggest that, although Gruyere's vulnerabilities are relatively outdated, their underlying principles remain highly relevant for explaining a wide range of modern security flaws. Overall, this research demonstrates that web system security analysis based on Gruyere not only deepens the understanding of vulnerability mechanisms but also provides practical support for technological innovation and security defense.</li>
</ul>

<h3>Title: Towards Pre-trained Graph Condensation via Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Yeyu Yan, Shuai Zheng, Wenjun Hui, Xiangkai Zhu, Dong Chen, Zhenfeng Zhu, Yao Zhao, Kunlun He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14722">https://arxiv.org/abs/2509.14722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14722">https://arxiv.org/pdf/2509.14722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14722]] Towards Pre-trained Graph Condensation via Optimal Transport(https://arxiv.org/abs/2509.14722)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph condensation (GC) aims to distill the original graph into a small-scale graph, mitigating redundancy and accelerating GNN training. However, conventional GC approaches heavily rely on rigid GNNs and task-specific supervision. Such a dependency severely restricts their reusability and generalization across various tasks and architectures. In this work, we revisit the goal of ideal GC from the perspective of GNN optimization consistency, and then a generalized GC optimization objective is derived, by which those traditional GC methods can be viewed nicely as special cases of this optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC) via optimal transport is proposed to transcend the limitations of task- and architecture-dependent GC methods. Specifically, a hybrid-interval graph diffusion augmentation is presented to suppress the weak generalization ability of the condensed graph on particular architectures by enhancing the uncertainty of node states. Meanwhile, the matching between optimal graph transport plan and representation transport plan is tactfully established to maintain semantic consistencies across source graph and condensed graph spaces, thereby freeing graph condensation from task dependencies. To further facilitate the adaptation of condensed graphs to various downstream tasks, a traceable semantic harmonizer from source nodes to condensed nodes is proposed to bridge semantic associations through the optimized representation transport plan in pre-training. Extensive experiments verify the superiority and versatility of PreGC, demonstrating its task-independent nature and seamless compatibility with arbitrary GNNs.</li>
</ul>

<h3>Title: Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Sosuke Hosokawa, Toshiharu Kawakami, Satoshi Kodera, Masamichi Ito, Norihiko Takeda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14723">https://arxiv.org/abs/2509.14723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14723">https://arxiv.org/pdf/2509.14723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14723]] Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models(https://arxiv.org/abs/2509.14723)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Single-cell foundation models (scFMs) have demonstrated state-of-the-art performance on various tasks, such as cell-type annotation and perturbation response prediction, by learning gene regulatory networks from large-scale transcriptome data. However, a significant challenge remains: the decision-making processes of these models are less interpretable compared to traditional methods like differential gene expression analysis. Recently, transcoders have emerged as a promising approach for extracting interpretable decision circuits from large language models (LLMs). In this work, we train a transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By leveraging the trained transcoder, we extract internal decision-making circuits from the C2S model. We demonstrate that the discovered circuits correspond to real-world biological mechanisms, confirming the potential of transcoders to uncover biologically plausible pathways within complex single-cell models.</li>
</ul>

<h3>Title: Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM</h3>
<ul>
<li><strong>Authors: </strong>Chenkun Tan, Pengyu Wang, Shaojun Zhou, Botian Jiang, Zhaowei Li, Dong Zhang, Xinghao Wang, Yaqian Zhou, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14735">https://arxiv.org/abs/2509.14735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14735">https://arxiv.org/pdf/2509.14735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14735]] Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM(https://arxiv.org/abs/2509.14735)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have gained significant attention due to their impressive ability to integrate vision and language modalities. Recent advancements in MLLMs have primarily focused on improving performance through high-quality datasets, novel architectures, and optimized training strategies. However, in this paper, we identify a previously overlooked issue, language prior conflict, a mismatch between the inherent language priors of large language models (LLMs) and the language priors in training datasets. This conflict leads to suboptimal vision-language alignment, as MLLMs are prone to adapting to the language style of training samples. To address this issue, we propose a novel training method called Decoupled Proxy Alignment (DPA). DPA introduces two key innovations: (1) the use of a proxy LLM during pretraining to decouple the vision-language alignment process from language prior interference, and (2) dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens. Extensive experiments demonstrate that DPA significantly mitigates the language prior conflict, achieving superior alignment performance across diverse datasets, model families, and scales. Our method not only improves the effectiveness of MLLM training but also shows exceptional generalization capabilities, making it a robust approach for vision-language alignment. Our code is available at this https URL.</li>
</ul>

<h3>Title: UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Wang, Shaojun Zhou, Chenkun Tan, Xinghao Wang, Wei Huang, Zhen Ye, Zhaowei Li, Botian Jiang, Dong Zhang, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14738">https://arxiv.org/abs/2509.14738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14738">https://arxiv.org/pdf/2509.14738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14738]] UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets(https://arxiv.org/abs/2509.14738)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Unified vision large language models (VLLMs) have recently achieved impressive advancements in both multimodal understanding and generation, powering applications such as visual question answering and text-guided image synthesis. However, progress in unified VLLMs remains constrained by the lack of datasets that fully exploit the synergistic potential between these two core abilities. Existing datasets typically address understanding and generation in isolation, thereby limiting the performance of unified VLLMs. To bridge this critical gap, we introduce a novel dataset construction framework, UnifiedVisual, and present UnifiedVisual-240K, a high-quality dataset meticulously designed to facilitate mutual enhancement between multimodal understanding and generation. UnifiedVisual-240K seamlessly integrates diverse visual and textual inputs and outputs, enabling comprehensive cross-modal reasoning and precise text-to-image alignment. Our dataset encompasses a wide spectrum of tasks and data sources, ensuring rich diversity and addressing key shortcomings of prior resources. Extensive experiments demonstrate that models trained on UnifiedVisual-240K consistently achieve strong performance across a wide range of tasks. Notably, these models exhibit significant mutual reinforcement between multimodal understanding and generation, further validating the effectiveness of our framework and dataset. We believe UnifiedVisual represents a new growth point for advancing unified VLLMs and unlocking their full potential. Our code and datasets is available at this https URL.</li>
</ul>

<h3>Title: Chain-of-Thought Re-ranking for Image Retrieval Tasks</h3>
<ul>
<li><strong>Authors: </strong>Shangrong Wu, Yanghong Zhou, Yang Chen, Feng Zhang, P. Y. Mok</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14746">https://arxiv.org/abs/2509.14746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14746">https://arxiv.org/pdf/2509.14746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14746]] Chain-of-Thought Re-ranking for Image Retrieval Tasks(https://arxiv.org/abs/2509.14746)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Image retrieval remains a fundamental yet challenging problem in computer vision. While recent advances in Multimodal Large Language Models (MLLMs) have demonstrated strong reasoning capabilities, existing methods typically employ them only for evaluation, without involving them directly in the ranking process. As a result, their rich multimodal reasoning abilities remain underutilized, leading to suboptimal performance. In this paper, we propose a novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue. Specifically, we design a listwise ranking prompt that enables MLLM to directly participate in re-ranking candidate images. This ranking process is grounded in an image evaluation prompt, which assesses how well each candidate aligns with users query. By allowing MLLM to perform listwise reasoning, our method supports global comparison, consistent reasoning, and interpretable decision-making - all of which are essential for accurate image retrieval. To enable structured and fine-grained analysis, we further introduce a query deconstruction prompt, which breaks down the original query into multiple semantic components. Extensive experiments on five datasets demonstrate the effectiveness of our CoTRR method, which achieves state-of-the-art performance across three image retrieval tasks, including text-to-image retrieval (TIR), composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our code is available at this https URL .</li>
</ul>

<h3>Title: Evaluating Large Language Models for Cross-Lingual Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Longfei Zuo, Pingjun Hong, Oliver Kraus, Barbara Plank, Robert Litschko</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14749">https://arxiv.org/abs/2509.14749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14749">https://arxiv.org/pdf/2509.14749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14749]] Evaluating Large Language Models for Cross-Lingual Retrieval(https://arxiv.org/abs/2509.14749)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-stage information retrieval (IR) has become a widely-adopted paradigm in search. While Large Language Models (LLMs) have been extensively evaluated as second-stage reranking models for monolingual IR, a systematic large-scale comparison is still lacking for cross-lingual IR (CLIR). Moreover, while prior work shows that LLM-based rerankers improve CLIR performance, their evaluation setup relies on lexical retrieval with machine translation (MT) for the first stage. This is not only prohibitively expensive but also prone to error propagation across stages. Our evaluation on passage-level and document-level CLIR reveals that further gains can be achieved with multilingual bi-encoders as first-stage retrievers and that the benefits of translation diminishes with stronger reranking models. We further show that pairwise rerankers based on instruction-tuned LLMs perform competitively with listwise rerankers. To the best of our knowledge, we are the first to study the interaction between retrievers and rerankers in two-stage CLIR with LLMs. Our findings reveal that, without MT, current state-of-the-art rerankers fall severely short when directly applied in CLIR.</li>
</ul>

<h3>Title: KAIO: A Collection of More Challenging Korean Questions</h3>
<ul>
<li><strong>Authors: </strong>Nahyun Lee, Guijin Son, Hyunwoo Ko, Kyubeen Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14752">https://arxiv.org/abs/2509.14752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14752">https://arxiv.org/pdf/2509.14752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14752]] KAIO: A Collection of More Challenging Korean Questions(https://arxiv.org/abs/2509.14752)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the advancement of mid/post-training techniques, LLMs are pushing their boundaries at an accelerated pace. Legacy benchmarks saturate quickly (e.g., broad suites like MMLU over the years, newer ones like GPQA-D even faster), which makes frontier progress hard to track. The problem is especially acute in Korean: widely used benchmarks are fewer, often translated or narrow in scope, and updated more slowly, so saturation and contamination arrive sooner. Accordingly, at this moment, there is no Korean benchmark capable of evaluating and ranking frontier models. To bridge this gap, we introduce KAIO, a Korean, math-centric benchmark that stresses long-chain reasoning. Unlike recent Korean suites that are at or near saturation, KAIO remains far from saturated: the best-performing model, GPT-5, attains 62.8, followed by Gemini-2.5-Pro (52.3). Open models such as Qwen3-235B and DeepSeek-R1 cluster falls below 30, demonstrating substantial headroom, enabling robust tracking of frontier progress in Korean. To reduce contamination, KAIO will remain private and be served via a held-out evaluator until the best publicly known model reaches at least 80% accuracy, after which we will release the set and iterate to a harder version.</li>
</ul>

<h3>Title: Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Sheta, Mathias Zinnen, Aline Sindel, Andreas Maier, Vincent Christlein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14755">https://arxiv.org/abs/2509.14755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14755">https://arxiv.org/pdf/2509.14755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14755]] Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks(https://arxiv.org/abs/2509.14755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Finding smell references in historic artworks is a challenging problem. Beyond artwork-specific challenges such as stylistic variations, their recognition demands exceptionally detailed annotation classes, resulting in annotation sparsity and extreme class imbalance. In this work, we explore the potential of synthetic data generation to alleviate these issues and enable accurate detection of smell-related objects. We evaluate several diffusion-based augmentation strategies and demonstrate that incorporating synthetic data into model training can improve detection performance. Our findings suggest that leveraging the large-scale pretraining of diffusion models offers a promising approach for improving detection accuracy, particularly in niche applications where annotations are scarce and costly to obtain. Furthermore, the proposed approach proves to be effective even with relatively small amounts of data, and scaling it up provides high potential for further enhancements.</li>
</ul>

<h3>Title: Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration</h3>
<ul>
<li><strong>Authors: </strong>Haoran Zhang, Yafu Li, Xuyang Hu, Dongrui Liu, Zhilin Wang, Bo Li, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14760">https://arxiv.org/abs/2509.14760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14760">https://arxiv.org/pdf/2509.14760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14760]] Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration(https://arxiv.org/abs/2509.14760)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.</li>
</ul>

<h3>Title: A Real-Time Multi-Model Parametric Representation of Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Yuan Gao, Wei Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14773">https://arxiv.org/abs/2509.14773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14773">https://arxiv.org/pdf/2509.14773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14773]] A Real-Time Multi-Model Parametric Representation of Point Clouds(https://arxiv.org/abs/2509.14773)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, parametric representations of point clouds have been widely applied in tasks such as memory-efficient mapping and multi-robot collaboration. Highly adaptive models, like spline surfaces or quadrics, are computationally expensive in detection or fitting. In contrast, real-time methods, such as Gaussian mixture models or planes, have low degrees of freedom, making high accuracy with few primitives difficult. To tackle this problem, a multi-model parametric representation with real-time surface detection and fitting is proposed. Specifically, the Gaussian mixture model is first employed to segment the point cloud into multiple clusters. Then, flat clusters are selected and merged into planes or curved surfaces. Planes can be easily fitted and delimited by a 2D voxel-based boundary description method. Surfaces with curvature are fitted by B-spline surfaces and the same boundary description method is employed. Through evaluations on multiple public datasets, the proposed surface detection exhibits greater robustness than the state-of-the-art approach, with 3.78 times improvement in efficiency. Meanwhile, this representation achieves a 2-fold gain in accuracy over Gaussian mixture models, operating at 36.4 fps on a low-power onboard computer.</li>
</ul>

<h3>Title: Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Cho, Yejin Jung, Nam Ik Cho, Jae Woong Soh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14777">https://arxiv.org/abs/2509.14777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14777">https://arxiv.org/pdf/2509.14777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14777]] Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models(https://arxiv.org/abs/2509.14777)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68\% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset.</li>
</ul>

<h3>Title: Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Sina Amirrajab, Zohaib Salahuddin, Sheng Kuang, Henry C. Woodruff, Philippe Lambin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14780">https://arxiv.org/abs/2509.14780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14780">https://arxiv.org/pdf/2509.14780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14780]] Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model(https://arxiv.org/abs/2509.14780)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data.</li>
</ul>

<h3>Title: Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery</h3>
<ul>
<li><strong>Authors: </strong>Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Nga-Chun Ng, Gwing Kei Yip, Gerald W.Y. Cheng, Yunlin Mao, Jing Cai, Liang-ting Lin, Jung Sun Yoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14788">https://arxiv.org/abs/2509.14788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14788">https://arxiv.org/pdf/2509.14788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14788]] Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery(https://arxiv.org/abs/2509.14788)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability. This work introduces a sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability. Evaluated across multiple benchmarks, the model achieves state-of-the-art performance on Human and BioSNAP datasets and remains competitive on BindingDB. In virtual screening tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in AUROC and BEDROC. Ablation studies confirm the critical role of learned aggregation, bilinear attention, and contrastive alignment in enhancing predictive robustness. Embedding visualizations reveal improved spatial correspondence with known binding pockets and highlight interpretable attention patterns over ligand-residue contacts. These results validate the framework's utility for scalable and structure-aware DTI prediction.</li>
</ul>

<h3>Title: SINAI at eRisk@CLEF 2023: Approaching Early Detection of Gambling with Natural Language Processing</h3>
<ul>
<li><strong>Authors: </strong>Alba Maria Marmol-Romero, Flor Miriam Plaza-del-Arco, Arturo Montejo-Raez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14797">https://arxiv.org/abs/2509.14797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14797">https://arxiv.org/pdf/2509.14797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14797]] SINAI at eRisk@CLEF 2023: Approaching Early Detection of Gambling with Natural Language Processing(https://arxiv.org/abs/2509.14797)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper describes the participation of the SINAI team in the eRisk@CLEF lab. Specifically, one of the proposed tasks has been addressed: Task 2 on the early detection of signs of pathological gambling. The approach presented in Task 2 is based on pre-trained models from Transformers architecture with comprehensive preprocessing data and data balancing techniques. Moreover, we integrate Long-short Term Memory (LSTM) architecture with automodels from Transformers. In this Task, our team has been ranked in seventh position, with an F1 score of 0.126, out of 49 participant submissions and achieves the highest values in recall metrics and metrics related to early detection.</li>
</ul>

<h3>Title: STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models</h3>
<ul>
<li><strong>Authors: </strong>Julian F. Schumann, Anna Mészáros, Jens Kober, Arkady Zgonnikov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14801">https://arxiv.org/abs/2509.14801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14801">https://arxiv.org/pdf/2509.14801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14801]] STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models(https://arxiv.org/abs/2509.14801)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>While trajectory prediction plays a critical role in enabling safe and effective path-planning in automated vehicles, standardized practices for evaluating such models remain underdeveloped. Recent efforts have aimed to unify dataset formats and model interfaces for easier comparisons, yet existing frameworks often fall short in supporting heterogeneous traffic scenarios, joint prediction models, or user documentation. In this work, we introduce STEP -- a new benchmarking framework that addresses these limitations by providing a unified interface for multiple datasets, enforcing consistent training and evaluation conditions, and supporting a wide range of prediction models. We demonstrate the capabilities of STEP in a number of experiments which reveal 1) the limitations of widely-used testing procedures, 2) the importance of joint modeling of agents for better predictions of interactions, and 3) the vulnerability of current state-of-the-art models against both distribution shifts and targeted attacks by adversarial agents. With STEP, we aim to shift the focus from the ``leaderboard'' approach to deeper insights about model behavior and generalization in complex multi-agent settings.</li>
</ul>

<h3>Title: SINAI at eRisk@CLEF 2022: Approaching Early Detection of Gambling and Eating Disorders with Natural Language Processing</h3>
<ul>
<li><strong>Authors: </strong>Alba Maria Marmol-Romero, Salud Maria Jimenez-Zafra, Flor Miriam Plaza-del-Arco, M. Dolores Molina-Gonzalez, Maria-Teresa Martin-Valdivia, Arturo Montejo-Raez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14806">https://arxiv.org/abs/2509.14806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14806">https://arxiv.org/pdf/2509.14806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14806]] SINAI at eRisk@CLEF 2022: Approaching Early Detection of Gambling and Eating Disorders with Natural Language Processing(https://arxiv.org/abs/2509.14806)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper describes the participation of the SINAI team in the eRisk@CLEF lab. Specifically, two of the proposed tasks have been addressed: i) Task 1 on the early detection of signs of pathological gambling, and ii) Task 3 on measuring the severity of the signs of eating disorders. The approach presented in Task 1 is based on the use of sentence embeddings from Transformers with features related to volumetry, lexical diversity, complexity metrics, and emotion-related scores, while the approach for Task 3 is based on text similarity estimation using contextualized word embeddings from Transformers. In Task 1, our team has been ranked in second position, with an F1 score of 0.808, out of 41 participant submissions. In Task 3, our team also placed second out of a total of 3 participating teams.</li>
</ul>

<h3>Title: ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance</h3>
<ul>
<li><strong>Authors: </strong>Hannah Sterz, Fabian David Schmidt, Goran Glavaš, Ivan Vulić</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14814">https://arxiv.org/abs/2509.14814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14814">https://arxiv.org/pdf/2509.14814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14814]] ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance(https://arxiv.org/abs/2509.14814)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As they become increasingly multilingual, Large Language Models (LLMs) exhibit more language confusion, i.e., they tend to generate answers in a language different from the language of the prompt or the answer language explicitly requested by the user. In this work, we propose ReCoVeR (REducing language COnfusion in VEctor Representations), a novel lightweight approach for reducing language confusion based on language-specific steering vectors. We first isolate language vectors with the help of multi-parallel corpus and then effectively leverage those vectors for effective LLM steering via fixed (i.e., unsupervised) as well as trainable steering functions. Our extensive evaluation, encompassing three benchmarks and 18 languages, shows that ReCoVeR effectively mitigates language confusion in both monolingual and cross-lingual setups while at the same time -- and in contrast to prior language steering methods -- retaining task performance. Our data code is available at this https URL.</li>
</ul>

<h3>Title: Fracture interactive geodesic active contours for bone segmentation</h3>
<ul>
<li><strong>Authors: </strong>Liheng Wang, Licheng Zhang, Hailin Xu, Jingxin Zhao, Xiuyun Su, Jiantao Li, Miutian Tang, Weilu Gao, Chong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14817">https://arxiv.org/abs/2509.14817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14817">https://arxiv.org/pdf/2509.14817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14817]] Fracture interactive geodesic active contours for bone segmentation(https://arxiv.org/abs/2509.14817)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>For bone segmentation, the classical geodesic active contour model is usually limited by its indiscriminate feature extraction, and then struggles to handle the phenomena of edge obstruction, edge leakage and bone fracture. Thus, we propose a fracture interactive geodesic active contour algorithm tailored for bone segmentation, which can better capture bone features and perform robustly to the presence of bone fractures and soft tissues. Inspired by orthopedic knowledge, we construct a novel edge-detector function that combines the intensity and gradient norm, which guides the contour towards bone edges without being obstructed by other soft tissues and therefore reduces mis-segmentation. Furthermore, distance information, where fracture prompts can be embedded, is introduced into the contour evolution as an adaptive step size to stabilize the evolution and help the contour stop at bone edges and fractures. This embedding provides a way to interact with bone fractures and improves the accuracy in the fracture regions. Experiments in pelvic and ankle segmentation demonstrate the effectiveness on addressing the aforementioned problems and show an accurate, stable and consistent performance, indicating a broader application in other bone anatomies. Our algorithm also provides insights into combining the domain knowledge and deep neural networks.</li>
</ul>

<h3>Title: ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Lopez Pellicer, Andre Mariucci, Plamen Angelov, Marwan Bukhari, Jemma G. Kerns</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14830">https://arxiv.org/abs/2509.14830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14830">https://arxiv.org/pdf/2509.14830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14830]] ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification(https://arxiv.org/abs/2509.14830)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.</li>
</ul>

<h3>Title: Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Stelios Zarifis, Ioannis Kordonis, Petros Maragos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14832">https://arxiv.org/abs/2509.14832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14832">https://arxiv.org/pdf/2509.14832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14832]] Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization(https://arxiv.org/abs/2509.14832)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Stochastic forecasting is critical for efficient decision-making in uncertain systems, such as energy markets and finance, where estimating the full distribution of future scenarios is essential. We propose Diffusion Scenario Tree (DST), a general framework for constructing scenario trees for multivariate prediction tasks using diffusion-based probabilistic forecasting models. DST recursively samples future trajectories and organizes them into a tree via clustering, ensuring non-anticipativity (decisions depending only on observed history) at each stage. We evaluate the framework on the optimization task of energy arbitrage in New York State's day-ahead electricity market. Experimental results show that our approach consistently outperforms the same optimization algorithms that use scenario trees from more conventional models and Model-Free Reinforcement Learning baselines. Furthermore, using DST for stochastic optimization yields more efficient decision policies, achieving higher performance by better handling uncertainty than deterministic and stochastic MPC variants using the same diffusion-based forecaster.</li>
</ul>

<h3>Title: LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring</h3>
<ul>
<li><strong>Authors: </strong>Jinhee Jang, Ayoung Moon, Minkyoung Jung, YoungBin Kim. Seung Jin Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14834">https://arxiv.org/abs/2509.14834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14834">https://arxiv.org/pdf/2509.14834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14834]] LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring(https://arxiv.org/abs/2509.14834)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods.</li>
</ul>

<h3>Title: V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qidong Wang, Junjie Hu, Ming Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14837">https://arxiv.org/abs/2509.14837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14837">https://arxiv.org/pdf/2509.14837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14837]] V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models(https://arxiv.org/abs/2509.14837)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Recent advances in causal interpretability have extended from language models to vision-language models (VLMs), seeking to reveal their internal mechanisms through input interventions. While textual interventions often target semantics, visual interventions typically rely on coarse pixel-level perturbations, limiting semantic insights on multimodal integration. In this study, we introduce V-SEAM, a novel framework that combines Visual Semantic Editing and Attention Modulating for causal interpretation of VLMs. V-SEAM enables concept-level visual manipulations and identifies attention heads with positive or negative contributions to predictions across three semantic levels: objects, attributes, and relationships. We observe that positive heads are often shared within the same semantic level but vary across levels, while negative heads tend to generalize broadly. Finally, we introduce an automatic method to modulate key head embeddings, demonstrating enhanced performance for both LLaVA and InstructBLIP across three diverse VQA benchmarks. Our data and code are released at: this https URL.</li>
</ul>

<h3>Title: [Re] Improving Interpretation Faithfulness for Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Izabela Kurek, Wojciech Trejter, Stipe Frkovic, Andro Erdelez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14846">https://arxiv.org/abs/2509.14846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14846">https://arxiv.org/pdf/2509.14846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14846]] [Re] Improving Interpretation Faithfulness for Vision Transformers(https://arxiv.org/abs/2509.14846)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability, diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors' claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study's findings, although minor discrepancies were found and discussed.</li>
</ul>

<h3>Title: Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support</h3>
<ul>
<li><strong>Authors: </strong>Xianrong Yao, Dong She, Chenxu Zhang, Yimeng Zhang, Yueru Sun, Noman Ahmed, Yang Gao, Zhanpeng Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14851">https://arxiv.org/abs/2509.14851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14851">https://arxiv.org/pdf/2509.14851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14851]] Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support(https://arxiv.org/abs/2509.14851)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker's emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE's reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support.</li>
</ul>

<h3>Title: MARIC: Multi-Agent Reasoning for Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Wonduk Seo, Minhyeong Yu, Hyunjin An, Seunghyun Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14860">https://arxiv.org/abs/2509.14860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14860">https://arxiv.org/pdf/2509.14860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14860]] MARIC: Multi-Agent Reasoning for Image Classification(https://arxiv.org/abs/2509.14860)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification.</li>
</ul>

<h3>Title: Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Zhengwei Wang, Gang Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14863">https://arxiv.org/abs/2509.14863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14863">https://arxiv.org/pdf/2509.14863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14863]] Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study(https://arxiv.org/abs/2509.14863)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph Transformers (GTs) show considerable potential in graph representation learning. The architecture of GTs typically integrates Graph Neural Networks (GNNs) with global attention mechanisms either in parallel or as a precursor to attention mechanisms, yielding a local-and-global or local-to-global attention scheme. However, as the global attention mechanism primarily captures long-range dependencies between nodes, these integration schemes may suffer from information loss, where the local neighborhood information learned by GNN could be diluted by the attention mechanism. Therefore, we propose G2LFormer, featuring a novel global-to-local attention scheme where the shallow network layers use attention mechanisms to capture global information, while the deeper layers employ GNN modules to learn local structural information, thereby preventing nodes from ignoring their immediate neighbors. An effective cross-layer information fusion strategy is introduced to allow local layers to retain beneficial information from global layers and alleviate information loss, with acceptable trade-offs in scalability. To validate the feasibility of the global-to-local attention scheme, we compare G2LFormer with state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The results indicate that G2LFormer exhibits excellent performance while keeping linear complexity.</li>
</ul>

<h3>Title: Controllable Localized Face Anonymization Via Diffusion Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Ali Salar, Qing Liu, Guoying Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14866">https://arxiv.org/abs/2509.14866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14866">https://arxiv.org/pdf/2509.14866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14866]] Controllable Localized Face Anonymization Via Diffusion Inpainting(https://arxiv.org/abs/2509.14866)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, diffusion</a></li>
<li><strong>Abstract: </strong>The growing use of portrait images in computer vision highlights the need to protect personal identities. At the same time, anonymized images must remain useful for downstream computer vision tasks. In this work, we propose a unified framework that leverages the inpainting ability of latent diffusion models to generate realistic anonymized images. Unlike prior approaches, we have complete control over the anonymization process by designing an adaptive attribute-guidance module that applies gradient correction during the reverse denoising process, aligning the facial attributes of the generated image with those of the synthesized target image. Our framework also supports localized anonymization, allowing users to specify which facial regions are left unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ datasets show that our method outperforms state-of-the-art approaches while requiring no additional model training. The source code is available on our page.</li>
</ul>

<h3>Title: Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens</h3>
<ul>
<li><strong>Authors: </strong>Issa Sugiura, Shuhei Kurita, Yusuke Oda, Ryuichiro Higashinaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14882">https://arxiv.org/abs/2509.14882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14882">https://arxiv.org/pdf/2509.14882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14882]] Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens(https://arxiv.org/abs/2509.14882)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose Llama-Mimi, a speech language model that uses a unified tokenizer and a single Transformer decoder to jointly model sequences of interleaved semantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi achieves state-of-the-art performance in acoustic consistency and possesses the ability to preserve speaker identity. Our analysis further demonstrates that increasing the number of quantizers improves acoustic fidelity but degrades linguistic performance, highlighting the inherent challenge of maintaining long-term coherence. We additionally introduce an LLM-as-a-Judge-based evaluation to assess the spoken content quality of generated outputs. Our models, code, and speech samples are publicly available.</li>
</ul>

<h3>Title: A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Ye Shen, Junying Wang, Farong Wen, Yijin Guo, Qi Jia, Zicheng Zhang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14886">https://arxiv.org/abs/2509.14886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14886">https://arxiv.org/pdf/2509.14886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14886]] A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation(https://arxiv.org/abs/2509.14886)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.</li>
</ul>

<h3>Title: Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis</h3>
<ul>
<li><strong>Authors: </strong>Hoang-Son Nguyen, Hoi-To Wai</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14887">https://arxiv.org/abs/2509.14887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14887">https://arxiv.org/pdf/2509.14887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14887]] Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis(https://arxiv.org/abs/2509.14887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning the graph underlying a networked system from nodal signals is crucial to downstream tasks in graph signal processing and machine learning. The presence of hidden nodes whose signals are not observable might corrupt the estimated graph. While existing works proposed various robustifications of vanilla graph learning objectives by explicitly accounting for the presence of these hidden nodes, a robustness analysis of "naive", hidden-node agnostic approaches is still underexplored. This work demonstrates that vanilla graph topology learning methods are implicitly robust to partial observations of low-pass filtered graph signals. We achieve this theoretical result through extending the restricted isometry property (RIP) to the Dirichlet energy function used in graph learning objectives. We show that smoothness-based graph learning formulation (e.g., the GL-SigRep method) on partial observations can recover the ground truth graph topology corresponding to the observed nodes. Synthetic and real data experiments corroborate our findings.</li>
</ul>

<h3>Title: Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics</h3>
<ul>
<li><strong>Authors: </strong>Guillermo Hijano Mendizabal, Davide Lancierini, Alex Marshall, Andrea Mauri, Patrick Haworth Owen, Mitesh Patel, Konstantinos Petridis, Shah Rukh Qasim, Nicola Serra, William Sutcliffe, Hanae Tilquin</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14894">https://arxiv.org/abs/2509.14894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14894">https://arxiv.org/pdf/2509.14894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14894]] Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics(https://arxiv.org/abs/2509.14894)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Experimental studies of beauty hadron decays face significant challenges due to a wide range of backgrounds arising from the numerous possible decay channels with similar final states. For a particular signal decay, the process for ascertaining the most relevant background processes necessitates a detailed analysis of final state particles, potential misidentifications, and kinematic overlaps, which, due to computational limitations, is restricted to the simulation of only the most relevant backgrounds. Moreover, this process typically relies on the physicist's intuition and expertise, as no systematic method exists. This paper has two primary goals. First, from a particle physics perspective, we present a novel approach that utilises Reinforcement Learning (RL) to overcome the aforementioned challenges by systematically determining the critical backgrounds affecting beauty hadron decay measurements. While beauty hadron physics serves as the case study in this work, the proposed strategy is broadly adaptable to other types of particle physics measurements. Second, from a Machine Learning perspective, we introduce a novel algorithm which exploits the synergy between RL and Genetic Algorithms (GAs) for environments with highly sparse rewards and a large trajectory space. This strategy leverages GAs to efficiently explore the trajectory space and identify successful trajectories, which are used to guide the RL agent's training. Our method also incorporates a transformer architecture for the RL agent to handle token sequences representing decays.</li>
</ul>

<h3>Title: Pseudo-Label Enhanced Cascaded Framework: 2nd Technical Report for LSVOS 2025 VOS Track</h3>
<ul>
<li><strong>Authors: </strong>An Yan, Leilei Cao, Feng Lu, Ran Hong, Youhai Jiang, Fengjie Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14901">https://arxiv.org/abs/2509.14901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14901">https://arxiv.org/pdf/2509.14901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14901]] Pseudo-Label Enhanced Cascaded Framework: 2nd Technical Report for LSVOS 2025 VOS Track(https://arxiv.org/abs/2509.14901)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Complex Video Object Segmentation (VOS) presents significant challenges in accurately segmenting objects across frames, especially in the presence of small and similar targets, frequent occlusions, rapid motion, and complex interactions. In this report, we present our solution for the LSVOS 2025 VOS Track based on the SAM2 framework. We adopt a pseudo-labeling strategy during training: a trained SAM2 checkpoint is deployed within the SAM2Long framework to generate pseudo labels for the MOSE test set, which are then combined with existing data for further training. For inference, the SAM2Long framework is employed to obtain our primary segmentation results, while an open-source SeC model runs in parallel to produce complementary predictions. A cascaded decision mechanism dynamically integrates outputs from both models, exploiting the temporal stability of SAM2Long and the concept-level robustness of SeC. Benefiting from pseudo-label training and cascaded multi-model inference, our approach achieves a J\&F score of 0.8616 on the MOSE test set -- +1.4 points over our SAM2Long baseline -- securing the 2nd place in the LSVOS 2025 VOS Track, and demonstrating strong robustness and accuracy in long, complex video segmentation scenarios.</li>
</ul>

<h3>Title: Robust Barycenters of Persistence Diagrams</h3>
<ul>
<li><strong>Authors: </strong>Keanu Sisouk, Eloi Tanguy, Julie Delon, Julien Tierny</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14904">https://arxiv.org/abs/2509.14904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14904">https://arxiv.org/pdf/2509.14904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14904]] Robust Barycenters of Persistence Diagrams(https://arxiv.org/abs/2509.14904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This short paper presents a general approach for computing robust Wasserstein barycenters of persistence diagrams. The classical method consists in computing assignment arithmetic means after finding the optimal transport plans between the barycenter and the persistence diagrams. However, this procedure only works for the transportation cost related to the $q$-Wasserstein distance $W_q$ when $q=2$. We adapt an alternative fixed-point method to compute a barycenter diagram for generic transportation costs ($q > 1$), in particular those robust to outliers, $q \in (1,2)$. We show the utility of our work in two applications: \emph{(i)} the clustering of persistence diagrams on their metric space and \emph{(ii)} the dictionary encoding of persistence diagrams. In both scenarios, we demonstrate the added robustness to outliers provided by our generalized framework. Our Python implementation is available at this address: this https URL .</li>
</ul>

<h3>Title: Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications</h3>
<ul>
<li><strong>Authors: </strong>Tahar Chettaoui, Naser Damer, Fadi Boutros</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14921">https://arxiv.org/abs/2509.14921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14921">https://arxiv.org/pdf/2509.14921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14921]] Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications(https://arxiv.org/abs/2509.14921)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric</a></li>
<li><strong>Abstract: </strong>Foundation models such as CLIP have demonstrated exceptional zero- and few-shot transfer capabilities across diverse vision tasks. However, when fine-tuned for highly specialized biometric tasks, face recognition (FR), morphing attack detection (MAD), and presentation attack detection (PAD), these models may suffer from over-specialization. Thus, they may lose one of their foundational strengths, cross-domain generalization. In this work, we systematically quantify these trade-offs by evaluating three instances of CLIP fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the original CLIP baseline on 14 general vision datasets under zero-shot and linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our results indicate that fine-tuned models suffer from over-specialization, especially when fine-tuned for complex tasks of FR. Also, our results pointed out that task complexity and classification head design, multi-class (FR) vs. binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The FRoundation model with the ViT-L backbone outperforms other approaches on the large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%. However, it experiences a substantial performance drop on ImageNetV2, reaching only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover, the larger CLIP architecture consistently preserves more of the model's original generalization ability than the smaller variant, indicating that increased model capacity may help mitigate over-specialization.</li>
</ul>

<h3>Title: A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis and Emotion Detection in Social Media Texts</h3>
<ul>
<li><strong>Authors: </strong>Kian Tohidi, Kia Dashtipour, Simone Rebora, Sevda Pourfaramarz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14922">https://arxiv.org/abs/2509.14922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14922">https://arxiv.org/pdf/2509.14922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14922]] A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis and Emotion Detection in Social Media Texts(https://arxiv.org/abs/2509.14922)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>This study presents a comprehensive comparative evaluation of four state-of-the-art Large Language Models (LLMs)--Claude 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o--for sentiment analysis and emotion detection in Persian social media texts. Comparative analysis among LLMs has witnessed a significant rise in recent years, however, most of these analyses have been conducted on English language tasks, creating gaps in understanding cross-linguistic performance patterns. This research addresses these gaps through rigorous experimental design using balanced Persian datasets containing 900 texts for sentiment analysis (positive, negative, neutral) and 1,800 texts for emotion detection (anger, fear, happiness, hate, sadness, surprise). The main focus was to allow for a direct and fair comparison among different models, by using consistent prompts, uniform processing parameters, and by analyzing the performance metrics such as precision, recall, F1-scores, along with misclassification patterns. The results show that all models reach an acceptable level of performance, and a statistical comparison of the best three models indicates no significant differences among them. However, GPT-4o demonstrated a marginally higher raw accuracy value for both tasks, while Gemini 2.0 Flash proved to be the most cost-efficient. The findings indicate that the emotion detection task is more challenging for all models compared to the sentiment analysis task, and the misclassification patterns can represent some challenges in Persian language texts. These findings establish performance benchmarks for Persian NLP applications and offer practical guidance for model selection based on accuracy, efficiency, and cost considerations, while revealing cultural and linguistic challenges that require consideration in multilingual AI system deployment.</li>
</ul>

<h3>Title: Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation</h3>
<ul>
<li><strong>Authors: </strong>Konrad Nowosadko, Franco Ruggeri, Ahmad Terra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14925">https://arxiv.org/abs/2509.14925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14925">https://arxiv.org/pdf/2509.14925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14925]] Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation(https://arxiv.org/abs/2509.14925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) methods that incorporate deep neural networks (DNN), though powerful, often lack transparency. Their black-box characteristic hinders interpretability and reduces trustworthiness, particularly in critical domains. To address this challenge in RL tasks, we propose a solution based on Self-Explaining Neural Networks (SENNs) along with explanation extraction methods to enhance interpretability while maintaining predictive accuracy. Our approach targets low-dimensionality problems to generate robust local and global explanations of the model's behaviour. We evaluate the proposed method on the resource allocation problem in mobile networks, demonstrating that SENNs can constitute interpretable solutions with competitive performance. This work highlights the potential of SENNs to improve transparency and trust in AI-driven decision-making for low-dimensional tasks. Our approach strong performance on par with the existing state-of-the-art methods, while providing robust explanations.</li>
</ul>

<h3>Title: Patent Language Model Pretraining with ModernBERT</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Yousefiramandi, Ciaran Cooney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14926">https://arxiv.org/abs/2509.14926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14926">https://arxiv.org/pdf/2509.14926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14926]] Patent Language Model Pretraining with ModernBERT(https://arxiv.org/abs/2509.14926)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.</li>
</ul>

<h3>Title: GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation</h3>
<ul>
<li><strong>Authors: </strong>Tan-Hiep To, Duy-Khang Nguyen, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14927">https://arxiv.org/abs/2509.14927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14927">https://arxiv.org/pdf/2509.14927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14927]] GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation(https://arxiv.org/abs/2509.14927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Key Opinion Leader (KOL) play a crucial role in modern marketing by shaping consumer perceptions and enhancing brand credibility. However, collaborating with human KOLs often involves high costs and logistical challenges. To address this, we present GenKOL, an interactive system that empowers marketing professionals to efficiently generate high-quality virtual KOL images using generative AI. GenKOL enables users to dynamically compose promotional visuals through an intuitive interface that integrates multiple AI capabilities, including garment generation, makeup transfer, background synthesis, and hair editing. These capabilities are implemented as modular, interchangeable services that can be deployed flexibly on local machines or in the cloud. This modular architecture ensures adaptability across diverse use cases and computational environments. Our system can significantly streamline the production of branded content, lowering costs and accelerating marketing workflows through scalable virtual KOL creation.</li>
</ul>

<h3>Title: Cross-Modal Knowledge Distillation for Speech Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Enzhi Wang, Qicheng Li, Zhiyuan Tang, Yuhang Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14930">https://arxiv.org/abs/2509.14930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14930">https://arxiv.org/pdf/2509.14930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14930]] Cross-Modal Knowledge Distillation for Speech Large Language Models(https://arxiv.org/abs/2509.14930)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.</li>
</ul>

<h3>Title: A Comparative Analysis of Transformer Models in Social Bot Detection</h3>
<ul>
<li><strong>Authors: </strong>Rohan Veit, Michael Lones</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14936">https://arxiv.org/abs/2509.14936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14936">https://arxiv.org/pdf/2509.14936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14936]] A Comparative Analysis of Transformer Models in Social Bot Detection(https://arxiv.org/abs/2509.14936)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Social media has become a key medium of communication in today's society. This realisation has led to many parties employing artificial users (or bots) to mislead others into believing untruths or acting in a beneficial manner to such parties. Sophisticated text generation tools, such as large language models, have further exacerbated this issue. This paper aims to compare the effectiveness of bot detection models based on encoder and decoder transformers. Pipelines are developed to evaluate the performance of these classifiers, revealing that encoder-based classifiers demonstrate greater accuracy and robustness. However, decoder-based models showed greater adaptability through task-specific alignment, suggesting more potential for generalisation across different use cases in addition to superior observa. These findings contribute to the ongoing effort to prevent digital environments being manipulated while protecting the integrity of online discussion.</li>
</ul>

<h3>Title: Hierarchical Federated Learning for Social Network with Mobility</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Chen, Wen Chen, Jun Li, Qingqing Wu, Ming Ding, Xuefeng Han, Xiumei Deng, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14938">https://arxiv.org/abs/2509.14938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14938">https://arxiv.org/pdf/2509.14938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14938]] Hierarchical Federated Learning for Social Network with Mobility(https://arxiv.org/abs/2509.14938)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) offers a decentralized solution that allows collaborative local model training and global aggregation, thereby protecting data privacy. In conventional FL frameworks, data privacy is typically preserved under the assumption that local data remains absolutely private, whereas the mobility of clients is frequently neglected in explicit modeling. In this paper, we propose a hierarchical federated learning framework based on the social network with mobility namely HFL-SNM that considers both data sharing among clients and their mobility patterns. Under the constraints of limited resources, we formulate a joint optimization problem of resource allocation and client scheduling, which objective is to minimize the energy consumption of clients during the FL process. In social network, we introduce the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate. We analyze the impact of effective data and redundant data on the model performance through preliminary experiments. We decouple the optimization problem into multiple sub-problems, analyze them based on preliminary experimental results, and propose Dynamic Optimization in Social Network with Mobility (DO-SNM) algorithm. Experimental results demonstrate that our algorithm achieves superior model performance while significantly reducing energy consumption, compared to traditional baseline algorithms.</li>
</ul>

<h3>Title: Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts</h3>
<ul>
<li><strong>Authors: </strong>Alessandra Stramiglio, Andrea Schimmenti, Valentina Pasqual, Marieke van Erp, Francesco Sovrano, Fabio Vitali</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14943">https://arxiv.org/abs/2509.14943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14943">https://arxiv.org/pdf/2509.14943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14943]] Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts(https://arxiv.org/abs/2509.14943)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Text Implicitness has always been challenging in Natural Language Processing (NLP), with traditional methods relying on explicit statements to identify entities and their relationships. From the sentence "Zuhdi attends church every Sunday", the relationship between Zuhdi and Christianity is evident for a human reader, but it presents a challenge when it must be inferred automatically. Large language models (LLMs) have proven effective in NLP downstream tasks such as text comprehension and information extraction (IE). This study examines how textual implicitness affects IE tasks in pre-trained LLMs: LLaMA 2.3, DeepSeekV1, and Phi1.5. We generate two synthetic datasets of 10k implicit and explicit verbalization of biographic information to measure the impact on LLM performance and analyze whether fine-tuning implicit data improves their ability to generalize in implicit reasoning tasks. This research presents an experiment on the internal reasoning processes of LLMs in IE, particularly in dealing with implicit and explicit contexts. The results demonstrate that fine-tuning LLM models with LoRA (low-rank adaptation) improves their performance in extracting information from implicit texts, contributing to better model interpretability and reliability.</li>
</ul>

<h3>Title: Stochastic Bilevel Optimization with Heavy-Tailed Noise</h3>
<ul>
<li><strong>Authors: </strong>Zhuanghua Liu, Luo Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14952">https://arxiv.org/abs/2509.14952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14952">https://arxiv.org/pdf/2509.14952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14952]] Stochastic Bilevel Optimization with Heavy-Tailed Noise(https://arxiv.org/abs/2509.14952)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper considers the smooth bilevel optimization in which the lower-level problem is strongly convex and the upper-level problem is possibly nonconvex. We focus on the stochastic setting that the algorithm can access the unbiased stochastic gradient evaluation with heavy-tailed noise, which is prevalent in many machine learning applications such as training large language models and reinforcement learning. We propose a nested-loop normalized stochastic bilevel approximation (N$^2$SBA) for finding an $\epsilon$-stationary point with the stochastic first-order oracle (SFO) complexity of $\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{4 p - 2}{p-1}}\big)$, where $\kappa$ is the condition number, $p\in(1,2]$ is the order of central moment for the noise, and $\sigma$ is the noise level. Furthermore, we specialize our idea to solve the nonconvex-strongly-concave minimax optimization problem, achieving an $\epsilon$-stationary point with the SFO complexity of $\tilde{\mathcal O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{3p-2}{p-1}}\big)$. All above upper bounds match the best-known results under the special case of the bounded variance setting, i.e., $p=2$.</li>
</ul>

<h3>Title: DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</h3>
<ul>
<li><strong>Authors: </strong>Zhuokang Shen, Kaisen Zhang, Bohan Jia, Yuan Fang, Zhou Yu, Shaohui Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14957">https://arxiv.org/abs/2509.14957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14957">https://arxiv.org/pdf/2509.14957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14957]] DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection(https://arxiv.org/abs/2509.14957)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>With the increasing prevalence of synthetic images, evaluating image authenticity and locating forgeries accurately while maintaining human interpretability remains a challenging task. Existing detection models primarily focus on simple authenticity classification, ultimately providing only a forgery probability or binary judgment, which offers limited explanatory insights into image authenticity. Moreover, while MLLM-based detection methods can provide more interpretable results, they still lag behind expert models in terms of pure authenticity classification accuracy. To address this, we propose DF-LLaVA, a simple yet effective framework that unlocks the intrinsic discrimination potential of MLLMs. Our approach first extracts latent knowledge from MLLMs and then injects it into training via prompts. This framework allows LLaVA to achieve outstanding detection accuracy exceeding expert models while still maintaining the interpretability offered by MLLMs. Extensive experiments confirm the superiority of our DF-LLaVA, achieving both high accuracy and explainability in synthetic image detection. Code is available online at: this https URL.</li>
</ul>

<h3>Title: Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification</h3>
<ul>
<li><strong>Authors: </strong>Xiang Tuo, Xu Xuemiao, Liu Bangzhen, Li Jinyi, Li Yong, He Shengfeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14958">https://arxiv.org/abs/2509.14958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14958">https://arxiv.org/pdf/2509.14958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14958]] Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification(https://arxiv.org/abs/2509.14958)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid growth of 3D digital content necessitates expandable recognition systems for open-world scenarios. However, existing 3D class-incremental learning methods struggle under extreme data scarcity due to geometric misalignment and texture bias. While recent approaches integrate 3D data with 2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by texture-biased projections and indiscriminate fusion of geometric-textural cues, leading to unstable decision prototypes and catastrophic forgetting. To address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a framework that enhances 3D geometric fidelity by leveraging CLIP's hierarchical spatial semantics. Specifically, we introduce a Structure-Aware Geometric Rectification module that hierarchically aligns 3D part structures with CLIP's intermediate spatial priors through attention-driven geometric fusion. Additionally, a Texture Amplification Module synthesizes minimal yet discriminative textures to suppress noise and reinforce cross-modal consistency. To further stabilize incremental prototypes, we employ a Base-Novel Discriminator that isolates geometric variations. Extensive experiments demonstrate that our method significantly improves 3D few-shot class-incremental learning, achieving superior geometric coherence and robustness to texture bias across cross-domain and within-domain settings.</li>
</ul>

<h3>Title: Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis</h3>
<ul>
<li><strong>Authors: </strong>Junhao Jia, Yunyou Liu, Cheng Yang, Yifei Sun, Feiwei Qin, Changmiao Wang, Yong Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14965">https://arxiv.org/abs/2509.14965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14965">https://arxiv.org/pdf/2509.14965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14965]] Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis(https://arxiv.org/abs/2509.14965)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Functional magnetic resonance imaging (fMRI) provides a powerful non-invasive window into the brain's functional organization by generating complex functional networks, typically modeled as graphs. These brain networks exhibit a hierarchical topology that is crucial for cognitive processing. However, due to inherent spatial constraints, standard Euclidean GNNs struggle to represent these hierarchical structures without high distortion, limiting their clinical performance. To address this limitation, we propose Brain-HGCN, a geometric deep learning framework based on hyperbolic geometry, which leverages the intrinsic property of negatively curved space to model the brain's network hierarchy with high fidelity. Grounded in the Lorentz model, our model employs a novel hyperbolic graph attention layer with a signed aggregation mechanism to distinctly process excitatory and inhibitory connections, ultimately learning robust graph-level representations via a geometrically sound Fréchet mean for graph readout. Experiments on two large-scale fMRI datasets for psychiatric disorder classification demonstrate that our approach significantly outperforms a wide range of state-of-the-art Euclidean baselines. This work pioneers a new geometric deep learning paradigm for fMRI analysis, highlighting the immense potential of hyperbolic GNNs in the field of computational psychiatry.</li>
</ul>

<h3>Title: RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</h3>
<ul>
<li><strong>Authors: </strong>Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14966">https://arxiv.org/abs/2509.14966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14966">https://arxiv.org/pdf/2509.14966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14966]] RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching(https://arxiv.org/abs/2509.14966)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: this https URL.</li>
</ul>

<h3>Title: FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference</h3>
<ul>
<li><strong>Authors: </strong>Carlos Barroso-Fernández, Alejandro Calvillo-Fernandez, Antonio de la Oliva, Carlos J. Bernardos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14968">https://arxiv.org/abs/2509.14968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14968">https://arxiv.org/pdf/2509.14968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14968]] FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference(https://arxiv.org/abs/2509.14968)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The upcoming generations of wireless technologies promise an era where everything is interconnected and intelligent. As the need for intelligence grows, networks must learn to better understand the physical world. However, deploying dedicated hardware to perceive the environment is not always feasible, mainly due to costs and/or complexity. Integrated Sensing and Communication (ISAC) has made a step forward in addressing this challenge. Within ISAC, passive sensing emerges as a cost-effective solution that reuses wireless communications to sense the environment, without interfering with existing communications. Nevertheless, the majority of current solutions are limited to one technology (mostly Wi-Fi or 5G), constraining the maximum accuracy reachable. As different technologies work with different spectrums, we see a necessity in integrating more than one technology to augment the coverage area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference. FAWN is based on the original transformers architecture, to fuse information from Wi-Fi and 5G, making the network capable of understanding the physical world without interfering with the current communication. To test our solution, we have built a prototype and integrated it in a real scenario. Results show errors below 0.6 m around 84% of times.</li>
</ul>

<h3>Title: SPATIALGEN: Layout-guided 3D Indoor Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14981">https://arxiv.org/abs/2509.14981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14981">https://arxiv.org/pdf/2509.14981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14981]] SPATIALGEN: Layout-guided 3D Indoor Scene Generation(https://arxiv.org/abs/2509.14981)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.</li>
</ul>

<h3>Title: PRISM: Product Retrieval In Shopping Carts using Hybrid Matching</h3>
<ul>
<li><strong>Authors: </strong>Arda Kabadayi, Senem Velipasalar, Jiajing Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14985">https://arxiv.org/abs/2509.14985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14985">https://arxiv.org/pdf/2509.14985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14985]] PRISM: Product Retrieval In Shopping Carts using Hybrid Matching(https://arxiv.org/abs/2509.14985)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Compared to traditional image retrieval tasks, product retrieval in retail settings is even more challenging. Products of the same type from different brands may have highly similar visual appearances, and the query image may be taken from an angle that differs significantly from view angles of the stored catalog images. Foundational models, such as CLIP and SigLIP, often struggle to distinguish these subtle but important local differences. Pixel-wise matching methods, on the other hand, are computationally expensive and incur prohibitively high matching times. In this paper, we propose a new, hybrid method, called PRISM, for product retrieval in retail settings by leveraging the advantages of both vision-language model-based and pixel-wise matching approaches. To provide both efficiency/speed and finegrained retrieval accuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP) is employed first to retrieve the top 35 most semantically similar products from a fixed gallery, thereby narrowing the search space significantly; 2) a segmentation model (YOLO-E) is applied to eliminate background clutter; 3) fine-grained pixel-level matching is performed using LightGlue across the filtered candidates. This framework enables more accurate discrimination between products with high inter-class similarity by focusing on subtle visual cues often missed by global models. Experiments performed on the ABV dataset show that our proposed PRISM outperforms the state-of-the-art image retrieval methods by 4.21% in top-1 accuracy while still remaining within the bounds of real-time processing for practical retail deployments.</li>
</ul>

<h3>Title: Blockchain-Enabled Explainable AI for Trusted Healthcare Systems</h3>
<ul>
<li><strong>Authors: </strong>Md Talha Mohsin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14987">https://arxiv.org/abs/2509.14987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14987">https://arxiv.org/pdf/2509.14987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14987]] Blockchain-Enabled Explainable AI for Trusted Healthcare Systems(https://arxiv.org/abs/2509.14987)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, federate, interpretability</a></li>
<li><strong>Abstract: </strong>This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF) for healthcare systems to tackle two essential challenges confronting health information networks: safe data exchange and comprehensible AI-driven clinical decision-making. Our architecture incorporates blockchain, ensuring patient records are immutable, auditable, and tamper-proof, alongside Explainable AI (XAI) methodologies that yield transparent and clinically relevant model predictions. By incorporating security assurances and interpretability requirements into a unified optimization pipeline, BXHF ensures both data-level trust (by verified and encrypted record sharing) and decision-level trust (with auditable and clinically aligned explanations). Its hybrid edge-cloud architecture allows for federated computation across different institutions, enabling collaborative analytics while protecting patient privacy. We demonstrate the framework's applicability through use cases such as cross-border clinical research networks, uncommon illness detection and high-risk intervention decision support. By ensuring transparency, auditability, and regulatory compliance, BXHF improves the credibility, uptake, and effectiveness of AI in healthcare, laying the groundwork for safer and more reliable clinical decision-making.</li>
</ul>

<h3>Title: UCorr: Wire Detection and Depth Estimation for Autonomous Drones</h3>
<ul>
<li><strong>Authors: </strong>Benedikt Kolbeinsson, Krystian Mikolajczyk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14989">https://arxiv.org/abs/2509.14989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14989">https://arxiv.org/pdf/2509.14989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14989]] UCorr: Wire Detection and Depth Estimation for Autonomous Drones(https://arxiv.org/abs/2509.14989)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In the realm of fully autonomous drones, the accurate detection of obstacles is paramount to ensure safe navigation and prevent collisions. Among these challenges, the detection of wires stands out due to their slender profile, which poses a unique and intricate problem. To address this issue, we present an innovative solution in the form of a monocular end-to-end model for wire segmentation and depth estimation. Our approach leverages a temporal correlation layer trained on synthetic data, providing the model with the ability to effectively tackle the complex joint task of wire detection and depth estimation. We demonstrate the superiority of our proposed method over existing competitive approaches in the joint task of wire detection and depth estimation. Our results underscore the potential of our model to enhance the safety and precision of autonomous drones, shedding light on its promising applications in real-world scenarios.</li>
</ul>

<h3>Title: No Modality Left Behind: Adapting to Missing Modalities via Knowledge Distillation for Brain Tumor Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shenghao Zhu, Yifei Chen, Weihong Chen, Shuo Jiang, Guanyu Zhou, Yuanhan Wang, Feiwei Qin, Changmiao Wang, Qiyuan Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15017">https://arxiv.org/abs/2509.15017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15017">https://arxiv.org/pdf/2509.15017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15017]] No Modality Left Behind: Adapting to Missing Modalities via Knowledge Distillation for Brain Tumor Segmentation(https://arxiv.org/abs/2509.15017)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate brain tumor segmentation is essential for preoperative evaluation and personalized treatment. Multi-modal MRI is widely used due to its ability to capture complementary tumor features across different sequences. However, in clinical practice, missing modalities are common, limiting the robustness and generalizability of existing deep learning methods that rely on complete inputs, especially under non-dominant modality combinations. To address this, we propose AdaMM, a multi-modal brain tumor segmentation framework tailored for missing-modality scenarios, centered on knowledge distillation and composed of three synergistic modules. The Graph-guided Adaptive Refinement Module explicitly models semantic associations between generalizable and modality-specific features, enhancing adaptability to modality absence. The Bi-Bottleneck Distillation Module transfers structural and textural knowledge from teacher to student models via global style matching and adversarial feature alignment. The Lesion-Presence-Guided Reliability Module predicts prior probabilities of lesion types through an auxiliary classification task, effectively suppressing false positives under incomplete inputs. Extensive experiments on the BraTS 2018 and 2024 datasets demonstrate that AdaMM consistently outperforms existing methods, exhibiting superior segmentation accuracy and robustness, particularly in single-modality and weak-modality configurations. In addition, we conduct a systematic evaluation of six categories of missing-modality strategies, confirming the superiority of knowledge distillation and offering practical guidance for method selection and future research. Our source code is available at this https URL.</li>
</ul>

<h3>Title: Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mario Sanz-Guerrero, Minh Duc Bui, Katharina von der Wense</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15020">https://arxiv.org/abs/2509.15020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15020">https://arxiv.org/pdf/2509.15020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15020]] Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs(https://arxiv.org/abs/2509.15020)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string "Answer:" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy -- tokenizing the space together with the answer letter -- as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model's confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results.</li>
</ul>

<h3>Title: Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering</h3>
<ul>
<li><strong>Authors: </strong>Xuanting Xie, Bingheng Li, Erlin Pan, Rui Hou, Wenyu Chen, Zhao Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15024">https://arxiv.org/abs/2509.15024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15024">https://arxiv.org/pdf/2509.15024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15024]] Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering(https://arxiv.org/abs/2509.15024)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Attention mechanisms have become a cornerstone in modern neural networks, driving breakthroughs across diverse domains. However, their application to graph structured data, where capturing topological connections is essential, remains underexplored and underperforming compared to Graph Neural Networks (GNNs), particularly in the graph clustering task. GNN tends to overemphasize neighborhood aggregation, leading to a homogenization of node representations. Conversely, Transformer tends to over globalize, highlighting distant nodes at the expense of meaningful local patterns. This dichotomy raises a key question: Is attention inherently redundant for unsupervised graph learning? To address this, we conduct a comprehensive empirical analysis, uncovering the complementary weaknesses of GNN and Transformer in graph clustering. Motivated by these insights, we propose the Attentive Graph Clustering Network (AGCN) a novel architecture that reinterprets the notion that graph is attention. AGCN directly embeds the attention mechanism into the graph structure, enabling effective global information extraction while maintaining sensitivity to local topological cues. Our framework incorporates theoretical analysis to contrast AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV cache mechanism to improve computational efficiency, and (2) a pairwise margin contrastive loss to boost the discriminative capacity of the attention space. Extensive experimental results demonstrate that AGCN outperforms state-of-the-art methods.</li>
</ul>

<h3>Title: CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Thomas Huber, Christina Niklaus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15027">https://arxiv.org/abs/2509.15027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15027">https://arxiv.org/pdf/2509.15027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15027]] CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models(https://arxiv.org/abs/2509.15027)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.</li>
</ul>

<h3>Title: AutoEdit: Automatic Hyperparameter Tuning for Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Chau Pham, Quan Dao, Mahesh Bhosale, Yunjie Tian, Dimitris Metaxas, David Doermann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15031">https://arxiv.org/abs/2509.15031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15031">https://arxiv.org/pdf/2509.15031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15031]] AutoEdit: Automatic Hyperparameter Tuning for Image Editing(https://arxiv.org/abs/2509.15031)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing's hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world.</li>
</ul>

<h3>Title: Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Padmaksha Roy, Almuatazbellah Boker, Lamine Mili</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15033">https://arxiv.org/abs/2509.15033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15033">https://arxiv.org/pdf/2509.15033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15033]] Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection(https://arxiv.org/abs/2509.15033)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we aim to improve multivariate anomaly detection (AD) by modeling the \textit{time-varying non-linear spatio-temporal correlations} found in multivariate time series data . In multivariate time series data, an anomaly may be indicated by the simultaneous deviation of interrelated time series from their expected collective behavior, even when no individual time series exhibits a clearly abnormal pattern on its own. In many existing approaches, time series variables are assumed to be (conditionally) independent, which oversimplifies real-world interactions. Our approach addresses this by modeling joint dependencies in the latent space and decoupling the modeling of \textit{marginal distributions, temporal dynamics, and inter-variable dependencies}. We use a transformer encoder to capture temporal patterns, and to model spatial (inter-variable) dependencies, we fit a multi-variate likelihood and a copula. The temporal and the spatial components are trained jointly in a latent space using a self-supervised contrastive learning objective to learn meaningful feature representations to separate normal and anomaly samples.</li>
</ul>

<h3>Title: From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets</h3>
<ul>
<li><strong>Authors: </strong>Juwon Kim, Hyunwook Lee, Hyotaek Jeon, Seungmin Jin, Sungahn Ko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15040">https://arxiv.org/abs/2509.15040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15040">https://arxiv.org/pdf/2509.15040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15040]] From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets(https://arxiv.org/abs/2509.15040)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Directional forecasting in financial markets requires both accuracy and interpretability. Before the advent of deep learning, interpretable approaches based on human-defined patterns were prevalent, but their structural vagueness and scale ambiguity hindered generalization. In contrast, deep learning models can effectively capture complex dynamics, yet often offer limited transparency. To bridge this gap, we propose a two-stage framework that integrates unsupervised pattern extracion with interpretable forecasting. (i) SIMPC segments and clusters multivariate time series, extracting recurrent patterns that are invariant to amplitude scaling and temporal distortion, even under varying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses the initial part of extracted patterns as input and forecasts subsequent partial sequences for short-term directional movement. Experiments on Bitcoin and three S&P 500 equities demonstrate that our method ranks first or second in 11 out of 12 metric--dataset combinations, consistently outperforming baselines. Unlike conventional deep learning models that output buy-or-sell signals without interpretable justification, our approach enables transparent decision-making by revealing the underlying pattern structures that drive predictive outcomes.</li>
</ul>

<h3>Title: Reinforcement Learning Agent for a 2D Shooter Game</h3>
<ul>
<li><strong>Authors: </strong>Thomas Ackermann, Moritz Spang, Hamza A. A. Gardi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15042">https://arxiv.org/abs/2509.15042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15042">https://arxiv.org/pdf/2509.15042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15042]] Reinforcement Learning Agent for a 2D Shooter Game(https://arxiv.org/abs/2509.15042)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Reinforcement learning agents in complex game environments often suffer from sparse rewards, training instability, and poor sample efficiency. This paper presents a hybrid training approach that combines offline imitation learning with online reinforcement learning for a 2D shooter game agent. We implement a multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Initial experiments using pure deep Q-Networks exhibited significant instability, with agents frequently reverting to poor policies despite occasional good performance. To address this, we developed a hybrid methodology that begins with behavioral cloning on demonstration data from rule-based agents, then transitions to reinforcement learning. Our hybrid approach achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure reinforcement learning methods which showed high variance and frequent performance degradation. The multi-head architecture enables effective knowledge transfer between learning modes while maintaining training stability. Results demonstrate that combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments where pure exploration proves insufficient.</li>
</ul>

<h3>Title: Communication Efficient Split Learning of ViTs with Attention-based Double Compression</h3>
<ul>
<li><strong>Authors: </strong>Federico Alvetreti, Jary Pomponi, Paolo Di Lorenzo, Simone Scardapane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15058">https://arxiv.org/abs/2509.15058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15058">https://arxiv.org/pdf/2509.15058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15058]] Communication Efficient Split Learning of ViTs with Attention-based Double Compression(https://arxiv.org/abs/2509.15058)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.</li>
</ul>

<h3>Title: Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Saleh Vahdatpour, Maryam Eyvazi, Yanqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15076">https://arxiv.org/abs/2509.15076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15076">https://arxiv.org/pdf/2509.15076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15076]] Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models(https://arxiv.org/abs/2509.15076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.</li>
</ul>

<h3>Title: Transplant-Ready? Evaluating AI Lung Segmentation Models in Candidates with Severe Lung Disease</h3>
<ul>
<li><strong>Authors: </strong>Jisoo Lee, Michael R. Harowicz, Yuwen Chen, Hanxue Gu, Isaac S. Alderete, Lin Li, Maciej A. Mazurowski, Matthew G. Hartwig</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15083">https://arxiv.org/abs/2509.15083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15083">https://arxiv.org/pdf/2509.15083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15083]] Transplant-Ready? Evaluating AI Lung Segmentation Models in Candidates with Severe Lung Disease(https://arxiv.org/abs/2509.15083)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This study evaluates publicly available deep-learning based lung segmentation models in transplant-eligible patients to determine their performance across disease severity levels, pathology categories, and lung sides, and to identify limitations impacting their use in preoperative planning in lung transplantation. This retrospective study included 32 patients who underwent chest CT scans at Duke University Health System between 2017 and 2019 (total of 3,645 2D axial slices). Patients with standard axial CT scans were selected based on the presence of two or more lung pathologies of varying severity. Lung segmentation was performed using three previously developed deep learning models: Unet-R231, TotalSegmentator, MedSAM. Performance was assessed using quantitative metrics (volumetric similarity, Dice similarity coefficient, Hausdorff distance) and a qualitative measure (four-point clinical acceptability scale). Unet-R231 consistently outperformed TotalSegmentator and MedSAM in general, for different severity levels, and pathology categories (p<0.05). All models showed significant performance declines from mild to moderate-to-severe cases, particularly in volumetric similarity (p<0.05), without significant differences among lung sides or pathology types. Unet-R231 provided the most accurate automated lung segmentation among evaluated models with TotalSegmentator being a close second, though their performance declined significantly in moderate-to-severe cases, emphasizing the need for specialized model fine-tuning in severe pathology contexts.</li>
</ul>

<h3>Title: Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Lei Wang, Jieming Bian, Letian Zhang, Jie Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15087">https://arxiv.org/abs/2509.15087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15087">https://arxiv.org/pdf/2509.15087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15087]] Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning(https://arxiv.org/abs/2509.15087)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive capabilities across various tasks, but fine-tuning them for domain-specific applications often requires substantial domain-specific data that may be distributed across multiple organizations. Federated Learning (FL) offers a privacy-preserving solution, but faces challenges with computational constraints when applied to LLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning approach, though a single LoRA module often struggles with heterogeneous data across diverse domains. This paper addresses two critical challenges in federated LoRA fine-tuning: 1. determining the optimal number and allocation of LoRA experts across heterogeneous clients, and 2. enabling clients to selectively utilize these experts based on their specific data characteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation and SElection), a novel framework that adaptively clusters clients based on representation similarity to allocate and train domain-specific LoRA experts. It also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows each client to select the optimal number of utilized experts. Our extensive experiments on diverse benchmark datasets demonstrate that FedLEASE significantly outperforms existing federated fine-tuning approaches in heterogeneous client settings while maintaining communication efficiency.</li>
</ul>

<h3>Title: LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyao Tu, Liang Zhang, Yujie Lin, Xin Lin, Haibo Zhang, Long Zhang, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15089">https://arxiv.org/abs/2509.15089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15089">https://arxiv.org/pdf/2509.15089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15089]] LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models(https://arxiv.org/abs/2509.15089)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The goal of open relation extraction (OpenRE) is to develop an RE model that can generalize to new relations not encountered during training. Existing studies primarily formulate OpenRE as a clustering task. They first cluster all test instances based on the similarity between the instances, and then manually assign a new relation to each cluster. However, their reliance on human annotation limits their practicality. In this paper, we propose an OpenRE framework based on large language models (LLMs), which directly predicts new relations for test instances by leveraging their strong language understanding and generation abilities, without human intervention. Specifically, our framework consists of two core components: (1) a relation discoverer (RD), designed to predict new relations for test instances based on \textit{demonstrations} formed by training instances with known relations; and (2) a relation predictor (RP), used to select the most likely relation for a test instance from $n$ candidate relations, guided by \textit{demonstrations} composed of their instances. To enhance the ability of our framework to predict new relations, we design a self-correcting inference strategy composed of three stages: relation discovery, relation denoising, and relation prediction. In the first stage, we use RD to preliminarily predict new relations for all test instances. Next, we apply RP to select some high-reliability test instances for each new relation from the prediction results of RD through a cross-validation method. During the third stage, we employ RP to re-predict the relations of all test instances based on the demonstrations constructed from these reliable test instances. Extensive experiments on three OpenRE datasets demonstrate the effectiveness of our framework. We release our code at this https URL.</li>
</ul>

<h3>Title: OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bo-Wen Yin, Jiao-Long Cao, Xuying Zhang, Yuming Chen, Ming-Ming Cheng, Qibin Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15096">https://arxiv.org/abs/2509.15096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15096">https://arxiv.org/pdf/2509.15096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15096]] OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation(https://arxiv.org/abs/2509.15096)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Recent research on representation learning has proved the merits of multi-modal clues for robust semantic segmentation. Nevertheless, a flexible pretrain-and-finetune pipeline for multiple visual modalities remains unexplored. In this paper, we propose a novel multi-modal learning framework, termed OmniSegmentor. It has two key innovations: 1) Based on ImageNet, we assemble a large-scale dataset for multi-modal pretraining, called ImageNeXt, which contains five popular visual modalities. 2) We provide an efficient pretraining manner to endow the model with the capacity to encode different modality information in the ImageNeXt. For the first time, we introduce a universal multi-modal pretraining framework that consistently amplifies the model's perceptual capabilities across various scenarios, regardless of the arbitrary combination of the involved modalities. Remarkably, our OmniSegmentor achieves new state-of-the-art records on a wide range of multi-modal semantic segmentation datasets, including NYU Depthv2, EventScape, MFNet, DeLiVER, SUNRGBD, and KITTI-360.</li>
</ul>

<h3>Title: The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Saleh Vahdatpour, Huaiyuan Chu, Yanqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15097">https://arxiv.org/abs/2509.15097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15097">https://arxiv.org/pdf/2509.15097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15097]] The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning(https://arxiv.org/abs/2509.15097)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The rising computational and energy demands of deep learning, particularly in large-scale architectures such as foundation models and large language models (LLMs), pose significant challenges to sustainability. Traditional gradient-based training methods are inefficient, requiring numerous iterative updates and high power consumption. To address these limitations, we propose a hybrid framework that combines hierarchical decomposition with FPGA-based direct equation solving and incremental learning. Our method divides the neural network into two functional tiers: lower layers are optimized via single-step equation solving on FPGAs for efficient and parallelizable feature extraction, while higher layers employ adaptive incremental learning to support continual updates without full retraining. Building upon this foundation, we introduce the Compound LLM framework, which explicitly deploys LLM modules across both hierarchy levels. The lower-level LLM handles reusable representation learning with minimal energy overhead, while the upper-level LLM performs adaptive decision-making through energy-aware updates. This integrated design enhances scalability, reduces redundant computation, and aligns with the principles of sustainable AI. Theoretical analysis and architectural insights demonstrate that our method reduces computational costs significantly while preserving high model performance, making it well-suited for edge deployment and real-time adaptation in energy-constrained environments.</li>
</ul>

<h3>Title: TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action</h3>
<ul>
<li><strong>Authors: </strong>Chenyue Zhou, Gürkan Solmaz, Flavio Cirillo, Kiril Gashteovski, Jonathan Fürst</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15098">https://arxiv.org/abs/2509.15098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15098">https://arxiv.org/pdf/2509.15098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15098]] TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action(https://arxiv.org/abs/2509.15098)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Humanitarian Mine Action has generated extensive best-practice knowledge, but much remains locked in unstructured reports. We introduce TextMine, an ontology-guided pipeline that uses Large Language Models to extract knowledge triples from HMA texts. TextMine integrates document chunking, domain-aware prompting, triple extraction, and both reference-based and LLM-as-a-Judge evaluation. We also create the first HMA ontology and a curated dataset of real-world demining reports. Experiments show ontology-aligned prompts boost extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format conformance by 20.9% over baselines. While validated on Cambodian reports, TextMine can adapt to global demining efforts or other domains, transforming unstructured data into structured knowledge.</li>
</ul>

<h3>Title: Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Liran Nochumsohn, Raz Marshanski, Hedi Zisling, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15105">https://arxiv.org/abs/2509.15105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15105">https://arxiv.org/pdf/2509.15105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15105]] Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting(https://arxiv.org/abs/2509.15105)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Time series forecasting (TSF) is critical in domains like energy, finance, healthcare, and logistics, requiring models that generalize across diverse datasets. Large pre-trained models such as Chronos and Time-MoE show strong zero-shot (ZS) performance but suffer from high computational costs. In this work, We introduce Super-Linear, a lightweight and scalable mixture-of-experts (MoE) model for general forecasting. It replaces deep architectures with simple frequency-specialized linear experts, trained on resampled data across multiple frequency regimes. A lightweight spectral gating mechanism dynamically selects relevant experts, enabling efficient, accurate forecasting. Despite its simplicity, Super-Linear matches state-of-the-art performance while offering superior efficiency, robustness to various sampling rates, and enhanced interpretability. The implementation of Super-Linear is available at \href{this https URL}{this https URL}</li>
</ul>

<h3>Title: Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges</h3>
<ul>
<li><strong>Authors: </strong>Amy Rafferty, Rishi Ramaesh, Ajitha Rajan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15107">https://arxiv.org/abs/2509.15107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15107">https://arxiv.org/pdf/2509.15107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15107]] Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges(https://arxiv.org/abs/2509.15107)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair</a></li>
<li><strong>Abstract: </strong>Artificial intelligence has shown significant promise in chest radiography, where deep learning models can approach radiologist-level diagnostic performance. Progress has been accelerated by large public datasets such as MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of thousands of labelled images with pathology annotations. However, these datasets also present important limitations. Automated label extraction from radiology reports introduces errors, particularly in handling uncertainty and negation, and radiologist review frequently disagrees with assigned labels. In addition, domain shift and population bias restrict model generalisability, while evaluation practices often overlook clinically meaningful measures. We conduct a systematic analysis of these challenges, focusing on label quality, dataset bias, and domain shift. Our cross-dataset domain shift evaluation across multiple model architectures revealed substantial external performance degradation, with pronounced reductions in AUPRC and F1 scores relative to internal testing. To assess dataset bias, we trained a source-classification model that distinguished datasets with near-perfect accuracy, and performed subgroup analyses showing reduced performance for minority age and sex groups. Finally, expert review by two board-certified radiologists identified significant disagreement with public dataset labels. Our findings highlight important clinical weaknesses of current benchmarks and emphasise the need for clinician-validated datasets and fairer evaluation frameworks.</li>
</ul>

<h3>Title: Large Language Model probabilities cannot distinguish between possible and impossible language</h3>
<ul>
<li><strong>Authors: </strong>Evelina Leivada, Raquel Montero, Paolo Morosi, Natalia Moskvina, Tamara Serrano, Marcel Aguilar, Fritz Guenther</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15114">https://arxiv.org/abs/2509.15114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15114">https://arxiv.org/pdf/2509.15114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15114]] Large Language Model probabilities cannot distinguish between possible and impossible language(https://arxiv.org/abs/2509.15114)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A controversial test for Large Language Models concerns the ability to discern possible from impossible language. While some evidence attests to the models' sensitivity to what crosses the limits of grammatically impossible language, this evidence has been contested on the grounds of the soundness of the testing material. We use model-internal representations to tap directly into the way Large Language Models represent the 'grammatical-ungrammatical' distinction. In a novel benchmark, we elicit probabilities from 4 models and compute minimal-pair surprisal differences, juxtaposing probabilities assigned to grammatical sentences to probabilities assigned to (i) lower frequency grammatical sentences, (ii) ungrammatical sentences, (iii) semantically odd sentences, and (iv) pragmatically odd sentences. The prediction is that if string-probabilities can function as proxies for the limits of grammar, the ungrammatical condition will stand out among the conditions that involve linguistic violations, showing a spike in the surprisal rates. Our results do not reveal a unique surprisal signature for ungrammatical prompts, as the semantically and pragmatically odd conditions consistently show higher surprisal. We thus demonstrate that probabilities do not constitute reliable proxies for model-internal representations of syntactic knowledge. Consequently, claims about models being able to distinguish possible from impossible language need verification through a different methodology.</li>
</ul>

<h3>Title: RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes</h3>
<ul>
<li><strong>Authors: </strong>Fang Li, Hao Zhang, Narendra Ahuja</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15123">https://arxiv.org/abs/2509.15123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15123">https://arxiv.org/pdf/2509.15123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15123]] RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes(https://arxiv.org/abs/2509.15123)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.</li>
</ul>

<h3>Title: Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Viktor Kovalchuk, Nikita Kotelevskii, Maxim Panov, Samuel Horváth, Martin Takáč</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15147">https://arxiv.org/abs/2509.15147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15147">https://arxiv.org/pdf/2509.15147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15147]] Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning(https://arxiv.org/abs/2509.15147)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) usually shares model weights or gradients, which is costly for large models. Logit-based FL reduces this cost by sharing only logits computed on a public proxy dataset. However, aggregating information from heterogeneous clients is still challenging. This paper studies this problem, introduces and compares three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead, improve robustness under non-IID data, and achieve accuracy competitive with centralized training.</li>
</ul>

<h3>Title: A1: Asynchronous Test-Time Scaling via Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jing Xiong, Qiujiang Chen, Fanghua Ye, Zhongwei Wan, Chuanyang Zheng, Chenyang Zhao, Hui Shen, Alexander Hanbo Li, Chaofan Tao, Haochen Tan, Haoli Bai, Lifeng Shang, Lingpeng Kong, Ngai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15148">https://arxiv.org/abs/2509.15148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15148">https://arxiv.org/pdf/2509.15148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15148]] A1: Asynchronous Test-Time Scaling via Conformal Prediction(https://arxiv.org/abs/2509.15148)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) benefit from test-time scaling, but existing methods face significant challenges, including severe synchronization overhead, memory bottlenecks, and latency, especially during speculative decoding with long reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a statistically guaranteed adaptive inference framework that addresses these challenges. A1 refines arithmetic intensity to identify synchronization as the dominant bottleneck, proposes an online calibration strategy to enable asynchronous inference, and designs a three-stage rejection sampling pipeline that supports both sequential and parallel scaling. Through experiments on the MATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model families, we demonstrate that A1 achieves a remarkable 56.7x speedup in test-time scaling and a 4.14x improvement in throughput, all while maintaining accurate rejection-rate control, reducing latency and memory overhead, and no accuracy loss compared to using target model scaling alone. These results position A1 as an efficient and principled solution for scalable LLM inference. We have released the code at this https URL.</li>
</ul>

<h3>Title: Self-Improving Embodied Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, Igor Mordatch</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15155">https://arxiv.org/abs/2509.15155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15155">https://arxiv.org/pdf/2509.15155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15155]] Self-Improving Embodied Foundation Models(https://arxiv.org/abs/2509.15155)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at this https URL .</li>
</ul>

<h3>Title: Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Haobo Yang, Minghao Guo, Dequan Yang, Wenyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15156">https://arxiv.org/abs/2509.15156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15156">https://arxiv.org/pdf/2509.15156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15156]] Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models(https://arxiv.org/abs/2509.15156)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.</li>
</ul>

<h3>Title: Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15157">https://arxiv.org/abs/2509.15157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15157">https://arxiv.org/pdf/2509.15157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15157]] Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning(https://arxiv.org/abs/2509.15157)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at this https URL.</li>
</ul>

<h3>Title: AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt</h3>
<ul>
<li><strong>Authors: </strong>Saket S. Chaturvedi, Gaurav Bagwe, Lan Zhang, Xiaoyong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15159">https://arxiv.org/abs/2509.15159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15159">https://arxiv.org/pdf/2509.15159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15159]] AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt(https://arxiv.org/abs/2509.15159)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly. We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.</li>
</ul>

<h3>Title: Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model</h3>
<ul>
<li><strong>Authors: </strong>Pak-Hei Yeung, Jayroop Ramesh, Pengfei Lyu, Ana Namburete, Jagath Rajapakse</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15167">https://arxiv.org/abs/2509.15167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15167">https://arxiv.org/pdf/2509.15167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15167]] Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model(https://arxiv.org/abs/2509.15167)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at this https URL.</li>
</ul>

<h3>Title: Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Aarushi Mahajan, Wayne Burleson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15170">https://arxiv.org/abs/2509.15170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15170">https://arxiv.org/pdf/2509.15170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15170]] Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting(https://arxiv.org/abs/2509.15170)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>Radio frequency fingerprint identification (RFFI) distinguishes wireless devices by the small variations in their analog circuits, avoiding heavy cryptographic authentication. While deep learning on spectrograms improves accuracy, models remain vulnerable to copying, tampering, and evasion. We present a stronger RFFI system combining watermarking for ownership proof and anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel spectrograms, we embed three watermarks: a simple trigger, an adversarially trained trigger robust to noise and filtering, and a hidden gradient/weight signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler (KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset, our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC, offering verifiable, tamper-resistant authentication.</li>
</ul>

<h3>Title: SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huy Nghiem, Advik Sachdeva, Hal Daumé III</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15174">https://arxiv.org/abs/2509.15174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15174">https://arxiv.org/pdf/2509.15174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15174]] SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models(https://arxiv.org/abs/2509.15174)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.</li>
</ul>

<h3>Title: Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding</h3>
<ul>
<li><strong>Authors: </strong>Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W.H. Lau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15178">https://arxiv.org/abs/2509.15178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15178">https://arxiv.org/pdf/2509.15178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15178]] Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding(https://arxiv.org/abs/2509.15178)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks. The code will be available at this https URL.</li>
</ul>

<h3>Title: Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11, YOLOv12 and Faster-RCNN</h3>
<ul>
<li><strong>Authors: </strong>Dewi Endah Kharismawati, Toni Kazic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15181">https://arxiv.org/abs/2509.15181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15181">https://arxiv.org/pdf/2509.15181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15181]] Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11, YOLOv12 and Faster-RCNN(https://arxiv.org/abs/2509.15181)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate maize seedling detection is crucial for precision agriculture, yet curated datasets remain scarce. We introduce MSDD, a high-quality aerial image dataset for maize seedling stand counting, with applications in early-season crop monitoring, yield prediction, and in-field management. Stand counting determines how many plants germinated, guiding timely decisions such as replanting or adjusting inputs. Traditional methods are labor-intensive and error-prone, while computer vision enables efficient, accurate detection. MSDD contains three classes-single, double, and triple plants-capturing diverse growth stages, planting setups, soil types, lighting conditions, camera angles, and densities, ensuring robustness for real-world use. Benchmarking shows detection is most reliable during V4-V6 stages and under nadir views. Among tested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for single plants. Single plant detection achieves precision up to 0.984 and recall up to 0.873, but detecting doubles and triples remains difficult due to rarity and irregular appearance, often from planting errors. Class imbalance further reduces accuracy in multi-plant detection. Despite these challenges, YOLO11 maintains efficient inference at 35 ms per image, with an additional 120 ms for saving outputs. MSDD establishes a strong foundation for developing models that enhance stand counting, optimize resource allocation, and support real-time decision-making. This dataset marks a step toward automating agricultural monitoring and advancing precision agriculture.</li>
</ul>

<h3>Title: Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Yue, Zidong Wang, Yuqing Wang, Wenlong Zhang, Xihui Liu, Wanli Ouyang, Lei Bai, Luping Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15185">https://arxiv.org/abs/2509.15185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15185">https://arxiv.org/pdf/2509.15185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15185]] Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation(https://arxiv.org/abs/2509.15185)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.</li>
</ul>

<h3>Title: Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yeongbin Seo, Dongha Lee, Jaehyung Kim, Jinyoung Yeo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15188">https://arxiv.org/abs/2509.15188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15188">https://arxiv.org/pdf/2509.15188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15188]] Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning(https://arxiv.org/abs/2509.15188)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.</li>
</ul>

<h3>Title: Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</h3>
<ul>
<li><strong>Authors: </strong>Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15194">https://arxiv.org/abs/2509.15194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15194">https://arxiv.org/pdf/2509.15194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15194]] Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation(https://arxiv.org/abs/2509.15194)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.</li>
</ul>

<h3>Title: Explaining deep learning for ECG using time-localized clusters</h3>
<ul>
<li><strong>Authors: </strong>Ahcène Boubekki, Konstantinos Patlatzoglou, Joseph Barker, Fu Siong Ng, Antônio H. Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15198">https://arxiv.org/abs/2509.15198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15198">https://arxiv.org/pdf/2509.15198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15198]] Explaining deep learning for ECG using time-localized clusters(https://arxiv.org/abs/2509.15198)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning has significantly advanced electrocardiogram (ECG) analysis, enabling automatic annotation, disease screening, and prognosis beyond traditional clinical capabilities. However, understanding these models remains a challenge, limiting interpretation and gaining knowledge from these developments. In this work, we propose a novel interpretability method for convolutional neural networks applied to ECG analysis. Our approach extracts time-localized clusters from the model's internal representations, segmenting the ECG according to the learned characteristics while quantifying the uncertainty of these representations. This allows us to visualize how different waveform regions contribute to the model's predictions and assess the certainty of its decisions. By providing a structured and interpretable view of deep learning models for ECG, our method enhances trust in AI-driven diagnostics and facilitates the discovery of clinically relevant electrophysiological patterns.</li>
</ul>

<h3>Title: CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness</h3>
<ul>
<li><strong>Authors: </strong>Ying Zheng, Yangfan Jiang, Kian-Lee Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15199">https://arxiv.org/abs/2509.15199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15199">https://arxiv.org/pdf/2509.15199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15199]] CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness(https://arxiv.org/abs/2509.15199)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair</a></li>
<li><strong>Abstract: </strong>Causal fairness in databases is crucial to preventing biased and inaccurate outcomes in downstream tasks. While most prior work assumes a known causal model, recent efforts relax this assumption by enforcing additional constraints. However, these approaches often fail to capture broader attribute relationships that are critical to maintaining utility. This raises a fundamental question: Can we harness the benefits of causal reasoning to design efficient and effective fairness solutions without relying on strong assumptions about the underlying causal model? In this paper, we seek to answer this question by introducing CausalPre, a scalable and effective causality-guided data pre-processing framework that guarantees justifiable fairness, a strong causal notion of fairness. CausalPre extracts causally fair relationships by reformulating the originally complex and computationally infeasible extraction task into a tailored distribution estimation problem. To ensure scalability, CausalPre adopts a carefully crafted variant of low-dimensional marginal factorization to approximate the joint distribution, complemented by a heuristic algorithm that efficiently tackles the associated computational challenge. Extensive experiments on benchmark datasets demonstrate that CausalPre is both effective and scalable, challenging the conventional belief that achieving causal fairness requires trading off relationship coverage for relaxed model assumptions.</li>
</ul>

<h3>Title: Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction</h3>
<ul>
<li><strong>Authors: </strong>Yuanbo Xie, Yingjie Zhang, Tianyun Liu, Duohe Ma, Tingwen Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15202">https://arxiv.org/abs/2509.15202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15202">https://arxiv.org/pdf/2509.15202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15202]] Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction(https://arxiv.org/abs/2509.15202)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks pose persistent threats to large language models (LLMs). Current safety alignment methods have attempted to address these issues, but they experience two significant limitations: insufficient safety alignment depth and unrobust internal defense mechanisms. These limitations make them vulnerable to adversarial attacks such as prefilling and refusal direction manipulation. We introduce DeepRefusal, a robust safety alignment framework that overcomes these issues. DeepRefusal forces the model to dynamically rebuild its refusal mechanisms from jailbreak states. This is achieved by probabilistically ablating the refusal direction across layers and token depths during fine-tuning. Our method not only defends against prefilling and refusal direction attacks but also demonstrates strong resilience against other unseen jailbreak strategies. Extensive evaluations on four open-source LLM families and six representative attacks show that DeepRefusal reduces attack success rates by approximately 95%, while maintaining model capabilities with minimal performance degradation.</li>
</ul>

<h3>Title: Fair-GPTQ: Bias-Aware Quantization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Irina Proskurina, Guillaume Metzler, Julien Velcin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15206">https://arxiv.org/abs/2509.15206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15206">https://arxiv.org/pdf/2509.15206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15206]] Fair-GPTQ: Bias-Aware Quantization for Large Language Models(https://arxiv.org/abs/2509.15206)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>High memory demands of generative language models have drawn attention to quantization, which reduces computational cost, memory usage, and latency by mapping model weights to lower-precision integers. Approaches such as GPTQ effectively minimize input-weight product errors during quantization; however, recent empirical studies show that they can increase biased outputs and degrade performance on fairness benchmarks, and it remains unclear which specific weights cause this issue. In this work, we draw new links between quantization and model fairness by adding explicit group-fairness constraints to the quantization objective and introduce Fair-GPTQ, the first quantization method explicitly designed to reduce unfairness in large language models. The added constraints guide the learning of the rounding operation toward less-biased text generation for protected groups. Specifically, we focus on stereotype generation involving occupational bias and discriminatory language spanning gender, race, and religion. Fair-GPTQ has minimal impact on performance, preserving at least 90% of baseline accuracy on zero-shot benchmarks, reduces unfairness relative to a half-precision model, and retains the memory and speed benefits of 4-bit quantization. We also compare the performance of Fair-GPTQ with existing debiasing methods and find that it achieves performance on par with the iterative null-space projection debiasing approach on racial-stereotype benchmarks. Overall, the results validate our theoretical solution to the quantization problem with a group-bias term, highlight its applicability for reducing group bias at quantization time in generative models, and demonstrate that our approach can further be used to analyze channel- and weight-level contributions to fairness during quantization.</li>
</ul>

<h3>Title: FlowRL: Matching Reward Distributions for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo Hua, Yuxin Zuo, Xingtai Lv, Qizheng Zhang, Lin Chen, Fanghao Shao, Bo Xue, Yunchong Song, Zhenjie Yang, Ganqu Cui, Ning Ding, Jianfeng Gao, Xiaodong Liu, Bowen Zhou, Hongyuan Mei, Zhouhan Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15207">https://arxiv.org/abs/2509.15207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15207">https://arxiv.org/pdf/2509.15207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15207]] FlowRL: Matching Reward Distributions for LLM Reasoning(https://arxiv.org/abs/2509.15207)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.</li>
</ul>

<h3>Title: Geometric Image Synchronization with Deep Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Pierre Fernandez, Tomáš Souček, Nikola Jovanović, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15208">https://arxiv.org/abs/2509.15208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15208">https://arxiv.org/pdf/2509.15208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15208]] Geometric Image Synchronization with Deep Watermarking(https://arxiv.org/abs/2509.15208)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>Synchronization is the task of estimating and inverting geometric transformations (e.g., crop, rotation) applied to an image. This work introduces SyncSeal, a bespoke watermarking method for robust image synchronization, which can be applied on top of existing watermarking methods to enhance their robustness against geometric transformations. It relies on an embedder network that imperceptibly alters images and an extractor network that predicts the geometric transformation to which the image was subjected. Both networks are end-to-end trained to minimize the error between the predicted and ground-truth parameters of the transformation, combined with a discriminator to maintain high perceptual quality. We experimentally validate our method on a wide variety of geometric and valuemetric transformations, demonstrating its effectiveness in accurately synchronizing images. We further show that our synchronization can effectively upgrade existing watermarking methods to withstand geometric transformations to which they were previously vulnerable.</li>
</ul>

<h3>Title: What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques</h3>
<ul>
<li><strong>Authors: </strong>Petros Stylianos Giouroukis, Dimitris Dimitriadis, Dimitrios Papadopoulos, Zhenwen Shao, Grigorios Tsoumakas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15211">https://arxiv.org/abs/2509.15211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15211">https://arxiv.org/pdf/2509.15211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15211]] What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques(https://arxiv.org/abs/2509.15211)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Slide decks, serving as digital reports that bridge the gap between presentation slides and written documents, are a prevalent medium for conveying information in both academic and corporate settings. Their multimodal nature, combining text, images, and charts, presents challenges for retrieval-augmented generation systems, where the quality of retrieval directly impacts downstream performance. Traditional approaches to slide retrieval often involve separate indexing of modalities, which can increase complexity and lose contextual information. This paper investigates various methodologies for effective slide retrieval, including visual late-interaction embedding models like ColPali, the use of visual rerankers, and hybrid retrieval techniques that combine dense retrieval with BM25, further enhanced by textual rerankers and fusion methods like Reciprocal Rank Fusion. A novel Vision-Language Models-based captioning pipeline is also evaluated, demonstrating significantly reduced embedding storage requirements compared to visual late-interaction techniques, alongside comparable retrieval performance. Our analysis extends to the practical aspects of these methods, evaluating their runtime performance and storage demands alongside retrieval efficacy, thus offering practical guidance for the selection and development of efficient and robust slide retrieval systems for real-world applications.</li>
</ul>

<h3>Title: RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, Fan Wang, Deli Zhao, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15212">https://arxiv.org/abs/2509.15212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15212">https://arxiv.org/pdf/2509.15212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15212]] RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation(https://arxiv.org/abs/2509.15212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.</li>
</ul>

<h3>Title: Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Zhang, Zijian Huang, Sophie Chen, Erfan Shayegani, Jiasi Chen, Nael Abu-Ghazaleh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15213">https://arxiv.org/abs/2509.15213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15213">https://arxiv.org/pdf/2509.15213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15213]] Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems(https://arxiv.org/abs/2509.15213)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Extended reality (XR) applications increasingly integrate Large Language Models (LLMs) to enhance user experience, scene understanding, and even generate executable XR content, and are often called "AI glasses". Despite these potential benefits, the integrated XR-LLM pipeline makes XR applications vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR systems in the literature and in practice and categorize them along different dimensions from a systems perspective. Building on this categorization, we identify a common threat model and demonstrate a series of proof-of-concept attacks on multiple XR platforms that employ various LLM models (Meta Quest 3, Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models). Although these platforms each implement LLM integration differently, they share vulnerabilities where an attacker can modify the public context surrounding a legitimate LLM query, resulting in erroneous visual or auditory feedback to users, thus compromising their safety or privacy, sowing confusion, or other harmful effects. To defend against these threats, we discuss mitigation strategies and best practices for developers, including an initial defense prototype, and call on the community to develop new protection mechanisms to mitigate these risks.</li>
</ul>

<h3>Title: Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sreejato Chatterjee, Linh Tran, Quoc Duy Nguyen, Roni Kirson, Drue Hamlin, Harvest Aquino, Hanjia Lyu, Jiebo Luo, Timothy Dye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15216">https://arxiv.org/abs/2509.15216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15216">https://arxiv.org/pdf/2509.15216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15216]] Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models(https://arxiv.org/abs/2509.15216)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (this https URL).</li>
</ul>

<h3>Title: LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15218">https://arxiv.org/abs/2509.15218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15218">https://arxiv.org/pdf/2509.15218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15218]] LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models(https://arxiv.org/abs/2509.15218)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the model's greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at this https URL to facilitate research.</li>
</ul>

<h3>Title: Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15220">https://arxiv.org/abs/2509.15220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15220">https://arxiv.org/pdf/2509.15220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15220]] Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model(https://arxiv.org/abs/2509.15220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: this https URL.</li>
</ul>

<h3>Title: Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15224">https://arxiv.org/abs/2509.15224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15224">https://arxiv.org/pdf/2509.15224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15224]] Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation(https://arxiv.org/abs/2509.15224)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.</li>
</ul>

<h3>Title: Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Silvio Mazzucco, Carl Persson, Mattia Segu, Pier Luigi Dovesi, Federico Tombari, Luc Van Gool, Matteo Poggi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15225">https://arxiv.org/abs/2509.15225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15225">https://arxiv.org/pdf/2509.15225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15225]] Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation(https://arxiv.org/abs/2509.15225)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
