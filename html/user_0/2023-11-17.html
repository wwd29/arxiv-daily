<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: MirrorNet: A TEE-Friendly Framework for Secure On-device DNN Inference. (arXiv:2311.09489v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09489">http://arxiv.org/abs/2311.09489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09489]] MirrorNet: A TEE-Friendly Framework for Secure On-device DNN Inference(http://arxiv.org/abs/2311.09489)</code></li>
<li>Summary: <p>Deep neural network (DNN) models have become prevalent in edge devices for
real-time inference. However, they are vulnerable to model extraction attacks
and require protection. Existing defense approaches either fail to fully
safeguard model confidentiality or result in significant latency issues. To
overcome these challenges, this paper presents MirrorNet, which leverages
Trusted Execution Environment (TEE) to enable secure on-device DNN inference.
It generates a TEE-friendly implementation for any given DNN model to protect
the model confidentiality, while meeting the stringent computation and storage
constraints of TEE. The framework consists of two key components: the backbone
model (BackboneNet), which is stored in the normal world but achieves lower
inference accuracy, and the Companion Partial Monitor (CPM), a lightweight
mirrored branch stored in the secure world, preserving model confidentiality.
During inference, the CPM monitors the intermediate results from the
BackboneNet and rectifies the classification output to achieve higher accuracy.
To enhance flexibility, MirrorNet incorporates two modules: the CPM Strategy
Generator, which generates various protection strategies, and the Performance
Emulator, which estimates the performance of each strategy and selects the most
optimal one. Extensive experiments demonstrate the effectiveness of MirrorNet
in providing security guarantees while maintaining low computation latency,
making MirrorNet a practical and promising solution for secure on-device DNN
inference. For the evaluation, MirrorNet can achieve a 18.6% accuracy gap
between authenticated and illegal use, while only introducing 0.99% hardware
overhead.
</p></li>
</ul>

<h3>Title: LightEMU: Hardware Assisted Fuzzing of Trusted Applications. (arXiv:2311.09532v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09532">http://arxiv.org/abs/2311.09532</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09532]] LightEMU: Hardware Assisted Fuzzing of Trusted Applications(http://arxiv.org/abs/2311.09532)</code></li>
<li>Summary: <p>Trusted Execution Environments (TEEs) are deployed in many CPU designs
because of the confidentiality and integrity guarantees they provide. ARM
TrustZone is a TEE extensively deployed on smart phones, IoT devices, and
notebooks. Specifically, TrustZone is used to separate code execution and data
into two worlds, normal world and secure world. However, this separation
inherently prevents traditional fuzzing approaches which rely upon
coverage-guided feedback and existing fuzzing research is, therefore, extremely
limited. In this paper, we present a native and generic method to perform
efficient and scalable feedback-driven fuzzing on Trusted Applications (TAs)
using ARM CoreSight. We propose LightEMU, a novel fuzzing framework that allows
us to fuzz TAs by decoupling them from relied TEE. We argue that LightEMU is a
promising first-stage approach for rapidly discovering TA vulnerabilities prior
to investing effort in whole system TEE evaluation precisely because the
majority of publicly disclosed TrustZone bugs reside in the TA code itself. We
implement LightEMU and adapt it to Teegris, Trusty, OP-TEE and QSEE and
evaluate 8 real-world TAs while triggering 3 unique crashes and achieving x10
time speedup when fuzzing TAs using the state-of-the-art TrustZone fuzzing
framework.
</p></li>
</ul>

<h3>Title: Scalable and Adaptively Secure Any-Trust Distributed Key Generation and All-hands Checkpointing. (arXiv:2311.09592v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09592">http://arxiv.org/abs/2311.09592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09592]] Scalable and Adaptively Secure Any-Trust Distributed Key Generation and All-hands Checkpointing(http://arxiv.org/abs/2311.09592)</code></li>
<li>Summary: <p>The classical distributed key generation protocols (DKG) are resurging due to
their widespread applications in blockchain. While efforts have been made to
improve DKG communication, practical large scale deployments are still yet to
come, due to various challenges including broadcast channel scalability and
worst-case complaint phase. In this paper, we propose a practical DKG for
DL-based cryptosystems, with only (quasi-)linear computation/communication cost
per participant, with the help of a public ledger, and beacon; Notably, our DKG
only incurs constant-size blockchain storage cost for broadcast, even in the
face of worst-case complaints. Moreover, our protocol satisfies adaptive
security. The key to our improvements lies in delegating the most costly
operations to an Any-Trust group. This group is randomly sampled and consists
of a small number of individuals. The population only trusts that at least one
member in the group is honest, without knowing which one. Additionally, we
introduce an extended broadcast channel based on a blockchain and data
dispersal network (such as IPFS), enabling reliable broadcasting of
arbitrary-size messages at the cost of constant-size blockchain storage, which
may be of independent interest.
</p>
<p>Our DKG leads to a fully practical instantiation of Filecoin's checkpointing
mechanism, in which all validators of a Proof-of-Stake (PoS) blockcahin
periodically run DKG and threshold signing to create checkpoints on Bitcoin,
thereby enhancing the security of the PoS chain. In comparison with another
checkpointing approach of Babylon (Oakland, 2023), ours enjoys a significally
smaller monetary cost of Bitcoin transaction fees. For a PoS chain with
$2^{12}$ validators, our cost is merely 0.6\% of that incurred by Babylon's
approach.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: HAL 9000: Skynet's Risk Manager. (arXiv:2311.09449v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09449">http://arxiv.org/abs/2311.09449</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09449]] HAL 9000: Skynet's Risk Manager(http://arxiv.org/abs/2311.09449)</code></li>
<li>Summary: <p>Intrusion Tolerant Systems (ITSs) are a necessary component for
cyber-services/infrastructures. Additionally, as cyberattacks follow a
multi-domain attack surface, a similar defensive approach should be applied,
namely, the use of an evolving multi-disciplinary solution that combines ITS,
cybersecurity and Artificial Intelligence (AI). With the increased popularity
of AI solutions, due to Big Data use-case scenarios and decision support and
automation scenarios, new opportunities to apply Machine Learning (ML)
algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS
can augment its intrusion tolerance capability, by learning from previous
attacks and from known vulnerabilities. As such, this work's contribution is
twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and
incorporates new components to increase its intrusion tolerance capability and
its adaptability to new adversaries; (2) an improved Risk Manager design that
leverages AI to improve ITSs by automatically assessing OS risks to intrusions,
and advise with safer configurations. One of the reasons that intrusions are
successful is due to bad configurations or slow adaptability to new threats.
This can be caused by the dependency that systems have for human intervention.
One of the characteristics in Skynet and HAL 9000 design is the removal of
human intervention. Being fully automatized lowers the chance of successful
intrusions caused by human error. Our experiments using Skynet, shows that HAL
is able to choose 15% safer configurations than the state-of-the-art risk
manager.
</p></li>
</ul>

<h3>Title: Market Research on IIoT Standard Compliance Monitoring Providers and deriving Attributes for IIoT Compliance Monitoring. (arXiv:2311.09991v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09991">http://arxiv.org/abs/2311.09991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09991]] Market Research on IIoT Standard Compliance Monitoring Providers and deriving Attributes for IIoT Compliance Monitoring(http://arxiv.org/abs/2311.09991)</code></li>
<li>Summary: <p>Adapting security architectures to common standards like IEC 62443 or ISO
27000 in the Industrial Internet of Things (IIoT) involves complex processes
and compliance reports. Automatic monitoring of compliance status would enhance
this process. Despite limited research, practical applications exist. This
paper conducts a market study on providers implementing IEC 62443 in IIoT,
aiming to formulate a catalog of monitorable attributes aligned with the
standard. The study reveals challenges, such as a lack of formal separation in
security architectures, limiting visibility. Despite these challenges,
practical implementations share commonalities, providing insights into viable
monitoring properties. The research serves as a crucial entry point into
developing a comprehensive catalog of monitorable attributes for IEC 62443
standards in IIoT.
</p>
<p>Aligned with the IEC 62443 SR catalog of document 3-3, monitorable attributes
are derived based on current research about IIoT security and Expert Knowledge.
The provided tables serve as an exemplary extract, not exhaustive, defining
three types of attributes based on their origin of creation.
</p></li>
</ul>

<h3>Title: Towards more Practical Threat Models in Artificial Intelligence Security. (arXiv:2311.09994v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09994">http://arxiv.org/abs/2311.09994</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09994]] Towards more Practical Threat Models in Artificial Intelligence Security(http://arxiv.org/abs/2311.09994)</code></li>
<li>Summary: <p>Recent works have identified a gap between research and practice in
artificial intelligence security: threats studied in academia do not always
reflect the practical use and security risks of AI. For example, while models
are often studied in isolation, they form part of larger ML pipelines in
practice. Recent works also brought forward that adversarial manipulations
introduced by academic attacks are impractical. We take a first step towards
describing the full extent of this disparity. To this end, we revisit the
threat models of the six most studied attacks in AI security research and match
them to AI usage in practice via a survey with \textbf{271} industrial
practitioners. On the one hand, we find that all existing threat models are
indeed applicable. On the other hand, there are significant mismatches:
research is often too generous with the attacker, assuming access to
information not frequently available in real-world settings. Our paper is thus
a call for action to study more practical threat models in artificial
intelligence security.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy Threats in Stable Diffusion Models. (arXiv:2311.09355v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09355">http://arxiv.org/abs/2311.09355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09355]] Privacy Threats in Stable Diffusion Models(http://arxiv.org/abs/2311.09355)</code></li>
<li>Summary: <p>This paper introduces a novel approach to membership inference attacks (MIA)
targeting stable diffusion computer vision models, specifically focusing on the
highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract
sensitive information about a model's training data, posing significant privacy
concerns. Despite its advancements in image synthesis, our research reveals
privacy vulnerabilities in the stable diffusion models' outputs. Exploiting
this information, we devise a black-box MIA that only needs to query the victim
model repeatedly. Our methodology involves observing the output of a stable
diffusion model at different generative epochs and training a classification
model to distinguish when a series of intermediates originated from a training
sample or not. We propose numerous ways to measure the membership features and
discuss what works best. The attack's efficacy is assessed using the ROC AUC
method, demonstrating a 60\% success rate in inferring membership information.
This paper contributes to the growing body of research on privacy and security
in machine learning, highlighting the need for robust defenses against MIAs.
Our findings prompt a reevaluation of the privacy implications of stable
diffusion models, urging practitioners and developers to implement enhanced
security measures to safeguard against such attacks.
</p></li>
</ul>

<h3>Title: How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities. (arXiv:2311.09447v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09447">http://arxiv.org/abs/2311.09447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09447]] How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities(http://arxiv.org/abs/2311.09447)</code></li>
<li>Summary: <p>The rapid progress in open-source Large Language Models (LLMs) is
significantly driving AI development forward. However, there is still a limited
understanding of their trustworthiness. Deploying these models at scale without
sufficient trustworthiness can pose significant risks, highlighting the need to
uncover these issues promptly. In this work, we conduct an assessment of
open-source LLMs on trustworthiness, scrutinizing them across eight different
aspects including toxicity, stereotypes, ethics, hallucination, fairness,
sycophancy, privacy, and robustness against adversarial demonstrations. We
propose an enhanced Chain of Utterances-based (CoU) prompting strategy by
incorporating meticulously crafted malicious demonstrations for trustworthiness
attack. Our extensive experiments encompass recent and representative series of
open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The
empirical outcomes underscore the efficacy of our attack strategy across
diverse aspects. More interestingly, our result analysis reveals that models
with superior performance in general NLP tasks do not always have greater
trustworthiness; in fact, larger models can be more vulnerable to attacks.
Additionally, models that have undergone instruction tuning, focusing on
instruction following, tend to be more susceptible, although fine-tuning LLMs
for safety alignment proves effective in mitigating adversarial trustworthiness
attacks.
</p></li>
</ul>

<h3>Title: Reducing Privacy Risks in Online Self-Disclosures with Language Models. (arXiv:2311.09538v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09538">http://arxiv.org/abs/2311.09538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09538]] Reducing Privacy Risks in Online Self-Disclosures with Language Models(http://arxiv.org/abs/2311.09538)</code></li>
<li>Summary: <p>Self-disclosure, while being common and rewarding in social media
interaction, also poses privacy risks. In this paper, we take the initiative to
protect the user-side privacy associated with online self-disclosure through
identification and abstraction. We develop a taxonomy of 19 self-disclosure
categories, and curate a large corpus consisting of 4.8K annotated disclosure
spans. We then fine-tune a language model for identification, achieving over
75% in Token F$_1$. We further conduct a HCI user study, with 82\% of
participants viewing the model positively, highlighting its real world
applicability. Motivated by the user feedback, we introduce the task of
self-disclosure abstraction. We experiment with both one-span abstraction and
three-span abstraction settings, and explore multiple fine-tuning strategies.
Our best model can generate diverse abstractions that moderately reduce privacy
risks while maintaining high utility according to human evaluation.
</p></li>
</ul>

<h3>Title: Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09441">http://arxiv.org/abs/2311.09441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09441]] Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning(http://arxiv.org/abs/2311.09441)</code></li>
<li>Summary: <p>Split Federated Learning (SFL) has recently emerged as a promising
distributed learning technology, leveraging the strengths of both federated
learning and split learning. It emphasizes the advantages of rapid convergence
while addressing privacy concerns. As a result, this innovation has received
significant attention from both industry and academia. However, since the model
is split at a specific layer, known as a cut layer, into both client-side and
server-side models for the SFL, the choice of the cut layer in SFL can have a
substantial impact on the energy consumption of clients and their privacy, as
it influences the training burden and the output of the client-side models.
Moreover, the design challenge of determining the cut layer is highly
intricate, primarily due to the inherent heterogeneity in the computing and
networking capabilities of clients. In this article, we provide a comprehensive
overview of the SFL process and conduct a thorough analysis of energy
consumption and privacy. This analysis takes into account the influence of
various system parameters on the cut layer selection strategy. Additionally, we
provide an illustrative example of the cut layer selection, aiming to minimize
the risk of clients from reconstructing the raw data at the server while
sustaining energy consumption within the required energy budget, which involve
trade-offs. Finally, we address open challenges in this field including their
applications to 6G technology. These directions represent promising avenues for
future research and development.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment. (arXiv:2311.09433v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09433">http://arxiv.org/abs/2311.09433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09433]] Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment(http://arxiv.org/abs/2311.09433)</code></li>
<li>Summary: <p>To ensure AI safety, instruction-tuned Large Language Models (LLMs) are
specifically trained to ensure alignment, which refers to making models behave
in accordance with human intentions. While these models have demonstrated
commendable results on various safety benchmarks, the vulnerability of their
safety alignment has not been extensively studied. This is particularly
troubling given the potential harm that LLMs can inflict. Existing attack
methods on LLMs often rely on poisoned training data or the injection of
malicious prompts. These approaches compromise the stealthiness and
generalizability of the attacks, making them susceptible to detection.
Additionally, these models often demand substantial computational resources for
implementation, making them less practical for real-world applications. In this
work, we introduce a novel attack framework, called Backdoor Activation Attack,
which injects trojan steering vectors into the activation layers of LLMs. These
malicious steering vectors can be triggered at inference time to steer the
models toward attacker-desired behaviors by manipulating their activations. In
particular, the steering vectors are generated by taking the difference between
benign and malicious activations. Then, the most effective steering vector is
selected and added to the forward passes of the LLMs. Our experiment results on
four primary alignment tasks show that our proposed method is highly effective
and adds little or no overhead to attack efficiency. Additionally, we discuss
potential countermeasures against such activation attacks. Our code and data
are available at https://email-haoran-for-link. Warning: this paper contains
content that can be offensive or upsetting.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Reading Between the Mud: A Challenging Motorcycle Racer Number Dataset. (arXiv:2311.09256v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09256">http://arxiv.org/abs/2311.09256</a></li>
<li>Code URL: https://github.com/jacobtyo/swintextspotter</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09256]] Reading Between the Mud: A Challenging Motorcycle Racer Number Dataset(http://arxiv.org/abs/2311.09256)</code></li>
<li>Summary: <p>This paper introduces the off-road motorcycle Racer number Dataset (RnD), a
new challenging dataset for optical character recognition (OCR) research. RnD
contains 2,411 images from professional motorsports photographers that depict
motorcycle racers in off-road competitions. The images exhibit a wide variety
of factors that make OCR difficult, including mud occlusions, motion blur,
non-standard fonts, glare, complex backgrounds, etc. The dataset has 5,578
manually annotated bounding boxes around visible motorcycle numbers, along with
transcribed digits and letters. Our experiments benchmark leading OCR
algorithms and reveal an end-to-end F1 score of only 0.527 on RnD, even after
fine-tuning. Analysis of performance on different occlusion types shows mud as
the primary challenge, degrading accuracy substantially compared to normal
conditions. But the models struggle with other factors including glare, blur,
shadows, and dust. Analysis exposes substantial room for improvement and
highlights failure cases of existing models. RnD represents a valuable new
benchmark to drive innovation in real-world OCR capabilities. The authors hope
the community will build upon this dataset and baseline experiments to make
progress on the open problem of robustly recognizing text in unconstrained
natural environments. The dataset is available at
https://github.com/JacobTyo/SwinTextSpotter.
</p></li>
</ul>

<h3>Title: NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios. (arXiv:2311.09269v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09269">http://arxiv.org/abs/2311.09269</a></li>
<li>Code URL: https://github.com/shuttlet/normnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09269]] NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios(http://arxiv.org/abs/2311.09269)</code></li>
<li>Summary: <p>Existing Object Pose Estimation (OPE) methods for stacked scenarios are not
robust to changes in object scale. This paper proposes a new 6DoF OPE network
(NormNet) for different scale objects in stacked scenarios. Specifically, each
object's scale is first learned with point-wise regression. Then, all objects
in the stacked scenario are normalized into the same scale through semantic
segmentation and affine transformation. Finally, they are fed into a shared
pose estimator to recover their 6D poses. In addition, we introduce a new
Sim-to-Real transfer pipeline, combining style transfer and domain
randomization. This improves the NormNet's performance on real data even if we
only train it on synthetic data. Extensive experiments demonstrate that the
proposed method achieves state-of-the-art performance on public benchmarks and
the MultiScale dataset we constructed. The real-world experiments show that our
method can robustly estimate the 6D pose of objects at different scales.
</p></li>
</ul>

<h3>Title: Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset. (arXiv:2311.09276v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09276">http://arxiv.org/abs/2311.09276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09276]] Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset(http://arxiv.org/abs/2311.09276)</code></li>
<li>Summary: <p>Accurate detection of inundated water extents during flooding events is
crucial in emergency response decisions and aids in recovery efforts. Satellite
Remote Sensing data provides a global framework for detecting flooding extents.
Specifically, Sentinel-1 C-Band Synthetic Aperture Radar (SAR) imagery has
proven to be useful in detecting water bodies due to low backscatter of water
features in both co-polarized and cross-polarized SAR imagery. However,
increased backscatter can be observed in certain flooded regions such as
presence of infrastructure and trees - rendering simple methods such as pixel
intensity thresholding and time-series differencing inadequate. Machine
Learning techniques has been leveraged to precisely capture flood extents in
flooded areas with bumps in backscatter but needs high amounts of labelled data
to work desirably. Hence, we created a labeled known water body extent and
flooded area extents during known flooding events covering about 36,000 sq.
kilometers of regions within mainland U.S and Bangladesh. Further, We also
leveraged citizen science by open-sourcing the dataset and hosting an open
competition based on the dataset to rapidly prototype flood extent detection
using community generated models. In this paper we present the information
about the dataset, the data processing pipeline, a baseline model and the
details about the competition, along with discussion on winning approaches. We
believe the dataset adds to already existing datasets based on Sentinel-1C SAR
data and leads to more robust modeling of flood extents. We also hope the
results from the competition pushes the research in flood extent detection
further.
</p></li>
</ul>

<h3>Title: Synthetically Enhanced: Unveiling Synthetic Data's Potential in Medical Imaging Research. (arXiv:2311.09402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09402">http://arxiv.org/abs/2311.09402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09402]] Synthetically Enhanced: Unveiling Synthetic Data's Potential in Medical Imaging Research(http://arxiv.org/abs/2311.09402)</code></li>
<li>Summary: <p>Chest X-rays (CXR) are the most common medical imaging study and are used to
diagnose multiple medical conditions. This study examines the impact of
synthetic data supplementation, using diffusion models, on the performance of
deep learning (DL) classifiers for CXR analysis. We employed three datasets:
CheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising
diffusion probabilistic models (DDPMs) to generate synthetic frontal
radiographs. Our approach ensured that synthetic images mirrored the
demographic and pathological traits of the original data. Evaluating the
classifiers' performance on internal and external datasets revealed that
synthetic data supplementation enhances model accuracy, particularly in
detecting less prevalent pathologies. Furthermore, models trained on synthetic
data alone approached the performance of those trained on real data. This
suggests that synthetic data can potentially compensate for real data shortages
in training robust DL models. However, despite promising outcomes, the
superiority of real data persists.
</p></li>
</ul>

<h3>Title: On the Quantification of Image Reconstruction Uncertainty without Training Data. (arXiv:2311.09639v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09639">http://arxiv.org/abs/2311.09639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09639]] On the Quantification of Image Reconstruction Uncertainty without Training Data(http://arxiv.org/abs/2311.09639)</code></li>
<li>Summary: <p>Computational imaging plays a pivotal role in determining hidden information
from sparse measurements. A robust inverse solver is crucial to fully
characterize the uncertainty induced by these measurements, as it allows for
the estimation of the complete posterior of unrecoverable targets. This, in
turn, facilitates a probabilistic interpretation of observational data for
decision-making. In this study, we propose a deep variational framework that
leverages a deep generative model to learn an approximate posterior
distribution to effectively quantify image reconstruction uncertainty without
the need for training data. We parameterize the target posterior using a
flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve
accurate uncertainty estimation. To bolster stability, we introduce a robust
flow-based model with bi-directional regularization and enhance expressivity
through gradient boosting. Additionally, we incorporate a space-filling design
to achieve substantial variance reduction on both latent prior space and target
posterior space. We validate our method on several benchmark tasks and two
real-world applications, namely fastMRI and black hole image reconstruction.
Our results indicate that our method provides reliable and high-quality image
reconstruction with robust uncertainty estimation.
</p></li>
</ul>

<h3>Title: Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes. (arXiv:2311.09652v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09652">http://arxiv.org/abs/2311.09652</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09652]] Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes(http://arxiv.org/abs/2311.09652)</code></li>
<li>Summary: <p>Event-based structured light systems have recently been introduced as an
exciting alternative to conventional frame-based triangulation systems for the
3D measurements of diffuse surfaces. Important benefits include the fast
capture speed and the high dynamic range provided by the event camera - albeit
at the cost of lower data quality. So far, both low-accuracy event-based as
well as high-accuracy frame-based 3D imaging systems are tailored to a specific
surface type, such as diffuse or specular, and can not be used for a broader
class of object surfaces ("mixed reflectance scenes"). In this paper, we
present a novel event-based structured light system that enables fast 3D
imaging of mixed reflectance scenes with high accuracy. On the captured events,
we use epipolar constraints that intrinsically enable decomposing the measured
reflections into diffuse, two-bounce specular, and other multi-bounce
reflections. The diffuse objects in the scene are reconstructed using
triangulation. Eventually, the reconstructed diffuse scene parts are used as a
"display" to evaluate the specular scene parts via deflectometry. This novel
procedure allows us to use the entire scene as a virtual screen, using only a
scanning laser and an event camera. The resulting system achieves fast and
motion-robust (14Hz) reconstructions of mixed reflectance scenes with &lt; 500
$\mu$m accuracy. Moreover, we introduce a "superfast" capture mode (250Hz) for
the 3D measurement of diffuse scenes.
</p></li>
</ul>

<h3>Title: Robust Contrastive Learning With Theory Guarantee. (arXiv:2311.09671v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09671">http://arxiv.org/abs/2311.09671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09671]] Robust Contrastive Learning With Theory Guarantee(http://arxiv.org/abs/2311.09671)</code></li>
<li>Summary: <p>Contrastive learning (CL) is a self-supervised training paradigm that allows
us to extract meaningful features without any label information. A typical CL
framework is divided into two phases, where it first tries to learn the
features from unlabelled data, and then uses those features to train a linear
classifier with the labeled data. While a fair amount of existing theoretical
works have analyzed how the unsupervised loss in the first phase can support
the supervised loss in the second phase, none has examined the connection
between the unsupervised loss and the robust supervised loss, which can shed
light on how to construct an effective unsupervised loss for the first phase of
CL. To fill this gap, our work develops rigorous theories to dissect and
identify which components in the unsupervised loss can help improve the robust
supervised loss and conduct proper experiments to verify our findings.
</p></li>
</ul>

<h3>Title: Redefining the Laparoscopic Spatial Sense: AI-based Intra- and Postoperative Measurement from Stereoimages. (arXiv:2311.09744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09744">http://arxiv.org/abs/2311.09744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09744]] Redefining the Laparoscopic Spatial Sense: AI-based Intra- and Postoperative Measurement from Stereoimages(http://arxiv.org/abs/2311.09744)</code></li>
<li>Summary: <p>A significant challenge in image-guided surgery is the accurate measurement
task of relevant structures such as vessel segments, resection margins, or
bowel lengths. While this task is an essential component of many surgeries, it
involves substantial human effort and is prone to inaccuracies. In this paper,
we develop a novel human-AI-based method for laparoscopic measurements
utilizing stereo vision that has been guided by practicing surgeons. Based on a
holistic qualitative requirements analysis, this work proposes a comprehensive
measurement method, which comprises state-of-the-art machine learning
architectures, such as RAFT-Stereo and YOLOv8. The developed method is assessed
in various realistic experimental evaluation environments. Our results outline
the potential of our method achieving high accuracies in distance measurements
with errors below 1 mm. Furthermore, on-surface measurements demonstrate
robustness when applied in challenging environments with textureless regions.
Overall, by addressing the inherent challenges of image-guided surgery, we lay
the foundation for a more robust and accurate solution for intra- and
postoperative measurements, enabling more precise, safe, and efficient surgical
procedures.
</p></li>
</ul>

<h3>Title: Rusty Detection Using Image Processing For Maintenance Of Stations. (arXiv:2311.09849v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09849">http://arxiv.org/abs/2311.09849</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09849]] Rusty Detection Using Image Processing For Maintenance Of Stations(http://arxiv.org/abs/2311.09849)</code></li>
<li>Summary: <p>This study addresses the challenge of accurately seg-menting rusted areas on
painted construction surfaces. A method leveraging digital image processing is
explored to calculate the percentage of rust present on painted coatings. The
proposed segmentation approach is based on the HSV color model. To equalize
luminosity and mitigate the influence of illumination, a fundamental model of
single-scale Retinex is applied specifically to the saturation component.
</p>
<p>Subsequently, the image undergoes further processing, involv-ing manual color
filtering. This step is crucial for refining the identification of rusted
regions. To enhance precision and filter out noise, the pixel areas selected
through color filtering are subjected to the DBScan algorithm. This multi-step
process aims to achieve a robust segmentation of rusted areas on painted
construction surfaces, providing a valuable contribution to the field of
corrosion detection and analysis.
</p></li>
</ul>

<h3>Title: Neural machine translation for automated feedback on children's early-stage writing. (arXiv:2311.09389v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09389">http://arxiv.org/abs/2311.09389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09389]] Neural machine translation for automated feedback on children's early-stage writing(http://arxiv.org/abs/2311.09389)</code></li>
<li>Summary: <p>In this work, we address the problem of assessing and constructing feedback
for early-stage writing automatically using machine learning. Early-stage
writing is typically vastly different from conventional writing due to phonetic
spelling and lack of proper grammar, punctuation, spacing etc. Consequently,
early-stage writing is highly non-trivial to analyze using common linguistic
metrics. We propose to use sequence-to-sequence models for "translating"
early-stage writing by students into "conventional" writing, which allows the
translated text to be analyzed using linguistic metrics. Furthermore, we
propose a novel robust likelihood to mitigate the effect of noise in the
dataset. We investigate the proposed methods using a set of numerical
experiments and demonstrate that the conventional text can be predicted with
high accuracy.
</p></li>
</ul>

<h3>Title: To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages. (arXiv:2311.09404v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09404">http://arxiv.org/abs/2311.09404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09404]] To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages(http://arxiv.org/abs/2311.09404)</code></li>
<li>Summary: <p>Perfect machine translation (MT) would render cross-lingual transfer (XLT) by
means of multilingual language models (LMs) superfluous. Given, on one hand,
the large body of work on improving XLT with multilingual LMs and, on the other
hand, recent advances in massively multilingual MT, in this work, we
systematically evaluate existing and propose new translation-based XLT
approaches for transfer to low-resource languages. We show that all
translation-based approaches dramatically outperform zero-shot XLT with
multilingual LMs, rendering the approach that combines the round-trip
translation of the source-language training data with the translation of the
target-language test instances the most effective. We next show that one can
obtain further empirical gains by adding reliable translations to other
high-resource languages to the training data. Moreover, we propose an effective
translation-based XLT strategy even for languages not supported by the MT
system. Finally, we show that model selection for XLT based on target-language
validation data obtained with MT outperforms model selection based on the
source-language data. We hope that our findings encourage adoption of more
robust translation-based baselines in XLT research.
</p></li>
</ul>

<h3>Title: Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs. (arXiv:2311.09469v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09469">http://arxiv.org/abs/2311.09469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09469]] Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs(http://arxiv.org/abs/2311.09469)</code></li>
<li>Summary: <p>Resolving ambiguities through interaction is a hallmark of natural language,
and modeling this behavior is a core challenge in crafting AI assistants. In
this work, we study such behavior in LMs by proposing a task-agnostic framework
for resolving ambiguity by asking users clarifying questions. Our framework
breaks down this objective into three subtasks: (1) determining when
clarification is needed, (2) determining what clarifying question to ask, and
(3) responding accurately with the new information gathered through
clarification. We evaluate systems across three NLP applications: question
answering, machine translation and natural language inference. For the first
subtask, we present a novel uncertainty estimation approach, intent-sim, that
determines the utility of querying for clarification by estimating the entropy
over user intents. Our method consistently outperforms existing uncertainty
estimation approaches at identifying predictions that will benefit from
clarification. When only allowed to ask for clarification on 10% of examples,
our system is able to double the performance gains over randomly selecting
examples to clarify. Furthermore, we find that intent-sim is robust,
demonstrating improvements across a wide range of NLP tasks and LMs. Together,
our work lays foundation for studying clarifying interactions with LMs.
</p></li>
</ul>

<h3>Title: Show Your Work with Confidence: Confidence Bands for Tuning Curves. (arXiv:2311.09480v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09480">http://arxiv.org/abs/2311.09480</a></li>
<li>Code URL: https://github.com/nalourie/opda</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09480]] Show Your Work with Confidence: Confidence Bands for Tuning Curves(http://arxiv.org/abs/2311.09480)</code></li>
<li>Summary: <p>The choice of hyperparameters greatly impacts performance in natural language
processing. Often, it is hard to tell if a method is better than another or
just better tuned. Tuning curves fix this ambiguity by accounting for tuning
effort. Specifically, they plot validation performance as a function of the
number of hyperparameter choices tried so far. While several estimators exist
for these curves, it is common to use point estimates, which we show fail
silently and give contradictory results when given too little data.
</p>
<p>Beyond point estimates, confidence bands are necessary to rigorously
establish the relationship between different approaches. We present the first
method to construct valid confidence bands for tuning curves. The bands are
exact, simultaneous, and distribution-free, thus they provide a robust basis
for comparing methods.
</p>
<p>Empirical analysis shows that while bootstrap confidence bands, which serve
as a baseline, fail to approximate their target confidence, ours achieve it
exactly. We validate our design with ablations, analyze the effect of sample
size, and provide guidance on comparing models with our method. To promote
confident comparisons in future work, we release a library implementing the
method at https://github.com/nalourie/opda .
</p></li>
</ul>

<h3>Title: SegMix: A Simple Structure-Aware Data Augmentation Method. (arXiv:2311.09505v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09505">http://arxiv.org/abs/2311.09505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09505]] SegMix: A Simple Structure-Aware Data Augmentation Method(http://arxiv.org/abs/2311.09505)</code></li>
<li>Summary: <p>Interpolation-based Data Augmentation (DA) methods (Mixup) linearly
interpolate the inputs and labels of two or more training examples. Mixup has
more recently been adapted to the field of Natural Language Processing (NLP),
mainly for sequence labeling tasks. However, such a simple adoption yields
mixed or unstable improvements over the baseline models. We argue that the
direct-adoption methods do not account for structures in NLP tasks. To this
end, we propose SegMix, a collection of interpolation-based DA algorithms that
can adapt to task-specific structures. SegMix poses fewer constraints on data
structures, is robust to various hyperparameter settings, applies to more task
settings, and adds little computational overhead. In the algorithm's core, we
apply interpolation methods on task-specific meaningful segments, in contrast
to applying them on sequences as in prior work. We find SegMix to be a flexible
framework that combines rule-based DA methods with interpolation-based methods,
creating interesting mixtures of DA techniques. We show that SegMix
consistently improves performance over strong baseline models in Named Entity
Recognition (NER) and Relation Extraction (RE) tasks, especially under
data-scarce settings. Furthermore, this method is easy to implement and adds
negligible training overhead.
</p></li>
</ul>

<h3>Title: SCORE: A framework for Self-Contradictory Reasoning Evaluation. (arXiv:2311.09603v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09603">http://arxiv.org/abs/2311.09603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09603]] SCORE: A framework for Self-Contradictory Reasoning Evaluation(http://arxiv.org/abs/2311.09603)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated impressive reasoning ability
in various language-based tasks. Despite many proposed reasoning methods aimed
at enhancing performance in downstream tasks, two fundamental questions
persist: Does reasoning genuinely support predictions, and how reliable is the
quality of reasoning? In this paper, we propose a framework \textsc{SCORE} to
analyze how well LLMs can reason. Specifically, we focus on self-contradictory
reasoning, where reasoning does not support the prediction. We find that LLMs
often contradict themselves when performing reasoning tasks that involve
contextual information and commonsense. The model may miss evidence or use
shortcuts, thereby exhibiting self-contradictory behaviors. We also employ the
Point-of-View (POV) method, which probes models to generate reasoning from
multiple perspectives, as a diagnostic tool for further analysis. We find that
though LLMs may appear to perform well in one-perspective settings, they fail
to stabilize such behavior in multi-perspectives settings. Even for correct
predictions, the reasoning may be messy and incomplete, and LLMs can easily be
led astray from good reasoning. \textsc{SCORE}'s results underscore the lack of
robustness required for trustworthy reasoning and the urgency for further
research to establish best practices for a comprehensive evaluation of
reasoning beyond accuracy-based metrics.
</p></li>
</ul>

<h3>Title: Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning. (arXiv:2311.09619v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09619">http://arxiv.org/abs/2311.09619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09619]] Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning(http://arxiv.org/abs/2311.09619)</code></li>
<li>Summary: <p>In-Context Learning (ICL) is an emergent capability of Large Language Models
(LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new
tasks. Previous studies have shown that using LLMs' outputs as labels is
effective in training models to select demonstrations. Such a label is expected
to estimate utility of a demonstration in ICL; however, it has not been well
understood how different labeling strategies affect results on target tasks.
This paper presents an analysis on different utility functions by focusing on
LLMs' output probability given ground-truth output, and task-specific reward
given LLMs' prediction. Unlike the previous work, we introduce a novel labeling
method, incremental utility, which estimates how much incremental knowledge is
brought into the LLMs by a demonstration. We conduct experiments with
instruction-tuned LLMs on binary/multi-class classification, segmentation, and
translation across Arabic, English, Finnish, Japanese, and Spanish. Our results
show that (1) the probability is effective when the probability values are
distributed across the whole value range (on the classification tasks), and (2)
the downstream metric is more robust when nuanced reward values are provided
with long outputs (on the segmentation and translation tasks). We then show
that the proposed incremental utility further helps ICL by contrasting how the
LLMs perform with and without the demonstrations.
</p></li>
</ul>

<h3>Title: Online Continual Knowledge Learning for Language Models. (arXiv:2311.09632v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09632">http://arxiv.org/abs/2311.09632</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09632]] Online Continual Knowledge Learning for Language Models(http://arxiv.org/abs/2311.09632)</code></li>
<li>Summary: <p>Large Language Models (LLMs) serve as repositories of extensive world
knowledge, enabling them to perform tasks such as question-answering and
fact-checking. However, this knowledge can become obsolete as global contexts
change. In this paper, we introduce a novel problem in the realm of continual
learning: Online Continual Knowledge Learning (OCKL). This problem formulation
aims to manage the dynamic nature of world knowledge in LMs under real-time
constraints. We propose a new benchmark and evaluation metric designed to
measure both the rate of new knowledge acquisition and the retention of
previously learned knowledge. Our empirical evaluation, conducted using a
variety of state-of-the-art methods, establishes robust base-lines for OCKL.
Our results reveal that existing continual learning approaches are
unfortunately insufficient for tackling the unique challenges posed by OCKL. We
identify key factors that influence the trade-off between knowledge acquisition
and retention, thereby advancing our understanding of how to train LMs in a
continually evolving environment.
</p></li>
</ul>

<h3>Title: Evolving Domain Adaptation of Pretrained Language Models for Text Classification. (arXiv:2311.09661v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09661">http://arxiv.org/abs/2311.09661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09661]] Evolving Domain Adaptation of Pretrained Language Models for Text Classification(http://arxiv.org/abs/2311.09661)</code></li>
<li>Summary: <p>Adapting pre-trained language models (PLMs) for time-series text
classification amidst evolving domain shifts (EDS) is critical for maintaining
accuracy in applications like stance detection. This study benchmarks the
effectiveness of evolving domain adaptation (EDA) strategies, notably
self-training, domain-adversarial training, and domain-adaptive pretraining,
with a focus on an incremental self-training method. Our analysis across
various datasets reveals that this incremental method excels at adapting PLMs
to EDS, outperforming traditional domain adaptation techniques. These findings
highlight the importance of continually updating PLMs to ensure their
effectiveness in real-world applications, paving the way for future research
into PLM robustness against the natural temporal evolution of language.
</p></li>
</ul>

<h3>Title: Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness. (arXiv:2311.09694v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09694">http://arxiv.org/abs/2311.09694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09694]] Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness(http://arxiv.org/abs/2311.09694)</code></li>
<li>Summary: <p>Are the longstanding robustness issues in NLP resolved by today's larger and
more performant models? To address this question, we conduct a thorough
investigation using 19 models of different sizes spanning different
architectural choices and pretraining objectives. We conduct evaluations using
(a) OOD and challenge test sets, (b) CheckLists, (c) contrast sets, and (d)
adversarial inputs. Our analysis reveals that not all OOD tests provide further
insight into robustness. Evaluating with CheckLists and contrast sets shows
significant gaps in model performance; merely scaling models does not make them
sufficiently robust. Finally, we point out that current approaches for
adversarial evaluations of models are themselves problematic: they can be
easily thwarted, and in their current forms, do not represent a sufficiently
deep probe of model robustness. We conclude that not only is the question of
robustness in NLP as yet unresolved, but even some of the approaches to measure
robustness need to be reassessed.
</p></li>
</ul>

<h3>Title: Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning. (arXiv:2311.09821v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09821">http://arxiv.org/abs/2311.09821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09821]] Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning(http://arxiv.org/abs/2311.09821)</code></li>
<li>Summary: <p>Knowledge in the real world is being updated constantly. However, it is
costly to frequently update large language models (LLMs). Therefore, it is
crucial for LLMs to understand the concept of temporal knowledge. However,
prior works on temporal question answering did not emphasize multi-answer and
multi-hop types of temporal reasoning. In this paper, we propose a complex
temporal question-answering (QA) dataset Complex-TR that focuses on
multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel
data augmentation strategy to improve the complex temporal reasoning capability
and robustness of LLMs. We conducted experiments on multiple temporal QA
datasets. Experimental results show that our method is able to improve LLMs'
performance on temporal QA benchmarks by significant margins.
</p></li>
</ul>

<h3>Title: Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting. (arXiv:2311.09790v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09790">http://arxiv.org/abs/2311.09790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09790]] Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting(http://arxiv.org/abs/2311.09790)</code></li>
<li>Summary: <p>Balancing the trade-off between accuracy and robustness is a long-standing
challenge in time series forecasting. While most of existing robust algorithms
have achieved certain suboptimal performance on clean data, sustaining the same
performance level in the presence of data perturbations remains extremely hard.
% In this paper, we study a wide array of perturbation scenarios and propose
novel defense mechanisms against adversarial attacks using real-world telecom
data. We compare our strategy against two existing adversarial training
algorithms under a range of maximal allowed perturbations, defined using
$\ell_{\infty}$-norm, $\in [0.1,0.4]$. % Our findings reveal that our hybrid
strategy, which is composed of a classifier to detect adversarial examples, a
denoiser to eliminate noise from the perturbed data samples, and a standard
forecaster, achieves the best performance on both clean and perturbed data. %
Our optimal model can retain up to $92.02\%$ the performance of the original
forecasting model in terms of Mean Squared Error (MSE) on clean data, while
being more robust than the standard adversarially trained models on perturbed
data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing
methods on normal and perturbed data, respectively. In addition, the components
of our models can be trained in parallel, resulting in better computational
efficiency. % Our results indicate that we can optimally balance the trade-off
between the performance and robustness of forecasting models by improving the
classifier and denoiser, even in the presence of sophisticated and destructive
poisoning attacks.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction. (arXiv:2311.09366v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09366">http://arxiv.org/abs/2311.09366</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09366]] LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction(http://arxiv.org/abs/2311.09366)</code></li>
<li>Summary: <p>While the potential of Open Information Extraction (Open IE) for Knowledge
Graph Construction (KGC) may seem promising, we find that the alignment of Open
IE extraction results with existing knowledge graphs to be inadequate. The
advent of Large Language Models (LLMs), especially the commercially available
OpenAI models, have reset expectations for what is possible with deep learning
models and have created a new field called prompt engineering. We investigate
the use of GPT models and prompt engineering for knowledge graph construction
with the Wikidata knowledge graph to address a similar problem to Open IE,
which we call Open Knowledge Extraction (OKE) using an approach we call the
Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the
entity linking task essential to construction of real world knowledge graphs.
We merge the CaRB benchmark scoring approach with data from the TekGen dataset
for the LOKE task. We then show that a well engineered prompt, paired with a
naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's
OpenIE 4 implementation on the OKE task, although it over-generates triples
compared to the reference set due to overall triple scarcity in the TekGen set.
Through an analysis of entity linkability in the CaRB dataset, as well as
outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver"
TekGen triples show that the task is significantly different in content from
OIE, if not structure. Through this analysis and a qualitative analysis of
sentence extractions via all methods, we found that LOKE-GPT extractions are of
high utility for the KGC task and suitable for use in semi-automated extraction
settings.
</p></li>
</ul>

<h3>Title: A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms. (arXiv:2311.09367v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09367">http://arxiv.org/abs/2311.09367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09367]] A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms(http://arxiv.org/abs/2311.09367)</code></li>
<li>Summary: <p>The rise of social media platforms has led to an increase in cyber-aggressive
behavior, encompassing a broad spectrum of hostile behavior, including
cyberbullying, online harassment, and the dissemination of offensive and hate
speech. These behaviors have been associated with significant societal
consequences, ranging from online anonymity to real-world outcomes such as
depression, suicidal tendencies, and, in some instances, offline violence.
Recognizing the societal risks associated with unchecked aggressive content,
this paper delves into the field of Aggression Content Detection and Behavioral
Analysis of Aggressive Users, aiming to bridge the gap between disparate
studies. In this paper, we analyzed the diversity of definitions and proposed a
unified cyber-aggression definition. We examine the comprehensive process of
Aggression Content Detection, spanning from dataset creation, feature selection
and extraction, and detection algorithm development. Further, we review studies
on Behavioral Analysis of Aggression that explore the influencing factors,
consequences, and patterns associated with cyber-aggressive behavior. This
systematic literature review is a cross-examination of content detection and
behavioral analysis in the realm of cyber-aggression. The integrated
investigation reveals the effectiveness of incorporating sociological insights
into computational techniques for preventing cyber-aggressive behavior.
Finally, the paper concludes by identifying research gaps and encouraging
further progress in the unified domain of socio-computational aggressive
behavior analysis.
</p></li>
</ul>

<h3>Title: SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU. (arXiv:2311.09502v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09502">http://arxiv.org/abs/2311.09502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09502]] SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU(http://arxiv.org/abs/2311.09502)</code></li>
<li>Summary: <p>Task-oriented dialogue (ToD) systems help users execute well-defined tasks
across a variety of domains (e.g., $\textit{flight booking}$ or $\textit{food
ordering}$), with their Natural Language Understanding (NLU) components being
dedicated to the analysis of user utterances, predicting users' intents
($\textit{Intent Detection}$, ID) and extracting values for informational slots
($\textit{Value Extraction}$, VE). In most domains, labelled NLU data is
scarce, making sample-efficient learning -- enabled with effective transfer
paradigms -- paramount. In this work, we introduce SQATIN, a new framework for
dialog NLU based on (i) instruction tuning and (ii) question-answering-based
formulation of ID and VE tasks. According to the evaluation on established NLU
benchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially
surpassing the performance of current models based on standard fine-tuning
objectives in both in-domain training and cross-domain transfer. SQATIN yields
particularly large performance gains in cross-domain transfer, owing to the
fact that our QA-based instruction tuning leverages similarities between
natural language descriptions of classes (i.e., slots and intents) across
domains.
</p></li>
</ul>

<h3>Title: A Reevaluation of Event Extraction: Past, Present, and Future Challenges. (arXiv:2311.09562v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09562">http://arxiv.org/abs/2311.09562</a></li>
<li>Code URL: https://github.com/ej0cl6/textee</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09562]] A Reevaluation of Event Extraction: Past, Present, and Future Challenges(http://arxiv.org/abs/2311.09562)</code></li>
<li>Summary: <p>Event extraction has attracted much attention in recent years due to its
potential for many applications. However, recent studies observe some
evaluation challenges, suggesting that reported scores might not reflect the
true performance. In this work, we first identify and discuss these evaluation
challenges, including the unfair comparisons resulting from different
assumptions about data or different data preprocessing steps, the
incompleteness of the current evaluation framework leading to potential dataset
bias or data split bias, and low reproducibility of prior studies. To address
these challenges, we propose TextEE, a standardized, fair, and reproducible
benchmark for event extraction. TextEE contains standardized data preprocessing
scripts and splits for more than ten datasets across different domains. In
addition, we aggregate and re-implement over ten event extraction approaches
published in recent years and conduct a comprehensive reevaluation. Finally, we
explore the capability of large language models in event extraction and discuss
some future challenges. We expect TextEE will serve as a reliable benchmark for
event extraction, facilitating future research in the field.
</p></li>
</ul>

<h3>Title: A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection. (arXiv:2311.09708v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09708">http://arxiv.org/abs/2311.09708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09708]] A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection(http://arxiv.org/abs/2311.09708)</code></li>
<li>Summary: <p>Our work addresses the problem of unsupervised Aspect Category Detection
using a small set of seed words. Recent works have focused on learning
embedding spaces for seed words and sentences to establish similarities between
sentences and aspects. However, aspect representations are limited by the
quality of initial seed words, and model performances are compromised by noise.
To mitigate this limitation, we propose a simple framework that automatically
enhances the quality of initial seed words and selects high-quality sentences
for training instead of using the entire dataset. Our main concepts are to add
a number of seed words to the initial set and to treat the task of noise
resolution as a task of augmenting data for a low-resource task. In addition,
we jointly train Aspect Category Detection with Aspect Term Extraction and
Aspect Term Polarity to further enhance performance. This approach facilitates
shared representation learning, allowing Aspect Category Detection to benefit
from the additional guidance offered by other tasks. Extensive experiments
demonstrate that our framework surpasses strong baselines on standard datasets.
</p></li>
</ul>

<h3>Title: MOKA: Moral Knowledge Augmentation for Moral Event Extraction. (arXiv:2311.09733v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09733">http://arxiv.org/abs/2311.09733</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09733]] MOKA: Moral Knowledge Augmentation for Moral Event Extraction(http://arxiv.org/abs/2311.09733)</code></li>
<li>Summary: <p>News media employ moral language to create memorable stories, and readers
often engage with the content that align with their values. Moral theories have
been applied to news analysis studying moral values in isolation, while the
intricate dynamics among participating entities in shaping moral events have
been overlooked. This is mainly due to the use of obscure language to conceal
evident ideology and values, coupled with the insufficient moral reasoning
capability in most existing NLP systems, where LLMs are no exception. To study
this phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of
5,494 structured annotations on 474 news articles by diverse US media across
the political spectrum. We further propose MOKA, a moral event extraction
framework with MOral Knowledge Augmentation, that leverages knowledge derived
from moral words and moral scenarios. Experimental results show that MOKA
outperforms competitive baselines across three moral event understanding tasks.
Further analyses illuminate the selective reporting of moral events by media
outlets of different ideological leanings, suggesting the significance of
event-level morality analysis in news. Our datasets and codebase are available
at https://github.com/launchnlp/MOKA.
</p></li>
</ul>

<h3>Title: CARE: Extracting Experimental Findings From Clinical Literature. (arXiv:2311.09736v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09736">http://arxiv.org/abs/2311.09736</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09736]] CARE: Extracting Experimental Findings From Clinical Literature(http://arxiv.org/abs/2311.09736)</code></li>
<li>Summary: <p>Extracting fine-grained experimental findings from literature can provide
massive utility for scientific applications. Prior work has focused on
developing annotation schemas and datasets for limited aspects of this problem,
leading to simpler information extraction datasets which do not capture the
real-world complexity and nuance required for this task. Focusing on
biomedicine, this work presents CARE (Clinical Aggregation-oriented Result
Extraction) -- a new IE dataset for the task of extracting clinical findings.
We develop a new annotation schema capturing fine-grained findings as n-ary
relations between entities and attributes, which includes phenomena challenging
for current IE systems such as discontinuous entity spans, nested relations,
and variable arity n-ary relations. Using this schema, we collect extensive
annotations for 700 abstracts from two sources: clinical trials and case
reports. We also benchmark the performance of various state-of-the-art IE
systems on our dataset, including extractive models and generative LLMs in
fully supervised and limited data settings. Our results demonstrate the
difficulty of our dataset -- even SOTA models such as GPT4 struggle,
particularly on relation extraction. We release our annotation schema and CARE
to encourage further research on extracting and aggregating scientific findings
from literature.
</p></li>
</ul>

<h3>Title: OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking. (arXiv:2311.09758v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09758">http://arxiv.org/abs/2311.09758</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09758]] OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking(http://arxiv.org/abs/2311.09758)</code></li>
<li>Summary: <p>Large language models (LLMs) have revolutionized the landscape of Natural
Language Processing systems, but are computationally expensive. To reduce the
cost without sacrificing performance, previous studies have explored various
approaches to harness the potential of Small Language Models (SLMs) as
cost-effective alternatives to their larger counterparts. Driven by findings
that SLMs and LLMs exhibit complementary strengths in a structured knowledge
extraction task, this work presents a novel SLM/LLM routing framework designed
to improve computational efficiency and enhance task performance. First,
exemplar pools are created to represent the types of contexts where each LM
provides a more reliable answer, leveraging a sentence embedding fine-tuned so
that context similarity is close to dialogue state similarity. Then, during
inference, the k-nearest exemplars to the testing instance are retrieved, and
the instance is routed according to majority vote. In dialogue state tracking
tasks, the proposed routing framework enhances performance substantially
compared to relying solely on LLMs, while reducing the computational costs by
over 50%.
</p></li>
</ul>

<h3>Title: GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets. (arXiv:2311.09860v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09860">http://arxiv.org/abs/2311.09860</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09860]] GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets(http://arxiv.org/abs/2311.09860)</code></li>
<li>Summary: <p>Named Entity Recognition (NER) models play a crucial role in various NLP
tasks, including information extraction (IE) and text understanding. In
academic writing, references to machine learning models and datasets are
fundamental components of various computer science publications and necessitate
accurate models for identification. Despite the advancements in NER, existing
ground truth datasets do not treat fine-grained types like ML model and model
architecture as separate entity types, and consequently, baseline models cannot
recognize them as such. In this paper, we release a corpus of 100 manually
annotated full-text scientific publications and a first baseline model for 10
entity types centered around ML models and datasets. In order to provide a
nuanced understanding of how ML models and datasets are mentioned and utilized,
our dataset also contains annotations for informal mentions like "our
BERT-based model" or "an image CNN". You can find the ground truth dataset and
code to replicate model training at https://data.gesis.org/gsap/gsap-ner.
</p></li>
</ul>

<h3>Title: Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction. (arXiv:2311.09386v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09386">http://arxiv.org/abs/2311.09386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09386]] Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction(http://arxiv.org/abs/2311.09386)</code></li>
<li>Summary: <p>Linear feature extraction at the presence of nonlinear dependencies among the
data is a fundamental challenge in unsupervised learning. We propose using a
Probabilistic Gram-Schmidt (PGS) type orthogonalization process in order to
detect and map out redundant dimensions. Specifically, by applying the PGS
process over any family of functions which presumably captures the nonlinear
dependencies in the data, we construct a series of covariance matrices that can
either be used to remove those dependencies from the principal components, or
to identify new large-variance directions. In the former case, we prove that
under certain assumptions the resulting algorithms detect and remove nonlinear
dependencies whenever those dependencies lie in the linear span of the chosen
function family. In the latter, we provide information-theoretic guarantees in
terms of entropy reduction. Both proposed methods extract linear features from
the data while removing nonlinear redundancies. We provide simulation results
on synthetic and real-world datasets which show improved performance over PCA
and state-of-the-art linear feature extraction algorithms, both in terms of
variance maximization of the extracted features, and in terms of improved
performance of classification algorithms.
</p></li>
</ul>

<h3>Title: Natural Disaster Analysis using Satellite Imagery and Social-Media Data for Emergency Response Situations. (arXiv:2311.09947v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09947">http://arxiv.org/abs/2311.09947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09947]] Natural Disaster Analysis using Satellite Imagery and Social-Media Data for Emergency Response Situations(http://arxiv.org/abs/2311.09947)</code></li>
<li>Summary: <p>Disaster Management is one of the most promising research areas because of
its significant economic, environmental and social repercussions. This research
focuses on analyzing different types of data (pre and post satellite images and
twitter data) related to disaster management for in-depth analysis of
location-wise emergency requirements. This research has been divided into two
stages, namely, satellite image analysis and twitter data analysis followed by
integration using location. The first stage involves pre and post disaster
satellite image analysis of the location using multi-class land cover
segmentation technique based on U-Net architecture. The second stage focuses on
mapping the region with essential information about the disaster situation and
immediate requirements for relief operations. The severely affected regions are
demarcated and twitter data is extracted using keywords respective to that
location. The extraction of situational information from a large corpus of raw
tweets adopts Content Word based Tweet Summarization (COWTS) technique. An
integration of these modules using real-time location-based mapping and
frequency analysis technique gathers multi-dimensional information in the
advent of disaster occurrence such as the Kerala and Mississippi floods that
were analyzed and validated as test cases. The novelty of this research lies in
the application of segmented satellite images for disaster relief using
highlighted land cover changes and integration of twitter data by mapping these
region-specific filters for obtaining a complete overview of the disaster.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedFusion: Manifold Driven Federated Learning for Multi-satellite and Multi-modality Fusion. (arXiv:2311.09540v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09540">http://arxiv.org/abs/2311.09540</a></li>
<li>Code URL: https://github.com/ldxdu/fedfusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09540]] FedFusion: Manifold Driven Federated Learning for Multi-satellite and Multi-modality Fusion(http://arxiv.org/abs/2311.09540)</code></li>
<li>Summary: <p>Multi-satellite, multi-modality in-orbit fusion is a challenging task as it
explores the fusion representation of complex high-dimensional data under
limited computational resources. Deep neural networks can reveal the underlying
distribution of multi-modal remote sensing data, but the in-orbit fusion of
multimodal data is more difficult because of the limitations of different
sensor imaging characteristics, especially when the multimodal data follows
non-independent identically distribution (Non-IID) distributions. To address
this problem while maintaining classification performance, this paper proposes
a manifold-driven multi-modality fusion framework, FedFusion, which randomly
samples local data on each client to jointly estimate the prominent manifold
structure of shallow features of each client and explicitly compresses the
feature matrices into a low-rank subspace through cascading and additive
approaches, which is used as the feature input of the subsequent classifier.
Considering the physical space limitations of the satellite constellation, we
developed a multimodal federated learning module designed specifically for
manifold data in a deep latent space. This module achieves iterative updating
of the sub-network parameters of each client through global weighted averaging,
constructing a framework that can represent compact representations of each
client. The proposed framework surpasses existing methods in terms of
performance on three multimodal datasets, achieving a classification average
accuracy of 94.35$\%$ while compressing communication costs by a factor of 4.
Furthermore, extensive numerical evaluations of real-world satellite images
were conducted on the orbiting edge computing architecture based on Jetson TX2
industrial modules, which demonstrated that FedFusion significantly reduced
training time by 48.4 minutes (15.18%) while optimizing accuracy.}
</p></li>
</ul>

<h3>Title: UFPS: A unified framework for partially-annotated federated segmentation in heterogeneous data distribution. (arXiv:2311.09757v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09757">http://arxiv.org/abs/2311.09757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09757]] UFPS: A unified framework for partially-annotated federated segmentation in heterogeneous data distribution(http://arxiv.org/abs/2311.09757)</code></li>
<li>Summary: <p>Partially supervised segmentation is a label-saving method based on datasets
with fractional classes labeled and intersectant. However, it is still far from
landing on real-world medical applications due to privacy concerns and data
heterogeneity. As a remedy without privacy leakage, federated partially
supervised segmentation (FPSS) is formulated in this work. The main challenges
for FPSS are class heterogeneity and client drift. We propose a Unified
Federated Partially-labeled Segmentation (UFPS) framework to segment pixels
within all classes for partially-annotated datasets by training a totipotential
global model without class collision. Our framework includes Unified Label
Learning and sparsed Unified Sharpness Aware Minimization for unification of
class and feature space, respectively. We find that vanilla combinations for
traditional methods in partially supervised segmentation and federated learning
are mainly hampered by class collision through empirical study. Our
comprehensive experiments on real medical datasets demonstrate better
deconflicting and generalization ability of UFPS compared with modified
methods.
</p></li>
</ul>

<h3>Title: FedCode: Communication-Efficient Federated Learning via Transferring Codebooks. (arXiv:2311.09270v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09270">http://arxiv.org/abs/2311.09270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09270]] FedCode: Communication-Efficient Federated Learning via Transferring Codebooks(http://arxiv.org/abs/2311.09270)</code></li>
<li>Summary: <p>Federated Learning (FL) is a distributed machine learning paradigm that
enables learning models from decentralized local data. While FL offers
appealing properties for clients' data privacy, it imposes high communication
burdens for exchanging model weights between a server and the clients. Existing
approaches rely on model compression techniques, such as pruning and weight
clustering to tackle this. However, transmitting the entire set of weight
updates at each federated round, even in a compressed format, limits the
potential for a substantial reduction in communication volume. We propose
FedCode where clients transmit only codebooks, i.e., the cluster centers of
updated model weight values. To ensure a smooth learning curve and proper
calibration of clusters between the server and the clients, FedCode
periodically transfers model weights after multiple rounds of solely
communicating codebooks. This results in a significant reduction in
communication volume between clients and the server in both directions, without
imposing significant computational overhead on the clients or leading to major
performance degradation of the models. We evaluate the effectiveness of FedCode
using various publicly available datasets with ResNet-20 and MobileNet backbone
model architectures. Our evaluations demonstrate a 12.2-fold data transmission
reduction on average while maintaining a comparable model performance with an
average accuracy loss of 1.3% compared to FedAvg. Further validation of FedCode
performance under non-IID data distributions showcased an average accuracy loss
of 2.0% compared to FedAvg while achieving approximately a 12.7-fold data
transmission reduction.
</p></li>
</ul>

<h3>Title: Contribution Evaluation in Federated Learning: Examining Current Approaches. (arXiv:2311.09856v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09856">http://arxiv.org/abs/2311.09856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09856]] Contribution Evaluation in Federated Learning: Examining Current Approaches(http://arxiv.org/abs/2311.09856)</code></li>
<li>Summary: <p>Federated Learning (FL) has seen increasing interest in cases where entities
want to collaboratively train models while maintaining privacy and governance
over their data. In FL, clients with private and potentially heterogeneous data
and compute resources come together to train a common model without raw data
ever leaving their locale. Instead, the participants contribute by sharing
local model updates, which, naturally, differ in quality. Quantitatively
evaluating the worth of these contributions is termed the Contribution
Evaluation (CE) problem. We review current CE approaches from the underlying
mathematical framework to efficiently calculate a fair value for each client.
Furthermore, we benchmark some of the most promising state-of-the-art
approaches, along with a new one we introduce, on MNIST and CIFAR-10, to
showcase their differences. Designing a fair and efficient CE method, while a
small part of the overall FL system design, is tantamount to the mainstream
adoption of FL.
</p></li>
</ul>

<h3>Title: Straggler-resilient Federated Learning: Tackling Computation Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network. (arXiv:2311.10002v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10002">http://arxiv.org/abs/2311.10002</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10002]] Straggler-resilient Federated Learning: Tackling Computation Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network(http://arxiv.org/abs/2311.10002)</code></li>
<li>Summary: <p>Federated Learning (FL) enables many resource-limited devices to train a
model collaboratively without data sharing. However, many existing works focus
on model-homogeneous FL, where the global and local models are the same size,
ignoring the inherently heterogeneous computational capabilities of different
devices and restricting resource-constrained devices from contributing to FL.
In this paper, we consider model-heterogeneous FL and propose Federated Partial
Model Training (FedPMT), where devices with smaller computational capabilities
work on partial models (subsets of the global model) and contribute to the
global model. Different from Dropout-based partial model generation, which
removes neurons in hidden layers at random, model training in FedPMT is
achieved from the back-propagation perspective. As such, all devices in FedPMT
prioritize the most crucial parts of the global model. Theoretical analysis
shows that the proposed partial model training design has a similar convergence
rate to the widely adopted Federated Averaging (FedAvg) algorithm,
$\mathcal{O}(1/T)$, with the sub-optimality gap enlarged by a constant factor
related to the model splitting design in FedPMT. Empirical results show that
FedPMT significantly outperforms the existing benchmark FedDrop. Meanwhile,
compared to the popular model-homogeneous benchmark, FedAvg, FedPMT reaches the
learning target in a shorter completion time, thus achieving a better trade-off
between learning accuracy and completion time.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models. (arXiv:2311.09428v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09428">http://arxiv.org/abs/2311.09428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09428]] Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models(http://arxiv.org/abs/2311.09428)</code></li>
<li>Summary: <p>This work investigates the potential of undermining both fairness and
detection performance in abusive language detection. In a dynamic and complex
digital world, it is crucial to investigate the vulnerabilities of these
detection models to adversarial fairness attacks to improve their fairness
robustness. We propose a simple yet effective framework FABLE that leverages
backdoor attacks as they allow targeted control over the fairness and detection
performance. FABLE explores three types of trigger designs (i.e., rare,
artificial, and natural triggers) and novel sampling strategies. Specifically,
the adversary can inject triggers into samples in the minority group with the
favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored
outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the
effectiveness of FABLE attacking fairness and utility in abusive language
detection.
</p></li>
</ul>

<h3>Title: FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children's Storybook Narratives. (arXiv:2311.09756v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09756">http://arxiv.org/abs/2311.09756</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09756]] FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children's Storybook Narratives(http://arxiv.org/abs/2311.09756)</code></li>
<li>Summary: <p>AI models (including LLM) often rely on narrative question-answering (QA)
datasets to provide customized QA functionalities to support downstream
children education applications; however, existing datasets only include QA
pairs that are grounded within the given storybook content, but children can
learn more when teachers refer the storybook content to real-world knowledge
(e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which is
annotated by children education experts, to supplement 278 storybook narratives
with educationally appropriate commonsense knowledge. The dataset has 5,868 QA
pairs that not only originate from the storybook narrative but also contain the
commonsense knowledge grounded by an external knowledge graph (i.e.,
ConceptNet). A follow-up experiment shows that a smaller model (T5-large)
fine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineered
LLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This result
suggests that: 1) our dataset brings novel challenges to existing LLMs, and 2)
human experts' data annotation are still critical as they have much nuanced
knowledge that LLMs do not know in the children educational domain.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09680">http://arxiv.org/abs/2311.09680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09680]] Trustworthy Large Models in Vision: A Survey(http://arxiv.org/abs/2311.09680)</code></li>
<li>Summary: <p>The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
in reliable methods. Despite the abundance of literature on trustworthy LMs in
language, a systematic survey specifically delving into the trustworthiness of
LMs in vision remains absent. In order to mitigate this gap, we summarize four
relevant concerns that obstruct the trustworthy usage in vision of LMs in this
survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers'
understanding of the field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
</p></li>
</ul>

<h3>Title: A Knowledge Distillation Approach for Sepsis Outcome Prediction from Multivariate Clinical Time Series. (arXiv:2311.09566v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09566">http://arxiv.org/abs/2311.09566</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09566]] A Knowledge Distillation Approach for Sepsis Outcome Prediction from Multivariate Clinical Time Series(http://arxiv.org/abs/2311.09566)</code></li>
<li>Summary: <p>Sepsis is a life-threatening condition triggered by an extreme infection
response. Our objective is to forecast sepsis patient outcomes using their
medical history and treatments, while learning interpretable state
representations to assess patients' risks in developing various adverse
outcomes. While neural networks excel in outcome prediction, their limited
interpretability remains a key issue. In this work, we use knowledge
distillation via constrained variational inference to distill the knowledge of
a powerful "teacher" neural network model with high predictive power to train a
"student" latent variable model to learn interpretable hidden state
representations to achieve high predictive performance for sepsis outcome
prediction. Using real-world data from the MIMIC-IV database, we trained an
LSTM as the "teacher" model to predict mortality for sepsis patients, given
information about their recent history of vital signs, lab values and
treatments. For our student model, we use an autoregressive hidden Markov model
(AR-HMM) to learn interpretable hidden states from patients' clinical time
series, and use the posterior distribution of the learned state representations
to predict various downstream outcomes, including hospital mortality, pulmonary
edema, need for diuretics, dialysis, and mechanical ventilation. Our results
show that our approach successfully incorporates the constraint to achieve high
predictive power similar to the teacher model, while maintaining the generative
performance.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring. (arXiv:2311.09668v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09668">http://arxiv.org/abs/2311.09668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09668]] Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring(http://arxiv.org/abs/2311.09668)</code></li>
<li>Summary: <p>The strong general capabilities of Large Language Models (LLMs) bring
potential ethical risks if they are unrestrictedly accessible to malicious
users. Token-level watermarking inserts watermarks in the generated texts by
altering the token probability distributions with a private random number
generator seeded by its prefix tokens. However, this watermarking algorithm
alters the logits during generation, which can lead to a downgraded text
quality if it chooses to promote tokens that are less relevant given the input.
In this work, we propose to improve the quality of texts generated by a
watermarked language model by Watermarking with Importance Scoring (WIS). At
each generation step, we estimate the importance of the token to generate, and
prevent it from being impacted by watermarking if it is important for the
semantic correctness of the output. We further propose three methods to predict
importance scoring, including a perturbation-based method and two model-based
methods. Empirical experiments show that our method can generate texts with
better quality with comparable level of detection rate.
</p></li>
</ul>

<h3>Title: Performance Trade-offs of Watermarking Large Language Models. (arXiv:2311.09816v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09816">http://arxiv.org/abs/2311.09816</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09816]] Performance Trade-offs of Watermarking Large Language Models(http://arxiv.org/abs/2311.09816)</code></li>
<li>Summary: <p>Amidst growing concerns of large language models (LLMs) being misused for
generating misinformation or completing homework assignments, watermarking has
emerged as an effective solution for distinguishing human-written and
LLM-generated text. A prominent watermarking strategy is to embed a signal into
generated text by upsampling a (pseudorandomly-chosen) subset of tokens at
every generation step. Although this signal is imperceptible to a human reader,
it is detectable through statistical testing. However, implanting such signals
alters the model's output distribution and can have unintended effects when
watermarked LLMs are used for downstream applications. In this work, we
evaluate the performance of watermarked LLMs on a diverse suite of tasks,
including text classification, textual entailment, reasoning, question
answering, translation, summarization, and language modeling. We find that
watermarking has negligible impact on the performance of tasks posed as k-class
classification problems in the average case. However, the accuracy can plummet
to that of a random classifier for some scenarios (that occur with
non-negligible probability). Tasks that are cast as multiple-choice questions
and short-form generation are surprisingly unaffected by watermarking. For
long-form generation tasks, including summarization and translation, we see a
drop of 15-20% in the performance due to watermarking. Our findings highlight
the trade-offs that users should be cognizant of when using watermarked models,
and point to cases where future research could improve existing trade-offs.
</p></li>
</ul>

<h3>Title: X-Mark: Towards Lossless Watermarking Through Lexical Redundancy. (arXiv:2311.09832v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09832">http://arxiv.org/abs/2311.09832</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09832]] X-Mark: Towards Lossless Watermarking Through Lexical Redundancy(http://arxiv.org/abs/2311.09832)</code></li>
<li>Summary: <p>Text watermarking has emerged as an important technique for detecting
machine-generated text. However, existing methods can severely degrade text
quality due to arbitrary vocabulary partitioning, which disrupts the language
model's expressiveness and impedes textual coherence. To mitigate this, we
introduce XMark, a novel approach that capitalizes on text redundancy within
the lexical space. Specifically, XMark incorporates a mutually exclusive rule
for synonyms during the language model decoding process, thereby integrating
prior knowledge into vocabulary partitioning and preserving the capabilities of
language generation. We present theoretical analyses and empirical evidence
demonstrating that XMark substantially enhances text generation fluency while
maintaining watermark detectability. Furthermore, we investigate watermarking's
impact on the emergent abilities of large language models, including zero-shot
and few-shot knowledge recall, logical reasoning, and instruction following.
Our comprehensive experiments confirm that XMark consistently outperforms
existing methods in retaining these crucial capabilities of LLMs.
</p></li>
</ul>

<h3>Title: FunctionMarker: Watermarking Language Datasets via Knowledge Injection. (arXiv:2311.09535v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09535">http://arxiv.org/abs/2311.09535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09535]] FunctionMarker: Watermarking Language Datasets via Knowledge Injection(http://arxiv.org/abs/2311.09535)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated superior performance in
various natural language processing tasks. Meanwhile, they require extensive
training data, raising concerns related to dataset copyright protection.
Backdoor-based watermarking is a viable approach to protect the copyright of
classification datasets. However, these methods may introduce malicious
misclassification behaviors into watermarked LLMs by attackers and also affect
the semantic information of the watermarked text. To address these issues, we
propose FunctionMarker, a novel copyright protection method for language
datasets via knowledge injection. FunctionMarker enables LLMs to learn specific
knowledge through fine-tuning on watermarked datasets, and we can extract the
embedded watermark by obtaining the responses of LLMs to specific
knowledge-related queries. Considering watermark capacity and stealthness, we
select customizable functions as specific knowledge for LLMs to learn and embed
the watermark into them. Moreover, FunctionMarker can embed multi-bit
watermarks while preserving the original semantic information, thereby
increasing the difficulty of adaptive attacks. We take mathematical functions
as an instance to evaluate the effectiveness of FunctionMarker, and experiments
show that only 0.3% of watermarked text achieves a 90% watermark extraction
accuracy in most cases, validating our method's effectiveness.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs. (arXiv:2311.09257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09257">http://arxiv.org/abs/2311.09257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09257]] UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs(http://arxiv.org/abs/2311.09257)</code></li>
<li>Summary: <p>Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.
\blfootnote{*Work done as a student researcher of Google, $\dagger$ indicates
equal contribution.
</p></li>
</ul>

<h3>Title: FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier. (arXiv:2311.09265v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09265">http://arxiv.org/abs/2311.09265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09265]] FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier(http://arxiv.org/abs/2311.09265)</code></li>
<li>Summary: <p>With the emergence of diffusion models and rapid development in image
processing, it has become effortless to generate fancy images in tasks such as
style transfer and image editing. However, these impressive image processing
approaches face consistency issues in video processing. In this paper, we
propose a powerful model-free toolkit called FastBlend to address the
consistency problem for video processing. Based on a patch matching algorithm,
we design two inference modes, including blending and interpolation. In the
blending mode, FastBlend eliminates video flicker by blending the frames within
a sliding window. Moreover, we optimize both computational efficiency and video
quality according to different application scenarios. In the interpolation
mode, given one or more keyframes rendered by diffusion models, FastBlend can
render the whole video. Since FastBlend does not modify the generation process
of diffusion models, it exhibits excellent compatibility. Extensive experiments
have demonstrated the effectiveness of FastBlend. In the blending mode,
FastBlend outperforms existing methods for video deflickering and video
synthesis. In the interpolation mode, FastBlend surpasses video interpolation
and model-based video processing approaches. The source codes have been
released on GitHub.
</p></li>
</ul>

<h3>Title: MDFL: Multi-domain Diffusion-driven Feature Learning. (arXiv:2311.09520v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09520">http://arxiv.org/abs/2311.09520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09520]] MDFL: Multi-domain Diffusion-driven Feature Learning(http://arxiv.org/abs/2311.09520)</code></li>
<li>Summary: <p>High-dimensional images, known for their rich semantic information, are
widely applied in remote sensing and other fields. The spatial information in
these images reflects the object's texture features, while the spectral
information reveals the potential spectral representations across different
bands. Currently, the understanding of high-dimensional images remains limited
to a single-domain perspective with performance degradation. Motivated by the
masking texture effect observed in the human visual system, we present a
multi-domain diffusion-driven feature learning network (MDFL) , a scheme to
redefine the effective information domain that the model really focuses on.
This method employs diffusion-based posterior sampling to explicitly consider
joint information interactions between the high-dimensional manifold structures
in the spectral, spatial, and frequency domains, thereby eliminating the
influence of masking texture effects in visual models. Additionally, we
introduce a feature reuse mechanism to gather deep and raw features of
high-dimensional data. We demonstrate that MDFL significantly improves the
feature extraction performance of high-dimensional data, thereby providing a
powerful aid for revealing the intrinsic patterns and structures of such data.
The experimental results on three multi-modal remote sensing datasets show that
MDFL reaches an average overall accuracy of 98.25%, outperforming various
state-of-the-art baseline schemes. The code will be released, contributing to
the computer vision community.
</p></li>
</ul>

<h3>Title: DECDM: Document Enhancement using Cycle-Consistent Diffusion Models. (arXiv:2311.09625v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09625">http://arxiv.org/abs/2311.09625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09625]] DECDM: Document Enhancement using Cycle-Consistent Diffusion Models(http://arxiv.org/abs/2311.09625)</code></li>
<li>Summary: <p>The performance of optical character recognition (OCR) heavily relies on
document image quality, which is crucial for automatic document processing and
document intelligence. However, most existing document enhancement methods
require supervised data pairs, which raises concerns about data separation and
privacy protection, and makes it challenging to adapt these methods to new
domain pairs. To address these issues, we propose DECDM, an end-to-end
document-level image translation method inspired by recent advances in
diffusion models. Our method overcomes the limitations of paired training by
independently training the source (noisy input) and target (clean output)
models, making it possible to apply domain-specific diffusion models to other
pairs. DECDM trains on one dataset at a time, eliminating the need to scan both
datasets concurrently, and effectively preserving data privacy from the source
or target domain. We also introduce simple data augmentation strategies to
improve character-glyph conservation during translation. We compare DECDM with
state-of-the-art methods on multiple synthetic data and benchmark datasets,
such as document denoising and {\color{black}shadow} removal, and demonstrate
the superiority of performance quantitatively and qualitatively.
</p></li>
</ul>

<h3>Title: DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics. (arXiv:2311.09753v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09753">http://arxiv.org/abs/2311.09753</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09753]] DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics(http://arxiv.org/abs/2311.09753)</code></li>
<li>Summary: <p>Diffusion models have advanced generative AI significantly in terms of
editing and creating naturalistic images. However, efficiently improving
generated image quality is still of paramount interest. In this context, we
propose a generic "naturalness" preserving loss function, viz., kurtosis
concentration (KC) loss, which can be readily applied to any standard diffusion
model pipeline to elevate the image quality. Our motivation stems from the
projected kurtosis concentration property of natural images, which states that
natural images have nearly constant kurtosis values across different band-pass
versions of the image. To retain the "naturalness" of the generated images, we
enforce reducing the gap between the highest and lowest kurtosis values across
the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note
that our approach does not require any additional guidance like classifier or
classifier-free guidance to improve the image quality. We validate the proposed
approach for three diverse tasks, viz., (1) personalized few-shot finetuning
using text guidance, (2) unconditional image generation, and (3) image
super-resolution. Integrating the proposed KC loss has improved the perceptual
quality across all these tasks in terms of both FID, MUSIQ score, and user
evaluation.
</p></li>
</ul>

<h3>Title: Scene Text Image Super-resolution based on Text-conditional Diffusion Models. (arXiv:2311.09759v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09759">http://arxiv.org/abs/2311.09759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09759]] Scene Text Image Super-resolution based on Text-conditional Diffusion Models(http://arxiv.org/abs/2311.09759)</code></li>
<li>Summary: <p>Scene Text Image Super-resolution (STISR) has recently achieved great success
as a preprocessing method for scene text recognition. STISR aims to transform
blurred and noisy low-resolution (LR) text images in real-world settings into
clear high-resolution (HR) text images suitable for scene text recognition. In
this study, we leverage text-conditional diffusion models (DMs), known for
their impressive text-to-image synthesis capabilities, for STISR tasks. Our
experimental results revealed that text-conditional DMs notably surpass
existing STISR methods. Especially when texts from LR text images are given as
input, the text-conditional DMs are able to produce superior quality
super-resolution text images. Utilizing this capability, we propose a novel
framework for synthesizing LR-HR paired text image datasets. This framework
consists of three specialized text-conditional DMs, each dedicated to text
image synthesis, super-resolution, and image degradation. These three modules
are vital for synthesizing distinct LR and HR paired images, which are more
suitable for training STISR methods. Our experiments confirmed that these
synthesized image pairs significantly enhance the performance of STISR methods
in the TextZoom evaluation.
</p></li>
</ul>

<h3>Title: DSR-Diff: Depth Map Super-Resolution with Diffusion Model. (arXiv:2311.09919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09919">http://arxiv.org/abs/2311.09919</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09919]] DSR-Diff: Depth Map Super-Resolution with Diffusion Model(http://arxiv.org/abs/2311.09919)</code></li>
<li>Summary: <p>Color-guided depth map super-resolution (CDSR) improve the spatial resolution
of a low-quality depth map with the corresponding high-quality color map,
benefiting various applications such as 3D reconstruction, virtual reality, and
augmented reality. While conventional CDSR methods typically rely on
convolutional neural networks or transformers, diffusion models (DMs) have
demonstrated notable effectiveness in high-level vision tasks. In this work, we
present a novel CDSR paradigm that utilizes a diffusion model within the latent
space to generate guidance for depth map super-resolution. The proposed method
comprises a guidance generation network (GGN), a depth map super-resolution
network (DSRN), and a guidance recovery network (GRN). The GGN is specifically
designed to generate the guidance while managing its compactness. Additionally,
we integrate a simple but effective feature fusion module and a
transformer-style feature extraction module into the DSRN, enabling it to
leverage guided priors in the extraction, fusion, and reconstruction of
multi-model images. Taking into account both accuracy and efficiency, our
proposed method has shown superior performance in extensive experiments when
compared to state-of-the-art methods. Our codes will be made available at
https://github.com/shiyuan7/DSR-Diff.
</p></li>
</ul>

<h3>Title: TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection. (arXiv:2311.09999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09999">http://arxiv.org/abs/2311.09999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09999]] TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection(http://arxiv.org/abs/2311.09999)</code></li>
<li>Summary: <p>Surface anomaly detection is a vital component in manufacturing inspection.
Reconstructive anomaly detection methods restore the normal appearance of an
object, ideally modifying only the anomalous regions. Due to the limitations of
commonly used reconstruction architectures, the produced reconstructions are
often poor and either still contain anomalies or lack details in anomaly-free
regions. Recent reconstructive methods adopt diffusion models, however with the
standard diffusion process the problems are not adequately addressed. We
propose a novel transparency-based diffusion process, where the transparency of
anomalous regions is progressively increased, restoring their normal appearance
accurately and maintaining the appearance of anomaly-free regions without loss
of detail. We propose TRANSparency DifFUSION (TransFusion), a discriminative
anomaly detection method that implements the proposed diffusion process,
enabling accurate downstream anomaly detection. TransFusion achieves
state-of-the-art performance on both the VisA and the MVTec AD datasets, with
an image-level AUROC of 98.5% and 99.2%, respectively.
</p></li>
</ul>

<h3>Title: What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization. (arXiv:2311.09741v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09741">http://arxiv.org/abs/2311.09741</a></li>
<li>Code URL: https://github.com/lyh6560new/p3sum</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09741]] What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization(http://arxiv.org/abs/2311.09741)</code></li>
<li>Summary: <p>In this work, we take a first step towards designing summarization systems
that are faithful to the author's opinions and perspectives. Focusing on a case
study of preserving political perspectives in news summarization, we find that
existing approaches alter the political opinions and stances of news articles
in more than 50% of summaries, misrepresenting the intent and perspectives of
the news authors. We thus propose P^3Sum, a diffusion model-based summarization
approach controlled by political perspective classifiers. In P^3Sum, the
political leaning of a generated summary is iteratively evaluated at each
decoding step, and any drift from the article's original stance incurs a loss
back-propagated to the embedding layers, steering the political stance of the
summary at inference time. Extensive experiments on three news summarization
datasets demonstrate that P^3Sum outperforms state-of-the-art summarization
systems and large language models by up to 11.4% in terms of the success rate
of stance preservation, with on-par performance on standard summarization
utility metrics. These findings highlight the lacunae that even for
state-of-the-art models it is still challenging to preserve author perspectives
in news summarization, while P^3Sum presents an important first step towards
evaluating and developing summarization systems that are faithful to author
intent and perspectives.
</p></li>
</ul>

<h3>Title: Scalable Diffusion for Materials Generation. (arXiv:2311.09235v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09235">http://arxiv.org/abs/2311.09235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09235]] Scalable Diffusion for Materials Generation(http://arxiv.org/abs/2311.09235)</code></li>
<li>Summary: <p>Generative models trained on internet-scale data are capable of generating
novel and realistic texts, images, and videos. A natural next question is
whether these models can advance science, for example by generating novel
stable materials. Traditionally, models with explicit structures (e.g., graphs)
have been used in modeling structural relationships in scientific data (e.g.,
atoms and bonds in crystals), but generating structures can be difficult to
scale to large and complex systems. Another challenge in generating materials
is the mismatch between standard generative modeling metrics and downstream
applications. For instance, common metrics such as the reconstruction error do
not correlate well with the downstream goal of discovering stable materials. In
this work, we tackle the scalability challenge by developing a unified crystal
representation that can represent any crystal structure (UniMat), followed by
training a diffusion probabilistic model on these UniMat representations. Our
empirical results suggest that despite the lack of explicit structure modeling,
UniMat can generate high fidelity crystal structures from larger and more
complex chemical systems, outperforming previous graph-based approaches under
various generative modeling metrics. To better connect the generation quality
of materials to downstream applications, such as discovering novel stable
materials, we propose additional metrics for evaluating generative models of
materials, including per-composition formation energy and stability with
respect to convex hulls through decomposition energy from Density Function
Theory (DFT). Lastly, we show that conditional generation with UniMat can scale
to previously established crystal datasets with up to millions of crystals
structures, outperforming random structure search (the current leading method
for structure discovery) in discovering new stable materials.
</p></li>
</ul>

<h3>Title: Diffusion-Augmented Neural Processes. (arXiv:2311.09848v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09848">http://arxiv.org/abs/2311.09848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09848]] Diffusion-Augmented Neural Processes(http://arxiv.org/abs/2311.09848)</code></li>
<li>Summary: <p>Over the last few years, Neural Processes have become a useful modelling tool
in many application areas, such as healthcare and climate sciences, in which
data are scarce and prediction uncertainty estimates are indispensable.
However, the current state of the art in the field (AR CNPs; Bruinsma et al.,
2023) presents a few issues that prevent its widespread deployment. This work
proposes an alternative, diffusion-based approach to NPs which, through
conditioning on noised datasets, addresses many of these limitations, whilst
also exceeding SOTA performance.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior. (arXiv:2311.09361v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09361">http://arxiv.org/abs/2311.09361</a></li>
<li>Code URL: https://github.com/jadgardner/ns_reni</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09361]] RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior(http://arxiv.org/abs/2311.09361)</code></li>
<li>Summary: <p>Inverse rendering is an ill-posed problem. Previous work has sought to
resolve this by focussing on priors for object or scene shape or appearance. In
this work, we instead focus on a prior for natural illuminations. Current
methods rely on spherical harmonic lighting or other generic representations
and, at best, a simplistic prior on the parameters. This results in limitations
for the inverse setting in terms of the expressivity of the illumination
conditions, especially when taking specular reflections into account. We
propose a conditional neural field representation based on a variational
auto-decoder and a transformer decoder. We extend Vector Neurons to build
equivariance directly into our architecture, and leveraging insights from depth
estimation through a scale-invariant loss function, we enable the accurate
representation of High Dynamic Range (HDR) images. The result is a compact,
rotation-equivariant HDR neural illumination model capable of capturing
complex, high-frequency features in natural environment maps. Training our
model on a curated dataset of 1.6K HDR environment maps of natural scenes, we
compare it against traditional representations, demonstrate its applicability
for an inverse rendering task and show environment map completion from partial
observations. We share our PyTorch implementation, dataset and trained models
at https://github.com/JADGardner/ns_reni
</p></li>
</ul>

<h3>Title: Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery. (arXiv:2311.09543v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09543">http://arxiv.org/abs/2311.09543</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09543]] Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery(http://arxiv.org/abs/2311.09543)</code></li>
<li>Summary: <p>Though significant progress in human pose and shape recovery from monocular
RGB images has been made in recent years, obtaining 3D human motion with high
accuracy and temporal consistency from videos remains challenging. Existing
video-based methods tend to reconstruct human motion from global image
features, which lack detailed representation capability and limit the
reconstruction accuracy. In this paper, we propose a Temporal-Aware Refining
Network (TAR), to synchronously explore temporal-aware global and local image
features for accurate pose and shape recovery. First, a global transformer
encoder is introduced to obtain temporal global features from static feature
sequences. Second, a bidirectional ConvGRU network takes the sequence of
high-resolution feature maps as input, and outputs temporal local feature maps
that maintain high resolution and capture the local motion of the human body.
Finally, a recurrent refinement module iteratively updates estimated SMPL
parameters by leveraging both global and local temporal information to achieve
accurate and smooth results. Extensive experiments demonstrate that our TAR
obtains more accurate results than previous state-of-the-art methods on popular
benchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M.
</p></li>
</ul>

<h3>Title: Improved TokenPose with Sparsity. (arXiv:2311.09653v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09653">http://arxiv.org/abs/2311.09653</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09653]] Improved TokenPose with Sparsity(http://arxiv.org/abs/2311.09653)</code></li>
<li>Summary: <p>Over the past few years, the vision transformer and its various forms have
gained significance in human pose estimation. By treating image patches as
tokens, transformers can capture global relationships wisely, estimate the
keypoint tokens by leveraging the visual tokens, and recognize the posture of
the human body. Nevertheless, global attention is computationally demanding,
which poses a challenge for scaling up transformer-based methods to
high-resolution features. In this paper, we introduce sparsity in both keypoint
token attention and visual token attention to improve human pose estimation.
Experimental results on the MPII dataset demonstrate that our model has a
higher level of accuracy and proved the feasibility of the method, achieving
new state-of-the-art results. The idea can also provide references for other
transformer-based models.
</p></li>
</ul>

<h3>Title: MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations. (arXiv:2311.09726v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09726">http://arxiv.org/abs/2311.09726</a></li>
<li>Code URL: https://github.com/guanyuezhen/ms-former</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09726]] MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations(http://arxiv.org/abs/2311.09726)</code></li>
<li>Summary: <p>Fully supervised change detection methods have achieved significant
advancements in performance, yet they depend severely on acquiring costly
pixel-level labels. Considering that the patch-level annotations also contain
abundant information corresponding to both changed and unchanged objects in
bi-temporal images, an intuitive solution is to segment the changes with
patch-level annotations. How to capture the semantic variations associated with
the changed and unchanged regions from the patch-level annotations to obtain
promising change results is the critical challenge for the weakly supervised
change detection task. In this paper, we propose a memory-supported transformer
(MS-Former), a novel framework consisting of a bi-directional attention block
(BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised
change detection with patch-level annotations. More specifically, the BAM
captures contexts associated with the changed and unchanged regions from the
temporal difference features to construct informative prototypes stored in the
memory bank. On the other hand, the BAM extracts useful information from the
prototypes as supplementary contexts to enhance the temporal difference
features, thereby better distinguishing changed and unchanged regions. After
that, the PSS guides the network learning valuable knowledge from the
patch-level annotations, thus further elevating the performance. Experimental
results on three benchmark datasets demonstrate the effectiveness of our
proposed method in the change detection task. The demo code for our work will
be publicly available at \url{https://github.com/guanyuezhen/MS-Former}.
</p></li>
</ul>

<h3>Title: Neural-Logic Human-Object Interaction Detection. (arXiv:2311.09817v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09817">http://arxiv.org/abs/2311.09817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09817]] Neural-Logic Human-Object Interaction Detection(http://arxiv.org/abs/2311.09817)</code></li>
<li>Summary: <p>The interaction decoder utilized in prevalent Transformer-based HOI detectors
typically accepts pre-composed human-object pairs as inputs. Though achieving
remarkable performance, such paradigm lacks feasibility and cannot explore
novel combinations over entities during decoding. We present L OGIC HOI, a new
HOI detector that leverages neural-logic reasoning and Transformer to infer
feasible interactions between entities. Specifically, we modify the
self-attention mechanism in vanilla Transformer, enabling it to reason over the
&lt;human, action, object&gt; triplet and constitute novel interactions. Meanwhile,
such reasoning process is guided by two crucial properties for understanding
HOI: affordances (the potential actions an object can facilitate) and proxemics
(the spatial relations between humans and objects). We formulate these two
properties in first-order logic and ground them into continuous space to
constrain the learning process of our approach, leading to improved performance
and zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO and
HICO-DET under both normal and zero-shot setups, achieving significant
improvements over existing methods.
</p></li>
</ul>

<h3>Title: DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's Distance. (arXiv:2311.09998v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09998">http://arxiv.org/abs/2311.09998</a></li>
<li>Code URL: https://github.com/atulkumarin/deepemd</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09998]] DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's Distance(http://arxiv.org/abs/2311.09998)</code></li>
<li>Summary: <p>The Earth Mover's Distance (EMD) is the measure of choice between point
clouds. However the computational cost to compute it makes it prohibitive as a
training loss, and the standard approach is to use a surrogate such as the
Chamfer distance. We propose an attention-based model to compute an accurate
approximation of the EMD that can be used as a training loss for generative
models. To get the necessary accurate estimation of the gradients we train our
model to explicitly compute the matching between point clouds instead of EMD
itself. We cast this new objective as the estimation of an attention matrix
that approximates the ground truth matching matrix. Experiments show that this
model provides an accurate estimate of the EMD and its gradient with a wall
clock speed-up of more than two orders of magnitude with respect to the exact
Hungarian matching algorithm and one order of magnitude with respect to the
standard approximate Sinkhorn algorithm, allowing in particular to train a
point cloud VAE with the EMD itself. Extensive evaluation show the remarkable
behaviour of this model when operating out-of-distribution, a key requirement
for a distance surrogate. Finally, the model generalizes very well to point
clouds during inference several times larger than during training.
</p></li>
</ul>

<h3>Title: Banach-Tarski Embeddings and Transformers. (arXiv:2311.09387v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09387">http://arxiv.org/abs/2311.09387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09387]] Banach-Tarski Embeddings and Transformers(http://arxiv.org/abs/2311.09387)</code></li>
<li>Summary: <p>We introduce a new construction of embeddings of arbitrary recursive data
structures into high dimensional vectors. These embeddings provide an
interpretable model for the latent state vectors of transformers. We
demonstrate that these embeddings can be decoded to the original data structure
when the embedding dimension is sufficiently large. This decoding algorithm has
a natural implementation as a transformer. We also show that these embedding
vectors can be manipulated directly to perform computations on the underlying
data without decoding. As an example we present an algorithm that constructs
the embedded parse tree of an embedded token sequence using only vector
operations in embedding space.
</p></li>
</ul>

<h3>Title: Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture. (arXiv:2311.09406v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09406">http://arxiv.org/abs/2311.09406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09406]] Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture(http://arxiv.org/abs/2311.09406)</code></li>
<li>Summary: <p>The transformer neural network architecture uses a form of attention in which
the dot product of query and key is divided by the square root of the key
dimension before applying softmax. This scaling of the dot product is designed
to avoid the absolute value of the dot products becoming so large that applying
softmax leads to vanishing gradients. In this paper, we propose some
alternative scalings, including dividing the dot product instead by the sum of
the key lengths before applying softmax. We use simulated keys and queries to
show that in many situations this appears to be more effective at avoiding
regions where applying softmax leads to vanishing gradients.
</p></li>
</ul>

<h3>Title: Striped Attention: Faster Ring Attention for Causal Transformers. (arXiv:2311.09431v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09431">http://arxiv.org/abs/2311.09431</a></li>
<li>Code URL: https://github.com/exists-forall/striped_attention</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09431]] Striped Attention: Faster Ring Attention for Causal Transformers(http://arxiv.org/abs/2311.09431)</code></li>
<li>Summary: <p>To help address the growing demand for ever-longer sequence lengths in
transformer models, Liu et al. recently proposed Ring Attention, an exact
attention algorithm capable of overcoming per-device memory bottle- necks by
distributing self-attention across multiple devices. In this paper, we study
the performance characteristics of Ring Attention in the important special case
of causal transformer models, and identify a key workload imbal- ance due to
triangular structure of causal attention computations. We propose a simple
extension to Ring Attention, which we call Striped Attention to fix this
imbalance. Instead of devices having contiguous subsequences, each device has a
subset of tokens distributed uniformly throughout the sequence, which we
demonstrate leads to more even workloads. In experiments running Striped
Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x
end-to-end throughput improvements over the original Ring Attention algorithm
on causal transformer training at a sequence length of 256k. Furthermore, on 16
TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of
786k. We release the code for our experiments as open source
</p></li>
</ul>

<h3>Title: LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks. (arXiv:2311.09564v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09564">http://arxiv.org/abs/2311.09564</a></li>
<li>Code URL: https://github.com/mihir3009/longbox</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09564]] LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks(http://arxiv.org/abs/2311.09564)</code></li>
<li>Summary: <p>Many large language models (LLMs) for medicine have largely been evaluated on
short texts, and their ability to handle longer sequences such as a complete
electronic health record (EHR) has not been systematically explored. Assessing
these models on long sequences is crucial since prior work in the general
domain has demonstrated performance degradation of LLMs on longer texts.
Motivated by this, we introduce LongBoX, a collection of seven medical datasets
in text-to-text format, designed to investigate model performance on long
sequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT)
and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We
further evaluate two techniques designed for long-sequence handling: (i)
local-global attention, and (ii) Fusion-in-Decoder (FiD). Our results
demonstrate mixed results with long-sequence handling - while scores on some
datasets increase, there is substantial room for improvement. We hope that
LongBoX facilitates the development of more effective long-sequence techniques
for the medical domain. Data and source code are available at
https://github.com/Mihir3009/LongBoX.
</p></li>
</ul>

<h3>Title: SurvTimeSurvival: Survival Analysis On The Patient With Multiple Visits/Records. (arXiv:2311.09854v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09854">http://arxiv.org/abs/2311.09854</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09854]] SurvTimeSurvival: Survival Analysis On The Patient With Multiple Visits/Records(http://arxiv.org/abs/2311.09854)</code></li>
<li>Summary: <p>The accurate prediction of survival times for patients with severe diseases
remains a critical challenge despite recent advances in artificial
intelligence. This study introduces "SurvTimeSurvival: Survival Analysis On
Patients With Multiple Visits/Records", utilizing the Transformer model to not
only handle the complexities of time-varying covariates but also covariates
data. We also tackle the data sparsity issue common to survival analysis
datasets by integrating synthetic data generation into the learning process of
our model. We show that our method outperforms state-of-the-art deep learning
approaches on both covariates and time-varying covariates datasets. Our
approach aims not only to enhance the understanding of individual patient
survival trajectories across various medical conditions, thereby improving
prediction accuracy, but also to play a pivotal role in designing clinical
trials and creating new treatments.
</p></li>
</ul>

<h3>Title: Self-supervised learning of multi-omics embeddings in the low-label, high-data regime. (arXiv:2311.09962v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09962">http://arxiv.org/abs/2311.09962</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09962]] Self-supervised learning of multi-omics embeddings in the low-label, high-data regime(http://arxiv.org/abs/2311.09962)</code></li>
<li>Summary: <p>Contrastive, self-supervised learning (SSL) is used to train a model that
predicts cancer type from miRNA, mRNA or RPPA expression data. This model, a
pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost,
standard benchmarks for tabular data, when labelled samples are scarce but the
number of unlabelled samples is high. This is despite the fact that the
datasets we use have $\mathcal{O}(10^{1})$ classes and
$\mathcal{O}(10^{2})-\mathcal{O}(10^{4})$ features. After demonstrating the
efficacy of our chosen method of self-supervised pretraining, we investigate
SSL for multi-modal models. A late-fusion model is proposed, where each omics
is passed through its own sub-network, the outputs of which are averaged and
passed to the pretraining or downstream objective function. Multi-modal
pretraining is shown to improve predictions from a single omics, and we argue
that this is useful for datasets with many unlabelled multi-modal samples, but
few labelled unimodal samples. Additionally, we show that pretraining each
omics-specific module individually is highly effective. This enables the
application of the proposed model in a variety of contexts where a large amount
of unlabelled data is available from each omics, but only a few labelled
samples.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation. (arXiv:2311.09467v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09467">http://arxiv.org/abs/2311.09467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09467]] Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation(http://arxiv.org/abs/2311.09467)</code></li>
<li>Summary: <p>Neural knowledge-to-text generation models often struggle to faithfully
generate descriptions for the input facts: they may produce hallucinations that
contradict the given facts, or describe facts not present in the input. To
reduce hallucinations, we propose a novel decoding method, TWEAK (Think While
Effectively Articulating Knowledge). TWEAK treats the generated sequences at
each decoding step and its future sequences as hypotheses, and ranks each
generation candidate based on how well their corresponding hypotheses support
the input facts using a Hypothesis Verification Model (HVM). We first
demonstrate the effectiveness of TWEAK by using a Natural Language Inference
(NLI) model as the HVM and report improved faithfulness with minimal impact on
the quality. We then replace the NLI model with our task-specific HVM trained
with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which
pairs input facts with their faithful and hallucinated descriptions with the
hallucinated spans marked. The new HVM improves the faithfulness and the
quality further and runs faster. Overall the best TWEAK variants improve on
average 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and
TekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality
measured by BERTScore over the same datasets. Since TWEAK is a decoding-only
approach, it can be integrated with any neural generative model without
retraining.
</p></li>
</ul>

<h3>Title: Prompt Optimisation with Random Sampling. (arXiv:2311.09569v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09569">http://arxiv.org/abs/2311.09569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09569]] Prompt Optimisation with Random Sampling(http://arxiv.org/abs/2311.09569)</code></li>
<li>Summary: <p>Using the generative nature of a language model to generate task-relevant
separators has shown competitive results compared to human-curated prompts like
"TL;DR". We demonstrate that even randomly chosen tokens from the vocabulary as
separators can achieve near-state-of-the-art performance. We analyse this
phenomenon in detail using three different random generation strategies,
establishing that the language space is rich with potential good separators,
regardless of the underlying language model size. These observations challenge
the common assumption that an effective prompt should be human-readable or
task-relevant. Experimental results show that using random separators leads to
an average 16% relative improvement across nine text classification tasks on
seven language models, compared to human-curated separators, and is on par with
automatic prompt searching methods.
</p></li>
</ul>

<h3>Title: GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding. (arXiv:2311.09707v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09707">http://arxiv.org/abs/2311.09707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09707]] GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding(http://arxiv.org/abs/2311.09707)</code></li>
<li>Summary: <p>Language models can serve as a valuable tool for software developers to
increase productivity. Large generative models can be used for code generation
and code completion, while smaller encoder-only models are capable of
performing code search tasks using natural language queries.These capabilities
are heavily influenced by the quality and diversity of the available training
data. Source code datasets used for training usually focus on the most popular
languages and testing is mostly conducted on the same distributions, often
overlooking low-resource programming languages. Motivated by the NLP
generalization taxonomy proposed by Hupkes et.\,al., we propose a new benchmark
dataset called GenCodeSearchNet (GeCS) which builds upon existing natural
language code search datasets to systemically evaluate the programming language
understanding generalization capabilities of language models. As part of the
full dataset, we introduce a new, manually curated subset StatCodeSearch that
focuses on R, a popular but so far underrepresented programming language that
is often used by researchers outside the field of computer science. For
evaluation and comparison, we collect several baseline results using fine-tuned
BERT-style models and GPT-style large language models in a zero-shot setting.
</p></li>
</ul>

<h3>Title: LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores. (arXiv:2311.09766v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09766">http://arxiv.org/abs/2311.09766</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09766]] LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores(http://arxiv.org/abs/2311.09766)</code></li>
<li>Summary: <p>Automatic evaluation of generated textual content presents an ongoing
challenge within the field of NLP. Given the impressive capabilities of modern
language models (LMs) across diverse NLP tasks, there is a growing trend to
employ these models in creating innovative evaluation metrics for automated
assessment of generation tasks. This paper investigates a pivotal question: Do
language model-driven evaluation metrics inherently exhibit bias favoring texts
generated by the same underlying language model? Specifically, we assess
whether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and
GPTScore--demonstrate a favorable bias toward their respective underlying LMs
in the context of summarization tasks. Our findings unveil a latent bias,
particularly pronounced when such evaluation metrics are used in an
reference-free manner without leveraging gold summaries. These results
underscore that assessments provided by generative evaluation models can be
influenced by factors beyond the inherent text quality, highlighting the
necessity of developing more dependable evaluation protocols in the future.
</p></li>
</ul>

<h3>Title: Language Generation from Human Brain Activities. (arXiv:2311.09889v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09889">http://arxiv.org/abs/2311.09889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09889]] Language Generation from Human Brain Activities(http://arxiv.org/abs/2311.09889)</code></li>
<li>Summary: <p>Generating human language through non-invasive brain-computer interfaces
(BCIs) has the potential to unlock many applications, such as serving disabled
patients and improving communication. Currently, however, generating language
via BCIs has been previously successful only within a classification setup for
selecting pre-generated sentence continuation candidates with the most likely
cortical semantic representation. Inspired by recent research that revealed
associations between the brain and the large computational language models, we
propose a generative language BCI that utilizes the capacity of a large
language model (LLM) jointly with a semantic brain decoder to directly generate
language from functional magnetic resonance imaging (fMRI) input. The proposed
model can generate coherent language sequences aligned with the semantic
content of visual or auditory language stimuli perceived, without prior
knowledge of any pre-generated candidates. We compare the language generated
from the presented model with a random control, pre-generated language
selection approach, and a standard LLM, which generates common coherent text
solely based on the next word likelihood according to statistical language
training data. The proposed model is found to generate language that is more
aligned with semantic stimulus in response to which brain input is sampled. Our
findings demonstrate the potential and feasibility of employing BCIs in direct
language generation.
</p></li>
</ul>

<h3>Title: Generative AI for Hate Speech Detection: Evaluation and Findings. (arXiv:2311.09993v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09993">http://arxiv.org/abs/2311.09993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09993]] Generative AI for Hate Speech Detection: Evaluation and Findings(http://arxiv.org/abs/2311.09993)</code></li>
<li>Summary: <p>Automatic hate speech detection using deep neural models is hampered by the
scarcity of labeled datasets, leading to poor generalization. To mitigate this
problem, generative AI has been utilized to generate large amounts of synthetic
hate speech sequences from available labeled examples, leveraging the generated
data in finetuning large pre-trained language models (LLMs). In this chapter,
we provide a review of relevant methods, experimental setups and evaluation of
this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT,
we apply and evaluate the impact of train set augmentation with generated data
using LLMs that have been already adapted for hate detection, including
RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical
study corroborates our previous findings, showing that this approach improves
hate speech generalization, boosting recall performance across data
distributions. In addition, we explore and compare the performance of the
finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results
demonstrate that while better generalization is achieved using the GPT-3.5
model, it achieves mediocre recall and low precision on most datasets. It is an
open question whether the sensitivity of models such as GPT-3.5, and onward,
can be improved using similar techniques of text generation.
</p></li>
</ul>

<h3>Title: SynDiffix: More accurate synthetic structured data. (arXiv:2311.09628v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09628">http://arxiv.org/abs/2311.09628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09628]] SynDiffix: More accurate synthetic structured data(http://arxiv.org/abs/2311.09628)</code></li>
<li>Summary: <p>This paper introduces SynDiffix, a mechanism for generating statistically
accurate, anonymous synthetic data for structured data. Recent open source and
commercial systems use Generative Adversarial Networks or Transformed Auto
Encoders to synthesize data, and achieve anonymity through
overfitting-avoidance. By contrast, SynDiffix exploits traditional mechanisms
of aggregation, noise addition, and suppression among others. Compared to
CTGAN, ML models generated from SynDiffix are twice as accurate, marginal and
column pairs data quality is one to two orders of magnitude more accurate, and
execution time is two orders of magnitude faster. Compared to the best
commercial product we measured (MostlyAI), ML model accuracy is comparable,
marginal and pairs accuracy is 5 to 10 times better, and execution time is an
order of magnitude faster. Similar to the other approaches, SynDiffix
anonymization is very strong. This paper describes SynDiffix and compares its
performance with other popular open source and commercial systems.
</p></li>
</ul>

<h3>Title: Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production. (arXiv:2311.09333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09333">http://arxiv.org/abs/2311.09333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09333]] Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production(http://arxiv.org/abs/2311.09333)</code></li>
<li>Summary: <p>A significant challenge for predictive maintenance in the pulp-and-paper
industry is the infrequency of paper breaks during the production process. In
this article, operational data is analyzed from a paper manufacturing machine
in which paper breaks are relatively rare but have a high economic impact.
Utilizing a dataset comprising 18,398 instances derived from a quality
assurance protocol, we address the scarcity of break events (124 cases) that
pose a challenge for machine learning predictive models. With the help of
Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority
Oversampling Technique (SMOTE), we implement a novel data augmentation
framework. This method ensures that the synthetic data mirrors the distribution
of the real operational data but also seeks to enhance the performance metrics
of predictive modeling. Before and after the data augmentation, we evaluate
three different machine learning algorithms-Decision Trees (DT), Random Forest
(RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our
study achieved significant improvements in predictive maintenance performance
metrics. The efficacy of CTGAN in addressing data scarcity was evident, with
the models' detection of machine breaks (Class 1) improving by over 30% for
Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression.
With this methodological advancement, this study contributes to industrial
quality control and maintenance scheduling by addressing rare event prediction
in manufacturing processes.
</p></li>
</ul>

<h3>Title: GEO: Generative Engine Optimization. (arXiv:2311.09735v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09735">http://arxiv.org/abs/2311.09735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09735]] GEO: Generative Engine Optimization(http://arxiv.org/abs/2311.09735)</code></li>
<li>Summary: <p>The advent of large language models (LLMs) has ushered in a new paradigm of
search engines that use generative models to gather and summarize information
to answer user queries. This emerging technology, which we formalize under the
unified framework of Generative Engines (GEs), has the potential to generate
accurate and personalized responses, and is rapidly replacing traditional
search engines like Google and Bing. Generative Engines typically satisfy
queries by synthesizing information from multiple sources and summarizing them
with the help of LLMs. While this shift significantly improves \textit{user}
utility and \textit{generative search engine} traffic, it results in a huge
challenge for the third stakeholder -- website and content creators. Given the
black-box and fast-moving nature of Generative Engines, content creators have
little to no control over when and how their content is displayed. With
generative engines here to stay, the right tools should be provided to ensure
that creator economy is not severely disadvantaged. To address this, we
introduce Generative Engine Optimization (GEO), a novel paradigm to aid content
creators in improving the visibility of their content in Generative Engine
responses through a black-box optimization framework for optimizing and
defining visibility metrics. We facilitate systematic evaluation in this new
paradigm by introducing GEO-bench, a benchmark of diverse user queries across
multiple domains, coupled with sources required to answer these queries.
Through rigorous evaluation, we show that GEO can boost visibility by up to
40\% in generative engine responses. Moreover, we show the efficacy of these
strategies varies across domains, underscoring the need for domain-specific
methods. Our work opens a new frontier in the field of information discovery
systems, with profound implications for generative engines and content
creators.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Chain of Images for Intuitively Reasoning. (arXiv:2311.09241v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09241">http://arxiv.org/abs/2311.09241</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09241]] Chain of Images for Intuitively Reasoning(http://arxiv.org/abs/2311.09241)</code></li>
<li>Summary: <p>The human brain is naturally equipped to comprehend and interpret visual
information rapidly. When confronted with complex problems or concepts, we use
flowcharts, sketches, and diagrams to aid our thought process. Leveraging this
inherent ability can significantly enhance logical reasoning. However, current
Large Language Models (LLMs) do not utilize such visual intuition to help their
thinking. Even the most advanced version language models (e.g., GPT-4V and
LLaVA) merely align images into textual space, which means their reasoning
processes remain purely verbal. To mitigate such limitations, we present a
Chain of Images (CoI) approach, which can convert complex language reasoning
problems to simple pattern recognition by generating a series of images as
intermediate representations. Furthermore, we have developed a CoI evaluation
dataset encompassing 15 distinct domains where images can intuitively aid
problem-solving. Based on this dataset, we aim to construct a benchmark to
assess the capability of future multimodal large-scale models to leverage
images for reasoning. In supporting our CoI reasoning, we introduce a symbolic
multimodal large language model (SyMLLM) that generates images strictly based
on language instructions and accepts both text and image as input. Experiments
on Geometry, Chess and Common Sense tasks sourced from the CoI evaluation
dataset show that CoI improves performance significantly over the pure-language
Chain of Thoughts (CoT) baselines. The code is available at
https://github.com/GraphPKU/CoI.
</p></li>
</ul>

<h3>Title: Efficient End-to-End Visual Document Understanding with Rationale Distillation. (arXiv:2311.09612v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09612">http://arxiv.org/abs/2311.09612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09612]] Efficient End-to-End Visual Document Understanding with Rationale Distillation(http://arxiv.org/abs/2311.09612)</code></li>
<li>Summary: <p>Understanding visually situated language requires recognizing text and visual
elements, and interpreting complex layouts. State-of-the-art methods commonly
use specialized pre-processing tools, such as optical character recognition
(OCR) systems, that map document image inputs to extracted information in the
space of textual tokens, and sometimes also employ large language models (LLMs)
to reason in text token space. However, the gains from external tools and LLMs
come at the cost of increased computational and engineering complexity. In this
paper, we ask whether small pretrained image-to-text models can learn selective
text or layout recognition and reasoning as an intermediate inference step in
an end-to-end model for pixel-level visual language understanding. We
incorporate the outputs of such OCR tools, LLMs, and larger multimodal models
as intermediate ``rationales'' on training data, and train a small student
model to predict both rationales and answers for input questions based on those
training examples. A student model based on Pix2Struct (282M parameters)
achieves consistent improvements on three visual document understanding
benchmarks representing infographics, scanned documents, and figures, with
improvements of more than 4\% absolute over a comparable Pix2Struct model that
predicts answers directly.
</p></li>
</ul>

<h3>Title: Auto-ICL: In-Context Learning without Human Supervision. (arXiv:2311.09263v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09263">http://arxiv.org/abs/2311.09263</a></li>
<li>Code URL: https://github.com/ecielyang/auto-icl</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09263]] Auto-ICL: In-Context Learning without Human Supervision(http://arxiv.org/abs/2311.09263)</code></li>
<li>Summary: <p>In the era of Large Language Models (LLMs), human-computer interaction has
evolved towards natural language, offering unprecedented flexibility. Despite
this, LLMs are heavily reliant on well-structured prompts to function
efficiently within the realm of In-Context Learning. Vanilla In-Context
Learning relies on human-provided contexts, such as labeled examples, explicit
instructions, or other guiding mechanisms that shape the model's outputs. To
address this challenge, our study presents a universal framework named
Automatic In-Context Learning. Upon receiving a user's request, we ask the
model to independently generate examples, including labels, instructions, or
reasoning pathways. The model then leverages this self-produced context to
tackle the given problem. Our approach is universally adaptable and can be
implemented in any setting where vanilla In-Context Learning is applicable. We
demonstrate that our method yields strong performance across a range of tasks,
standing up well when compared to existing methods.
</p></li>
</ul>

<h3>Title: Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models. (arXiv:2311.09278v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09278">http://arxiv.org/abs/2311.09278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09278]] Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models(http://arxiv.org/abs/2311.09278)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have greatly propelled the progress in natural
language(NL)-centric tasks based on NL interface. However, the NL form is not
enough for world knowledge. Current works focus on this question by injecting
specific symbolic knowledge into LLM, which ignore two critical challenges: the
interrelations between various symbols and the balance between symbolic-centric
and NL-centric capabilities. In this work, we tackle these challenges from both
a data and framework perspective and introduce Symbol-LLM series models. First,
we collect 34 symbolic tasks, covering ~20 different forms, which are unified
to capture symbol interrelations. Then, a two-stage tuning framework succeeds
in injecting symbolic knowledge without loss of the generality ability.
Extensive experiments on both symbol- and NL-centric tasks demonstrate the
balanced and superior performances of Symbol-LLM series models.
</p></li>
</ul>

<h3>Title: Improving fit to human reading times via temperature-scaled surprisal. (arXiv:2311.09325v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09325">http://arxiv.org/abs/2311.09325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09325]] Improving fit to human reading times via temperature-scaled surprisal(http://arxiv.org/abs/2311.09325)</code></li>
<li>Summary: <p>Past studies have provided broad support for that words with lower
predictability (i.e., higher surprisal) require more time for comprehension by
using large language models (LLMs) to simulate humans' cognitive load. In
general, these studies have implicitly assumed that the probability scores from
LLMs are accurate, ignoring the discrepancies between human cognition and LLMs
from this standpoint. Inspired by the concept of probability calibration, we
are the first work to focus on the probability distribution for human reading
simulation. We propose to use temperature-scaled surprisal, a surprisal
calculated by shaped probability, to be the predictor of human reading times.
Our results across three corpora consistently revealed that such a surprisal
can drastically improve the prediction of reading times. Setting the
temperature to be approximately 2.5 across all models and datasets can yield up
to an 89% of increase in delta log-likelihood in our setting. We also propose a
calibration metric to quantify the possible human-likeness bias. Further
analysis was done and provided insights into this phenomenon.
</p></li>
</ul>

<h3>Title: Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09335">http://arxiv.org/abs/2311.09335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09335]] Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization(http://arxiv.org/abs/2311.09335)</code></li>
<li>Summary: <p>Despite their remarkable performance on abstractive summarization, large
language models (LLMs) face two significant challenges: their considerable size
and tendency to hallucinate. Hallucinations are concerning because they erode
the reliability of LLMs and raise safety issues. Pruning is a technique that
reduces model size by removing redundant weights to create sparse models that
enable more efficient inference. Pruned models yield comparable performance to
their counterpart full-sized models, making them ideal alternatives when
operating on a limited budget. However, the effect that pruning has upon
hallucinations in abstractive summarization with LLMs has yet to be explored.
In this paper, we provide an extensive empirical study on the hallucinations
produced by pruned models across three standard summarization tasks, two
pruning approaches, three instruction-tuned LLMs, and three hallucination
evaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less
compared to their full-sized counterparts. Our follow-up analysis suggests that
pruned models tend to depend more on the source input and less on their
parametric knowledge from pre-training for generation. This greater dependency
on the source input leads to a higher lexical overlap between generated content
and the source input, which can be a reason for the reduction in
hallucinations.
</p></li>
</ul>

<h3>Title: Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback. (arXiv:2311.09336v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09336">http://arxiv.org/abs/2311.09336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09336]] Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback(http://arxiv.org/abs/2311.09336)</code></li>
<li>Summary: <p>Recent improvements in text generation have leveraged human feedback to
improve the quality of the generated output. However, human feedback is not
always available, especially during inference. In this work, we propose an
inference time optimization method FITO to use fine-grained actionable feedback
in the form of error type, error location and severity level that are predicted
by a learned error pinpoint model for iterative refinement. FITO starts with an
initial output, then iteratively incorporates the feedback via a refinement
model that generates an improved output conditioned on the feedback. Given the
uncertainty of consistent refined samples at iterative steps, we formulate
iterative refinement into a local search problem and develop a simulated
annealing based algorithm that balances exploration of the search space and
optimization for output quality. We conduct experiments on three text
generation tasks, including machine translation, long-form question answering
(QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on
Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at
long form QA and topic summarization respectively, with a single iteration of
refinement. With our simulated annealing algorithm, we see further quality
improvements, including up to 1.7 MetricX improvements over the baseline
approach.
</p></li>
</ul>

<h3>Title: Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization. (arXiv:2311.09344v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09344">http://arxiv.org/abs/2311.09344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09344]] Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization(http://arxiv.org/abs/2311.09344)</code></li>
<li>Summary: <p>Parameter-efficient fine-tuning (PEFT) using labeled task data can
significantly improve the performance of large language models (LLMs) on the
downstream task. However, there are 7000 languages in the world and many of
these languages lack labeled data for real-world language generation tasks. In
this paper, we propose to improve zero-shot cross-lingual transfer by composing
language or task specialized parameters. Our method composes language and task
PEFT modules via element-wise arithmetic operations to leverage unlabeled data
and English labeled data. We extend our approach to cases where labeled data
from more languages is available and propose to arithmetically compose PEFT
modules trained on languages related to the target. Empirical results on
summarization demonstrate that our method is an effective strategy that obtains
consistent gains using minimal training of PEFT modules.
</p></li>
</ul>

<h3>Title: Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science. (arXiv:2311.09358v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09358">http://arxiv.org/abs/2311.09358</a></li>
<li>Code URL: https://github.com/pnnl/expert2</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09358]] Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science(http://arxiv.org/abs/2311.09358)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown remarkable achievements in natural
language processing tasks, producing high-quality outputs. However, LLMs still
exhibit limitations, including the generation of factually incorrect
information. In safety-critical applications, it is important to assess the
confidence of LLM-generated content to make informed decisions. Retrieval
Augmented Language Models (RALMs) is relatively a new area of research in NLP.
RALMs offer potential benefits for scientific NLP tasks, as retrieved
documents, can serve as evidence to support model-generated content. This
inclusion of evidence enhances trustworthiness, as users can verify and explore
the retrieved documents to validate model outputs. Quantifying uncertainty in
RALM generations further improves trustworthiness, with retrieved text and
confidence scores contributing to a comprehensive and reliable model for
scientific applications. However, there is limited to no research on UQ for
RALMs, particularly in scientific contexts. This study aims to address this gap
by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific
tasks. This research investigates how uncertainty scores vary when scientific
knowledge is incorporated as pretraining and retrieval data and explores the
relationship between uncertainty scores and the accuracy of model-generated
outputs. We observe that an existing RALM finetuned with scientific knowledge
as the retrieval data tends to be more confident in generating predictions
compared to the model pretrained only with scientific knowledge. We also found
that RALMs are overconfident in their predictions, making inaccurate
predictions more confidently than accurate ones. Scientific knowledge provided
either as pretraining or retrieval corpus does not help alleviate this issue.
We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.
</p></li>
</ul>

<h3>Title: When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour. (arXiv:2311.09410v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09410">http://arxiv.org/abs/2311.09410</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09410]] When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour(http://arxiv.org/abs/2311.09410)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have been demonstrating the ability to solve
complex tasks by delivering answers that are positively evaluated by humans due
in part to the intensive use of human feedback that refines responses. However,
the suggestibility transmitted through human feedback increases the inclination
to produce responses that correspond to the user's beliefs or misleading
prompts as opposed to true facts, a behaviour known as sycophancy. This
phenomenon decreases the bias, robustness, and, consequently, their
reliability.
</p>
<p>In this paper, we shed light on the suggestibility of LLMs to sycophantic
behaviour, demonstrating these tendencies via human-influenced prompts over
different tasks. Our investigation reveals that LLMs show sycophantic
tendencies when responding to queries involving subjective opinions and
statements that should elicit a contrary response based on facts, demonstrating
a lack of robustness.
</p></li>
</ul>

<h3>Title: Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents. (arXiv:2311.09513v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09513">http://arxiv.org/abs/2311.09513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09513]] Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents(http://arxiv.org/abs/2311.09513)</code></li>
<li>Summary: <p>This paper contains what the Georgetown InfoSense group has done in regard to
solving the challenges presented by TREC iKAT 2023. Our submitted runs
outperform the median runs by a significant margin, exhibiting superior
performance in nDCG across various cut numbers and in overall success rate. Our
approach uses a Generate-Retrieve-Generate method, which we've found to greatly
outpace Retrieve-Then-Generate approaches for the purposes of iKAT. Our
solution involves the use of Large Language Models (LLMs) for initial answers,
answer grounding by BM25, passage quality filtering by logistic regression, and
answer generation by LLMs again. We leverage several purpose-built Language
Models, including BERT, Chat-based, and text-to-transfer-based models, for text
understanding, classification, generation, and summarization. The official
results of the TREC evaluation contradict our initial self-evaluation, which
may suggest that a decrease in the reliance on our retrieval and classification
methods is better. Nonetheless, our findings suggest that the sequence of
involving these different components matters, where we see an essentiality of
using LLMs before using search engines.
</p></li>
</ul>

<h3>Title: GEE! Grammar Error Explanation with Large Language Models. (arXiv:2311.09517v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09517">http://arxiv.org/abs/2311.09517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09517]] GEE! Grammar Error Explanation with Large Language Models(http://arxiv.org/abs/2311.09517)</code></li>
<li>Summary: <p>Grammatical error correction tools are effective at correcting grammatical
errors in users' input sentences but do not provide users with \textit{natural
language} explanations about their errors. Such explanations are essential for
helping users learn the language by gaining a deeper understanding of its
grammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we
propose the task of grammar error explanation, where a system needs to provide
one-sentence explanations for each grammatical error in a pair of erroneous and
corrected sentences. We analyze the capability of GPT-4 in grammar error
explanation, and find that it only produces explanations for 60.2% of the
errors using one-shot prompting. To improve upon this performance, we develop a
two-step pipeline that leverages fine-tuned and prompted large language models
to perform structured atomic token edit extraction, followed by prompting GPT-4
to generate explanations. We evaluate our pipeline on German and Chinese
grammar error correction data sampled from language learners with a wide range
of proficiency levels. Human evaluation reveals that our pipeline produces
93.9% and 98.0% correct explanations for German and Chinese data, respectively.
To encourage further research in this area, we will open-source our data and
code.
</p></li>
</ul>

<h3>Title: Effective Large Language Model Adaptation for Improved Grounding. (arXiv:2311.09533v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09533">http://arxiv.org/abs/2311.09533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09533]] Effective Large Language Model Adaptation for Improved Grounding(http://arxiv.org/abs/2311.09533)</code></li>
<li>Summary: <p>Large language models (LLMs) have achieved remarkable advancements in natural
language understanding, generation, and manipulation of text-based data.
However, one major issue towards their widespread deployment in the real world
is that they can generate "hallucinated" answers that are not factual. Towards
this end, this paper focuses on improving grounding from a holistic perspective
with a novel framework, AGREE, Adaptation of LLMs for GRounding EnhancEment. We
start with the design of an iterative test-time adaptation (TTA) capability
that takes into account the support information generated in self-grounded
responses. To effectively enable this capability, we tune LLMs to ground the
claims in their responses to retrieved documents by providing citations. This
tuning on top of the pre-trained LLMs requires a small amount of data that
needs to be constructed in a particular way to learn the grounding information,
for which we introduce a data construction method. Our results show that the
tuning-based AGREE framework generates better grounded responses with more
accurate citations compared to prompting-based approaches.
</p></li>
</ul>

<h3>Title: Towards Pragmatic Awareness in Question Answering: A Case Study in Maternal and Infant Health. (arXiv:2311.09542v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09542">http://arxiv.org/abs/2311.09542</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09542]] Towards Pragmatic Awareness in Question Answering: A Case Study in Maternal and Infant Health(http://arxiv.org/abs/2311.09542)</code></li>
<li>Summary: <p>Questions posed by information-seeking users often contain implicit false or
potentially harmful assumptions. In a high-risk domain such as maternal and
infant health, a question-answering system must recognize these pragmatic
constraints and go beyond simply answering user questions, examining them in
context to respond helpfully. To achieve this, we study pragmatic inferences
made when mothers ask questions about pregnancy and infant care. Some of the
inferences in these questions evade detection by existing methods, risking the
possibility of QA systems failing to address them which can have dangerous
health and policy implications. We explore the viability of detecting
inferences from questions using large language models and illustrate that
informing existing QA pipelines with pragmatic inferences produces responses
that can mitigate the propagation of harmful beliefs.
</p></li>
</ul>

<h3>Title: A Speed Odyssey for Deployable Quantization of LLMs. (arXiv:2311.09550v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09550">http://arxiv.org/abs/2311.09550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09550]] A Speed Odyssey for Deployable Quantization of LLMs(http://arxiv.org/abs/2311.09550)</code></li>
<li>Summary: <p>The large language model era urges faster and less costly inference. Prior
model compression works on LLMs tend to undertake a software-centric approach
primarily focused on the simulated quantization performance. By neglecting the
feasibility of deployment, these approaches are typically disabled in real
practice. They used to drastically push down the quantization bit range for a
reduced computation which might not be supported by the mainstream hardware, or
involve sophisticated algorithms that introduce extra computation or memory
access overhead. We argue that pursuing a hardware-centric approach in the
construction of quantization algorithms is crucial. In this regard, we are
driven to build our compression method on top of hardware awareness,
eliminating impractical algorithm choices while maximizing the benefit of
hardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel
implementation called FastGEMM and a combined recipe of quantization
strategies. Extensive experiments manifest the superiority of our W4A8 method
which brings the actual speed boosting up to \textbf{4$\times$} compared to
Hugging Face FP16 inference and \textbf{2.23$\times$} vs. the state-of-the-art
inference engine TensorRT-LLM in FP16, and \textbf{1.45$\times$} vs.
TensorRT-LLM in INT8, yet without substantially harming the performance.
</p></li>
</ul>

<h3>Title: Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition. (arXiv:2311.09552v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09552">http://arxiv.org/abs/2311.09552</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09552]] Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition(http://arxiv.org/abs/2311.09552)</code></li>
<li>Summary: <p>Recognizing fallacies is crucial for ensuring the quality and validity of
arguments across various domains. However, computational fallacy recognition
faces challenges due to the diverse genres, domains, and types of fallacies
found in datasets. This leads to a highly multiclass, and even multi-label,
setup with substantial class imbalance. In this study, we aim to enhance
existing models for fallacy recognition by incorporating additional context and
by leveraging large language models to generate synthetic data, thus increasing
the representation of the infrequent classes. We experiment with GPT3.5 to
generate synthetic examples and we examine the impact of prompt settings for
this. Moreover, we explore zero-shot and few-shot scenarios to evaluate the
effectiveness of using the generated examples for training smaller models
within a unified fallacy recognition framework. Furthermore, we analyze the
overlap between the synthetic data and existing fallacy datasets. Finally, we
investigate the usefulness of providing supplementary context for detecting
fallacy types that need such context, e.g., diversion fallacies. Our evaluation
results demonstrate consistent improvements across fallacy types, datasets, and
generators.
</p></li>
</ul>

<h3>Title: LifeTox: Unveiling Implicit Toxicity in Life Advice. (arXiv:2311.09585v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09585">http://arxiv.org/abs/2311.09585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09585]] LifeTox: Unveiling Implicit Toxicity in Life Advice(http://arxiv.org/abs/2311.09585)</code></li>
<li>Summary: <p>As large language models become increasingly integrated into daily life,
detecting implicit toxicity across diverse contexts is crucial. To this end, we
introduce LifeTox, a dataset designed for identifying implicit toxicity within
a broad range of advice-seeking scenarios. Unlike existing safety datasets,
LifeTox comprises diverse contexts derived from personal experiences through
open-ended questions. Experiments demonstrate that RoBERTa fine-tuned on
LifeTox matches or surpasses the zero-shot performance of large language models
in toxicity classification tasks. These results underscore the efficacy of
LifeTox in addressing the complex challenges inherent in implicit toxicity.
</p></li>
</ul>

<h3>Title: Multi-Step Dialogue Workflow Action Prediction. (arXiv:2311.09593v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09593">http://arxiv.org/abs/2311.09593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09593]] Multi-Step Dialogue Workflow Action Prediction(http://arxiv.org/abs/2311.09593)</code></li>
<li>Summary: <p>In task-oriented dialogue, a system often needs to follow a sequence of
actions, called a workflow, that complies with a set of guidelines in order to
complete a task. In this paper, we propose the novel problem of multi-step
workflow action prediction, in which the system predicts multiple future
workflow actions. Accurate prediction of multiple steps allows for multi-turn
automation, which can free up time to focus on more complex tasks. We propose
three modeling approaches that are simple to implement yet lead to more action
automation: 1) fine-tuning on a training dataset, 2) few-shot in-context
learning leveraging retrieval and large language model prompting, and 3)
zero-shot graph traversal, which aggregates historical action sequences into a
graph for prediction. We show that multi-step action prediction produces
features that improve accuracy on downstream dialogue tasks like predicting
task success, and can increase automation of steps by 20% without requiring as
much feedback from a human overseeing the system.
</p></li>
</ul>

<h3>Title: Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion. (arXiv:2311.09602v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09602">http://arxiv.org/abs/2311.09602</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09602]] Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion(http://arxiv.org/abs/2311.09602)</code></li>
<li>Summary: <p>Situations and events evoke emotions in humans, but to what extent do they
inform the prediction of emotion detection models? Prior work in emotion
trigger or cause identification focused on training models to recognize events
that trigger an emotion. Instead, this work investigates how well
human-annotated emotion triggers correlate with features that models deemed
salient in their prediction of emotions. First, we introduce a novel dataset
EmoTrigger, consisting of 900 social media posts sourced from three different
datasets; these were annotated by experts for emotion triggers with high
agreement. Using EmoTrigger, we evaluate the ability of large language models
(LLMs) to identify emotion triggers, and conduct a comparative analysis of the
features considered important for these tasks between LLMs and fine-tuned
models. Our analysis reveals that emotion triggers are largely not considered
salient features for emotion prediction models, instead there is intricate
interplay between various features and the task of emotion detection.
</p></li>
</ul>

<h3>Title: GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks. (arXiv:2311.09606v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09606">http://arxiv.org/abs/2311.09606</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09606]] GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks(http://arxiv.org/abs/2311.09606)</code></li>
<li>Summary: <p>Large language models (LLMs) have the ability to perform in-context learning
(ICL) of new tasks by conditioning on prompts comprising a few task examples.
This work studies the problem of selecting the best examples given a candidate
pool to improve ICL performance on given a test input. Existing approaches
either require training with feedback from a much larger LLM or are
computationally expensive. We propose a novel metric, GistScore, based on
Example Gisting, a novel approach for training example retrievers for ICL using
an attention bottleneck via Gisting, a recent technique for compressing task
instructions. To tradeoff performance with ease of use, we experiment with both
fine-tuning gist models on each dataset and multi-task training a single model
on a large collection of datasets. On 21 diverse datasets spanning 9 tasks, we
show that our fine-tuned models get state-of-the-art ICL performance with 20%
absolute average gain over off-the-shelf retrievers and 7% over the best prior
methods. Our multi-task model generalizes well out-of-the-box to new task
categories, datasets, and prompt templates with retrieval speeds that are
consistently thousands of times faster than the best prior training-free
method.
</p></li>
</ul>

<h3>Title: Evaluating In-Context Learning of Libraries for Code Generation. (arXiv:2311.09635v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09635">http://arxiv.org/abs/2311.09635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09635]] Evaluating In-Context Learning of Libraries for Code Generation(http://arxiv.org/abs/2311.09635)</code></li>
<li>Summary: <p>Contemporary Large Language Models (LLMs) exhibit a high degree of code
generation and comprehension capability. A particularly promising area is their
ability to interpret code modules from unfamiliar libraries for solving
user-instructed tasks. Recent work has shown that large proprietary LLMs can
learn novel library usage in-context from demonstrations. These results raise
several open questions: whether demonstrations of library usage is required,
whether smaller (and more open) models also possess such capabilities, etc. In
this work, we take a broader approach by systematically evaluating a diverse
array of LLMs across three scenarios reflecting varying levels of domain
specialization to understand their abilities and limitations in generating code
based on libraries defined in-context. Our results show that even smaller
open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding
of novel code libraries based on specification presented in-context. Our
findings further reveal that LLMs exhibit a surprisingly high proficiency in
learning novel library modules even when provided with just natural language
descriptions or raw code implementations of the functions, which are often
cheaper to obtain than demonstrations. Overall, our results pave the way for
harnessing LLMs in more adaptable and dynamic coding environments.
</p></li>
</ul>

<h3>Title: Event Causality Is Key to Computational Story Understanding. (arXiv:2311.09648v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09648">http://arxiv.org/abs/2311.09648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09648]] Event Causality Is Key to Computational Story Understanding(http://arxiv.org/abs/2311.09648)</code></li>
<li>Summary: <p>Psychological research suggests the central role of event causality in human
story understanding. Further, event causality has been heavily utilized in
symbolic story generation. However, few machine learning systems for story
understanding employ event causality, partially due to the lack of reliable
methods for identifying open-world causal event relations. Leveraging recent
progress in large language models (LLMs), we present the first method for event
causality identification that leads to material improvements in computational
story understanding. We design specific prompts for extracting event causal
relations from GPT. Against human-annotated event causal relations in the
GLUCOSE dataset, our technique performs on par with supervised models, while
being easily generalizable to stories of different types and lengths. The
extracted causal relations lead to 5.7\% improvements on story quality
evaluation and 8.7\% on story video-text alignment. Our findings indicate
enormous untapped potential for event causality in computational story
understanding.
</p></li>
</ul>

<h3>Title: Structured Chemistry Reasoning with Large Language Models. (arXiv:2311.09656v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09656">http://arxiv.org/abs/2311.09656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09656]] Structured Chemistry Reasoning with Large Language Models(http://arxiv.org/abs/2311.09656)</code></li>
<li>Summary: <p>This paper studies the problem of solving complex chemistry problems with
large language models (LLMs). Despite the extensive general knowledge in LLMs
(such as GPT-4), they struggle with chemistry reasoning that requires faithful
grounded reasoning with diverse chemical knowledge and an integrative
understanding of chemical interactions. We propose InstructChem, a new
structured reasoning approach that substantially boosts the LLMs' chemical
reasoning capabilities. InstructChem explicitly decomposes the reasoning into
three critical phrases, including chemical formulae generation by LLMs that
offers the basis for subsequent grounded reasoning, step-by-step reasoning that
makes multi-step derivations with the identified formulae for a preliminary
answer, and iterative review-and-refinement that steers LLMs to progressively
revise the previous phases for increasing confidence, leading to the final
high-confidence answer. We conduct extensive experiments on four different
chemistry challenges, including quantum chemistry, quantum mechanics, physical
chemistry, and chemistry kinetics. Our approach significantly enhances GPT-4 on
chemistry reasoning, yielding an 8% average absolute improvement and a 30% peak
improvement. We further use the generated reasoning by GPT-4 to fine-tune
smaller LMs (e.g., Vicuna) and observe strong improvement of the smaller LMs.
This validates our approach and enables LLMs to generate high-quality
reasoning.
</p></li>
</ul>

<h3>Title: Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds. (arXiv:2311.09665v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09665">http://arxiv.org/abs/2311.09665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09665]] Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds(http://arxiv.org/abs/2311.09665)</code></li>
<li>Summary: <p>This study investigates the potential of Large Language Models (LLMs) to
simulate human group dynamics, particularly within politically charged
contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to
role-play as Democrat and Republican personas, engaging in a structured
interaction akin to human group study. Our approach evaluates how agents'
responses evolve through social influence. Our key findings indicate that LLM
agents role-playing detailed personas and without Chain-of-Thought (CoT)
reasoning closely align with human behaviors, while having CoT reasoning hurts
the alignment. However, incorporating explicit biases into agent prompts does
not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning
LLMs with human data shows promise in achieving human-like behavior but poses a
risk of overfitting certain behaviors. These findings show the potential and
limitations of using LLM agents in modeling human group phenomena.
</p></li>
</ul>

<h3>Title: R-Tuning: Teaching Large Language Models to Refuse Unknown Questions. (arXiv:2311.09677v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09677">http://arxiv.org/abs/2311.09677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09677]] R-Tuning: Teaching Large Language Models to Refuse Unknown Questions(http://arxiv.org/abs/2311.09677)</code></li>
<li>Summary: <p>Large language models (LLMs) have revolutionized numerous domains with their
impressive performance but still face their challenges. A predominant issue is
the propensity for these models to generate non-existent facts, a concern
termed hallucination. Our research is motivated by the observation that
previous instruction tuning methods force the model to complete a sentence no
matter whether the model knows the knowledge or not. When the question is out
of the parametric knowledge, it will try to make up something and fail to
indicate when it lacks knowledge. In this paper, we present a new approach
called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized
by first identifying the knowledge gap between parametric knowledge and the
instruction tuning data. Then, we construct the refusal-aware data based on the
knowledge intersection, to tune LLMs to refrain from responding to questions
beyond its parametric knowledge. Experimental results demonstrate this new
instruction tuning approach effectively improves a model's ability to answer
known questions and refrain from answering unknown questions. Furthermore, when
tested on out-of-domain datasets, the refusal ability was found to be a
meta-skill that could be generalized to other tasks. Further analysis
surprisingly finds that learning the uncertainty during training displays a
better ability to estimate uncertainty than uncertainty-based testing. Our code
will be released at https://github.com/shizhediao/R-Tuning.
</p></li>
</ul>

<h3>Title: MacGyver: Are Large Language Models Creative Problem Solvers?. (arXiv:2311.09682v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09682">http://arxiv.org/abs/2311.09682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09682]] MacGyver: Are Large Language Models Creative Problem Solvers?(http://arxiv.org/abs/2311.09682)</code></li>
<li>Summary: <p>We explore the creative problem-solving capabilities of modern large language
models (LLMs) in a constrained setting. The setting requires circumventing a
cognitive bias known in psychology as ''functional fixedness'' to use familiar
objects in innovative or unconventional ways. To this end, we create MacGyver,
an automatically generated dataset consisting of 1,600 real-world problems that
deliberately trigger functional fixedness and require thinking
'out-of-the-box'. We then present our collection of problems to both LLMs and
humans to compare and contrast their problem-solving abilities. We show that
MacGyver is challenging for both groups, but in unique and complementary ways.
For example, humans typically excel in solving problems that they are familiar
with but may struggle with tasks requiring domain-specific knowledge, leading
to a higher variance. On the other hand, LLMs, being exposed to a variety of
highly specialized knowledge, attempt broader problems but are prone to
overconfidence and propose actions that are physically infeasible or
inefficient. We also provide a detailed error analysis of LLMs, and demonstrate
the potential of enhancing their problem-solving ability with novel prompting
techniques such as iterative step-wise reflection and divergent-convergent
thinking. This work provides insight into the creative problem-solving
capabilities of humans and AI and illustrates how psychological paradigms can
be extended into large-scale tasks for comparing humans and machines.
</p></li>
</ul>

<h3>Title: Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation. (arXiv:2311.09684v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09684">http://arxiv.org/abs/2311.09684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09684]] Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation(http://arxiv.org/abs/2311.09684)</code></li>
<li>Summary: <p>This study examines the effect of prompt engineering on the performance of
Large Language Models (LLMs) in clinical note generation. We introduce an
Automatic Prompt Optimization (APO) framework to refine initial prompts and
compare the outputs of medical experts, non-medical experts, and APO-enhanced
GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in
standardizing prompt quality across clinical note sections. A human-in-the-loop
approach shows that experts maintain content quality post-APO, with a
preference for their own modifications, suggesting the value of expert
customization. We recommend a two-phase optimization process, leveraging
APO-GPT4 for consistency and expert input for personalization.
</p></li>
</ul>

<h3>Title: BLT: Can Large Language Models Handle Basic Legal Text?. (arXiv:2311.09693v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09693">http://arxiv.org/abs/2311.09693</a></li>
<li>Code URL: https://github.com/blairstanek/blt</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09693]] BLT: Can Large Language Models Handle Basic Legal Text?(http://arxiv.org/abs/2311.09693)</code></li>
<li>Summary: <p>We find that the best publicly available LLMs like GPT-4 and PaLM 2 currently
perform poorly at basic text handling required of lawyers or paralegals, such
as looking up the text at a line of a witness deposition or at a subsection of
a contract. We introduce a benchmark to quantify this poor performance, which
casts into doubt LLMs' current reliability as-is for legal practice. Finetuning
for these tasks brings an older LLM to near-perfect performance on our test set
and also raises performance on a related legal task. This stark result
highlights the need for more domain expertise in LLM training.
</p></li>
</ul>

<h3>Title: Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?. (arXiv:2311.09702v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09702">http://arxiv.org/abs/2311.09702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09702]] Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?(http://arxiv.org/abs/2311.09702)</code></li>
<li>Summary: <p>Despite the recent advancement in large language models (LLMs) and their high
performances across numerous benchmarks, recent research has unveiled that LLMs
suffer from hallucinations and unfaithful reasoning. This work studies a
specific type of hallucination induced by semantic associations. Specifically,
we investigate to what extent LLMs take shortcuts from certain keyword/entity
biases in the prompt instead of following the correct reasoning path. To
quantify this phenomenon, we propose a novel probing method and benchmark
called EureQA. We start from questions that LLMs will answer correctly with
utmost certainty, and mask the important entity with evidence sentence
recursively, asking models to find masked entities according to a chain of
evidence before answering the question.
</p>
<p>During the construction of the evidence, we purposefully replace semantic
clues (entities) that may lead to the correct answer with distractor clues
(evidence) that will not directly lead to the correct answer but require a
chain-like reasoning process. We evaluate if models can follow the correct
reasoning chain instead of short-cutting through distractor clues. We find that
existing LLMs lack the necessary capabilities to follow correct reasoning paths
and resist the attempt of greedy shortcuts. We show that the distractor
semantic associations often lead to model hallucination, which is strong
evidence that questions the validity of current LLM reasoning.
</p></li>
</ul>

<h3>Title: Large Language Model Inference with Lexical Shortlisting. (arXiv:2311.09709v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09709">http://arxiv.org/abs/2311.09709</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09709]] Large Language Model Inference with Lexical Shortlisting(http://arxiv.org/abs/2311.09709)</code></li>
<li>Summary: <p>Large language model (LLM) inference is computation and memory intensive, so
we adapt lexical shortlisting to it hoping to improve both. While lexical
shortlisting is well-explored in tasks like machine translation, it requires
modifications before being suitable for LLMs as the intended applications vary
significantly. Our work studies two heuristics to shortlist sub-vocabulary at
LLM inference time: Unicode-based script filtering and corpus-based selection.
We explore different LLM families and sizes, and we find that lexical
shortlisting can reduce the memory usage of some models by nearly 50\% and has
an upper bound of 25\% improvement in generation speed. In this pilot study, we
also identify the drawbacks of such vocabulary selection methods and propose
avenues for future research.
</p></li>
</ul>

<h3>Title: You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments. (arXiv:2311.09718v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09718">http://arxiv.org/abs/2311.09718</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09718]] You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments(http://arxiv.org/abs/2311.09718)</code></li>
<li>Summary: <p>The versatility of Large Language Models (LLMs) on natural language
understanding tasks has made them popular for research in social sciences. In
particular, to properly understand the properties and innate personas of LLMs,
researchers have performed studies that involve using prompts in the form of
questions that ask LLMs of particular opinions. In this study, we take a
cautionary step back and examine whether the current format of prompting
enables LLMs to provide responses in a consistent and robust manner. We first
construct a dataset that contains 693 questions encompassing 39 different
instruments of persona measurement on 115 persona axes. Additionally, we design
a set of prompts containing minor variations and examine LLM's capabilities to
generate accurate answers, as well as consistency variations to examine their
consistency towards simple perturbations such as switching the option order.
Our experiments on 15 different open-source LLMs reveal that even simple
perturbations are sufficient to significantly downgrade a model's
question-answering ability, and that most LLMs have low negation consistency.
Our results suggest that the currently widespread practice of prompting is
insufficient to accurately capture model perceptions, and we discuss potential
alternatives to improve such issues.
</p></li>
</ul>

<h3>Title: On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering. (arXiv:2311.09721v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09721">http://arxiv.org/abs/2311.09721</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09721]] On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering(http://arxiv.org/abs/2311.09721)</code></li>
<li>Summary: <p>This study introduces a new long-form database question answering dataset
designed to evaluate how Large Language Models (LLMs) interact with a SQL
interpreter. The task necessitates LLMs to strategically generate multiple SQL
queries to retrieve sufficient data from a database, to reason with the
acquired context, and to synthesize them into a comprehensive analytical
narrative. Our findings highlight that this task poses great challenges even
for the state-of-the-art GPT-4 model. We propose and evaluate two interaction
strategies, and provide a fine-grained analysis of the individual stages within
the interaction. A key discovery is the identification of two primary
bottlenecks hindering effective interaction: the capacity for planning and the
ability to generate multiple SQL queries. To address the challenge of
accurately assessing answer quality, we introduce a multi-agent evaluation
framework that simulates the academic peer-review process, enhancing the
precision and reliability of our evaluations. This framework allows for a more
nuanced understanding of the strengths and limitations of current LLMs in
complex retrieval and reasoning tasks.
</p></li>
</ul>

<h3>Title: Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks. (arXiv:2311.09730v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09730">http://arxiv.org/abs/2311.09730</a></li>
<li>Code URL: https://github.com/jiaxin-pei/llm-group-bias</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09730]] Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks(http://arxiv.org/abs/2311.09730)</code></li>
<li>Summary: <p>Human perception of language depends on personal backgrounds like gender and
ethnicity. While existing studies have shown that large language models (LLMs)
hold values that are closer to certain societal groups, it is unclear whether
their prediction behaviors on subjective NLP tasks also exhibit a similar bias.
In this study, leveraging the POPQUORN dataset which contains annotations of
diverse demographic backgrounds, we conduct a series of experiments on four
popular LLMs to investigate their capability to understand group differences
and potential biases in their predictions for politeness and offensiveness. We
find that for both tasks, model predictions are closer to the labels from White
and female participants. We further explore prompting with the target
demographic labels and show that including the target demographic in the prompt
actually worsens the model's performance. More specifically, when being
prompted to respond from the perspective of "Black" and "Asian" individuals,
models show lower performance in predicting both overall scores as well as the
scores from corresponding groups. Our results suggest that LLMs hold gender and
racial biases for subjective NLP tasks and that demographic-infused prompts
alone may be insufficient to mitigate such effects. Code and data are available
at https://github.com/Jiaxin-Pei/LLM-Group-Bias.
</p></li>
</ul>

<h3>Title: Prudent Silence or Foolish Babble? Examining Large Language Models' Responses to the Unknown. (arXiv:2311.09731v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09731">http://arxiv.org/abs/2311.09731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09731]] Prudent Silence or Foolish Babble? Examining Large Language Models' Responses to the Unknown(http://arxiv.org/abs/2311.09731)</code></li>
<li>Summary: <p>Large Language Models (LLMs) often struggle when faced with situations where
they lack the prerequisite knowledge to generate a sensical response. In these
cases, models tend to fabricate and hallucinate, rather than appropriately
signaling uncertainty as humans would. This behavior misaligns with human
conversational norms and presents challenges surrounding responsible and
ethical AI development. This work aims to systematically investigate LLMs'
behaviors in such situations. We curate an adversarial question-answering
benchmark containing unanswerable questions targeting information absent from
the LLM's training data. Concretely, these unanswerable questions contain
non-existent concepts or false premises. When presented with such unanswerable
questions, an LLM should appropriately convey uncertainty, and be able to
challenge the premise and refuse to generate a response. While facing
answerable valid questions, a model should demonstrate a positive correlation
between accuracy and confidence. Using a model-agnostic unified confidence
elicitation approach, we observe that LLMs that have gone through instruction
finetuning and reinforcement learning from human feedback (RLHF) perform
significantly better than their counterparts that do not. Moreover, uncertainty
expression 1 through our elicitation method does not always stay consistent
with the perceived confidence of the direct response of an LLM. Our findings
call for further research into teaching LLMs to proactively and reliably
express uncertainty.
</p></li>
</ul>

<h3>Title: How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?. (arXiv:2311.09755v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09755">http://arxiv.org/abs/2311.09755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09755]] How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?(http://arxiv.org/abs/2311.09755)</code></li>
<li>Summary: <p>Pruning and quantization form the foundation of model compression for neural
networks, enabling efficient inference for large language models (LLMs).
Recently, various quantization and pruning techniques have demonstrated
state-of-the-art performance in a post-training setting. They rely upon
calibration data, a small set of unlabeled examples, to generate layer
activations. However, no prior work has systematically investigated how the
calibration data impacts the effectiveness of model compression methods. In
this paper, we present the first extensive empirical study on the effect of
calibration data upon LLM performance. We trial a variety of pruning and
quantization methods, tasks, models, and datasets. Surprisingly, we find
substantial variations in downstream task performance, contrasting existing
work that suggests a greater level of robustness to the calibration data.
Finally, we make a series of recommendations for the effective use of
calibration data in LLM quantization and pruning.
</p></li>
</ul>

<h3>Title: Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models. (arXiv:2311.09762v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09762">http://arxiv.org/abs/2311.09762</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09762]] Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models(http://arxiv.org/abs/2311.09762)</code></li>
<li>Summary: <p>Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning
capabilities of Large Language Models (LLMs) by generating a series of
rationales before the final answer. We analyze the reasoning paths generated by
CoT and find two issues in multi-step reasoning: (i) Generating rationales
irrelevant to the question, (ii) Unable to compose subquestions or queries for
generating/retrieving all the relevant information. To address them, we propose
a graph-guided CoT prompting method, which guides the LLMs to reach the correct
answer with graph representation/verification steps. Specifically, we first
leverage LLMs to construct a "question/rationale graph" by using knowledge
extraction prompting given the initial question and the rationales generated in
the previous steps. Then, the graph verification step diagnoses the current
rationale triplet by comparing it with the existing question/rationale graph to
filter out irrelevant rationales and generate follow-up questions to obtain
relevant information. Additionally, we generate CoT paths that exclude the
extracted graph information to represent the context information missed from
the graph extraction. Our graph-guided reasoning method shows superior
performance compared to previous CoT prompting and the variants on multi-hop
question answering benchmark datasets.
</p></li>
</ul>

<h3>Title: Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations. (arXiv:2311.09763v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09763">http://arxiv.org/abs/2311.09763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09763]] Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations(http://arxiv.org/abs/2311.09763)</code></li>
<li>Summary: <p>Existing studies in backdoor defense have predominantly focused on the
training phase, overlooking the critical aspect of testing time defense. This
gap becomes particularly pronounced in the context of Large Language Models
(LLMs) deployed as Web Services, which typically offer only black-box access,
rendering training-time defenses impractical. To bridge this gap, our work
introduces defensive demonstrations, an innovative backdoor defense strategy
for blackbox large language models. Our method involves identifying the task
and retrieving task-relevant demonstrations from an uncontaminated pool. These
demonstrations are then combined with user queries and presented to the model
during testing, without requiring any modifications/tuning to the black-box
model or insights into its internal mechanisms. Defensive demonstrations are
designed to counteract the adverse effects of triggers, aiming to recalibrate
and correct the behavior of poisoned models during test-time evaluations.
Extensive experiments show that defensive demonstrations are effective in
defending both instance-level and instruction-level backdoor attacks, not only
rectifying the behavior of poisoned models but also surpassing existing
baselines in most scenarios.
</p></li>
</ul>

<h3>Title: To be or not to be? an exploration of continuously controllable prompt engineering. (arXiv:2311.09773v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09773">http://arxiv.org/abs/2311.09773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09773]] To be or not to be? an exploration of continuously controllable prompt engineering(http://arxiv.org/abs/2311.09773)</code></li>
<li>Summary: <p>As the use of large language models becomes more widespread, techniques like
parameter-efficient fine-tuning and other methods for controlled generation are
gaining traction for customizing models and managing their outputs. However,
the challenge of precisely controlling how prompts influence these models is an
area ripe for further investigation. In response, we introduce ControlPE
(Continuously Controllable Prompt Engineering). ControlPE enables finer
adjustments to prompt effects, complementing existing prompt engineering, and
effectively controls continuous targets. This approach harnesses the power of
LoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting,
enabling fine-tuned adjustments to the impact of prompts. Our methodology
involves generating specialized datasets for prompt distillation, incorporating
these prompts into the LoRA model, and carefully adjusting LoRA merging weight
to regulate the influence of prompts. This provides a dynamic and adaptable
tool for prompt control. Through our experiments, we have validated the
practicality and efficacy of ControlPE. It proves to be a promising solution
for control a variety of prompts, ranging from generating short responses
prompts, refusal prompts to chain-of-thought prompts.
</p></li>
</ul>

<h3>Title: Investigating Data Contamination in Modern Benchmarks for Large Language Models. (arXiv:2311.09783v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09783">http://arxiv.org/abs/2311.09783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09783]] Investigating Data Contamination in Modern Benchmarks for Large Language Models(http://arxiv.org/abs/2311.09783)</code></li>
<li>Summary: <p>Recent observations have underscored a disparity between the inflated
benchmark scores and the actual performance of LLMs, raising concerns about
potential contamination of evaluation benchmarks. This issue is especially
critical for closed-source models and certain open-source models where training
data transparency is lacking. In this paper we study data contamination by
proposing two methods tailored for both open-source and proprietary LLMs. We
first introduce a retrieval-based system to explore potential overlaps between
evaluation benchmarks and pretraining corpora. We further present a novel
investigation protocol named \textbf{T}estset \textbf{S}lot Guessing
(\textit{TS-Guessing}), applicable to both open and proprietary models. This
approach entails masking a wrong answer in a multiple-choice question and
prompting the model to fill in the gap. Additionally, it involves obscuring an
unlikely word in an evaluation example and asking the model to produce it. We
find that certain commercial LLMs could surprisingly guess the missing option
in various test sets. Specifically, in the TruthfulQA benchmark, we find that
LLMs exhibit notable performance improvement when provided with additional
metadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4
demonstrated an exact match rate of 52\% and 57\%, respectively, in guessing
the missing options in benchmark test data. We hope these results underscore
the need for more robust evaluation methodologies and benchmarks in the field.
</p></li>
</ul>

<h3>Title: Interpreting User Requests in the Context of Natural Language Standing Instructions. (arXiv:2311.09796v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09796">http://arxiv.org/abs/2311.09796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09796]] Interpreting User Requests in the Context of Natural Language Standing Instructions(http://arxiv.org/abs/2311.09796)</code></li>
<li>Summary: <p>Users of natural language interfaces, generally powered by Large Language
Models (LLMs),often must repeat their preferences each time they make a similar
request. To alleviate this, we propose including some of a user's preferences
and instructions in natural language -- collectively termed standing
instructions -- as additional context for such interfaces. For example, when a
user states I'm hungry, their previously expressed preference for Persian food
will be automatically added to the LLM prompt, so as to influence the search
for relevant restaurants. We develop NLSI, a language-to-program dataset
consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is
paired with a user profile (a set of users specific standing instructions) and
corresponding structured representations (API calls). A key challenge in NLSI
is to identify which subset of the standing instructions is applicable to a
given dialogue. NLSI contains diverse phenomena, from simple preferences to
interdependent instructions such as triggering a hotel search whenever the user
is booking tickets to an event. We conduct experiments on NLSI using prompting
with large language models and various retrieval approaches, achieving a
maximum of 44.7% exact match on API prediction. Our results demonstrate the
challenges in identifying the relevant standing instructions and their
interpretation into API calls.
</p></li>
</ul>

<h3>Title: How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!. (arXiv:2311.09799v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09799">http://arxiv.org/abs/2311.09799</a></li>
<li>Code URL: https://github.com/minnesotanlp/diversity-extraction-from-llms</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09799]] How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!(http://arxiv.org/abs/2311.09799)</code></li>
<li>Summary: <p>Collecting diverse human data on subjective NLP topics is costly and
challenging. As Large Language Models (LLMs) have developed human-like
capabilities, there is a recent trend in collaborative efforts between humans
and LLMs for generating diverse data, offering potential scalable and efficient
solutions. However, the extent of LLMs' capability to generate diverse
perspectives on subjective topics remains an unexplored question. In this
study, we investigate LLMs' capacity for generating diverse perspectives and
rationales on subjective topics, such as social norms and argumentative texts.
We formulate this problem as diversity extraction in LLMs and propose a
criteria-based prompting technique to ground diverse opinions and measure
perspective diversity from the generated criteria words. Our results show that
measuring semantic diversity through sentence embeddings and distance metrics
is not enough to measure perspective diversity. To see how far we can extract
diverse perspectives from LLMs, or called diversity coverage, we employ a
step-by-step recall prompting for generating more outputs from the model in an
iterative manner. As we apply our prompting method to other tasks (hate speech
labeling and story continuation), indeed we find that LLMs are able to generate
diverse opinions according to the degree of task subjectivity.
</p></li>
</ul>

<h3>Title: $\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning. (arXiv:2311.09800v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09800">http://arxiv.org/abs/2311.09800</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09800]] $\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning(http://arxiv.org/abs/2311.09800)</code></li>
<li>Summary: <p>Factuality is a crucial requirement in information seeking dialogue: the
system should respond to the user's queries so that the responses are
meaningful and aligned with the knowledge provided to the system. However, most
modern large language models suffer from hallucinations, that is, they generate
responses not supported by or contradicting the knowledge source. To mitigate
the issue and increase faithfulness of information-seeking dialogue systems, we
introduce BeInfo, a simple yet effective method that applies behavioural tuning
to aid information-seeking dialogue. Relying on three standard datasets, we
show that models tuned with BeInfo} become considerably more faithful to the
knowledge source both for datasets and domains seen during BeInfo-tuning, as
well as on unseen domains, when applied in a zero-shot manner. In addition, we
show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo
demonstrate strong performance on data from real `production' conversations and
outperform GPT4 when tuned on a limited amount of such realistic in-domain
dialogues.
</p></li>
</ul>

<h3>Title: The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text. (arXiv:2311.09807v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09807">http://arxiv.org/abs/2311.09807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09807]] The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text(http://arxiv.org/abs/2311.09807)</code></li>
<li>Summary: <p>This study investigates the consequences of training large language models
(LLMs) on synthetic data generated by their predecessors, an increasingly
prevalent practice aimed at addressing the limited supply of human-generated
training data. Diverging from the usual emphasis on performance metrics, we
focus on the impact of this training methodology on linguistic diversity,
especially when conducted recursively over time. To assess this, we developed a
set of novel metrics targeting lexical, syntactic, and semantic diversity,
applying them in recursive fine-tuning experiments across various natural
language generation tasks. Our findings reveal a marked decrease in the
diversity of the models' outputs through successive iterations. This trend
underscores the potential risks of training LLMs on predecessor-generated text,
particularly concerning the preservation of linguistic richness. Our study
highlights the need for careful consideration of the long-term effects of such
training approaches on the linguistic capabilities of LLMs.
</p></li>
</ul>

<h3>Title: Large Language Models for Propaganda Span Annotation. (arXiv:2311.09812v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09812">http://arxiv.org/abs/2311.09812</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09812]] Large Language Models for Propaganda Span Annotation(http://arxiv.org/abs/2311.09812)</code></li>
<li>Summary: <p>The use of propagandistic techniques in online communication has increased in
recent years, aiming to manipulate online audiences. Efforts to automatically
detect and debunk such content have been made, addressing various modeling
scenarios. These include determining whether the content (text, image, or
multimodal) (i) is propagandistic, (ii) employs one or more techniques, and
(iii) includes techniques with identifiable spans. Significant research efforts
have been devoted to the first two scenarios compared to the latter. Therefore,
in this study, we focus on the task of detecting propagandistic textual spans.
We investigate whether large language models such as GPT-4 can be utilized to
perform the task of an annotator. For the experiments, we used an in-house
developed dataset consisting of annotations from multiple annotators. Our
results suggest that providing more information to the model as prompts
improves the annotation agreement and performance compared to human
annotations. We plan to make the annotated labels from multiple annotators,
including GPT-4, available for the community.
</p></li>
</ul>

<h3>Title: SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models. (arXiv:2311.09818v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09818">http://arxiv.org/abs/2311.09818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09818]] SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models(http://arxiv.org/abs/2311.09818)</code></li>
<li>Summary: <p>Many knowledge sources consist of both structured information such as
relational databases as well as unstructured free text. Building a
conversational interface to such data sources is challenging.
</p>
<p>This paper introduces SUQL, Structured and Unstructured Query Language, the
first formal executable representation that naturally covers compositions of
structured and unstructured data queries. Specifically, it augments SQL with
several free-text primitives to form a precise, succinct, and expressive
representation. This paper also presents a conversational search agent based on
large language models, including a few-shot contextual semantic parser for
SUQL.
</p>
<p>To validate our approach, we introduce a dataset consisting of crowdsourced
questions and conversations about real restaurants. Over 51% of the questions
in the dataset require both structured and unstructured data, suggesting that
it is a common phenomenon. We show that our few-shot conversational agent based
on SUQL finds an entity satisfying all user requirements 89.3% of the time,
compared to just 65.0% for a strong and commonly used baseline.
</p></li>
</ul>

<h3>Title: Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks. (arXiv:2311.09825v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09825">http://arxiv.org/abs/2311.09825</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09825]] Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks(http://arxiv.org/abs/2311.09825)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated considerable advances, and
several claims have been made about their exceeding human performance. However,
in real-world tasks, domain knowledge is often required. Low-resource learning
methods like Active Learning (AL) have been proposed to tackle the cost of
domain expert annotation, raising this question: Can LLMs surpass compact
models trained with expert annotations in domain-specific tasks? In this work,
we conduct an empirical experiment on four datasets from three different
domains comparing SOTA LLMs with small models trained on expert annotations
with AL. We found that small models can outperform GPT-3.5 with a few hundreds
of labeled data, and they achieve higher or similar performance with GPT-4
despite that they are hundreds time smaller. Based on these findings, we posit
that LLM predictions can be used as a warmup method in real-world applications
and human experts remain indispensable in tasks involving data annotation
driven by domain-specific knowledge.
</p></li>
</ul>

<h3>Title: Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking. (arXiv:2311.09827v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09827">http://arxiv.org/abs/2311.09827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09827]] Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking(http://arxiv.org/abs/2311.09827)</code></li>
<li>Summary: <p>While large language models (LLMs) have demonstrated increasing power, they
have also given rise to a wide range of harmful behaviors. As representatives,
jailbreak attacks can provoke harmful or unethical responses from LLMs, even
after safety alignment. In this paper, we investigate a novel category of
jailbreak attacks specifically designed to target the cognitive structure and
processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in
the face of (1) multilingual cognitive overload, (2) veiled expression, and (3)
effect-to-cause reasoning. Different from previous jailbreak attacks, our
proposed cognitive overload is a black-box attack with no need for knowledge of
model architecture or access to model weights. Experiments conducted on
AdvBench and MasterKey reveal that various LLMs, including both popular
open-source model Llama 2 and the proprietary model ChatGPT, can be compromised
through cognitive overload. Motivated by cognitive psychology work on managing
cognitive load, we further investigate defending cognitive overload attack from
two perspectives. Empirical studies show that our cognitive overload from three
perspectives can jailbreak all studied LLMs successfully, while existing
defense strategies can hardly mitigate the caused malicious uses effectively.
</p></li>
</ul>

<h3>Title: FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models. (arXiv:2311.09829v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09829">http://arxiv.org/abs/2311.09829</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09829]] FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models(http://arxiv.org/abs/2311.09829)</code></li>
<li>Summary: <p>The effective assessment of the instruction-following ability of large
language models (LLMs) is of paramount importance. A model that cannot adhere
to human instructions might be not able to provide reliable and helpful
responses. In pursuit of this goal, various benchmarks have been constructed to
evaluate the instruction-following capacity of these models. However, these
benchmarks are limited to a single language and are constructed using automated
approaches, which restricts their applicability and the quality of the test
examples they contain. To bridge this gap, we introduce the FollowEval
benchmark in this paper. This benchmark is composed of instances in both
English and Chinese, and all test examples are crafted by human experts.
Furthermore, the FollowEval benchmark is designed to assess LLMs across five
critical dimensions of instruction following: string manipulation, commonsense
reasoning, logical reasoning, spatial reasoning, and response constraints. To
enhance the complexity and present a sufficient challenge, each test example is
designed to evaluate more than one dimension. We have evaluated various LLMs
using the FollowEval benchmark and found that their performance significantly
lags behind that of humans. This highlights the considerable room for
improvement in the instruction-following ability of these models.
</p></li>
</ul>

<h3>Title: ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks. (arXiv:2311.09835v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09835">http://arxiv.org/abs/2311.09835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09835]] ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks(http://arxiv.org/abs/2311.09835)</code></li>
<li>Summary: <p>Large language models have shown promising performance in code generation
benchmarks. However, a considerable divide exists between these benchmark
achievements and their practical applicability, primarily attributed to
real-world programming's reliance on pre-existing libraries. Instead of
evaluating LLMs to code from scratch, this work aims to propose a new
evaluation setup where LLMs use open-source libraries to finish machine
learning tasks. Therefore, we propose ML-Bench, an expansive benchmark
developed to assess the effectiveness of LLMs in leveraging existing functions
in open-source libraries. Consisting of 10044 samples spanning 130 tasks over
14 notable machine learning GitHub repositories. In this setting, given a
specific machine learning task instruction and the accompanying README in a
codebase, an LLM is tasked to generate code to accomplish the task. This
necessitates the comprehension of long and language-code interleaved documents,
as well as the understanding of complex cross-file code structures, introducing
new challenges. Notably, while GPT-4 exhibits remarkable improvement over other
LLMs, it manages to accomplish only 39.73\% of the tasks, leaving a huge space
for improvement. We address these challenges by proposing ML-Agent, designed to
effectively navigate the codebase, locate documentation, retrieve code, and
generate executable code. Empirical results demonstrate that ML-Agent, built
upon GPT-4, results in further improvements. Code, data, and models are
available at \url{https://ml-bench.github.io/}.
</p></li>
</ul>

<h3>Title: PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models. (arXiv:2311.09861v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09861">http://arxiv.org/abs/2311.09861</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09861]] PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models(http://arxiv.org/abs/2311.09861)</code></li>
<li>Summary: <p>As Large Language Models (LLMs) are becoming prevalent in various fields,
there is an urgent need for improved NLP benchmarks that encompass all the
necessary knowledge of individual discipline. Many contemporary benchmarks for
foundational models emphasize a broad range of subjects but often fall short in
presenting all the critical subjects and encompassing necessary professional
knowledge of them. This shortfall has led to skewed results, given that LLMs
exhibit varying performance across different subjects and knowledge areas. To
address this issue, we present psybench, the first comprehensive Chinese
evaluation suite that covers all the necessary knowledge required for graduate
entrance exams. psybench offers a deep evaluation of a model's strengths and
weaknesses in psychology through multiple-choice questions. Our findings show
significant differences in performance across different sections of a subject,
highlighting the risk of skewed results when the knowledge in test sets is not
balanced. Notably, only the ChatGPT model reaches an average accuracy above
$70\%$, indicating that there is still plenty of room for improvement. We
expect that psybench will help to conduct thorough evaluations of base models'
strengths and weaknesses and assist in practical application in the field of
psychology.
</p></li>
</ul>

<h3>Title: Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models. (arXiv:2311.09862v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09862">http://arxiv.org/abs/2311.09862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09862]] Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models(http://arxiv.org/abs/2311.09862)</code></li>
<li>Summary: <p>Large language models (LLMs) are revolutionizing various fields by leveraging
large text corpora for context-aware intelligence. Due to the context size,
however, encoding an entire graph with LLMs is fundamentally limited. This
paper explores how to better integrate graph data with LLMs and presents a
novel approach using various encoding modalities (e.g., text, image, and motif)
and approximation of global connectivity of a graph using different prompting
methods to enhance LLMs' effectiveness in handling complex graph structures.
The study also introduces GraphTMI, a new benchmark for evaluating LLMs in
graph structure analysis, focusing on factors such as homophily, motif
presence, and graph difficulty. Key findings reveal that image modality,
supported by advanced vision-language models like GPT-4V, is more effective
than text in managing token limits while retaining critical information. The
research also examines the influence of different factors on each encoding
modality's performance. This study highlights the current limitations and
charts future directions for LLMs in graph understanding and reasoning tasks.
</p></li>
</ul>

<h3>Title: Hijacking Large Language Models via Adversarial In-Context Learning. (arXiv:2311.09948v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09948">http://arxiv.org/abs/2311.09948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09948]] Hijacking Large Language Models via Adversarial In-Context Learning(http://arxiv.org/abs/2311.09948)</code></li>
<li>Summary: <p>In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs
for specific tasks by utilizing labeled examples as demonstrations in the
precondition prompts. Despite its promising performance, ICL suffers from
instability with the choice and arrangement of examples. Additionally, crafted
adversarial attacks pose a notable threat to the robustness of ICL. However,
existing attacks are either easy to detect, rely on external models, or lack
specificity towards ICL. To address these issues, this work introduces a novel
transferable attack for ICL, aiming to hijack LLMs to generate the targeted
response. The proposed LLM hijacking attack leverages a gradient-based prompt
search method to learn and append imperceptible adversarial suffixes to the
in-context demonstrations. Extensive experimental results on various tasks and
datasets demonstrate the effectiveness of our LLM hijacking attack, resulting
in a distracted attention towards adversarial tokens, consequently leading to
the targeted unwanted outputs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Center Focusing Network for Real-Time LiDAR Panoptic Segmentation. (arXiv:2311.09499v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09499">http://arxiv.org/abs/2311.09499</a></li>
<li>Code URL: https://github.com/gangzhang842/cfnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09499]] Center Focusing Network for Real-Time LiDAR Panoptic Segmentation(http://arxiv.org/abs/2311.09499)</code></li>
<li>Summary: <p>LiDAR panoptic segmentation facilitates an autonomous vehicle to
comprehensively understand the surrounding objects and scenes and is required
to run in real time. The recent proposal-free methods accelerate the algorithm,
but their effectiveness and efficiency are still limited owing to the
difficulty of modeling non-existent instance centers and the costly
center-based clustering modules. To achieve accurate and real-time LiDAR
panoptic segmentation, a novel center focusing network (CFNet) is introduced.
Specifically, the center focusing feature encoding (CFFE) is proposed to
explicitly understand the relationships between the original LiDAR points and
virtual instance centers by shifting the LiDAR points and filling in the center
points. Moreover, to leverage the redundantly detected centers, a fast center
deduplication module (CDM) is proposed to select only one center for each
instance. Experiments on the SemanticKITTI and nuScenes panoptic segmentation
benchmarks demonstrate that our CFNet outperforms all existing methods by a
large margin and is 1.6 times faster than the most efficient method. The code
is available at https://github.com/GangZhang842/CFNet.
</p></li>
</ul>

<h3>Title: Comprehensive Evaluation and Insights into the Use of Deep Neural Networks to Detect and Quantify Lymphoma Lesions in PET/CT Images. (arXiv:2311.09614v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09614">http://arxiv.org/abs/2311.09614</a></li>
<li>Code URL: https://github.com/microsoft/lymphoma-segmentation-dnn</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09614]] Comprehensive Evaluation and Insights into the Use of Deep Neural Networks to Detect and Quantify Lymphoma Lesions in PET/CT Images(http://arxiv.org/abs/2311.09614)</code></li>
<li>Summary: <p>This study performs comprehensive evaluation of four neural network
architectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion
segmentation from PET/CT images. These networks were trained, validated, and
tested on a diverse, multi-institutional dataset of 611 cases. Internal testing
(88 cases; total metabolic tumor volume (TMTV) range [0.52, 2300] ml) showed
SegResNet as the top performer with a median Dice similarity coefficient (DSC)
of 0.76 and median false positive volume (FPV) of 4.55 ml; all networks had a
median false negative volume (FNV) of 0 ml. On the unseen external test set
(145 cases with TMTV range: [0.10, 2480] ml), SegResNet achieved the best
median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml.
We assessed reproducibility of six lesion measures, calculated their prediction
errors, and examined DSC performance in relation to these lesion measures,
offering insights into segmentation accuracy and clinical relevance.
Additionally, we introduced three lesion detection criteria, addressing the
clinical need for identifying lesions, counting them, and segmenting based on
metabolic characteristics. We also performed expert intra-observer variability
analysis revealing the challenges in segmenting ``easy'' vs. ``hard'' cases, to
assist in the development of more resilient segmentation algorithms. Finally,
we performed inter-observer agreement assessment underscoring the importance of
a standardized ground truth segmentation protocol involving multiple expert
annotators. Code is available at:
https://github.com/microsoft/lymphoma-segmentation-dnn
</p></li>
</ul>

<h3>Title: Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality MRI Segmentation. (arXiv:2311.09737v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09737">http://arxiv.org/abs/2311.09737</a></li>
<li>Code URL: https://github.com/cuttle-fish-my/gm-guided-dg</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09737]] Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality MRI Segmentation(http://arxiv.org/abs/2311.09737)</code></li>
<li>Summary: <p>Cross-modal MRI segmentation is of great value for computer-aided medical
diagnosis, enabling flexible data acquisition and model generalization.
However, most existing methods have difficulty in handling local variations in
domain shift and typically require a significant amount of data for training,
which hinders their usage in practice. To address these problems, we propose a
novel adaptive domain generalization framework, which integrates a
learning-free cross-domain representation based on image gradient maps and a
class prior-informed test-time adaptation strategy for mitigating local domain
shift. We validate our approach on two multi-modal MRI datasets with six
cross-modal segmentation tasks. Across all the task settings, our method
consistently outperforms competing approaches and shows a stable performance
even with limited training data.
</p></li>
</ul>

<h3>Title: PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments. (arXiv:2311.09819v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09819">http://arxiv.org/abs/2311.09819</a></li>
<li>Code URL: https://github.com/seanxuu/PWISeg</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09819]] PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments(http://arxiv.org/abs/2311.09819)</code></li>
<li>Summary: <p>In surgical procedures, correct instrument counting is essential. Instance
segmentation is a location method that locates not only an object's bounding
box but also each pixel's specific details. However, obtaining mask-level
annotations is labor-intensive in instance segmentation. To address this issue,
we propose a novel yet effective weakly-supervised surgical instrument instance
segmentation approach, named Point-based Weakly-supervised Instance
Segmentation (PWISeg). PWISeg adopts an FCN-based architecture with
point-to-box and point-to-mask branches to model the relationships between
feature points and bounding boxes, as well as feature points and segmentation
masks on FPN, accomplishing instrument detection and segmentation jointly in a
single model. Since mask level annotations are hard to available in the real
world, for point-to-mask training, we introduce an unsupervised projection
loss, utilizing the projected relation between predicted masks and bboxes as
supervision signal. On the other hand, we annotate a few pixels as the key
pixel for each instrument. Based on this, we further propose a key pixel
association loss and a key pixel distribution loss, driving the point-to-mask
branch to generate more accurate segmentation predictions. To comprehensively
evaluate this task, we unveil a novel surgical instrument dataset with manual
annotations, setting up a benchmark for further research. Our comprehensive
research trial validated the superior performance of our PWISeg. The results
show that the accuracy of surgical instrument segmentation is improved,
surpassing most methods of instance segmentation via weakly supervised bounding
boxes. This improvement is consistently observed in our proposed dataset and
when applied to the public HOSPI-Tools dataset.
</p></li>
</ul>

<h3>Title: Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model. (arXiv:2311.09847v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09847">http://arxiv.org/abs/2311.09847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09847]] Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model(http://arxiv.org/abs/2311.09847)</code></li>
<li>Summary: <p>Foundational models, pretrained on a large scale, have demonstrated
substantial success across non-medical domains. However, training these models
typically requires large, comprehensive datasets, which contrasts with the
smaller and more heterogeneous datasets common in biomedical imaging. Here, we
propose a multi-task learning strategy that decouples the number of training
tasks from memory requirements. We trained a Universal bioMedical PreTrained
model (UMedPT) on a multi-task database including tomographic, microscopic, and
X-ray images, with various labelling strategies such as classification,
segmentation, and object detection. The UMedPT foundational model outperformed
ImageNet pretraining and the previous state-of-the-art models. For tasks
related to the pretraining database, it maintained its performance with only 1%
of the original training data and without fine-tuning. For out-of-domain tasks
it required not more than 50% of the original training data. In an external
independent validation imaging features extracted using UMedPT proved to be a
new standard for cross-center transferability.
</p></li>
</ul>

<h3>Title: Spoken Word2Vec: A Perspective And Some Techniques. (arXiv:2311.09319v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09319">http://arxiv.org/abs/2311.09319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09319]] Spoken Word2Vec: A Perspective And Some Techniques(http://arxiv.org/abs/2311.09319)</code></li>
<li>Summary: <p>Text word embeddings that encode distributional semantic features work by
modeling contextual similarities of frequently occurring words. Acoustic word
embeddings, on the other hand, typically encode low-level phonetic
similarities. Semantic embeddings for spoken words have been previously
explored using similar algorithms to Word2Vec, but the resulting vectors still
mainly encoded phonetic rather than semantic features. In this paper, we
examine the assumptions and architectures used in previous works and show
experimentally how Word2Vec algorithms fail to encode distributional semantics
when the input units are acoustically correlated. In addition, previous works
relied on the simplifying assumptions of perfect word segmentation and
clustering by word type. Given these conditions, a trivial solution identical
to text-based embeddings has been overlooked. We follow this simpler path using
automatic word type clustering and examine the effects on the resulting
embeddings, highlighting the true challenges in this task.
</p></li>
</ul>

<h3>Title: Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based Human Activity Recognition in Smart Homes. (arXiv:2311.09514v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09514">http://arxiv.org/abs/2311.09514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09514]] Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based Human Activity Recognition in Smart Homes(http://arxiv.org/abs/2311.09514)</code></li>
<li>Summary: <p>There has been a resurgence of applications focused on Human Activity
Recognition (HAR) in smart homes, especially in the field of ambient
intelligence and assisted living technologies. However, such applications
present numerous significant challenges to any automated analysis system
operating in the real world, such as variability, sparsity, and noise in sensor
measurements. Although state-of-the-art HAR systems have made considerable
strides in addressing some of these challenges, they especially suffer from a
practical limitation: they require successful pre-segmentation of continuous
sensor data streams before automated recognition, i.e., they assume that an
oracle is present during deployment, which is capable of identifying time
windows of interest across discrete sensor events. To overcome this limitation,
we propose a novel graph-guided neural network approach that performs activity
recognition by learning explicit co-firing relationships between sensors. We
accomplish this by learning a more expressive graph structure representing the
sensor network in a smart home, in a data-driven manner. Our approach maps
discrete input sensor measurements to a feature space through the application
of attention mechanisms and hierarchical pooling of node embeddings. We
demonstrate the effectiveness of our proposed approach by conducting several
experiments on CASAS datasets, showing that the resulting graph-guided neural
network outperforms the state-of-the-art method for HAR in smart homes across
multiple datasets and by large margins. These results are promising because
they push HAR for smart homes closer to real-world applications.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
