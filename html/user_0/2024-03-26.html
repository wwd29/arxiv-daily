<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-26</h1>
<h3>Title: A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data  Overlap Estimation in the Eduation Context</h3>
<ul>
<li><strong>Authors: </strong>Zhangquan Chen, Chunjiang Liu, Haobin Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15426">https://arxiv.org/abs/2403.15426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15426">https://arxiv.org/pdf/2403.15426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15426]] A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data  Overlap Estimation in the Eduation Context(https://arxiv.org/abs/2403.15426)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an end-to-end prior-based three-phases supervised fine-tuned model, which is proved more competitive than traditional fine-tuning method. More specifically, our model realizes the structural disassembly and incremental guided output of educational knowledge. To this end, we robustify data classification of three types via a sampler and overlap estimation neural network, and inject the preprocessing datasets into pre-trained model in three batches for LORA fine-tuning. Then, we design a prior module couples system prompt, vector databases, and abstract syntax tree task segmentation. Finally, the compression method and regularization constraint are applied to the prior-based fine-tuned model, followed by text filter at the output end to obtain incremental guided results. Our model represents the first research effort to truly embody the tutor role with the features of abundant educational knowledge, step-by-step incremental guided outputs and non-disclosure of answers. Extensive experiments report that our model also achieves state-of-the-art in code abilities compared to open-source models, reaching an impressive 75.10% on the HumanEval (@pass 1) benchmark. Additionally, our model maintains strong conversational capabilities, with the 13B quantized version achieving scores of 56.34, 50.60, and 45.27 respectively on the MMLU, C-Eval, and AGIEval (5 shot) dialogue evaluation benchmarks.</li>
</ul>

<h3>Title: Distilling Named Entity Recognition Models for Endangered Species from  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jesse Atuhurra, Seiveright Cargill Dujohn, Hidetaka Kamigaito, Hiroyuki Shindo, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15430">https://arxiv.org/abs/2403.15430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15430">https://arxiv.org/pdf/2403.15430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15430]] Distilling Named Entity Recognition Models for Endangered Species from  Large Language Models(https://arxiv.org/abs/2403.15430)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Natural language processing (NLP) practitioners are leveraging large language models (LLM) to create structured datasets from semi-structured and unstructured data sources such as patents, papers, and theses, without having domain-specific knowledge. At the same time, ecological experts are searching for a variety of means to preserve biodiversity. To contribute to these efforts, we focused on endangered species and through in-context learning, we distilled knowledge from GPT-4. In effect, we created datasets for both named entity recognition (NER) and relation extraction (RE) via a two-stage process: 1) we generated synthetic data from GPT-4 of four classes of endangered species, 2) humans verified the factual accuracy of the synthetic data, resulting in gold data. Eventually, our novel dataset contains a total of 3.6K sentences, evenly divided between 1.8K NER and 1.8K RE sentences. The constructed dataset was then used to fine-tune both general BERT and domain-specific BERT variants, completing the knowledge distillation process from GPT-4 to BERT, because GPT-4 is resource intensive. Experiments show that our knowledge transfer approach is effective at creating a NER model suitable for detecting endangered species from texts.</li>
</ul>

<h3>Title: Using Contextual Information for Sentence-level Morpheme Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Prabin Bhandari, Abhishek Paudel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15436">https://arxiv.org/abs/2403.15436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15436">https://arxiv.org/pdf/2403.15436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15436]] Using Contextual Information for Sentence-level Morpheme Segmentation(https://arxiv.org/abs/2403.15436)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements in morpheme segmentation primarily emphasize word-level segmentation, often neglecting the contextual relevance within the sentence. In this study, we redefine the morpheme segmentation task as a sequence-to-sequence problem, treating the entire sentence as input rather than isolating individual words. Our findings reveal that the multilingual model consistently exhibits superior performance compared to monolingual counterparts. While our model did not surpass the performance of the current state-of-the-art, it demonstrated comparable efficacy with high-resource languages while revealing limitations in low-resource language scenarios.</li>
</ul>

<h3>Title: Federated Learning based on Pruning and Recovery</h3>
<ul>
<li><strong>Authors: </strong>Chengjie Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15439">https://arxiv.org/abs/2403.15439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15439">https://arxiv.org/pdf/2403.15439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15439]] Federated Learning based on Pruning and Recovery(https://arxiv.org/abs/2403.15439)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>A novel federated learning training framework for heterogeneous environments is presented, taking into account the diverse network speeds of clients in realistic settings. This framework integrates asynchronous learning algorithms and pruning techniques, effectively addressing the inefficiencies of traditional federated learning algorithms in scenarios involving heterogeneous devices, as well as tackling the staleness issue and inadequate training of certain clients in asynchronous algorithms. Through the incremental restoration of model size during training, the framework expedites model training while preserving model accuracy. Furthermore, enhancements to the federated learning aggregation process are introduced, incorporating a buffering mechanism to enable asynchronous federated learning to operate akin to synchronous learning. Additionally, optimizations in the process of the server transmitting the global model to clients reduce communication overhead. Our experiments across various datasets demonstrate that: (i) significant reductions in training time and improvements in convergence accuracy are achieved compared to conventional asynchronous FL and HeteroFL; (ii) the advantages of our approach are more pronounced in scenarios with heterogeneous clients and non-IID client data.</li>
</ul>

<h3>Title: Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient  LLMs Under Compression</h3>
<ul>
<li><strong>Authors: </strong>Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian Bartoldson, Ajay Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15447">https://arxiv.org/abs/2403.15447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15447">https://arxiv.org/pdf/2403.15447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15447]] Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient  LLMs Under Compression(https://arxiv.org/abs/2403.15447)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% sparsity. Moreover, employing quantization within a moderate bit range could unexpectedly improve certain trustworthiness dimensions such as ethics and fairness. Conversely, extreme quantization to very low bit levels (3 bits) tends to significantly reduce trustworthiness. This increased risk cannot be uncovered by looking at benign performance alone, in turn, mandating comprehensive trustworthiness evaluation in practice. These findings culminate in practical recommendations for simultaneously achieving high utility, efficiency, and trustworthiness in LLMs. Models and code are available at https://decoding-comp-trust.github.io/.</li>
</ul>

<h3>Title: Hatred Stems from Ignorance! Distillation of the Persuasion Modes in  Countering Conversational Hate Speech</h3>
<ul>
<li><strong>Authors: </strong>Ghadi Alyahya, Abeer Aldayel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15449">https://arxiv.org/abs/2403.15449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15449">https://arxiv.org/pdf/2403.15449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15449]] Hatred Stems from Ignorance! Distillation of the Persuasion Modes in  Countering Conversational Hate Speech(https://arxiv.org/abs/2403.15449)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Examining the factors that the counter-speech uses is at the core of understanding the optimal methods for confronting hate speech online. Various studies assess the emotional base factor used in counter speech, such as emotion-empathy, offensiveness, and level of hostility. To better understand the counter-speech used in conversational interactions, this study distills persuasion modes into reason, emotion, and credibility and then evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) conversation interactions concerning racism, sexism, and religion. The evaluation covers the distinct behaviors of human versus generated counter-speech. We also assess the interplay between the replies' stance and each mode of persuasion in the counter-speech. Notably, we observe nuanced differences in the counter-speech persuasion modes for open and closed interactions -- especially on the topic level -- with a general tendency to use reason as a persuasion mode to express the counterpoint to hate comments. The generated counter-speech tends to exhibit an emotional persuasion mode, while human counters lean towards using reasoning. Furthermore, our study shows that reason as a persuasion mode tends to obtain more supportive replies than do other persuasion types. The findings highlight the potential of incorporating persuasion modes into studies about countering hate speech, as these modes can serve as an optimal means of explainability and paves the way for the further adoption of the reply's stance and the role it plays in assessing what comprises the optimal counter-speech.</li>
</ul>

<h3>Title: Loops On Retrieval Augmented Generation (LoRAG)</h3>
<ul>
<li><strong>Authors: </strong>Ayush Thakur, Rashmi Vashisth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15450">https://arxiv.org/abs/2403.15450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15450">https://arxiv.org/pdf/2403.15450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15450]] Loops On Retrieval Augmented Generation (LoRAG)(https://arxiv.org/abs/2403.15450)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents Loops On Retrieval Augmented Generation (LoRAG), a new framework designed to enhance the quality of retrieval-augmented text generation through the incorporation of an iterative loop mechanism. The architecture integrates a generative model, a retrieval mechanism, and a dynamic loop module, allowing for iterative refinement of the generated text through interactions with relevant information retrieved from the input context. Experimental evaluations on benchmark datasets demonstrate that LoRAG surpasses existing state-of-the-art models in terms of BLEU score, ROUGE score, and perplexity, showcasing its effectiveness in achieving both coherence and relevance in generated text. The qualitative assessment further illustrates LoRAG's capability to produce contextually rich and coherent outputs. This research contributes valuable insights into the potential of iterative loops in mitigating challenges in text generation, positioning LoRAG as a promising advancement in the field.</li>
</ul>

<h3>Title: Towards Enabling FAIR Dataspaces Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Benedikt T. Arnold, Johannes Theissen-Lipp, Diego Collarana, Christoph Lange, Sandra Geisler, Edward Curry, Stefan Decker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15451">https://arxiv.org/abs/2403.15451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15451">https://arxiv.org/pdf/2403.15451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15451]] Towards Enabling FAIR Dataspaces Using Large Language Models(https://arxiv.org/abs/2403.15451)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Dataspaces have recently gained adoption across various sectors, including traditionally less digitized domains such as culture. Leveraging Semantic Web technologies helps to make dataspaces FAIR, but their complexity poses a significant challenge to the adoption of dataspaces and increases their cost. The advent of Large Language Models (LLMs) raises the question of how these models can support the adoption of FAIR dataspaces. In this work, we demonstrate the potential of LLMs in dataspaces with a concrete example. We also derive a research agenda for exploring this emerging field.</li>
</ul>

<h3>Title: Span-Oriented Information Extraction -- A Unifying Perspective on  Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yifan Ding, Michael Yankoski, Tim Weninger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15453">https://arxiv.org/abs/2403.15453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15453">https://arxiv.org/pdf/2403.15453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15453]] Span-Oriented Information Extraction -- A Unifying Perspective on  Information Extraction(https://arxiv.org/abs/2403.15453)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Information Extraction refers to a collection of tasks within Natural Language Processing (NLP) that identifies sub-sequences within text and their labels. These tasks have been used for many years to link extract relevant information and to link free text to structured data. However, the heterogeneity among information extraction tasks impedes progress in this area. We therefore offer a unifying perspective centered on what we define to be spans in text. We then re-orient these seemingly incongruous tasks into this unified perspective and then re-present the wide assortment of information extraction tasks as variants of the same basic Span-Oriented Information Extraction task.</li>
</ul>

<h3>Title: Emotion Detection with Transformers: A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Rezapour</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15454">https://arxiv.org/abs/2403.15454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15454">https://arxiv.org/pdf/2403.15454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15454]] Emotion Detection with Transformers: A Comparative Study(https://arxiv.org/abs/2403.15454)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.</li>
</ul>

<h3>Title: LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach  Combining Predictive Agent Reasoning and Critical Agent Instruction</h3>
<ul>
<li><strong>Authors: </strong>Hejie Cui, Zhuocheng Shen, Jieyu Zhang, Hui Shao, Lianhui Qin, Joyce C. Ho, Carl Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15464">https://arxiv.org/abs/2403.15464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15464">https://arxiv.org/pdf/2403.15464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15464]] LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach  Combining Predictive Agent Reasoning and Critical Agent Instruction(https://arxiv.org/abs/2403.15464)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying Large Language Models (LLMs) to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies. Furthermore, we propose a novel approach that utilizes LLM agents with different roles: a predictor agent that makes predictions and generates reasoning processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the reasoning of the predictor agent. Our results demonstrate that with the proposed approach, LLMs can achieve decent few-shot performance compared to traditional supervised learning methods in EHR-based disease predictions, suggesting its potential for health-oriented applications.</li>
</ul>

<h3>Title: Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and  Markov Chains, by Using Rollout Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Yuchao Li, Dimitri Bertsekas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15465">https://arxiv.org/abs/2403.15465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15465">https://arxiv.org/pdf/2403.15465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15465]] Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and  Markov Chains, by Using Rollout Algorithms(https://arxiv.org/abs/2403.15465)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper we consider a transformer with an $n$-gram structure, such as the one underlying ChatGPT. The transformer provides next word probabilities, which can be used to generate word sequences. We consider methods for computing word sequences that are highly likely, based on these probabilities. Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we propose methods to compute highly likely sequences of $N$ words in time that is a low order polynomial in $N$ and in the vocabulary size of the $n$-gram. These methods are based on the rollout approach from approximate dynamic programming, a form of single policy iteration, which can improve the performance of any given heuristic policy. In our case we use a greedy heuristic that generates as next word one that has the highest probability. We show with analysis, examples, and computational experimentation that our methods are capable of generating highly likely sequences with a modest increase in computation over the greedy heuristic. While our analysis and experiments are focused on Markov chains of the type arising in transformer and ChatGPT-like models, our methods apply to general finite-state Markov chains, and related inference applications of Hidden Markov Models (HMM), where Viterbi decoding is used extensively.</li>
</ul>

<h3>Title: Using Super-Resolution Imaging for Recognition of Low-Resolution Blurred  License Plates: A Comparative Study of Real-ESRGAN, A-ESRGAN, and StarSRGAN</h3>
<ul>
<li><strong>Authors: </strong>Ching-Hsiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15466">https://arxiv.org/abs/2403.15466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15466">https://arxiv.org/pdf/2403.15466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15466]] Using Super-Resolution Imaging for Recognition of Low-Resolution Blurred  License Plates: A Comparative Study of Real-ESRGAN, A-ESRGAN, and StarSRGAN(https://arxiv.org/abs/2403.15466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the robust development of technology, license plate recognition technology can now be properly applied in various scenarios, such as road monitoring, tracking of stolen vehicles, detection at parking lot entrances and exits, and so on. However, the precondition for these applications to function normally is that the license plate must be 'clear' enough to be recognized by the system with the correct license plate number. If the license plate becomes blurred due to some external factors, then the accuracy of recognition will be greatly reduced. Although there are many road surveillance cameras in Taiwan, the quality of most cameras is not good, often leading to the inability to recognize license plate numbers due to low photo resolution. Therefore, this study focuses on using super-resolution technology to process blurred license plates. This study will mainly fine-tune three super-resolution models: Real-ESRGAN, A-ESRGAN, and StarSRGAN, and compare their effectiveness in enhancing the resolution of license plate photos and enabling accurate license plate recognition. By comparing different super-resolution models, it is hoped to find the most suitable model for this task, providing valuable references for future researchers.</li>
</ul>

<h3>Title: Don't be a Fool: Pooling Strategies in Offensive Language Detection from  User-Intended Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Seunguk Yu, Juhwan Choi, Youngbin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15467">https://arxiv.org/abs/2403.15467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15467">https://arxiv.org/pdf/2403.15467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15467]] Don't be a Fool: Pooling Strategies in Offensive Language Detection from  User-Intended Adversarial Attacks(https://arxiv.org/abs/2403.15467)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, to models pre-trained on noisy texts by employing these pooling strategies.</li>
</ul>

<h3>Title: Vi-Mistral-X: Building a Vietnamese Language Model with Advanced  Continual Pre-training</h3>
<ul>
<li><strong>Authors: </strong>James Vo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15470">https://arxiv.org/abs/2403.15470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15470">https://arxiv.org/pdf/2403.15470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15470]] Vi-Mistral-X: Building a Vietnamese Language Model with Advanced  Continual Pre-training(https://arxiv.org/abs/2403.15470)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advancement of Large Language Models (LLMs) has significantly transformed the field of natural language processing, although the focus on English-centric models has created a noticeable research gap for specific languages, including Vietnamese. To address this issue, this paper presents vi-mistral-x, an innovative Large Language Model designed expressly for the Vietnamese language. It utilizes a unique method of continual pre-training, based on the Mistral architecture, which incorporates grouped-query attention and sliding window attention techniques. This model, vi-Mistral-X, marks a significant step forward in improving the understanding and generation of the Vietnamese language. It introduces an additional phase of continual pre-training, specifically adapted for Vietnamese, enhancing the model's capability in understanding complex language nuances and generating accurate, context-aware Vietnamese text. Through comprehensive testing on various benchmarks, vi-mistral-x has shown to outperform existing Vietnamese LLMs in several key areas, including text classification, question answering, and text generation. Particularly, in the Vietnamese Multitask Language Understanding (VMLU) benchmark, vi-mistral-x sets a new standard, outperforming other available models significantly. This paper highlights the critical role of continual pre-training in advancing language-specific LLMs and opens new avenues for the development of multilingual models. We aim for vi-mistral-x to not just be an important asset for processing the Vietnamese language but also to encourage more advancements in creating large language models for languages that are less represented.</li>
</ul>

<h3>Title: Efficient argument classification with compact language models and  ChatGPT-4 refinements</h3>
<ul>
<li><strong>Authors: </strong>Marcin Pietron, Rafał Olszowski, Jakub Gomułka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15473">https://arxiv.org/abs/2403.15473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15473">https://arxiv.org/pdf/2403.15473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15473]] Efficient argument classification with compact language models and  ChatGPT-4 refinements(https://arxiv.org/abs/2403.15473)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer</a></li>
<li><strong>Abstract: </strong>Argument mining (AM) is defined as the task of automatically identifying and extracting argumentative components (e.g. premises, claims, etc.) and detecting the existing relations among them (i.e., support, attack, no relations). Deep learning models enable us to analyze arguments more efficiently than traditional methods and extract their semantics. This paper presents comparative studies between a few deep learning-based models in argument mining. The work concentrates on argument classification. The research was done on a wide spectrum of datasets (Args.me, UKP, US2016). The main novelty of this paper is the ensemble model which is based on BERT architecture and ChatGPT-4 as fine tuning model. The presented results show that BERT+ChatGPT-4 outperforms the rest of the models including other Transformer-based and LSTM-based models. The observed improvement is, in most cases, greater than 10The presented analysis can provide crucial insights into how the models for argument classification should be further improved. Additionally, it can help develop a prompt-based algorithm to eliminate argument classification errors.</li>
</ul>

<h3>Title: Learning to Infer Generative Template Programs for Visual Concepts</h3>
<ul>
<li><strong>Authors: </strong>R. Kenny Jones, Siddhartha Chaudhuri, Daniel Ritchie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15476">https://arxiv.org/abs/2403.15476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15476">https://arxiv.org/pdf/2403.15476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15476]] Learning to Infer Generative Template Programs for Visual Concepts(https://arxiv.org/abs/2403.15476)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>People grasp flexible visual concepts from a few examples. We explore a neurosymbolic system that learns how to infer programs that capture visual concepts in a domain-general fashion. We introduce Template Programs: programmatic expressions from a domain-specific language that specify structural and parametric patterns common to an input concept. Our framework supports multiple concept-related tasks, including few-shot generation and co-segmentation through parsing. We develop a learning paradigm that allows us to train networks that infer Template Programs directly from visual datasets that contain concept groupings. We run experiments across multiple visual domains: 2D layouts, Omniglot characters, and 3D shapes. We find that our method outperforms task-specific alternatives, and performs competitively against domain-specific approaches for the limited domains where they exist.</li>
</ul>

<h3>Title: Integrating Supervised Extractive and Generative Language Models for  Suicide Risk Evidence Summarization</h3>
<ul>
<li><strong>Authors: </strong>Rika Tanaka, Yusuke Fukazawa</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15478">https://arxiv.org/abs/2403.15478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15478">https://arxiv.org/pdf/2403.15478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15478]] Integrating Supervised Extractive and Generative Language Models for  Suicide Risk Evidence Summarization(https://arxiv.org/abs/2403.15478)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>We propose a method that integrates supervised extractive and generative language models for providing supporting evidence of suicide risk in the CLPsych 2024 shared task. Our approach comprises three steps. Initially, we construct a BERT-based model for estimating sentence-level suicide risk and negative sentiment. Next, we precisely identify high suicide risk sentences by emphasizing elevated probabilities of both suicide risk and negative sentiment. Finally, we integrate generative summaries using the MentaLLaMa framework and extractive summaries from identified high suicide risk sentences and a specialized dictionary of suicidal risk words. SophiaADS, our team, achieved 1st place for highlight extraction and ranked 10th for summary generation, both based on recall and consistency metrics, respectively.</li>
</ul>

<h3>Title: Multi-Level Feedback Generation with Large Language Models for  Empowering Novice Peer Counselors</h3>
<ul>
<li><strong>Authors: </strong>Alicja Chaszczewicz, Raj Sanjay Shah, Ryan Louie, Bruce A Arnow, Robert Kraut, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15482">https://arxiv.org/abs/2403.15482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15482">https://arxiv.org/pdf/2403.15482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15482]] Multi-Level Feedback Generation with Large Language Models for  Empowering Novice Peer Counselors(https://arxiv.org/abs/2403.15482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the large number of people with mental health issues who use peer counseling. Our work aims to leverage large language models to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualitative and quantitative evaluation with domain experts, we demonstrate that our method minimizes the risk of potentially harmful and low-quality feedback generation which is desirable in such high-stakes scenarios.</li>
</ul>

<h3>Title: RakutenAI-7B: Extending Large Language Models for Japanese</h3>
<ul>
<li><strong>Authors: </strong>Rakuten Group Inc., Aaron Levine, Connie Huang, Chenguang Wang, Eduardo Batista, Ewa Szymanska, Hongyi Ding, Hou Wei Chou, Jean-François Pessiot, Johanes Effendi, Justin Chiu, Kai Torben Ohlhus, Karan Chopra, Keiji Shinzato, Koji Murakami, Lee Xiong, Lei Chen, Maki Kubota, Maksim Tkachenko, Miroku Lee, Naoki Takahashi, Prathyusha Jwalapuram, Ryutaro Tatsushima, Saurabh Jain, Sunil Kumar Yadav, Ting Cai, Wei-Te Chen, Yandi Xia, Yuki Nakayama, Yutaka Higashiyama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15484">https://arxiv.org/abs/2403.15484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15484">https://arxiv.org/pdf/2403.15484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15484]] RakutenAI-7B: Extending Large Language Models for Japanese(https://arxiv.org/abs/2403.15484)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce RakutenAI-7B, a suite of Japanese-oriented large language models that achieve the best performance on the Japanese LM Harness benchmarks among the open 7B models. Along with the foundation model, we release instruction- and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat respectively, under the Apache 2.0 license.</li>
</ul>

<h3>Title: Sequence-to-Sequence Language Models for Character and Emotion Detection  in Dream Narratives</h3>
<ul>
<li><strong>Authors: </strong>Gustave Cortal (ENS Paris Saclay, LISN)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15486">https://arxiv.org/abs/2403.15486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15486">https://arxiv.org/pdf/2403.15486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15486]] Sequence-to-Sequence Language Models for Character and Emotion Detection  in Dream Narratives(https://arxiv.org/abs/2403.15486)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The study of dreams has been central to understanding human (un)consciousness, cognition, and culture for centuries. Analyzing dreams quantitatively depends on labor-intensive, manual annotation of dream narratives. We automate this process through a natural language sequence-to-sequence generation framework. This paper presents the first study on character and emotion detection in the English portion of the open DreamBank corpus of dream narratives. Our results show that language models can effectively address this complex task. To get insight into prediction performance, we evaluate the impact of model size, prediction order of characters, and the consideration of proper names and character traits. We compare our approach with a large language model using in-context learning. Our supervised models perform better while having 28 times fewer parameters. Our model and its generated annotations are made publicly available.</li>
</ul>

<h3>Title: Open Source Conversational LLMs do not know most Spanish words</h3>
<ul>
<li><strong>Authors: </strong>Javier Conde, Miguel González, Nina Melero, Raquel Ferrando, Gonzalo Martínez, Elena Merino-Gómez, José Alberto Hernández, Pedro Reviriego</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15491">https://arxiv.org/abs/2403.15491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15491">https://arxiv.org/pdf/2403.15491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15491]] Open Source Conversational LLMs do not know most Spanish words(https://arxiv.org/abs/2403.15491)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The growing interest in Large Language Models (LLMs) and in particular in conversational models with which users can interact has led to the development of a large number of open-source chat LLMs. These models are evaluated on a wide range of benchmarks to assess their capabilities in answering questions or solving problems on almost any possible topic or to test their ability to reason or interpret texts. Instead, the evaluation of the knowledge that these models have of the languages has received much less attention. For example, the words that they can recognize and use in different languages. In this paper, we evaluate the knowledge that open-source chat LLMs have of Spanish words by testing a sample of words in a reference dictionary. The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.</li>
</ul>

<h3>Title: Twin Auto-Encoder Model for Learning Separable Representation in  Cyberattack Detection</h3>
<ul>
<li><strong>Authors: </strong>Phai Vu Dinh, Quang Uy Nguyen, Thai Hoang Dinh, Diep N. Nguyen, Bao Son Pham, Eryk Dutkiewicz</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15509">https://arxiv.org/abs/2403.15509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15509">https://arxiv.org/pdf/2403.15509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15509]] Twin Auto-Encoder Model for Learning Separable Representation in  Cyberattack Detection(https://arxiv.org/abs/2403.15509)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Representation Learning (RL) plays a pivotal role in the success of many problems including cyberattack detection. Most of the RL methods for cyberattack detection are based on the latent vector of Auto-Encoder (AE) models. An AE transforms raw data into a new latent representation that better exposes the underlying characteristics of the input data. Thus, it is very useful for identifying cyberattacks. However, due to the heterogeneity and sophistication of cyberattacks, the representation of AEs is often entangled/mixed resulting in the difficulty for downstream attack detection models. To tackle this problem, we propose a novel mod called Twin Auto-Encoder (TAE). TAE deterministically transforms the latent representation into a more distinguishable representation namely the \textit{separable representation} and the reconstructsuct the separable representation at the output. The output of TAE called the \textit{reconstruction representation} is input to downstream models to detect cyberattacks. We extensively evaluate the effectiveness of TAE using a wide range of bench-marking datasets. Experiment results show the superior accuracy of TAE over state-of-the-art RL models and well-known machine learning algorithms. Moreover, TAE also outperforms state-of-the-art models on some sophisticated and challenging attacks. We then investigate various characteristics of TAE to further demonstrate its superiority.</li>
</ul>

<h3>Title: Privacy-Preserving End-to-End Spoken Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yinggui Wang, Wei Huang, Le Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15510">https://arxiv.org/abs/2403.15510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15510">https://arxiv.org/pdf/2403.15510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15510]] Privacy-Preserving End-to-End Spoken Language Understanding(https://arxiv.org/abs/2403.15510)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Spoken language understanding (SLU), one of the key enabling technologies for human-computer interaction in IoT devices, provides an easy-to-use user interface. Human speech can contain a lot of user-sensitive information, such as gender, identity, and sensitive content. New types of security and privacy breaches have thus emerged. Users do not want to expose their personal sensitive information to malicious attacks by untrusted third parties. Thus, the SLU system needs to ensure that a potential malicious attacker cannot deduce the sensitive attributes of the users, while it should avoid greatly compromising the SLU accuracy. To address the above challenge, this paper proposes a novel SLU multi-task privacy-preserving model to prevent both the speech recognition (ASR) and identity recognition (IR) attacks. The model uses the hidden layer separation technique so that SLU information is distributed only in a specific portion of the hidden layer, and the other two types of information are removed to obtain a privacy-secure hidden layer. In order to achieve good balance between efficiency and privacy, we introduce a new mechanism of model pre-training, namely joint adversarial training, to further enhance the user privacy. Experiments over two SLU datasets show that the proposed method can reduce the accuracy of both the ASR and IR attacks close to that of a random guess, while leaving the SLU performance largely unaffected.</li>
</ul>

<h3>Title: Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion  Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Phai Vu Dinh, Diep N. Nguyen, Dinh Thai Hoang, Quang Uy Nguyen, Eryk Dutkiewicz, Son Pham Bao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15511">https://arxiv.org/abs/2403.15511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15511">https://arxiv.org/pdf/2403.15511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15511]] Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion  Detection Systems(https://arxiv.org/abs/2403.15511)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>While intrusion detection systems (IDSs) benefit from the diversity and generalization of IoT data features, the data diversity (e.g., the heterogeneity and high dimensions of data) also makes it difficult to train effective machine learning models in IoT IDSs. This also leads to potentially redundant/noisy features that may decrease the accuracy of the detection engine in IDSs. This paper first introduces a novel neural network architecture called Multiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that can process inputs from different sources with different characteristics. The MIAE model is trained in an unsupervised learning mode to transform the heterogeneous inputs into lower-dimensional representation, which helps classifiers distinguish between normal behaviour and different types of attacks. To distil and retain more relevant features but remove less important/redundant ones during the training process, we further design and embed a feature selection layer right after the representation layer of MIAE resulting in a new model called MIAEFS. This layer learns the importance of features in the representation vector, facilitating the selection of informative features from the representation vector. The results on three IDS datasets, i.e., NSLKDD, UNSW-NB15, and IDS2017, show the superior performance of MIAE and MIAEFS compared to other methods, e.g., conventional classifiers, dimensionality reduction models, unsupervised representation learning methods with different input dimensions, and unsupervised feature selection models. Moreover, MIAE and MIAEFS combined with the Random Forest (RF) classifier achieve accuracy of 96.5% in detecting sophisticated attacks, e.g., Slowloris. The average running time for detecting an attack sample using RF with the representation of MIAE and MIAEFS is approximate 1.7E-6 seconds, whilst the model size is lower than 1 MB.</li>
</ul>

<h3>Title: Enhancing Effectiveness and Robustness in a Low-Resource Regime via  Decision-Boundary-aware Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Kyohoon Jin, Junho Lee, Juhwan Choi, Sangmin Song, Youngbin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15512">https://arxiv.org/abs/2403.15512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15512">https://arxiv.org/pdf/2403.15512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15512]] Enhancing Effectiveness and Robustness in a Low-Resource Regime via  Decision-Boundary-aware Data Augmentation(https://arxiv.org/abs/2403.15512)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Efforts to leverage deep learning models in low-resource regimes have led to numerous augmentation studies. However, the direct application of methods such as mixup and cutout to text data, is limited due to their discrete characteristics. While methods using pretrained language models have exhibited efficiency, they require additional considerations for robustness. Inspired by recent studies on decision boundaries, this paper proposes a decision-boundary-aware data augmentation strategy to enhance robustness using pretrained language models. The proposed technique first focuses on shifting the latent features closer to the decision boundary, followed by reconstruction to generate an ambiguous version with a soft label. Additionally, mid-K sampling is suggested to enhance the diversity of the generated sentences. This paper demonstrates the performance of the proposed augmentation strategy compared to other methods through extensive experiments. Furthermore, the ablation study reveals the effect of soft labels and mid-K sampling and the extensibility of the method with curriculum data augmentation.</li>
</ul>

<h3>Title: GTC: GNN-Transformer Co-contrastive Learning for Self-supervised  Heterogeneous Graph Representation</h3>
<ul>
<li><strong>Authors: </strong>Yundong Sun, Dongjie Zhu, Yansong Wang, Zhaoshuo Tian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15520">https://arxiv.org/abs/2403.15520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15520">https://arxiv.org/pdf/2403.15520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15520]] GTC: GNN-Transformer Co-contrastive Learning for Self-supervised  Heterogeneous Graph Representation(https://arxiv.org/abs/2403.15520)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have emerged as the most powerful weapon for various graph tasks due to the message-passing mechanism's great local information aggregation ability. However, over-smoothing has always hindered GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs, Transformers can model global information and multi-hop interactions via multi-head self-attention and a proper Transformer structure can show more immunity to the over-smoothing problem. So, can we propose a novel framework to combine GNN and Transformer, integrating both GNN's local information aggregation and Transformer's global information modeling ability to eliminate the over-smoothing problem? To realize this, this paper proposes a collaborative learning scheme for GNN-Transformer and constructs GTC architecture. GTC leverages the GNN and Transformer branch to encode node information from different views respectively, and establishes contrastive learning tasks based on the encoded cross-view information to realize self-supervised heterogeneous graph representation. For the Transformer branch, we propose Metapath-aware Hop2Token and CG-Hetphormer, which can cooperate with GNN to attentively encode neighborhood information from different levels. As far as we know, this is the first attempt in the field of graph representation learning to utilize both GNN and Transformer to collaboratively capture different view information and conduct cross-view contrastive learning. The experiments on real datasets show that GTC exhibits superior performance compared with state-of-the-art methods. Codes can be available at https://github.com/PHD-lanyu/GTC.</li>
</ul>

<h3>Title: Medical Image Data Provenance for Medical Cyber-Physical System</h3>
<ul>
<li><strong>Authors: </strong>Vijay Kumar, Kolin Paul</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15522">https://arxiv.org/abs/2403.15522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15522">https://arxiv.org/pdf/2403.15522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15522]] Medical Image Data Provenance for Medical Cyber-Physical System(https://arxiv.org/abs/2403.15522)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, watermark</a></li>
<li><strong>Abstract: </strong>Continuous advancements in medical technology have led to the creation of affordable mobile imaging devices suitable for telemedicine and remote monitoring. However, the rapid examination of large populations poses challenges, including the risk of fraudulent practices by healthcare professionals and social workers exchanging unverified images via mobile applications. To mitigate these risks, this study proposes using watermarking techniques to embed a device fingerprint (DFP) into captured images, ensuring data provenance. The DFP, representing the unique attributes of the capturing device and raw image, is embedded into raw images before storage, thus enabling verification of image authenticity and source. Moreover, a robust remote validation method is introduced to authenticate images, enhancing the integrity of medical image data in interconnected healthcare systems. Through a case study on mobile fundus imaging, the effectiveness of the proposed framework is evaluated in terms of computational efficiency, image quality, security, and trustworthiness. This approach is suitable for a range of applications, including telemedicine, the Internet of Medical Things (IoMT), eHealth, and Medical Cyber-Physical Systems (MCPS) applications, providing a reliable means to maintain data provenance in diagnostic settings utilizing medical images or videos.</li>
</ul>

<h3>Title: LimGen: Probing the LLMs for Generating Suggestive Limitations of  Research Papers</h3>
<ul>
<li><strong>Authors: </strong>Abdur Rahman Bin Md Faizullah, Ashok Urlana, Rahul Mishra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15529">https://arxiv.org/abs/2403.15529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15529">https://arxiv.org/pdf/2403.15529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15529]] LimGen: Probing the LLMs for Generating Suggestive Limitations of  Research Papers(https://arxiv.org/abs/2403.15529)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called LimGen, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at https://github.com/armbf/LimGen.</li>
</ul>

<h3>Title: Language-Based Depth Hints for Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Dylan Auty, Krystian Mikolajczyk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15551">https://arxiv.org/abs/2403.15551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15551">https://arxiv.org/pdf/2403.15551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15551]] Language-Based Depth Hints for Monocular Depth Estimation(https://arxiv.org/abs/2403.15551)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation (MDE) is inherently ambiguous, as a given image may result from many different 3D scenes and vice versa. To resolve this ambiguity, an MDE system must make assumptions about the most likely 3D scenes for a given input. These assumptions can be either explicit or implicit. In this work, we demonstrate the use of natural language as a source of an explicit prior about the structure of the world. The assumption is made that human language encodes the likely distribution in depth-space of various objects. We first show that a language model encodes this implicit bias during training, and that it can be extracted using a very simple learned approach. We then show that this prediction can be provided as an explicit source of assumption to an MDE system, using an off-the-shelf instance segmentation model that provides the labels used as the input to the language model. We demonstrate the performance of our method on the NYUD2 dataset, showing improvement compared to the baseline and to random controls.</li>
</ul>

<h3>Title: An Optimization Framework to Enforce Multi-View Consistency for  Texturing 3D Meshes Using Pre-Trained Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Zhao, Chen Song, Xiaodong Gu, Yuan Dong, Qi Zuo, Weihao Yuan, Zilong Dong, Liefeng Bo, Qixing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15559">https://arxiv.org/abs/2403.15559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15559">https://arxiv.org/pdf/2403.15559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15559]] An Optimization Framework to Enforce Multi-View Consistency for  Texturing 3D Meshes Using Pre-Trained Text-to-Image Models(https://arxiv.org/abs/2403.15559)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A fundamental problem in the texturing of 3D meshes using pre-trained text-to-image models is to ensure multi-view consistency. State-of-the-art approaches typically use diffusion models to aggregate multi-view inputs, where common issues are the blurriness caused by the averaging operation in the aggregation step or inconsistencies in local features. This paper introduces an optimization framework that proceeds in four stages to achieve multi-view consistency. Specifically, the first stage generates an over-complete set of 2D textures from a predefined set of viewpoints using an MV-consistent diffusion process. The second stage selects a subset of views that are mutually consistent while covering the underlying 3D model. We show how to achieve this goal by solving semi-definite programs. The third stage performs non-rigid alignment to align the selected views across overlapping regions. The fourth stage solves an MRF problem to associate each mesh face with a selected view. In particular, the third and fourth stages are iterated, with the cuts obtained in the fourth stage encouraging non-rigid alignment in the third stage to focus on regions close to the cuts. Experimental results show that our approach significantly outperforms baseline approaches both qualitatively and quantitatively.</li>
</ul>

<h3>Title: Data-centric Prediction Explanation via Kernelized Stein Discrepancy</h3>
<ul>
<li><strong>Authors: </strong>Mahtab Sarvmaili, Hassan Sajjad, Ga Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15576">https://arxiv.org/abs/2403.15576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15576">https://arxiv.org/pdf/2403.15576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15576]] Data-centric Prediction Explanation via Kernelized Stein Discrepancy(https://arxiv.org/abs/2403.15576)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing example-based prediction explanation methods often bridge test and training data points through the model's parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain), a straightforward prediction explanation method exploiting properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperforms existing methods from various aspects, including 1) preciseness (fine-grained explanation), 2) consistency, and 3) computation efficiency, leading to a surprisingly simple, effective, and robust prediction explanation solution.</li>
</ul>

<h3>Title: U-ARE-ME: Uncertainty-Aware Rotation Estimation in Manhattan  Environments</h3>
<ul>
<li><strong>Authors: </strong>Aalok Patwardhan, Callum Rhodes, Gwangbin Bae, Andrew J. Davison</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15583">https://arxiv.org/abs/2403.15583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15583">https://arxiv.org/pdf/2403.15583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15583]] U-ARE-ME: Uncertainty-Aware Rotation Estimation in Manhattan  Environments(https://arxiv.org/abs/2403.15583)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Camera rotation estimation from a single image is a challenging task, often requiring depth data and/or camera intrinsics, which are generally not available for in-the-wild videos. Although external sensors such as inertial measurement units (IMUs) can help, they often suffer from drift and are not applicable in non-inertial reference frames. We present U-ARE-ME, an algorithm that estimates camera rotation along with uncertainty from uncalibrated RGB images. Using a Manhattan World assumption, our method leverages the per-pixel geometric priors encoded in single-image surface normal predictions and performs optimisation over the SO(3) manifold. Given a sequence of images, we can use the per-frame rotation estimates and their uncertainty to perform multi-frame optimisation, achieving robustness and temporal consistency. Our experiments demonstrate that U-ARE-ME performs comparably to RGB-D methods and is more robust than sparse feature-based SLAM methods. We encourage the reader to view the accompanying video at https://callum-rhodes.github.io/U-ARE-ME for a visual overview of our method.</li>
</ul>

<h3>Title: MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Mai A. Shaaban, Adnan Khan, Mohammad Yaqub</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15585">https://arxiv.org/abs/2403.15585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15585">https://arxiv.org/pdf/2403.15585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15585]] MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis(https://arxiv.org/abs/2403.15585)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients' medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynamically refines few-shot data for real-time adjustment to new patient scenarios. Moreover, VG aids in focusing the model's attention on relevant regions of interest in X-ray images, enhancing the identification of abnormalities. We release MedPromptX-VQA, a new in-context visual question answering dataset encompassing interleaved image and EHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the SOTA performance of MedPromptX, achieving an 11% improvement in F1-score compared to the baselines. Code and data are available at \url{https://github.com/BioMedIA-MBZUAI/MedPromptX}.</li>
</ul>

<h3>Title: FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in  RKHSs</h3>
<ul>
<li><strong>Authors: </strong>Sepehr Dehdashtian, Lan Wang, Vishnu Naresh Boddeti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15593">https://arxiv.org/abs/2403.15593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15593">https://arxiv.org/pdf/2403.15593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15593]] FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in  RKHSs(https://arxiv.org/abs/2403.15593)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP's image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative optimization involving closed-form solvers, which leads to $4\times$-$10\times$ faster training than the existing methods. 3) Sample Efficiency: Under sample-limited conditions, FairerCLIP significantly outperforms baselines when they fail entirely. And, 4) Performance: Empirically, FairerCLIP achieves appreciable accuracy gains on benchmark fairness and spurious correlation datasets over their respective baselines.</li>
</ul>

<h3>Title: Forward Learning for Gradient-based Black-box Saliency Map Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeliang Zhang, Mingqian Feng, Jinyang Jiang, Rongyi Zhu, Yijie Peng, Chenliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15603">https://arxiv.org/abs/2403.15603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15603">https://arxiv.org/pdf/2403.15603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15603]] Forward Learning for Gradient-based Black-box Saliency Map Generation(https://arxiv.org/abs/2403.15603)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Gradient-based saliency maps are widely used to explain deep neural network decisions. However, as models become deeper and more black-box, such as in closed-source APIs like ChatGPT, computing gradients become challenging, hindering conventional explanation methods. In this work, we introduce a novel unified framework for estimating gradients in black-box settings and generating saliency maps to interpret model decisions. We employ the likelihood ratio method to estimate output-to-input gradients and utilize them for saliency map generation. Additionally, we propose blockwise computation techniques to enhance estimation accuracy. Extensive experiments in black-box settings validate the effectiveness of our method, demonstrating accurate gradient estimation and explainability of generated saliency maps. Furthermore, we showcase the scalability of our approach by applying it to explain GPT-Vision, revealing the continued relevance of gradient-based explanation methods in the era of large, closed-source, and black-box models.</li>
</ul>

<h3>Title: Efficiently Assemble Normalization Layers and Regularization for  Federated Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Khiem Le, Long Ho, Cuong Do, Danh Le-Phuoc, Kok-Seng Wong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15605">https://arxiv.org/abs/2403.15605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15605">https://arxiv.org/pdf/2403.15605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15605]] Efficiently Assemble Normalization Layers and Regularization for  Federated Domain Generalization(https://arxiv.org/abs/2403.15605)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. Federated Domain Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the Federated Learning paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while retaining discrimination of those features. Then, we incorporate a simple yet effective regularizer to guide these models in directly capturing domain-invariant representations that the global model's classifier can leverage. Extensive experimental results on two benchmark datasets, i.e., PACS and Office-Home, and a real-world medical dataset, Camelyon17, indicate that our proposed method outperforms other existing methods in addressing this particular problem.</li>
</ul>

<h3>Title: NaturalTurn: A Method to Segment Transcripts into Naturalistic  Conversational Turns</h3>
<ul>
<li><strong>Authors: </strong>Gus Cooney, Andrew Reece</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15615">https://arxiv.org/abs/2403.15615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15615">https://arxiv.org/pdf/2403.15615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15615]] NaturalTurn: A Method to Segment Transcripts into Naturalistic  Conversational Turns(https://arxiv.org/abs/2403.15615)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Conversation is the subject of increasing interest in the social, cognitive, and computational sciences. And yet, as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational turns--the basic building blocks of social interaction. We introduce "NaturalTurn," a turn segmentation algorithm designed to accurately capture the dynamics of naturalistic exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize conversation. Using data from a large conversation corpus, we show how NaturalTurn-derived transcripts demonstrate favorable statistical and inferential characteristics compared to transcripts derived from existing methods. The NaturalTurn algorithm represents an improvement in machine-generated transcript processing methods, or "turn models" that will enable researchers to associate turn-taking dynamics with the broader outcomes that result from social interaction, a central goal of conversation science.</li>
</ul>

<h3>Title: Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian  Splatting</h3>
<ul>
<li><strong>Authors: </strong>Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, Qing Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15624">https://arxiv.org/abs/2403.15624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15624">https://arxiv.org/pdf/2403.15624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15624]] Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian  Splatting(https://arxiv.org/abs/2403.15624)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary 3D scene understanding presents a significant challenge in computer vision, withwide-ranging applications in embodied agents and augmented reality systems. Previous approaches haveadopted Neural Radiance Fields (NeRFs) to analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel open-vocabulary scene understanding approach based on 3D Gaussian Splatting. Our keyidea is distilling pre-trained 2D semantics into 3D Gaussians. We design a versatile projection approachthat maps various 2Dsemantic features from pre-trained image encoders into a novel semantic component of 3D Gaussians, withoutthe additional training required by NeRFs. We further build a 3D semantic network that directly predictsthe semantic component from raw 3D Gaussians for fast inference. We explore several applications ofSemantic Gaussians: semantic segmentation on ScanNet-20, where our approach attains a 4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene understanding counterparts; object part segmentation,sceneediting, and spatial-temporal segmentation with better qualitative results over 2D and 3D baselines,highlighting its versatility and effectiveness on supporting diverse downstream tasks.</li>
</ul>

<h3>Title: Differentially Private Next-Token Prediction of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Flemings, Meisam Razaviyayn, Murali Annavaram</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15638">https://arxiv.org/abs/2403.15638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15638">https://arxiv.org/pdf/2403.15638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15638]] Differentially Private Next-Token Prediction of Large Language Models(https://arxiv.org/abs/2403.15638)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, instead providing differential privacy at prediction rather than during training. Our results show that PMixED achieves a stronger privacy guarantee than sample-level privacy and outperforms DP-SGD for privacy $\epsilon = 8$ on large-scale datasets.</li>
</ul>

<h3>Title: RetiGen: A Framework for Generalized Retinal Diagnosis Using Multi-View  Fundus Images</h3>
<ul>
<li><strong>Authors: </strong>Ze Chen, Gongyu Zhang, Jiayu Huo, Joan Nunez do Rio, Charalampos Komninos, Yang Liu, Rachel Sparks, Sebastien Ourselin, Christos Bergeles, Timothy Jackson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15647">https://arxiv.org/abs/2403.15647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15647">https://arxiv.org/pdf/2403.15647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15647]] RetiGen: A Framework for Generalized Retinal Diagnosis Using Multi-View  Fundus Images(https://arxiv.org/abs/2403.15647)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study introduces a novel framework for enhancing domain generalization in medical imaging, specifically focusing on utilizing unlabelled multi-view colour fundus photographs. Unlike traditional approaches that rely on single-view imaging data and face challenges in generalizing across diverse clinical settings, our method leverages the rich information in the unlabelled multi-view imaging data to improve model robustness and accuracy. By incorporating a class balancing method, a test-time adaptation technique and a multi-view optimization strategy, we address the critical issue of domain shift that often hampers the performance of machine learning models in real-world applications. Experiments comparing various state-of-the-art domain generalization and test-time optimization methodologies show that our approach consistently outperforms when combined with existing baseline and state-of-the-art methods. We also show our online method improves all existing techniques. Our framework demonstrates improvements in domain generalization capabilities and offers a practical solution for real-world deployment by facilitating online adaptation to new, unseen datasets. Our code is available at https://github.com/zgy600/RetiGen .</li>
</ul>

<h3>Title: What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle  Gaze Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yihua Cheng, Yaning Zhu, Zongji Wang, Hongquan Hao, Yongwei Liu, Shiqing Cheng, Xi Wang, Hyung Jin Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15664">https://arxiv.org/abs/2403.15664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15664">https://arxiv.org/pdf/2403.15664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15664]] What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle  Gaze Estimation(https://arxiv.org/abs/2403.15664)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Driver's eye gaze holds a wealth of cognitive and intentional cues crucial for intelligent vehicles. Despite its significance, research on in-vehicle gaze estimation remains limited due to the scarcity of comprehensive and well-annotated datasets in real driving scenarios. In this paper, we present three novel elements to advance in-vehicle gaze research. Firstly, we introduce IVGaze, a pioneering dataset capturing in-vehicle gaze, collected from 125 subjects and covering a large range of gaze and head poses within vehicles. Conventional gaze collection systems are inadequate for in-vehicle use. In this dataset, we propose a new vision-based solution for in-vehicle gaze collection, introducing a refined gaze target calibration method to tackle annotation challenges. Second, our research focuses on in-vehicle gaze estimation leveraging the IVGaze. In-vehicle face images often suffer from low resolution, prompting our introduction of a gaze pyramid transformer that leverages transformer-based multilevel features integration. Expanding upon this, we introduce the dual-stream gaze pyramid transformer (GazeDPTR). Employing perspective transformation, we rotate virtual cameras to normalize images, utilizing camera pose to merge normalized and original images for accurate gaze estimation. GazeDPTR shows state-of-the-art performance on the IVGaze dataset. Thirdly, we explore a novel strategy for gaze zone classification by extending the GazeDPTR. A foundational tri-plane and project gaze onto these planes are newly defined. Leveraging both positional features from the projection points and visual attributes from images, we achieve superior performance compared to relying solely on visual features, substantiating the advantage of gaze estimation. Our project is available at https://yihua.zone/work/ivgaze.</li>
</ul>

<h3>Title: AI for Biomedicine in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Bi, Sajib Acharjee Dip, Daniel Hajialigol, Sindhura Kommu, Hanwen Liu, Meng Lu, Xuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15673">https://arxiv.org/abs/2403.15673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15673">https://arxiv.org/pdf/2403.15673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15673]] AI for Biomedicine in the Era of Large Language Models(https://arxiv.org/abs/2403.15673)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in large language models, exemplified by models like ChatGPT, have showcased significant prowess in natural language tasks, such as translating languages, constructing chatbots, and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences: biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent large language models to drive biomedical knowledge discoveries? In this survey, we will explore the application of large language models to three crucial categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. Furthermore, we will delve into large language model challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation</li>
</ul>

<h3>Title: EAGLE: A Domain Generalization Framework for AI-generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15690">https://arxiv.org/abs/2403.15690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15690">https://arxiv.org/pdf/2403.15690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15690]] EAGLE: A Domain Generalization Framework for AI-generated Text Detection(https://arxiv.org/abs/2403.15690)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models. While supervised AI-generated text detectors perform well on text generated by older LLMs, with the frequent release of new LLMs, building supervised detectors for identifying text from such new models would require new labeled training data, which is infeasible in practice. In this work, we tackle this problem and propose a domain generalization framework for the detection of AI-generated text from unseen target generators. Our proposed framework, EAGLE, leverages the labeled data that is available so far from older language models and learns features invariant across these generators, in order to detect text generated by an unknown target generator. EAGLE learns such domain-invariant features by combining the representational power of self-supervised contrastive learning with domain adversarial training. Through our experiments we demonstrate how EAGLE effectively achieves impressive performance in detecting text generated by unseen target generators, including recent state-of-the-art ones such as GPT-4 and Claude, reaching detection scores of within 4.7% of a fully supervised detector.</li>
</ul>

<h3>Title: Technical Report: Masked Skeleton Sequence Modeling for Learning Larval  Zebrafish Behavior Latent Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Lanxin Xu, Shuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15693">https://arxiv.org/abs/2403.15693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15693">https://arxiv.org/pdf/2403.15693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15693]] Technical Report: Masked Skeleton Sequence Modeling for Learning Larval  Zebrafish Behavior Latent Embeddings(https://arxiv.org/abs/2403.15693)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>In this report, we introduce a novel self-supervised learning method for extracting latent embeddings from behaviors of larval zebrafish. Drawing inspiration from Masked Modeling techniquesutilized in image processing with Masked Autoencoders (MAE) \cite{he2022masked} and in natural language processing with Generative Pre-trained Transformer (GPT) \cite{radford2018improving}, we treat behavior sequences as a blend of images and language. For the skeletal sequences of swimming zebrafish, we propose a pioneering Transformer-CNN architecture, the Sequence Spatial-Temporal Transformer (SSTFormer), designed to capture the inter-frame correlation of different joints. This correlation is particularly valuable, as it reflects the coordinated movement of various parts of the fish body across adjacent frames. To handle the high frame rate, we segment the skeleton sequence into distinct time slices, analogous to "words" in a sentence, and employ self-attention transformer layers to encode the consecutive frames within each slice, capturing the spatial correlation among different joints. Furthermore, we incorporate a CNN-based attention module to enhance the representations outputted by the transformer layers. Lastly, we introduce a temporal feature aggregation operation between time slices to improve the discrimination of similar behaviors.</li>
</ul>

<h3>Title: Group Benefits Instances Selection for Data Purification</h3>
<ul>
<li><strong>Authors: </strong>Zhenhuang Cai, Chuanyi Zhang, Dan Huang, Yuanbo Chen, Xiuyun Guan, Yazhou Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15694">https://arxiv.org/abs/2403.15694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15694">https://arxiv.org/pdf/2403.15694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15694]] Group Benefits Instances Selection for Data Purification(https://arxiv.org/abs/2403.15694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Manually annotating datasets for training deep models is very labor-intensive and time-consuming. To overcome such inferiority, directly leveraging web images to conduct training data becomes a natural choice. Nevertheless, the presence of label noise in web data usually degrades the model performance. Existing methods for combating label noise are typically designed and tested on synthetic noisy datasets. However, they tend to fail to achieve satisfying results on real-world noisy datasets. To this end, we propose a method named GRIP to alleviate the noisy label problem for both synthetic and real-world datasets. Specifically, GRIP utilizes a group regularization strategy that estimates class soft labels to improve noise robustness. Soft label supervision reduces overfitting on noisy labels and learns inter-class similarities to benefit classification. Furthermore, an instance purification operation globally identifies noisy labels by measuring the difference between each training sample and its class soft label. Through operations at both group and instance levels, our approach integrates the advantages of noise-robust and noise-cleaning methods and remarkably alleviates the performance degradation caused by noisy labels. Comprehensive experimental results on synthetic and real-world datasets demonstrate the superiority of GRIP over the existing state-of-the-art methods.</li>
</ul>

<h3>Title: SceneX:Procedural Controllable Large-scale Scene Generation via  Large-language Models</h3>
<ul>
<li><strong>Authors: </strong>Mengqi Zhou, Jun Hou, Chuanchen Luo, Yuxi Wang, Zhaoxiang Zhang, Junran Peng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15698">https://arxiv.org/abs/2403.15698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15698">https://arxiv.org/pdf/2403.15698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15698]] SceneX:Procedural Controllable Large-scale Scene Generation via  Large-language Models(https://arxiv.org/abs/2403.15698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Due to its great application potential, large-scale scene generation has drawn extensive attention in academia and industry. Recent research employs powerful generative models to create desired scenes and achieves promising results. However, most of these methods represent the scene using 3D primitives (e.g. point cloud or radiance field) incompatible with the industrial pipeline, which leads to a substantial gap between academic research and industrial deployment. Procedural Controllable Generation (PCG) is an efficient technique for creating scalable and high-quality assets, but it is unfriendly for ordinary users as it demands profound domain expertise. To address these issues, we resort to using the large language model (LLM) to drive the procedural modeling. In this paper, we introduce a large-scale scene generation framework, SceneX, which can automatically produce high-quality procedural models according to designers' textual descriptions.Specifically, the proposed method comprises two components, PCGBench and PCGPlanner. The former encompasses an extensive collection of accessible procedural assets and thousands of hand-craft API documents. The latter aims to generate executable actions for Blender to produce controllable and precise 3D assets guided by the user's instructions. Our SceneX can generate a city spanning 2.5 km times 2.5 km with delicate layout and geometric structures, drastically reducing the time cost from several weeks for professional PCG engineers to just a few hours for an ordinary user. Extensive experiments demonstrated the capability of our method in controllable large-scale scene generation and editing, including asset placement and season translation.</li>
</ul>

<h3>Title: FEEL: A Framework for Evaluating Emotional Support Capability with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huaiwen Zhang, Yu Chen, Ming Wang, Shi Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15699">https://arxiv.org/abs/2403.15699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15699">https://arxiv.org/pdf/2403.15699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15699]] FEEL: A Framework for Evaluating Emotional Support Capability with Large  Language Models(https://arxiv.org/abs/2403.15699)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emotional Support Conversation (ESC) is a typical dialogue that can effec-tively assist the user in mitigating emotional pressures. However, owing to the inherent subjectivity involved in analyzing emotions, current non-artificial methodologies face challenges in effectively appraising the emo-tional support capability. These metrics exhibit a low correlation with human judgments. Concurrently, manual evaluation methods extremely will cause high costs. To solve these problems, we propose a novel model FEEL (Framework for Evaluating Emotional Support Capability with Large Lan-guage Models), employing Large Language Models (LLMs) as evaluators to assess emotional support capabilities. The model meticulously considers var-ious evaluative aspects of ESC to apply a more comprehensive and accurate evaluation method for ESC. Additionally, it employs a probability distribu-tion approach for a more stable result and integrates an ensemble learning strategy, leveraging multiple LLMs with assigned weights to enhance evalua-tion accuracy. To appraise the performance of FEEL, we conduct extensive experiments on existing ESC model dialogues. Experimental results demon-strate our model exhibits a substantial enhancement in alignment with human evaluations compared to the baselines. Our source code is available at https://github.com/Ansisy/FEEL.</li>
</ul>

<h3>Title: G-ACIL: Analytic Learning for Exemplar-Free Generalized Class  Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Huiping Zhuang, Yizhu Chen, Di Fang, Run He, Kai Tong, Hongxin Wei, Ziqian Zeng, Cen Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15706">https://arxiv.org/abs/2403.15706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15706">https://arxiv.org/pdf/2403.15706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15706]] G-ACIL: Analytic Learning for Exemplar-Free Generalized Class  Incremental Learning(https://arxiv.org/abs/2403.15706)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Class incremental learning (CIL) trains a network on sequential tasks with separated categories but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution, leading to intensified forgetting. Existing attempts for the GCIL either have poor performance, or invade data privacy by saving historical exemplars. To address this, in this paper, we propose an exemplar-free generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts analytic learning (a gradient-free training technique), and delivers an analytical solution (i.e., closed-form) to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, allowing an equivalence between the incremental learning and its joint training, i.e., the weight-invariant property. Such an equivalence is theoretically validated through matrix analysis tools, and hence contributes interpretability in GCIL. It is also empirically evidenced by experiments on various datasets and settings of GCIL. The results show that the G-ACIL exhibits leading performance with high robustness compared with existing competitive GCIL methods. Codes will be ready at https://github.com/ZHUANGHP/Analytic-continual-learning.</li>
</ul>

<h3>Title: Identifiable Latent Neural Causal Models</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15711">https://arxiv.org/abs/2403.15711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15711">https://arxiv.org/pdf/2403.15711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15711]] Identifiable Latent Neural Causal Models(https://arxiv.org/abs/2403.15711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findings to latent post-nonlinear causal models. We translate our findings into a practical algorithm, allowing for the acquisition of reliable latent causal representations. Our algorithm, guided by our underlying theory, has demonstrated outstanding performance across a diverse range of synthetic and real-world datasets. The empirical observations align closely with the theoretical findings, affirming the robustness and effectiveness of our approach.</li>
</ul>

<h3>Title: PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture  Search</h3>
<ul>
<li><strong>Authors: </strong>Chensheng Peng, Zhaoyu Zeng, Jinling Gao, Jundong Zhou, Masayoshi Tomizuka, Xinbing Wang, Chenghu Zhou, Nanyang Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15712">https://arxiv.org/abs/2403.15712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15712">https://arxiv.org/pdf/2403.15712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15712]] PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture  Search(https://arxiv.org/abs/2403.15712)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multiple object tracking is a critical task in autonomous driving. Existing works primarily focus on the heuristic design of neural networks to obtain high accuracy. As tracking accuracy improves, however, neural networks become increasingly complex, posing challenges for their practical application in real driving scenarios due to the high level of latency. In this paper, we explore the use of the neural architecture search (NAS) methods to search for efficient architectures for tracking, aiming for low real-time latency while maintaining relatively high accuracy. Another challenge for object tracking is the unreliability of a single sensor, therefore, we propose a multi-modal framework to improve the robustness. Experiments demonstrate that our algorithm can run on edge devices within lower latency constraints, thus greatly reducing the computational requirements for multi-modal object tracking while keeping lower latency.</li>
</ul>

<h3>Title: EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance  Detection</h3>
<ul>
<li><strong>Authors: </strong>Daijun Ding, Li Dong, Zhichao Huang, Guangning Xu, Xu Huang, Bo Liu, Liwen Jing, Bowen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15715">https://arxiv.org/abs/2403.15715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15715">https://arxiv.org/pdf/2403.15715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15715]] EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance  Detection(https://arxiv.org/abs/2403.15715)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Stance detection aims to determine the attitude expressed in text towards a given target. Zero-shot stance detection (ZSSD) has emerged to classify stances towards unseen targets during inference. Recent data augmentation techniques for ZSSD increase transferable knowledge between targets through text or target augmentation. However, these methods exhibit limitations. Target augmentation lacks logical connections between generated targets and source text, while text augmentation relies solely on training data, resulting in insufficient generalization. To address these issues, we propose an encoder-decoder data augmentation (EDDA) framework. The encoder leverages large language models and chain-of-thought prompting to summarize texts into target-specific if-then rationales, establishing logical relationships. The decoder generates new samples based on these expressions using a semantic correlation word replacement strategy to increase syntactic diversity. We also analyze the generated expressions to develop a rationale-enhanced network that fully utilizes the augmented data. Experiments on benchmark datasets demonstrate our approach substantially improves over state-of-the-art ZSSD techniques. The proposed EDDA framework increases semantic relevance and syntactic variety in augmented texts while enabling interpretable rationale-based learning.</li>
</ul>

<h3>Title: A hybrid LLM workflow can help identify user privilege related variables  in programs of any size</h3>
<ul>
<li><strong>Authors: </strong>Haizhou Wang, Zhilong Wang, Peng Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15723">https://arxiv.org/abs/2403.15723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15723">https://arxiv.org/pdf/2403.15723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15723]] A hybrid LLM workflow can help identify user privilege related variables  in programs of any size(https://arxiv.org/abs/2403.15723)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>Many programs involves operations and logic manipulating user privileges, which is essential for the security of an organization. Therefore, one common malicious goal of attackers is to obtain or escalate the privileges, causing privilege leakage. To protect the program and the organization against privilege leakage attacks, it is important to eliminate the vulnerabilities which can be exploited to achieve such attacks. Unfortunately, while memory vulnerabilities are less challenging to find, logic vulnerabilities are much more imminent, harmful and difficult to identify. Accordingly, many analysts choose to find user privilege related (UPR) variables first as start points to investigate the code where the UPR variables may be used to see if there exists any vulnerabilities, especially the logic ones. In this paper, we introduce a large language model (LLM) workflow that can assist analysts in identifying such UPR variables, which is considered to be a very time-consuming task. Specifically, our tool will audit all the variables in a program and output a UPR score, which is the degree of relationship (closeness) between the variable and user privileges, for each variable. The proposed approach avoids the drawbacks introduced by directly prompting a LLM to find UPR variables by focusing on leverage the LLM at statement level instead of supplying LLM with very long code snippets. Those variables with high UPR scores are essentially potential UPR variables, which should be manually investigated. Our experiments show that using a typical UPR score threshold (i.e., UPR score >0.8), the false positive rate (FPR) is only 13.49%, while UPR variable found is significantly more than that of the heuristic based method.</li>
</ul>

<h3>Title: PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on  Scientific Documents</h3>
<ul>
<li><strong>Authors: </strong>Nan Zhang, Connor Heaton, Sean Timothy Okonsky, Prasenjit Mitra, Hilal Ezgi Toraman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15724">https://arxiv.org/abs/2403.15724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15724">https://arxiv.org/pdf/2403.15724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15724]] PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on  Scientific Documents(https://arxiv.org/abs/2403.15724)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Optical Character Recognition (OCR) is an established task with the objective of identifying the text present in an image. While many off-the-shelf OCR models exist, they are often trained for either scientific (e.g., formulae) or generic printed English text. Extracting text from chemistry publications requires an OCR model that is capable in both realms. Nougat, a recent tool, exhibits strong ability to parse academic documents, but is unable to parse tables in PubMed articles, which comprises a significant part of the academic community and is the focus of this work. To mitigate this gap, we present the Printed English and Chemical Equations (PEaCE) dataset, containing both synthetic and real-world records, and evaluate the efficacy of transformer-based OCR models when trained on this resource. Given that real-world records contain artifacts not present in synthetic records, we propose transformations that mimic such qualities. We perform a suite of experiments to explore the impact of patch size, multi-domain training, and our proposed transformations, ultimately finding that models with a small patch size trained on multiple domains using the proposed transformations yield the best performance. Our dataset and code is available at https://github.com/ZN1010/PEaCE.</li>
</ul>

<h3>Title: Convection-Diffusion Equation: A Theoretically Certified Framework for  Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Tangjun Wang, Chenglong Bao, Zuoqiang Shi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15726">https://arxiv.org/abs/2403.15726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15726">https://arxiv.org/pdf/2403.15726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15726]] Convection-Diffusion Equation: A Theoretically Certified Framework for  Neural Networks(https://arxiv.org/abs/2403.15726)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we study the partial differential equation models of neural networks. Neural network can be viewed as a map from a simple base model to a complicate function. Based on solid analysis, we show that this map can be formulated by a convection-diffusion equation. This theoretically certified framework gives mathematical foundation and more understanding of neural networks. Moreover, based on the convection-diffusion equation model, we design a novel network structure, which incorporates diffusion mechanism into network architecture. Extensive experiments on both benchmark datasets and real-world applications validate the performance of the proposed model.</li>
</ul>

<h3>Title: Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion  Collider</h3>
<ul>
<li><strong>Authors: </strong>Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, hep-ex, physics.ins-det</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15729">https://arxiv.org/abs/2403.15729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15729">https://arxiv.org/pdf/2403.15729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15729]] Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion  Collider(https://arxiv.org/abs/2403.15729)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to assess the effectiveness of responses. Furthermore, we describe the concept of prompt template-based instruction-tuning which provides flexibility and accuracy in summarization. Importantly, the implementation relies on LangChain, which serves as the foundation of our entire workflow. This integration ensures efficiency and scalability, facilitating smooth deployment and accessibility for various user groups within the Electron Ion Collider (EIC) community. This innovative AI-driven framework not only simplifies the understanding of vast datasets but also encourages collaborative participation, thereby empowering researchers. As a demonstration, a web application has been developed to explain each stage of the RAG Agent development in detail.</li>
</ul>

<h3>Title: LLMs Instruct LLMs:An Extraction and Editing Method</h3>
<ul>
<li><strong>Authors: </strong>Xin Zhang, Tianjie Ju, Huijia Liang, Ying Fu, Qin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15736">https://arxiv.org/abs/2403.15736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15736">https://arxiv.org/pdf/2403.15736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15736]] LLMs Instruct LLMs:An Extraction and Editing Method(https://arxiv.org/abs/2403.15736)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The interest in updating Large Language Models (LLMs) without retraining from scratch is substantial, yet it comes with some challenges.This is especially true for situations demanding complex reasoning with limited samples, a scenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation for LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG) are inadequate for this critical issue, particularly evident in our exploration of a specific medical context that epitomize the PCRA-LLM's distinct needs.To address the issue, we propose a Sequential Fusion method to incorporate knowledge from complex context into LLMs. This method employs a two-stage framework: initially, it leverages general LLMs to construct knowledge graphs (KGs) for extracting knowledge from complex texts; subsequently, it updates the domain LLMs through knowledge edit. According to our method, the domain LLM achieved a 71.69\% accuracy in question answering tasks. Subsequently, we broadened our assessment to a novel dataset we developed in the economics and management field, where our method realized a 75\% accuracy. These outcomes underline the efficacy and adaptability of our approach for PCRA-LLM across various domains.</li>
</ul>

<h3>Title: Few-shot Dialogue Strategy Learning for Motivational Interviewing via  Inductive Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhouhang Xie, Bodhisattwa Prasad Majumder, Mengjie Zhao, Yoshinori Maeda, Keiichi Yamada, Hiromi Wakaki, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15737">https://arxiv.org/abs/2403.15737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15737">https://arxiv.org/pdf/2403.15737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15737]] Few-shot Dialogue Strategy Learning for Motivational Interviewing via  Inductive Reasoning(https://arxiv.org/abs/2403.15737)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We consider the task of building a dialogue system that can motivate users to adopt positive lifestyle changes: Motivational Interviewing. Addressing such a task requires a system that can infer \textit{how} to motivate a user effectively. We propose DIIT, a framework that is capable of learning and applying conversation strategies in the form of natural language inductive rules from expert demonstrations. Automatic and human evaluation on instruction-following large language models show natural language strategy descriptions discovered by DIIR can improve active listening skills, reduce unsolicited advice, and promote more collaborative and less authoritative responses, outperforming various demonstration utilization methods.</li>
</ul>

<h3>Title: Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuai Zhao, Linchao Zhu, Ruijie Quan, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15740">https://arxiv.org/abs/2403.15740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15740">https://arxiv.org/pdf/2403.15740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15740]] Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large  Language Models(https://arxiv.org/abs/2403.15740)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Web user data plays a central role in the ecosystem of pre-trained large language models (LLMs) and their fine-tuned variants. Billions of data are crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web users}} confirm if LLMs misuse their data without permission? In this work, we suggest that users repeatedly insert personal passphrases into their documents, enabling LLMs to memorize them. These concealed passphrases in user documents, referred to as \textit{ghost sentences}, once they are identified in the generated content of LLMs, users can be sure that their data is used for training. To explore the effectiveness and usage of this copyrighting tool, we define the \textit{user training data identification} task with ghost sentences. Multiple datasets from various sources at different scales are created and tested with LLMs of different sizes. For evaluation, we introduce a last $k$ words verification manner along with two metrics: document and user identification accuracy. In the specific case of instruction tuning of a 3B LLaMA model, 11 out of 16 users with ghost sentences identify their data within the generation content. These 16 users contribute 383 examples to $\sim$1.8M training documents. For continuing pre-training of a 1.1B TinyLlama model, 61 out of 64 users with ghost sentences identify their data within the LLM output. These 64 users contribute 1156 examples to $\sim$10M training documents.</li>
</ul>

<h3>Title: AOCIL: Exemplar-free Analytic Online Class Incremental Learning with Low  Time and Resource Consumption</h3>
<ul>
<li><strong>Authors: </strong>Huiping Zhuang, Yuchen Liu, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Yi Wang, Lap-Pui Chau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15751">https://arxiv.org/abs/2403.15751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15751">https://arxiv.org/pdf/2403.15751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15751]] AOCIL: Exemplar-free Analytic Online Class Incremental Learning with Low  Time and Resource Consumption(https://arxiv.org/abs/2403.15751)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Online Class Incremental Learning (OCIL) aims to train the model in a task-by-task manner, where data arrive in mini-batches at a time while previous data are not accessible. A significant challenge is known as Catastrophic Forgetting, i.e., loss of the previous knowledge on old data. To address this, replay-based methods show competitive results but invade data privacy, while exemplar-free methods protect data privacy but struggle for accuracy. In this paper, we proposed an exemplar-free approach -- Analytic Online Class Incremental Learning (AOCIL). Instead of back-propagation, we design the Analytic Classifier (AC) updated by recursive least square, cooperating with a frozen backbone. AOCIL simultaneously achieves high accuracy, low resource consumption and data privacy protection. We conduct massive experiments on four existing benchmark datasets, and the results demonstrate the strong capability of handling OCIL scenarios. Codes will be ready.</li>
</ul>

<h3>Title: Towards Human-Like Machine Comprehension: Few-Shot Relational Learning  in Visually-Rich Documents</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Tang Li, Chenhui Chu, Nengjun Zhu, Rui Wang, Pinpin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15765">https://arxiv.org/abs/2403.15765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15765">https://arxiv.org/pdf/2403.15765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15765]] Towards Human-Like Machine Comprehension: Few-Shot Relational Learning  in Visually-Rich Documents(https://arxiv.org/abs/2403.15765)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification techniques. This approach aims to generate relation representations that are more aware of the spatial context and unseen relation in a manner similar to human perception. Experimental results demonstrate the effectiveness of our proposed method by showcasing its ability to outperform existing methods. This study also opens up new possibilities for practical applications.</li>
</ul>

<h3>Title: BEND: Bagging Deep Learning Training Based on Efficient Neural Network  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jia Wei, Xingjun Zhang, Witold Pedrycz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15766">https://arxiv.org/abs/2403.15766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15766">https://arxiv.org/pdf/2403.15766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15766]] BEND: Bagging Deep Learning Training Based on Efficient Neural Network  Diffusion(https://arxiv.org/abs/2403.15766)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Bagging has achieved great success in the field of machine learning by integrating multiple base classifiers to build a single strong classifier to reduce model variance. The performance improvement of bagging mainly relies on the number and diversity of base classifiers. However, traditional deep learning model training methods are expensive to train individually and difficult to train multiple models with low similarity in a restricted dataset. Recently, diffusion models, which have been tremendously successful in the fields of imaging and vision, have been found to be effective in generating neural network model weights and biases with diversity. We creatively propose a Bagging deep learning training algorithm based on Efficient Neural network Diffusion (BEND). The originality of BEND comes from the first use of a neural network diffusion model to efficiently build base classifiers for bagging. Our approach is simple but effective, first using multiple trained model weights and biases as inputs to train autoencoder and latent diffusion model to realize a diffusion model from noise to valid neural network parameters. Subsequently, we generate several base classifiers using the trained diffusion model. Finally, we integrate these ba se classifiers for various inference tasks using the Bagging method. Resulting experiments on multiple models and datasets show that our proposed BEND algorithm can consistently outperform the mean and median accuracies of both the original trained model and the diffused model. At the same time, new models diffused using the diffusion model have higher diversity and lower cost than multiple models trained using traditional methods. The BEND approach successfully introduces diffusion models into the new deep learning training domain and provides a new paradigm for future deep learning training and inference.</li>
</ul>

<h3>Title: Adversarial Defense Teacher for Cross-Domain Object Detection under Poor  Visibility Conditions</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Wang, Yinzhe Shen, Martin Lauer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15786">https://arxiv.org/abs/2403.15786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15786">https://arxiv.org/pdf/2403.15786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15786]] Adversarial Defense Teacher for Cross-Domain Object Detection under Poor  Visibility Conditions(https://arxiv.org/abs/2403.15786)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Existing object detectors encounter challenges in handling domain shifts between training and real-world data, particularly under poor visibility conditions like fog and night. Cutting-edge cross-domain object detection methods use teacher-student frameworks and compel teacher and student models to produce consistent predictions under weak and strong augmentations, respectively. In this paper, we reveal that manually crafted augmentations are insufficient for optimal teaching and present a simple yet effective framework named Adversarial Defense Teacher (ADT), leveraging adversarial defense to enhance teaching quality. Specifically, we employ adversarial attacks, encouraging the model to generalize on subtly perturbed inputs that effectively deceive the model. To address small objects under poor visibility conditions, we propose a Zoom-in Zoom-out strategy, which zooms-in images for better pseudo-labels and zooms-out images and pseudo-labels to learn refined features. Our results demonstrate that ADT achieves superior performance, reaching 54.5% mAP on Foggy Cityscapes, surpassing the previous state-of-the-art by 2.6% mAP.</li>
</ul>

<h3>Title: In-Context Matting</h3>
<ul>
<li><strong>Authors: </strong>He Guo, Zixuan Ye, Zhiguo Cao, Hao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15789">https://arxiv.org/abs/2403.15789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15789">https://arxiv.org/pdf/2403.15789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15789]] In-Context Matting(https://arxiv.org/abs/2403.15789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce in-context matting, a novel task setting of image matting. Given a reference image of a certain foreground and guided priors such as points, scribbles, and masks, in-context matting enables automatic alpha estimation on a batch of target images of the same foreground category, without additional auxiliary input. This setting marries good performance in auxiliary input-based matting and ease of use in automatic matting, which finds a good trade-off between customization and automation. To overcome the key challenge of accurate foreground matching, we introduce IconMatting, an in-context matting model built upon a pre-trained text-to-image diffusion model. Conditioned on inter- and intra-similarity matching, IconMatting can make full use of reference context to generate accurate target alpha mattes. To benchmark the task, we also introduce a novel testing dataset ICM-$57$, covering 57 groups of real-world images. Quantitative and qualitative results on the ICM-57 testing set show that IconMatting rivals the accuracy of trimap-based matting while retaining the automation level akin to automatic matting. Code is available at https://github.com/tiny-smart/in-context-matting</li>
</ul>

<h3>Title: Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled  Autoencoder for Mixed Tabular Datasets</h3>
<ul>
<li><strong>Authors: </strong>Samuel Stocksieker, Denys Pommeret, Arthur Charpentier</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15790">https://arxiv.org/abs/2403.15790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15790">https://arxiv.org/pdf/2403.15790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15790]] Boarding for ISS: Imbalanced Self-Supervised: Discovery of a Scaled  Autoencoder for Mixed Tabular Datasets(https://arxiv.org/abs/2403.15790)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The field of imbalanced self-supervised learning, especially in the context of tabular data, has not been extensively studied. Existing research has predominantly focused on image datasets. This paper aims to fill this gap by examining the specific challenges posed by data imbalance in self-supervised learning in the domain of tabular data, with a primary focus on autoencoders. Autoencoders are widely employed for learning and constructing a new representation of a dataset, particularly for dimensionality reduction. They are also often used for generative model learning, as seen in variational autoencoders. When dealing with mixed tabular data, qualitative variables are often encoded using a one-hot encoder with a standard loss function (MSE or Cross Entropy). In this paper, we analyze the drawbacks of this approach, especially when categorical variables are imbalanced. We propose a novel metric to balance learning: a Multi-Supervised Balanced MSE. This approach reduces the reconstruction error by balancing the influence of variables. Finally, we empirically demonstrate that this new metric, compared to the standard MSE: i) outperforms when the dataset is imbalanced, especially when the learning process is insufficient, and ii) provides similar results in the opposite case.</li>
</ul>

<h3>Title: MRC-based Nested Medical NER with Co-prediction and Adaptive  Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Xiaojing Du, Hanjie Zhao, Danyan Xing, Yuxiang Jia, Hongying Zan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15800">https://arxiv.org/abs/2403.15800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15800">https://arxiv.org/pdf/2403.15800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15800]] MRC-based Nested Medical NER with Co-prediction and Adaptive  Pre-training(https://arxiv.org/abs/2403.15800)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In medical information extraction, medical Named Entity Recognition (NER) is indispensable, playing a crucial role in developing medical knowledge graphs, enhancing medical question-answering systems, and analyzing electronic medical records. The challenge in medical NER arises from the complex nested structures and sophisticated medical terminologies, distinguishing it from its counterparts in traditional domains. In response to these complexities, we propose a medical NER model based on Machine Reading Comprehension (MRC), which uses a task-adaptive pre-training strategy to improve the model's capability in the medical field. Meanwhile, our model introduces multiple word-pair embeddings and multi-granularity dilated convolution to enhance the model's representation ability and uses a combined predictor of Biaffine and MLP to improve the model's recognition performance. Experimental evaluations conducted on the CMeEE, a benchmark for Chinese nested medical NER, demonstrate that our proposed model outperforms the compared state-of-the-art (SOTA) models.</li>
</ul>

<h3>Title: Computational Sentence-level Metrics Predicting Human Sentence  Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Kun Sun, Rong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15822">https://arxiv.org/abs/2403.15822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15822">https://arxiv.org/pdf/2403.15822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15822]] Computational Sentence-level Metrics Predicting Human Sentence  Comprehension(https://arxiv.org/abs/2403.15822)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The majority of research in computational psycholinguistics has concentrated on the processing of words. This study introduces innovative methods for computing sentence-level metrics using multilingual large language models. The metrics developed sentence surprisal and sentence relevance and then are tested and compared to validate whether they can predict how humans comprehend sentences as a whole across languages. These metrics offer significant interpretability and achieve high accuracy in predicting human sentence reading speeds. Our results indicate that these computational sentence-level metrics are exceptionally effective at predicting and elucidating the processing difficulties encountered by readers in comprehending sentences as a whole across a variety of languages. Their impressive performance and generalization capabilities provide a promising avenue for future research in integrating LLMs and cognitive science.</li>
</ul>

<h3>Title: Once for Both: Single Stage of Importance and Sparsity Search for Vision  Transformer Compression</h3>
<ul>
<li><strong>Authors: </strong>Hancheng Ye, Chong Yu, Peng Ye, Renqiu Xia, Yansong Tang, Jiwen Lu, Tao Chen, Bo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15835">https://arxiv.org/abs/2403.15835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15835">https://arxiv.org/pdf/2403.15835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15835]] Once for Both: Single Stage of Importance and Sparsity Search for Vision  Transformer Compression(https://arxiv.org/abs/2403.15835)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent Vision Transformer Compression (VTC) works mainly follow a two-stage scheme, where the importance score of each model unit is first evaluated or preset in each submodule, followed by the sparsity score evaluation according to the target sparsity constraint. Such a separate evaluation process induces the gap between importance and sparsity score distributions, thus causing high search costs for VTC. In this work, for the first time, we investigate how to integrate the evaluations of importance and sparsity scores into a single stage, searching the optimal subnets in an efficient manner. Specifically, we present OFB, a cost-efficient approach that simultaneously evaluates both importance and sparsity scores, termed Once for Both (OFB), for VTC. First, a bi-mask scheme is developed by entangling the importance score and the differentiable sparsity score to jointly determine the pruning potential (prunability) of each unit. Such a bi-mask search strategy is further used together with a proposed adaptive one-hot loss to realize the progressive-and-efficient search for the most important subnet. Finally, Progressive Masked Image Modeling (PMIM) is proposed to regularize the feature space to be more representative during the search process, which may be degraded by the dimension reduction. Extensive experiments demonstrate that OFB can achieve superior compression performance over state-of-the-art searching-based and pruning-based methods under various Vision Transformer architectures, meanwhile promoting search efficiency significantly, e.g., costing one GPU search day for the compression of DeiT-S on ImageNet-1K.</li>
</ul>

<h3>Title: TablePuppet: A Generic Framework for Relational Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Lijie Xu, Chulin Xie, Yiran Guo, Gustavo Alonso, Bo Li, Guoliang Li, Wei Wang, Wentao Wu, Ce Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15839">https://arxiv.org/abs/2403.15839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15839">https://arxiv.org/pdf/2403.15839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15839]] TablePuppet: A Generic Framework for Relational Federated Learning(https://arxiv.org/abs/2403.15839)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns. This raises the question: can we directly run FL on distributed relational tables? In this paper, we formalize this problem as relational federated learning (RFL). We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU). In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table. TablePuppet incorporates computation/communication optimizations to deal with the duplicate tuples introduced by joins, as well as differential privacy (DP) to protect against both feature and label leakages. We demonstrate the efficiency of TablePuppet in combination with two widely-used ML training algorithms, stochastic gradient descent (SGD) and alternating direction method of multipliers (ADMM), and compare their computation/communication complexity. We evaluate the SGD/ADMM algorithms developed atop TablePuppet by training diverse ML models. Our experimental results show that TablePuppet achieves model accuracy comparable to the centralized baselines running directly atop the SQL results. Moreover, ADMM takes less communication time than SGD to converge to similar model accuracy.</li>
</ul>

<h3>Title: Inpainting-Driven Mask Optimization for Object Removal</h3>
<ul>
<li><strong>Authors: </strong>Kodai Shimosato, Norimichi Ukita</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15849">https://arxiv.org/abs/2403.15849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15849">https://arxiv.org/pdf/2403.15849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15849]] Inpainting-Driven Mask Optimization for Object Removal(https://arxiv.org/abs/2403.15849)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper proposes a mask optimization method for improving the quality of object removal using image inpainting. While many inpainting methods are trained with a set of random masks, a target for inpainting may be an object, such as a person, in many realistic scenarios. This domain gap between masks in training and inference images increases the difficulty of the inpainting task. In our method, this domain gap is resolved by training the inpainting network with object masks extracted by segmentation, and such object masks are also used in the inference step. Furthermore, to optimize the object masks for inpainting, the segmentation network is connected to the inpainting network and end-to-end trained to improve the inpainting performance. The effect of this end-to-end training is further enhanced by our mask expansion loss for achieving the trade-off between large and small masks. Experimental results demonstrate the effectiveness of our method for better object removal using image inpainting.</li>
</ul>

<h3>Title: Initialisation and Topology Effects in Decentralised Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Arash Badie-Modiri, Chiara Boldrini, Lorenzo Valerio, János Kertész, Márton Karsai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15855">https://arxiv.org/abs/2403.15855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15855">https://arxiv.org/pdf/2403.15855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15855]] Initialisation and Topology Effects in Decentralised Federated Learning(https://arxiv.org/abs/2403.15855)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for more efficient and scalable artificial neural network training in a distributed and uncoordinated environment, offering a deeper understanding of the intertwining roles of network structure and learning dynamics.</li>
</ul>

<h3>Title: Diffusion-based Aesthetic QR Code Generation via Scanning-Robust  Perceptual Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jia-Wei Liao, Winston Wang, Tzu-Sian Wang, Li-Xuan Peng, Cheng-Fu Chou, Jun-Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15878">https://arxiv.org/abs/2403.15878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15878">https://arxiv.org/pdf/2403.15878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15878]] Diffusion-based Aesthetic QR Code Generation via Scanning-Robust  Perceptual Guidance(https://arxiv.org/abs/2403.15878)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>QR codes, prevalent in daily applications, lack visual appeal due to their conventional black-and-white design. Integrating aesthetics while maintaining scannability poses a challenge. In this paper, we introduce a novel diffusion-model-based aesthetic QR code generation pipeline, utilizing pre-trained ControlNet and guided iterative refinement via a novel classifier guidance (SRG) based on the proposed Scanning-Robust Loss (SRL) tailored with QR code mechanisms, which ensures both aesthetics and scannability. To further improve the scannability while preserving aesthetics, we propose a two-stage pipeline with Scanning-Robust Perceptual Guidance (SRPG). Moreover, we can further enhance the scannability of the generated QR code by post-processing it through the proposed Scanning-Robust Projected Gradient Descent (SRPGD) post-processing technique based on SRL with proven convergence. With extensive quantitative, qualitative, and subjective experiments, the results demonstrate that the proposed approach can generate diverse aesthetic QR codes with flexibility in detail. In addition, our pipelines outperforming existing models in terms of Scanning Success Rate (SSR) 86.67% (+40%) with comparable aesthetic scores. The pipeline combined with SRPGD further achieves 96.67% (+50%). Our code will be available https://github.com/jwliao1209/DiffQRCode.</li>
</ul>

<h3>Title: VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for  Vietnamese Natural Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Phong Nguyen-Thuan Do, Son Quoc Tran, Phu Gia Hoang, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15882">https://arxiv.org/abs/2403.15882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15882">https://arxiv.org/pdf/2403.15882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15882]] VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for  Vietnamese Natural Language Understanding(https://arxiv.org/abs/2403.15882)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The success of Natural Language Understanding (NLU) benchmarks in various languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across a wide range of tasks. To establish a standardized set of benchmarks for Vietnamese NLU, we introduce the first Vietnamese Language Understanding Evaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets covering different NLU tasks, including text classification, span extraction, and natural language understanding. To provide an insightful overview of the current state of Vietnamese NLU, we then evaluate seven state-of-the-art pre-trained models, including both multilingual and Vietnamese monolingual models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new state-of-the-art pre-trained model that achieves superior results across all tasks in the VLUE benchmark. Our model combines the proficiency of a multilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT is developed based on the XLM-RoBERTa model, with an additional pretraining step utilizing a significant amount of Vietnamese textual data to enhance its adaptation to the Vietnamese language. For the purpose of future research, CafeBERT is made publicly available for research purposes.</li>
</ul>

<h3>Title: UPSS: a User-centric Private Storage System with its applications</h3>
<ul>
<li><strong>Authors: </strong>Arastoo Bozorgi, Mahya Soleimani Jadidi, Jonathan Anderson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15884">https://arxiv.org/abs/2403.15884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15884">https://arxiv.org/pdf/2403.15884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15884]] UPSS: a User-centric Private Storage System with its applications(https://arxiv.org/abs/2403.15884)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Strong confidentiality, integrity, user control, reliability and performance are critical requirements in privacy-sensitive applications. Such applications would benefit from a data storage and sharing infrastructure that provides these properties even in decentralized topologies with untrusted storage backends, but users today are forced to choose between systemic security properties and system reliability or performance. As an alternative to this status quo we present UPSS: the user-centric private sharing system, a cryptographic storage system that can be used as a conventional filesystem or as the foundation for security-sensitive applications such as redaction with integrity and private revision control. We demonstrate that both the security and performance properties of UPSS exceed that of existing cryptographic filesystems and that its performance is comparable to mature conventional filesystems - in some cases, even superior. Whether used directly via its Rust API or as a conventional filesystem, UPSS provides strong security and practical performance on untrusted storage.</li>
</ul>

<h3>Title: Human Motion Prediction under Unexpected Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Jiangbei Yue, Baiyi Li, Julien Pettré, Armin Seyfried, He Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15891">https://arxiv.org/abs/2403.15891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15891">https://arxiv.org/pdf/2403.15891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15891]] Human Motion Prediction under Unexpected Perturbation(https://arxiv.org/abs/2403.15891)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>We investigate a new task in human motion prediction, which is predicting motions under unexpected physical perturbation potentially involving multiple people. Compared with existing research, this task involves predicting less controlled, unpremeditated and pure reactive motions in response to external impact and how such motions can propagate through people. It brings new challenges such as data scarcity and predicting complex interactions. To this end, we propose a new method capitalizing differential physics and deep neural networks, leading to an explicit Latent Differential Physics (LDP) model. Through experiments, we demonstrate that LDP has high data efficiency, outstanding prediction accuracy, strong generalizability and good explainability. Since there is no similar research, a comprehensive comparison with 11 adapted baselines from several relevant domains is conducted, showing LDP outperforming existing research both quantitatively and qualitatively, improving prediction accuracy by as much as 70%, and demonstrating significantly stronger generalization.</li>
</ul>

<h3>Title: Deep Gaussian Covariance Network with Trajectory Sampling for  Data-Efficient Policy Search</h3>
<ul>
<li><strong>Authors: </strong>Can Bogoclu, Robert Vosshall, Kevin Cremanns, Dirk Roos</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15908">https://arxiv.org/abs/2403.15908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15908">https://arxiv.org/pdf/2403.15908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15908]] Deep Gaussian Covariance Network with Trajectory Sampling for  Data-Efficient Policy Search(https://arxiv.org/abs/2403.15908)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty unaware solutions. We propose to combine trajectory sampling and deep Gaussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; Gaussian processes, Bayesian neural networks, and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and probabilistic models. During our tests, we place particular emphasis on the robustness of the learned policies with respect to noisy initial states.</li>
</ul>

<h3>Title: An Embarrassingly Simple Defense Against Backdoor Attacks On SSL</h3>
<ul>
<li><strong>Authors: </strong>Aryan Satpathy, Nilaksh, Dhruva Rajwade</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15918">https://arxiv.org/abs/2403.15918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15918">https://arxiv.org/pdf/2403.15918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15918]] An Embarrassingly Simple Defense Against Backdoor Attacks On SSL(https://arxiv.org/abs/2403.15918)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Self Supervised Learning (SSL) has emerged as a powerful paradigm to tackle data landscapes with absence of human supervision. The ability to learn meaningful tasks without the use of labeled data makes SSL a popular method to manage large chunks of data in the absence of labels. However, recent work indicates SSL to be vulnerable to backdoor attacks, wherein models can be controlled, possibly maliciously, to suit an adversary's motives. Li et.al (2022) introduce a novel frequency-based backdoor attack: CTRL. They show that CTRL can be used to efficiently and stealthily gain control over a victim's model trained using SSL. In this work, we devise two defense strategies against frequency-based attacks in SSL: One applicable before model training and the second to be applied during model inference. Our first contribution utilizes the invariance property of the downstream task to defend against backdoor attacks in a generalizable fashion. We observe the ASR (Attack Success Rate) to reduce by over 60% across experiments. Our Inference-time defense relies on evasiveness of the attack and uses the luminance channel to defend against attacks. Using object classification as the downstream task for SSL, we demonstrate successful defense strategies that do not require re-training of the model. Code is available at https://github.com/Aryan-Satpathy/Backdoor.</li>
</ul>

<h3>Title: X-Portrait: Expressive Portrait Animation with Hierarchical Motion  Attention</h3>
<ul>
<li><strong>Authors: </strong>You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15931">https://arxiv.org/abs/2403.15931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15931">https://arxiv.org/pdf/2403.15931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15931]] X-Portrait: Expressive Portrait Animation with Hierarchical Motion  Attention(https://arxiv.org/abs/2403.15931)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.</li>
</ul>

<h3>Title: LlamBERT: Large-scale low-cost data annotation in NLP</h3>
<ul>
<li><strong>Authors: </strong>Bálint Csanády, Lajos Muzsai, Péter Vedres, Zoltán Nádasdy, András Lukács</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15938">https://arxiv.org/abs/2403.15938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15938">https://arxiv.org/pdf/2403.15938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15938]] LlamBERT: Large-scale low-cost data annotation in NLP(https://arxiv.org/abs/2403.15938)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.</li>
</ul>

<h3>Title: Geotokens and Geotransformers</h3>
<ul>
<li><strong>Authors: </strong>Eren Unlu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15940">https://arxiv.org/abs/2403.15940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15940">https://arxiv.org/pdf/2403.15940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15940]] Geotokens and Geotransformers(https://arxiv.org/abs/2403.15940)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In transformer architectures, position encoding primarily provides a sense of sequence for input tokens. While the original transformer paper's method has shown satisfactory results in general language processing tasks, there have been new proposals, such as Rotary Position Embedding (RoPE), for further improvement. This paper presents geotokens, input components for transformers, each linked to a specific geological location. Unlike typical language sequences, for these tokens, the order is not as vital as the geographical coordinates themselves. To represent the relative position in this context and to keep a balance between the real world distance and the distance in the embedding space, we design a position encoding approach drawing from the RoPE structure but tailored for spherical coordinates.</li>
</ul>

<h3>Title: Feature Manipulation for DDPM based Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhenglin Li, Yangchen Huang, Mengran Zhu, Jingyu Zhang, JingHao Chang, Houze Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15943">https://arxiv.org/abs/2403.15943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15943">https://arxiv.org/pdf/2403.15943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15943]] Feature Manipulation for DDPM based Change Detection(https://arxiv.org/abs/2403.15943)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Change Detection is a classic task of computer vision that receives a bi-temporal image pair as input and separates the semantically changed and unchanged regions of it. The diffusion model is used in image synthesis and as a feature extractor and has been applied to various downstream tasks. Using this, a feature map is extracted from the pre-trained diffusion model from the large-scale data set, and changes are detected through the additional network. On the one hand, the current diffusion-based change detection approach focuses only on extracting a good feature map using the diffusion model. It obtains and uses differences without further adjustment to the created feature map. Our method focuses on manipulating the feature map extracted from the Diffusion Model to be more semantically useful, and for this, we propose two methods: Feature Attention and FDAF. Our model with Feature Attention achieved a state-of-the-art F1 score (90.18) and IoU (83.86) on the LEVIR-CD dataset.</li>
</ul>

<h3>Title: Deep Domain Adaptation: A Sim2Real Neural Approach for Improving  Eye-Tracking Systems</h3>
<ul>
<li><strong>Authors: </strong>Viet Dung Nguyen, Reynold Bailey, Gabriel J. Diaz, Chengyi Ma, Alexander Fix, Alexander Ororbia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15947">https://arxiv.org/abs/2403.15947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15947">https://arxiv.org/pdf/2403.15947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15947]] Deep Domain Adaptation: A Sim2Real Neural Approach for Improving  Eye-Tracking Systems(https://arxiv.org/abs/2403.15947)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Eye image segmentation is a critical step in eye tracking that has great influence over the final gaze estimate. Segmentation models trained using supervised machine learning can excel at this task, their effectiveness is determined by the degree of overlap between the narrow distributions of image properties defined by the target dataset and highly specific training datasets, of which there are few. Attempts to broaden the distribution of existing eye image datasets through the inclusion of synthetic eye images have found that a model trained on synthetic images will often fail to generalize back to real-world eye images. In remedy, we use dimensionality-reduction techniques to measure the overlap between the target eye images and synthetic training data, and to prune the training dataset in a manner that maximizes distribution overlap. We demonstrate that our methods result in robust, improved performance when tackling the discrepancy between simulation and real-world data samples.</li>
</ul>

<h3>Title: Finding needles in a haystack: A Black-Box Approach to Invisible  Watermark Detection</h3>
<ul>
<li><strong>Authors: </strong>Minzhou Pan, Zhengting Wang, Xin Dong, Vikash Sehwag, Lingjuan Lyu, Xue Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15955">https://arxiv.org/abs/2403.15955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15955">https://arxiv.org/pdf/2403.15955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15955]] Finding needles in a haystack: A Black-Box Approach to Invisible  Watermark Detection(https://arxiv.org/abs/2403.15955)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark</a></li>
<li><strong>Abstract: </strong>In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a black-box and annotation-free setting. WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques. We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset. Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5. In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking methods. As invisible watermarks become increasingly prevalent, while specific decoding techniques remain undisclosed, our approach provides a versatile solution and establishes a path toward increasing accountability, transparency, and trust in our digital visual content.</li>
</ul>

<h3>Title: Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance  Fields</h3>
<ul>
<li><strong>Authors: </strong>unhong Zhao, Wei Ying, Yaoqiang Pan, Zhenfeng Yi, Chao Chen, Kewei Hu, Hanwen Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15981">https://arxiv.org/abs/2403.15981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15981">https://arxiv.org/pdf/2403.15981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15981]] Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance  Fields(https://arxiv.org/abs/2403.15981)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate collection of plant phenotyping is critical to optimising sustainable farming practices in precision agriculture. Traditional phenotyping in controlled laboratory environments, while valuable, falls short in understanding plant growth under real-world conditions. Emerging sensor and digital technologies offer a promising approach for direct phenotyping of plants in farm environments. This study investigates a learning-based phenotyping method using the Neural Radiance Field to achieve accurate in-situ phenotyping of pepper plants in greenhouse environments. To quantitatively evaluate the performance of this method, traditional point cloud registration on 3D scanning data is implemented for comparison. Experimental result shows that NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the 3D scanning methods. The mean distance error between the scanner-based method and the NeRF-based method is 0.865mm. This study shows that the learning-based NeRF method achieves similar accuracy to 3D scanning-based methods but with improved scalability and robustness.</li>
</ul>

<h3>Title: Knowledge-guided Machine Learning: Current Trends and Future Prospects</h3>
<ul>
<li><strong>Authors: </strong>Anuj Karpatne, Xiaowei Jia, Vipin Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15989">https://arxiv.org/abs/2403.15989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15989">https://arxiv.org/pdf/2403.15989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15989]] Knowledge-guided Machine Learning: Current Trends and Future Prospects(https://arxiv.org/abs/2403.15989)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.</li>
</ul>

<h3>Title: BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, Zhiwei Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15992">https://arxiv.org/abs/2403.15992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15992">https://arxiv.org/pdf/2403.15992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15992]] BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval(https://arxiv.org/abs/2403.15992)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of large language models to advance the field of medical image retrieval beyond existing text-image retrieval solutions. It marks our preliminary step towards developing a system capable of facilitating text-to-image, image-to-text, and keyword-based retrieval tasks.</li>
</ul>

<h3>Title: Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial  Expression Spotting</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Deng, Hideaki Hayashi, Hajime Nagahara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15994">https://arxiv.org/abs/2403.15994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15994">https://arxiv.org/pdf/2403.15994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15994]] Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial  Expression Spotting(https://arxiv.org/abs/2403.15994)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Facial expression spotting is a significant but challenging task in facial expression analysis. The accuracy of expression spotting is affected not only by irrelevant facial movements but also by the difficulty of perceiving subtle motions in micro-expressions. In this paper, we propose a Multi-Scale Spatio-Temporal Graph Convolutional Network (SpoT-GCN) for facial expression spotting. To extract more robust motion features, we track both short- and long-term motion of facial muscles in compact sliding windows whose window length adapts to the temporal receptive field of the network. This strategy, termed the receptive field adaptive sliding window strategy, effectively magnifies the motion features while alleviating the problem of severe head movement. The subtle motion features are then converted to a facial graph representation, whose spatio-temporal graph patterns are learned by a graph convolutional network. This network learns both local and global features from multiple scales of facial graph structures using our proposed facial local graph pooling (FLGP). Furthermore, we introduce supervised contrastive learning to enhance the discriminative capability of our model for difficult-to-classify frames. The experimental results on the SAMM-LV and CAS(ME)^2 datasets demonstrate that our method achieves state-of-the-art performance, particularly in micro-expression spotting. Ablation studies further verify the effectiveness of our proposed modules.</li>
</ul>

<h3>Title: SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal  Visual Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Xiaojun Hou, Jiazheng Xing, Yijie Qian, Yaowei Guo, Shuo Xin, Junhao Chen, Kai Tang, Mengmeng Wang, Zhengkai Jiang, Liang Liu, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16002">https://arxiv.org/abs/2403.16002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16002">https://arxiv.org/pdf/2403.16002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16002]] SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal  Visual Object Tracking(https://arxiv.org/abs/2403.16002)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Multimodal Visual Object Tracking (VOT) has recently gained significant attention due to its robustness. Early research focused on fully fine-tuning RGB-based trackers, which was inefficient and lacked generalized representation due to the scarcity of multimodal data. Therefore, recent studies have utilized prompt tuning to transfer pre-trained RGB-based trackers to multimodal data. However, the modality gap limits pre-trained knowledge recall, and the dominance of the RGB modality persists, preventing the full utilization of information from other modalities. To address these issues, we propose a novel symmetric multimodal tracking framework called SDSTrack. We introduce lightweight adaptation for efficient fine-tuning, which directly transfers the feature extraction ability from RGB to other domains with a small number of trainable parameters and integrates multimodal features in a balanced, symmetric manner. Furthermore, we design a complementary masked patch distillation strategy to enhance the robustness of trackers in complex environments, such as extreme weather, poor imaging, and sensor failure. Extensive experiments demonstrate that SDSTrack outperforms state-of-the-art methods in various multimodal tracking scenarios, including RGB+Depth, RGB+Thermal, and RGB+Event tracking, and exhibits impressive results in extreme conditions. Our source code is available at https://github.com/hoqolo/SDSTrack.</li>
</ul>

<h3>Title: A Federated Parameter Aggregation Method for Node Classification Tasks  with Different Graph Network Structures</h3>
<ul>
<li><strong>Authors: </strong>Hao Song, Jiacheng Yao, Zhengxi Li, Shaocong Xu, Shibo Jin, Jiajun Zhou, Chenbo Fu, Qi Xuan, Shanqing Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16004">https://arxiv.org/abs/2403.16004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16004">https://arxiv.org/pdf/2403.16004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16004]] A Federated Parameter Aggregation Method for Node Classification Tasks  with Different Graph Network Structures(https://arxiv.org/abs/2403.16004)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, robust, membership infer, federate</a></li>
<li><strong>Abstract: </strong>Over the past few years, federated learning has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy. However, in the area of graph neural networks, the nodes and network structures of graphs held by clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario. Therefore, this work proposes a federated aggregation method FLGNN applied to various graph federation scenarios and investigates the aggregation effect of parameter sharing at each layer of the graph neural network model. The effectiveness of the federated aggregation method FLGNN is verified by experiments on real datasets. Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and differential privacy defense experiments. The results show that FLGNN performs good robustness, and the success rate of privacy theft is further reduced by adding differential privacy defense methods.</li>
</ul>

<h3>Title: CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral  Therapy-based Mental Health Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Na</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16008">https://arxiv.org/abs/2403.16008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16008">https://arxiv.org/pdf/2403.16008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16008]] CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral  Therapy-based Mental Health Question Answering(https://arxiv.org/abs/2403.16008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent advancements in artificial intelligence highlight the potential of language models in psychological health support. While models trained on data from mental health service platform have achieved preliminary success, challenges persist in areas such as data scarcity, quality, and ensuring a solid foundation in psychological techniques. To address these challenges, this study introduces a novel approach to enhance the precision and efficacy of psychological support through large language models. Specifically, we design a specific prompt derived from principles of Cognitive Behavioral Therapy (CBT) and have generated the CBT QA dataset, specifically for Chinese psychological health Q&A based on CBT structured intervention strategies. Unlike previous methods, our dataset emphasizes professional and structured response. Utilizing this dataset, we fine-tuned the large language model, giving birth to CBT-LLM, the large-scale language model specifically designed for Cognitive Behavioral Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in generating structured, professional, and highly relevant responses in psychological health support tasks, showcasing its practicality and quality. The model is available on Hugging Face: https://huggingface.co/Hongbin37/CBT-LLM.</li>
</ul>

<h3>Title: SM2C: Boost the Semi-supervised Segmentation for Medical Image by using  Meta Pseudo Labels and Mixed Images</h3>
<ul>
<li><strong>Authors: </strong>Yifei Wang, Chuhong Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16009">https://arxiv.org/abs/2403.16009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16009">https://arxiv.org/pdf/2403.16009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16009]] SM2C: Boost the Semi-supervised Segmentation for Medical Image by using  Meta Pseudo Labels and Mixed Images(https://arxiv.org/abs/2403.16009)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recently, machine learning-based semantic segmentation algorithms have demonstrated their potential to accurately segment regions and contours in medical images, allowing the precise location of anatomical structures and abnormalities. Although medical images are difficult to acquire and annotate, semi-supervised learning methods are efficient in dealing with the scarcity of labeled data. However, overfitting is almost inevitable due to the limited images for training. Furthermore, the intricate shapes of organs and lesions in medical images introduce additional complexity in different cases, preventing networks from acquiring a strong ability to generalize. To this end, we introduce a novel method called Scaling-up Mix with Multi-Class (SM2C). This method uses three strategies - scaling-up image size, multi-class mixing, and object shape jittering - to improve the ability to learn semantic features within medical images. By diversifying the shape of the segmentation objects and enriching the semantic information within each sample, the SM2C demonstrates its potential, especially in the training of unlabelled data. Extensive experiments demonstrate the effectiveness of the SM2C on three benchmark medical image segmentation datasets. The proposed framework shows significant improvements over state-of-the-art counterparts.</li>
</ul>

<h3>Title: Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)</h3>
<ul>
<li><strong>Authors: </strong>Eyoel Gebre, Krishna Saxena, Timothy Tran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16016">https://arxiv.org/abs/2403.16016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16016">https://arxiv.org/pdf/2403.16016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16016]] Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)(https://arxiv.org/abs/2403.16016)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image inpainting is the process of taking an image and generating lost or intentionally occluded portions. Inpainting has countless applications including restoring previously damaged pictures, restoring the quality of images that have been degraded due to compression, and removing unwanted objects/text. Modern inpainting techniques have shown remarkable ability in generating sensible completions for images with mask occlusions. In our paper, an overview of the progress of inpainting techniques will be provided, along with identifying current leading approaches, focusing on their strengths and weaknesses. A critical gap in these existing models will be addressed, focusing on the ability to prompt and control what exactly is generated. We will additionally justify why we think this is the natural next progressive step that inpainting models must take, and provide multiple approaches to implementing this functionality. Finally, we will evaluate the results of our approaches by qualitatively checking whether they generate high-quality images that correctly inpaint regions with the objects that they are instructed to produce.</li>
</ul>

<h3>Title: PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for  Faster Inference</h3>
<ul>
<li><strong>Authors: </strong>Tanvir Mahmud, Burhaneddin Yaman, Chun-Hao Liu, Diana Marculescu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16020">https://arxiv.org/abs/2403.16020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16020">https://arxiv.org/pdf/2403.16020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16020]] PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for  Faster Inference(https://arxiv.org/abs/2403.16020)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As deep neural networks evolve from convolutional neural networks (ConvNets) to advanced vision transformers (ViTs), there is an increased need to eliminate redundant data for faster processing without compromising accuracy. Previous methods are often architecture-specific or necessitate re-training, restricting their applicability with frequent model updates. To solve this, we first introduce a novel property of lightweight ConvNets: their ability to identify key discriminative patch regions in images, irrespective of model's final accuracy or size. We demonstrate that fully-connected layers are the primary bottleneck for ConvNets performance, and their suppression with simple weight recalibration markedly enhances discriminative patch localization performance. Using this insight, we introduce PaPr, a method for substantially pruning redundant patches with minimal accuracy loss using lightweight ConvNets across a variety of deep learning architectures, including ViTs, ConvNets, and hybrid transformers, without any re-training. Moreover, the simple early-stage one-step patch pruning with PaPr enhances existing patch reduction methods. Through extensive testing on diverse architectures, PaPr achieves significantly higher accuracy over state-of-the-art patch reduction methods with similar FLOP count reduction. More specifically, PaPr reduces about 70% of redundant patches in videos with less than 0.8% drop in accuracy, and up to 3.7x FLOPs reduction, which is a 15% more reduction with 2.5% higher accuracy.</li>
</ul>

<h3>Title: A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA</h3>
<ul>
<li><strong>Authors: </strong>Ayush Thakur, Rashmi Vashisth</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16024">https://arxiv.org/abs/2403.16024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16024">https://arxiv.org/pdf/2403.16024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16024]] A Unified Module for Accelerating STABLE-DIFFUSION: LCM-LORA(https://arxiv.org/abs/2403.16024)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive study on the unified module for accelerating stable-diffusion processes, specifically focusing on the lcm-lora module. Stable-diffusion processes play a crucial role in various scientific and engineering domains, and their acceleration is of paramount importance for efficient computational performance. The standard iterative procedures for solving fixed-source discrete ordinates problems often exhibit slow convergence, particularly in optically thick scenarios. To address this challenge, unconditionally stable diffusion-acceleration methods have been developed, aiming to enhance the computational efficiency of transport equations and discrete ordinates problems. This study delves into the theoretical foundations and numerical results of unconditionally stable diffusion synthetic acceleration methods, providing insights into their stability and performance for model discrete ordinates problems. Furthermore, the paper explores recent advancements in diffusion model acceleration, including on device acceleration of large diffusion models via gpu aware optimizations, highlighting the potential for significantly improved inference latency. The results and analyses in this study provide important insights into stable diffusion processes and have important ramifications for the creation and application of acceleration methods specifically, the lcm-lora module in a variety of computing environments.</li>
</ul>

<h3>Title: VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections</h3>
<ul>
<li><strong>Authors: </strong>Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich, Jingrui He, Bo Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16030">https://arxiv.org/abs/2403.16030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16030">https://arxiv.org/pdf/2403.16030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16030]] VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections(https://arxiv.org/abs/2403.16030)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from complex graph topological information and makes heavy feature engineering offline and independent, such that mini-batch training of graph transformers is possible by loading each node's token list in batches. We further prove this PPR tokenization is viable as a graph convolution network with a fixed polynomial filter and jumping knowledge. However, only using personalized PageRank may limit information carried by a token list, which could not support different graph inductive biases for model training. To this end, (2) we rewire graphs by introducing multiple types of virtual connections through structure- and content-based super nodes that enable PPR tokenization to encode local and global contexts, long-range interaction, and heterophilous information into each node's token list, and then formalize our Virtual Connection Ranking based Graph Transformer (VCR-Graphormer).</li>
</ul>

<h3>Title: Node Classification via Semantic-Structural Attention-Enhanced Graph  Convolutional Networks</h3>
<ul>
<li><strong>Authors: </strong>Hongyin Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16033">https://arxiv.org/abs/2403.16033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16033">https://arxiv.org/pdf/2403.16033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16033]] Node Classification via Semantic-Structural Attention-Enhanced Graph  Convolutional Networks(https://arxiv.org/abs/2403.16033)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, extraction</a></li>
<li><strong>Abstract: </strong>Graph data, also known as complex network data, is omnipresent across various domains and applications. Prior graph neural network models primarily focused on extracting task-specific structural features through supervised learning objectives, but they fell short in capturing the inherent semantic and structural features of the entire graph. In this paper, we introduce the semantic-structural attention-enhanced graph convolutional network (SSA-GCN), which not only models the graph structure but also extracts generalized unsupervised features to enhance vertex classification performance. The SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic information through unsupervised feature extraction from a knowledge graph perspective; secondly, it obtains structural information through unsupervised feature extraction from a complex network perspective; and finally, it integrates these features through a cross-attention mechanism. By leveraging these features, we augment the graph convolutional network, thereby enhancing the model's generalization capabilities. Our experiments on the Cora and CiteSeer datasets demonstrate the performance improvements achieved by our proposed method. Furthermore, our approach also exhibits excellent accuracy under privacy settings, making it a robust and effective solution for graph data analysis.</li>
</ul>

<h3>Title: Monotonic Paraphrasing Improves Generalization of Language Model  Prompting</h3>
<ul>
<li><strong>Authors: </strong>Qin Liu, Fei Wang, Nan Xu, Tianyi Yan, Tao Meng, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16038">https://arxiv.org/abs/2403.16038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16038">https://arxiv.org/pdf/2403.16038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16038]] Monotonic Paraphrasing Improves Generalization of Language Model  Prompting(https://arxiv.org/abs/2403.16038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity. However, finding the prompt with the lowest perplexity is challenging, given the enormous space of possible prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara), an end-to-end decoding strategy that paraphrases given prompts or instructions into their lower perplexity counterparts based on an ensemble of a paraphrase LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or instruction executor) that constrains the generation for lower perplexity. The ensemble decoding process can efficiently paraphrase the original prompt without altering its semantic meaning, while monotonically decreasing the perplexity of each generation as calculated by the target LM. We explore in detail both greedy and search-based decoding as two alternative decoding schemes of MonoPara. Notably, MonoPara does not require any training and can monotonically lower the perplexity of the paraphrased prompt or instruction, leading to improved performance of zero-shot LM prompting as evaluated on a wide selection of tasks. In addition, MonoPara is also shown to effectively improve LMs' generalization on perturbed and unseen task instructions.</li>
</ul>

<h3>Title: Semantic Is Enough: Only Semantic Information For NeRF Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Ruibo Wang, Song Zhang, Ping Huang, Donghai Zhang, Wei Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16043">https://arxiv.org/abs/2403.16043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16043">https://arxiv.org/pdf/2403.16043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16043]] Semantic Is Enough: Only Semantic Information For NeRF Reconstruction(https://arxiv.org/abs/2403.16043)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent research that combines implicit 3D representation with semantic information, like Semantic-NeRF, has proven that NeRF model could perform excellently in rendering 3D structures with semantic labels. This research aims to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing solely on semantic output and removing the RGB output component. We reformulate the model and its training procedure to leverage only the cross-entropy loss between the model semantic output and the ground truth semantic images, removing the colour data traditionally used in the original Semantic-NeRF approach. We then conduct a series of identical experiments using the original and the modified Semantic-NeRF model. Our primary objective is to obverse the impact of this modification on the model performance by Semantic-NeRF, focusing on tasks such as scene understanding, object detection, and segmentation. The results offer valuable insights into the new way of rendering the scenes and provide an avenue for further research and development in semantic-focused 3D scene understanding.</li>
</ul>

<h3>Title: A General and Efficient Federated Split Learning with Pre-trained Image  Transformers for Heterogeneous Data</h3>
<ul>
<li><strong>Authors: </strong>Yifan Shi, Yuhui Zhang, Ziyue Huang, Xiaofeng Yang, Li Shen, Wei Chen, Xueqian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16050">https://arxiv.org/abs/2403.16050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16050">https://arxiv.org/pdf/2403.16050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16050]] A General and Efficient Federated Split Learning with Pre-trained Image  Transformers for Heterogeneous Data(https://arxiv.org/abs/2403.16050)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate, transformer</a></li>
<li><strong>Abstract: </strong>Federated Split Learning (FSL) is a promising distributed learning paradigm in practice, which gathers the strengths of both Federated Learning (FL) and Split Learning (SL) paradigms, to ensure model privacy while diminishing the resource overhead of each client, especially on large transformer models in a resource-constrained environment, e.g., Internet of Things (IoT). However, almost all works merely investigate the performance with simple neural network models in FSL. Despite the minor efforts focusing on incorporating Vision Transformers (ViT) as model architectures, they train ViT from scratch, thereby leading to enormous training overhead in each device with limited resources. Therefore, in this paper, we harness Pre-trained Image Transformers (PITs) as the initial model, coined FES-PIT, to accelerate the training process and improve model robustness. Furthermore, we propose FES-PTZO to hinder the gradient inversion attack, especially having the capability compatible with black-box scenarios, where the gradient information is unavailable. Concretely, FES-PTZO approximates the server gradient by utilizing a zeroth-order (ZO) optimization, which replaces the backward propagation with just one forward process. Empirically, we are the first to provide a systematic evaluation of FSL methods with PITs in real-world datasets, different partial device participations, and heterogeneous data splits. Our experiments verify the effectiveness of our algorithms.</li>
</ul>

<h3>Title: Segment Anything Model for Road Network Graph Extraction</h3>
<ul>
<li><strong>Authors: </strong>Congrui Hetang, Haoru Xue, Cindy Le, Tianwei Yue, Wenping Wang, Yihui He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16051">https://arxiv.org/abs/2403.16051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16051">https://arxiv.org/pdf/2403.16051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16051]] Segment Anything Model for Road Network Graph Extraction(https://arxiv.org/abs/2403.16051)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We propose SAM-Road, an adaptation of the Segment Anything Model (SAM) for extracting large-scale, vectorized road network graphs from satellite imagery. To predict graph geometry, we formulate it as a dense semantic segmentation task, leveraging the inherent strengths of SAM. The image encoder of SAM is fine-tuned to produce probability masks for roads and intersections, from which the graph vertices are extracted via simple non-maximum suppression. To predict graph topology, we designed a lightweight transformer-based graph neural network, which leverages the SAM image embeddings to estimate the edge existence probabilities between vertices. Our approach directly predicts the graph vertices and edges for large regions without expensive and complex post-processing heuristics, and is capable of building complete road network graphs spanning multiple square kilometers in a matter of seconds. With its simple, straightforward, and minimalist design, SAM-Road achieves comparable accuracy with the state-of-the-art method RNGDet++, while being 40 times faster on the City-scale dataset. We thus demonstrate the power of a foundational vision model when applied to a graph learning task. The code is available at https://github.com/htcr/sam_road.</li>
</ul>

<h3>Title: Qibo: A Large Language Model for Traditional Chinese Medicine</h3>
<ul>
<li><strong>Authors: </strong>Heyi Zhang, Xin Wang, Zhaopeng Meng, Yongzhe Jia, Dawei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16056">https://arxiv.org/abs/2403.16056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16056">https://arxiv.org/pdf/2403.16056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16056]] Qibo: A Large Language Model for Traditional Chinese Medicine(https://arxiv.org/abs/2403.16056)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, which is a specialized tool for evaluating the performance of LLMs in the TCM domain. This tool will provide an important basis for quantifying and comparing the understanding and application capabilities of different models in the field of traditional Chinese medicine, and provide guidance for future research directions and practical applications of intelligent assistants for traditional Chinese medicine. Finally, we conducted sufficient experiments to prove that Qibo has good performance in the field of traditional Chinese medicine.</li>
</ul>

<h3>Title: Port Forwarding Services Are Forwarding Security Risks</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Wang, Yue Xue, Xuan Feng, Chao Zhou, Xianghang Mi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16060">https://arxiv.org/abs/2403.16060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16060">https://arxiv.org/pdf/2403.16060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16060]] Port Forwarding Services Are Forwarding Security Risks(https://arxiv.org/abs/2403.16060)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>We conduct the first comprehensive security study on representative port forwarding services (PFS), which emerge in recent years and make the web services deployed in internal networks available on the Internet along with better usability but less complexity compared to traditional techniques (e.g., NAT traversal techniques). Our study is made possible through a set of novel methodologies, which are designed to uncover the technical mechanisms of PFS, experiment attack scenarios for PFS protocols, automatically discover and snapshot port-forwarded websites (PFWs) at scale, and classify PFWs into well-observed categories. Leveraging these methodologies, we have observed the widespread adoption of PFS with millions of PFWs distributed across tens of thousands of ISPs worldwide. Furthermore, 32.31% PFWs have been classified into website categories that serve access to critical data or infrastructure, such as, web consoles for industrial control systems, IoT controllers, code repositories, and office automation systems. And 18.57% PFWs didn't enforce any access control for external visitors. Also identified are two types of attacks inherent in the protocols of Oray (one well-adopted PFS provider), and the notable abuse of PFSes by malicious actors in activities such as malware distribution, botnet operation and phishing.</li>
</ul>

<h3>Title: Robust Diffusion Models for Adversarial Purification</h3>
<ul>
<li><strong>Authors: </strong>Guang Lin, Zerui Tao, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16067">https://arxiv.org/abs/2403.16067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16067">https://arxiv.org/pdf/2403.16067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16067]] Robust Diffusion Models for Adversarial Purification(https://arxiv.org/abs/2403.16067)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT). However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well. Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive. We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also mitigate the accuracy-robustness trade-off of DMs for the first time, which also provides DM-based AP an efficient adaptive ability to new attacks. Extensive experiments are conducted to demonstrate that our method achieves the state-of-the-art results and exhibits generalization against different attacks.</li>
</ul>

<h3>Title: Argument Quality Assessment in the Age of Instruction-Following Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Henning Wachsmuth, Gabriella Lapesa, Elena Cabrio, Anne Lauscher, Joonsuk Park, Eva Maria Vecchi, Serena Villata, Timon Ziegenbein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16084">https://arxiv.org/abs/2403.16084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16084">https://arxiv.org/pdf/2403.16084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16084]] Argument Quality Assessment in the Age of Instruction-Following Large  Language Models(https://arxiv.org/abs/2403.16084)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The computational treatment of arguments on controversial issues has been subject to extensive NLP research, due to its envisioned impact on opinion formation, decision making, writing education, and the like. A critical task in any such application is the assessment of an argument's quality - but it is also particularly challenging. In this position paper, we start from a brief survey of argument quality research, where we identify the diversity of quality notions and the subjectiveness of their perception as the main hurdles towards substantial progress on argument quality assessment. We argue that the capabilities of instruction-following large language models (LLMs) to leverage knowledge across contexts enable a much more reliable assessment. Rather than just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they need to be instructed systematically with argumentation theories and scenarios as well as with ways to solve argument-related problems. We discuss the real-world opportunities and ethical issues emerging thereby.</li>
</ul>

<h3>Title: Are NeRFs ready for autonomous driving? Towards closing the  real-to-simulation gap</h3>
<ul>
<li><strong>Authors: </strong>Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16092">https://arxiv.org/abs/2403.16092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16092">https://arxiv.org/pdf/2403.16092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16092]] Are NeRFs ready for autonomous driving? Towards closing the  real-to-simulation gap(https://arxiv.org/abs/2403.16092)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different pre-training strategies. Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators.</li>
</ul>

<h3>Title: A Transformer approach for Electricity Price Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Oscar Llorente Gonzalez, Jose Portela</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16108">https://arxiv.org/abs/2403.16108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16108">https://arxiv.org/pdf/2403.16108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16108]] A Transformer approach for Electricity Price Forecasting(https://arxiv.org/abs/2403.16108)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.</li>
</ul>

<h3>Title: EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Xiangpeng Yang, Linchao Zhu, Hehe Fan, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16111">https://arxiv.org/abs/2403.16111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16111">https://arxiv.org/pdf/2403.16111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16111]] EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing(https://arxiv.org/abs/2403.16111)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current diffusion-based video editing primarily focuses on local editing (\textit{e.g.,} object/background editing) or global style editing by utilizing various dense correspondences. However, these methods often fail to accurately edit the foreground and background simultaneously while preserving the original layout. We find that the crux of the issue stems from the imprecise distribution of attention weights across designated regions, including inaccurate text-to-attribute control and attention leakage. To tackle this issue, we introduce EVA, a \textbf{zero-shot} and \textbf{multi-attribute} video editing framework tailored for human-centric videos with complex motions. We incorporate a Spatial-Temporal Layout-Guided Attention mechanism that leverages the intrinsic positive and negative correspondences of cross-frame diffusion features. To avoid attention leakage, we utilize these correspondences to boost the attention scores of tokens within the same attribute across all video frames while limiting interactions between tokens of different attributes in the self-attention layer. For precise text-to-attribute manipulation, we use discrete text embeddings focused on specific layout areas within the cross-attention layer. Benefiting from the precise attention weight distribution, EVA can be easily generalized to multi-object editing scenarios and achieves accurate identity mapping. Extensive experiments demonstrate EVA achieves state-of-the-art results in real-world scenarios. Full results are provided at https://knightyxp.github.io/EVA/</li>
</ul>

<h3>Title: Enhancing Video Transformers for Action Understanding with VLM-aided  Training</h3>
<ul>
<li><strong>Authors: </strong>Hui Lu, Hu Jian, Ronald Poppe, Albert Ali Salah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16128">https://arxiv.org/abs/2403.16128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16128">https://arxiv.org/pdf/2403.16128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16128]] Enhancing Video Transformers for Action Understanding with VLM-aided  Training(https://arxiv.org/abs/2403.16128)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Owing to their ability to extract relevant spatio-temporal video embeddings, Vision Transformers (ViTs) are currently the best performing models in video action understanding. However, their generalization over domains or datasets is somewhat limited. In contrast, Visual Language Models (VLMs) have demonstrated exceptional generalization performance, but are currently unable to process videos. Consequently, they cannot extract spatio-temporal patterns that are crucial for action understanding. In this paper, we propose the Four-tiered Prompts (FTP) framework that takes advantage of the complementary strengths of ViTs and VLMs. We retain ViTs' strong spatio-temporal representation ability but improve the visual encodings to be more comprehensive and general by aligning them with VLM outputs. The FTP framework adds four feature processors that focus on specific aspects of human action in videos: action category, action components, action description, and context information. The VLMs are only employed during training, and inference incurs a minimal computation cost. Our approach consistently yields state-of-the-art performance. For instance, we achieve remarkable top-1 accuracy of 93.8% on Kinetics-400 and 83.4% on Something-Something V2, surpassing VideoMAEv2 by 2.8% and 2.6%, respectively.</li>
</ul>

<h3>Title: A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation</h3>
<ul>
<li><strong>Authors: </strong>Miuru Abeysiriwardana, Deshan Sumanathilaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16129">https://arxiv.org/abs/2403.16129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16129">https://arxiv.org/pdf/2403.16129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16129]] A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation(https://arxiv.org/abs/2403.16129)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores techniques that focus on understanding and resolving ambiguity in language within the field of natural language processing (NLP), highlighting the complexity of linguistic phenomena such as polysemy and homonymy and their implications for computational models. Focusing extensively on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from deep learning techniques to leveraging lexical resources and knowledge graphs like WordNet. The paper introduces cutting-edge methodologies like word sense extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy by predicting new word senses. It examines specific applications in biomedical disambiguation and language specific optimisation and discusses the significance of cognitive metaphors in discourse analysis. The research identifies persistent challenges in the field, such as the scarcity of sense annotated corpora and the complexity of informal clinical texts. It concludes by suggesting future directions, including using large language models, visual WSD, and multilingual WSD systems, emphasising the ongoing evolution in addressing lexical complexities in NLP. This thinking perspective highlights the advancement in this field to enable computers to understand language more accurately.</li>
</ul>

<h3>Title: Salience DETR: Enhancing Detection Transformer with Hierarchical  Salience Filtering Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xiuquan Hou, Meiqin Liu, Senlin Zhang, Ping Wei, Badong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16131">https://arxiv.org/abs/2403.16131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16131">https://arxiv.org/pdf/2403.16131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16131]] Salience DETR: Enhancing Detection Transformer with Hierarchical  Salience Filtering Refinement(https://arxiv.org/abs/2403.16131)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>DETR-like methods have significantly increased detection performance in an end-to-end manner. The mainstream two-stage frameworks of them perform dense self-attention and select a fraction of queries for sparse cross-attention, which is proven effective for improving performance but also introduces a heavy computational burden and high dependence on stable query selection. This paper demonstrates that suboptimal two-stage selection strategies result in scale bias and redundancy due to the mismatch between selected queries and objects in two-stage initialization. To address these issues, we propose hierarchical salience filtering refinement, which performs transformer encoding only on filtered discriminative queries, for a better trade-off between computational efficiency and precision. The filtering process overcomes scale bias through a novel scale-independent salience supervision. To compensate for the semantic misalignment among queries, we introduce elaborate query refinement modules for stable two-stage initialization. Based on above improvements, the proposed Salience DETR achieves significant improvements of +4.0% AP, +0.2% AP, +4.4% AP on three challenging task-specific detection datasets, as well as 49.2% AP on COCO 2017 with less FLOPs. The code is available at https://github.com/xiuqhou/Salience-DETR.</li>
</ul>

<h3>Title: A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A  Knowledge-Based Perspective</h3>
<ul>
<li><strong>Authors: </strong>Ziwen Zhao, Yuhua Li, Yixiong Zou, Ruixuan Li, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16137">https://arxiv.org/abs/2403.16137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16137">https://arxiv.org/pdf/2403.16137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16137]] A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A  Knowledge-Based Perspective(https://arxiv.org/abs/2403.16137)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge. In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/Pretext.</li>
</ul>

<h3>Title: A Little Leak Will Sink a Great Ship: Survey of Transparency for Large  Language Models from Start to Finish</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Kaneko, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16139">https://arxiv.org/abs/2403.16139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16139">https://arxiv.org/pdf/2403.16139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16139]] A Little Leak Will Sink a Great Ship: Survey of Transparency for Large  Language Models from Start to Finish(https://arxiv.org/abs/2403.16139)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained on massive web-crawled corpora. This poses risks of leakage, including personal information, copyrighted texts, and benchmark datasets. Such leakage leads to undermining human trust in AI due to potential unauthorized generation of content or overestimation of performance. We establish the following three criteria concerning the leakage issues: (1) leakage rate: the proportion of leaked data in training data, (2) output rate: the ease of generating leaked data, and (3) detection rate: the detection performance of leaked versus non-leaked data. Despite the leakage rate being the origin of data leakage issues, it is not understood how it affects the output rate and detection rate. In this paper, we conduct an experimental survey to elucidate the relationship between the leakage rate and both the output rate and detection rate for personal information, copyrighted texts, and benchmark data. Additionally, we propose a self-detection approach that uses few-shot learning in which LLMs detect whether instances are present or absent in their training data, in contrast to previous methods that do not employ explicit learning. To explore the ease of generating leaked information, we create a dataset of prompts designed to elicit personal information, copyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs produce leaked information in most cases despite less such data in their training set. This indicates even small amounts of leaked data can greatly affect outputs. Our self-detection method showed superior performance compared to existing detection methods.</li>
</ul>

<h3>Title: Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes</h3>
<ul>
<li><strong>Authors: </strong>Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16141">https://arxiv.org/abs/2403.16141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16141">https://arxiv.org/pdf/2403.16141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16141]] Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes(https://arxiv.org/abs/2403.16141)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics. However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present. In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds. Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies. This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation. To assess our methodology, we created an urban scene dataset masked with moving objects. Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively.</li>
</ul>

<h3>Title: A Survey on Consumer IoT Traffic: Security and Privacy</h3>
<ul>
<li><strong>Authors: </strong>Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Fangming Wang, Yu Zhang, Zheli Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16149">https://arxiv.org/abs/2403.16149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16149">https://arxiv.org/pdf/2403.16149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16149]] A Survey on Consumer IoT Traffic: Security and Privacy(https://arxiv.org/abs/2403.16149)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious traffic analysis, security analysis, and measurement. At last, we discuss the new challenges and future research directions.</li>
</ul>

<h3>Title: Towards Online Real-Time Memory-based Video Inpainting Transformers</h3>
<ul>
<li><strong>Authors: </strong>Guillaume Thiry, Hao Tang, Radu Timofte, Luc Van Gool</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16161">https://arxiv.org/abs/2403.16161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16161">https://arxiv.org/pdf/2403.16161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16161]] Towards Online Real-Time Memory-based Video Inpainting Transformers(https://arxiv.org/abs/2403.16161)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Video inpainting tasks have seen significant improvements in recent years with the rise of deep neural networks and, in particular, vision transformers. Although these models show promising reconstruction quality and temporal consistency, they are still unsuitable for live videos, one of the last steps to make them completely convincing and usable. The main limitations are that these state-of-the-art models inpaint using the whole video (offline processing) and show an insufficient frame rate. In our approach, we propose a framework to adapt existing inpainting transformers to these constraints by memorizing and refining redundant computations while maintaining a decent inpainting quality. Using this framework with some of the most recent inpainting models, we show great online results with a consistent throughput above 20 frames per second. The code and pretrained models will be made available upon acceptance.</li>
</ul>

<h3>Title: An Analytic Solution to Covariance Propagation in Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Oren Wright, Yorie Nakahira, José M. F. Moura</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16163">https://arxiv.org/abs/2403.16163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16163">https://arxiv.org/pdf/2403.16163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16163]] An Analytic Solution to Covariance Propagation in Neural Networks(https://arxiv.org/abs/2403.16163)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.</li>
</ul>

<h3>Title: Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method</h3>
<ul>
<li><strong>Authors: </strong>Jie Tian, Lingxiao Yang, Ran Ji, Yuexin Ma, Lan Xu, Jingyi Yu, Ye Shi, Jingya Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16169">https://arxiv.org/abs/2403.16169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16169">https://arxiv.org/pdf/2403.16169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16169]] Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method(https://arxiv.org/abs/2403.16169)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Gaze plays a crucial role in revealing human attention and intention, shedding light on the cognitive processes behind human actions. The integration of gaze guidance with the dynamics of hand-object interactions boosts the accuracy of human motion prediction. However, the lack of datasets that capture the intricate relationship and consistency among gaze, hand, and object movements remains a substantial hurdle. In this paper, we introduce the first Gaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task for synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI, features simultaneous 3D modeling of gaze, hand, and object interactions, comprising 479 sequences with an average duration of 19.1 seconds, 812 sub-sequences, and 33 objects of various sizes. We propose a hierarchical framework centered on a gaze-guided hand-object interaction diffusion model, named GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions into spatial-temporal features and goal pose conditions at different levels of information granularity. During the diffusion phase, two gaze-conditioned diffusion models are stacked to simplify the complex synthesis of hand-object motions. Here, the object motion diffusion model generates sequences of object motions based on gaze conditions, while the hand motion diffusion model produces hand motions based on the generated object motion. To improve fine-grained goal pose alignment, we introduce a Spherical Gaussian constraint to guide the denoising step. In the subsequent post-diffusion phase, we optimize the generated hand motions using contact consistency. Our extensive experiments highlight the uniqueness of our dataset and the effectiveness of our approach.</li>
</ul>

<h3>Title: Subspace Defense: Discarding Adversarial Perturbations by Learning a  Subspace for Clean Signals</h3>
<ul>
<li><strong>Authors: </strong>Rui Zheng, Yuhao Zhou, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16176">https://arxiv.org/abs/2403.16176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16176">https://arxiv.org/pdf/2403.16176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16176]] Subspace Defense: Discarding Adversarial Perturbations by Learning a  Subspace for Clean Signals(https://arxiv.org/abs/2403.16176)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks that place carefully crafted perturbations on normal examples to fool DNNs. To better understand such attacks, a characterization of the features carried by adversarial examples is needed. In this paper, we tackle this challenge by inspecting the subspaces of sample features through spectral analysis. We first empirically show that the features of either clean signals or adversarial perturbations are redundant and span in low-dimensional linear subspaces respectively with minimal overlap, and the classical low-dimensional subspace projection can suppress perturbation features out of the subspace of clean signals. This makes it possible for DNNs to learn a subspace where only features of clean signals exist while those of perturbations are discarded, which can facilitate the distinction of adversarial examples. To prevent the residual perturbations that is inevitable in subspace learning, we propose an independence criterion to disentangle clean signals from perturbations. Experimental results show that the proposed strategy enables the model to inherently suppress adversaries, which not only boosts model robustness but also motivates new directions of effective adversarial defense.</li>
</ul>

<h3>Title: ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Zequan Liu, Jiawen Lyn, Wei Zhu, Xing Tian, Yvette Graham</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16187">https://arxiv.org/abs/2403.16187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16187">https://arxiv.org/pdf/2403.16187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16187]] ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language  Models(https://arxiv.org/abs/2403.16187)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters.</li>
</ul>

<h3>Title: Cross-domain Multi-modal Few-shot Object Detection via Rich Text</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Shangguan, Daniel Seita, Mohammad Rostami</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16188">https://arxiv.org/abs/2403.16188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16188">https://arxiv.org/pdf/2403.16188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16188]] Cross-domain Multi-modal Few-shot Object Detection via Rich Text(https://arxiv.org/abs/2403.16188)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Cross-modal feature extraction and integration have led to steady performance improvements in few-shot learning tasks due to generating richer features. However, existing multi-modal object detection (MM-OD) methods degrade when facing significant domain-shift and are sample insufficient. We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate domain shift. Specifically, we study the Cross-Domain few-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based multi-modal few-shot object detection method that utilizes rich text semantic information as an auxiliary modality to achieve domain adaptation in the context of FSOD. Our proposed network contains (i) a multi-modal feature aggregation module that aligns the vision and language support feature embeddings and (ii) a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce multi-modal feature alignment and thus to enhance the model's language understanding capability. We evaluate our model on common standard cross-domain object detection datasets and demonstrate that our approach considerably outperforms existing FSOD methods.</li>
</ul>

<h3>Title: Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised  Landmark Discovery</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Tourani, Ahmed Alwheibi, Arif Mahmood, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16194">https://arxiv.org/abs/2403.16194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16194">https://arxiv.org/pdf/2403.16194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16194]] Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised  Landmark Discovery(https://arxiv.org/abs/2403.16194)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Unsupervised landmarks discovery (ULD) for an object category is a challenging computer vision problem. In pursuit of developing a robust ULD framework, we explore the potential of a recent paradigm of self-supervised learning algorithms, known as diffusion models. Some recent works have shown that these models implicitly contain important correspondence cues. Towards harnessing the potential of diffusion models for the ULD task, we make the following core contributions. First, we propose a ZeroShot ULD baseline based on simple clustering of random pixel locations with nearest neighbour matching. It delivers better results than existing ULD methods. Second, motivated by the ZeroShot performance, we develop a ULD algorithm based on diffusion features using self-training and clustering which also outperforms prior methods by notable margins. Third, we introduce a new proxy task based on generating latent pose codes and also propose a two-stage clustering mechanism to facilitate effective pseudo-labeling, resulting in a significant performance improvement. Overall, our approach consistently outperforms state-of-the-art methods on four challenging benchmarks AFLW, MAFL, CatHeads and LS3D by significant margins.</li>
</ul>

<h3>Title: Behind the (Digital Crime) Scenes: An MSC Model</h3>
<ul>
<li><strong>Authors: </strong>Mario Raciti, Giampaolo Bella</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16196">https://arxiv.org/abs/2403.16196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16196">https://arxiv.org/pdf/2403.16196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16196]] Behind the (Digital Crime) Scenes: An MSC Model(https://arxiv.org/abs/2403.16196)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Criminal investigations are inherently complex as they typically involve interactions among various actors like investigators, prosecutors, and defendants. The pervasive integration of technology in daily life adds an extra layer of complexity, especially in crimes that involve a digital element. The establishment of digital forensics as a foundational discipline for extracting digital evidence further exacerbates the complex nature of criminal investigations, leading to the proliferation of multiple scenarios. Recognising the need to structure standard operating procedures for the handling of digital evidence, the representation of digital forensics as a protocol emerges as a valuable opportunity to identify security and privacy threats. In this paper, we delineate the protocols that compose digital forensics within a criminal case, formalise them as message sequence charts (MSCs), and identify their functional requirements.</li>
</ul>

<h3>Title: Diffusion Model is a Good Pose Estimator from 3D RF-Vision</h3>
<ul>
<li><strong>Authors: </strong>Junqiao Fan, Jianfei Yang, Yuecong Xu, Lihua Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16198">https://arxiv.org/abs/2403.16198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16198">https://arxiv.org/pdf/2403.16198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16198]] Diffusion Model is a Good Pose Estimator from 3D RF-Vision(https://arxiv.org/abs/2403.16198)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Human pose estimation (HPE) from Radio Frequency vision (RF-vision) performs human sensing using RF signals that penetrate obstacles without revealing privacy (e.g., facial information). Recently, mmWave radar has emerged as a promising RF-vision sensor, providing radar point clouds by processing RF signals. However, the mmWave radar has a limited resolution with severe noise, leading to inaccurate and inconsistent human pose estimation. This work proposes mmDiff, a novel diffusion-based pose estimator tailored for noisy radar data. Our approach aims to provide reliable guidance as conditions to diffusion models. Two key challenges are addressed by mmDiff: (1) miss-detection of parts of human bodies, which is addressed by a module that isolates feature extraction from different body parts, and (2) signal inconsistency due to environmental interference, which is tackled by incorporating prior knowledge of body structure and motion. Several modules are designed to achieve these goals, whose features work as the conditions for the subsequent diffusion model, eliminating the miss-detection and instability of HPE based on RF-vision. Extensive experiments demonstrate that mmDiff outperforms existing methods significantly, achieving state-of-the-art performances on public datasets.</li>
</ul>

<h3>Title: From Discrete to Continuous: Deep Fair Clustering With Transferable  Representations</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16201">https://arxiv.org/abs/2403.16201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16201">https://arxiv.org/pdf/2403.16201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16201]] From Discrete to Continuous: Deep Fair Clustering With Transferable  Representations(https://arxiv.org/abs/2403.16201)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We consider the problem of deep fair clustering, which partitions data into clusters via the representations extracted by deep neural networks while hiding sensitive data attributes. To achieve fairness, existing methods present a variety of fairness-related objective functions based on the group fairness criterion. However, these works typically assume that the sensitive attributes are discrete and do not work for continuous sensitive variables, such as the proportion of the female population in an area. Besides, the potential of the representations learned from clustering tasks to improve performance on other tasks is ignored by existing works. In light of these limitations, we propose a flexible deep fair clustering method that can handle discrete and continuous sensitive attributes simultaneously. Specifically, we design an information bottleneck style objective function to learn fair and clustering-friendly representations. Furthermore, we explore for the first time the transferability of the extracted representations to other downstream tasks. Unlike existing works, we impose fairness at the representation level, which could guarantee fairness for the transferred task regardless of clustering results. To verify the effectiveness of the proposed method, we perform extensive experiments on datasets with discrete and continuous sensitive attributes, demonstrating the advantage of our method in comparison with state-of-the-art methods.</li>
</ul>

<h3>Title: FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial  Temporal Network</h3>
<ul>
<li><strong>Authors: </strong>Geetanjali Sharma, Gaurav Jaswal, Aditya Nigam, Raghavendra Ramachandra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16202">https://arxiv.org/abs/2403.16202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16202">https://arxiv.org/pdf/2403.16202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16202]] FH-SSTNet: Forehead Creases based User Verification using Spatio-Spatial  Temporal Network(https://arxiv.org/abs/2403.16202)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Biometric authentication, which utilizes contactless features, such as forehead patterns, has become increasingly important for identity verification and access management. The proposed method is based on learning a 3D spatio-spatial temporal convolution to create detailed pictures of forehead patterns. We introduce a new CNN model called the Forehead Spatio-Spatial Temporal Network (FH-SSTNet), which utilizes a 3D CNN architecture with triplet loss to capture distinguishing features. We enhance the model's discrimination capability using Arcloss in the network's head. Experimentation on the Forehead Creases version 1 (FH-V1) dataset, containing 247 unique subjects, demonstrates the superior performance of FH-SSTNet compared to existing methods and pre-trained CNNs like ResNet50, especially for forehead-based user verification. The results demonstrate the superior performance of FH-SSTNet for forehead-based user verification, confirming its effectiveness in identity authentication.</li>
</ul>

<h3>Title: Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing</h3>
<ul>
<li><strong>Authors: </strong>Yongqing Liang, Congyi Zhang, Junli Zhao, Wenping Wang, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16207">https://arxiv.org/abs/2403.16207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16207">https://arxiv.org/pdf/2403.16207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16207]] Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing(https://arxiv.org/abs/2403.16207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deducing the 3D face from a skull is an essential but challenging task in forensic science and archaeology. Existing methods for automated facial reconstruction yield inaccurate results, suffering from the non-determinative nature of the problem that a skull with a sparse set of tissue depth cannot fully determine the skinned face. Additionally, their texture-less results require further post-processing stages to achieve a photo-realistic appearance. This paper proposes an end-to-end 3D face reconstruction and exploration tool, providing textured 3D faces for reference. With the help of state-of-the-art text-to-image diffusion models and image-based facial reconstruction techniques, we generate an initial reference 3D face, whose biological profile aligns with the given skull. We then adapt these initial faces to meet the statistical expectations of extruded anatomical landmarks on the skull through an optimization process. The joint statistical distribution of tissue depths is learned on a small set of anatomical landmarks on the skull. To support further adjustment, we propose an efficient face adaptation tool to assist users in tuning tissue depths, either globally or at local regions, while observing plausible visual feedback. Experiments conducted on a real skull-face dataset demonstrated the effectiveness of our proposed pipeline in terms of reconstruction accuracy, diversity, and stability.</li>
</ul>

<h3>Title: Frankenstein: Generating Semantic-Compositional 3D Scenes in One  Tri-Plane</h3>
<ul>
<li><strong>Authors: </strong>Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, Hongdong Li, Pan Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16210">https://arxiv.org/abs/2403.16210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16210">https://arxiv.org/pdf/2403.16210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16210]] Frankenstein: Generating Semantic-Compositional 3D Scenes in One  Tri-Plane(https://arxiv.org/abs/2403.16210)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes. During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes. Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts. The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar cloth re-targeting.</li>
</ul>

<h3>Title: Dual-modal Prior Semantic Guided Infrared and Visible Image Fusion for  Intelligent Transportation System</h3>
<ul>
<li><strong>Authors: </strong>Jing Li, Lu Bai, Bin Yang, Chang Li, Lingfei Ma, Lixin Cui, Edwin R. Hancock</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16227">https://arxiv.org/abs/2403.16227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16227">https://arxiv.org/pdf/2403.16227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16227]] Dual-modal Prior Semantic Guided Infrared and Visible Image Fusion for  Intelligent Transportation System(https://arxiv.org/abs/2403.16227)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Infrared and visible image fusion (IVF) plays an important role in intelligent transportation system (ITS). The early works predominantly focus on boosting the visual appeal of the fused result, and only several recent approaches have tried to combine the high-level vision task with IVF. However, they prioritize the design of cascaded structure to seek unified suitable features and fit different tasks. Thus, they tend to typically bias toward to reconstructing raw pixels without considering the significance of semantic features. Therefore, we propose a novel prior semantic guided image fusion method based on the dual-modality strategy, improving the performance of IVF in ITS. Specifically, to explore the independent significant semantic of each modality, we first design two parallel semantic segmentation branches with a refined feature adaptive-modulation (RFaM) mechanism. RFaM can perceive the features that are semantically distinct enough in each semantic segmentation branch. Then, two pilot experiments based on the two branches are conducted to capture the significant prior semantic of two images, which then is applied to guide the fusion task in the integration of semantic segmentation branches and fusion branches. In addition, to aggregate both high-level semantics and impressive visual effects, we further investigate the frequency response of the prior semantics, and propose a multi-level representation-adaptive fusion (MRaF) module to explicitly integrate the low-frequent prior semantic with the high-frequent details. Extensive experiments on two public datasets demonstrate the superiority of our method over the state-of-the-art image fusion approaches, in terms of either the visual appeal or the high-level semantics.</li>
</ul>

<h3>Title: Adversarially Masked Video Consistency for Unsupervised Domain  Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Zhu, Junwei Liang, Po-Yao Huang, Alex Hauptmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16242">https://arxiv.org/abs/2403.16242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16242">https://arxiv.org/pdf/2403.16242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16242]] Adversarially Masked Video Consistency for Unsupervised Domain  Adaptation(https://arxiv.org/abs/2403.16242)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We study the problem of unsupervised domain adaptation for egocentric videos. We propose a transformer-based model to learn class-discriminative and domain-invariant feature representations. It consists of two novel designs. The first module is called Generative Adversarial Domain Alignment Network with the aim of learning domain-invariant representations. It simultaneously learns a mask generator and a domain-invariant encoder in an adversarial way. The domain-invariant encoder is trained to minimize the distance between the source and target domain. The masking generator, conversely, aims at producing challenging masks by maximizing the domain distance. The second is a Masked Consistency Learning module to learn class-discriminative representations. It enforces the prediction consistency between the masked target videos and their full forms. To better evaluate the effectiveness of domain adaptation methods, we construct a more challenging benchmark for egocentric videos, U-Ego4D. Our method achieves state-of-the-art performance on the Epic-Kitchen and the proposed U-Ego4D benchmark.</li>
</ul>

<h3>Title: Partially Blinded Unlearning: Class Unlearning for Deep Networks a  Bayesian Perspective</h3>
<ul>
<li><strong>Authors: </strong>Subhodip Panda, Shashwat Sourav, Prathosh A.P</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16246">https://arxiv.org/abs/2403.16246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16246">https://arxiv.org/pdf/2403.16246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16246]] Partially Blinded Unlearning: Class Unlearning for Deep Networks a  Bayesian Perspective(https://arxiv.org/abs/2403.16246)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In order to adhere to regulatory standards governing individual data privacy and safety, machine learning models must systematically eliminate information derived from specific subsets of a user's training data that can no longer be utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal area of research, facilitating the process of selectively discarding information designated to specific sets or classes of data from a pre-trained model, thereby eliminating the necessity for extensive retraining from scratch. The principal aim of this study is to formulate a methodology tailored for the purposeful elimination of information linked to a specific class of data from a pre-trained classification network. This intentional removal is crafted to degrade the model's performance specifically concerning the unlearned data class while concurrently minimizing any detrimental impacts on the model's performance in other classes. To achieve this goal, we frame the class unlearning problem from a Bayesian perspective, which yields a loss function that minimizes the log-likelihood associated with the unlearned data with a stability regularization in parameter space. This stability regularization incorporates Mohalanobis distance with respect to the Fisher Information matrix and $l_2$ distance from the pre-trained model parameters. Our novel approach, termed \textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing state-of-the-art class unlearning methods, demonstrating superior effectiveness. Notably, PBU achieves this efficacy without requiring awareness of the entire training dataset but only to the unlearned data points, marking a distinctive feature of its performance.</li>
</ul>

<h3>Title: Large Language Models Offer an Alternative to the Traditional Approach  of Topic Modelling</h3>
<ul>
<li><strong>Authors: </strong>Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16248">https://arxiv.org/abs/2403.16248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16248">https://arxiv.org/pdf/2403.16248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16248]] Large Language Models Offer an Alternative to the Traditional Approach  of Topic Modelling(https://arxiv.org/abs/2403.16248)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Topic modelling, as a well-established unsupervised technique, has found extensive use in automatically detecting significant topics within a corpus of documents. However, classic topic modelling approaches (e.g., LDA) have certain drawbacks, such as the lack of semantic understanding and the presence of overlapping topics. In this work, we investigate the untapped potential of large language models (LLMs) as an alternative for uncovering the underlying topics within extensive text corpora. To this end, we introduce a framework that prompts LLMs to generate topics from a given set of documents and establish evaluation protocols to assess the clustering efficacy of LLMs. Our findings indicate that LLMs with appropriate prompts can stand out as a viable alternative, capable of generating relevant topic titles and adhering to human guidelines to refine and merge topics. Through in-depth experiments and evaluation, we summarise the advantages and constraints of employing LLMs in topic extraction.</li>
</ul>

<h3>Title: Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal  Contrastive Learning via Local Token Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16257">https://arxiv.org/abs/2403.16257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16257">https://arxiv.org/pdf/2403.16257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16257]] Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal  Contrastive Learning via Local Token Unlearning(https://arxiv.org/abs/2403.16257)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Multimodal contrastive learning has emerged as a powerful paradigm for building high-quality features using the complementary strengths of various data modalities. However, the open nature of such systems inadvertently increases the possibility of backdoor attacks. These attacks subtly embed malicious behaviors within the model during training, which can be activated by specific triggers in the inference phase, posing significant security risks. Despite existing countermeasures through fine-tuning that reduce the adverse impacts of such attacks, these defenses often degrade the clean accuracy and necessitate the construction of extensive clean training pairs. In this paper, we explore the possibility of a less-cost defense from the perspective of model unlearning, that is, whether the model can be made to quickly \textbf{u}nlearn \textbf{b}ackdoor \textbf{t}hreats (UBT) by constructing a small set of poisoned samples. Specifically, we strengthen the backdoor shortcuts to discover suspicious samples through overfitting training prioritized by weak similarity samples. Building on the initial identification of suspicious samples, we introduce an innovative token-based localized forgetting training regime. This technique specifically targets the poisoned aspects of the model, applying a focused effort to unlearn the backdoor associations and trying not to damage the integrity of the overall model. Experimental results show that our method not only ensures a minimal success rate for attacks, but also preserves the model's high clean accuracy.</li>
</ul>

<h3>Title: Object Detectors in the Open Environment:Challenges, Solutions, and  Outlook</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Liang, Wei Wang, Ruoyu Chen, Aishan Liu, Boxi Wu, Ee-Chien Chang, Xiaochun Cao, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16271">https://arxiv.org/abs/2403.16271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16271">https://arxiv.org/pdf/2403.16271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16271]] Object Detectors in the Open Environment:Challenges, Solutions, and  Outlook(https://arxiv.org/abs/2403.16271)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>With the emergence of foundation models, deep learning-based object detectors have shown practical usability in closed set scenarios. However, for real-world tasks, object detectors often operate in open environments, where crucial factors (\eg, data distribution, objective) that influence model learning are often changing. The dynamic and intricate nature of the open environment poses novel and formidable challenges to object detectors. Unfortunately, current research on object detectors in open environments lacks a comprehensive analysis of their distinctive characteristics, challenges, and corresponding solutions, which hinders their secure deployment in critical real-world scenarios. This paper aims to bridge this gap by conducting a comprehensive review and analysis of object detectors in open environments. We initially identified limitations of key structural components within the existing detection pipeline and propose the open environment object detector challenge framework that includes four quadrants (\ie, out-of-domain, out-of-category, robust learning, and incremental learning) based on the dimensions of the data / target changes. For each quadrant of challenges in the proposed framework, we present a detailed description and systematic analysis of the overarching goals and core difficulties, systematically review the corresponding solutions, and benchmark their performance over multiple widely adopted datasets. In addition, we engage in a discussion of open problems and potential avenues for future research. This paper aims to provide a fresh, comprehensive, and systematic understanding of the challenges and solutions associated with open-environment object detectors, thus catalyzing the development of more solid applications in real-world scenarios.</li>
</ul>

<h3>Title: L-MAE: Longitudinal masked auto-encoder with time and severity-aware  encoding for diabetic retinopathy progression prediction</h3>
<ul>
<li><strong>Authors: </strong>Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Alireza Rezaei, Hugo Le Boité, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16272">https://arxiv.org/abs/2403.16272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16272">https://arxiv.org/pdf/2403.16272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16272]] L-MAE: Longitudinal masked auto-encoder with time and severity-aware  encoding for diabetic retinopathy progression prediction(https://arxiv.org/abs/2403.16272)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pre-training strategies based on self-supervised learning (SSL) have proven to be effective pretext tasks for many downstream tasks in computer vision. Due to the significant disparity between medical and natural images, the application of typical SSL is not straightforward in medical imaging. Additionally, those pretext tasks often lack context, which is critical for computer-aided clinical decision support. In this paper, we developed a longitudinal masked auto-encoder (MAE) based on the well-known Transformer-based MAE. In particular, we explored the importance of time-aware position embedding as well as disease progression-aware masking. Taking into account the time between examinations instead of just scheduling them offers the benefit of capturing temporal changes and trends. The masking strategy, for its part, evolves during follow-up to better capture pathological changes, ensuring a more accurate assessment of disease progression. Using OPHDIAT, a large follow-up screening dataset targeting diabetic retinopathy (DR), we evaluated the pre-trained weights on a longitudinal task, which is to predict the severity label of the next visit within 3 years based on the past time series examinations. Our results demonstrated the relevancy of both time-aware position embedding and masking strategies based on disease progression knowledge. Compared to popular baseline models and standard longitudinal Transformers, these simple yet effective extensions significantly enhance the predictive ability of deep classification models.</li>
</ul>

<h3>Title: AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary  Alignment for Temporal Referential Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Tang, Daiki Shimada, Jing Bi, Chenliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16276">https://arxiv.org/abs/2403.16276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16276">https://arxiv.org/pdf/2403.16276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16276]] AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary  Alignment for Temporal Referential Dialogue(https://arxiv.org/abs/2403.16276)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD). While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited. Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively. To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations. We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment of audio-visual information. Additionally, we develop the A5-222K dataset, encompassing more than 200,000 audio-text pairings, to facilitate the audio and text alignments. Our experiments demonstrate that AVicuna can effectively handle TRD in audio-visual videos and achieve state-of-the-art performance on various audio-visual video understanding tasks, particularly in untrimmed videos. We further investigate the optimal audio-interleaving rate for interleaved audio-visual inputs, which maximizes performance on the Audio-Visual Event Dense Localization task.</li>
</ul>

<h3>Title: latentSplat: Autoencoding Variational Gaussians for Fast Generalizable  3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16292">https://arxiv.org/abs/2403.16292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16292">https://arxiv.org/pdf/2403.16292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16292]] latentSplat: Autoencoding Variational Gaussians for Fast Generalizable  3D Reconstruction(https://arxiv.org/abs/2403.16292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data.</li>
</ul>

<h3>Title: Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling</h3>
<ul>
<li><strong>Authors: </strong>Boyang Li, Zhiling Lan, Michael E. Papka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16293">https://arxiv.org/abs/2403.16293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16293">https://arxiv.org/pdf/2403.16293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16293]] Interpretable Modeling of Deep Reinforcement Learning Driven Scheduling(https://arxiv.org/abs/2403.16293)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In the field of high-performance computing (HPC), there has been recent exploration into the use of deep reinforcement learning for cluster scheduling (DRL scheduling), which has demonstrated promising outcomes. However, a significant challenge arises from the lack of interpretability in deep neural networks (DNN), rendering them as black-box models to system managers. This lack of model interpretability hinders the practical deployment of DRL scheduling. In this work, we present a framework called IRL (Interpretable Reinforcement Learning) to address the issue of interpretability of DRL scheduling. The core idea is to interpret DNN (i.e., the DRL policy) as a decision tree by utilizing imitation learning. Unlike DNN, decision tree models are non-parametric and easily comprehensible to humans. To extract an effective and efficient decision tree, IRL incorporates the Dataset Aggregation (DAgger) algorithm and introduces the notion of critical state to prune the derived decision tree. Through trace-based experiments, we demonstrate that IRL is capable of converting a black-box DNN policy into an interpretable rulebased decision tree while maintaining comparable scheduling performance. Additionally, IRL can contribute to the setting of rewards in DRL scheduling.</li>
</ul>

<h3>Title: SoK: An Essential Guide For Using Malware Sandboxes In Security  Applications: Challenges, Pitfalls, and Lessons Learned</h3>
<ul>
<li><strong>Authors: </strong>Omar Alrawi, Miuyin Yong Wong, Athanasios Avgetidis, Kevin Valakuzhy, Boladji Vinny Adjibi, Konstantinos Karakatsanis, Mustaque Ahamad, Doug Blough, Fabian Monrose, Manos Antonakakis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16304">https://arxiv.org/abs/2403.16304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16304">https://arxiv.org/pdf/2403.16304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16304]] SoK: An Essential Guide For Using Malware Sandboxes In Security  Applications: Challenges, Pitfalls, and Lessons Learned(https://arxiv.org/abs/2403.16304)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Malware sandboxes provide many benefits for security applications, but they are complex. These complexities can overwhelm new users in different research areas and make it difficult to select, configure, and use sandboxes. Even worse, incorrectly using sandboxes can have a negative impact on security applications. In this paper, we address this knowledge gap by systematizing 84 representative papers for using x86/64 malware sandboxes in the academic literature. We propose a novel framework to simplify sandbox components and organize the literature to derive practical guidelines for using sandboxes. We evaluate the proposed guidelines systematically using three common security applications and demonstrate that the choice of different sandboxes can significantly impact the results. Specifically, our results show that the proposed guidelines improve the sandbox observable activities by at least 1.6x and up to 11.3x. Furthermore, we observe a roughly 25% improvement in accuracy, precision, and recall when using the guidelines to help with a malware family classification task. We conclude by affirming that there is no "silver bullet" sandbox deployment that generalizes, and we recommend that users apply our framework to define a scope for their analysis, a threat model, and derive context about how the sandbox artifacts will influence their intended use case. Finally, it is important that users document their experiment, limitations, and potential solutions for reproducibility</li>
</ul>

<h3>Title: AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans</h3>
<ul>
<li><strong>Authors: </strong>Cedric Perauer, Laurenz Adrian Heidrich, Haifan Zhang, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16318">https://arxiv.org/abs/2403.16318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16318">https://arxiv.org/pdf/2403.16318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16318]] AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans(https://arxiv.org/abs/2403.16318)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments. Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations. Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain. To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations. To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals. To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image- and point-based self-supervised features, and perform graph-cuts to isolate individual pseudo-instances. We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals. To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene-level instance segmentations. Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline. The code will be made publicly available at https://github.com/artonson/autoinst.</li>
</ul>

<h3>Title: Impact of Video Compression Artifacts on Fisheye Camera Visual  Perception Tasks</h3>
<ul>
<li><strong>Authors: </strong>Madhumitha Sakthi, Louis Kerofsky, Varun Ravi Kumar, Senthil Yogamani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16338">https://arxiv.org/abs/2403.16338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16338">https://arxiv.org/pdf/2403.16338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16338]] Impact of Video Compression Artifacts on Fisheye Camera Visual  Perception Tasks(https://arxiv.org/abs/2403.16338)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous driving systems require extensive data collection schemes to cover the diverse scenarios needed for building a robust and safe system. The data volumes are in the order of Exabytes and have to be stored for a long period of time (i.e., more than 10 years of the vehicle's life cycle). Lossless compression doesn't provide sufficient compression ratios, hence, lossy video compression has been explored. It is essential to prove that lossy video compression artifacts do not impact the performance of the perception algorithms. However, there is limited work in this area to provide a solid conclusion. In particular, there is no such work for fisheye cameras, which have high radial distortion and where compression may have higher artifacts. Fisheye cameras are commonly used in automotive systems for 3D object detection task. In this work, we provide the first analysis of the impact of standard video compression codecs on wide FOV fisheye camera images. We demonstrate that the achievable compression with negligible impact depends on the dataset and temporal prediction of the video codec. We propose a radial distortion-aware zonal metric to evaluate the performance of artifacts in fisheye images. In addition, we present a novel method for estimating affine mode parameters of the latest VVC codec, and suggest some areas for improvement in video codecs for the application to fisheye imagery.</li>
</ul>

<h3>Title: Enhanced Facet Generation with LLM Editing</h3>
<ul>
<li><strong>Authors: </strong>Joosung Lee, Jinhong Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16345">https://arxiv.org/abs/2403.16345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16345">https://arxiv.org/pdf/2403.16345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16345]] Enhanced Facet Generation with LLM Editing(https://arxiv.org/abs/2403.16345)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input without a search engine. The first strategy is multi-task learning to predict SERP. By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules. The second strategy is to enhance the facets by combining Large Language Model (LLM) and the small model. Overall performance improves when small model and LLM are combined rather than facet generation individually.</li>
</ul>

<h3>Title: ChebMixer: Efficient Graph Representation Learning with MLP Mixer</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Kui, Haonan Yan, Qinsong Li, Liming Chen, Beiji Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16358">https://arxiv.org/abs/2403.16358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16358">https://arxiv.org/pdf/2403.16358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16358]] ChebMixer: Efficient Graph Representation Learning with MLP Mixer(https://arxiv.org/abs/2403.16358)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Graph neural networks have achieved remarkable success in learning graph representations, especially graph Transformer, which has recently shown superior performance on various graph mining tasks. However, graph Transformer generally treats nodes as tokens, which results in quadratic complexity regarding the number of nodes during self-attention computation. The graph MLP Mixer addresses this challenge by using the efficient MLP Mixer technique from computer vision. However, the time-consuming process of extracting graph tokens limits its performance. In this paper, we present a novel architecture named ChebMixer, a newly graph MLP Mixer that uses fast Chebyshev polynomials-based spectral filtering to extract a sequence of tokens. Firstly, we produce multiscale representations of graph nodes via fast Chebyshev polynomial-based spectral filtering. Next, we consider each node's multiscale representations as a sequence of tokens and refine the node representation with an effective MLP Mixer. Finally, we aggregate the multiscale representations of nodes through Chebyshev interpolation. Owing to the powerful representation capabilities and fast computational properties of MLP Mixer, we can quickly extract more informative node representations to improve the performance of downstream tasks. The experimental results prove our significant improvements in a variety of scenarios ranging from graph node classification to medical image segmentation.</li>
</ul>

<h3>Title: Generating Potent Poisons and Backdoors from Scratch with Guided  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16365">https://arxiv.org/abs/2403.16365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16365">https://arxiv.org/pdf/2403.16365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16365]] Generating Potent Poisons and Backdoors from Scratch with Guided  Diffusion(https://arxiv.org/abs/2403.16365)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, diffusion</a></li>
<li><strong>Abstract: </strong>Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection. As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it. Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons. However, some base samples may be significantly more amenable to poisoning than others. As a result, we may be able to craft more potent poisons by carefully choosing the base samples. In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks. Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor attack to boost its effectiveness. Our implementation code is publicly available at: https://github.com/hsouri/GDP .</li>
</ul>

<h3>Title: Distilling Semantic Priors from SAM to Efficient Image Restoration  Models</h3>
<ul>
<li><strong>Authors: </strong>Quan Zhang, Xiaoyu Liu, Wei Li, Hanting Chen, Junchao Liu, Jie Hu, Zhiwei Xiong, Chun Yuan, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16368">https://arxiv.org/abs/2403.16368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16368">https://arxiv.org/pdf/2403.16368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16368]] Distilling Semantic Priors from SAM to Efficient Image Restoration  Models(https://arxiv.org/abs/2403.16368)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In image restoration (IR), leveraging semantic priors from segmentation models has been a common approach to improve performance. The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks. However, the computational cost of SAM is prohibitive for IR, compared to existing smaller IR models. The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency. To address this issue, we propose a general framework to distill SAM's semantic knowledge to boost exiting IR models without interfering with their inference process. Specifically, our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD) scheme. SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image. SPD leverages a self-distillation manner to distill the fused semantic priors to boost the performance of original IR models. Additionally, we design a semantic-guided relation (SGR) module for SPD, which ensures semantic feature representation space consistency to fully distill the priors. We demonstrate the effectiveness of our framework across multiple IR models and tasks, including deraining, deblurring, and denoising.</li>
</ul>

<h3>Title: Learning Action-based Representations Using Invariance</h3>
<ul>
<li><strong>Authors: </strong>Max Rudolph, Caleb Chuck, Kevin Black, Misha Lvovsky, Scott Niekum, Amy Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16369">https://arxiv.org/abs/2403.16369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16369">https://arxiv.org/pdf/2403.16369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16369]] Learning Action-based Representations Using Invariance(https://arxiv.org/abs/2403.16369)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts distant state features that are relevant for control. We demonstrate that action-bisimulation pretraining on reward-free, uniformly random data improves sample efficiency in several environments, including a photorealistic 3D simulation domain, Habitat. Additionally, we provide theoretical analysis and qualitative results demonstrating the information captured by action-bisimulation.</li>
</ul>

<h3>Title: GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model  for Distortion-aware Panoramic Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Weiming Zhang, Yexin Liu, Xu Zheng, Lin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16370">https://arxiv.org/abs/2403.16370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16370">https://arxiv.org/pdf/2403.16370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16370]] GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model  for Distortion-aware Panoramic Semantic Segmentation(https://arxiv.org/abs/2403.16370)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper tackles a novel yet challenging problem: how to transfer knowledge from the emerging Segment Anything Model (SAM) -- which reveals impressive zero-shot instance segmentation capacity -- to learn a compact panoramic semantic segmentation model, i.e., student, without requiring any labeled data. This poses considerable challenges due to SAM's inability to provide semantic labels and the large capacity gap between SAM and the student. To this end, we propose a novel framework, called GoodSAM, that introduces a teacher assistant (TA) to provide semantic information, integrated with SAM to generate ensemble logits to achieve knowledge transfer. Specifically, we propose a Distortion-Aware Rectification (DAR) module that first addresses the distortion problem of panoramic images by imposing prediction-level consistency and boundary enhancement. This subtly enhances TA's prediction capacity on panoramic images. DAR then incorporates a cross-task complementary fusion block to adaptively merge the predictions of SAM and TA to obtain more reliable ensemble logits. Moreover, we introduce a Multi-level Knowledge Adaptation (MKA) module to efficiently transfer the multi-level feature knowledge from TA and ensemble logits to learn a compact student model. Extensive experiments on two benchmarks show that our GoodSAM achieves a remarkable +3.75\% mIoU improvement over the state-of-the-art (SOTA) domain adaptation methods. Also, our most lightweight model achieves comparable performance to the SOTA methods with only 3.7M parameters.</li>
</ul>

<h3>Title: SignSGD with Federated Voting</h3>
<ul>
<li><strong>Authors: </strong>Chanho Park, H. Vincent Poor, Namyoon Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16372">https://arxiv.org/abs/2403.16372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16372">https://arxiv.org/pdf/2403.16372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16372]] SignSGD with Federated Voting(https://arxiv.org/abs/2403.16372)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices. However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server. SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit quantization. However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers. To overcome this, we propose a novel signSGD optimizer with \textit{federated voting} (signSGD-FV). The idea of federated voting is to exploit learnable weights to perform weighted majority voting. The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities. Subsequently, these weights are employed to decode the signs of the aggregated local gradients in such a way to minimize the sign decoding error probability. We provide a unified convergence rate analysis framework applicable to scenarios where the estimated weights are known to the parameter server either perfectly or imperfectly. We demonstrate that the proposed signSGD-FV algorithm has a theoretical convergence guarantee even when edge devices use heterogeneous mini-batch sizes. Experimental results show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence rate, especially in heterogeneous mini-batch sizes.</li>
</ul>

<h3>Title: Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and  Distance-Aware Bi-Projection Fusion</h3>
<ul>
<li><strong>Authors: </strong>Hao Ai, Lin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16376">https://arxiv.org/abs/2403.16376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16376">https://arxiv.org/pdf/2403.16376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16376]] Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and  Distance-Aware Bi-Projection Fusion(https://arxiv.org/abs/2403.16376)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>360 depth estimation has recently received great attention for 3D reconstruction owing to its omnidirectional field of view (FoV). Recent approaches are predominantly focused on cross-projection fusion with geometry-based re-projection: they fuse 360 images with equirectangular projection (ERP) and another projection type, e.g., cubemap projection to estimate depth with the ERP format. However, these methods suffer from 1) limited local receptive fields, making it hardly possible to capture large FoV scenes, and 2) prohibitive computational cost, caused by the complex cross-projection fusion module design. In this paper, we propose Elite360D, a novel framework that inputs the ERP image and icosahedron projection (ICOSAP) point set, which is undistorted and spatially continuous. Elite360D is superior in its capacity in learning a representation from a local-with-global perspective. With a flexible ERP image encoder, it includes an ICOSAP point encoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M parameters). Specifically, the ERP image encoder can take various perspective image-trained backbones (e.g., ResNet, Transformer) to extract local features. The point encoder extracts the global features from the ICOSAP. Then, the B2F module captures the semantic- and distance-aware dependencies between each pixel of the ERP feature and the entire ICOSAP feature set. Without specific backbone design and obvious computational cost increase, Elite360D outperforms the prior arts on several benchmark datasets.</li>
</ul>

<h3>Title: FlashEval: Towards Fast and Accurate Evaluation of Text-to-image  Diffusion Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16379">https://arxiv.org/abs/2403.16379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16379">https://arxiv.org/pdf/2403.16379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16379]] FlashEval: Towards Fast and Accurate Evaluation of Text-to-image  Diffusion Generative Models(https://arxiv.org/abs/2403.16379)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, there has been significant progress in the development of text-to-image generative models. Evaluating the quality of the generative models is one essential step in the development process. Unfortunately, the evaluation process could consume a significant amount of computational resources, making the required periodic evaluation of model performance (e.g., monitoring training progress) impractical. Therefore, we seek to improve the evaluation efficiency by selecting the representative subset of the text-image dataset. We systematically investigate the design choices, including the selection criteria (textural features or image-based metrics) and the selection granularity (prompt-level or set-level). We find that the insights from prior work on subset selection for training data do not generalize to this problem, and we propose FlashEval, an iterative search algorithm tailored to evaluation data selection. We demonstrate the effectiveness of FlashEval on ranking diffusion models with various configurations, including architectures, quantization levels, and sampler schedules on COCO and DiffusionDB datasets. Our searched 50-item subset could achieve comparable evaluation quality to the randomly sampled 500-item subset for COCO annotations on unseen models, achieving a 10x evaluation speedup. We release the condensed subset of these commonly used datasets to help facilitate diffusion algorithm design and evaluation, and open-source FlashEval as a tool for condensing future datasets, accessible at https://github.com/thu-nics/FlashEval.</li>
</ul>

<h3>Title: Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators  for Reasoning-Based Chart VQA</h3>
<ul>
<li><strong>Authors: </strong>Li Zhuowan, Jasani Bhavan, Tang Peng, Ghadar Shabnam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16385">https://arxiv.org/abs/2403.16385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16385">https://arxiv.org/pdf/2403.16385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16385]] Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators  for Reasoning-Based Chart VQA(https://arxiv.org/abs/2403.16385)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics. Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions. In this work, we address the lack of reasoning ability by data augmentation. We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images. The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results highlight the significance of the proposed step-by-step generation. By training with the LLM-augmented data (LAMENDA), we significantly enhance the chart VQA models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets. In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written questions in the ChartQA dataset, which needs strong reasoning. We hope our work underscores the potential of synthetic data and encourages further exploration of data augmentation using LLMs for reasoning-heavy tasks.</li>
</ul>

<h3>Title: Dia-LLaMA: Towards Large Language Model-driven CT Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhixuan Chen, Luyang Luo, Yequan Bie, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16386">https://arxiv.org/abs/2403.16386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16386">https://arxiv.org/pdf/2403.16386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16386]] Dia-LLaMA: Towards Large Language Model-driven CT Report Generation(https://arxiv.org/abs/2403.16386)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical report generation has achieved remarkable advancements yet has still been faced with several challenges. First, the inherent imbalance in the distribution of normal and abnormal cases may lead models to exhibit a biased focus on normal samples, resulting in unreliable diagnoses. Second, the frequent occurrence of common template sentences in the reports may overwhelm the critical abnormal information. Moreover, existing works focus on 2D chest X-rays, leaving CT report generation underexplored due to the high-dimensional nature of CT images and the limited availability of CT-report pairs. Recently, LLM has shown a great ability to generate reliable answers with appropriate prompts, which shed light on addressing the aforementioned challenges. In this paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report generation by incorporating diagnostic information as guidance prompts. Considering the high dimension of CT, we leverage a pre-trained ViT3D with perceiver to extract the visual information. To tailor the LLM for report generation and emphasize abnormality, we extract additional diagnostic information by referring to a disease prototype memory bank, which is updated during training to capture common disease representations. Furthermore, we introduce disease-aware attention to enable the model to adjust attention for different diseases. Experiments on the chest CT dataset demonstrated that our proposed method outperformed previous methods and achieved state-of-the-art on both clinical efficacy performance and natural language generation metrics. The code will be made publically available.</li>
</ul>

<h3>Title: Is There a One-Model-Fits-All Approach to Information Extraction?  Revisiting Task Definition Biases</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Huang, Qianyu He, Zhixu Li, Jiaqing Liang, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16396">https://arxiv.org/abs/2403.16396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16396">https://arxiv.org/pdf/2403.16396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16396]] Is There a One-Model-Fits-All Approach to Information Extraction?  Revisiting Task Definition Biases(https://arxiv.org/abs/2403.16396)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Definition bias is a negative phenomenon that can mislead models. Definition bias in information extraction appears not only across datasets from different domains but also within datasets sharing the same domain. We identify two types of definition bias in IE: bias among information extraction datasets and bias between information extraction datasets and instruction tuning datasets. To systematically investigate definition bias, we conduct three probing experiments to quantitatively analyze it and discover the limitations of unified information extraction and large language models in solving definition bias. To mitigate definition bias in information extraction, we propose a multi-stage framework consisting of definition bias measurement, bias-aware fine-tuning, and task-specific bias mitigation. Experimental results demonstrate the effectiveness of our framework in addressing definition bias. Resources of this paper can be found at https://github.com/EZ-hwh/definition-bias</li>
</ul>

<h3>Title: Rethinking the Representation in Federated Unsupervised Learning with  Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu, Huabin Zhu, Binhui Yao, Tao Wang, Xiaolin Zheng, Yanchao Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16398">https://arxiv.org/abs/2403.16398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16398">https://arxiv.org/pdf/2403.16398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16398]] Rethinking the Representation in Federated Unsupervised Learning with  Non-IID Data(https://arxiv.org/abs/2403.16398)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning achieves effective performance in modeling decentralized data. In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data. However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models. The former indicates that representation collapse in local model will subsequently impact the global model and other local models. The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data. Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA). FUR in each client avoids representation collapse via dispersing samples uniformly, and EUA in server promotes unified representation by constraining consistent client model updating. To extensively validate the performance of FedU2, we conduct both cross-device and cross-silo evaluation experiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.</li>
</ul>

<h3>Title: ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D  Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Hannah Schieber, Shiyu Li, Niklas Corell, Philipp Beckerle, Julian Kreimeier, Daniel Roth</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16400">https://arxiv.org/abs/2403.16400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16400">https://arxiv.org/pdf/2403.16400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16400]] ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D  Pose Estimation(https://arxiv.org/abs/2403.16400)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In medical and industrial domains, providing guidance for assembly processes is critical to ensure efficiency and safety. Errors in assembly can lead to significant consequences such as extended surgery times, and prolonged manufacturing or maintenance times in industry. Assembly scenarios can benefit from in-situ AR visualization to provide guidance, reduce assembly times and minimize errors. To enable in-situ visualization 6D pose estimation can be leveraged. Existing 6D pose estimation techniques primarily focus on individual objects and static captures. However, assembly scenarios have various dynamics including occlusion during assembly and dynamics in the assembly objects appearance. Existing work, combining object detection/6D pose estimation and assembly state detection focuses either on pure deep learning-based approaches, or limit the assembly state detection to building blocks. To address the challenges of 6D pose estimation in combination with assembly state detection, our approach ASDF builds upon the strengths of YOLOv8, a real-time capable object detection framework. We extend this framework, refine the object pose and fuse pose knowledge with network-detected pose information. Utilizing our late fusion in our Pose2State module results in refined 6D pose estimation and assembly state detection. By combining both pose and state information, our Pose2State module predicts the final assembly state with precision. Our evaluation on our ASDF dataset shows that our Pose2State module leads to an improved assembly state detection and that the improvement of the assembly state further leads to a more robust 6D pose estimation. Moreover, on the GBOT dataset, we outperform the pure deep learning-based network, and even outperform the hybrid and pure tracking-based approaches.</li>
</ul>

<h3>Title: Ensemble Adversarial Defense via Integration of Multiple Dispersed Low  Curvature Models</h3>
<ul>
<li><strong>Authors: </strong>Kaikang Zhao, Xi Chen, Wei Huang, Liuxin Ding, Xianglong Kong, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16405">https://arxiv.org/abs/2403.16405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16405">https://arxiv.org/pdf/2403.16405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16405]] Ensemble Adversarial Defense via Integration of Multiple Dispersed Low  Curvature Models(https://arxiv.org/abs/2403.16405)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The integration of an ensemble of deep learning models has been extensively explored to enhance defense against adversarial attacks. The diversity among sub-models increases the attack cost required to deceive the majority of the ensemble, thereby improving the adversarial robustness. While existing approaches mainly center on increasing diversity in feature representations or dispersion of first-order gradients with respect to input, the limited correlation between these diversity metrics and adversarial robustness constrains the performance of ensemble adversarial defense. In this work, we aim to enhance ensemble diversity by reducing attack transferability. We identify second-order gradients, which depict the loss curvature, as a key factor in adversarial robustness. Computing the Hessian matrix involved in second-order gradients is computationally expensive. To address this, we approximate the Hessian-vector product using differential approximation. Given that low curvature provides better robustness, our ensemble model was designed to consider the influence of curvature among different sub-models. We introduce a novel regularizer to train multiple more-diverse low-curvature network models. Extensive experiments across various datasets demonstrate that our ensemble model exhibits superior robustness against a range of attacks, underscoring the effectiveness of our approach.</li>
</ul>

<h3>Title: An incremental MaxSAT-based model to learn balanced rules</h3>
<ul>
<li><strong>Authors: </strong>Antônio Carlos Souza Ferreira Júnior, Thiago Alves Rocha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16418">https://arxiv.org/abs/2403.16418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16418">https://arxiv.org/pdf/2403.16418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16418]] An incremental MaxSAT-based model to learn balanced rules(https://arxiv.org/abs/2403.16418)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance that involves learning a set of rules by incrementally applying the model in a dataset. Finally, IMLIB and IMLI are compared using diverse databases. IMLIB obtained results comparable to IMLI in terms of accuracy, generating more balanced rules with smaller sizes.</li>
</ul>

<h3>Title: Refining Text-to-Image Generation: Towards Accurate Training-Free  Glyph-Enhanced Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, Man Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16422">https://arxiv.org/abs/2403.16422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16422">https://arxiv.org/pdf/2403.16422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16422]] Refining Text-to-Image Generation: Towards Accurate Training-Free  Glyph-Enhanced Image Generation(https://arxiv.org/abs/2403.16422)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Over the past few years, Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention. However, vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images. The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications. To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout. Nevertheless, our study reveals that these models still face three primary challenges, prompting us to develop a testbed to facilitate future research. We introduce a benchmark, LenCom-Eval, specifically designed for testing models' capability in generating images with Lengthy and Complex visual text. Subsequently, we introduce a training-free framework to enhance the two-stage generation approaches. We examine the effectiveness of our approach on both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable improvements across a range of evaluation metrics, including CLIPScore, OCR precision, recall, F1 score, accuracy, and edit distance scores. For instance, our proposed framework improves the backbone model, TextDiffuser, by more than 23\% and 13.5\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval, respectively. Our work makes a unique contribution to the field by focusing on generating images with long and rare text sequences, a niche previously unexplored by existing literature</li>
</ul>

<h3>Title: Benchmarks and Challenges in Pose Estimation for Egocentric Hand  Interactions with Objects</h3>
<ul>
<li><strong>Authors: </strong>Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16428">https://arxiv.org/abs/2403.16428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16428">https://arxiv.org/pdf/2403.16428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16428]] Benchmarks and Challenges in Pose Estimation for Egocentric Hand  Interactions with Objects(https://arxiv.org/abs/2403.16428)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3D understanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from different views. Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects. Our efforts will enrich the community's knowledge foundation and facilitate future hand studies on egocentric hand-object interactions.</li>
</ul>

<h3>Title: DOCTR: Disentangled Object-Centric Transformer for Point Scene  Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxuan Yu, Hao Wang, Weiming Li, Qiang Wang, Soonyong Cho, Younghun Sung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16431">https://arxiv.org/abs/2403.16431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16431">https://arxiv.org/pdf/2403.16431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16431]] DOCTR: Disentangled Object-Centric Transformer for Point Scene  Understanding(https://arxiv.org/abs/2403.16431)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Point scene understanding is a challenging task to process real-world scene point cloud, which aims at segmenting each object, estimating its pose, and reconstructing its mesh simultaneously. Recent state-of-the-art method first segments each object and then processes them independently with multiple stages for the different sub-tasks. This leads to a complex pipeline to optimize and makes it hard to leverage the relationship constraints between multiple objects. In this work, we propose a novel Disentangled Object-Centric TRansformer (DOCTR) that explores object-centric representation to facilitate learning with multiple objects for the multiple sub-tasks in a unified manner. Each object is represented as a query, and a Transformer decoder is adapted to iteratively optimize all the queries involving their relationship. In particular, we introduce a semantic-geometry disentangled query (SGDQ) design that enables the query features to attend separately to semantic information and geometric information relevant to the corresponding sub-tasks. A hybrid bipartite matching module is employed to well use the supervisions from all the sub-tasks during training. Qualitative and quantitative experimental results demonstrate that our method achieves state-of-the-art performance on the challenging ScanNet dataset. Code is available at https://github.com/SAITPublic/DOCTR.</li>
</ul>

<h3>Title: $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on  Prompt-based Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yue Xu, Wenjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16432">https://arxiv.org/abs/2403.16432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16432">https://arxiv.org/pdf/2403.16432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16432]] $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on  Prompt-based Language Models(https://arxiv.org/abs/2403.16432)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\textit{LinkPrompt}$, as well as the transferability of UATs generated by \textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo.</li>
</ul>

<h3>Title: InstUPR : Instruction-based Unsupervised Passage Reranking with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chao-Wei Huang, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16435">https://arxiv.org/abs/2403.16435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16435">https://arxiv.org/pdf/2403.16435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16435]] InstUPR : Instruction-based Unsupervised Passage Reranking with Large  Language Models(https://arxiv.org/abs/2403.16435)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces InstUPR, an unsupervised passage reranking method based on large language models (LLMs). Different from existing approaches that rely on extensive training with query-document pairs or retrieval-specific instructions, our method leverages the instruction-following capabilities of instruction-tuned LLMs for passage reranking without any additional fine-tuning. To achieve this, we introduce a soft score aggregation technique and employ pairwise reranking for unsupervised passage reranking. Experiments on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised baselines as well as an instruction-tuned reranker, highlighting its effectiveness and superiority. Source code to reproduce all experiments is open-sourced at https://github.com/MiuLab/InstUPR</li>
</ul>

<h3>Title: RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Lin, Zhe Liu, Zhongyu Xia, Xinhao Wang, Yongtao Wang, Shengxiang Qi, Yang Dong, Nan Dong, Le Zhang, Ce Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16440">https://arxiv.org/abs/2403.16440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16440">https://arxiv.org/pdf/2403.16440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16440]] RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection(https://arxiv.org/abs/2403.16440)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Three-dimensional object detection is one of the key tasks in autonomous driving. To reduce costs in practice, low-cost multi-view cameras for 3D object detection are proposed to replace the expansive LiDAR sensors. However, relying solely on cameras is difficult to achieve highly accurate and robust 3D object detection. An effective solution to this issue is combining multi-view cameras with the economical millimeter-wave radar sensor to achieve more reliable multi-modal 3D object detection. In this paper, we introduce RCBEVDet, a radar-camera fusion 3D object detection method in the bird's eye view (BEV). Specifically, we first design RadarBEVNet for radar BEV feature extraction. RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section (RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based encoder and a transformer-based encoder are proposed to extract radar features, with an injection and extraction module to facilitate communication between the two encoders. The RCS-aware BEV encoder takes RCS as the object size prior to scattering the point feature in BEV. Besides, we present the Cross-Attention Multi-layer Fusion module to automatically align the multi-modal BEV feature from radar and camera with the deformable attention mechanism, and then fuse the feature with channel and spatial fusion layers. Experimental results show that RCBEVDet achieves new state-of-the-art radar-camera fusion results on nuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore, RCBEVDet achieves better 3D detection results than all real-time camera-only and radar-camera 3D object detectors with a faster inference speed at 21~28 FPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.</li>
</ul>

<h3>Title: If CLIP Could Talk: Understanding Vision-Language Model Representations  Through Their Preferred Concept Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16442">https://arxiv.org/abs/2403.16442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16442">https://arxiv.org/pdf/2403.16442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16442]] If CLIP Could Talk: Understanding Vision-Language Model Representations  Through Their Preferred Concept Descriptions(https://arxiv.org/abs/2403.16442)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.</li>
</ul>

<h3>Title: CodeS: Natural Language to Code Repository via Multi-Layer Sketch</h3>
<ul>
<li><strong>Authors: </strong>Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, Zhiguang Yang, Yongji Wang, Qianxiang Wang, Lizhen Cui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16443">https://arxiv.org/abs/2403.16443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16443">https://arxiv.org/pdf/2403.16443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16443]] CodeS: Natural Language to Code Repository via Multi-Layer Sketch(https://arxiv.org/abs/2403.16443)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository's directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis. For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies. Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task.</li>
</ul>

<h3>Title: KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for  Fine-Tuning Korean Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dongjun Jang, Sungjoo Byun, Hyemi Jo, Hyopil Shin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16444">https://arxiv.org/abs/2403.16444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16444">https://arxiv.org/pdf/2403.16444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16444]] KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for  Fine-Tuning Korean Large Language Models(https://arxiv.org/abs/2403.16444)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction Tuning on Large Language Models is an essential process for model to function well and achieve high performance in specific tasks. Accordingly, in mainstream languages such as English, instruction-based datasets are being constructed and made publicly available. In the case of Korean, publicly available models and datasets all rely on using the output of ChatGPT or translating datasets built in English. In this paper, We introduce \textit{KIT-19} as an instruction dataset for the development of LLM in Korean. \textit{KIT-19} is a dataset created in an instruction format, comprising 19 existing open-source datasets for Korean NLP tasks. In this paper, we train a Korean Pretrained LLM using \textit{KIT-19} to demonstrate its effectiveness. The experimental results show that the model trained on \textit{KIT-19} significantly outperforms existing Korean LLMs. Based on the its quality and empirical results, this paper proposes that \textit{KIT-19} has the potential to make a substantial contribution to the future improvement of Korean LLMs' performance.</li>
</ul>

<h3>Title: Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,  Data, and Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Lei Liu, Xiaoyan Yang, Fangzhou Li, Chenfei Chi, Yue Shen, Shiwei Lyu Ming Zhang, Xiaowei Ma, Xiangguo Lyu, Liya Ma, Zhiqiang Zhang, Wei Xue, Yiran Huang, Jinjie Gu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16446">https://arxiv.org/abs/2403.16446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16446">https://arxiv.org/pdf/2403.16446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16446]] Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,  Data, and Algorithm(https://arxiv.org/abs/2403.16446)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are gaining increasing interests to improve clinical efficiency for medical diagnosis, owing to their unprecedented performance in modelling natural language. Ensuring the safe and reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure. Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP. The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities. Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice.</li>
</ul>

<h3>Title: FedAC: A Adaptive Clustered Federated Learning Framework for  Heterogeneous Data</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16460">https://arxiv.org/abs/2403.16460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16460">https://arxiv.org/pdf/2403.16460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16460]] FedAC: A Adaptive Clustered Federated Learning Framework for  Heterogeneous Data(https://arxiv.org/abs/2403.16460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex, heterogeneous environments. Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different non-IID settings compared to SOTA methods.</li>
</ul>

<h3>Title: Plaintext-Free Deep Learning for Privacy-Preserving Medical Image  Analysis via Frequency Information Embedding</h3>
<ul>
<li><strong>Authors: </strong>Mengyu Sun, Ziyuan Yang, Maosong Ran, Zhiwen Wang, Hui Yu, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16473">https://arxiv.org/abs/2403.16473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16473">https://arxiv.org/pdf/2403.16473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16473]] Plaintext-Free Deep Learning for Privacy-Preserving Medical Image  Analysis via Frequency Information Embedding(https://arxiv.org/abs/2403.16473)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In the fast-evolving field of medical image analysis, Deep Learning (DL)-based methods have achieved tremendous success. However, these methods require plaintext data for training and inference stages, raising privacy concerns, especially in the sensitive area of medical data. To tackle these concerns, this paper proposes a novel framework that uses surrogate images for analysis, eliminating the need for plaintext images. This approach is called Frequency-domain Exchange Style Fusion (FESF). The framework includes two main components: Image Hidden Module (IHM) and Image Quality Enhancement Module~(IQEM). The~IHM performs in the frequency domain, blending the features of plaintext medical images into host medical images, and then combines this with IQEM to improve and create surrogate images effectively. During the diagnostic model training process, only surrogate images are used, enabling anonymous analysis without any plaintext data during both training and inference stages. Extensive evaluations demonstrate that our framework effectively preserves the privacy of medical images and maintains diagnostic accuracy of DL models at a relatively high level, proving its effectiveness across various datasets and DL-based models.</li>
</ul>

<h3>Title: CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid  Convolution and Transformer Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Wei Xu, Junjie Luo, Qi Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16494">https://arxiv.org/abs/2403.16494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16494">https://arxiv.org/pdf/2403.16494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16494]] CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid  Convolution and Transformer Neural Networks(https://arxiv.org/abs/2403.16494)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present CT-Bound, a fast boundary estimation method for noisy images using a hybrid Convolution and Transformer neural network. The proposed architecture decomposes boundary estimation into two tasks: local detection and global regularization of image boundaries. It first estimates a parametric representation of boundary structures only using the input image within a small receptive field and then refines the boundary structure in the parameter domain without accessing the input image. Because of this, a part of the network can be easily trained using naive, synthetic images and still generalized to real images, and the entire architecture is computationally efficient as the boundary refinement is non-iterative and not in the image domain. Compared with the previous highest accuracy methods, our experiment shows that CT-Bound is 100 times faster, producing comparably accurate, high-quality boundary and color maps. We also demonstrate that CT-Bound can produce boundary and color maps on real captured images without extra fine-tuning and real-time boundary map and color map videos at ten frames per second.</li>
</ul>

<h3>Title: LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural  Network for Traffic Flow Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Qinyao Luo, Silu He, Xing Han, Yuhan Wang, Haifeng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16495">https://arxiv.org/abs/2403.16495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16495">https://arxiv.org/pdf/2403.16495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16495]] LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural  Network for Traffic Flow Forecasting(https://arxiv.org/abs/2403.16495)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models. However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow. Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation. To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow. First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmasked subseries and their temporal context in a pretraining manner, forcing the model to efficiently learn compressed and contextual subseries temporal representations from long historical series. Then, based on the learned representations, long-term trend is extracted by using stacked 1D dilated convolution layers, and periodic features are extracted by dynamic graph convolution layers. For the difficulties in making time-step level prediction, LSTTN adopts a short-term trend extractor to learn fine-grained short-term temporal features. Finally, LSTTN fuses the long-term trend, periodic features and short-term features to obtain the prediction results. Experiments on four real-world datasets show that in 60-minute-ahead long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\% and a maximum improvement of 16.78\% over baseline models. The source code is available at https://github.com/GeoX-Lab/LSTTN.</li>
</ul>

<h3>Title: Self-Supervised Learning for Medical Image Data with Anatomy-Oriented  Imaging Planes</h3>
<ul>
<li><strong>Authors: </strong>Tianwei Zhang, Dong Wei, Mengmeng Zhua, Shi Gu, Yefeng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16499">https://arxiv.org/abs/2403.16499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16499">https://arxiv.org/pdf/2403.16499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16499]] Self-Supervised Learning for Medical Image Data with Anatomy-Oriented  Imaging Planes(https://arxiv.org/abs/2403.16499)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has emerged as a powerful tool for pretraining deep networks on unlabeled data, prior to transfer learning of target tasks with limited annotation. The relevance between the pretraining pretext and target tasks is crucial to the success of transfer learning. Various pretext tasks have been proposed to utilize properties of medical image data (e.g., three dimensionality), which are more relevant to medical image analysis than generic ones for natural images. However, previous work rarely paid attention to data with anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance imaging views. As these imaging planes are defined according to the anatomy of the imaged organ, pretext tasks effectively exploiting this information can pretrain the networks to gain knowledge on the organ of interest. In this work, we propose two complementary pretext tasks for this group of medical image data based on the spatial relationship of the imaging planes. The first is to learn the relative orientation between the imaging planes and implemented as regressing their intersecting lines. The second exploits parallel imaging planes to regress their relative slice locations within a stack. Both pretext tasks are conceptually straightforward and easy to implement, and can be combined in multitask learning for better representation learning. Thorough experiments on two anatomical structures (heart and knee) and representative target tasks (semantic segmentation and classification) demonstrate that the proposed pretext tasks are effective in pretraining deep networks for remarkably boosted performance on the target tasks, and superior to other recent approaches.</li>
</ul>

<h3>Title: LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent  Classification</h3>
<ul>
<li><strong>Authors: </strong>Liu Junhua, Tan Yong Keat, Fu Bin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16504">https://arxiv.org/abs/2403.16504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16504">https://arxiv.org/pdf/2403.16504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16504]] LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent  Classification(https://arxiv.org/abs/2403.16504)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retraining and fine-tune. Comprehensive experiments demonstrate that LARA achieves state-of-the-art performance on multi-turn intent classification tasks, enhancing the average accuracy by 3.67% compared to existing methods.</li>
</ul>

<h3>Title: Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h3>
<ul>
<li><strong>Authors: </strong>Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16510">https://arxiv.org/abs/2403.16510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16510">https://arxiv.org/pdf/2403.16510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16510]] Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework(https://arxiv.org/abs/2403.16510)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{https://github.com/ICTMCG/Make-Your-Anchor}.</li>
</ul>

<h3>Title: LLMs Are Few-Shot In-Context Low-Resource Language Learners</h3>
<ul>
<li><strong>Authors: </strong>Samuel Cahyawijaya, Holy Lovenia, Pascale Fung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16512">https://arxiv.org/abs/2403.16512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16512">https://arxiv.org/pdf/2403.16512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16512]] LLMs Are Few-Shot In-Context Low-Resource Language Learners(https://arxiv.org/abs/2403.16512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing ICL research, particularly for low-resource languages.</li>
</ul>

<h3>Title: Let Real Images be as a Judger, Spotting Fake Images Synthesized with  Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyou Liang, Run Wang, Weifeng Liu, Yuyang Zhang, Wenyuan Yang, Lina Wang, Xingkai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16513">https://arxiv.org/abs/2403.16513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16513">https://arxiv.org/pdf/2403.16513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16513]] Let Real Images be as a Judger, Spotting Fake Images Synthesized with  Generative Models(https://arxiv.org/abs/2403.16513)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>In the last few years, generative models have shown their powerful capabilities in synthesizing realistic images in both quality and diversity (i.e., facial images, and natural subjects). Unfortunately, the artifact patterns in fake images synthesized by different generative models are inconsistent, leading to the failure of previous research that relied on spotting subtle differences between real and fake. In our preliminary experiments, we find that the artifacts in fake images always change with the development of the generative model, while natural images exhibit stable statistical properties. In this paper, we employ natural traces shared only by real images as an additional predictive target in the detector. Specifically, the natural traces are learned from the wild real images and we introduce extended supervised contrastive learning to bring them closer to real images and further away from fake ones. This motivates the detector to make decisions based on the proximity of images to the natural traces. To conduct a comprehensive experiment, we built a high-quality and diverse dataset that includes generative models comprising 6 GAN and 6 diffusion models, to evaluate the effectiveness in generalizing unknown forgery techniques and robustness in surviving different transformations. Experimental results show that our proposed method gives 96.1% mAP significantly outperforms the baselines. Extensive experiments conducted on the widely recognized platform Midjourney reveal that our proposed method achieves an accuracy exceeding 78.4%, underscoring its practicality for real-world application deployment. The source code and partial self-built dataset are available in supplementary material.</li>
</ul>

<h3>Title: Visually Guided Generative Text-Layout Pre-training for Document  Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16516">https://arxiv.org/abs/2403.16516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16516">https://arxiv.org/pdf/2403.16516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16516]] Visually Guided Generative Text-Layout Pre-training for Document  Intelligence(https://arxiv.org/abs/2403.16516)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, generative</a></li>
<li><strong>Abstract: </strong>Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length. ViTLP can function as a native OCR model to localize and recognize texts of document images. Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that ViTLP achieves competitive performance over existing baselines on benchmark VDU tasks, including information extraction, document classification, and document question answering.</li>
</ul>

<h3>Title: CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal  Representation Learning for AD classification</h3>
<ul>
<li><strong>Authors: </strong>Guangqian Yang, Kangrui Du, Zhihan Yang, Ye Du, Yongping Zheng, Shujun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16520">https://arxiv.org/abs/2403.16520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16520">https://arxiv.org/pdf/2403.16520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16520]] CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal  Representation Learning for AD classification(https://arxiv.org/abs/2403.16520)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is an incurable neurodegenerative condition leading to cognitive and functional deterioration. Given the lack of a cure, prompt and precise AD diagnosis is vital, a complex process dependent on multiple factors and multi-modal data. While successful efforts have been made to integrate multi-modal representation learning into medical datasets, scant attention has been given to 3D medical images. In this paper, we propose Contrastive Masked Vim Autoencoder (CMViM), the first efficient representation learning method tailored for 3D multi-modal data. Our proposed framework is built on a masked Vim autoencoder to learn a unified multi-modal representation and long-dependencies contained in 3D medical images. We also introduce an intra-modal contrastive learning module to enhance the capability of the multi-modal Vim encoder for modeling the discriminative features in the same modality, and an inter-modal contrastive learning module to alleviate misaligned representation among modalities. Our framework consists of two main steps: 1) incorporate the Vision Mamba (Vim) into the mask autoencoder to reconstruct 3D masked multi-modal data efficiently. 2) align the multi-modal representations with contrastive learning mechanisms from both intra-modal and inter-modal aspects. Our framework is pre-trained and validated ADNI2 dataset and validated on the downstream task for AD classification. The proposed CMViM yields 2.7\% AUC performance improvement compared with other state-of-the-art methods.</li>
</ul>

<h3>Title: ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise  Optimization in Medical Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Haiqiao Wang, Zhuoyuan Wang, Dong Ni, Yi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16526">https://arxiv.org/abs/2403.16526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16526">https://arxiv.org/pdf/2403.16526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16526]] ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise  Optimization in Medical Image Registration(https://arxiv.org/abs/2403.16526)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Deformable image registration plays a crucial role in medical imaging, aiding in disease diagnosis and image-guided interventions. Traditional iterative methods are slow, while deep learning (DL) accelerates solutions but faces usability and precision challenges. This study introduces a pyramid network with the enhanced motion decomposition Transformer (ModeTv2) operator, showcasing superior pairwise optimization (PO) akin to traditional methods. We re-implement ModeT operator with CUDA extensions to enhance its computational efficiency. We further propose RegHead module which refines deformation fields, improves the realism of deformation and reduces parameters. By adopting the PO, the proposed network balances accuracy, efficiency, and generalizability. Extensive experiments on two public brain MRI datasets and one abdominal CT dataset demonstrate the network's suitability for PO, providing a DL model with enhanced usability and interpretability. The code is publicly available.</li>
</ul>

<h3>Title: An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zizhao Hu, Shaochong Jia, Mohammad Rostami</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16530">https://arxiv.org/abs/2403.16530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16530">https://arxiv.org/pdf/2403.16530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16530]] An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in  Diffusion Models(https://arxiv.org/abs/2403.16530)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have been widely used for conditional data cross-modal generation tasks such as text-to-image and text-to-video. However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc. We approach this problem from a multimodal data fusion perspective and investigate how different fusion strategies can affect vision-language alignment. We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost text-to-image alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank text-to-image attention calculations. We perform experiments using a text-to-image generation task on the MS-COCO dataset. We compare our intermediate fusion mechanism with the classic early fusion mechanism on two common conditioning methods on a U-shaped ViT backbone. Our intermediate fusion model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and 50% increased training speed compared to a strong U-ViT baseline with an early fusion.</li>
</ul>

<h3>Title: VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate  Spatiotemporal Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, Junwei Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16536">https://arxiv.org/abs/2403.16536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16536">https://arxiv.org/pdf/2403.16536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16536]] VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate  Spatiotemporal Forecasting(https://arxiv.org/abs/2403.16536)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Combining CNNs or ViTs, with RNNs for spatiotemporal forecasting, has yielded unparalleled results in predicting temporal and spatial dynamics. However, modeling extensive global information remains a formidable challenge; CNNs are limited by their narrow receptive fields, and ViTs struggle with the intensive computational demands of their attention mechanisms. The emergence of recent Mamba-based architectures has been met with enthusiasm for their exceptional long-sequence modeling capabilities, surpassing established vision models in efficiency and accuracy, which motivates us to develop an innovative architecture tailored for spatiotemporal forecasting. In this paper, we propose the VMRNN cell, a new recurrent unit that integrates the strengths of Vision Mamba blocks with LSTM. We construct a network centered on VMRNN cells to tackle spatiotemporal prediction tasks effectively. Our extensive evaluations show that our proposed approach secures competitive results on a variety of tasks while maintaining a smaller model size. Our code is available at https://github.com/yyyujintang/VMRNN-PyTorch.</li>
</ul>

<h3>Title: DOrA: 3D Visual Grounding with Order-Aware Referring</h3>
<ul>
<li><strong>Authors: </strong>Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16539">https://arxiv.org/abs/2403.16539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16539">https://arxiv.org/pdf/2403.16539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16539]] DOrA: 3D Visual Grounding with Order-Aware Referring(https://arxiv.org/abs/2403.16539)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>3D visual grounding aims to identify the target object within a 3D point cloud scene referred to by a natural language description. While previous works attempt to exploit the verbo-visual relation with proposed cross-modal transformers, unstructured natural utterances and scattered objects might lead to undesirable performances. In this paper, we introduce DOrA, a novel 3D visual grounding framework with Order-Aware referring. DOrA is designed to leverage Large Language Models (LLMs) to parse language description, suggesting a referential order of anchor objects. Such ordered anchor objects allow DOrA to update visual features and locate the target object during the grounding process. Experimental results on the NR3D and ScanRefer datasets demonstrate our superiority in both low-resource and full-data scenarios. In particular, DOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% grounding accuracy under 1% data and 10% data settings, respectively.</li>
</ul>

<h3>Title: Differentially Private Online Federated Learning with Correlated Noise</h3>
<ul>
<li><strong>Authors: </strong>Jiaojiao Zhang, Linglingzhi Zhu, Mikael Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16542">https://arxiv.org/abs/2403.16542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16542">https://arxiv.org/pdf/2403.16542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16542]] Differentially Private Online Federated Learning with Correlated Noise(https://arxiv.org/abs/2403.16542)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.</li>
</ul>

<h3>Title: Efficient Information Extraction in Few-Shot Relation Classification  through Contrastive Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16543">https://arxiv.org/abs/2403.16543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16543">https://arxiv.org/pdf/2403.16543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16543]] Efficient Information Extraction in Few-Shot Relation Classification  through Contrastive Representation Learning(https://arxiv.org/abs/2403.16543)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning. While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped. To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens. Our method employs contrastive learning to extract complementary discriminative information from these individual representations. This is particularly relevant in low-resource settings where information is scarce. Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available. We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints.</li>
</ul>

<h3>Title: Accelerating Federated Learning by Selecting Beneficial Herd of Local  Gradients</h3>
<ul>
<li><strong>Authors: </strong>Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16557">https://arxiv.org/abs/2403.16557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16557">https://arxiv.org/pdf/2403.16557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16557]] Accelerating Federated Learning by Selecting Beneficial Herd of Local  Gradients(https://arxiv.org/abs/2403.16557)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed machine learning framework in communication network systems. However, the systems' Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence. In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model. Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients. These top portion of the gradients will be selected and sent to the server for global aggregation. We conduct experiments on different datasets, models and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset, thus accelerating model convergence.</li>
</ul>

<h3>Title: Elysium: Exploring Object-level Perception in Videos via MLLM</h3>
<ul>
<li><strong>Authors: </strong>Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, Can Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16558">https://arxiv.org/abs/2403.16558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16558">https://arxiv.org/pdf/2403.16558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16558]] Elysium: Exploring Object-level Perception in Videos via MLLM(https://arxiv.org/abs/2403.16558)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied. This lack of exploration is primarily due to two key challenges. Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships. Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden. To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset paired with novel tasks: Referring Single Object Tracking (RSOT) and Video Referring Expression Generation (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions. Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge. Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that makes the first attempt to conduct object-level tasks in videos without requiring any additional plug-in or expert models.</li>
</ul>

<h3>Title: FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinyuan Ji, Zhaowei Zhu, Wei Xi, Olga Gadyatskaya, Zilong Song, Yong Cai, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16561">https://arxiv.org/abs/2403.16561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16561">https://arxiv.org/pdf/2403.16561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16561]] FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning(https://arxiv.org/abs/2403.16561)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, noise learning</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise. Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models. We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets. The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios.</li>
</ul>

<h3>Title: Revealing Vulnerabilities of Neural Networks in Parameter Learning and  Defense Against Explanation-Aware Backdoors</h3>
<ul>
<li><strong>Authors: </strong>Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16569">https://arxiv.org/abs/2403.16569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16569">https://arxiv.org/pdf/2403.16569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16569]] Revealing Vulnerabilities of Neural Networks in Parameter Learning and  Defense Against Explanation-Aware Backdoors(https://arxiv.org/abs/2403.16569)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks, achieving an approximate decrease of ~99\% in the Attack Success Rate (ASR) and a ~91\% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks.</li>
</ul>

<h3>Title: NSINA: A News Corpus for Sinhala</h3>
<ul>
<li><strong>Authors: </strong>Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16571">https://arxiv.org/abs/2403.16571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16571">https://arxiv.org/pdf/2403.16571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16571]] NSINA: A News Corpus for Sinhala(https://arxiv.org/abs/2403.16571)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.</li>
</ul>

<h3>Title: SegICL: A Universal In-context Learning Framework for Enhanced  Segmentation in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16578">https://arxiv.org/abs/2403.16578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16578">https://arxiv.org/pdf/2403.16578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16578]] SegICL: A Universal In-context Learning Framework for Enhanced  Segmentation in Medical Imaging(https://arxiv.org/abs/2403.16578)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentation performance on OOD modalities and tasks. This indicates that SegICL effectively address new segmentation tasks based on contextual information. Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks. Our code will be released soon.</li>
</ul>

<h3>Title: Can Large Language Models (or Humans) Distill Text?</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak, Moa Johansson, Richard Johansson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16584">https://arxiv.org/abs/2403.16584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16584">https://arxiv.org/pdf/2403.16584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16584]] Can Large Language Models (or Humans) Distill Text?(https://arxiv.org/abs/2403.16584)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We investigate the potential of large language models (LLMs) to distill text: to remove the textual traces of an undesired forbidden variable. We employ a range of LLMs with varying architectures and training approaches to distill text by identifying and removing information about the target variable while preserving other relevant signals. Our findings shed light on the strengths and limitations of LLMs in addressing the distillation and provide insights into the strategies for leveraging these models in computational social science investigations involving text data. In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation. Furthermore, we find that human annotators also struggle to distill sentiment while preserving other semantic content. This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of distillation methods that achieve statistical independence in representation space if this is difficult for human coders operating on raw text to attain.</li>
</ul>

<h3>Title: Deciphering the Interplay between Local Differential Privacy, Average  Bayesian Privacy, and Maximum Bayesian Privacy</h3>
<ul>
<li><strong>Authors: </strong>Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16591">https://arxiv.org/abs/2403.16591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16591">https://arxiv.org/pdf/2403.16591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16591]] Deciphering the Interplay between Local Differential Privacy, Average  Bayesian Privacy, and Maximum Bayesian Privacy(https://arxiv.org/abs/2403.16591)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $\epsilon_{p,a} \leq \frac{1}{\sqrt{2}}\sqrt{(\epsilon_{p,m} + \epsilon)\cdot(e^{\epsilon_{p,m} + \epsilon} - 1)}$ and the equivalence between $\xi$-MBP and $2\xi$-LDP established under uniform prior distribution. These relationships fortify our understanding of the privacy guarantees provided by various mechanisms, leading to the realization that a mechanism satisfying $\xi$-LDP also confers $\xi$-MBP, and vice versa. Our work not only lays the groundwork for future empirical exploration but also promises to enhance the design of privacy-preserving algorithms that do not compromise on utility, thereby fostering the development of trustworthy machine learning solutions.</li>
</ul>

<h3>Title: TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain  Machine Generated Text Detection Techniques</h3>
<ul>
<li><strong>Authors: </strong>Ashok Urlana, Aditya Saibewar, Bala Mallikarjunarao Garlapati, Charaka Vinayak Kumar, Ajeet Kumar Singh, Srinivasa Rao Chalamala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16592">https://arxiv.org/abs/2403.16592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16592">https://arxiv.org/pdf/2403.16592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16592]] TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain  Machine Generated Text Detection Techniques(https://arxiv.org/abs/2403.16592)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Large Language Models (LLMs) exhibit remarkable ability to generate fluent content across a wide spectrum of user queries. However, this capability has raised concerns regarding misinformation and personal information leakage. In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts. Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches. We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods. Our methods obtain an accuracy of 86.9\% on the test set of subtask-A mono and 83.7\% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies.</li>
</ul>

<h3>Title: SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for  Aerial Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Aysim Toker, Marvin Eisenberger, Daniel Cremers, Laura Leal-Taixé</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16605">https://arxiv.org/abs/2403.16605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16605">https://arxiv.org/pdf/2403.16605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16605]] SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for  Aerial Semantic Segmentation(https://arxiv.org/abs/2403.16605)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery. Yet, a prevalent limitation of supervised learning techniques remains the need for extensive manual annotations by experts. In this work, we explore the potential of generative image diffusion to address the scarcity of annotated data in earth observation tasks. The main idea is to learn the joint data manifold of images and labels, leveraging recent advancements in denoising diffusion probabilistic models. To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation. We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity. Both aspects are crucial for earth observation data, where semantic classes can vary severely in scale and occurrence frequency. We employ the novel data instances for downstream segmentation, as a form of data augmentation. In our experiments, we provide comparisons to prior works based on discriminative diffusion models or GANs. We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation -- both compared to baselines and when training only on the original data.</li>
</ul>

<h3>Title: Conversational Grounding: Annotation and Analysis of Grounding Acts and  Grounding Units</h3>
<ul>
<li><strong>Authors: </strong>Biswesh Mohapatra, Seemab Hassan, Laurent Romary, Justine Cassell</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16609">https://arxiv.org/abs/2403.16609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16609">https://arxiv.org/pdf/2403.16609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16609]] Conversational Grounding: Annotation and Analysis of Grounding Acts and  Grounding Units(https://arxiv.org/abs/2403.16609)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared. This process, known as conversational grounding, is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information. The proficiencies of an agent in grounding the conveyed information significantly contribute to building a reliable dialog system. Despite recent advancements in dialog systems, there exists a noticeable deficit in their grounding capabilities. Traum provided a framework for conversational grounding introducing Grounding Acts and Grounding Units, but substantial progress, especially in the realm of Large Language Models, remains lacking. To bridge this gap, we present the annotation of two dialog corpora employing Grounding Acts, Grounding Units, and a measure of their degree of grounding. We discuss our key findings during the annotation and also provide a baseline model to test the performance of current Language Models in categorizing the grounding acts of the dialogs. Our work aims to provide a useful resource for further research in making conversations with machines better understood and more reliable in natural day-to-day collaborative dialogs.</li>
</ul>

<h3>Title: Semantically Enriched Cross-Lingual Sentence Embeddings for  Crisis-related Social Media Texts</h3>
<ul>
<li><strong>Authors: </strong>Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16614">https://arxiv.org/abs/2403.16614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16614">https://arxiv.org/pdf/2403.16614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16614]] Semantically Enriched Cross-Lingual Sentence Embeddings for  Crisis-related Social Media Texts(https://arxiv.org/abs/2403.16614)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Tasks such as semantic search and clustering on crisis-related social media texts enhance our comprehension of crisis discourse, aiding decision-making and targeted interventions. Pre-trained language models have advanced performance in crisis informatics, but their contextual embeddings lack semantic meaningfulness. Although the CrisisTransformers family includes a sentence encoder to address the semanticity issue, it remains monolingual, processing only English texts. Furthermore, employing separate models for different languages leads to embeddings in distinct vector spaces, introducing challenges when comparing semantic similarities between multi-lingual texts. Therefore, we propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed crisis-related social media texts for over 50 languages, such that texts with similar meanings are in close proximity within the same vector space, irrespective of language diversity. Results in sentence encoding and sentence matching tasks are promising, suggesting these models could serve as robust baselines when embedding multi-lingual crisis-related social media texts. The models are publicly available at: https://huggingface.co/crisistransformers.</li>
</ul>

<h3>Title: SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</h3>
<ul>
<li><strong>Authors: </strong>Yuda Song, Zehao Sun, Xuanwu Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16627">https://arxiv.org/abs/2403.16627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16627">https://arxiv.org/pdf/2403.16627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16627]] SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions(https://arxiv.org/abs/2403.16627)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have positioned them at the forefront of image generation. Despite their superior performance, diffusion models are not without drawbacks; they are characterized by complex architectures and substantial computational demands, resulting in significant latency due to their iterative sampling process. To mitigate these limitations, we introduce a dual approach involving model miniaturization and a reduction in sampling steps, aimed at significantly decreasing model latency. Our methodology leverages knowledge distillation to streamline the U-Net and image decoder architectures, and introduces an innovative one-step DM training technique that utilizes feature matching and score distillation. We present two models, SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS (30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU, respectively. Moreover, our training approach offers promising applications in image-conditioned control, facilitating efficient image-to-image translation.</li>
</ul>

<h3>Title: A comparative analysis of embedding models for patent similarity</h3>
<ul>
<li><strong>Authors: </strong>Grazia Sveva Ascione, Valerio Sterzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16630">https://arxiv.org/abs/2403.16630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16630">https://arxiv.org/pdf/2403.16630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16630]] A comparative analysis of embedding models for patent similarity(https://arxiv.org/abs/2403.16630)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity. Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed.</li>
</ul>

<h3>Title: AI-Generated Video Detection via Spatio-Temporal Anomaly Learning</h3>
<ul>
<li><strong>Authors: </strong>Jianfa Bai, Man Lin, Gang Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16638">https://arxiv.org/abs/2403.16638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16638">https://arxiv.org/pdf/2403.16638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16638]] AI-Generated Video Detection via Spatio-Temporal Anomaly Learning(https://arxiv.org/abs/2403.16638)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos. Malicious users can easily create non-existent videos to spread false information. This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal convolutional neural network (CNN). Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively. Results of such sub-detectors are fused to further enhance the discrimination ability. A large-scale generated video dataset (GVD) is constructed as a benchmark for model training and evaluation. Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme. Code and dataset will be available at https://github.com/multimediaFor/AIGVDet.</li>
</ul>

<h3>Title: Clustering Propagation for Universal Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Ding, Liulei Li, Wenguan Wang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16646">https://arxiv.org/abs/2403.16646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16646">https://arxiv.org/pdf/2403.16646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16646]] Clustering Propagation for Universal Medical Image Segmentation(https://arxiv.org/abs/2403.16646)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Prominent solutions for medical image segmentation are typically tailored for automatic or interactive setups, posing challenges in facilitating progress achieved in one task to another.$_{\!}$ This$_{\!}$ also$_{\!}$ necessitates$_{\!}$ separate$_{\!}$ models for each task, duplicating both training time and parameters.$_{\!}$ To$_{\!}$ address$_{\!}$ above$_{\!}$ issues,$_{\!}$ we$_{\!}$ introduce$_{\!}$ S2VNet,$_{\!}$ a$_{\!}$ universal$_{\!}$ framework$_{\!}$ that$_{\!}$ leverages$_{\!}$ Slice-to-Volume$_{\!}$ propagation$_{\!}$ to$_{\!}$ unify automatic/interactive segmentation within a single model and one training session. Inspired by clustering-based segmentation techniques, S2VNet makes full use of the slice-wise structure of volumetric data by initializing cluster centers from the cluster$_{\!}$ results$_{\!}$ of$_{\!}$ previous$_{\!}$ slice.$_{\!}$ This enables knowledge acquired from prior slices to assist in the segmentation of the current slice, further efficiently bridging the communication between remote slices using mere 2D networks. Moreover, such a framework readily accommodates interactive segmentation with no architectural change, simply by initializing centroids from user inputs. S2VNet distinguishes itself by swift inference speeds and reduced memory consumption compared to prevailing 3D solutions. It can also handle multi-class interactions with each of them serving to initialize different centroids. Experiments on three benchmarks demonstrate S2VNet surpasses task-specified solutions on both automatic/interactive setups.</li>
</ul>

<h3>Title: A Novel Loss Function-based Support Vector Machine for Binary  Classification</h3>
<ul>
<li><strong>Authors: </strong>Yan Li, Liping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16654">https://arxiv.org/abs/2403.16654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16654">https://arxiv.org/pdf/2403.16654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16654]] A Novel Loss Function-based Support Vector Machine for Binary  Classification(https://arxiv.org/abs/2403.16654)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss SVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the degree of penalty for the correctly classified samples within the margin. This oversight affects the generalization ability of the SVM classifier to some extent. To address this limitation, from the perspective of confidence margin, we propose a novel Slide loss function ($\ell_s$) to construct the support vector machine classifier($\ell_s$-SVM). By introducing the concept of proximal stationary point, and utilizing the property of Lipschitz continuity, we derive the first-order optimality conditions for $\ell_s$-SVM. Based on this, we define the $\ell_s$ support vectors and working set of $\ell_s$-SVM. To efficiently handle $\ell_s$-SVM, we devise a fast alternating direction method of multipliers with the working set ($\ell_s$-ADMM), and provide the convergence analysis. The numerical experiments on real world datasets confirm the robustness and effectiveness of the proposed method.</li>
</ul>

<h3>Title: Grammatical vs Spelling Error Correction: An Investigation into the  Responsiveness of Transformer-based Language Models using BART and MarianMT</h3>
<ul>
<li><strong>Authors: </strong>Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala, Suriya KS</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16655">https://arxiv.org/abs/2403.16655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16655">https://arxiv.org/pdf/2403.16655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16655]] Grammatical vs Spelling Error Correction: An Investigation into the  Responsiveness of Transformer-based Language Models using BART and MarianMT(https://arxiv.org/abs/2403.16655)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Text continues to remain a relevant form of representation for information. Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech. While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilized to transform the images and speech signals into text content. All these variety of mechanisms of text generation also introduce errors into the captured text. This project aims at analyzing different kinds of error that occurs in text documents. The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text. Transfer learning of these models with available dataset is performed to finetune their capacity for error correction. A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories. It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%).</li>
</ul>

<h3>Title: Graph Augmentation for Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16656">https://arxiv.org/abs/2403.16656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16656">https://arxiv.org/pdf/2403.16656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16656]] Graph Augmentation for Recommendation(https://arxiv.org/abs/2403.16656)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information bottleneck (GIB)-regularized augmentation paradigm, which automatically distills informative self-supervision information and adaptively adjusts contrastive view generation. Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model. The outcomes consistently unveil its superiority over existing baseline methods. The source code for our model is publicly available at: https://github.com/HKUDS/GraphAug.</li>
</ul>

<h3>Title: RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking  on Russia-Ukraine Conflict</h3>
<ul>
<li><strong>Authors: </strong>Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16662">https://arxiv.org/abs/2403.16662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16662">https://arxiv.org/pdf/2403.16662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16662]] RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking  on Russia-Ukraine Conflict(https://arxiv.org/abs/2403.16662)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence. High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans. However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge. To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web. Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation. To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations. Experimental results demonstrate the prospect of optimized evidence in increasing fact-checking performance and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks.</li>
</ul>

<h3>Title: DeepKnowledge: Generalisation-Driven Deep Learning Testing</h3>
<ul>
<li><strong>Authors: </strong>Sondess Missaoui, Simos Gerasimou, Nikolaos Matragkas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16768">https://arxiv.org/abs/2403.16768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16768">https://arxiv.org/pdf/2403.16768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16768]] DeepKnowledge: Generalisation-Driven Deep Learning Testing(https://arxiv.org/abs/2403.16768)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Despite their unprecedented success, DNNs are notoriously fragile to small shifts in data distribution, demanding effective testing techniques that can assess their dependability. Despite recent advances in DNN testing, there is a lack of systematic testing approaches that assess the DNN's capability to generalise and operate comparably beyond data in their training distribution. We address this gap with DeepKnowledge, a systematic testing methodology for DNN-based systems founded on the theory of knowledge generalisation, which aims to enhance DNN robustness and reduce the residual risk of 'black box' models. Conforming to this theory, DeepKnowledge posits that core computational DNN units, termed Transfer Knowledge neurons, can generalise under domain shift. DeepKnowledge provides an objective confidence measurement on testing activities of DNN given data distribution shifts and uses this information to instrument a generalisation-informed test adequacy criterion to check the transfer knowledge capacity of a test set. Our empirical evaluation of several DNNs, across multiple datasets and state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepKnowledge and its ability to support the engineering of more dependable DNNs. We report improvements of up to 10 percentage points over state-of-the-art coverage criteria for detecting adversarial attacks on several benchmarks, including MNIST, SVHN, and CIFAR.</li>
</ul>

<h3>Title: Synthetic Data Generation and Joint Learning for Robust Code-Mixed  Translation</h3>
<ul>
<li><strong>Authors: </strong>Kartik, Sanjana Soni, Anoop Kunchukuttan, Tanmoy Chakraborty, Md Shad Akhtar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16771">https://arxiv.org/abs/2403.16771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16771">https://arxiv.org/pdf/2403.16771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16771]] Synthetic Data Generation and Joint Learning for Robust Code-Mixed  Translation(https://arxiv.org/abs/2403.16771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish to English translation. Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods.</li>
</ul>

<h3>Title: The Anatomy of Adversarial Attacks: Concept-based XAI Dissection</h3>
<ul>
<li><strong>Authors: </strong>Georgii Mikriukov, Gesina Schwalbe, Franz Motzkus, Korinna Bade</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16782">https://arxiv.org/abs/2403.16782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16782">https://arxiv.org/pdf/2403.16782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16782]] The Anatomy of Adversarial Attacks: Concept-based XAI Dissection(https://arxiv.org/abs/2403.16782)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial attacks (AAs) pose a significant threat to the reliability and robustness of deep neural networks. While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored. In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by convolutional neural networks (CNNs) using eXplainable artificial intelligence (XAI) techniques. Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings. First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones. Second, the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack's success. Notably, we discover that these components are target-specific, i.e., are similar for a given target class throughout different AA techniques and starting classes. Our findings provide valuable insights into the nature of AAs and their impact on learned representations, paving the way for the development of more robust and interpretable deep learning models, as well as effective defenses against adversarial threats.</li>
</ul>

<h3>Title: HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Linglin Jing, Yiming Ding, Yunpeng Gao, Zhigang Wang, Xu Yan, Dong Wang, Gerald Schaefer, Hui Fang, Bin Zhao, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16788">https://arxiv.org/abs/2403.16788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16788">https://arxiv.org/pdf/2403.16788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16788]] HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic  Segmentation(https://arxiv.org/abs/2403.16788)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Event-based semantic segmentation has gained popularity due to its capability to deal with scenarios under high-speed motion and extreme lighting conditions, which cannot be addressed by conventional RGB cameras. Since it is hard to annotate event data, previous approaches rely on event-to-image reconstruction to obtain pseudo labels for training. However, this will inevitably introduce noise, and learning from noisy pseudo labels, especially when generated from a single source, may reinforce the errors. This drawback is also called confirmation bias in pseudo-labeling. In this paper, we propose a novel hybrid pseudo-labeling framework for unsupervised event-based semantic segmentation, HPL-ESS, to alleviate the influence of noisy pseudo labels. In particular, we first employ a plain unsupervised domain adaptation framework as our baseline, which can generate a set of pseudo labels through self-training. Then, we incorporate offline event-to-image reconstruction into the framework, and obtain another set of pseudo labels by predicting segmentation maps on the reconstructed images. A noisy label learning strategy is designed to mix the two sets of pseudo labels and enhance the quality. Moreover, we propose a soft prototypical alignment module to further improve the consistency of target domain features. Extensive experiments show that our proposed method outperforms existing state-of-the-art methods by a large margin on the DSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses several supervised methods.</li>
</ul>

<h3>Title: Iso-Diffusion: Improving Diffusion Probabilistic Models Using the  Isotropy of the Additive Gaussian Noise</h3>
<ul>
<li><strong>Authors: </strong>Dilum Fernando, Dhananjaya jayasundara, Roshan Godaliyadda, Chaminda Bandara, Parakrama Ekanayake, Vijitha Herath</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16790">https://arxiv.org/abs/2403.16790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16790">https://arxiv.org/pdf/2403.16790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16790]] Iso-Diffusion: Improving Diffusion Probabilistic Models Using the  Isotropy of the Additive Gaussian Noise(https://arxiv.org/abs/2403.16790)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in the realm of generative AI. Despite their high performance, there is room for improvement, especially in terms of sample fidelity by utilizing statistical properties that impose structural integrity, such as isotropy. Minimizing the mean squared error between the additive and predicted noise alone does not impose constraints on the predicted noise to be isotropic. Thus, we were motivated to utilize the isotropy of the additive noise as a constraint on the objective function to enhance the fidelity of DDPMs. Our approach is simple and can be applied to any DDPM variant. We validate our approach by presenting experiments conducted on four synthetic 2D datasets as well as on unconditional image generation. As demonstrated by the results, the incorporation of this constraint improves the fidelity metrics, Precision and Density for the 2D datasets as well as for the unconditional image generation.</li>
</ul>

<h3>Title: Iterative Refinement of Project-Level Code Context for Precise Code  Generation with Compiler Feedback</h3>
<ul>
<li><strong>Authors: </strong>Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, Hai Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16792">https://arxiv.org/abs/2403.16792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16792">https://arxiv.org/pdf/2403.16792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16792]] Iterative Refinement of Project-Level Code Context for Precise Code  Generation with Compiler Feedback(https://arxiv.org/abs/2403.16792)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable progress in automated code generation. Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate ProCoder with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that ProCoder significantly improves the vanilla LLMs by over 80% in generating code dependent on project context, and consistently outperforms the existing retrieval-based code generation baselines.</li>
</ul>

<h3>Title: CurbNet: Curb Detection Framework Based on LiDAR Point Cloud  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Guoyang Zhao, Fulong Ma, Yuxuan Liu, Weiqing Qi, Ming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16794">https://arxiv.org/abs/2403.16794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16794">https://arxiv.org/pdf/2403.16794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16794]] CurbNet: Curb Detection Framework Based on LiDAR Point Cloud  Segmentation(https://arxiv.org/abs/2403.16794)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Curb detection is an important function in intelligent driving and can be used to determine drivable areas of the road. However, curbs are difficult to detect due to the complex road environment. This paper introduces CurbNet, a novel framework for curb detection, leveraging point cloud segmentation. Addressing the dearth of comprehensive curb datasets and the absence of 3D annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames, which represents the largest and most categorically diverse collection of curb point clouds currently available. Recognizing that curbs are primarily characterized by height variations, our approach harnesses spatially-rich 3D point clouds for training. To tackle the challenges presented by the uneven distribution of curb features on the xy-plane and their reliance on z-axis high-frequency features, we introduce the multi-scale and channel attention (MSCA) module, a bespoke solution designed to optimize detection performance. Moreover, we propose an adaptive weighted loss function group, specifically formulated to counteract the imbalance in the distribution of curb point clouds relative to other categories. Our extensive experimentation on 2 major datasets has yielded results that surpass existing benchmarks set by leading curb detection and point cloud segmentation models. By integrating multi-clustering and curve fitting techniques in our post-processing stage, we have substantially reduced noise in curb detection, thereby enhancing precision to 0.8744. Notably, CurbNet has achieved an exceptional average metrics of over 0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark. Furthermore, corroborative real-world experiments and dataset analyzes mutually validate each other, solidifying CurbNet's superior detection proficiency and its robust generalizability.</li>
</ul>

<h3>Title: Multiple-Source Localization from a Single-Snapshot Observation Using  Graph Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zonghan Zhang, Zijian Zhang, Zhiqian Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16818">https://arxiv.org/abs/2403.16818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16818">https://arxiv.org/pdf/2403.16818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16818]] Multiple-Source Localization from a Single-Snapshot Observation Using  Graph Bayesian Optimization(https://arxiv.org/abs/2403.16818)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Due to the significance of its various applications, source localization has garnered considerable attention as one of the most important means to confront diffusion hazards. Multi-source localization from a single-snapshot observation is especially relevant due to its prevalence. However, the inherent complexities of this problem, such as limited information, interactions among sources, and dependence on diffusion models, pose challenges to resolution. Current methods typically utilize heuristics and greedy selection, and they are usually bonded with one diffusion model. Consequently, their effectiveness is constrained. To address these limitations, we propose a simulation-based method termed BOSouL. Bayesian optimization (BO) is adopted to approximate the results for its sample efficiency. A surrogate function models uncertainty from the limited information. It takes sets of nodes as the input instead of individual nodes. BOSouL can incorporate any diffusion model in the data acquisition process through simulations. Empirical studies demonstrate that its performance is robust across graph structures and diffusion models. The code is available at https://github.com/XGraph-Team/BOSouL.</li>
</ul>

<h3>Title: Convergence of a model-free entropy-regularized inverse reinforcement  learning algorithm</h3>
<ul>
<li><strong>Authors: </strong>Titouan Renard, Andreas Schlaginhaufen, Tingting Ni, Maryam Kamgarpour</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16829">https://arxiv.org/abs/2403.16829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16829">https://arxiv.org/pdf/2403.16829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16829]] Convergence of a model-free entropy-regularized inverse reinforcement  learning algorithm(https://arxiv.org/abs/2403.16829)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\varepsilon$-optimal using $\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\varepsilon$-close to the expert policy in total variation distance.</li>
</ul>

<h3>Title: UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation  Model for Urban Indicator Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xixuan Hao, Wei Chen, Yibo Yan, Siru Zhong, Kun Wang, Qingsong Wen, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16831">https://arxiv.org/abs/2403.16831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16831">https://arxiv.org/pdf/2403.16831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16831]] UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation  Model for Urban Indicator Prediction(https://arxiv.org/abs/2403.16831)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Urban indicator prediction aims to infer socio-economic metrics in diverse urban landscapes using data-driven methods. However, prevalent pre-trained models, particularly those reliant on satellite imagery, face dual challenges. Firstly, concentrating solely on macro-level patterns from satellite data may introduce bias, lacking nuanced details at micro levels, such as architectural details at a place. Secondly, the lack of interpretability in pre-trained models limits their utility in providing transparent evidence for urban planning. In response to these issues, we devise a novel Vision-Language Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates multi-granularity information from both macro (satellite) and micro (street-view) levels, overcoming the limitations of prior pre-trained models. Moreover, it introduces automatic text generation and calibration, elevating interpretability in downstream applications by producing high-quality text descriptions of urban imagery. Rigorous experiments conducted across six socio-economic tasks underscore UrbanVLP's superior performance. We also deploy a web platform to verify its practicality.</li>
</ul>

<h3>Title: Do LLM Agents Have Regret? A Case Study in Online Learning and Games</h3>
<ul>
<li><strong>Authors: </strong>Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16843">https://arxiv.org/abs/2403.16843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16843">https://arxiv.org/pdf/2403.16843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16843]] Do LLM Agents Have Regret? A Case Study in Online Learning and Games(https://arxiv.org/abs/2403.16843)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.</li>
</ul>

<h3>Title: GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Zhan Qu, Daniel Gomm, Michael Färber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16846">https://arxiv.org/abs/2403.16846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16846">https://arxiv.org/pdf/2403.16846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16846]] GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs(https://arxiv.org/abs/2403.16846)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs with time-varying interactions, face a significant challenge in explainability due to their complex model structure. Counterfactual explanations, crucial for understanding model decisions, examine how input graph changes affect outcomes. This paper introduces two novel counterfactual explanation methods for TGNNs: GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer for Dynamic Graphs). They treat explanations as a search problem, seeking input graph alterations that alter model predictions. GreeDy uses a simple, greedy approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm. Experiments show both methods effectively generate clear explanations. Notably, CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher success rate in finding significant counterfactual inputs. This highlights CoDy's potential in clarifying TGNN decision-making, increasing their transparency and trustworthiness in practice.</li>
</ul>

<h3>Title: Multiple Object Tracking as ID Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ruopeng Gao, Yijun Zhang, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16848">https://arxiv.org/abs/2403.16848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16848">https://arxiv.org/pdf/2403.16848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16848]] Multiple Object Tracking as ID Prediction(https://arxiv.org/abs/2403.16848)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In Multiple Object Tracking (MOT), tracking-by-detection methods have stood the test for a long time, which split the process into two parts according to the definition: object detection and association. They leverage robust single-frame detectors and treat object association as a post-processing step through hand-crafted heuristic algorithms and surrogate tasks. However, the nature of heuristic techniques prevents end-to-end exploitation of training data, leading to increasingly cumbersome and challenging manual modification while facing complicated or novel scenarios. In this paper, we regard this object association task as an End-to-End in-context ID prediction problem and propose a streamlined baseline called MOTIP. Specifically, we form the target embeddings into historical trajectory information while considering the corresponding IDs as in-context prompts, then directly predict the ID labels for the objects in the current frame. Thanks to this end-to-end process, MOTIP can learn tracking capabilities straight from training data, freeing itself from burdensome hand-crafted algorithms. Without bells and whistles, our method achieves impressive state-of-the-art performance in complex scenarios like DanceTrack and SportsMOT, and it performs competitively with other transformer-based methods on MOT17. We believe that MOTIP demonstrates remarkable potential and can serve as a starting point for future research. The code is available at https://github.com/MCG-NJU/MOTIP.</li>
</ul>

<h3>Title: Towards Explainability in Legal Outcome Prediction Models</h3>
<ul>
<li><strong>Authors: </strong>Josef Valvoda, Ryan Cotterell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16852">https://arxiv.org/abs/2403.16852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16852">https://arxiv.org/pdf/2403.16852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16852]] Towards Explainability in Legal Outcome Prediction Models(https://arxiv.org/abs/2403.16852)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.</li>
</ul>

<h3>Title: An Expert is Worth One Token: Synergizing Multiple Expert LLMs as  Generalist via Expert Token Routing</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Chai, Guoyin Wang, Jing Su, Tianjie Zhang, Xuanwen Huang, Xuwu Wang, Jingjing Xu, Jianbo Yuan, Hongxia Yang, Fei Wu, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16854">https://arxiv.org/abs/2403.16854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16854">https://arxiv.org/pdf/2403.16854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16854]] An Expert is Worth One Token: Synergizing Multiple Expert LLMs as  Generalist via Expert Token Routing(https://arxiv.org/abs/2403.16854)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.</li>
</ul>

<h3>Title: CipherFormer: Efficient Transformer Private Inference with Low Round  Complexity</h3>
<ul>
<li><strong>Authors: </strong>Weize Wang, Yi Kuang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16860">https://arxiv.org/abs/2403.16860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16860">https://arxiv.org/pdf/2403.16860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16860]] CipherFormer: Efficient Transformer Private Inference with Low Round  Complexity(https://arxiv.org/abs/2403.16860)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>There is a growing trend to outsource the inference task of large transformer models to cloud servers. However, this poses a severe threat to users' private data as they are exposed to cloud servers after uploading. Although several works attempted to provide private inference for transformer models, their hundreds of communication rounds limit the application scenarios. Motivated by the desire to minimize round complexity, we propose CipherFormer, a novel transformer private inference scheme using homomorphic encryption and garbled circuits. We present a protocol for quickly computing homomorphic matrix multiplications. We then modify the attention mechanism and design the corresponding garbled circuits. Furthermore, we show how to use a lightweight attention mechanism and mixed-bitwidth to reduce the inference latency while maintaining accuracy. In comparison with an advanced homomorphic encryption scheme on text classification tasks, our model improves accuracy by 3% to 11% while performing private inference with a 7.7x-11.9x speedup.</li>
</ul>

<h3>Title: Encoding of lexical tone in self-supervised models of spoken language</h3>
<ul>
<li><strong>Authors: </strong>Gaofei Shen, Michaela Watkins, Afra Alishahi, Arianna Bisazza, Grzegorz Chrupała</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16865">https://arxiv.org/abs/2403.16865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16865">https://arxiv.org/pdf/2403.16865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16865]] Encoding of lexical tone in self-supervised models of spoken language(https://arxiv.org/abs/2403.16865)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.</li>
</ul>

<h3>Title: Discrete Latent Graph Generative Modeling with Diffusion Bridges</h3>
<ul>
<li><strong>Authors: </strong>Van Khoa Nguyen, Yoann Boget, Frantzeska Lavda, Alexandros Kalousis</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16883">https://arxiv.org/abs/2403.16883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16883">https://arxiv.org/pdf/2403.16883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16883]] Discrete Latent Graph Generative Modeling with Diffusion Bridges(https://arxiv.org/abs/2403.16883)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLAD the first latent space graph generative model with competitive performance. Our source code is published at: \url{https://github.com/v18nguye/GLAD}.</li>
</ul>

<h3>Title: CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance  Fields from Sparse Inputs</h3>
<ul>
<li><strong>Authors: </strong>Yingji Zhong, Lanqing Hong, Zhenguo Li, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16885">https://arxiv.org/abs/2403.16885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16885">https://arxiv.org/pdf/2403.16885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16885]] CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance  Fields from Sparse Inputs(https://arxiv.org/abs/2403.16885)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRF) have shown impressive capabilities for photorealistic novel view synthesis when trained on dense inputs. However, when trained on sparse inputs, NeRF typically encounters issues of incorrect density or color predictions, mainly due to insufficient coverage of the scene causing partial and sparse supervision, thus leading to significant performance degradation. While existing works mainly consider ray-level consistency to construct 2D learning regularization based on rendered color, depth, or semantics on image planes, in this paper we propose a novel approach that models 3D spatial field consistency to improve NeRF's performance with sparse inputs. Specifically, we first adopt a voxel-based ray sampling strategy to ensure that the sampled rays intersect with a certain voxel in 3D space. We then randomly sample additional points within the voxel and apply a Transformer to infer the properties of other points on each ray, which are then incorporated into the volume rendering. By backpropagating through the rendering loss, we enhance the consistency among neighboring points. Additionally, we propose to use a contrastive loss on the encoder output of the Transformer to further improve consistency within each voxel. Experiments demonstrate that our method yields significant improvement over different radiance fields in the sparse inputs setting, and achieves comparable performance with current works.</li>
</ul>

<h3>Title: Towards Secure and Trusted-by-Design Smart Contracts</h3>
<ul>
<li><strong>Authors: </strong>Zaynah Dargaye, Önder Gürcan, Florent Kirchner, Sara Tucci-Piergiovanni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16903">https://arxiv.org/abs/2403.16903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16903">https://arxiv.org/pdf/2403.16903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16903]] Towards Secure and Trusted-by-Design Smart Contracts(https://arxiv.org/abs/2403.16903)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Distributed immutable ledgers, or blockchains, allow the secure digitization of evidential transactions without relying on a trusted third-party. Evidential transactions involve the exchange of any form of physical evidence, such as money, birth certificate, visas, tickets, etc. Most of the time, evidential transactions occur in the context of complex procedures, called evidential protocols, among physical agents. The blockchain provides the mechanisms to transfer evidence, while smart contracts - programs executing within the blockchain in a decentralized and replicated fashion - allow encoding evidential protocols on top of a blockchain. As a smart contract foregoes trusted third-parties and runs on several machines anonymously, it constitutes a highly critical program that has to be secure and trusted-by-design. While most of the current smart contract languages focus on easy programmability, they do not directly address the need of guaranteeing trust and accountability, which becomes a significant issue when evidential protocols are encoded as smart contracts.</li>
</ul>

<h3>Title: New Intent Discovery with Attracting and Dispersing Prototype</h3>
<ul>
<li><strong>Authors: </strong>Shun Zhang, Jian Yang, Jiaqi Bai, Chaoran Yan, Tongliang Li, Zhao Yan, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16913">https://arxiv.org/abs/2403.16913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16913">https://arxiv.org/pdf/2403.16913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16913]] New Intent Discovery with Attracting and Dispersing Prototype(https://arxiv.org/abs/2403.16913)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>New Intent Discovery (NID) aims to recognize known and infer new intent categories with the help of limited labeled and large-scale unlabeled data. The task is addressed as a feature-clustering problem and recent studies augment instance representation. However, existing methods fail to capture cluster-friendly representations, since they show less capability to effectively control and coordinate within-cluster and between-cluster distances. Tailored to the NID problem, we propose a Robust and Adaptive Prototypical learning (RAP) framework for globally distinct decision boundaries for both known and new intent categories. Specifically, a robust prototypical attracting learning (RPAL) method is designed to compel instances to gravitate toward their corresponding prototype, achieving greater within-cluster compactness. To attain larger between-cluster separation, another adaptive prototypical dispersing learning (APDL) method is devised to maximize the between-cluster distance from the prototype-to-prototype perspective. Experimental results evaluated on three challenging benchmarks (CLINC, BANKING, and StackOverflow) of our method with better cluster-friendly representation demonstrate that RAP brings in substantial improvements over the current state-of-the-art methods (even large language model) by a large margin (average +5.5% improvement).</li>
</ul>

<h3>Title: PropTest: Automatic Property Testing for Improved Visual Programming</h3>
<ul>
<li><strong>Authors: </strong>Jaywon Koo, Ziyan Yang, Paola Cascante-Bonilla, Baishakhi Ray, Vicente Ordonez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16921">https://arxiv.org/abs/2403.16921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16921">https://arxiv.org/pdf/2403.16921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16921]] PropTest: Automatic Property Testing for Improved Visual Programming(https://arxiv.org/abs/2403.16921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual Programming has emerged as an alternative to end-to-end black-box visual reasoning models. This type of methods leverage Large Language Models (LLMs) to decompose a problem and generate the source code for an executable computer program. This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data. We propose PropTest, a general strategy that improves visual programming by further using an LLM to generate code that tests for visual properties in an initial round of proposed solutions. Particularly, our method tests for data-type consistency, as well as syntactic and semantic properties in the generated solutions. Our proposed solution outperforms baselines and achieves comparable results to state-of-the-art methods while using smaller and publicly available LLMs (CodeLlama-7B and WizardCoder-15B). This is demonstrated across different benchmarks on visual question answering and referring expression comprehension, showing the efficacy of our approach in enhancing the performance and generalization of visual reasoning tasks. Specifically, PropTest improves ViperGPT by obtaining 48.66% accuracy (+8.3%) on the A-OKVQA benchmark and 52.8% (+3.3%) on the RefCOCO+ benchmark using CodeLlama-7B.</li>
</ul>

<h3>Title: FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN</h3>
<ul>
<li><strong>Authors: </strong>Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16930">https://arxiv.org/abs/2403.16930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16930">https://arxiv.org/pdf/2403.16930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16930]] FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN(https://arxiv.org/abs/2403.16930)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, generative</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) provides a privacy-preserving mechanism for distributed training of machine learning models on networked devices (e.g., mobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the edge by creating models without sharing the actual data across the network. Existing research works typically focus on generic aspects of non-IID data and heterogeneity in client's system characteristics, but they often neglect the issue of insufficient data for model development, which can arise from uneven class label distribution and highly variable data volumes across edge nodes. In this work, we propose FLIGAN, a novel approach to address the issue of data incompleteness in FL. First, we leverage Generative Adversarial Networks (GANs) to adeptly capture complex data distributions and generate synthetic data that closely resemble the real-world data. Then, we use synthetic data to enhance the robustness and completeness of datasets across nodes. Our methodology adheres to FL's privacy requirements by generating synthetic data in a federated manner without sharing the actual data in the process. We incorporate techniques such as classwise sampling and node grouping, designed to improve the federated GAN's performance, enabling the creation of high-quality synthetic datasets and facilitating efficient FL training. Empirical results from our experiments demonstrate that FLIGAN significantly improves the model accuracy, especially in scenarios with high class imbalances, achieving up to a 20% increase in model accuracy over traditional FL baselines.</li>
</ul>

<h3>Title: SPACE-IDEAS: A Dataset for Salient Information Detection in Space  Innovation</h3>
<ul>
<li><strong>Authors: </strong>Andrés García-Silva, Cristian Berrío, José Manuel Gómez-Pérez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16941">https://arxiv.org/abs/2403.16941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16941">https://arxiv.org/pdf/2403.16941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16941]] SPACE-IDEAS: A Dataset for Salient Information Detection in Space  Innovation(https://arxiv.org/abs/2403.16941)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Detecting salient parts in text using natural language processing has been widely used to mitigate the effects of information overflow. Nevertheless, most of the datasets available for this task are derived mainly from academic publications. We introduce SPACE-IDEAS, a dataset for salient information detection from innovation ideas related to the Space domain. The text in SPACE-IDEAS varies greatly and includes informal, technical, academic and business-oriented writing styles. In addition to a manually annotated dataset we release an extended version that is annotated using a large generative language model. We train different sentence and sequential sentence classifiers, and show that the automatically annotated dataset can be leveraged using multitask learning to train better classifiers.</li>
</ul>

<h3>Title: Aligning with Human Judgement: The Role of Pairwise Preference in Large  Language Model Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16950">https://arxiv.org/abs/2403.16950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16950">https://arxiv.org/pdf/2403.16950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16950]] Aligning with Human Judgement: The Role of Pairwise Preference in Large  Language Model Evaluators(https://arxiv.org/abs/2403.16950)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of LLMs and demonstrate how PAIRS benefits from calibration.</li>
</ul>

<h3>Title: Data Mixing Laws: Optimizing Data Mixtures by Predicting Language  Modeling Performance</h3>
<ul>
<li><strong>Authors: </strong>Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16952">https://arxiv.org/abs/2403.16952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16952">https://arxiv.org/pdf/2403.16952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16952]] Data Mixing Laws: Optimizing Data Mixtures by Predicting Language  Modeling Performance(https://arxiv.org/abs/2403.16952)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 1B model trained for 100B tokens in RedPajama, reaching a performance comparable to the one trained for 48% more steps on the default mixture. Extending the application of data mixing laws to continual training accurately predicts the critical mixture proportion that avoids catastrophic forgetting and outlooks the potential for dynamic data schedules</li>
</ul>

<h3>Title: Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation  Training-Freely with Isolated Diffusion Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16954">https://arxiv.org/abs/2403.16954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16954">https://arxiv.org/pdf/2403.16954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16954]] Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation  Training-Freely with Isolated Diffusion Guidance(https://arxiv.org/abs/2403.16954)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-image diffusion models have achieved great success in synthesizing high-quality and diverse images given target text prompts. Despite the revolutionary image generation ability, current state-of-the-art models still struggle to deal with multi-concept generation accurately in many cases. This phenomenon is known as ``concept bleeding" and displays as the unexpected overlapping or merging of various concepts. This paper presents a general approach for text-to-image diffusion models to address the mutual interference between different subjects and their attachments in complex scenes, pursuing better text-image consistency. The core idea is to isolate the synthesizing processes of different concepts. We propose to bind each attachment to corresponding subjects separately with split text prompts. Besides, we introduce a revision method to fix the concept bleeding problem in multi-subject synthesis. We first depend on pre-trained object detection and segmentation models to obtain the layouts of subjects. Then we isolate and resynthesize each subject individually with corresponding text prompts to avoid mutual interference. Overall, we achieve a training-free strategy, named Isolated Diffusion, to optimize multi-concept text-to-image synthesis. It is compatible with the latest Stable Diffusion XL (SDXL) and prior Stable Diffusion (SD) models. We compare our approach with alternative methods using a variety of multi-concept text prompts and demonstrate its effectiveness with clear advantages in text-image consistency and user study.</li>
</ul>

<h3>Title: TwinLiteNetPlus: A Stronger Model for Real-time Drivable Area and Lane  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Quang-Huy Che, Duc-Tri Le, Minh-Quan Pham, Vinh-Tiep Nguyen, Duc-Khai Lam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16958">https://arxiv.org/abs/2403.16958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16958">https://arxiv.org/pdf/2403.16958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16958]] TwinLiteNetPlus: A Stronger Model for Real-time Drivable Area and Lane  Segmentation(https://arxiv.org/abs/2403.16958)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is crucial for autonomous driving, particularly for Drivable Area and Lane Segmentation, ensuring safety and navigation. To address the high computational costs of current state-of-the-art (SOTA) models, this paper introduces TwinLiteNetPlus (TwinLiteNet$^+$), a model adept at balancing efficiency and accuracy. TwinLiteNet$^+$ incorporates standard and depth-wise separable dilated convolutions, reducing complexity while maintaining high accuracy. It is available in four configurations, from the robust 1.94 million-parameter TwinLiteNet$^+_{\text{Large}}$ to the ultra-compact 34K-parameter TwinLiteNet$^+_{\text{Nano}}$. Notably, TwinLiteNet$^+_{\text{Large}}$ attains a 92.9\% mIoU for Drivable Area Segmentation and a 34.2\% IoU for Lane Segmentation. These results notably outperform those of current SOTA models while requiring a computational cost that is approximately 11 times lower in terms of Floating Point Operations (FLOPs) compared to the existing SOTA model. Extensively tested on various embedded devices, TwinLiteNet$^+$ demonstrates promising latency and power efficiency, underscoring its suitability for real-world autonomous vehicle applications.</li>
</ul>

<h3>Title: Be Yourself: Bounded Attention for Multi-Subject Text-to-Image  Generation</h3>
<ul>
<li><strong>Authors: </strong>Omer Dahary, Or Patashnik, Kfir Aberman, Daniel Cohen-Or</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16990">https://arxiv.org/abs/2403.16990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16990">https://arxiv.org/pdf/2403.16990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16990]] Be Yourself: Bounded Attention for Multi-Subject Text-to-Image  Generation(https://arxiv.org/abs/2403.16990)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for bounding the information flow in the sampling process. Bounded Attention prevents detrimental leakage among subjects and enables guiding the generation to promote each subject's individuality, even with complex multi-subject conditioning. Through extensive experimentation, we demonstrate that our method empowers the generation of multiple subjects that better align with given prompts and layouts.</li>
</ul>

<h3>Title: Comp4D: LLM-Guided Compositional 4D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Dejia Xu, Hanwen Liang, Neel P. Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N. Plataniotis, Zhangyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16993">https://arxiv.org/abs/2403.16993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16993">https://arxiv.org/pdf/2403.16993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16993]] Comp4D: LLM-Guided Compositional 4D Scene Generation(https://arxiv.org/abs/2403.16993)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models for 2D and 3D content creation have sparked a surge of interest in generating 4D content. However, the scarcity of 3D scene datasets constrains current methodologies to primarily object-centric generation. To overcome this limitation, we present Comp4D, a novel framework for Compositional 4D Generation. Unlike conventional methods that generate a singular 4D representation of the entire scene, Comp4D innovatively constructs each 4D object within the scene separately. Utilizing Large Language Models (LLMs), the framework begins by decomposing an input text prompt into distinct entities and maps out their trajectories. It then constructs the compositional 4D scene by accurately positioning these objects along their designated paths. To refine the scene, our method employs a compositional score distillation technique guided by the pre-defined trajectories, utilizing pre-trained diffusion models across text-to-image, text-to-video, and text-to-3D domains. Extensive experiments demonstrate our outstanding 4D content creation capability compared to prior arts, showcasing superior visual quality, motion fidelity, and enhanced object interactions.</li>
</ul>

<h3>Title: Language Rectified Flow: Advancing Diffusion Language Generation with  Probabilistic Flows</h3>
<ul>
<li><strong>Authors: </strong>Shujian Zhang, Lemeng Wu, Chengyue Gong, Xingchao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16995">https://arxiv.org/abs/2403.16995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16995">https://arxiv.org/pdf/2403.16995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16995]] Language Rectified Flow: Advancing Diffusion Language Generation with  Probabilistic Flows(https://arxiv.org/abs/2403.16995)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effectively decreases the inference time. Experiments on three challenging fine-grained control tasks and multiple high-quality text editing show that our method consistently outperforms its baselines. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.</li>
</ul>

<h3>Title: DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving</h3>
<ul>
<li><strong>Authors: </strong>Tianqi Wang, Enze Xie, Ruihang Chu, Zhenguo Li, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16996">https://arxiv.org/abs/2403.16996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16996">https://arxiv.org/pdf/2403.16996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16996]] DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving(https://arxiv.org/abs/2403.16996)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>End-to-end driving has made significant progress in recent years, demonstrating benefits such as system simplicity and competitive driving performance under both open-loop and closed-loop settings. Nevertheless, the lack of interpretability and controllability in its driving decisions hinders real-world deployment for end-to-end driving systems. In this paper, we collect a comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA simulator. It contains sensor data, control decisions, and chain-of-thought labels to indicate the reasoning process. We utilize the challenging driving scenarios from the CARLA leaderboard 2.0, which involve high-speed driving and lane-changing, and propose a rule-based expert policy to control the vehicle and generate ground truth labels for its reasoning process across different driving aspects and the final decisions. This dataset can serve as an open-loop end-to-end driving benchmark, enabling the evaluation of accuracy in various chain-of-thought aspects and the final decision. In addition, we propose a baseline model called DriveCoT-Agent, trained on our dataset, to generate chain-of-thought predictions and final decisions. The trained model exhibits strong performance in both open-loop and closed-loop evaluations, demonstrating the effectiveness of our proposed dataset.</li>
</ul>

<h3>Title: Understanding Long Videos in One Multimodal Language Model Pass</h3>
<ul>
<li><strong>Authors: </strong>Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16998">https://arxiv.org/abs/2403.16998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16998">https://arxiv.org/pdf/2403.16998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16998]] Understanding Long Videos in One Multimodal Language Model Pass(https://arxiv.org/abs/2403.16998)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), known to contain a strong awareness of world knowledge, have allowed recent approaches to achieve excellent performance on Long-Video Understanding benchmarks, but at high inference costs. In this work, we first propose Likelihood Selection, a simple technique that unlocks faster inference in autoregressive LLMs for multiple-choice tasks common in long-video benchmarks. In addition to faster inference, we discover the resulting models to yield surprisingly good accuracy on long-video tasks, even with no video specific information. Building on this, we inject video-specific object-centric information extracted from off-the-shelf pre-trained models and utilize natural language as a medium for information fusion. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across long-video and fine-grained action recognition benchmarks. Code available at: https://github.com/kahnchana/mvu</li>
</ul>

<h3>Title: Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.16999">https://arxiv.org/abs/2403.16999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.16999">https://arxiv.org/pdf/2403.16999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.16999]] Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal  Language Models(https://arxiv.org/abs/2403.16999)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents Visual CoT, a novel pipeline that leverages the reasoning capabilities of multi-modal large language models (MLLMs) by incorporating visual Chain-of-Thought (CoT) reasoning. While MLLMs have shown promise in various visual tasks, they often lack interpretability and struggle with complex visual inputs. To address these challenges, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We collect and introduce the Visual CoT dataset comprising 373k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Importantly, the introduced benchmark is capable of evaluating MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available to foster further research in this direction.</li>
</ul>

<h3>Title: Learning Spatial Adaptation and Temporal Coherence in Diffusion Models  for Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, Tao Mei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17000">https://arxiv.org/abs/2403.17000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17000">https://arxiv.org/pdf/2403.17000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17000]] Learning Spatial Adaptation and Temporal Coherence in Diffusion Models  for Video Super-Resolution(https://arxiv.org/abs/2403.17000)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models are just at a tipping point for image super-resolution task. Nevertheless, it is not trivial to capitalize on diffusion models for video super-resolution which necessitates not only the preservation of visual appearance from low-resolution to high-resolution videos, but also the temporal consistency across video frames. In this paper, we propose a novel approach, pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video super-resolution. SATeCo pivots on learning spatial-temporal guidance from low-resolution videos to calibrate both latent-space high-resolution video denoising and pixel-space video reconstruction. Technically, SATeCo freezes all the parameters of the pre-trained UNet and VAE, and only optimizes two deliberately-designed spatial feature adaptation (SFA) and temporal feature alignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame features via adaptively estimating affine parameters for each pixel, guaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA delves into feature interaction within a 3D local window (tubelet) through self-attention, and executes cross-attention between tubelet and its low-resolution counterpart to guide temporal feature alignment. Extensive experiments conducted on the REDS4 and Vid4 datasets demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17001">https://arxiv.org/abs/2403.17001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17001">https://arxiv.org/pdf/2403.17001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17001]] VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation(https://arxiv.org/abs/2403.17001)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at https://vp3d-cvpr24.github.io.</li>
</ul>

<h3>Title: SD-DiT: Unleashing the Power of Self-supervised Discrimination in  Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, Chang Wen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17004">https://arxiv.org/abs/2403.17004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17004">https://arxiv.org/pdf/2403.17004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17004]] SD-DiT: Unleashing the Power of Self-supervised Discrimination in  Diffusion Transformer(https://arxiv.org/abs/2403.17004)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT) has emerged as the new trend of generative diffusion models on image generation. In view of extremely slow convergence in typical DiT, recent breakthroughs have been driven by mask strategy that significantly improves the training efficiency of DiT with additional intra-image contextual learning. Despite this progress, mask strategy still suffers from two inherent limitations: (a) training-inference discrepancy and (b) fuzzy relations between mask reconstruction & generative diffusion process, resulting in sub-optimal training of DiT. In this work, we address these limitations by novelly unleashing the self-supervised discrimination knowledge to boost DiT training. Technically, we frame our DiT in a teacher-student manner. The teacher-student discriminative pairs are built on the diffusion noises along the same Probability Flow Ordinary Differential Equation (PF-ODE). Instead of applying mask reconstruction loss over both DiT encoder and decoder, we decouple DiT encoder and decoder to separately tackle discriminative and generative objectives. In particular, by encoding discriminative pairs with student and teacher DiT encoders, a new discriminative loss is designed to encourage the inter-image alignment in the self-supervised embedding space. After that, student samples are fed into student DiT decoder to perform the typical generative diffusion task. Extensive experiments are conducted on ImageNet dataset, and our method achieves a competitive balance between training cost and generative capacity.</li>
</ul>

<h3>Title: TRIP: Temporal Residual Learning with Image Noise Prior for  Image-to-Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17005">https://arxiv.org/abs/2403.17005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17005">https://arxiv.org/pdf/2403.17005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17005]] TRIP: Temporal Residual Learning with Image Noise Prior for  Image-to-Video Diffusion Models(https://arxiv.org/abs/2403.17005)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-video generation have demonstrated the utility of powerful diffusion models. Nevertheless, the problem is not trivial when shaping diffusion models to animate static image (i.e., image-to-video generation). The difficulty originates from the aspect that the diffusion process of subsequent animated frames should not only preserve the faithful alignment with the given image but also pursue temporal coherence among adjacent frames. To alleviate this, we present TRIP, a new recipe of image-to-video diffusion paradigm that pivots on image noise prior derived from static image to jointly trigger inter-frame relational reasoning and ease the coherent temporal modeling via temporal residual learning. Technically, the image noise prior is first attained through one-step backward diffusion process based on both static image and noised video latent codes. Next, TRIP executes a residual-like dual-path scheme for noise prediction: 1) a shortcut path that directly takes image noise prior as the reference noise of each frame to amplify the alignment between the first frame and subsequent frames; 2) a residual path that employs 3D-UNet over noised video and static image latent codes to enable inter-frame relational reasoning, thereby easing the learning of the residual noise for each frame. Furthermore, both reference and residual noise of each frame are dynamically merged via attention mechanism for final video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT datasets demonstrate the effectiveness of our TRIP for image-to-video generation. Please see our project page at https://trip-i2v.github.io/TRIP/.</li>
</ul>

<h3>Title: Invertible Diffusion Models for Compressed Sensing</h3>
<ul>
<li><strong>Authors: </strong>Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17006">https://arxiv.org/abs/2403.17006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17006">https://arxiv.org/pdf/2403.17006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17006]] Invertible Diffusion Models for Compressed Sensing(https://arxiv.org/abs/2403.17006)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference.</li>
</ul>

<h3>Title: DreamLIP: Language-Image Pre-training with Long Captions</h3>
<ul>
<li><strong>Authors: </strong>Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, Yujun Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17007">https://arxiv.org/abs/2403.17007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17007">https://arxiv.org/pdf/2403.17007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17007]] DreamLIP: Language-Image Pre-training with Long Captions(https://arxiv.org/abs/2403.17007)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Language-image pre-training largely relies on how precisely and thoroughly a text describes its paired image. In practice, however, the contents of an image can be so rich that well describing them requires lengthy captions (e.g., with 10 sentences), which are usually missing in existing datasets. Consequently, there are currently no clear evidences on whether and how language-image pre-training could benefit from long captions. To figure this out, we first re-caption 30M images with detailed descriptions using a pre-trained Multi-modality Large Language Model (MLLM), and then study the usage of the resulting captions under a contrastive learning framework. We observe that, each sentence within a long caption is very likely to describe the image partially (e.g., an object). Motivated by this, we propose to dynamically sample sub-captions from the text label to construct multiple positive pairs, and introduce a grouping loss to match the embeddings of each sub-caption with its corresponding local image patches in a self-supervised manner. Experimental results on a wide rage of downstream tasks demonstrate the consistent superiority of our method, termed DreamLIP, over previous alternatives, highlighting its fine-grained representational capacity. It is noteworthy that, on the tasks of image-text retrieval and semantic segmentation, our model trained with 30M image-text pairs achieves on par or even better performance than CLIP trained with 400M pairs. Project page is available at https://zyf0619sjtu.github.io/dream-lip.</li>
</ul>

<h3>Title: Optimizing LiDAR Placements for Robust Driving Perception in Adverse  Conditions</h3>
<ul>
<li><strong>Authors: </strong>Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, Xiaonan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17009">https://arxiv.org/abs/2403.17009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17009">https://arxiv.org/pdf/2403.17009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17009]] Optimizing LiDAR Placements for Robust Driving Perception in Adverse  Conditions(https://arxiv.org/abs/2403.17009)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The robustness of driving perception systems under unprecedented conditions is crucial for safety-critical usages. Latest advancements have prompted increasing interests towards multi-LiDAR perception. However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline that encompasses LiDAR placement optimization, data generation, and downstream evaluations. Our framework makes three appealing contributions. 1) To identify the most effective configurations for multi-LiDAR systems, we introduce a Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements. 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 364,000-frame dataset from both clean and adverse conditions. Extensive experiments demonstrate that LiDAR placements optimized using our approach outperform various baselines. We showcase exceptional robustness in both 3D object detection and LiDAR semantic segmentation tasks, under diverse adverse weather and sensor failure conditions. Code and benchmark toolkit are publicly available.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
