<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Secure Lightweight Authentication for Multi User IoT Environment. (arXiv:2207.10353v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10353">http://arxiv.org/abs/2207.10353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10353] Secure Lightweight Authentication for Multi User IoT Environment](http://arxiv.org/abs/2207.10353)</code></li>
<li>Summary: <p>The Internet of Things (IoT) is giving a boost to a plethora of new
opportunities for the robust and sustainable deployment of cyber physical
systems. The cornerstone of any IoT system is the sensing devices. These
sensing devices have considerable resource constraints, including insufficient
battery capacity, CPU capability, and physical security. Because of such
resource constraints, designing lightweight cryptographic protocols is an
opportunity. Remote User Authentication ensures that two parties establish a
secure and durable session key. This study presents a lightweight and safe
authentication strategy for the user-gateway (U GW) IoT network model. The
proposed system is designed leveraging Elliptic Curve Cryptography (ECC). We
undertake a formal security analysis with both the Automated Validation of
Internet Security Protocols (AVISPA) and Burrows Abadi Needham (BAN) logic
tools and an information security assessment with the Delev Yao channel. We use
publish subscribe based Message Queuing Telemetry Transport (MQTT) protocol for
communication. Additionally, the performance analysis and comparison of
security features show that the proposed scheme is resilient to well known
cryptographic threats.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: LPYOLO: Low Precision YOLO for Face Detection on FPGA. (arXiv:2207.10482v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10482">http://arxiv.org/abs/2207.10482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10482] LPYOLO: Low Precision YOLO for Face Detection on FPGA](http://arxiv.org/abs/2207.10482)</code></li>
<li>Summary: <p>In recent years, number of edge computing devices and artificial intelligence
applications on them have advanced excessively. In edge computing, decision
making processes and computations are moved from servers to edge devices.
Hence, cheap and low power devices are required. FPGAs are very low power,
inclined to do parallel operations and deeply suitable devices for running
Convolutional Neural Networks (CNN) which are the fundamental unit of an
artificial intelligence application. Face detection on surveillance systems is
the most expected application on the security market. In this work, TinyYolov3
architecture is redesigned and deployed for face detection. It is a CNN based
object detection method and developed for embedded systems. PYNQ-Z2 is selected
as a target board which has low-end Xilinx Zynq 7020 System-on-Chip (SoC) on
it. Redesigned TinyYolov3 model is defined in numerous bit width precisions
with Brevitas library which brings fundamental CNN layers and activations in
integer quantized form. Then, the model is trained in a quantized structure
with WiderFace dataset. In order to decrease latency and power consumption,
onchip memory of the FPGA is configured as a storage of whole network
parameters and the last activation function is modified as rescaled HardTanh
instead of Sigmoid. Also, high degree of parallelism is applied to logical
resources of the FPGA. The model is converted to an HLS based application with
using FINN framework and FINN-HLS library which includes the layer definitions
in C++. Later, the model is synthesized and deployed. CPU of the SoC is
employed with multithreading mechanism and responsible for preprocessing,
postprocessing and TCP/IP streaming operations. Consequently, 2.4 Watt total
board power consumption, 18 Frames-Per-Second (FPS) throughput and 0.757 mAP
accuracy rate on Easy category of the WiderFace are achieved with 4 bits
precision model.
</p></li>
</ul>

<h3>Title: A Reinforcement Learning-based Offensive semantics Censorship System for Chatbots. (arXiv:2207.10569v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10569">http://arxiv.org/abs/2207.10569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10569] A Reinforcement Learning-based Offensive semantics Censorship System for Chatbots](http://arxiv.org/abs/2207.10569)</code></li>
<li>Summary: <p>The rapid development of artificial intelligence (AI) technology has enabled
large-scale AI applications to land in the market and practice. However, while
AI technology has brought many conveniences to people in the productization
process, it has also exposed many security issues. Especially, attacks against
online learning vulnerabilities of chatbots occur frequently. Therefore, this
paper proposes a semantics censorship chatbot system based on reinforcement
learning, which is mainly composed of two parts: the Offensive semantics
censorship model and the semantics purification model. Offensive semantics
review can combine the context of user input sentences to detect the rapid
evolution of Offensive semantics and respond to Offensive semantics responses.
The semantics purification model For the case of chatting robot models, it has
been contaminated by large numbers of offensive semantics, by strengthening the
offensive reply learned by the learning algorithm, rather than rolling back to
the early versions. In addition, by integrating a once-through learning
approach, the speed of semantics purification is accelerated while reducing the
impact on the quality of replies. The experimental results show that our
proposed approach reduces the probability of the chat model generating
offensive replies and that the integration of the few-shot learning algorithm
improves the training speed rapidly while effectively slowing down the decline
in BLEU values.
</p></li>
</ul>

<h3>Title: High-Level Approaches to Hardware Security: A Tutorial. (arXiv:2207.10466v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10466">http://arxiv.org/abs/2207.10466</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10466] High-Level Approaches to Hardware Security: A Tutorial](http://arxiv.org/abs/2207.10466)</code></li>
<li>Summary: <p>Designers use third-party intellectual property (IP) cores and outsource
various steps in the integrated circuit (IC) design and manufacturing flow. As
a result, security vulnerabilities have been rising. This is forcing IC
designers and end users to re-evaluate their trust in ICs. If attackers get
hold of an unprotected IC, they can reverse engineer the IC and pirate the IP.
Similarly, if attackers get hold of a design, they can insert malicious
circuits or take advantage of "backdoors" in a design. Unintended design bugs
can also result in security weaknesses. This tutorial paper provides an
introduction to the domain of hardware security through two pedagogical
examples of hardware security problems. The first is a walk-through of the scan
chain-based side channel attack. The second is a walk-through of logic locking
of digital designs. The tutorial material is accompanied by open access digital
resources that are linked in this article.
</p></li>
</ul>

<h3>Title: Comparative Study on Supervised versus Semi-supervised Machine Learning for Anomaly Detection of In-vehicle CAN Network. (arXiv:2207.10286v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10286">http://arxiv.org/abs/2207.10286</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10286] Comparative Study on Supervised versus Semi-supervised Machine Learning for Anomaly Detection of In-vehicle CAN Network](http://arxiv.org/abs/2207.10286)</code></li>
<li>Summary: <p>As the central nerve of the intelligent vehicle control system, the
in-vehicle network bus is crucial to the security of vehicle driving. One of
the best standards for the in-vehicle network is the Controller Area Network
(CAN bus) protocol. However, the CAN bus is designed to be vulnerable to
various attacks due to its lack of security mechanisms. To enhance the security
of in-vehicle networks and promote the research in this area, based upon a
large scale of CAN network traffic data with the extracted valuable features,
this study comprehensively compared fully-supervised machine learning with
semi-supervised machine learning methods for CAN message anomaly detection.
Both traditional machine learning models (including single classifier and
ensemble models) and neural network based deep learning models are evaluated.
Furthermore, this study proposed a deep autoencoder based semi-supervised
learning method applied for CAN message anomaly detection and verified its
superiority over other semi-supervised methods. Extensive experiments show that
the fully-supervised methods generally outperform semi-supervised ones as they
are using more information as inputs. Typically the developed XGBoost based
model obtained state-of-the-art performance with the best accuracy (98.65%),
precision (0.9853), and ROC AUC (0.9585) beating other methods reported in the
literature.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Don't Forget Me: Accurate Background Recovery for Text Removal via Modeling Local-Global Context. (arXiv:2207.10273v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10273">http://arxiv.org/abs/2207.10273</a></li>
<li>Code URL: <a href="https://github.com/lcy0604/ctrnet">https://github.com/lcy0604/ctrnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10273] Don't Forget Me: Accurate Background Recovery for Text Removal via Modeling Local-Global Context](http://arxiv.org/abs/2207.10273)</code></li>
<li>Summary: <p>Text removal has attracted increasingly attention due to its various
applications on privacy protection, document restoration, and text editing. It
has shown significant progress with deep neural network. However, most of the
existing methods often generate inconsistent results for complex background. To
address this issue, we propose a Contextual-guided Text Removal Network, termed
as CTRNet. CTRNet explores both low-level structure and high-level
discriminative context feature as prior knowledge to guide the process of
background restoration. We further propose a Local-global Content Modeling
(LGCM) block with CNNs and Transformer-Encoder to capture local features and
establish the long-term relationship among pixels globally. Finally, we
incorporate LGCM with context guidance for feature modeling and decoding.
Experiments on benchmark datasets, SCUT-EnsText and SCUT-Syn show that CTRNet
significantly outperforms the existing state-of-the-art methods. Furthermore, a
qualitative experiment on examination papers also demonstrates the
generalization ability of our method. The codes and supplement materials are
available at https://github.com/lcy0604/CTRNet.
</p></li>
</ul>

<h3>Title: Real-Time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition. (arXiv:2207.10519v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10519">http://arxiv.org/abs/2207.10519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10519] Real-Time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition](http://arxiv.org/abs/2207.10519)</code></li>
<li>Summary: <p>With an increasing number of elders living alone, care-giving from a distance
becomes a compelling need, particularly for safety. Real-time monitoring and
action recognition are essential to raise an alert timely when abnormal
behaviors or unusual activities occur. While wearable sensors are widely
recognized as a promising solution, highly depending on user's ability and
willingness makes them inefficient. In contrast, video streams collected
through non-contact optical cameras provide richer information and release the
burden on elders. In this paper, leveraging the Independently-Recurrent neural
Network (IndRNN) we propose a novel Real-time Elderly Monitoring for senior
Safety (REMS) based on lightweight human action recognition (HAR) technology.
Using captured skeleton images, the REMS scheme is able to recognize abnormal
behaviors or actions and preserve the user's privacy. To achieve high accuracy,
the HAR module is trained and fine-tuned using multiple databases. An extensive
experimental study verified that REMS system performs action recognition
accurately and timely. REMS meets the design goals as a privacy-preserving
elderly safety monitoring system and possesses the potential to be adopted in
various smart monitoring systems.
</p></li>
</ul>

<h3>Title: Improving Privacy-Preserving Vertical Federated Learning by Efficient Communication with ADMM. (arXiv:2207.10226v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10226">http://arxiv.org/abs/2207.10226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10226] Improving Privacy-Preserving Vertical Federated Learning by Efficient Communication with ADMM](http://arxiv.org/abs/2207.10226)</code></li>
<li>Summary: <p>Federated learning (FL) enables distributed devices to jointly train a shared
model while keeping the training data local. Different from the horizontal FL
(HFL) setting where each client has partial data samples, vertical FL (VFL),
which allows each client to collect partial features, has attracted intensive
research efforts recently. In this paper, we identified two challenges that
state-of-the-art VFL frameworks are facing: (1) some works directly average the
learned feature embeddings and therefore might lose the unique properties of
each local feature set; (2) server needs to communicate gradients with the
clients for each training step, incurring high communication cost that leads to
rapid consumption of privacy budgets. In this paper, we aim to address the
above challenges and propose an efficient VFL with multiple linear heads (VIM)
framework, where each head corresponds to local clients by taking the separate
contribution of each client into account. In addition, we propose an
Alternating Direction Method of Multipliers (ADMM)-based method to solve our
optimization problem, which reduces the communication cost by allowing multiple
local updates in each step, and thus leads to better performance under
differential privacy. We consider various settings including VFL with model
splitting and without model splitting. For both settings, we carefully analyze
the differential privacy mechanism for our framework. Moreover, we show that a
byproduct of our framework is that the weights of learned linear heads reflect
the importance of local clients. We conduct extensive evaluations and show that
on four real-world datasets, VIM achieves significantly higher performance and
faster convergence compared with state-of-the-arts. We also explicitly evaluate
the importance of local clients and show that VIM enables functionalities such
as client-level explanation and client denoising.
</p></li>
</ul>

<h3>Title: Widespread Underestimation of Sensitivity in Differentially Private Libraries and How to Fix It. (arXiv:2207.10635v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10635">http://arxiv.org/abs/2207.10635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10635] Widespread Underestimation of Sensitivity in Differentially Private Libraries and How to Fix It](http://arxiv.org/abs/2207.10635)</code></li>
<li>Summary: <p>We identify a new class of vulnerabilities in implementations of differential
privacy. Specifically, they arise when computing basic statistics such as sums,
thanks to discrepancies between the implemented arithmetic using finite data
types (namely, ints or floats) and idealized arithmetic over the reals or
integers. These discrepancies cause the sensitivity of the implemented
statistics (i.e., how much one individual's data can affect the result) to be
much higher than the sensitivity we expect. Consequently, essentially all
differential privacy libraries fail to introduce enough noise to hide
individual-level information as required by differential privacy, and we show
that this may be exploited in realistic attacks on differentially private query
systems. In addition to presenting these vulnerabilities, we also provide a
number of solutions, which modify or constrain the way in which the sum is
implemented in order to recover the idealized or near-idealized bounds on
sensitivity.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Multilingual Disinformation Detection for Digital Advertising. (arXiv:2207.10649v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10649">http://arxiv.org/abs/2207.10649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10649] Multilingual Disinformation Detection for Digital Advertising](http://arxiv.org/abs/2207.10649)</code></li>
<li>Summary: <p>In today's world, the presence of online disinformation and propaganda is
more widespread than ever. Independent publishers are funded mostly via digital
advertising, which is unfortunately also the case for those publishing
disinformation content. The question of how to remove such publishers from
advertising inventory has long been ignored, despite the negative impact on the
open internet. In this work, we make the first step towards quickly detecting
and red-flagging websites that potentially manipulate the public with
disinformation. We build a machine learning model based on multilingual text
embeddings that first determines whether the page mentions a topic of interest,
then estimates the likelihood of the content being malicious, creating a
shortlist of publishers that will be reviewed by human experts. Our system
empowers internal teams to proactively, rather than defensively, blacklist
unsafe content, thus protecting the reputation of the advertisement provider.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: In Defense of Online Models for Video Instance Segmentation. (arXiv:2207.10661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10661">http://arxiv.org/abs/2207.10661</a></li>
<li>Code URL: <a href="https://github.com/wjf5203/vnext">https://github.com/wjf5203/vnext</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10661] In Defense of Online Models for Video Instance Segmentation](http://arxiv.org/abs/2207.10661)</code></li>
<li>Summary: <p>In recent years, video instance segmentation (VIS) has been largely advanced
by offline models, while online models gradually attracted less attention
possibly due to their inferior performance. However, online methods have their
inherent advantage in handling long video sequences and ongoing videos while
offline models fail due to the limit of computational resources. Therefore, it
would be highly desirable if online models can achieve comparable or even
better performance than offline models. By dissecting current online models and
offline models, we demonstrate that the main cause of the performance gap is
the error-prone association between frames caused by the similar appearance
among different instances in the feature space. Observing this, we propose an
online framework based on contrastive learning that is able to learn more
discriminative instance embeddings for association and fully exploit history
information for stability. Despite its simplicity, our method outperforms all
online and offline methods on three benchmarks. Specifically, we achieve 49.5
AP on YouTube-VIS 2019, a significant improvement of 13.2 AP and 2.1 AP over
the prior online and offline art, respectively. Moreover, we achieve 30.2 AP on
OVIS, a more challenging dataset with significant crowding and occlusions,
surpassing the prior art by 14.8 AP. The proposed method won first place in the
video instance segmentation track of the 4th Large-scale Video Object
Segmentation Challenge (CVPR2022). We hope the simplicity and effectiveness of
our method, as well as our insight into current methods, could shed light on
the exploration of VIS models.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Image Generation Network for Covert Transmission in Online Social Network. (arXiv:2207.10292v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10292">http://arxiv.org/abs/2207.10292</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10292] Image Generation Network for Covert Transmission in Online Social Network](http://arxiv.org/abs/2207.10292)</code></li>
<li>Summary: <p>Online social networks have stimulated communications over the Internet more
than ever, making it possible for secret message transmission over such noisy
channels. In this paper, we propose a Coverless Image Steganography Network,
called CIS-Net, that synthesizes a high-quality image directly conditioned on
the secret message to transfer. CIS-Net is composed of four modules, namely,
the Generation, Adversarial, Extraction, and Noise Module. The receiver can
extract the hidden message without any loss even the images have been distorted
by JPEG compression attacks. To disguise the behaviour of steganography, we
collected images in the context of profile photos and stickers and train our
network accordingly. As such, the generated images are more inclined to escape
from malicious detection and attack. The distinctions from previous image
steganography methods are majorly the robustness and losslessness against
diverse attacks. Experiments over diverse public datasets have manifested the
superior ability of anti-steganalysis.
</p></li>
</ul>

<h3>Title: A Ransomware Triage Approach using a Task Memory based on Meta-Transfer Learning Framework. (arXiv:2207.10242v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10242">http://arxiv.org/abs/2207.10242</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10242] A Ransomware Triage Approach using a Task Memory based on Meta-Transfer Learning Framework](http://arxiv.org/abs/2207.10242)</code></li>
<li>Summary: <p>Solutions for rapid prioritization of different ransomware have been raised
to formulate fast response plans to minimize socioeconomic damage from the
massive growth of ransomware attacks in recent years. To address this concern,
we propose a ransomware triage approach that can rapidly classify and
prioritize different ransomware classes. Our Siamese Neural Network (SNN) based
approach utilizes a pre-trained ResNet18 network in a meta-learning fashion to
reduce the biases in weight and parameter calculations typically associated
with a machine learning model trained with a limited number of training
samples. Instead of image features typically used as inputs to many existing
machine learning-based triage applications, our approach uses the entropy
features directly obtained from the ransomware binary files to improve feature
representation, resilient to obfuscation noise, and computationally less
expensive. Our triage approach can classify ransomware samples into the correct
classes if the ransomware features significantly match known ransomware
profiles. Our evaluation shows that this classification part of our proposed
approach achieves the accuracy exceeding 88% and outperforms other similar
classification only machine learning-based approaches. In addition, we offer a
new triage strategy based on the normalized and regularized weight ratios that
evaluate the level of similarity matching across ransomware classes to identify
any risky and unknown ransomware (e.g., zero-day attacks) so that a rapid
further analysis can be conducted
</p></li>
</ul>

<h3>Title: Illusionary Attacks on Sequential Decision Makers and Countermeasures. (arXiv:2207.10170v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10170">http://arxiv.org/abs/2207.10170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10170] Illusionary Attacks on Sequential Decision Makers and Countermeasures](http://arxiv.org/abs/2207.10170)</code></li>
<li>Summary: <p>Autonomous intelligent agents deployed to the real-world need to be robust
against adversarial attacks on sensory inputs. Existing work in reinforcement
learning focuses on minimum-norm perturbation attacks, which were originally
introduced to mimic a notion of perceptual invariance in computer vision. In
this paper, we note that such minimum-norm perturbation attacks can be
trivially detected by victim agents, as these result in observation sequences
that are not consistent with the victim agent's actions. Furthermore, many
real-world agents, such as physical robots, commonly operate under human
supervisors, which are not susceptible to such perturbation attacks. As a
result, we propose to instead focus on illusionary attacks, a novel form of
attack that is consistent with the world model of the victim agent. We provide
a formal definition of this novel attack framework, explore its characteristics
under a variety of conditions, and conclude that agents must seek realism
feedback to be robust to illusionary attacks.
</p></li>
</ul>

<h3>Title: Knowledge-enhanced Black-box Attacks for Recommendations. (arXiv:2207.10307v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10307">http://arxiv.org/abs/2207.10307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10307] Knowledge-enhanced Black-box Attacks for Recommendations](http://arxiv.org/abs/2207.10307)</code></li>
<li>Summary: <p>Recent studies have shown that deep neural networks-based recommender systems
are vulnerable to adversarial attacks, where attackers can inject carefully
crafted fake user profiles (i.e., a set of items that fake users have
interacted with) into a target recommender system to achieve malicious
purposes, such as promote or demote a set of target items. Due to the security
and privacy concerns, it is more practical to perform adversarial attacks under
the black-box setting, where the architecture/parameters and training data of
target systems cannot be easily accessed by attackers. However, generating
high-quality fake user profiles under black-box setting is rather challenging
with limited resources to target systems. To address this challenge, in this
work, we introduce a novel strategy by leveraging items' attribute information
(i.e., items' knowledge graph), which can be publicly accessible and provide
rich auxiliary knowledge to enhance the generation of fake user profiles. More
specifically, we propose a knowledge graph-enhanced black-box attacking
framework (KGAttack) to effectively learn attacking policies through deep
reinforcement learning techniques, in which knowledge graph is seamlessly
integrated into hierarchical policy networks to generate fake user profiles for
performing adversarial black-box attacks. Comprehensive experiments on various
real-world datasets demonstrate the effectiveness of the proposed attacking
framework under the black-box setting.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: A Generalized &amp; Robust Framework For Timestamp Supervision in Temporal Action Segmentation. (arXiv:2207.10137v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10137">http://arxiv.org/abs/2207.10137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10137] A Generalized &amp; Robust Framework For Timestamp Supervision in Temporal Action Segmentation](http://arxiv.org/abs/2207.10137)</code></li>
<li>Summary: <p>In temporal action segmentation, Timestamp supervision requires only a
handful of labelled frames per video sequence. For unlabelled frames, previous
works rely on assigning hard labels, and performance rapidly collapses under
subtle violations of the annotation assumptions. We propose a novel
Expectation-Maximization (EM) based approach that leverages the label
uncertainty of unlabelled frames and is robust enough to accommodate possible
annotation errors. With accurate timestamp annotations, our proposed method
produces SOTA results and even exceeds the fully-supervised setup in several
metrics and datasets. When applied to timestamp annotations with missing action
segments, our method presents stable performance. To further test our
formulation's robustness, we introduce the new challenging annotation setup of
Skip-tag supervision. This setup relaxes constraints and requires annotations
of any fixed number of random frames in a video, making it more flexible than
Timestamp supervision while remaining competitive.
</p></li>
</ul>

<h3>Title: GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning. (arXiv:2207.10158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10158">http://arxiv.org/abs/2207.10158</a></li>
<li>Code URL: <a href="https://github.com/seleucia/goca">https://github.com/seleucia/goca</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10158] GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning](http://arxiv.org/abs/2207.10158)</code></li>
<li>Summary: <p>Clustering is a ubiquitous tool in unsupervised learning. Most of the
existing self-supervised representation learning methods typically cluster
samples based on visually dominant features. While this works well for
image-based self-supervision, it often fails for videos, which require
understanding motion rather than focusing on background. Using optical flow as
complementary information to RGB can alleviate this problem. However, we
observe that a naive combination of the two views does not provide meaningful
gains. In this paper, we propose a principled way to combine two views.
Specifically, we propose a novel clustering strategy where we use the initial
cluster assignment of each view as prior to guide the final cluster assignment
of the other view. This idea will enforce similar cluster structures for both
views, and the formed clusters will be semantically abstract and robust to
noisy inputs coming from each individual view. Additionally, we propose a novel
regularization strategy to address the feature collapse problem, which is
common in cluster-based self-supervised learning methods. Our extensive
evaluation shows the effectiveness of our learned representations on downstream
tasks, e.g., video retrieval and action recognition. Specifically, we
outperform the state of the art by 7% on UCF and 4% on HMDB for video
retrieval, and 5% on UCF and 6% on HMDB for video classification
</p></li>
</ul>

<h3>Title: Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach. (arXiv:2207.10188v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10188">http://arxiv.org/abs/2207.10188</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10188] Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach](http://arxiv.org/abs/2207.10188)</code></li>
<li>Summary: <p>Deep neural network quantization with adaptive bitwidths has gained
increasing attention due to the ease of model deployment on various platforms
with different resource budgets. In this paper, we propose a meta-learning
approach to achieve this goal. Specifically, we propose MEBQAT, a simple yet
effective way of bitwidth-adaptive quantization aware training (QAT) where
meta-learning is effectively combined with QAT by redefining meta-learning
tasks to incorporate bitwidths. After being deployed on a platform, MEBQAT
allows the (meta-)trained model to be quantized to any candidate bitwidth then
helps to conduct inference without much accuracy drop from quantization.
Moreover, with a few-shot learning scenario, MEBQAT can also adapt a model to
any bitwidth as well as any unseen target classes by adding conventional
optimization or metric-based meta-learning. We design variants of MEBQAT to
support both (1) a bitwidth-adaptive quantization scenario and (2) a new
few-shot learning scenario where both quantization bitwidths and target classes
are jointly adapted. We experimentally demonstrate their validity in multiple
QAT schemes. By comparing their performance to (bitwidth-dedicated) QAT,
existing bitwidth adaptive QAT and vanilla meta-learning, we find that merging
bitwidths into meta-learning tasks achieves a higher level of robustness.
</p></li>
</ul>

<h3>Title: On the Robustness of 3D Object Detectors. (arXiv:2207.10205v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10205">http://arxiv.org/abs/2207.10205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10205] On the Robustness of 3D Object Detectors](http://arxiv.org/abs/2207.10205)</code></li>
<li>Summary: <p>In recent years, significant progress has been achieved for 3D object
detection on point clouds thanks to the advances in 3D data collection and deep
learning techniques. Nevertheless, 3D scenes exhibit a lot of variations and
are prone to sensor inaccuracies as well as information loss during
pre-processing. Thus, it is crucial to design techniques that are robust
against these variations. This requires a detailed analysis and understanding
of the effect of such variations. This work aims to analyze and benchmark
popular point-based 3D object detectors against several data corruptions. To
the best of our knowledge, we are the first to investigate the robustness of
point-based 3D object detectors. To this end, we design and evaluate
corruptions that involve data addition, reduction, and alteration. We further
study the robustness of different modules against local and global variations.
Our experimental results reveal several intriguing findings. For instance, we
show that methods that integrate Transformers at a patch or object level lead
to increased robustness, compared to using Transformers at the point level.
</p></li>
</ul>

<h3>Title: Towards Accurate Open-Set Recognition via Background-Class Regularization. (arXiv:2207.10287v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10287">http://arxiv.org/abs/2207.10287</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10287] Towards Accurate Open-Set Recognition via Background-Class Regularization](http://arxiv.org/abs/2207.10287)</code></li>
<li>Summary: <p>In open-set recognition (OSR), classifiers should be able to reject
unknown-class samples while maintaining high closed-set classification
accuracy. To effectively solve the OSR problem, previous studies attempted to
limit latent feature space and reject data located outside the limited space
via offline analyses, e.g., distance-based feature analyses, or complicated
network architectures. To conduct OSR via a simple inference process (without
offline analyses) in standard classifier architectures, we use distance-based
classifiers instead of conventional Softmax classifiers. Afterwards, we design
a background-class regularization strategy, which uses background-class data as
surrogates of unknown-class ones during training phase. Specifically, we
formulate a novel regularization loss suitable for distance-based classifiers,
which reserves sufficiently large class-wise latent feature spaces for known
classes and forces background-class samples to be located far away from the
limited spaces. Through our extensive experiments, we show that the proposed
method provides robust OSR results, while maintaining high closed-set
classification accuracy.
</p></li>
</ul>

<h3>Title: AugRmixAT: A Data Processing and Training Method for Improving Multiple Robustness and Generalization Performance. (arXiv:2207.10290v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10290">http://arxiv.org/abs/2207.10290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10290] AugRmixAT: A Data Processing and Training Method for Improving Multiple Robustness and Generalization Performance](http://arxiv.org/abs/2207.10290)</code></li>
<li>Summary: <p>Deep neural networks are powerful, but they also have shortcomings such as
their sensitivity to adversarial examples, noise, blur, occlusion, etc.
Moreover, ensuring the reliability and robustness of deep neural network models
is crucial for their application in safety-critical areas. Much previous work
has been proposed to improve specific robustness. However, we find that the
specific robustness is often improved at the sacrifice of the additional
robustness or generalization ability of the neural network model. In
particular, adversarial training methods significantly hurt the generalization
performance on unperturbed data when improving adversarial robustness. In this
paper, we propose a new data processing and training method, called AugRmixAT,
which can simultaneously improve the generalization ability and multiple
robustness of neural network models. Finally, we validate the effectiveness of
AugRmixAT on the CIFAR-10/100 and Tiny-ImageNet datasets. The experiments
demonstrate that AugRmixAT can improve the model's generalization performance
while enhancing the white-box robustness, black-box robustness, common
corruption robustness, and partial occlusion robustness.
</p></li>
</ul>

<h3>Title: On an Edge-Preserving Variational Model for Optical Flow Estimation. (arXiv:2207.10302v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10302">http://arxiv.org/abs/2207.10302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10302] On an Edge-Preserving Variational Model for Optical Flow Estimation](http://arxiv.org/abs/2207.10302)</code></li>
<li>Summary: <p>It is well known that classical formulations resembling the Horn and Schunck
model are still largely competitive due to the modern implementation practices.
In most cases, these models outperform many modern flow estimation methods. In
view of this, we propose an effective implementation design for an
edge-preserving $L^1$ regularization approach to optical flow. The mathematical
well-posedness of our proposed model is studied in the space of functions of
bounded variations $BV(\Omega,\mathbb{R}^2)$. The implementation scheme is
designed in multiple steps. The flow field is computed using the robust
Chambolle-Pock primal-dual algorithm. Motivated by the recent studies of Castro
and Donoho we extend the heuristic of iterated median filtering to our flow
estimation. Further, to refine the flow edges we use the weighted median filter
established by Li and Osher as a post-processing step. Our experiments on the
Middlebury dataset show that the proposed method achieves the best average
angular and end-point errors compared to some of the state-of-the-art Horn and
Schunck based variational methods.
</p></li>
</ul>

<h3>Title: Learning from Data with Noisy Labels Using Temporal Self-Ensemble. (arXiv:2207.10354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10354">http://arxiv.org/abs/2207.10354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10354] Learning from Data with Noisy Labels Using Temporal Self-Ensemble](http://arxiv.org/abs/2207.10354)</code></li>
<li>Summary: <p>There are inevitably many mislabeled data in real-world datasets. Because
deep neural networks (DNNs) have an enormous capacity to memorize noisy labels,
a robust training scheme is required to prevent labeling errors from degrading
the generalization performance of DNNs. Current state-of-the-art methods
present a co-training scheme that trains dual networks using samples associated
with small losses. In practice, however, training two networks simultaneously
can burden computing resources. In this study, we propose a simple yet
effective robust training scheme that operates by training only a single
network. During training, the proposed method generates temporal self-ensemble
by sampling intermediate network parameters from the weight trajectory formed
by stochastic gradient descent optimization. The loss sum evaluated with these
self-ensembles is used to identify incorrectly labeled samples. In parallel,
our method generates multi-view predictions by transforming an input data into
various forms and considers their agreement to identify incorrectly labeled
samples. By combining the aforementioned metrics, we present the proposed {\it
self-ensemble-based robust training} (SRT) method, which can filter the samples
with noisy labels to reduce their influence on training. Experiments on
widely-used public datasets demonstrate that the proposed method achieves a
state-of-the-art performance in some categories without training the dual
networks.
</p></li>
</ul>

<h3>Title: DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network. (arXiv:2207.10434v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10434">http://arxiv.org/abs/2207.10434</a></li>
<li>Code URL: <a href="https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal">https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10434] DC-ShadowNet: Single-Image Hard and Soft Shadow Removal Using Unsupervised Domain-Classifier Guided Network](http://arxiv.org/abs/2207.10434)</code></li>
<li>Summary: <p>Shadow removal from a single image is generally still an open problem. Most
existing learning-based methods use supervised learning and require a large
number of paired images (shadow and corresponding non-shadow images) for
training. A recent unsupervised method, Mask-ShadowGAN, addresses this
limitation. However, it requires a binary mask to represent shadow regions,
making it inapplicable to soft shadows. To address the problem, in this paper,
we propose an unsupervised domain-classifier guided shadow removal network,
DC-ShadowNet. Specifically, we propose to integrate a shadow/shadow-free domain
classifier into a generator and its discriminator, enabling them to focus on
shadow regions. To train our network, we introduce novel losses based on
physics-based shadow-free chromaticity, shadow-robust perceptual features, and
boundary smoothness. Moreover, we show that our unsupervised network can be
used for test-time training that further improves the results. Our experiments
show that all these novel components allow our method to handle soft shadows,
and also to perform better on hard shadows both quantitatively and
qualitatively than the existing state-of-the-art shadow removal methods.
</p></li>
</ul>

<h3>Title: Towards Efficient Adversarial Training on Vision Transformers. (arXiv:2207.10498v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10498">http://arxiv.org/abs/2207.10498</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10498] Towards Efficient Adversarial Training on Vision Transformers](http://arxiv.org/abs/2207.10498)</code></li>
<li>Summary: <p>Vision Transformer (ViT), as a powerful alternative to Convolutional Neural
Network (CNN), has received much attention. Recent work showed that ViTs are
also vulnerable to adversarial examples like CNNs. To build robust ViTs, an
intuitive way is to apply adversarial training since it has been shown as one
of the most effective ways to accomplish robust CNNs. However, one major
limitation of adversarial training is its heavy computational cost. The
self-attention mechanism adopted by ViTs is a computationally intense operation
whose expense increases quadratically with the number of input patches, making
adversarial training on ViTs even more time-consuming. In this work, we first
comprehensively study fast adversarial training on a variety of vision
transformers and illustrate the relationship between the efficiency and
robustness. Then, to expediate adversarial training on ViTs, we propose an
efficient Attention Guided Adversarial Training mechanism. Specifically,
relying on the specialty of self-attention, we actively remove certain patch
embeddings of each layer with an attention-guided dropping strategy during
adversarial training. The slimmed self-attention modules accelerate the
adversarial training on ViTs significantly. With only 65\% of the fast
adversarial training time, we match the state-of-the-art results on the
challenging ImageNet benchmark.
</p></li>
</ul>

<h3>Title: MetaComp: Learning to Adapt for Online Depth Completion. (arXiv:2207.10623v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10623">http://arxiv.org/abs/2207.10623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10623] MetaComp: Learning to Adapt for Online Depth Completion](http://arxiv.org/abs/2207.10623)</code></li>
<li>Summary: <p>Relying on deep supervised or self-supervised learning, previous methods for
depth completion from paired single image and sparse depth data have achieved
impressive performance in recent years. However, facing a new environment where
the test data occurs online and differs from the training data in the RGB image
content and depth sparsity, the trained model might suffer severe performance
drop. To encourage the trained model to work well in such conditions, we expect
it to be capable of adapting to the new environment continuously and
effectively. To achieve this, we propose MetaComp. It utilizes the
meta-learning technique to simulate adaptation policies during the training
phase, and then adapts the model to new environments in a self-supervised
manner in testing. Considering that the input is multi-modal data, it would be
challenging to adapt a model to variations in two modalities simultaneously,
due to significant differences in structure and form of the two modal data.
Therefore, we further propose to disentangle the adaptation procedure in the
basic meta-learning training into two steps, the first one focusing on the
depth sparsity while the second attending to the image content. During testing,
we take the same strategy to adapt the model online to new multi-modal data.
Experimental results and comprehensive ablations show that our MetaComp is
capable of adapting to the depth completion in a new environment effectively
and robust to changes in different modalities.
</p></li>
</ul>

<h3>Title: Online Domain Adaptation for Semantic Segmentation in Ever-Changing Conditions. (arXiv:2207.10667v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10667">http://arxiv.org/abs/2207.10667</a></li>
<li>Code URL: <a href="https://github.com/theo2021/onda">https://github.com/theo2021/onda</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10667] Online Domain Adaptation for Semantic Segmentation in Ever-Changing Conditions](http://arxiv.org/abs/2207.10667)</code></li>
<li>Summary: <p>Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between
training and testing data and is, in most cases, carried out in offline manner.
However, domain changes may occur continuously and unpredictably during
deployment (e.g. sudden weather changes). In such conditions, deep neural
networks witness dramatic drops in accuracy and offline adaptation may not be
enough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA)
for semantic segmentation. We design a pipeline that is robust to continuous
domain shifts, either gradual or sudden, and we evaluate it in the case of
rainy and foggy scenarios. Our experiments show that our framework can
effectively adapt to new domains during deployment, while not being affected by
catastrophic forgetting of the previous domains.
</p></li>
</ul>

<h3>Title: Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for Adversarial Robustness. (arXiv:2207.10283v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10283">http://arxiv.org/abs/2207.10283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10283] Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for Adversarial Robustness](http://arxiv.org/abs/2207.10283)</code></li>
<li>Summary: <p>Defending deep neural networks against adversarial examples is a key
challenge for AI safety. To improve the robustness effectively, recent methods
focus on important data points near the decision boundary in adversarial
training. However, these methods are vulnerable to Auto-Attack, which is an
ensemble of parameter-free attacks for reliable evaluation. In this paper, we
experimentally investigate the causes of their vulnerability and find that
existing methods reduce margins between logits for the true label and the other
labels while keeping their gradient norms non-small values. Reduced margins and
non-small gradient norms cause their vulnerability since the largest logit can
be easily flipped by the perturbation. Our experiments also show that the
histogram of the logit margins has two peaks, i.e., small and large logit
margins. From the observations, we propose switching one-versus-the-rest loss
(SOVR), which uses one-versus-the-rest loss when data have small logit margins
so that it increases the margins. We find that SOVR increases logit margins
more than existing methods while keeping gradient norms small and outperforms
them in terms of the robustness against Auto-Attack.
</p></li>
</ul>

<h3>Title: Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning. (arXiv:2207.10295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10295">http://arxiv.org/abs/2207.10295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10295] Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning](http://arxiv.org/abs/2207.10295)</code></li>
<li>Summary: <p>Impressive results in natural language processing (NLP) based on the
Transformer neural network architecture have inspired researchers to explore
viewing offline reinforcement learning (RL) as a generic sequence modeling
problem. Recent works based on this paradigm have achieved state-of-the-art
results in several of the mostly deterministic offline Atari and D4RL
benchmarks. However, because these methods jointly model the states and actions
as a single sequencing problem, they struggle to disentangle the effects of the
policy and world dynamics on the return. Thus, in adversarial or stochastic
environments, these methods lead to overly optimistic behavior that can be
dangerous in safety-critical systems like autonomous driving. In this work, we
propose a method that addresses this optimism bias by explicitly disentangling
the policy and world models, which allows us at test time to search for
policies that are robust to multiple possible futures in the environment. We
demonstrate our method's superior performance on a variety of autonomous
driving tasks in simulation.
</p></li>
</ul>

<h3>Title: Towards Better Evaluation for Dynamic Link Prediction. (arXiv:2207.10128v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10128">http://arxiv.org/abs/2207.10128</a></li>
<li>Code URL: <a href="https://github.com/fpour/dgb">https://github.com/fpour/dgb</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10128] Towards Better Evaluation for Dynamic Link Prediction](http://arxiv.org/abs/2207.10128)</code></li>
<li>Summary: <p>There has been recent success in learning from static graphs, but despite
their prevalence, learning from time-evolving graphs remains challenging. We
design new, more stringent evaluation procedures for link prediction specific
to dynamic graphs, which reflect real-world considerations and can better
compare different methods' strengths and weaknesses. In particular, we create
two visualization techniques to understand the recurring patterns of edges over
time. They show that many edges reoccur at later time steps. Therefore, we
propose a pure memorization baseline called EdgeBank. It achieves surprisingly
strong performance across multiple settings, partly due to the easy negative
edges used in the current evaluation setting. Hence, we introduce two more
challenging negative sampling strategies that improve robustness and can better
match real-world applications. Lastly, we introduce five new dynamic graph
datasets from a diverse set of domains missing from current benchmarks,
providing new challenges and opportunities for future research.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Careful What You Wish For: on the Extraction of Adversarially Trained Models. (arXiv:2207.10561v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10561">http://arxiv.org/abs/2207.10561</a></li>
<li>Code URL: <a href="https://github.com/kacemkhaled/model-stealing">https://github.com/kacemkhaled/model-stealing</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10561] Careful What You Wish For: on the Extraction of Adversarially Trained Models](http://arxiv.org/abs/2207.10561)</code></li>
<li>Summary: <p>Recent attacks on Machine Learning (ML) models such as evasion attacks with
adversarial examples and models stealing through extraction attacks pose
several security and privacy threats. Prior work proposes to use adversarial
training to secure models from adversarial examples that can evade the
classification of a model and deteriorate its performance. However, this
protection technique affects the model's decision boundary and its prediction
probabilities, hence it might raise model privacy risks. In fact, a malicious
user using only a query access to the prediction output of a model can extract
it and obtain a high-accuracy and high-fidelity surrogate model. To have a
greater extraction, these attacks leverage the prediction probabilities of the
victim model. Indeed, all previous work on extraction attacks do not take into
consideration the changes in the training process for security purposes. In
this paper, we propose a framework to assess extraction attacks on
adversarially trained models with vision datasets. To the best of our
knowledge, our work is the first to perform such evaluation. Through an
extensive empirical study, we demonstrate that adversarially trained models are
more vulnerable to extraction attacks than models obtained under natural
training circumstances. They can achieve up to $\times1.2$ higher accuracy and
agreement with a fraction lower than $\times0.75$ of the queries. We
additionally find that the adversarial robustness capability is transferable
through extraction attacks, i.e., extracted Deep Neural Networks (DNNs) from
robust models show an enhanced accuracy to adversarial examples compared to
extracted DNNs from naturally trained (i.e. standard) models.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Slimmable Quantum Federated Learning. (arXiv:2207.10221v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10221">http://arxiv.org/abs/2207.10221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10221] Slimmable Quantum Federated Learning](http://arxiv.org/abs/2207.10221)</code></li>
<li>Summary: <p>Quantum federated learning (QFL) has recently received increasing attention,
where quantum neural networks (QNNs) are integrated into federated learning
(FL). In contrast to the existing static QFL methods, we propose slimmable QFL
(SlimQFL) in this article, which is a dynamic QFL framework that can cope with
time-varying communication channels and computing energy limitations. This is
made viable by leveraging the unique nature of a QNN where its angle parameters
and pole parameters can be separately trained and dynamically exploited.
Simulation results corroborate that SlimQFL achieves higher classification
accuracy than Vanilla QFL, particularly under poor channel conditions on
average.
</p></li>
</ul>

<h3>Title: FOCUS: Fairness via Agent-Awareness for Federated Learning on Heterogeneous Data. (arXiv:2207.10265v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10265">http://arxiv.org/abs/2207.10265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10265] FOCUS: Fairness via Agent-Awareness for Federated Learning on Heterogeneous Data](http://arxiv.org/abs/2207.10265)</code></li>
<li>Summary: <p>Federated learning (FL) provides an effective paradigm to train machine
learning models over distributed data with privacy protection. However, recent
studies show that FL is subject to various security, privacy, and fairness
threats due to the potentially malicious and heterogeneous local agents. For
instance, it is vulnerable to local adversarial agents who only contribute
low-quality data, with the goal of harming the performance of those with
high-quality data. This kind of attack hence breaks existing definitions of
fairness in FL that mainly focus on a certain notion of performance parity. In
this work, we aim to address this limitation and propose a formal definition of
fairness via agent-awareness for FL (FAA), which takes the heterogeneous data
contributions of local agents into account. In addition, we propose a fair FL
training algorithm based on agent clustering (FOCUS) to achieve FAA.
Theoretically, we prove the convergence and optimality of FOCUS under mild
conditions for linear models and general convex loss functions with bounded
smoothness. We also prove that FOCUS always achieves higher fairness measured
by FAA compared with standard FedAvg protocol under both linear models and
general convex loss functions. Empirically, we evaluate FOCUS on four datasets,
including synthetic data, images, and texts under different settings, and we
show that FOCUS achieves significantly higher fairness based on FAA while
maintaining similar or even higher prediction accuracy compared with FedAvg.
</p></li>
</ul>

<h3>Title: UniFed: A Benchmark for Federated Learning Frameworks. (arXiv:2207.10308v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10308">http://arxiv.org/abs/2207.10308</a></li>
<li>Code URL: <a href="https://github.com/ai-secure/flbenchmark-toolkit">https://github.com/ai-secure/flbenchmark-toolkit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10308] UniFed: A Benchmark for Federated Learning Frameworks](http://arxiv.org/abs/2207.10308)</code></li>
<li>Summary: <p>Federated Learning (FL) has become a practical and popular paradigm in
machine learning. However, currently, there is no systematic solution that
covers diverse use cases. Practitioners often face the challenge of how to
select a matching FL framework for their use case. In this work, we present
UniFed, the first unified benchmark for standardized evaluation of the existing
open-source FL frameworks. With 15 evaluation scenarios, we present both
qualitative and quantitative evaluation results of nine existing popular
open-sourced FL frameworks, from the perspectives of functionality, usability,
and system performance. We also provide suggestions on framework selection
based on the benchmark conclusions and point out future improvement directions.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection. (arXiv:2207.10246v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10246">http://arxiv.org/abs/2207.10246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10246] GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection](http://arxiv.org/abs/2207.10246)</code></li>
<li>Summary: <p>Facial forgery by deepfakes has raised severe societal concerns. Several
solutions have been proposed by the vision community to effectively combat the
misinformation on the internet via automated deepfake detection systems. Recent
studies have demonstrated that facial analysis-based deep learning models can
discriminate based on protected attributes. For the commercial adoption and
massive roll-out of the deepfake detection technology, it is vital to evaluate
and understand the fairness (the absence of any prejudice or favoritism) of
deepfake detectors across demographic variations such as gender and race. As
the performance differential of deepfake detectors between demographic
subgroups would impact millions of people of the deprived sub-group. This paper
aims to evaluate the fairness of the deepfake detectors across males and
females. However, existing deepfake datasets are not annotated with demographic
labels to facilitate fairness analysis. To this aim, we manually annotated
existing popular deepfake datasets with gender labels and evaluated the
performance differential of current deepfake detectors across gender. Our
analysis on the gender-labeled version of the datasets suggests (a) current
deepfake datasets have skewed distribution across gender, and (b) commonly
adopted deepfake detectors obtain unequal performance across gender with mostly
males outperforming females. Finally, we contributed a gender-balanced and
annotated deepfake dataset, GBDF, to mitigate the performance differential and
to promote research and development towards fairness-aware deep fake detectors.
The GBDF dataset is publicly available at: https://github.com/aakash4305/GBDF
</p></li>
</ul>

<h3>Title: A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing. (arXiv:2207.10614v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10614">http://arxiv.org/abs/2207.10614</a></li>
<li>Code URL: <a href="https://github.com/apple/ml-dms-dataset">https://github.com/apple/ml-dms-dataset</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10614] A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing](http://arxiv.org/abs/2207.10614)</code></li>
<li>Summary: <p>A key algorithm for understanding the world is material segmentation, which
assigns a label (metal, glass, etc.) to each pixel. We find that a model
trained on existing data underperforms in some settings and propose to address
this with a large-scale dataset of 3.2 million dense segments on 44,560 indoor
and outdoor images, which is 23x more segments than existing data. Our data
covers a more diverse set of scenes, objects, viewpoints and materials, and
contains a more fair distribution of skin types. We show that a model trained
on our data outperforms a state-of-the-art model across datasets and
viewpoints. We propose a large-scale scene parsing benchmark and baseline of
0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across
46 materials.
</p></li>
</ul>

<h3>Title: RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping. (arXiv:2207.10653v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10653">http://arxiv.org/abs/2207.10653</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10653] RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping](http://arxiv.org/abs/2207.10653)</code></li>
<li>Summary: <p>Fairness has become an essential problem in many domains of Machine Learning
(ML), such as classification, natural language processing, and Generative
Adversarial Networks (GANs). In this research effort, we study the unfairness
of GANs. We formally define a new fairness notion for generative models in
terms of the distribution of generated samples sharing the same protected
attributes (gender, race, etc.). The defined fairness notion (representational
fairness) requires the distribution of the sensitive attributes at the test
time to be uniform, and, in particular for GAN model, we show that this
fairness notion is violated even when the dataset contains equally represented
groups, i.e., the generator favors generating one group of samples over the
others at the test time. In this work, we shed light on the source of this
representation bias in GANs along with a straightforward method to overcome
this problem. We first show on two widely used datasets (MNIST, SVHN) that when
the norm of the gradient of one group is more important than the other during
the discriminator's training, the generator favours sampling data from one
group more than the other at test time. We then show that controlling the
groups' gradient norm by performing group-wise gradient norm clipping in the
discriminator during the training leads to a more fair data generation in terms
of representational fairness compared to existing models while preserving the
quality of generated samples.
</p></li>
</ul>

<h3>Title: Action2Score: An Embedding Approach To Score Player Action. (arXiv:2207.10297v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10297">http://arxiv.org/abs/2207.10297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10297] Action2Score: An Embedding Approach To Score Player Action](http://arxiv.org/abs/2207.10297)</code></li>
<li>Summary: <p>Multiplayer Online Battle Arena (MOBA) is one of the most successful game
genres. MOBA games such as League of Legends have competitive environments
where players race for their rank. In most MOBA games, a player's rank is
determined by the match result (win or lose). It seems natural because of the
nature of team play, but in some sense, it is unfair because the players who
put a lot of effort lose their rank just in case of loss and some players even
get free-ride on teammates' efforts in case of a win. To reduce the
side-effects of the team-based ranking system and evaluate a player's
performance impartially, we propose a novel embedding model that converts a
player's actions into quantitative scores based on the actions' respective
contribution to the team's victory. Our model is built using a sequence-based
deep learning model with a novel loss function working on the team match. The
sequence-based deep learning model process the action sequence from the game
start to the end of a player in a team play using a GRU unit that takes a
hidden state from the previous step and the current input selectively. The loss
function is designed to help the action score to reflect the final score and
the success of the team. We showed that our model can evaluate a player's
individual performance fairly and analyze the contributions of the player's
respective actions.
</p></li>
</ul>

<h3>Title: Detecting and Preventing Shortcut Learning for Fair Medical AI using Shortcut Testing (ShorT). (arXiv:2207.10384v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10384">http://arxiv.org/abs/2207.10384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10384] Detecting and Preventing Shortcut Learning for Fair Medical AI using Shortcut Testing (ShorT)](http://arxiv.org/abs/2207.10384)</code></li>
<li>Summary: <p>Machine learning (ML) holds great promise for improving healthcare, but it is
critical to ensure that its use will not propagate or amplify health
disparities. An important step is to characterize the (un)fairness of ML models</li>
<li>their tendency to perform differently across subgroups of the population -
and to understand its underlying mechanisms. One potential driver of
algorithmic unfairness, shortcut learning, arises when ML models base
predictions on improper correlations in the training data. However, diagnosing
this phenomenon is difficult, especially when sensitive attributes are causally
linked with disease. Using multi-task learning, we propose the first method to
assess and mitigate shortcut learning as a part of the fairness assessment of
clinical ML systems, and demonstrate its application to clinical tasks in
radiology and dermatology. Finally, our approach reveals instances when
shortcutting is not responsible for unfairness, highlighting the need for a
holistic approach to fairness mitigation in medical AI.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.10257">http://arxiv.org/abs/2207.10257</a></li>
<li>Code URL: <a href="https://github.com/jgkwak95/surf-gan">https://github.com/jgkwak95/surf-gan</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.10257] Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis](http://arxiv.org/abs/2207.10257)</code></li>
<li>Summary: <p>Over the years, 2D GANs have achieved great successes in photorealistic
portrait generation. However, they lack 3D understanding in the generation
process, thus they suffer from multi-view inconsistency problem. To alleviate
the issue, many 3D-aware GANs have been proposed and shown notable results, but
3D GANs struggle with editing semantic attributes. The controllability and
interpretability of 3D GANs have not been much explored. In this work, we
propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware
GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of
discovering semantic attributes during training and controlling them in an
unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN
to obtain a high-fidelity 3D-controllable generator. Unlike existing
latent-based methods allowing implicit pose control, the proposed
3D-controllable StyleGAN enables explicit pose control over portrait
generation. This distillation allows direct compatibility between 3D control
and many StyleGAN-based techniques (e.g., inversion and stylization), and also
brings an advantage in terms of computational resources. Our codes are
available at https://github.com/jgkwak95/SURF-GAN.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
