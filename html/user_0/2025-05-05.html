<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-05</h1>
<h3>Title: FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bithiah Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00725">https://arxiv.org/abs/2505.00725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00725">https://arxiv.org/pdf/2505.00725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00725]] FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models(https://arxiv.org/abs/2505.00725)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Motivated by the emerging demand in the financial industry for the automatic analysis of unstructured and structured data at scale, Question Answering (QA) systems can provide lucrative and competitive advantages to companies by facilitating the decision making of financial advisers. Consequently, we propose a novel financial QA system using the transformer-based pre-trained BERT language model to address the limitations of data scarcity and language specificity in the financial domain. Our system focuses on financial non-factoid answer selection, which retrieves a set of passage-level texts and selects the most relevant as the answer. To increase efficiency, we formulate the answer selection task as a re-ranking problem, in which our system consists of an Answer Retriever using BM25, a simple information retrieval approach, to first return a list of candidate answers, and an Answer Re-ranker built with variants of pre-trained BERT language models to re-rank and select the most relevant answers. We investigate various learning, further pre-training, and fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a model built from applying the Transfer and Adapt further fine-tuning and pointwise learning approach, is the most effective, improving the state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on NDCG, and 21% on Precision@1.</li>
</ul>

<h3>Title: Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes</h3>
<ul>
<li><strong>Authors: </strong>Neil Joshi, Joshua Carney, Nathanael Kuo, Homer Li, Cheng Peng, Myron Brown</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00734">https://arxiv.org/abs/2505.00734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00734">https://arxiv.org/pdf/2505.00734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00734]] Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes(https://arxiv.org/abs/2505.00734)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Production of photorealistic, navigable 3D site models requires a large volume of carefully collected images that are often unavailable to first responders for disaster relief or law enforcement. Real-world challenges include limited numbers of images, heterogeneous unposed cameras, inconsistent lighting, and extreme viewpoint differences for images collected from varying altitudes. To promote research aimed at addressing these challenges, we have developed the first public benchmark dataset for 3D reconstruction and novel view synthesis based on multiple calibrated ground-level, security-level, and airborne cameras. We present datasets that pose real-world challenges, independently evaluate calibration of unposed cameras and quality of novel rendered views, demonstrate baseline performance using recent state-of-practice methods, and identify challenges for further research.</li>
</ul>

<h3>Title: MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection</h3>
<ul>
<li><strong>Authors: </strong>Qiushi Yang, Yuan Yao, Miaomiao Cui, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00739">https://arxiv.org/abs/2505.00739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00739">https://arxiv.org/pdf/2505.00739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00739]] MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection(https://arxiv.org/abs/2505.00739)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional capabilities in interactive object segmentation for both images and videos. However, as a foundational model on interactive segmentation, SAM2 performs segmentation directly based on mask memory from the past six frames, leading to two significant challenges. Firstly, during inference in videos, objects may disappear since SAM2 relies solely on memory without accounting for object motion information, which limits its long-range object tracking capabilities. Secondly, its memory is constructed from fixed past frames, making it susceptible to challenges associated with object disappearance or occlusion, due to potentially inaccurate segmentation results in memory. To address these problems, we present MoSAM, incorporating two key strategies to integrate object motion cues into the model and establish more reliable feature memory. Firstly, we propose Motion-Guided Prompting (MGP), which represents the object motion in both sparse and dense manners, then injects them into SAM2 through a set of motion-guided prompts. MGP enables the model to adjust its focus towards the direction of motion, thereby enhancing the object tracking capabilities. Furthermore, acknowledging that past segmentation results may be inaccurate, we devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically identifies frames likely to contain accurate segmentation in both pixel- and frame-level. By eliminating potentially inaccurate mask predictions from memory, we can leverage more reliable memory features to exploit similar regions for improving segmentation results. Extensive experiments on various benchmarks of video object segmentation and video instance segmentation demonstrate that our MoSAM achieves state-of-the-art results compared to other competitors.</li>
</ul>

<h3>Title: Zoomer: Adaptive Image Focus Optimization for Black-box MLLM</h3>
<ul>
<li><strong>Authors: </strong>Jiaxu Qian, Chendong Wang, Yifan Yang, Chaoyun Zhang, Huiqiang Jiang, Xufang Luo, Yu Kang, Qingwei Lin, Anlan Zhang, Shiqi Jiang, Ting Cao, Tianjun Mao, Suman Banerjee, Guyue Liu, Saravan Rajmohan, Dongmei Zhang, Yuqing Yang, Qi Zhang, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00742">https://arxiv.org/abs/2505.00742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00742">https://arxiv.org/pdf/2505.00742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00742]] Zoomer: Adaptive Image Focus Optimization for Black-box MLLM(https://arxiv.org/abs/2505.00742)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal large language models (MLLMs) have broadened the scope of vision-language tasks, excelling in applications like image captioning and interactive question-answering. However, these models struggle with accurately processing visual data, particularly in tasks requiring precise object recognition and fine visual details. Stringent token limits often result in the omission of critical information, hampering performance. To address these limitations, we introduce \SysName, a novel visual prompting mechanism designed to enhance MLLM performance while preserving essential visual details within token limits. \SysName features three key innovations: a prompt-aware strategy that dynamically highlights relevant image regions, a spatial-preserving orchestration schema that maintains object integrity, and a budget-aware prompting method that balances global context with crucial visual details. Comprehensive evaluations across multiple datasets demonstrate that \SysName consistently outperforms baseline methods, achieving up to a $26.9\%$ improvement in accuracy while significantly reducing token consumption.</li>
</ul>

<h3>Title: DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Yinfeng Yu, Dongsheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00743">https://arxiv.org/abs/2505.00743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00743">https://arxiv.org/pdf/2505.00743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00743]] DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation(https://arxiv.org/abs/2505.00743)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Vision-and-Language Navigation (VLN) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. The agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. Despite significant advancements in this field, two major limitations persist: (1) Many existing methods input complete language instructions directly into multi-layer Transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) Current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. We propose a Dual Object Perception-Enhancement Network (DOPE) to address these issues to improve navigation performance. First, we design a Text Semantic Extraction (TSE) to extract relatively essential phrases from the text and input them into the Text Object Perception-Augmentation (TOPA) to fully leverage details such as objects and actions within the instructions. Second, we introduce an Image Object Perception-Augmentation (IOPA), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. Extensive experiments on the R2R and REVERIE datasets validate the efficacy of the proposed approach.</li>
</ul>

<h3>Title: Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Dung Nguyen, Minh Khoi Ho, Huy Ta, Thanh Tam Nguyen, Qi Chen, Kumar Rav, Quy Duong Dang, Satwik Ramchandre, Son Lam Phung, Zhibin Liao, Minh-Son To, Johan Verjans, Phi Le Nguyen, Vu Minh Hieu Phan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00744">https://arxiv.org/abs/2505.00744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00744">https://arxiv.org/pdf/2505.00744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00744]] Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering(https://arxiv.org/abs/2505.00744)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to inadequate localization reasoning. This work reveals a critical limitation in current medical LMMs: instead of analyzing relevant pathological regions, they often rely on linguistic patterns or attend to irrelevant image areas when responding to disease-related queries. To address this, we introduce HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive benchmark designed to evaluate LMMs' localization abilities and hallucination robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA pairs, with doctor-annotated anatomical segmentation masks for pathological regions. To improve visual reasoning, we propose the Localize-before-Answer (LobA) framework, which trains LMMs to localize target regions of interest and self-prompt to emphasize segmented pathological areas, generating grounded and reliable answers. Experimental results demonstrate that our approach significantly outperforms state-of-the-art biomedical LMMs on the challenging HEAL-MedVQA benchmark, advancing robustness in medical VQA.</li>
</ul>

<h3>Title: InstructAttribute: Fine-grained Object Attributes editing with Instruction</h3>
<ul>
<li><strong>Authors: </strong>Xingxi Yin, Jingfeng Zhang, Zhi Li, Yicheng Li, Yin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00751">https://arxiv.org/abs/2505.00751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00751">https://arxiv.org/pdf/2505.00751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00751]] InstructAttribute: Fine-grained Object Attributes editing with Instruction(https://arxiv.org/abs/2505.00751)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models, renowned for their advanced generative abilities, are extensively utilized in image editing applications, demonstrating remarkable effectiveness. However, achieving precise control over fine-grained attributes still presents considerable challenges. Existing image editing techniques either fail to modify the attributes of an object or struggle to preserve its structure and maintain consistency in other areas of the image. To address these challenges, we propose the Structure-Preserving and Attribute Amplification (SPAA), a training-free method which enables precise control over the color and material transformations of objects by editing the self-attention maps and cross-attention values. Furthermore, we constructed the Attribute Dataset, which encompasses nearly all colors and materials associated with various objects, by integrating multimodal large language models (MLLM) to develop an automated pipeline for data filtering and instruction labeling. Training on this dataset, we present our InstructAttribute, an instruction-based model designed to facilitate fine-grained editing of color and material attributes. Extensive experiments demonstrate that our method achieves superior performance in object-level color and material editing, outperforming existing instruction-based image editing approaches.</li>
</ul>

<h3>Title: DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking</h3>
<ul>
<li><strong>Authors: </strong>Xuzhao Li, Xuchen Li, Shiyu Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00752">https://arxiv.org/abs/2505.00752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00752">https://arxiv.org/pdf/2505.00752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00752]] DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking(https://arxiv.org/abs/2505.00752)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Nighttime UAV tracking presents significant challenges due to extreme illumination variations and viewpoint changes, which severely degrade tracking performance. Existing approaches either rely on light enhancers with high computational costs or introduce redundant domain adaptation mechanisms, failing to fully utilize the dynamic features in varying perspectives. To address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic \textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end tracking framework designed for nighttime UAV scenarios. DARTer leverages a Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime features from static and dynamic templates, enhancing representation robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates Vision Transformer layers based on extracted features, significantly improving efficiency by reducing redundant computations. Our model eliminates the need for complex multi-task loss functions, enabling a streamlined training process. Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate the superiority of DARTer over state-of-the-art trackers. These results confirm that DARTer effectively balances tracking accuracy and efficiency, making it a promising solution for real-world nighttime UAV tracking applications.</li>
</ul>

<h3>Title: A Survey on Large Language Model based Human-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Yuwei Cao, Dongyuan Li, Renhe Jiang, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00753">https://arxiv.org/abs/2505.00753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00753">https://arxiv.org/pdf/2505.00753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00753]] A Survey on Large Language Model based Human-Agent Systems(https://arxiv.org/abs/2505.00753)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at this https URL.</li>
</ul>

<h3>Title: P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors</h3>
<ul>
<li><strong>Authors: </strong>Atsuya Watanabe, Ratna Aisuwarya, Lei Jing</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00755">https://arxiv.org/abs/2505.00755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00755">https://arxiv.org/pdf/2505.00755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00755]] P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors(https://arxiv.org/abs/2505.00755)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>This work presents P2P-Insole, a low-cost approach for estimating and visualizing 3D human skeletal data using insole-type sensors integrated with IMUs. Each insole, fabricated with e-textile garment techniques, costs under USD 1, making it significantly cheaper than commercial alternatives and ideal for large-scale production. Our approach uses foot pressure distribution, acceleration, and rotation data to overcome limitations, providing a lightweight, minimally intrusive, and privacy-aware solution. The system employs a Transformer model for efficient temporal feature extraction, enriched by first and second derivatives in the input stream. Including multimodal information, such as accelerometers and rotational measurements, improves the accuracy of complex motion pattern recognition. These facts are demonstrated experimentally, while error metrics show the robustness of the approach in various posture estimation tasks. This work could be the foundation for a low-cost, practical application in rehabilitation, injury prevention, and health monitoring while enabling further development through sensor optimization and expanded datasets.</li>
</ul>

<h3>Title: Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L</h3>
<ul>
<li><strong>Authors: </strong>Woong-Chan Byun, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00757">https://arxiv.org/abs/2505.00757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00757">https://arxiv.org/pdf/2505.00757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00757]] Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L(https://arxiv.org/abs/2505.00757)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>4D radar has attracted attention in autonomous driving due to its ability to enable robust 3D object detection even under adverse weather conditions. To practically deploy such technologies, it is essential to achieve real-time processing within low-power embedded environments. Addressing this, we present the first on-chip implementation of a 4D radar-based 3D object detection model on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D tensors, posing a significant challenge. To overcome this limitation, we introduce a tensor transformation method that reshapes 5D inputs into 4D formats during the compilation process, enabling direct deployment without altering the model structure. The proposed system achieves 46.47% AP_3D and 52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while achieving an inference speed of 13.76 Hz. These results demonstrate the applicability of 4D radar-based perception technologies to autonomous driving systems.</li>
</ul>

<h3>Title: Multi-Modal Language Models as Text-to-Image Model Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Chen, Candace Ross, Reyhane Askari-Hemmat, Koustuv Sinha, Melissa Hall, Michal Drozdzal, Adriana Romero-Soriano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00759">https://arxiv.org/abs/2505.00759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00759">https://arxiv.org/pdf/2505.00759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00759]] Multi-Modal Language Models as Text-to-Image Model Evaluators(https://arxiv.org/abs/2505.00759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.</li>
</ul>

<h3>Title: Reasoning Capabilities and Invariability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Raganato, Rafael Peñaloza, Marco Viviani, Gabriella Pasi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00776">https://arxiv.org/abs/2505.00776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00776">https://arxiv.org/pdf/2505.00776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00776]] Reasoning Capabilities and Invariability of Large Language Models(https://arxiv.org/abs/2505.00776)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency. In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning. Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction. An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement. An additional test with chain-of-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.</li>
</ul>

<h3>Title: AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring</h3>
<ul>
<li><strong>Authors: </strong>Oluwanisola Ibikunle, Hara Talasila, Debvrat Varshney, Jilu Li, John Paden, Maryam Rahnemoonfar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00786">https://arxiv.org/abs/2505.00786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00786">https://arxiv.org/pdf/2505.00786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00786]] AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring(https://arxiv.org/abs/2505.00786)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tracking internal layers in radar echograms with high accuracy is essential for understanding ice sheet dynamics and quantifying the impact of accelerated ice discharge in Greenland and other polar regions due to contemporary global climate warming. Deep learning algorithms have become the leading approach for automating this task, but the absence of a standardized and well-annotated echogram dataset has hindered the ability to test and compare algorithms reliably, limiting the advancement of state-of-the-art methods for the radar echogram layer tracking problem. This study introduces the first comprehensive ``deep learning ready'' radar echogram dataset derived from Snow Radar airborne data collected during the National Aeronautics and Space Administration Operation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation, wet) with varying along-track resolutions. To demonstrate its utility, we evaluated the performance of five deep learning models on the dataset. Our results show that while current computer vision segmentation algorithms can identify and track snow layer pixels in echogram images, advanced end-to-end models are needed to directly extract snow depth and annual accumulation from echograms, reducing or eliminating post-processing. The dataset and accompanying benchmarking framework provide a valuable resource for advancing radar echogram layer tracking and snow accumulation estimation, advancing our understanding of polar ice sheets response to climate warming.</li>
</ul>

<h3>Title: Improving Routing in Sparse Mixture of Experts with Graph of Tokens</h3>
<ul>
<li><strong>Authors: </strong>Tam Nguyen, Ngoc N. Tran, Khai Nguyen, Richard G. Baraniuk</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00792">https://arxiv.org/abs/2505.00792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00792">https://arxiv.org/pdf/2505.00792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00792]] Improving Routing in Sparse Mixture of Experts with Graph of Tokens(https://arxiv.org/abs/2505.00792)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Sparse Mixture of Experts (SMoE) has emerged as a key to achieving unprecedented scalability in deep learning. By activating only a small subset of parameters per sample, SMoE achieves an exponential increase in parameter counts while maintaining a constant computational overhead. However, SMoE models are susceptible to routing fluctuations--changes in the routing of a given input to its target expert--at the late stage of model training, leading to model non-robustness. In this work, we unveil the limitation of SMoE through the perspective of the probabilistic graphical model (PGM). Through this PGM framework, we highlight the independence in the expert-selection of tokens, which exposes the model to routing fluctuation and non-robustness. Alleviating this independence, we propose the novel Similarity-Aware (S)MoE, which considers interactions between tokens during expert selection. We then derive a new PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE layer. Leveraging the token similarities captured by the attention matrix, we propose the innovative Attention-Aware (S)MoE, which employs the attention matrix to guide the routing of tokens to appropriate experts in (S)MoE. We theoretically prove that Similarity/Attention-Aware routing help reduce the entropy of expert selection, resulting in more stable token routing mechanisms. We empirically validate our models on various tasks and domains, showing significant improvements in reducing routing fluctuations, enhancing accuracy, and increasing model robustness over the baseline MoE-Transformer with token routing via softmax gating.</li>
</ul>

<h3>Title: Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging</h3>
<ul>
<li><strong>Authors: </strong>Fadi Abdeladhim Zidi, Abdelkrim Ouafi, Fares Bougourzi, Cosimo Distante, Abdelmalik Taleb-Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00805">https://arxiv.org/abs/2505.00805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00805">https://arxiv.org/pdf/2505.00805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00805]] Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging(https://arxiv.org/abs/2505.00805)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>As one of the most widely cultivated and consumed crops, wheat is essential to global food security. However, wheat production is increasingly challenged by pests, diseases, climate change, and water scarcity, threatening yields. Traditional crop monitoring methods are labor-intensive and often ineffective for early issue detection. Hyperspectral imaging (HSI) has emerged as a non-destructive and efficient technology for remote crop health assessment. However, the high dimensionality of HSI data and limited availability of labeled samples present notable challenges. In recent years, deep learning has shown great promise in addressing these challenges due to its ability to extract and analysis complex structures. Despite advancements in applying deep learning methods to HSI data for wheat crop analysis, no comprehensive survey currently exists in this field. This review addresses this gap by summarizing benchmark datasets, tracking advancements in deep learning methods, and analyzing key applications such as variety classification, disease detection, and yield estimation. It also highlights the strengths, limitations, and future opportunities in leveraging deep learning methods for HSI-based wheat crop analysis. We have listed the current state-of-the-art papers and will continue tracking updating them in the following this https URL.</li>
</ul>

<h3>Title: A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i</h3>
<ul>
<li><strong>Authors: </strong>Kola Ayonrinde, Louis Jaburi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00808">https://arxiv.org/abs/2505.00808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00808">https://arxiv.org/pdf/2505.00808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00808]] A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i(https://arxiv.org/abs/2505.00808)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Mechanistic Interpretability aims to understand neural networks through causal explanations. We argue for the Explanatory View Hypothesis: that Mechanistic Interpretability research is a principled approach to understanding models because neural networks contain implicit explanations which can be extracted and understood. We hence show that Explanatory Faithfulness, an assessment of how well an explanation fits a model, is well-defined. We propose a definition of Mechanistic Interpretability (MI) as the practice of producing Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural networks, allowing us to distinguish MI from other interpretability paradigms and detail MI's inherent limits. We formulate the Principle of Explanatory Optimism, a conjecture which we argue is a necessary precondition for the success of Mechanistic Interpretability.</li>
</ul>

<h3>Title: Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Jordi de la Torre</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00810">https://arxiv.org/abs/2505.00810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00810">https://arxiv.org/pdf/2505.00810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00810]] Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval(https://arxiv.org/abs/2505.00810)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Objective: To develop and evaluate a scalable methodology for harmonizing inconsistent units in large-scale clinical datasets, addressing a key barrier to data interoperability. Materials and Methods: We designed a novel unit harmonization system combining BM25, sentence embeddings, Bayesian optimization, and a bidirectional transformer based binary classifier for retrieving and matching laboratory test entries. The system was evaluated using the Optum Clinformatics Datamart dataset (7.5 billion entries). We implemented a multi-stage pipeline: filtering, identification, harmonization proposal generation, automated re-ranking, and manual validation. Performance was assessed using Mean Reciprocal Rank (MRR) and other standard information retrieval metrics. Results: Our hybrid retrieval approach combining BM25 and sentence embeddings (MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and embedding-only (MRR: 0.5277) approaches. The transformer-based reranker further improved performance (absolute MRR improvement: 0.10), bringing the final system MRR to 0.9833. The system achieved 83.39\% precision at rank 1 and 94.66\% recall at rank 5. Discussion: The hybrid architecture effectively leverages the complementary strengths of lexical and semantic approaches. The reranker addresses cases where initial retrieval components make errors due to complex semantic relationships in medical terminology. Conclusion: Our framework provides an efficient, scalable solution for unit harmonization in clinical datasets, reducing manual effort while improving accuracy. Once harmonized, data can be reused seamlessly in different analyses, ensuring consistency across healthcare systems and enabling more reliable multi-institutional studies and meta-analyses.</li>
</ul>

<h3>Title: Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kuan Zhang, Chengliang Chai, Jingzhe Xu, Chi Zhang, Ye Yuan, Guoren Wang, Lei Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00812">https://arxiv.org/abs/2505.00812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00812">https://arxiv.org/pdf/2505.00812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00812]] Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization(https://arxiv.org/abs/2505.00812)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.</li>
</ul>

<h3>Title: Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Mario Sänger, Ulf Leser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00814">https://arxiv.org/abs/2505.00814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00814">https://arxiv.org/pdf/2505.00814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00814]] Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction(https://arxiv.org/abs/2505.00814)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent approach in RE. Several studies report improved performance when incorporating additional context information while fine-tuning PLMs for RE. However, variations in the PLMs applied, the databases used for augmentation, hyper-parameter optimization, and evaluation methods complicate direct comparisons between studies and raise questions about the generalizability of these findings. Our study addresses this research gap by evaluating PLMs enhanced with contextual information on five datasets spanning four relation scenarios within a consistent evaluation framework. We evaluate three baseline PLMs and first conduct extensive hyperparameter optimization. After selecting the top-performing model, we enhance it with additional data, including textual entity descriptions, relational information from knowledge graphs, and molecular structure encodings. Our findings illustrate the importance of i) the choice of the underlying language model and ii) a comprehensive hyperparameter optimization for achieving strong extraction performance. Although inclusion of context information yield only minor overall improvements, an ablation study reveals substantial benefits for smaller PLMs when such external data was included during fine-tuning.</li>
</ul>

<h3>Title: Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Andrew Adiletta, Berk Sunar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00817">https://arxiv.org/abs/2505.00817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00817">https://arxiv.org/pdf/2505.00817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00817]] Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models(https://arxiv.org/abs/2505.00817)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, large language model</a></li>
<li><strong>Abstract: </strong>Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches. A significant challenge is the massive size of LLMs, which, by nature of their compute intensive operation, quickly evicts embedding vectors from the cache. We address this by balancing the number of tokens monitored against the amount of information leaked. Monitoring more tokens increases potential vocabulary leakage but raises the chance of missing cache hits due to eviction; monitoring fewer tokens improves detection reliability but limits vocabulary coverage. Through extensive experimentation, we demonstrate the feasibility of leaking tokens from LLMs via cache side-channels. Our findings reveal a new vulnerability in LLM deployments, highlighting that even sophisticated models are susceptible to traditional side-channel attacks. We discuss the implications for privacy and security in LLM-serving infrastructures and suggest considerations for mitigating such threats. For proof of concept we consider two concrete attack scenarios: Our experiments show that an attacker can recover as much as 80%-90% of a high entropy API key with single shot monitoring. As for English text we can reach a 40% recovery rate with a single shot. We should note that the rate highly depends on the monitored token set and these rates can be improved by targeting more specialized output domains.</li>
</ul>

<h3>Title: Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures</h3>
<ul>
<li><strong>Authors: </strong>Heng-Sheng Chang, Prashant G. Mehta</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00818">https://arxiv.org/abs/2505.00818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00818">https://arxiv.org/pdf/2505.00818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00818]] Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures(https://arxiv.org/abs/2505.00818)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents a mathematical framework for causal nonlinear prediction in settings where observations are generated from an underlying hidden Markov model (HMM). Both the problem formulation and the proposed solution are motivated by the decoder-only transformer architecture, in which a finite sequence of observations (tokens) is mapped to the conditional probability of the next token. Our objective is not to construct a mathematical model of a transformer. Rather, our interest lies in deriving, from first principles, transformer-like architectures that solve the prediction problem for which the transformer is designed. The proposed framework is based on an original optimal control approach, where the prediction objective (MMSE) is reformulated as an optimal control problem. An analysis of the optimal control problem is presented leading to a fixed-point equation on the space of probability measures. To solve the fixed-point equation, we introduce the dual filter, an iterative algorithm that closely parallels the architecture of decoder-only transformers. These parallels are discussed in detail along with the relationship to prior work on mathematical modeling of transformers as transport on the space of probability measures. Numerical experiments are provided to illustrate the performance of the algorithm using parameter values used in researchscale transformer models.</li>
</ul>

<h3>Title: Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Qianxi Fu, Youngjoon Suh, Xiaojing Zhang, Yoonjin Won</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00823">https://arxiv.org/abs/2505.00823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00823">https://arxiv.org/pdf/2505.00823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00823]] Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks(https://arxiv.org/abs/2505.00823)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Phase change plays a critical role in thermal management systems, yet quantitative characterization of multiphase heat transfer remains limited by the challenges of measuring temperature fields in chaotic, rapidly evolving flow regimes. While computational methods offer spatiotemporal resolution in idealized cases, replicating complex experimental conditions remains prohibitively difficult. Here, we present a data-driven framework that leverages a conditional generative adversarial network (CGAN) to infer temperature fields from geometric phase contours in a canonical pool boiling configuration where advanced data collection techniques are restricted. Using high-speed imaging data and simulation-informed training, our model demonstrates the ability to reconstruct temperature fields with errors below 6%. We further show that standard data augmentation strategies are effective in enhancing both accuracy and physical plausibility of the predicted maps across both simulation and experimental datasets when precise physical constraints are not applicable. Our results highlight the potential of deep generative models to bridge the gap between observable multiphase phenomena and underlying thermal transport, offering a powerful approach to augment and interpret experimental measurements in complex two-phase systems.</li>
</ul>

<h3>Title: Intersectional Divergence: Measuring Fairness in Regression</h3>
<ul>
<li><strong>Authors: </strong>Joe Germino, Nuno Moniz, Nitesh V. Chawla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00830">https://arxiv.org/abs/2505.00830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00830">https://arxiv.org/pdf/2505.00830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00830]] Intersectional Divergence: Measuring Fairness in Regression(https://arxiv.org/abs/2505.00830)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>Research on fairness in machine learning has been mainly framed in the context of classification tasks, leaving critical gaps in regression. In this paper, we propose a seminal approach to measure intersectional fairness in regression tasks, going beyond the focus on single protected attributes from existing work to consider combinations of all protected attributes. Furthermore, we contend that it is insufficient to measure the average error of groups without regard for imbalanced domain preferences. To this end, we propose Intersectional Divergence (ID) as the first fairness measure for regression tasks that 1) describes fair model behavior across multiple protected attributes and 2) differentiates the impact of predictions in target ranges most relevant to users. We extend our proposal demonstrating how ID can be adapted into a loss function, IDLoss, and used in optimization problems. Through an extensive experimental evaluation, we demonstrate how ID allows unique insights into model behavior and fairness, and how incorporating IDLoss into optimization can considerably improve single-attribute and intersectional model fairness while maintaining a competitive balance in predictive performance.</li>
</ul>

<h3>Title: From Texts to Shields: Convergence of Large Language Models and Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Tao Li, Ya-Ting Yang, Yunian Pan, Quanyan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00841">https://arxiv.org/abs/2505.00841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00841">https://arxiv.org/pdf/2505.00841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00841]] From Texts to Shields: Convergence of Large Language Models and Cybersecurity(https://arxiv.org/abs/2505.00841)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, fair, interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity.</li>
</ul>

<h3>Title: OET: Optimization-based prompt injection Evaluation Toolkit</h3>
<ul>
<li><strong>Authors: </strong>Jinsheng Pan, Xiaogeng Liu, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00843">https://arxiv.org/abs/2505.00843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00843">https://arxiv.org/pdf/2505.00843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00843]] OET: Optimization-based prompt injection Evaluation Toolkit(https://arxiv.org/abs/2505.00843)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, enabling their widespread adoption across various domains. However, their susceptibility to prompt injection attacks poses significant security risks, as adversarial inputs can manipulate model behavior and override intended instructions. Despite numerous defense strategies, a standardized framework to rigorously evaluate their effectiveness, especially under adaptive adversarial scenarios, is lacking. To address this gap, we introduce OET, an optimization-based evaluation toolkit that systematically benchmarks prompt injection attacks and defenses across diverse datasets using an adaptive testing framework. Our toolkit features a modular workflow that facilitates adversarial string generation, dynamic attack execution, and comprehensive result analysis, offering a unified platform for assessing adversarial robustness. Crucially, the adaptive testing framework leverages optimization methods with both white-box and black-box access to generate worst-case adversarial examples, thereby enabling strict red-teaming evaluations. Extensive experiments underscore the limitations of current defense mechanisms, with some models remaining susceptible even after implementing security enhancements.</li>
</ul>

<h3>Title: TherMod Communication: Low Power or Hot Air?</h3>
<ul>
<li><strong>Authors: </strong>Christiana Chamon</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00849">https://arxiv.org/abs/2505.00849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00849">https://arxiv.org/pdf/2505.00849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00849]] TherMod Communication: Low Power or Hot Air?(https://arxiv.org/abs/2505.00849)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, steal</a></li>
<li><strong>Abstract: </strong>The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme leverages statistical physics to enable secure communication with zero average power flow in a wired channel. While the original KLJN scheme requires significant power for operation, a recent wireless modification, TherMod, proposed by Basar claims a "low power" implementation. This paper critically examines this claim. We explain that the additional components inherent in Basar's wireless adaptation substantially increase power consumption, rendering the "low power" assertion inappropriate. Furthermore, we clarify that the security claims of the original KLJN scheme do not directly translate to this wireless adaptation, implying significant security breach. Finally, the scheme looks identical one of the stealth communicators from 2005, which was shown not to be secure.</li>
</ul>

<h3>Title: ICQuant: Index Coding enables Low-bit LLM Quantization</h3>
<ul>
<li><strong>Authors: </strong>Xinlin Li, Osama Hanna, Christina Fragouli, Suhas Diggavi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00850">https://arxiv.org/abs/2505.00850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00850">https://arxiv.org/pdf/2505.00850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00850]] ICQuant: Index Coding enables Low-bit LLM Quantization(https://arxiv.org/abs/2505.00850)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid deployment of Large Language Models (LLMs) highlights the need for efficient low-bit post-training quantization (PTQ), due to their high memory costs. A key challenge in weight quantization is the presence of outliers, which inflate quantization ranges and lead to large errors. While a number of outlier suppression techniques have been proposed, they either: fail to effectively shrink the quantization range, or incur (relatively) high bit overhead. In this paper, we present ICQuant, a novel framework that leverages outlier statistics to design an efficient index coding scheme for outlier-aware weight-only quantization. Compared to existing outlier suppression techniques requiring $\approx 1$ bit overhead to halve the quantization range, ICQuant requires only $\approx 0.3$ bits; a significant saving in extreme compression regimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing quantizers to eliminate outliers, improving the quantization quality. Using just 2.3 bits per weight and simple scalar quantizers, ICQuant improves the zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150% relative to QTIP and QuIP#; and it achieves comparable performance to the best-known fine-tuned quantizer (PV-tuning) without fine-tuning.</li>
</ul>

<h3>Title: Duality on the Thermodynamics of the Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Scheme</h3>
<ul>
<li><strong>Authors: </strong>Sarah Flanery, Anson Trapani, Christiana Chamon, Leyla Nazhandali</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00858">https://arxiv.org/abs/2505.00858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00858">https://arxiv.org/pdf/2505.00858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00858]] Duality on the Thermodynamics of the Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Scheme(https://arxiv.org/abs/2505.00858)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>This study investigates a duality approach to information leak detection in the generalized Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme. While previous work by Chamon and Kish sampled voltages at zero-current instances, this research explores sampling currents at zero-voltage crossings. The objective is to determine if this dual approach can reveal information leaks in non-equilibrium KLJN systems. Results indicate that the duality method successfully detects information leaks, further supporting the necessity of thermal equilibrium for unconditional security in KLJN systems.</li>
</ul>

<h3>Title: Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Tianya Zhao, Ningning Wang, Junqing Zhang, Xuyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00881">https://arxiv.org/abs/2505.00881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00881">https://arxiv.org/pdf/2505.00881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00881]] Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting(https://arxiv.org/abs/2505.00881)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, data-free, large language model</a></li>
<li><strong>Abstract: </strong>While supervised deep neural networks (DNNs) have proven effective for device authentication via radio frequency (RF) fingerprinting, they are hindered by domain shift issues and the scarcity of labeled data. The success of large language models has led to increased interest in unsupervised pre-trained models (PTMs), which offer better generalization and do not require labeled datasets, potentially addressing the issues mentioned above. However, the inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently explored. In this paper, we thoroughly investigate data-free backdoor attacks on such PTMs in RF fingerprinting, focusing on a practical scenario where attackers lack access to downstream data, label information, and training processes. To realize the backdoor attack, we carefully design a set of triggers and predefined output representations (PORs) for the PTMs. By mapping triggers and PORs through backdoor training, we can implant backdoor behaviors into the PTMs, thereby introducing vulnerabilities across different downstream RF fingerprinting tasks without requiring prior knowledge. Extensive experiments demonstrate the wide applicability of our proposed attack to various input domains, protocols, and PTMs. Furthermore, we explore potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack.</li>
</ul>

<h3>Title: Balancing Security and Liquidity: A Time-Weighted Snapshot Framework for DAO Governance Voting</h3>
<ul>
<li><strong>Authors: </strong>Zayn Wang, Frank Pu, Vinci Cheung, Robert Hao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00888">https://arxiv.org/abs/2505.00888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00888">https://arxiv.org/pdf/2505.00888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00888]] Balancing Security and Liquidity: A Time-Weighted Snapshot Framework for DAO Governance Voting(https://arxiv.org/abs/2505.00888)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>As new project upgrading the blockchain industry, novel forms of attack challenges developers to rethink about the design of their innovations. In the growth stage of the development, Decentralized Autonomous Organizations (DAO) introduces different approaches in managing fund through voting in governance tokens. However, relying on tokens as a weight for voting introduces opportunities for hackers to manipulate voting results through flash loan, allowing malicious proposals - fund withdrawal from DAO to hacker's wallet - to execute through the smart contract. In this research, we learned different defense mechanism against the flash loan attack, and their weakness in accessibility that compromise the security of different blockchain projects. Based on our observation, we propose a new defensing structure and apply it with cases.</li>
</ul>

<h3>Title: NeMo-Inspector: A Visualization Tool for LLM Generation Analysis</h3>
<ul>
<li><strong>Authors: </strong>Daria Gitman, Igor Gitman, Evelina Bakhturina</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00903">https://arxiv.org/abs/2505.00903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00903">https://arxiv.org/pdf/2505.00903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00903]] NeMo-Inspector: A Visualization Tool for LLM Generation Analysis(https://arxiv.org/abs/2505.00903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Adapting Large Language Models (LLMs) to novel tasks and enhancing their overall capabilities often requires large, high-quality training datasets. Synthetic data, generated at scale, serves a valuable alternative when real-world data is scarce or difficult to obtain. However, ensuring the quality of synthetic datasets is challenging, as developers must manually inspect and refine numerous samples to identify errors and areas for improvement. This process is time-consuming and requires specialized tools. We introduce NeMo-Inspector, an open-source tool designed to simplify the analysis of synthetic datasets with integrated inference capabilities. We demonstrate its effectiveness through two real-world cases. Analysis and cleaning of the synthetically generated GSM-Plus dataset with NeMo-Inspector led to a significant decrease in low-quality samples from 46.99% to 19.51%. The tool also helped identify and correct generation errors in OpenMath models, improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from Nemotron-4-340B.</li>
</ul>

<h3>Title: How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias</h3>
<ul>
<li><strong>Authors: </strong>Ruiquan Huang, Yingbin Liang, Jing Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00926">https://arxiv.org/abs/2505.00926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00926">https://arxiv.org/pdf/2505.00926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00926]] How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias(https://arxiv.org/abs/2505.00926)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as `even pairs' and `parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.</li>
</ul>

<h3>Title: Compact Recurrent Transformer with Persistent Memory</h3>
<ul>
<li><strong>Authors: </strong>Edison Mucllari, Zachary Daniels, David Zhang, Qiang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00929">https://arxiv.org/abs/2505.00929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00929">https://arxiv.org/pdf/2505.00929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00929]] Compact Recurrent Transformer with Persistent Memory(https://arxiv.org/abs/2505.00929)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The Transformer architecture has shown significant success in many language processing and visual tasks. However, the method faces challenges in efficiently scaling to long sequences because the self-attention computation is quadratic with respect to the input length. To overcome this limitation, several approaches scale to longer sequences by breaking long sequences into a series of segments, restricting self-attention to local dependencies between tokens within each segment and using a memory mechanism to manage information flow between segments. However, these approached generally introduce additional compute overhead that restricts them from being used for applications where limited compute memory and power are of great concern (such as edge computing). We propose a novel and efficient Compact Recurrent Transformer (CRT), which combines shallow Transformer models that process short local segments with recurrent neural networks to compress and manage a single persistent memory vector that summarizes long-range global information between segments. We evaluate CRT on WordPTB and WikiText-103 for next-token-prediction tasks, as well as on the Toyota Smarthome video dataset for classification. CRT achieves comparable or superior prediction results to full-length Transformers in the language datasets while using significantly shorter segments (half or quarter size) and substantially reduced FLOPs. Our approach also demonstrates state-of-the-art performance on the Toyota Smarthome video dataset.</li>
</ul>

<h3>Title: Robust Root Cause Diagnosis using In-Distribution Interventions</h3>
<ul>
<li><strong>Authors: </strong>Lokesh Nagalapatti, Ashutosh Srivastava, Sunita Sarawagi, Amit Sharma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00930">https://arxiv.org/abs/2505.00930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00930">https://arxiv.org/pdf/2505.00930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00930]] Robust Root Cause Diagnosis using In-Distribution Interventions(https://arxiv.org/abs/2505.00930)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Diagnosing the root cause of an anomaly in a complex interconnected system is a pressing problem in today's cloud services and industrial operations. We propose In-Distribution Interventions (IDI), a novel algorithm that predicts root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes should take on anomalous values; 2) **Fix:** had the root cause nodes assumed usual values, the target node would not have been anomalous. Prior methods of assessing the fix condition rely on counterfactuals inferred from a Structural Causal Model (SCM) trained on historical data. But since anomalies are rare and fall outside the training distribution, the fitted SCMs yield unreliable counterfactual estimates. IDI overcomes this by relying on interventional estimates obtained by solely probing the fitted SCM at in-distribution inputs. We present a theoretical analysis comparing and bounding the errors in assessing the fix condition using interventional and counterfactual estimates. We then conduct experiments by systematically varying the SCM's complexity to demonstrate the cases where IDI's interventional approach outperforms the counterfactual approach and vice versa. Experiments on both synthetic and PetShop RCD benchmark datasets demonstrate that \our\ consistently identifies true root causes more accurately and robustly than nine existing state-of-the-art RCD baselines. Code is released at this https URL.</li>
</ul>

<h3>Title: Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing</h3>
<ul>
<li><strong>Authors: </strong>Timur Jaganov, John Blake, Julián Villegas, Nicholas Carr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00931">https://arxiv.org/abs/2505.00931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00931">https://arxiv.org/pdf/2505.00931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00931]] Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing(https://arxiv.org/abs/2505.00931)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the potential for Large Language Models (LLMs) to scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first developed DynaWrite-a modular, microservices-based grammatical tutoring application which supports multiple LLMs to generate dynamic feedback to learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural chat to have the most potential to scale-up DA in the language learning classroom. Further testing of these two candidates found both models performed similarly in their ability to accurately identify grammatical errors in user sentences. However, GPT-4o consistently outperformed neural chat in the quality of its DA by generating clear, consistent, and progressively explicit hints. Real-time responsiveness and system stability were also confirmed through detailed performance testing, with GPT-4o exhibiting sufficient speed and stability. This study shows that LLMs can be used to scale-up dynamic assessment and thus enable dynamic assessment to be delivered to larger groups than possible in traditional teacher-learner settings.</li>
</ul>

<h3>Title: A Self-Supervised Transformer for Unusable Shared Bike Detection</h3>
<ul>
<li><strong>Authors: </strong>Yin Huang, Yongqi Dong, Youhua Tang, Alvaro García Hernandez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00932">https://arxiv.org/abs/2505.00932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00932">https://arxiv.org/pdf/2505.00932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00932]] A Self-Supervised Transformer for Unusable Shared Bike Detection(https://arxiv.org/abs/2505.00932)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>The rapid expansion of bike-sharing systems (BSS) has greatly improved urban "last-mile" connectivity, yet large-scale deployments face escalating operational challenges, particularly in detecting faulty bikes. Existing detection approaches either rely on static model-based thresholds that overlook dynamic spatiotemporal (ST) usage patterns or employ supervised learning methods that struggle with label scarcity and class imbalance. To address these limitations, this paper proposes a novel Self-Supervised Transformer (SSTransformer) framework for automatically detecting unusable shared bikes, leveraging ST features extracted from GPS trajectories and trip records. The model incorporates a self-supervised pre-training strategy to enhance its feature extraction capabilities, followed by fine-tuning for efficient status recognition. In the pre-training phase, the Transformer encoder learns generalized representations of bike movement via a self-supervised objective; in the fine-tuning phase, the encoder is adapted to a downstream binary classification task. Comprehensive experiments on a real-world dataset of 10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate that SSTransformer significantly outperforms traditional machine learning, ensemble learning, and deep learning baselines, achieving the best accuracy (97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the effectiveness of self-supervised Transformer on ST data for capturing complex anomalies in BSS, paving the way toward more reliable and scalable maintenance solutions for shared mobility.</li>
</ul>

<h3>Title: TunnElQNN: A Hybrid Quantum-classical Neural Network for Efficient Learning</h3>
<ul>
<li><strong>Authors: </strong>A. H. Abbas</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.app-ph, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00933">https://arxiv.org/abs/2505.00933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00933">https://arxiv.org/pdf/2505.00933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00933]] TunnElQNN: A Hybrid Quantum-classical Neural Network for Efficient Learning(https://arxiv.org/abs/2505.00933)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hybrid quantum-classical neural networks (HQCNNs) represent a promising frontier in machine learning, leveraging the complementary strengths of both models. In this work, we propose the development of TunnElQNN, a non-sequential architecture composed of alternating classical and quantum layers. Within the classical component, we employ the Tunnelling Diode Activation Function (TDAF), inspired by the I-V characteristics of quantum tunnelling. We evaluate the performance of this hybrid model on a synthetic dataset of interleaving half-circle for multi-class classification tasks with varying degrees of class overlap. The model is compared against a baseline hybrid architecture that uses the conventional ReLU activation function (ReLUQNN). Our results show that the TunnElQNN model consistently outperforms the ReLUQNN counterpart. Furthermore, we analyse the decision boundaries generated by TunnElQNN under different levels of class overlap and compare them to those produced by a neural network implementing TDAF within a fully classical architecture. These findings highlight the potential of integrating physics-inspired activation functions with quantum components to enhance the expressiveness and robustness of hybrid quantum-classical machine learning architectures.</li>
</ul>

<h3>Title: CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion</h3>
<ul>
<li><strong>Authors: </strong>Boyuan Meng, Xiaohan Zhang, Peilin Li, Zhe Wu, Yiming Li, Wenkai Zhao, Beinan Yu, Hui-Liang Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00938">https://arxiv.org/abs/2505.00938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00938">https://arxiv.org/pdf/2505.00938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00938]] CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion(https://arxiv.org/abs/2505.00938)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects across different domains with limited class instances. Feature confusion, including object-background confusion and object-object confusion, presents significant challenges in both cross-domain and few-shot settings. In this work, we introduce CDFormer, a cross-domain few-shot object detection transformer against feature confusion, to address these challenges. The method specifically tackles feature confusion through two key modules: object-background distinguishing (OBD) and object-object distinguishing (OOD). The OBD module leverages a learnable background token to differentiate between objects and background, while the OOD module enhances the distinction between objects of different classes. Experimental results demonstrate that CDFormer outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0% mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively, when fine-tuned.</li>
</ul>

<h3>Title: StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Wang, Molei Liu, Jing Lei, Francis Bach, Zijian Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.CO, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00940">https://arxiv.org/abs/2505.00940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00940">https://arxiv.org/pdf/2505.00940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00940]] StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization(https://arxiv.org/abs/2505.00940)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, fair</a></li>
<li><strong>Abstract: </strong>When synthesizing multisource high-dimensional data, a key objective is to extract low-dimensional feature representations that effectively approximate the original features across different sources. Such general feature extraction facilitates the discovery of transferable knowledge, mitigates systematic biases such as batch effects, and promotes fairness. In this paper, we propose Stable Principal Component Analysis (StablePCA), a novel method for group distributionally robust learning of latent representations from high-dimensional multi-source data. A primary challenge in generalizing PCA to the multi-source regime lies in the nonconvexity of the fixed rank constraint, rendering the minimax optimization nonconvex. To address this challenge, we employ the Fantope relaxation, reformulating the problem as a convex minimax optimization, with the objective defined as the maximum loss across sources. To solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox algorithm with explicit closed-form updates. Theoretically, we establish the global convergence of the Mirror Prox algorithm, with the convergence rate provided from the optimization perspective. Furthermore, we offer practical criteria to assess how closely the solution approximates the original nonconvex formulation. Through extensive numerical experiments, we demonstrate StablePCA's high accuracy and efficiency in extracting robust low-dimensional representations across various finite-sample scenarios.</li>
</ul>

<h3>Title: FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenxin Zhang, Ding Xu, Guangzhen Yao, Xiaojian Lin, Renxiang Guan, Chengze Du, Renda Han, Xi Xuan, Cuicui Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00941">https://arxiv.org/abs/2505.00941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00941">https://arxiv.org/pdf/2505.00941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00941]] FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection(https://arxiv.org/abs/2505.00941)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, transformer</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection is critical for system monitoring and risk identification, across various domains, such as finance and healthcare. However, for most reconstruction-based approaches, detecting anomalies remains a challenge due to the complexity of sequential patterns in time series data. On the one hand, reconstruction-based techniques are susceptible to computational deviation stemming from anomalies, which can lead to impure representations of normal sequence patterns. On the other hand, they often focus on the time-domain dependencies of time series, while ignoring the alignment of frequency information beyond the time domain. To address these challenges, we propose a novel Frequency-augmented Convolutional Transformer (FreCT). FreCT utilizes patch operations to generate contrastive views and employs an improved Transformer architecture integrated with a convolution module to capture long-term dependencies while preserving local topology information. The introduced frequency analysis based on Fourier transformation could enhance the model's ability to capture crucial characteristics beyond the time domain. To protect the training quality from anomalies and improve the robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and absolute error to optimize consistency information in both time and frequency domains. Extensive experiments on four public datasets demonstrate that FreCT outperforms existing methods in identifying anomalies.</li>
</ul>

<h3>Title: Addressing Noise and Stochasticity in Fraud Detection for Service Networks</h3>
<ul>
<li><strong>Authors: </strong>Wenxin Zhang, Ding Xu, Xi Xuan, Lei Jiang, Guangzhen Yao, Renda Han, Xiangxiang Lang, Cuicui Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00946">https://arxiv.org/abs/2505.00946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00946">https://arxiv.org/pdf/2505.00946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00946]] Addressing Noise and Stochasticity in Fraud Detection for Service Networks(https://arxiv.org/abs/2505.00946)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Fraud detection is crucial in social service networks to maintain user trust and improve service network security. Existing spectral graph-based methods address this challenge by leveraging different graph filters to capture signals with different frequencies in service networks. However, most graph filter-based methods struggle with deriving clean and discriminative graph signals. On the one hand, they overlook the noise in the information propagation process, resulting in degradation of filtering ability. On the other hand, they fail to discriminate the frequency-specific characteristics of graph signals, leading to distortion of signals fusion. To address these issues, we develop a novel spectral graph network based on information bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB splits the original graph into homophilic and heterophilic subgraphs to better capture the signals at different frequencies. For the first limitation, SGNN-IB applies information bottleneck theory to extract key characteristics of encoded representations. For the second limitation, SGNN-IB introduces prototype learning to implement signal fusion, preserving the frequency-specific characteristics of signals. Extensive experiments on three real-world datasets demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.</li>
</ul>

<h3>Title: Tree-Sliced Wasserstein Distance with Nonlinear Projection</h3>
<ul>
<li><strong>Authors: </strong>Thanh Tran, Viet-Hoang Tran, Thanh Chu, Trang Pham, Laurent El Ghaoui, Tam Le, Tan M. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00968">https://arxiv.org/abs/2505.00968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00968">https://arxiv.org/pdf/2505.00968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00968]] Tree-Sliced Wasserstein Distance with Nonlinear Projection(https://arxiv.org/abs/2505.00968)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tree-Sliced methods have recently emerged as an alternative to the traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines with tree-based metric spaces and incorporating a splitting mechanism for projecting measures. This approach enhances the ability to capture the topological structures of integration domains in Sliced Optimal Transport while maintaining low computational costs. Building on this foundation, we propose a novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW) distance, substituting the linear projections in earlier versions with general projections, while ensuring the injectivity of the associated Radon Transform and preserving the well-definedness of the resulting metric. By designing appropriate projections, we construct efficient metrics for measures on both Euclidean spaces and spheres. Finally, we validate our proposed metric through extensive numerical experiments for Euclidean and spherical datasets. Applications include gradient flows, self-supervised learning, and generative models, where our methods demonstrate significant improvements over recent SW and TSW variants.</li>
</ul>

<h3>Title: A Minimax-MDP Framework with Future-imposed Conditions for Learning-augmented Problems</h3>
<ul>
<li><strong>Authors: </strong>Xin Chen, Yuze Chen, Yuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00973">https://arxiv.org/abs/2505.00973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00973">https://arxiv.org/pdf/2505.00973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00973]] A Minimax-MDP Framework with Future-imposed Conditions for Learning-augmented Problems(https://arxiv.org/abs/2505.00973)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study a class of sequential decision-making problems with augmented predictions, potentially provided by a machine learning algorithm. In this setting, the decision-maker receives prediction intervals for unknown parameters that become progressively refined over time, and seeks decisions that are competitive with the hindsight optimal under all possible realizations of both parameters and predictions. We propose a minimax Markov Decision Process (minimax-MDP) framework, where the system state consists of an adversarially evolving environment state and an internal state controlled by the decision-maker. We introduce a set of future-imposed conditions that characterize the feasibility of minimax-MDPs and enable the design of efficient, often closed-form, robustly competitive policies. We illustrate the framework through three applications: multi-period inventory ordering with refining demand predictions, resource allocation with uncertain utility functions, and a multi-phase extension of the minimax-MDP applied to the inventory problem with time-varying ordering costs. Our results provide a tractable and versatile approach to robust online decision-making under predictive uncertainty.</li>
</ul>

<h3>Title: Attack and defense techniques in large language models: A survey and new perspectives</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Liao, Kang Chen, Yuanguo Lin, Kangkang Li, Yunxuan Liu, Hefeng Chen, Xingwang Huang, Yuanhui Yu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00976">https://arxiv.org/abs/2505.00976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00976">https://arxiv.org/pdf/2505.00976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00976]] Attack and defense techniques in large language models: A survey and new perspectives(https://arxiv.org/abs/2505.00976)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.</li>
</ul>

<h3>Title: A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts</h3>
<ul>
<li><strong>Authors: </strong>Yingquan Chen, Qianmu Li, Xiaocong Wu, Huifeng Li, Qing Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00977">https://arxiv.org/abs/2505.00977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00977">https://arxiv.org/pdf/2505.00977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00977]] A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts(https://arxiv.org/abs/2505.00977)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating high-quality steganographic text is a fundamental challenge in the field of generative linguistic steganography. This challenge arises primarily from two aspects: firstly, the capabilities of existing models in text generation are limited; secondly, embedding algorithms fail to effectively mitigate the negative impacts of sensitive information's properties, such as semantic content or randomness. Specifically, to ensure that the recipient can accurately extract hidden information, embedding algorithms often have to consider selecting candidate words with relatively low probabilities. This phenomenon leads to a decrease in the number of high-probability candidate words and an increase in low-probability candidate words, thereby compromising the semantic coherence and logical fluency of the steganographic text and diminishing the overall quality of the generated steganographic material. To address this issue, this paper proposes a novel embedding algorithm, character-based diffusion embedding algorithm (CDEA). Unlike existing embedding algorithms that strive to eliminate the impact of sensitive information's properties on the generation process, CDEA leverages sensitive information's properties. It enhances the selection frequency of high-probability candidate words in the candidate pool based on general statistical properties at the character level and grouping methods based on power-law distributions, while reducing the selection frequency of low-probability candidate words in the candidate pool. Furthermore, to ensure the effective transformation of sensitive information in long sequences, we also introduce the XLNet model. Experimental results demonstrate that the combination of CDEA and XLNet significantly improves the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility.</li>
</ul>

<h3>Title: Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuhui Jiang, Shengjie Ma, Chengjin Xu, Cehao Yang, Liyu Zhang, Jian Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00979">https://arxiv.org/abs/2505.00979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00979">https://arxiv.org/pdf/2505.00979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00979]] Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models(https://arxiv.org/abs/2505.00979)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.</li>
</ul>

<h3>Title: LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment</h3>
<ul>
<li><strong>Authors: </strong>Jiahuan Long, Xin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00980">https://arxiv.org/abs/2505.00980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00980">https://arxiv.org/pdf/2505.00980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00980]] LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment(https://arxiv.org/abs/2505.00980)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation provides an additional depth dimension to RGB images, making it widely applicable in various fields such as virtual reality, autonomous driving and robotic navigation. However, existing depth estimation algorithms often struggle to effectively balance performance and computational efficiency, which poses challenges for deployment on resource-constrained devices. To address this, we propose LMDepth, a lightweight Mamba-based monocular depth estimation network, designed to reconstruct high-precision depth information while maintaining low computational overhead. Specifically, we propose a modified pyramid spatial pooling module that serves as a multi-scale feature aggregator and context extractor, ensuring global spatial information for accurate depth estimation. Moreover, we integrate multiple depth Mamba blocks into the decoder. Designed with linear computations, the Mamba Blocks enable LMDepth to efficiently decode depth information from global features, providing a lightweight alternative to Transformer-based architectures that depend on complex attention mechanisms. Extensive experiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of our proposed LMDepth. Compared to previous lightweight depth estimation methods, LMDepth achieves higher performance with fewer parameters and lower computational complexity (measured by GFLOPs). We further deploy LMDepth on an embedded platform with INT8 quantization, validating its practicality for real-world edge applications.</li>
</ul>

<h3>Title: Position: Enough of Scaling LLMs! Lets Focus on Downscaling</h3>
<ul>
<li><strong>Authors: </strong>Ayan Sengupta, Yash Goel, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00985">https://arxiv.org/abs/2505.00985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00985">https://arxiv.org/pdf/2505.00985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00985]] Position: Enough of Scaling LLMs! Lets Focus on Downscaling(https://arxiv.org/abs/2505.00985)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.</li>
</ul>

<h3>Title: On-demand Test-time Adaptation for Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Xiao Ma, Young D. Kwon, Dong Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00986">https://arxiv.org/abs/2505.00986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00986">https://arxiv.org/pdf/2505.00986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00986]] On-demand Test-time Adaptation for Edge Devices(https://arxiv.org/abs/2505.00986)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Continual Test-time adaptation (CTTA) continuously adapts the deployed model on every incoming batch of data. While achieving optimal accuracy, existing CTTA approaches present poor real-world applicability on resource-constrained edge devices, due to the substantial memory overhead and energy consumption. In this work, we first introduce a novel paradigm -- on-demand TTA -- which triggers adaptation only when a significant domain shift is detected. Then, we present OD-TTA, an on-demand TTA framework for accurate and efficient adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a lightweight domain shift detection mechanism to activate TTA only when it is needed, drastically reducing the overall computation overhead, 2) a source domain selection module that chooses an appropriate source model for adaptation, ensuring high and robust accuracy, 3) a decoupled Batch Normalization (BN) update scheme to enable memory-efficient adaptation with small batch sizes. Extensive experiments show that OD-TTA achieves comparable and even better performance while reducing the energy and computation overhead remarkably, making TTA a practical reality.</li>
</ul>

<h3>Title: Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yu Hua, Weiming Liu, Gui Xu, Yaqing Hou, Yew-Soon Ong, Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00998">https://arxiv.org/abs/2505.00998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00998">https://arxiv.org/pdf/2505.00998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00998]] Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis(https://arxiv.org/abs/2505.00998)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. Recent score-based generative models (SGMs) have demonstrated impressive results on this task. However, their training process involves complex curvature trajectories, leading to unstable training process. In this paper, we propose a Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for human motion synthesis. DSDFM consists of two stages. The first human motion reconstruction stage aims to learn the latent space distribution of human motions. The second diverse motion generation stage aims to build connections between the Gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. This stage is achieved by the designed deterministic feature mapping procedure with DerODE and stochastic diverse output generation procedure with this http URL is easy to train compared to previous SGMs-based methods and can enhance diversity without introducing additional training this http URL qualitative and quantitative experiments, DSDFM achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.</li>
</ul>

<h3>Title: 3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer</h3>
<ul>
<li><strong>Authors: </strong>Kamel Aouaidjia, Aofan Li, Wenhao Zhang, Chongsheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01003">https://arxiv.org/abs/2505.01003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01003">https://arxiv.org/pdf/2505.01003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01003]] 3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer(https://arxiv.org/abs/2505.01003)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the prevailing techniques for 3D human pose estimation. However, Transformer-based methods either ignore the spatial neighborhood relationships between the joints when used for skeleton representations or disregard the local temporal patterns of the local joint movements in skeleton sequence modeling, while GCN-based methods often neglect the need for pose-specific representations. To address these problems, we propose a new method that exploits the graph modeling capability of GCN to represent each skeleton with multiple graphs of different orders, incorporated with a newly introduced Graph Order Attention module that dynamically emphasizes the most representative orders for each joint. The resulting spatial features of the sequence are further processed using a proposed temporal Body Aware Transformer that models the global body feature dependencies in the sequence with awareness of the local inter-skeleton feature dependencies of joints. Given that our 3D pose output aligns with the central 2D pose in the sequence, we improve the self-attention mechanism to be aware of the central pose while diminishing its focus gradually towards the first and the last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I datasets demonstrate the effectiveness of the proposed method. Code and models are made available on Github.</li>
</ul>

<h3>Title: Token-free Models for Sarcasm Detection</h3>
<ul>
<li><strong>Authors: </strong>Sumit Mamtani, Maitreya Sonawane, Kanika Agarwal, Nishanth Sanjeev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01006">https://arxiv.org/abs/2505.01006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01006">https://arxiv.org/pdf/2505.01006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01006]] Token-free Models for Sarcasm Detection(https://arxiv.org/abs/2505.01006)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tokenization is a foundational step in most natural language processing (NLP) pipelines, yet it introduces challenges such as vocabulary mismatch and out-of-vocabulary issues. Recent work has shown that models operating directly on raw text at the byte or character level can mitigate these limitations. In this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of sarcasm detection in both social media (Twitter) and non-social media (news headlines) domains. We fine-tune and benchmark these models against token-based baselines and state-of-the-art approaches. Our results show that ByT5-small and CANINE outperform token-based counterparts and achieve new state-of-the-art performance, improving accuracy by 0.77% and 0.49% on the News Headlines and Twitter Sarcasm datasets, respectively. These findings underscore the potential of token-free models for robust NLP in noisy and informal domains such as social media.</li>
</ul>

<h3>Title: Towards the Resistance of Neural Network Watermarking to Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Ling Tang, Yuefeng Chen, Hui Xue, Quanshi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01007">https://arxiv.org/abs/2505.01007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01007">https://arxiv.org/pdf/2505.01007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01007]] Towards the Resistance of Neural Network Watermarking to Fine-tuning(https://arxiv.org/abs/2505.01007)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>This paper proves a new watermarking method to embed the ownership information into a deep neural network (DNN), which is robust to fine-tuning. Specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised Fourier transform to extract frequency components from the convolutional filter. Additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. In this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. Preliminary experiments demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content</h3>
<ul>
<li><strong>Authors: </strong>Haoyue Bai, Yiyou Sun, Wei Cheng, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01008">https://arxiv.org/abs/2505.01008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01008">https://arxiv.org/pdf/2505.01008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01008]] Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content(https://arxiv.org/abs/2505.01008)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The recent proliferation of photorealistic images created by generative models has sparked both excitement and concern, as these images are increasingly indistinguishable from real ones to the human eye. While offering new creative and commercial possibilities, the potential for misuse, such as in misinformation and fraud, highlights the need for effective detection methods. Current detection approaches often rely on access to model weights or require extensive collections of real image datasets, limiting their scalability and practical application in real world scenarios. In this work, we introduce a novel black box detection framework that requires only API access, sidestepping the need for model weights or large auxiliary datasets. Our approach leverages a corrupt and recover strategy: by masking part of an image and assessing the model ability to reconstruct it, we measure the likelihood that the image was generated by the model itself. For black-box models that do not support masked image inputs, we incorporate a cost efficient surrogate model trained to align with the target model distribution, enhancing detection capability. Our framework demonstrates strong performance, outperforming baseline methods by 4.31% in mean average precision across eight diffusion model variant datasets.</li>
</ul>

<h3>Title: Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Jongwook Han, Dongmin Choi, Woojung Song, Eun-Ju Lee, Yohan Jo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01015">https://arxiv.org/abs/2505.01015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01015">https://arxiv.org/pdf/2505.01015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01015]] Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark(https://arxiv.org/abs/2505.01015)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs' value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage and thus ecological validity. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects' actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 27 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data.</li>
</ul>

<h3>Title: Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing</h3>
<ul>
<li><strong>Authors: </strong>Ruyu Yan, Da-Qing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01032">https://arxiv.org/abs/2505.01032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01032">https://arxiv.org/pdf/2505.01032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01032]] Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing(https://arxiv.org/abs/2505.01032)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Edge detection is crucial in image processing, but existing methods often produce overly detailed edge maps, affecting clarity. Fixed-window statistical testing faces issues like scale mismatch and computational redundancy. To address these, we propose a novel Multi-scale Adaptive Independence Testing-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive Statistical Testing-based edge detection and denoising method that integrates a channel attention mechanism with independence testing. A gradient-driven adaptive window strategy adjusts window sizes dynamically, improving detail preservation and noise suppression. EDD-MAIT achieves better robustness, accuracy, and efficiency, outperforming traditional and learning-based methods on BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and reduced runtime. It also shows robustness against Gaussian noise, generating accurate and clean edge maps in noisy environments.</li>
</ul>

<h3>Title: Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Lui Yoshida</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01035">https://arxiv.org/abs/2505.01035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01035">https://arxiv.org/pdf/2505.01035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01035]] Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?(https://arxiv.org/abs/2505.01035)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the necessity and impact of a detailed rubric in automated essay scoring (AES) using large language models (LLMs). While using rubrics are standard in LLM-based AES, creating detailed rubrics requires substantial ef-fort and increases token usage. We examined how different levels of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11 dataset. Our experiments compared three conditions: a full rubric, a simplified rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5 Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of four models maintained similar scoring accuracy with the simplified rubric compared to the detailed one, while significantly reducing token usage. However, one model (Gemini 1.5 Flash) showed decreased performance with more detailed rubrics. The findings suggest that simplified rubrics may be sufficient for most LLM-based AES applications, offering a more efficient alternative without compromis-ing scoring accuracy. However, model-specific evaluation remains crucial as per-formance patterns vary across different LLMs.</li>
</ul>

<h3>Title: Edge Detection based on Channel Attention and Inter-region Independence Test</h3>
<ul>
<li><strong>Authors: </strong>Ru-yu Yan, Da-Qing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01040">https://arxiv.org/abs/2505.01040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01040">https://arxiv.org/pdf/2505.01040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01040]] Edge Detection based on Channel Attention and Inter-region Independence Test(https://arxiv.org/abs/2505.01040)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing edge detection methods often suffer from noise amplification and excessive retention of non-salient details, limiting their applicability in high-precision industrial scenarios. To address these challenges, we propose CAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM) and Edge Detection via Independence Testing (EDIT). The CAM module adaptively enhances discriminative edge features through multi-channel fusion, while the EDIT module employs region-wise statistical independence analysis (using Fisher's exact test and chi-square test) to suppress uncorrelated this http URL experiments on BSDS500 and NYUDv2 datasets demonstrate state-of-the-art performance. Among the nine comparison algorithms, the F-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of 19.2\% to 26.5\% over traditional methods (Canny, CannySR), and better than the latest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations further reveal a 2.2\% PSNR improvement under Gaussian noise compared to baseline methods. Qualitative results exhibit cleaner edge maps with reduced artifacts, demonstrating its potential for high-precision industrial applications.</li>
</ul>

<h3>Title: Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Hao, Jianyuan Guo, Li Shen, Yong Luo, Han Hu, Guoxia Wang, Dianhai Yu, Yonggang Wen, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01043">https://arxiv.org/abs/2505.01043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01043">https://arxiv.org/pdf/2505.01043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01043]] Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities(https://arxiv.org/abs/2505.01043)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several components$\unicode{x2013}$such as weights, activations, and gradients$\unicode{x2013}$each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in this https URL.</li>
</ul>

<h3>Title: Capability-Based Multi-Tenant Access Management in Crowdsourced Drone Services</h3>
<ul>
<li><strong>Authors: </strong>Junaid Akram, Ali Anaissi, Awais Akram, Youcef Djenouri, Palash Ingle, Rutvij H. Jhaveri</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01048">https://arxiv.org/abs/2505.01048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01048">https://arxiv.org/pdf/2505.01048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01048]] Capability-Based Multi-Tenant Access Management in Crowdsourced Drone Services(https://arxiv.org/abs/2505.01048)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>We propose a capability-based access control method that leverages OAuth 2.0 and Verifiable Credentials (VCs) to share resources in crowdsourced drone services. VCs securely encode claims about entities, offering flexibility. However, standardized protocols for VCs are lacking, limiting their adoption. To address this, we integrate VCs into OAuth 2.0, creating a novel access token. This token encapsulates VCs using JSON Web Tokens (JWT) and employs JWT-based methods for proof of possession. Our method streamlines VC verification with JSON Web Signatures (JWS) requires only minor adjustments to current OAuth 2.0 systems. Furthermore, in order to increase security and efficiency in multi-tenant environments, we provide a novel protocol for VC creation that makes use of the OAuth 2.0 client credentials grant. Using VCs as access tokens enhances OAuth 2.0, supporting long-term use and efficient data management. This system aids bushfire management authorities by ensuring high availability, enhanced privacy, and improved data portability. It supports multi-tenancy, allowing drone operators to control data access policies in a decentralized environment.</li>
</ul>

<h3>Title: Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Nishant Jain, Xunpeng Huang, Yian Ma, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AP, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01049">https://arxiv.org/abs/2505.01049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01049">https://arxiv.org/pdf/2505.01049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01049]] Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees(https://arxiv.org/abs/2505.01049)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Consistency models have recently emerged as a compelling alternative to traditional SDE based diffusion models, offering a significant acceleration in generation by producing high quality samples in very few steps. Despite their empirical success, a proper theoretic justification for their speed up is still lacking. In this work, we provide the analysis which bridges this gap, showing that given a consistency model which can map the input at a given time to arbitrary timestamps along the reverse trajectory, one can achieve KL divergence of order $ O(\varepsilon^2) $ using only $ O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant step size, where d is the data dimension. Additionally, under minimal assumptions on the data distribution an increasingly common setting in recent diffusion model analyses we show that a similar KL convergence guarantee can be obtained, with the number of steps scaling as $ O\left(d \log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide a theoretical analysis for estimation of such consistency models, concluding that accurate learning is feasible using small discretization steps, both in smooth and non smooth settings. Notably, our results for the non smooth case yield best in class convergence rates compared to existing SDE or ODE based analyses under minimal assumptions.</li>
</ul>

<h3>Title: Transferable Adversarial Attacks on Black-Box Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, Matt Fredrikson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01050">https://arxiv.org/abs/2505.01050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01050">https://arxiv.org/pdf/2505.01050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01050]] Transferable Adversarial Attacks on Black-Box Vision-Language Models(https://arxiv.org/abs/2505.01050)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to proprietary black-box models in text-only and vision-only contexts, the extent and effectiveness of such vulnerabilities remain underexplored for VLLMs. We present a comprehensive analysis demonstrating that targeted adversarial examples are highly transferable to widely-used proprietary VLLMs such as GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to induce specific attacker-chosen interpretations of visual information, such as misinterpreting hazardous content as safe, overlooking sensitive or restricted material, or generating detailed incorrect responses aligned with the attacker's intent. Furthermore, we discover that universal perturbations -- modifications applicable to a wide set of images -- can consistently induce these misinterpretations across multiple proprietary VLLMs. Our experimental results on object recognition, visual question answering, and image captioning show that this vulnerability is common across current state-of-the-art models, and underscore an urgent need for robust mitigations to ensure the safe and secure deployment of VLLMs.</li>
</ul>

<h3>Title: GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Boris Kriuk, Matey Yordanov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01057">https://arxiv.org/abs/2505.01057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01057">https://arxiv.org/pdf/2505.01057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01057]] GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation(https://arxiv.org/abs/2505.01057)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces GeloVec, a new CNN-based attention smoothing framework for semantic segmentation that addresses critical limitations in conventional approaches. While existing attention-backed segmentation methods suffer from boundary instability and contextual discontinuities during feature mapping, our framework implements a higher-dimensional geometric smoothing method to establish a robust manifold relationships between visually coherent regions. GeloVec combines modified Chebyshev distance metrics with multispatial transformations to enhance segmentation accuracy through stabilized feature extraction. The core innovation lies in the adaptive sampling weights system that calculates geometric distances in n-dimensional feature space, achieving superior edge preservation while maintaining intra-class homogeneity. The multispatial transformation matrix incorporates tensorial projections with orthogonal basis vectors, creating more discriminative feature representations without sacrificing computational efficiency. Experimental validation across multiple benchmark datasets demonstrates significant improvements in segmentation performance, with mean Intersection over Union (mIoU) gains of 2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets respectively compared to state-of-the-art methods. GeloVec's mathematical foundation in Riemannian geometry provides theoretical guarantees on segmentation stability. Importantly, our framework maintains computational efficiency through parallelized implementation of geodesic transformations and exhibits strong generalization capabilities across disciplines due to the absence of information loss during transformations.</li>
</ul>

<h3>Title: Monotone Peridynamic Neural Operator for Nonlinear Material Modeling with Conditionally Unique Solutions</h3>
<ul>
<li><strong>Authors: </strong>Jihong Wang, Xiaochuan Tian, Zhongqiang Zhang, Stewart Silling, Siavash Jafarzadeh, Yue Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01060">https://arxiv.org/abs/2505.01060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01060">https://arxiv.org/pdf/2505.01060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01060]] Monotone Peridynamic Neural Operator for Nonlinear Material Modeling with Conditionally Unique Solutions(https://arxiv.org/abs/2505.01060)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Data-driven methods have emerged as powerful tools for modeling the responses of complex nonlinear materials directly from experimental measurements. Among these methods, the data-driven constitutive models present advantages in physical interpretability and generalizability across different boundary conditions/domain settings. However, the well-posedness of these learned models is generally not guaranteed a priori, which makes the models prone to non-physical solutions in downstream simulation tasks. In this study, we introduce monotone peridynamic neural operator (MPNO), a novel data-driven nonlocal constitutive model learning approach based on neural operators. Our approach learns a nonlocal kernel together with a nonlinear constitutive relation, while ensuring solution uniqueness through a monotone gradient network. This architectural constraint on gradient induces convexity of the learnt energy density function, thereby guaranteeing solution uniqueness of MPNO in small deformation regimes. To validate our approach, we evaluate MPNO's performance on both synthetic and real-world datasets. On synthetic datasets with manufactured kernel and constitutive relation, we show that the learnt model converges to the ground-truth as the measurement grid size decreases both theoretically and numerically. Additionally, our MPNO exhibits superior generalization capabilities than the conventional neural networks: it yields smaller displacement solution errors in down-stream tasks with new and unseen loadings. Finally, we showcase the practical utility of our approach through applications in learning a homogenized model from molecular dynamics data, highlighting its expressivity and robustness in real-world scenarios.</li>
</ul>

<h3>Title: Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01064">https://arxiv.org/abs/2505.01064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01064">https://arxiv.org/pdf/2505.01064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01064]] Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs(https://arxiv.org/abs/2505.01064)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.</li>
</ul>

<h3>Title: Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation</h3>
<ul>
<li><strong>Authors: </strong>David Jin, Qian Fu, Yuekang Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01065">https://arxiv.org/abs/2505.01065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01065">https://arxiv.org/pdf/2505.01065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01065]] Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation(https://arxiv.org/abs/2505.01065)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in code-related tasks, raising concerns about their potential for automated exploit generation (AEG). This paper presents the first systematic study on LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical proficiency. To mitigate dataset bias, we introduce a benchmark with refactored versions of five software security labs. Additionally, we design an LLM-based attacker to systematically prompt LLMs for exploit generation. Our experiments reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to uncensored models, while Llama3 is the most resistant. However, no model successfully generates exploits for refactored labs, though GPT-4o's minimal errors highlight the potential for LLM-driven AEG advancements.</li>
</ul>

<h3>Title: A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Ding, Qian Fu, Junchen Ding, Gelei Deng, Yi Liu, Yuekang Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01067">https://arxiv.org/abs/2505.01067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01067">https://arxiv.org/pdf/2505.01067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01067]] A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories(https://arxiv.org/abs/2505.01067)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have spurred the development of diverse AI applications from code generation and video editing to text generation; however, AI supply chains such as Hugging Face, which host pretrained models and their associated configuration files contributed by the public, face significant security challenges; in particular, configuration files originally intended to set up models by specifying parameters and initial settings can be exploited to execute unauthorized code, yet research has largely overlooked their security compared to that of the models themselves; in this work, we present the first comprehensive study of malicious configurations on Hugging Face, identifying three attack scenarios (file, website, and repository operations) that expose inherent risks; to address these threats, we introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in the context of their associated runtime code and critical libraries, effectively detecting suspicious elements with low false positive rates and high accuracy; our extensive evaluation uncovers thousands of suspicious repositories and configuration files, underscoring the urgent need for enhanced security validation in AI model hosting platforms.</li>
</ul>

<h3>Title: Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yijie Jin, Junjie Peng, Xuanchao Lin, Haochen Yuan, Lan Wang, Cangzhi Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01068">https://arxiv.org/abs/2505.01068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01068">https://arxiv.org/pdf/2505.01068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01068]] Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs(https://arxiv.org/abs/2505.01068)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets.</li>
</ul>

<h3>Title: Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits</h3>
<ul>
<li><strong>Authors: </strong>Edvin Fasth, Sagar Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01070">https://arxiv.org/abs/2505.01070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01070">https://arxiv.org/pdf/2505.01070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01070]] Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits(https://arxiv.org/abs/2505.01070)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) has become a powerful tool for training compact student models using larger, pretrained teacher models, often requiring less data and computational resources. Teacher models typically possess more layers and thus exhibit richer feature representations compared to their student counterparts. Furthermore, student models tend to learn simpler, surface-level features in their early layers. This discrepancy can increase errors in groups where labels spuriously correlate with specific input attributes, leading to a decline in group fairness even when overall accuracy remains comparable to the teacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs), which enable predictions at multiple intermediate layers, have been employed. Confidence margins derived from these early exits have been utilized to reweight both cross-entropy and distillation losses on a per-instance basis. In this paper, we propose that leveraging Laplace approximation-based methods to obtain well-calibrated uncertainty estimates can also effectively reweight challenging instances and improve group fairness. We hypothesize that Laplace approximation offers a more robust identification of difficult or ambiguous instances compared to margin-based approaches. To validate our claims, we benchmark our approach using a Bert-based model on the MultiNLI dataset.</li>
</ul>

<h3>Title: Federated Adapter on Foundation Models: An Out-Of-Distribution Approach</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Yang, Guodong Long, Tianyi Zhou, Qinghua Lu, Shanshan Ye, Jing Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01075">https://arxiv.org/abs/2505.01075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01075">https://arxiv.org/pdf/2505.01075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01075]] Federated Adapter on Foundation Models: An Out-Of-Distribution Approach(https://arxiv.org/abs/2505.01075)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>As foundation models gain prominence, Federated Foundation Models (FedFM) have emerged as a privacy-preserving approach to collaboratively fine-tune models in federated learning (FL) frameworks using distributed datasets across clients. A key challenge for FedFM, given the versatile nature of foundation models, is addressing out-of-distribution (OOD) generalization, where unseen tasks or clients may exhibit distribution shifts leading to suboptimal performance. Although numerous studies have explored OOD generalization in conventional FL, these methods are inadequate for FedFM due to the challenges posed by large parameter scales and increased data heterogeneity. To address these, we propose FedOA, which employs adapter-based parameter-efficient fine-tuning methods for efficacy and introduces personalized adapters with feature distance-based regularization to align distributions and guarantee OOD generalization for each client. Theoretically, we demonstrate that the conventional aggregated global model in FedFM inherently retains OOD generalization capabilities, and our proposed method enhances the personalized model's OOD generalization through regularization informed by the global model, with proven convergence under general non-convex settings. Empirically, the effectiveness of the proposed method is validated on benchmark datasets across various NLP tasks.</li>
</ul>

<h3>Title: Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Daniele Molino, Francesco di Feola, Linlin Shen, Paolo Soda, Valerio Guarrasi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01091">https://arxiv.org/abs/2505.01091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01091">https://arxiv.org/pdf/2505.01091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01091]] Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation(https://arxiv.org/abs/2505.01091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have revolutionized Artificial Intelligence (AI), particularly in multimodal applications. However, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent need for clinical accuracy. In this work, we introduce a framework specifically designed for multimodal medical data generation. By enabling the generation of multi-view chest X-rays and their associated clinical report, it bridges the gap between general-purpose vision-language models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR dataset, the proposed framework shows superior performance in generating high-fidelity images and semantically coherent reports. Our quantitative evaluation reveals significant results in terms of FID and BLEU scores, showcasing the quality of the generated data. Notably, our framework achieves comparable or even superior performance compared to real data on downstream disease classification tasks, underlining its potential as a tool for medical research and diagnostics. This study highlights the importance of domain-specific adaptations in enhancing the relevance and utility of generative models for clinical applications, paving the way for future advancements in synthetic multimodal medical data generation.</li>
</ul>

<h3>Title: VSC: Visual Search Compositional Text-to-Image Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Do Huu Dat, Nam Hyeonu, Po-Yuan Mao, Tae-Hyun Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01104">https://arxiv.org/abs/2505.01104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01104">https://arxiv.org/pdf/2505.01104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01104]] VSC: Visual Search Compositional Text-to-Image Diffusion Model(https://arxiv.org/abs/2505.01104)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have shown impressive capabilities in generating realistic visuals from natural-language prompts, yet they often struggle with accurately binding attributes to corresponding objects, especially in prompts containing multiple attribute-object pairs. This challenge primarily arises from the limitations of commonly used text encoders, such as CLIP, which can fail to encode complex linguistic relationships and modifiers effectively. Existing approaches have attempted to mitigate these issues through attention map control during inference and the use of layout information or fine-tuning during training, yet they face performance drops with increased prompt complexity. In this work, we introduce a novel compositional generation method that leverages pairwise image embeddings to improve attribute-object binding. Our approach decomposes complex prompts into sub-prompts, generates corresponding images, and computes visual prototypes that fuse with text embeddings to enhance representation. By applying segmentation-based localization training, we address cross-attention misalignment, achieving improved accuracy in binding multiple attributes to objects. Our approaches outperform existing compositional text-to-image diffusion models on the benchmark T2I CompBench, achieving better image quality, evaluated by humans, and emerging robustness under scaling number of binding pairs in the prompt.</li>
</ul>

<h3>Title: CoCoAFusE: Beyond Mixtures of Experts via Model Fusion</h3>
<ul>
<li><strong>Authors: </strong>Aurelio Raffa Ugolini, Mara Tanelli, Valentina Breschi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01105">https://arxiv.org/abs/2505.01105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01105">https://arxiv.org/pdf/2505.01105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01105]] CoCoAFusE: Beyond Mixtures of Experts via Model Fusion(https://arxiv.org/abs/2505.01105)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Many learning problems involve multiple patterns and varying degrees of uncertainty dependent on the covariates. Advances in Deep Learning (DL) have addressed these issues by learning highly nonlinear input-output dependencies. However, model interpretability and Uncertainty Quantification (UQ) have often straggled behind. In this context, we introduce the Competitive/Collaborative Fusion of Experts (CoCoAFusE), a novel, Bayesian Covariates-Dependent Modeling technique. CoCoAFusE builds on the very philosophy behind Mixtures of Experts (MoEs), blending predictions from several simple sub-models (or "experts") to achieve high levels of expressiveness while retaining a substantial degree of local interpretability. Our formulation extends that of a classical Mixture of Experts by contemplating the fusion of the experts' distributions in addition to their more usual mixing (i.e., superimposition). Through this additional feature, CoCoAFusE better accommodates different scenarios for the intermediate behavior between generating mechanisms, resulting in tighter credible bounds on the response variable. Indeed, only resorting to mixing, as in classical MoEs, may lead to multimodality artifacts, especially over smooth transitions. Instead, CoCoAFusE can avoid these artifacts even under the same structure and priors for the experts, leading to greater expressiveness and flexibility in modeling. This new approach is showcased extensively on a suite of motivating numerical examples and a collection of real-data ones, demonstrating its efficacy in tackling complex regression problems where uncertainty is a key quantity of interest.</li>
</ul>

<h3>Title: Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study</h3>
<ul>
<li><strong>Authors: </strong>Ali Mammadov, Loic Le Folgoc, Julien Adam, Anne Buronfosse, Gilles Hayem, Guillaume Hocquet, Pietro Gori</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01109">https://arxiv.org/abs/2505.01109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01109">https://arxiv.org/pdf/2505.01109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01109]] Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study(https://arxiv.org/abs/2505.01109)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multiple Instance Learning (MIL) has emerged as the best solution for Whole Slide Image (WSI) classification. It consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. MIL includes two main approaches: instance-based and embedding-based. In the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. In the latter, bag classification is performed after aggregating patch embeddings. Even if instance-based methods are naturally more interpretable, embedding-based MILs have usually been preferred in the past due to their robustness to poor feature extractors. However, recently, the quality of feature embeddings has drastically increased using self-supervised learning (SSL). Nevertheless, many authors continue to endorse the superiority of embedding-based MIL. To investigate this further, we conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6 self-supervised methods with 4 backbones, 4 foundation models, and various pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL methods never used before in the pathology domain. Through these extensive experiments, we show that with a good SSL feature extractor, simple instance-based MILs, with very few parameters, obtain similar or better performance than complex, state-of-the-art (SOTA) embedding-based MIL methods, setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple instance-based MIL methods are naturally more interpretable and explainable to clinicians, our results suggest that more effort should be put into well-adapted SSL methods for WSI rather than into complex embedding-based MIL methods.</li>
</ul>

<h3>Title: MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Murtadha Ahmed, Wenbo, Liu yunfeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01110">https://arxiv.org/abs/2505.01110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01110">https://arxiv.org/pdf/2505.01110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01110]] MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning(https://arxiv.org/abs/2505.01110)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in In-Context Learning (ICL). However, the fixed position length constraints in pre-trained models limit the number of demonstration examples. Recent efforts to extend context suffer from attention dispersion as the number of demonstrations increases. In this paper, we introduce Mitigating Attention Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective self-attention as the context size grows. We first split the context into multiple windows, each filled to the model's context capacity, which are processed separately. Then, we introduce an additional layer to recalibrate the attention weights, prioritizing the query tokens as the number of demonstrations increases. Our empirical results show that MateICL can effectively leverage larger contexts to improve ICL performance. Compared to retrieval-based baselines, MateICL consistently achieves better performance without requiring an externally trained retrieval model. Despite recent advances in inference strategies (e.g., 32k token contexts), our results demonstrate that MateICL remains beneficial in computationally resource-constrained settings. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Incorporating Inductive Biases to Energy-based Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yukun Li, Li-Ping Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01111">https://arxiv.org/abs/2505.01111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01111">https://arxiv.org/pdf/2505.01111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01111]] Incorporating Inductive Biases to Energy-based Generative Models(https://arxiv.org/abs/2505.01111)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the advent of score-matching techniques for model training and Langevin dynamics for sample generation, energy-based models (EBMs) have gained renewed interest as generative models. Recent EBMs usually use neural networks to define their energy functions. In this work, we introduce a novel hybrid approach that combines an EBM with an exponential family model to incorporate inductive bias into data modeling. Specifically, we augment the energy term with a parameter-free statistic function to help the model capture key data statistics. Like an exponential family model, the hybrid model aims to align the distribution statistics with data statistics during model training, even when it only approximately maximizes the data likelihood. This property enables us to impose constraints on the hybrid model. Our empirical study validates the hybrid model's ability to match statistics. Furthermore, experimental results show that data fitting and generation improve when suitable informative statistics are incorporated into the hybrid model.</li>
</ul>

<h3>Title: Risk Analysis and Design Against Adversarial Actions</h3>
<ul>
<li><strong>Authors: </strong>Marco C. Campi, Algo Carè, Luis G. Crespo, Simone Garatti, Federico A. Ramponi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01130">https://arxiv.org/abs/2505.01130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01130">https://arxiv.org/pdf/2505.01130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01130]] Risk Analysis and Design Against Adversarial Actions(https://arxiv.org/abs/2505.01130)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Learning models capable of providing reliable predictions in the face of adversarial actions has become a central focus of the machine learning community in recent years. This challenge arises from observing that data encountered at deployment time often deviate from the conditions under which the model was trained. In this paper, we address deployment-time adversarial actions and propose a versatile, well-principled framework to evaluate the model's robustness against attacks of diverse types and intensities. While we initially focus on Support Vector Regression (SVR), the proposed approach extends naturally to the broad domain of learning via relaxed optimization techniques. Our results enable an assessment of the model vulnerability without requiring additional test data and operate in a distribution-free setup. These results not only provide a tool to enhance trust in the model's applicability but also aid in selecting among competing alternatives. Later in the paper, we show that our findings also offer useful insights for establishing new results within the out-of-distribution framework.</li>
</ul>

<h3>Title: Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Rogelio A Mancisidor, Robert Jenssen, Shujian Yu, Michael Kampffmeyer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01134">https://arxiv.org/abs/2505.01134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01134">https://arxiv.org/pdf/2505.01134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01134]] Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders(https://arxiv.org/abs/2505.01134)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal learning with variational autoencoders (VAEs) requires estimating joint distributions to evaluate the evidence lower bound (ELBO). Current methods, the product and mixture of experts, aggregate single-modality distributions assuming independence for simplicity, which is an overoptimistic assumption. This research introduces a novel methodology for aggregating single-modality distributions by exploiting the principle of consensus of dependent experts (CoDE), which circumvents the aforementioned assumption. Utilizing the CoDE method, we propose a novel ELBO that approximates the joint likelihood of the multimodal data by learning the contribution of each subset of modalities. The resulting CoDE-VAE model demonstrates better performance in terms of balancing the trade-off between generative coherence and generative quality, as well as generating more precise log-likelihood estimations. CoDE-VAE further minimizes the generative quality gap as the number of modalities increases. In certain cases, it reaches a generative quality similar to that of unimodal VAEs, which is a desirable property that is lacking in most current methods. Finally, the classification accuracy achieved by CoDE-VAE is comparable to that of state-of-the-art multimodal VAE models.</li>
</ul>

<h3>Title: Active Sybil Attack and Efficient Defense Strategy in IPFS DHT</h3>
<ul>
<li><strong>Authors: </strong>V. H. M. Netto, T. Cholez, C. L. Ignat</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01139">https://arxiv.org/abs/2505.01139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01139">https://arxiv.org/pdf/2505.01139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01139]] Active Sybil Attack and Efficient Defense Strategy in IPFS DHT(https://arxiv.org/abs/2505.01139)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P) storage that relies on Kademlia, a Distributed Hash Table (DHT) structure commonly used in P2P systems for its proved scalability. However, DHTs are known to be vulnerable to Sybil attacks, in which a single entity controls multiple malicious nodes. Recent studies have shown that IPFS is affected by a passive content eclipse attack, leveraging Sybils, in which adversarial nodes hide received indexed information from other peers, making the content appear unavailable. Fortunately, the latest mitigation strategy coupling an attack detection based on statistical tests and a wider publication strategy upon detection was able to circumvent it. In this work, we present a new active attack, with malicious nodes responding with semantically correct but intentionally false data, exploiting both an optimized placement of Sybils to stay below the detection threshold and an early trigger of the content discovery termination in Kubo, the main IPFS implementation. Our attack achieves to completely eclipse content on the latest Kubo release. When evaluated against the most recent known mitigation, it successfully denies access to the target content in approximately 80\% of lookup attempts. To address this vulnerability, we propose a new mitigation called SR-DHT-Store, which enables efficient, Sybil-resistant content publication without relying on attack detection but instead on a systematic and precise use of region-based queries, defined by a dynamically computed XOR distance to the target ID. SR-DHT-Store can be combined with other defense mechanisms resulting in a defense strategy that completely mitigates both passive and active Sybil attacks at a lower overhead, while allowing an incremental deployment.</li>
</ul>

<h3>Title: On the Limitations of Steering in Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Chebrolu Niranjan, Kokil Jaidka, Gerard Christopher Yeo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01162">https://arxiv.org/abs/2505.01162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01162">https://arxiv.org/pdf/2505.01162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01162]] On the Limitations of Steering in Language Model Alignment(https://arxiv.org/abs/2505.01162)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Steering vectors are a promising approach to aligning language model behavior at inference time. In this paper, we propose a framework to assess the limitations of steering vectors as alignment mechanisms. Using a framework of transformer hook interventions and antonym-based function vectors, we evaluate the role of prompt structure and context complexity in steering effectiveness. Our findings indicate that steering vectors are promising for specific alignment tasks, such as value alignment, but may not provide a robust foundation for general-purpose alignment in LLMs, particularly in complex scenarios. We establish a methodological foundation for future investigations into steering capabilities of reasoning models.</li>
</ul>

<h3>Title: Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series</h3>
<ul>
<li><strong>Authors: </strong>Thanh Son Nguyen, Dang Minh Duc Nguyen, Van Thanh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01163">https://arxiv.org/abs/2505.01163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01163">https://arxiv.org/pdf/2505.01163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01163]] Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series(https://arxiv.org/abs/2505.01163)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Accurate time series forecasting is essential in many real-time applications that demand both high predictive accuracy and computational efficiency. This study provides an empirical comparison between a Polynomial Classifier and a Radial Basis Function Neural Network (RBFNN) across four real-world time series datasets (weather conditions, gold prices, crude oil prices, and beer production volumes) that cover both seasonal and nonseasonal patterns. Model performance is evaluated by forecasting accuracy (using Mean Absolute Error, Root Mean Squared Error, and Coefficient of Variation of Root Mean Squared Error) and computational time to assess each model's viability for real time forecasting. The results show that the PC yields more accurate and faster forecasts for non seasonal series, whereas the RBFNN performs better on series with pronounced seasonal patterns. From an interpretability standpoint, the polynomial model offers a simpler, more transparent structure (in contrast to the black box nature of neural network), which is advantageous for understanding and trust in real time decision making. The performance differences between PC and RBFNN are statistically significant, as confirmed by paired t tests and Wilcoxon signed rank tests. These findings provide practical guidance for model selection in time series forecasting, indicating that PC may be preferable for quick, interpretable forecasts in non-seasonal contexts, whereas RBFNN is superior for capturing complex seasonal behaviors</li>
</ul>

<h3>Title: Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Ma, Zhihao Wu, Wang Lu, Xin Gao, Jinghang Yue, Taolin Zhang, Lipo Wang, Youfang Lin, Jing Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01168">https://arxiv.org/abs/2505.01168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01168">https://arxiv.org/pdf/2505.01168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01168]] Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability(https://arxiv.org/abs/2505.01168)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The development of model ensemble attacks has significantly improved the transferability of adversarial examples, but this progress also poses severe threats to the security of deep neural networks. Existing methods, however, face two critical challenges: insufficient capture of shared gradient directions across models and a lack of adaptive weight allocation mechanisms. To address these issues, we propose a novel method Harmonized Ensemble for Adversarial Transferability (HEAT), which introduces domain generalization into adversarial example generation for the first time. HEAT consists of two key modules: Consensus Gradient Direction Synthesizer, which uses Singular Value Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight Orchestrator which dynamically balances intra-domain coherence, stabilizing gradients within individual models, and inter-domain diversity, enhancing transferability across models. Experimental results demonstrate that HEAT significantly outperforms existing methods across various datasets and settings, offering a new perspective and direction for adversarial attack research.</li>
</ul>

<h3>Title: FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiangtong Tan, Hu Yu, Jie Huang, Jie Xiao, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01172">https://arxiv.org/abs/2505.01172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01172">https://arxiv.org/pdf/2505.01172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01172]] FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis(https://arxiv.org/abs/2505.01172)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Long video generation involves generating extended videos using models trained on short videos, suffering from distribution shifts due to varying frame counts. It necessitates the use of local information from the original short frames to enhance visual and motion quality, and global information from the entire long frames to ensure appearance consistency. Existing training-free methods struggle to effectively integrate the benefits of both, as appearance and motion in videos are closely coupled, leading to motion inconsistency and visual quality. In this paper, we reveal that global and local information can be precisely decoupled into consistent appearance and motion intensity information by applying Principal Component Analysis (PCA), allowing for refined complementary integration of global consistency and local quality. With this insight, we propose FreePCA, a training-free long video generation paradigm based on PCA that simultaneously achieves high consistency and quality. Concretely, we decouple consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. Critically, we progressively integrate these features to preserve original quality and ensure smooth transitions, while further enhancing consistency by reusing the mean statistics of the initial noise. Experiments demonstrate that FreePCA can be applied to various video diffusion models without requiring training, leading to substantial improvements. Code is available at this https URL.</li>
</ul>

<h3>Title: LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures</h3>
<ul>
<li><strong>Authors: </strong>Francisco Aguilera-Martínez, Fernando Berzal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01177">https://arxiv.org/abs/2505.01177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01177">https://arxiv.org/pdf/2505.01177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01177]] LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures(https://arxiv.org/abs/2505.01177)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to evolve, it is critical to assess the security threats and vulnerabilities that may arise both during their training phase and after models have been deployed. This survey seeks to define and categorize the various attacks targeting LLMs, distinguishing between those that occur during the training phase and those that affect already trained models. A thorough analysis of these attacks is presented, alongside an exploration of defense mechanisms designed to mitigate such threats. Defenses are classified into two primary categories: prevention-based and detection-based defenses. Furthermore, our survey summarizes possible attacks and their corresponding defense strategies. It also provides an evaluation of the effectiveness of the known defense mechanisms for the different security threats. Our survey aims to offer a structured framework for securing LLMs, while also identifying areas that require further research to improve and strengthen defenses against emerging security challenges.</li>
</ul>

<h3>Title: Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks</h3>
<ul>
<li><strong>Authors: </strong>M. Saeid HaghighiFard, Sinem Coleri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01186">https://arxiv.org/abs/2505.01186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01186">https://arxiv.org/pdf/2505.01186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01186]] Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks(https://arxiv.org/abs/2505.01186)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Hierarchical Federated Learning (HFL) has recently emerged as a promising solution for intelligent decision-making in vehicular networks, helping to address challenges such as limited communication resources, high vehicle mobility, and data heterogeneity. However, HFL remains vulnerable to adversarial and unreliable vehicles, whose misleading updates can significantly compromise the integrity and convergence of the global model. To address these challenges, we propose a novel defense framework that integrates dynamic vehicle selection with robust anomaly detection within a cluster-based HFL architecture, specifically designed to counter Gaussian noise and gradient ascent attacks. The framework performs a comprehensive reliability assessment for each vehicle by evaluating historical accuracy, contribution frequency, and anomaly records. Anomaly detection combines Z-score and cosine similarity analyses on model updates to identify both statistical outliers and directional deviations in model updates. To further refine detection, an adaptive thresholding mechanism is incorporated into the cosine similarity metric, dynamically adjusting the threshold based on the historical accuracy of each vehicle to enforce stricter standards for consistently high-performing vehicles. In addition, a weighted gradient averaging mechanism is implemented, which assigns higher weights to gradient updates from more trustworthy vehicles. To defend against coordinated attacks, a cross-cluster consistency check is applied to identify collaborative attacks in which multiple compromised clusters coordinate misleading updates. Together, these mechanisms form a multi-level defense strategy to filter out malicious contributions effectively. Simulation results show that the proposed algorithm significantly reduces convergence time compared to benchmark methods across both 1-hop and 3-hop topologies.</li>
</ul>

<h3>Title: A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Najmus Sakib Sizan, Md. Abu Layek, Khondokar Fida Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01196">https://arxiv.org/abs/2505.01196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01196">https://arxiv.org/pdf/2505.01196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01196]] A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture(https://arxiv.org/abs/2505.01196)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>To improve crop forecasting and provide farmers with actionable data-driven insights, we propose a novel approach integrating IoT, machine learning, and blockchain technologies. Using IoT, real-time data from sensor networks continuously monitor environmental conditions and soil nutrient levels, significantly improving our understanding of crop growth dynamics. Our study demonstrates the exceptional accuracy of the Random Forest model, achieving a 99.45\% accuracy rate in predicting optimal crop types and yields, thereby offering precise crop projections and customized recommendations. To ensure the security and integrity of the sensor data used for these forecasts, we integrate the Ethereum blockchain, which provides a robust and secure platform. This ensures that the forecasted data remain tamper-proof and reliable. Stakeholders can access real-time and historical crop projections through an intuitive online interface, enhancing transparency and facilitating informed decision-making. By presenting multiple predicted crop scenarios, our system enables farmers to optimize production strategies effectively. This integrated approach promises significant advances in precision agriculture, making crop forecasting more accurate, secure, and user-friendly.</li>
</ul>

<h3>Title: Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Dhaini, Ege Erdogan, Nils Feldhus, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01198">https://arxiv.org/abs/2505.01198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01198">https://arxiv.org/pdf/2505.01198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01198]] Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods(https://arxiv.org/abs/2505.01198)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, explainability</a></li>
<li><strong>Abstract: </strong>While research on applications and evaluations of explanation methods continues to expand, fairness of the explanation methods concerning disparities in their performance across subgroups remains an often overlooked aspect. In this paper, we address this gap by showing that, across three tasks and five language models, widely used post-hoc feature attribution methods exhibit significant gender disparity with respect to their faithfulness, robustness, and complexity. These disparities persist even when the models are pre-trained or fine-tuned on particularly unbiased datasets, indicating that the disparities we observe are not merely consequences of biased training data. Our results highlight the importance of addressing disparities in explanations when developing and applying explainability methods, as these can lead to biased outcomes against certain subgroups, with particularly critical implications in high-stakes contexts. Furthermore, our findings underscore the importance of incorporating the fairness of explanations, alongside overall model fairness and explainability, as a requirement in regulatory frameworks.</li>
</ul>

<h3>Title: CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Tsai-Ning Wang, Lin-Lin Chen, Neil Zeghidour, Aaqib Saeed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01199">https://arxiv.org/abs/2505.01199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01199">https://arxiv.org/pdf/2505.01199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01199]] CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning(https://arxiv.org/abs/2505.01199)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical audio signals, such as heart and lung sounds, play a crucial role in clinical diagnosis. However, analyzing these signals remains challenging: traditional methods rely on handcrafted features or supervised deep learning models that demand extensive labeled datasets, limiting their scalability and applicability. To address these issues, we propose CaReAQA, an audio-language model that integrates a foundation audio model with the reasoning capabilities of large language models, enabling clinically relevant, open-ended diagnostic responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of annotated medical audio recordings enriched with metadata and paired question-answer examples, intended to drive progress in diagnostic reasoning research. Evaluation results show that CaReAQA achieves 86.2% accuracy on open-ended diagnostic reasoning tasks, outperforming baseline models. It also generalizes well to closed-ended classification tasks, achieving an average accuracy of 56.9% on unseen datasets. Our findings show how audio-language integration and reasoning advances medical diagnostics, enabling efficient AI systems for clinical decision support.</li>
</ul>

<h3>Title: T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Xian, Weiqin Jiao, Hao Cheng, Berend Jan van der Zwaag, Yanqiu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01207">https://arxiv.org/abs/2505.01207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01207">https://arxiv.org/pdf/2505.01207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01207]] T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph(https://arxiv.org/abs/2505.01207)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sparse-view camera pose estimation, which aims to estimate the 6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from different viewpoints, is a fundamental yet challenging problem in remote sensing applications. Existing methods often overlook the translation information between each pair of viewpoints, leading to suboptimal performance in sparse-view scenarios. To address this limitation, we introduce T-Graph, a lightweight, plug-and-play module to enhance camera pose estimation in sparse-view settings. T-graph takes paired image features as input and maps them through a Multilayer Perceptron (MLP). It then constructs a fully connected translation graph, where nodes represent cameras and edges encode their translation relationships. It can be seamlessly integrated into existing models as an additional branch in parallel with the original prediction, maintaining efficiency and ease of use. Furthermore, we introduce two pairwise translation representations, relative-t and pair-t, formulated under different local coordinate systems. While relative-t captures intuitive spatial relationships, pair-t offers a rotation-disentangled alternative. The two representations contribute to enhanced adaptability across diverse application scenarios, further improving our module's robustness. Extensive experiments on two state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D and IMC PhotoTourism) validate both the effectiveness and generalizability of T-Graph. The results demonstrate consistent improvements across various metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8 viewpoints.</li>
</ul>

<h3>Title: Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks</h3>
<ul>
<li><strong>Authors: </strong>Akira Tamamori</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01218">https://arxiv.org/abs/2505.01218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01218">https://arxiv.org/pdf/2505.01218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01218]] Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks(https://arxiv.org/abs/2505.01218)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional Hopfield networks, using Hebbian learning, face severe storage capacity limits ($\approx 0.14$ P/N) and spurious attractors. Kernel Logistic Regression (KLR) offers a non-linear approach, mapping patterns to high-dimensional feature spaces for improved separability. Our previous work showed KLR dramatically improves capacity and noise robustness over conventional methods. This paper quantitatively analyzes the attractor structures in KLR-trained networks via extensive simulations. We evaluated recall from diverse initial states across wide storage loads (up to 4.0 P/N) and noise levels. We quantified convergence rates and speed. Our analysis confirms KLR's superior performance: high capacity (up to 4.0 P/N) and robustness. The attractor landscape is remarkably "clean," with near-zero spurious fixed points. Recall failures under high load/noise are primarily due to convergence to other learned patterns, not spurious ones. Dynamics are exceptionally fast (typically 1-2 steps for high-similarity states). This characterization reveals how KLR reshapes dynamics for high-capacity associative memory, highlighting its effectiveness and contributing to AM understanding.</li>
</ul>

<h3>Title: RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Kui Jiang, Yan Luo, Junjun Jiang, Xin Xu, Fei Ma, Fei Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01224">https://arxiv.org/abs/2505.01224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01224">https://arxiv.org/pdf/2505.01224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01224]] RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement(https://arxiv.org/abs/2505.01224)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Underwater image enhancement (UIE) is a critical preprocessing step for marine vision applications, where wavelength-dependent attenuation causes severe content degradation and color distortion. While recent state space models like Mamba show potential for long-range dependency modeling, their unfolding operations and fixed scan paths on 1D sequences fail to adapt to local object semantics and global relation modeling, limiting their efficacy in complex underwater environments. To address this, we enhance conventional Mamba with the sorting-based scanning mechanism that dynamically reorders scanning sequences based on statistical distribution of spatial correlation of all pixels. In this way, it encourages the network to prioritize the most informative components--structural and semantic features. Upon building this mechanism, we devise a Visually Self-adaptive State Block (VSSB) that harmonizes dynamic sorting of Mamba with input-dependent dynamic convolution, enabling coherent integration of global context and local relational cues. This exquisite design helps eliminate global focus bias, especially for widely distributed contents, which greatly weakens the statistical frequency. For robust feature extraction and refinement, we design a cross-feature bridge (CFB) to adaptively fuse multi-scale representations. These efforts compose the novel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive experiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms the state-of-the-art approach WMamba in both quantitative metrics and visual fidelity, averagely achieving 0.55 dB performance gain on the three benchmarks. Our code is available at this https URL</li>
</ul>

<h3>Title: Core-Set Selection for Data-efficient Land Cover Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Keiller Nogueira, Akram Zaytar, Wanli Ma, Ribana Roscher, Ronny Hänsch, Caleb Robinson, Anthony Ortiz, Simone Nsutezo, Rahul Dodhia, Juan M. Lavista Ferres, Oktay Karakuş, Paul L. Rosin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01225">https://arxiv.org/abs/2505.01225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01225">https://arxiv.org/pdf/2505.01225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01225]] Core-Set Selection for Data-efficient Land Cover Segmentation(https://arxiv.org/abs/2505.01225)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The increasing accessibility of remotely sensed data and the potential of such data to inform large-scale decision-making has driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models must be trained on large datasets. However, the common assumption that broadly larger datasets lead to better outcomes tends to overlook the complexities of the data distribution, the potential for introducing biases and noise, and the computational resources required for processing and storing vast datasets. Therefore, effective solutions should consider both the quantity and quality of data. In this paper, we propose six novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets that rely on imagery only, labels only, and a combination of each. We benchmark these approaches against a random-selection baseline on three commonly used land cover classification datasets: DFC2022, Vaihingen, and Potsdam. In each of the datasets, we demonstrate that training on a subset of samples outperforms the random baseline, and some approaches outperform training on all available data. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at this https URL.</li>
</ul>

<h3>Title: EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Dhaini, Kafaite Zahra Hussain, Efstratios Zaradoukas, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01238">https://arxiv.org/abs/2505.01238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01238">https://arxiv.org/pdf/2505.01238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01238]] EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models(https://arxiv.org/abs/2505.01238)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, transformer</a></li>
<li><strong>Abstract: </strong>As Natural Language Processing (NLP) models continue to evolve and become integral to high-stakes applications, ensuring their interpretability remains a critical challenge. Given the growing variety of explainability methods and diverse stakeholder requirements, frameworks that help stakeholders select appropriate explanations tailored to their specific use cases are increasingly important. To address this need, we introduce EvalxNLP, a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models. EvalxNLP integrates eight widely recognized explainability techniques from the Explainable AI (XAI) literature, enabling users to generate and evaluate explanations based on key properties such as faithfulness, plausibility, and complexity. Our framework also provides interactive, LLM-based textual explanations, facilitating user understanding of the generated explanations and evaluation outcomes. Human evaluation results indicate high user satisfaction with EvalxNLP, suggesting it is a promising framework for benchmarking explanation methods across diverse user groups. By offering a user-friendly and extensible platform, EvalxNLP aims at democratizing explainability tools and supporting the systematic comparison and advancement of XAI techniques in NLP.</li>
</ul>

<h3>Title: PHSafe: Disclosure Avoidance for the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC)</h3>
<ul>
<li><strong>Authors: </strong>William Sexton, Skye Berghel, Bayard Carlson, Sam Haney, Luke Hartman, Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Amritha Pai, Simran Rajpal, David Pujol, Ruchit Shrestha, Daniel Simmons-Marengo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01254">https://arxiv.org/abs/2505.01254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01254">https://arxiv.org/pdf/2505.01254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01254]] PHSafe: Disclosure Avoidance for the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC)(https://arxiv.org/abs/2505.01254)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>This article describes the disclosure avoidance algorithm that the U.S. Census Bureau used to protect the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC). The tabulations contain statistics of counts of U.S. persons living in certain types of households, including averages. The article describes the PHSafe algorithm, which is based on adding noise drawn from a discrete Gaussian distribution to the statistics of interest. We prove that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy. We then describe how the algorithm was implemented on Tumult Analytics and briefly outline the parameterization and tuning of the algorithm.</li>
</ul>

<h3>Title: CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Somers, Baptiste Standaert, Victor Joos, Alexandre Alahi, Christophe De Vleeschouwer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01257">https://arxiv.org/abs/2505.01257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01257">https://arxiv.org/pdf/2505.01257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01257]] CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking(https://arxiv.org/abs/2505.01257)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Online multi-object tracking has been recently dominated by tracking-by-detection (TbD) methods, where recent advances rely on increasingly sophisticated heuristics for tracklet representation, feature fusion, and multi-stage matching. The key strength of TbD lies in its modular design, enabling the integration of specialized off-the-shelf models like motion predictors and re-identification. However, the extensive usage of human-crafted rules for temporal associations makes these methods inherently limited in their ability to capture the complex interplay between various tracking cues. In this work, we introduce CAMEL, a novel association module for Context-Aware Multi-Cue ExpLoitation, that learns resilient association strategies directly from data, breaking free from hand-crafted heuristics while maintaining TbD's valuable modularity. At its core, CAMEL employs two transformer-based modules and relies on a novel association-centric training scheme to effectively model the complex interactions between tracked targets and their various association cues. Unlike end-to-end detection-by-tracking approaches, our method remains lightweight and fast to train while being able to leverage external off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack, achieves state-of-the-art performance on multiple tracking benchmarks. Our code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications</h3>
<ul>
<li><strong>Authors: </strong>Elie Saad, Mariem Besbes, Marc Zolghadri, Victor Czmil, Claude Baron, Vincent Bourgeois</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01261">https://arxiv.org/abs/2505.01261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01261">https://arxiv.org/pdf/2505.01261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01261]] Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications(https://arxiv.org/abs/2505.01261)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The challenge of electronic component obsolescence is particularly critical in systems with long life cycles. Various obsolescence management methods are employed to mitigate its impact, with obsolescence forecasting being a highly sought-after and prominent approach. As a result, numerous machine learning-based forecasting methods have been proposed. However, machine learning models require a substantial amount of relevant data to achieve high precision, which is lacking in the current obsolescence landscape in some situations. This work introduces a novel framework for obsolescence forecasting based on deep learning. The proposed framework solves the lack of available data through deep generative modeling, where new obsolescence cases are generated and used to augment the training dataset. The augmented dataset is then used to train a classical machine learning-based obsolescence forecasting model. To train classical forecasting models using augmented datasets, existing classical supervised-learning classifiers are adapted for semi-supervised learning within this framework. The proposed framework demonstrates state-of-the-art results on benchmarking datasets.</li>
</ul>

<h3>Title: Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</h3>
<ul>
<li><strong>Authors: </strong>Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01267">https://arxiv.org/abs/2505.01267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01267">https://arxiv.org/pdf/2505.01267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01267]] Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain(https://arxiv.org/abs/2505.01267)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, diffusion</a></li>
<li><strong>Abstract: </strong>The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.</li>
</ul>

<h3>Title: Anti-adversarial Learning: Desensitizing Prompts for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuan Li, Zhe Yin, Xiaodong Gu, Beijun Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01273">https://arxiv.org/abs/2505.01273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01273">https://arxiv.org/pdf/2505.01273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01273]] Anti-adversarial Learning: Desensitizing Prompts for Large Language Models(https://arxiv.org/abs/2505.01273)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>With the widespread use of LLMs, preserving privacy in user prompts has become crucial, as prompts risk exposing privacy and sensitive data to the cloud LLMs. Traditional techniques like homomorphic encryption, secure multi-party computation, and federated learning face challenges due to heavy computational costs and user participation requirements, limiting their applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel method for desensitizing LLM prompts. The core idea of PromptObfus is "anti-adversarial" learning, which perturbs privacy words in the prompt to obscure sensitive information while retaining the stability of model predictions. Specifically, PromptObfus frames prompt desensitization as a masked language modeling task, replacing privacy-sensitive terms with a [MASK] token. A desensitization model is trained to generate candidate replacements for each masked position. These candidates are subsequently selected based on gradient feedback from a surrogate model, ensuring minimal disruption to the task output. We demonstrate the effectiveness of our approach on three NLP tasks. Results show that PromptObfus effectively prevents privacy inference from remote LLMs while preserving task performance.</li>
</ul>

<h3>Title: MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyan Wang, Xiangchi Song, In-Young Ko</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01279">https://arxiv.org/abs/2505.01279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01279">https://arxiv.org/pdf/2505.01279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01279]] MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting(https://arxiv.org/abs/2505.01279)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Accurate traffic forecasting and swift inference provision are essential for intelligent transportation systems. However, the present Graph Convolutional Network (GCN)-based approaches cannot extract and fuse multi-granular spatiotemporal features across various spatial and temporal scales sufficiently, proven to yield less accurate forecasts. Besides, additional feature extraction branches introduced in prior studies critically increased model complexity and extended inference time, making it challenging to provide fast inference for traffic forecasting. In this paper, we propose MultiGran-STGCNFog, an efficient fog distributed inference system with a novel traffic forecasting model that employs multi-granular spatiotemporal feature fusion on generated dynamic traffic graphs to fully capture interdependent traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer execution order and layer-device scheduling scheme simultaneously, contributes to considerable inference throughput improvement by leveraging heterogeneous fog devices in a pipelined manner. Extensive experiments on real-world datasets demonstrate the superiority of the proposed method over selected baselines.</li>
</ul>

<h3>Title: 2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables</h3>
<ul>
<li><strong>Authors: </strong>Yajuan Zhang, Jiahai Jiang, Yule Yan, Liang Yang, Ping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01286">https://arxiv.org/abs/2505.01286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01286">https://arxiv.org/pdf/2505.01286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01286]] 2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables(https://arxiv.org/abs/2505.01286)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate wind power forecasting can help formulate scientific dispatch plans, which is of great significance for maintaining the safety, stability, and efficient operation of the power system. In recent years, wind power forecasting methods based on deep learning have focused on extracting the spatiotemporal correlations among data, achieving significant improvements in forecasting accuracy. However, they exhibit two limitations. First, there is a lack of modeling for the inter-variable relationships, which limits the accuracy of the forecasts. Second, by treating endogenous and exogenous variables equally, it leads to unnecessary interactions between the endogenous and exogenous variables, increasing the complexity of the model. In this paper, we propose the 2DXformer, which, building upon the previous work's focus on spatiotemporal correlations, addresses the aforementioned two limitations. Specifically, we classify the inputs of the model into three types: exogenous static variables, exogenous dynamic variables, and endogenous variables. First, we embed these variables as variable tokens in a channel-independent manner. Then, we use the attention mechanism to capture the correlations among exogenous variables. Finally, we employ a multi-layer perceptron with residual connections to model the impact of exogenous variables on endogenous variables. Experimental results on two real-world large-scale datasets indicate that our proposed 2DXformer can further improve the performance of wind power forecasting. The code is available in this repository: \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Li, Xuebin Ren, Shusen Yang, Liang Shi, Chia-Mu Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01292">https://arxiv.org/abs/2505.01292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01292">https://arxiv.org/pdf/2505.01292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01292]] Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams(https://arxiv.org/abs/2505.01292)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Local Differential Privacy (LDP) enables massive data collection and analysis while protecting end users' privacy against untrusted aggregators. It has been applied to various data types (e.g., categorical, numerical, and graph data) and application settings (e.g., static and streaming). Recent findings indicate that LDP protocols can be easily disrupted by poisoning or manipulation attacks, which leverage injected/corrupted fake users to send crafted data conforming to the LDP reports. However, current attacks primarily target static protocols, neglecting the security of LDP protocols in the streaming settings. Our research fills the gap by developing novel fine-grained manipulation attacks to LDP protocols for data streams. By reviewing the attack surfaces in existing algorithms, We introduce a unified attack framework with composable modules, which can manipulate the LDP estimated stream toward a target stream. Our attack framework can adapt to state-of-the-art streaming LDP algorithms with different analytic tasks (e.g., frequency and mean) and LDP models (event-level, user-level, w-event level). We validate our attacks theoretically and through extensive experiments on real-world datasets, and finally explore a possible defense mechanism for mitigating these attacks.</li>
</ul>

<h3>Title: A Transformer-based Neural Architecture Search Method</h3>
<ul>
<li><strong>Authors: </strong>Shang Wang, Huanrong Tang, Jianquan Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01314">https://arxiv.org/abs/2505.01314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01314">https://arxiv.org/pdf/2505.01314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01314]] A Transformer-based Neural Architecture Search Method(https://arxiv.org/abs/2505.01314)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents a neural architecture search method based on Transformer architecture, searching cross multihead attention computation ways for different number of encoder and decoder combinations. In order to search for neural network structures with better translation results, we considered perplexity as an auxiliary evaluation metric for the algorithm in addition to BLEU scores and iteratively improved each individual neural network within the population by a multi-objective genetic algorithm. Experimental results show that the neural network structures searched by the algorithm outperform all the baseline models, and that the introduction of the auxiliary evaluation metric can find better models than considering only the BLEU score as an evaluation metric.</li>
</ul>

<h3>Title: Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System</h3>
<ul>
<li><strong>Authors: </strong>Sheikh Samit Muhaimin, Spyridon Mastorakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01315">https://arxiv.org/abs/2505.01315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01315">https://arxiv.org/pdf/2505.01315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01315]] Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System(https://arxiv.org/abs/2505.01315)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, extraction, large language model</a></li>
<li><strong>Abstract: </strong>The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses.</li>
</ul>

<h3>Title: FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Li, Weijie Wang, Qiang Li, Bruno Lepri, Nicu Sebe, Weizhi Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01322">https://arxiv.org/abs/2505.01322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01322">https://arxiv.org/pdf/2505.01322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01322]] FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors(https://arxiv.org/abs/2505.01322)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-driven object insertion in 3D scenes is an emerging task that enables intuitive scene editing through natural language. However, existing 2D editing-based methods often rely on spatial priors such as 2D masks or 3D bounding boxes, and they struggle to ensure consistency of the inserted object. These limitations hinder flexibility and scalability in real-world applications. In this paper, we propose FreeInsert, a novel framework that leverages foundation models including MLLMs, LGMs, and diffusion models to disentangle object generation from spatial placement. This enables unsupervised and flexible object insertion in 3D scenes without spatial priors. FreeInsert starts with an MLLM-based parser that extracts structured semantics, including object types, spatial relationships, and attachment regions, from user instructions. These semantics guide both the reconstruction of the inserted object for 3D consistency and the learning of its degrees of freedom. We leverage the spatial reasoning capabilities of MLLMs to initialize object pose and scale. A hierarchical, spatially aware refinement stage further integrates spatial semantics and MLLM-inferred priors to enhance placement. Finally, the appearance of the object is improved using the inserted-object image to enhance visual fidelity. Experimental results demonstrate that FreeInsert achieves semantically coherent, spatially precise, and visually realistic 3D insertions without relying on spatial priors, offering a user-friendly and flexible editing experience.</li>
</ul>

<h3>Title: Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability</h3>
<ul>
<li><strong>Authors: </strong>Anass Grini, Oumaima Taheri, Btissam El Khamlichi, Amal El Fallah-Seghrouchni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01328">https://arxiv.org/abs/2505.01328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01328">https://arxiv.org/pdf/2505.01328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01328]] Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability(https://arxiv.org/abs/2505.01328)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>While machine learning has significantly advanced Network Intrusion Detection Systems (NIDS), particularly within IoT environments where devices generate large volumes of data and are increasingly susceptible to cyber threats, these models remain vulnerable to adversarial attacks. Our research reveals a critical flaw in existing adversarial attack methodologies: the frequent violation of domain-specific constraints, such as numerical and categorical limits, inherent to IoT and network traffic. This leads to up to 80.3% of adversarial examples being invalid, significantly overstating real-world vulnerabilities. These invalid examples, though effective in fooling models, do not represent feasible attacks within practical IoT deployments. Consequently, relying on these results can mislead resource allocation for defense, inflating the perceived susceptibility of IoT-enabled NIDS models to adversarial manipulation. Furthermore, we demonstrate that simpler surrogate models like Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate, we analyze the transferability of adversarial severity to other ML/DL models commonly used in IoT contexts. This work underscores the importance of considering both domain constraints and model architecture when evaluating and designing robust ML/DL models for security-critical IoT and network applications.</li>
</ul>

<h3>Title: Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation</h3>
<ul>
<li><strong>Authors: </strong>Hwanwoo Kim, Panos Toulis, Eric Laber</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01361">https://arxiv.org/abs/2505.01361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01361">https://arxiv.org/pdf/2505.01361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01361]] Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation(https://arxiv.org/abs/2505.01361)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Temporal Difference (TD) learning is a foundational algorithm in reinforcement learning (RL). For nearly forty years, TD learning has served as a workhorse for applied RL as well as a building block for more complex and specialized algorithms. However, despite its widespread use, it is not without drawbacks, the most prominent being its sensitivity to step size. A poor choice of step size can dramatically inflate the error of value estimates and slow convergence. Consequently, in practice, researchers must use trial and error in order to identify a suitable step size -- a process that can be tedious and time consuming. As an alternative, we propose implicit TD algorithms that reformulate TD updates into fixed-point equations. These updates are more stable and less sensitive to step size without sacrificing computational efficiency. Moreover, our theoretical analysis establishes asymptotic convergence guarantees and finite-time error bounds. Our results demonstrate their robustness and practicality for modern RL tasks, establishing implicit TD as a versatile tool for policy evaluation and value approximation.</li>
</ul>

<h3>Title: Monitoring morphometric drift in lifelong learning segmentation of the spinal cord</h3>
<ul>
<li><strong>Authors: </strong>Enamundram Naga Karthik, Sandrine Bédard, Jan Valošek, Christoph S. Aigner, Elise Bannier, Josef Bednařík, Virginie Callot, Anna Combes, Armin Curt, Gergely David, Falk Eippert, Lynn Farner, Michael G Fehlings, Patrick Freund, Tobias Granberg, Cristina Granziera, RHSCIR Network Imaging Group, Ulrike Horn, Tomáš Horák, Suzanne Humphreys, Markus Hupp, Anne Kerbrat, Nawal Kinany, Shannon Kolind, Petr Kudlička, Anna Lebret, Lisa Eunyoung Lee, Caterina Mainero, Allan R. Martin, Megan McGrath, Govind Nair, Kristin P. O'Grady, Jiwon Oh, Russell Ouellette, Nikolai Pfender, Dario Pfyffer, Pierre-François Pradat, Alexandre Prat, Emanuele Pravatà, Daniel S. Reich, Ilaria Ricchi, Naama Rotem-Kohavi, Simon Schading-Sassenhausen, Maryam Seif, Andrew Smith, Seth A Smith, Grace Sweeney, Roger Tam, Anthony Traboulsee, Constantina Andrada Treaba, Charidimos Tsagkas, Zachary Vavasour, Dimitri Van De Ville, Kenneth Arnold Weber II, Sarath Chandar, Julien Cohen-Adad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01364">https://arxiv.org/abs/2505.01364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01364">https://arxiv.org/pdf/2505.01364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01364]] Monitoring morphometric drift in lifelong learning segmentation of the spinal cord(https://arxiv.org/abs/2505.01364)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Morphometric measures derived from spinal cord segmentations can serve as diagnostic and prognostic biomarkers in neurological diseases and injuries affecting the spinal cord. While robust, automatic segmentation methods to a wide variety of contrasts and pathologies have been developed over the past few years, whether their predictions are stable as the model is updated using new datasets has not been assessed. This is particularly important for deriving normative values from healthy participants. In this study, we present a spinal cord segmentation model trained on a multisite $(n=75)$ dataset, including 9 different MRI contrasts and several spinal cord pathologies. We also introduce a lifelong learning framework to automatically monitor the morphometric drift as the model is updated using additional datasets. The framework is triggered by an automatic GitHub Actions workflow every time a new model is created, recording the morphometric values derived from the model's predictions over time. As a real-world application of the proposed framework, we employed the spinal cord segmentation model to update a recently-introduced normative database of healthy participants containing commonly used measures of spinal cord morphometry. Results showed that: (i) our model outperforms previous versions and pathology-specific models on challenging lumbar spinal cord cases, achieving an average Dice score of $0.95 \pm 0.03$; (ii) the automatic workflow for monitoring morphometric drift provides a quick feedback loop for developing future segmentation models; and (iii) the scaling factor required to update the database of morphometric measures is nearly constant among slices across the given vertebral levels, showing minimum drift between the current and previous versions of the model monitored by the framework. The model is freely available in Spinal Cord Toolbox v7.0.</li>
</ul>

<h3>Title: Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii</h3>
<ul>
<li><strong>Authors: </strong>Kola Ayonrinde, Louis Jaburi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01372">https://arxiv.org/abs/2505.01372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01372">https://arxiv.org/pdf/2505.01372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01372]] Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii(https://arxiv.org/abs/2505.01372)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Mechanistic Interpretability (MI) aims to understand neural networks through causal explanations. Though MI has many explanation-generating methods, progress has been limited by the lack of a universal approach to evaluating explanations. Here we analyse the fundamental question "What makes a good explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on four perspectives from the Philosophy of Science - the Bayesian, Kuhnian, Deutschian, and Nomological - to systematically evaluate and improve explanations in MI. We find that Compact Proofs consider many explanatory virtues and are hence a promising approach. Fruitful research directions implied by our framework include (1) clearly defining explanatory simplicity, (2) focusing on unifying explanations and (3) deriving universal principles for neural networks. Improved MI methods enhance our ability to monitor, predict, and steer AI systems.</li>
</ul>

<h3>Title: Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Fahong Zhang, Yilei Shi, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01385">https://arxiv.org/abs/2505.01385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01385">https://arxiv.org/pdf/2505.01385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01385]] Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing(https://arxiv.org/abs/2505.01385)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of mapping polygonal buildings from remote sensing images and introduces a novel algorithm, the Global Collinearity-aware Polygonizer (GCP). GCP, built upon an instance segmentation framework, processes binary masks produced by any instance segmentation model. The algorithm begins by collecting polylines sampled along the contours of the binary masks. These polylines undergo a refinement process using a transformer-based regression module to ensure they accurately fit the contours of the targeted building instances. Subsequently, a collinearity-aware polygon simplification module simplifies these refined polylines and generate the final polygon representation. This module employs dynamic programming technique to optimize an objective function that balances the simplicity and fidelity of the polygons, achieving globally optimal solutions. Furthermore, the optimized collinearity-aware objective is seamlessly integrated into network training, enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has been validated on two public benchmarks for polygonal building mapping. Further experiments reveal that applying the collinearity-aware polygon simplification module to arbitrary polylines, without prior knowledge, enhances accuracy over traditional methods such as the Douglas-Peucker algorithm. This finding underscores the broad applicability of GCP. The code for the proposed method will be made available at this https URL.</li>
</ul>

<h3>Title: Carbon Aware Transformers Through Joint Model-Hardware Optimization</h3>
<ul>
<li><strong>Authors: </strong>Irene Wang, Newsha Ardalani, Mostafa Elhoushi, Daniel Jiang, Samuel Hsia, Ekin Sumbul, Divya Mahajan, Carole-Jean Wu, Bilge Acun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01386">https://arxiv.org/abs/2505.01386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01386">https://arxiv.org/pdf/2505.01386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01386]] Carbon Aware Transformers Through Joint Model-Hardware Optimization(https://arxiv.org/abs/2505.01386)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The rapid growth of machine learning (ML) systems necessitates a more comprehensive evaluation of their environmental impact, particularly their carbon footprint, which comprises operational carbon from training and inference execution and embodied carbon from hardware manufacturing and its entire life-cycle. Despite the increasing importance of embodied emissions, there is a lack of tools and frameworks to holistically quantify and optimize the total carbon footprint of ML systems. To address this, we propose CATransformers, a carbon-aware architecture search framework that enables sustainability-driven co-optimization of ML models and hardware architectures. By incorporating both operational and embodied carbon metrics into early design space exploration of domain-specific hardware accelerators, CATransformers demonstrates that optimizing for carbon yields design choices distinct from those optimized solely for latency or energy efficiency. We apply our framework to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models achieving up to 17% reduction in total carbon emissions while maintaining accuracy and latency compared to state-of-the-art edge small CLIP baselines. This work underscores the need for holistic optimization methods to design high-performance, environmentally sustainable AI systems.</li>
</ul>

<h3>Title: Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer</h3>
<ul>
<li><strong>Authors: </strong>Alice Natalina Caragliano, Claudia Tacconi, Carlo Greco, Lorenzo Nibid, Edy Ippolito, Michele Fiore, Giuseppe Perrone, Sara Ramella, Paolo Soda, Valerio Guarrasi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01390">https://arxiv.org/abs/2505.01390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01390">https://arxiv.org/pdf/2505.01390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01390]] Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer(https://arxiv.org/abs/2505.01390)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>This study proposes a novel approach combining Multimodal Deep Learning with intrinsic eXplainable Artificial Intelligence techniques to predict pathological response in non-small cell lung cancer patients undergoing neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal deep learning approaches, we introduce an intermediate fusion strategy that integrates imaging and clinical data, enabling efficient interaction between data modalities. The proposed Multimodal Doctor-in-the-Loop method further enhances clinical relevance by embedding clinicians' domain knowledge directly into the training process, guiding the model's focus gradually from broader lung regions to specific lesions. Results demonstrate improved predictive accuracy and explainability, providing insights into optimal data integration strategies for clinical applications.</li>
</ul>

<h3>Title: VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Teymoorianfard, Shiqing Ma, Amir Houmansadr</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01406">https://arxiv.org/abs/2505.01406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01406">https://arxiv.org/pdf/2505.01406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01406]] VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models(https://arxiv.org/abs/2505.01406)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>The rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. Existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. In this work, we introduce VIDSTAMP, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. By fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal perceptual impact. Leveraging architectural components such as 3D convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. VIDSTAMP embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log P-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. Code: Code: \url{this https URL}</li>
</ul>

<h3>Title: Evaluating Frontier Models for Stealth and Situational Awareness</h3>
<ul>
<li><strong>Authors: </strong>Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis Ho, Rohin Shah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01420">https://arxiv.org/abs/2505.01420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01420">https://arxiv.org/pdf/2505.01420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01420]] Evaluating Frontier Models for Stealth and Situational Awareness(https://arxiv.org/abs/2505.01420)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal</a></li>
<li><strong>Abstract: </strong>Recent work has demonstrated the plausibility of frontier AI models scheming -- knowingly and covertly pursuing an objective misaligned with its developer's intentions. Such behavior could be very hard to detect, and if present in future advanced systems, could pose severe loss of control risk. It is therefore important for AI developers to rule out harm from scheming prior to model deployment. In this paper, we present a suite of scheming reasoning evaluations measuring two types of reasoning capabilities that we believe are prerequisites for successful scheming: First, we propose five evaluations of ability to reason about and circumvent oversight (stealth). Second, we present eleven evaluations for measuring a model's ability to instrumentally reason about itself, its environment and its deployment (situational awareness). We demonstrate how these evaluations can be used as part of a scheming inability safety case: a model that does not succeed on these evaluations is almost certainly incapable of causing severe harm via scheming in real deployment. We run our evaluations on current frontier models and find that none of them show concerning levels of either situational awareness or stealth.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
