<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-07</h1>
<h3>Title: Teaching Language Models to Critique via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhihui Xie, Jie chen, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03492">https://arxiv.org/abs/2502.03492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03492">https://arxiv.org/pdf/2502.03492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03492]] Teaching Language Models to Critique via Reinforcement Learning(https://arxiv.org/abs/2502.03492)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose $\texttt{CTRL}$, a framework for $\texttt{C}$ritic $\texttt{T}$raining via $\texttt{R}$einforcement $\texttt{L}$earning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with $\texttt{CTRL}$ significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.</li>
</ul>

<h3>Title: Path Planning for Masked Diffusion Model Sampling</h3>
<ul>
<li><strong>Authors: </strong>Fred Zhangzhi Peng, Zachary Bezemek, Sawan Patel, Sherwood Yao, Jarrid Rector-Brooks, Alexander Tong, Pranam Chatterjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03540">https://arxiv.org/abs/2502.03540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03540">https://arxiv.org/pdf/2502.03540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03540]] Path Planning for Masked Diffusion Model Sampling(https://arxiv.org/abs/2502.03540)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate how the order in which tokens are unmasked during masked diffusion models (MDMs) inference affects generative quality. We derive an expanded evidence lower bound (ELBO) that introduces a planner, responsible for selecting which tokens to unmask at each step. Our analysis suggests that alternative unmasking strategies can improve generative performance. Based on these insights, we propose Path Planning (P2), a sampling framework that leverages pre-trained BERT or the denoiser itself to guide unmasking decisions. P2 generalizes all known MDM sampling strategies and enables significant improvements across diverse domains including language generation (in-context learning, code generation, story infilling, mathematical reasoning, reverse curse correction) and biological sequence generation (protein and RNA sequences).</li>
</ul>

<h3>Title: Kronecker Mask and Interpretive Prompts are Language-Action Video Learners</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03549">https://arxiv.org/abs/2502.03549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03549">https://arxiv.org/pdf/2502.03549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03549]] Kronecker Mask and Interpretive Prompts are Language-Action Video Learners(https://arxiv.org/abs/2502.03549)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Contrastive language-image pretraining (CLIP) has significantly advanced image-based vision learning. A pressing topic subsequently arises: how can we effectively adapt CLIP to the video domain? Recent studies have focused on adjusting either the textual or visual branch of CLIP for action recognition. However, we argue that adaptations of both branches are crucial. In this paper, we propose \textbf{CLAVER}: a \textbf{C}ontrastive \textbf{L}anguage-\textbf{A}ction \textbf{V}ideo Learn\textbf{er}, designed to shift CLIP's focus from the alignment of static visual objects and concrete nouns to the alignment of dynamic action behaviors and abstract verbs. Specifically, we introduce a novel Kronecker mask attention for temporal modeling. Our tailored Kronecker mask offers three benefits 1) it expands the temporal receptive field for each token, 2) it serves as an effective spatiotemporal heterogeneity inductive bias, mitigating the issue of spatiotemporal homogenization, and 3) it can be seamlessly plugged into transformer-based models. Regarding the textual branch, we leverage large language models to generate diverse, sentence-level and semantically rich interpretive prompts of actions, which shift the model's focus towards the verb comprehension. Extensive experiments on various benchmarks and learning scenarios demonstrate the superiority and generality of our approach. The code will be available soon.</li>
</ul>

<h3>Title: Efficient Global Neural Architecture Search</h3>
<ul>
<li><strong>Authors: </strong>Shahid Siddiqui, Christos Kyrkou, Theocharis Theocharides</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03553">https://arxiv.org/abs/2502.03553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03553">https://arxiv.org/pdf/2502.03553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03553]] Efficient Global Neural Architecture Search(https://arxiv.org/abs/2502.03553)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Neural architecture search (NAS) has shown promise towards automating neural network design for a given task, but it is computationally demanding due to training costs associated with evaluating a large number of architectures to find the optimal one. To speed up NAS, recent works limit the search to network building blocks (modular search) instead of searching the entire architecture (global search), approximate candidates' performance evaluation in lieu of complete training, and use gradient descent rather than naturally suitable discrete optimization approaches. However, modular search does not determine network's macro architecture i.e. depth and width, demanding manual trial and error post-search, hence lacking automation. In this work, we revisit NAS and design a navigable, yet architecturally diverse, macro-micro search space. In addition, to determine relative rankings of candidates, existing methods employ consistent approximations across entire search spaces, whereas different networks may not be fairly comparable under one training protocol. Hence, we propose an architecture-aware approximation with variable training schemes for different networks. Moreover, we develop an efficient search strategy by disjoining macro-micro network design that yields competitive architectures in terms of both accuracy and size. Our proposed framework achieves a new state-of-the-art on EMNIST and KMNIST, while being highly competitive on the CIFAR-10, CIFAR-100, and FashionMNIST datasets and being 2-4x faster than the fastest global search methods. Lastly, we demonstrate the transferability of our framework to real-world computer vision problems by discovering competitive architectures for face recognition applications.</li>
</ul>

<h3>Title: Code Simulation as a Proxy for High-order Tasks in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, X. Angelo Huang, Samuele Marro, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03568">https://arxiv.org/abs/2502.03568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03568">https://arxiv.org/pdf/2502.03568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03568]] Code Simulation as a Proxy for High-order Tasks in Large Language Models(https://arxiv.org/abs/2502.03568)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many reasoning, planning, and problem-solving tasks share an intrinsic algorithmic nature: correctly simulating each step is a sufficient condition to solve them correctly. We collect pairs of naturalistic and synthetic reasoning tasks to assess the capabilities of Large Language Models (LLM). While naturalistic tasks often require careful human handcrafting, we show that synthetic data is, in many cases, a good proxy that is much easier to collect at scale. We leverage common constructs in programming as the counterpart of the building blocks of naturalistic reasoning tasks, such as straight-line programs, code that contains critical paths, and approximate and redundant instructions. We further assess the capabilities of LLMs on sorting problems and repeated operations via sorting algorithms and nested loops. Our synthetic datasets further reveal that while the most powerful LLMs exhibit relatively strong execution capabilities, the process is fragile: it is negatively affected by memorisation and seems to rely heavily on pattern recognition. Our contribution builds upon synthetically testing the reasoning capabilities of LLMs as a scalable complement to handcrafted human-annotated problems.</li>
</ul>

<h3>Title: Clone-Resistant Weights in Metric Spaces: A Framework for Handling Redundancy Bias</h3>
<ul>
<li><strong>Authors: </strong>Damien Berriaud, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03576">https://arxiv.org/abs/2502.03576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03576">https://arxiv.org/pdf/2502.03576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03576]] Clone-Resistant Weights in Metric Spaces: A Framework for Handling Redundancy Bias(https://arxiv.org/abs/2502.03576)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We are given a set of elements in a metric space. The distribution of the elements is arbitrary, possibly adversarial. Can we weigh the elements in a way that is resistant to such (adversarial) manipulations? This problem arises in various contexts. For instance, the elements could represent data points, requiring robust domain adaptation. Alternatively, they might represent tasks to be aggregated into a benchmark; or questions about personal political opinions in voting advice applications. This article introduces a theoretical framework for dealing with such problems. We propose clone-proof representation functions as a solution concept. These functions distribute importance across elements of a set such that similar objects (``clones'') share (some of) their weights, thus avoiding a potential bias introduced by their multiplicity. Our framework extends the maximum uncertainty principle to accommodate general metric spaces and includes a set of axioms - symmetry, continuity, and clone-proofness - that guide the construction of representation functions. Finally, we address the existence of representation functions satisfying our axioms in the significant case of Euclidean spaces and propose a general method for their construction.</li>
</ul>

<h3>Title: Clinically-Inspired Hierarchical Multi-Label Classification of Chest X-rays with a Penalty-Based Loss Function</h3>
<ul>
<li><strong>Authors: </strong>Mehrdad Asadi, Komi Sodok√©, Ian J. Gerard, Marta Kersten-Oertel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03591">https://arxiv.org/abs/2502.03591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03591">https://arxiv.org/pdf/2502.03591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03591]] Clinically-Inspired Hierarchical Multi-Label Classification of Chest X-rays with a Penalty-Based Loss Function(https://arxiv.org/abs/2502.03591)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In this work, we present a novel approach to multi-label chest X-ray (CXR) image classification that enhances clinical interpretability while maintaining a streamlined, single-model, single-run training pipeline. Leveraging the CheXpert dataset and VisualCheXbert-derived labels, we incorporate hierarchical label groupings to capture clinically meaningful relationships between diagnoses. To achieve this, we designed a custom hierarchical binary cross-entropy (HBCE) loss function that enforces label dependencies using either fixed or data-driven penalty types. Our model achieved a mean area under the receiver operating characteristic curve (AUROC) of 0.903 on the test set. Additionally, we provide visual explanations and uncertainty estimations to further enhance model interpretability. All code, model configurations, and experiment details are made available.</li>
</ul>

<h3>Title: Bilevel ZOFO: Bridging Parameter-Efficient and Zeroth-Order Techniques for Efficient LLM Fine-Tuning and Meta-Training</h3>
<ul>
<li><strong>Authors: </strong>Reza Shirkavand, Qi He, Peiran Yu, Heng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03604">https://arxiv.org/abs/2502.03604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03604">https://arxiv.org/pdf/2502.03604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03604]] Bilevel ZOFO: Bridging Parameter-Efficient and Zeroth-Order Techniques for Efficient LLM Fine-Tuning and Meta-Training(https://arxiv.org/abs/2502.03604)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks using First-Order (FO) optimizers presents significant computational challenges. Parameter-Efficient Fine-Tuning(PEFT) methods have been proposed to address these challenges by freezing most model parameters and training only a small subset. While PEFT is efficient, it may not outperform full fine-tuning when high task-specific performance is required. Zeroth-Order (ZO) methods offer an alternative for fine-tuning the entire pre-trained model by approximating gradients using only the forward pass, thus eliminating the computational burden of back-propagation in first-order methods. However, when implementing ZO methods, a hard prompt is crucial, and relying on simple, fixed hard prompts may not be optimal. In this paper, we propose a bilevel optimization framework that complements ZO methods with PEFT to mitigate sensitivity to hard prompts while efficiently and effectively fine-tuning LLMs. Our Bilevel ZOFO (Zeroth-Order-First-Order) method employs a double-loop optimization strategy, where only the gradient of the PEFT model and the forward pass of the base model are required. We provide convergence guarantees for Bilevel ZOFO. Empirically, we demonstrate that Bilevel ZOFO outperforms both PEFT and ZO methods in single-task settings while maintaining similar memory efficiency. Additionally, we show its strong potential for multitask learning. Compared to current first-order meta-training algorithms for multitask learning, our method has significantly lower computational demands while maintaining or improving performance.</li>
</ul>

<h3>Title: A Novel Zero-Touch, Zero-Trust, AI/ML Enablement Framework for IoT Network Security</h3>
<ul>
<li><strong>Authors: </strong>Sushil Shakya, Robert Abbas, Sasa Maric</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03614">https://arxiv.org/abs/2502.03614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03614">https://arxiv.org/pdf/2502.03614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03614]] A Novel Zero-Touch, Zero-Trust, AI/ML Enablement Framework for IoT Network Security(https://arxiv.org/abs/2502.03614)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>The IoT facilitates a connected, intelligent, and sustainable society; therefore, it is imperative to protect the IoT ecosystem. The IoT-based 5G and 6G will leverage the use of machine learning and artificial intelligence (ML/AI) more to pave the way for autonomous and collaborative secure IoT networks. Zero-touch, zero-trust IoT security with AI and machine learning (ML) enablement frameworks offers a powerful approach to securing the expanding landscape of Internet of Things (IoT) devices. This paper presents a novel framework based on the integration of Zero Trust, Zero Touch, and AI/ML powered for the detection, mitigation, and prevention of DDoS attacks in modern IoT ecosystems. The focus will be on the new integrated framework by establishing zero trust for all IoT traffic, fixed and mobile 5G/6G IoT network traffic, and data security (quarantine-zero touch and dynamic policy enforcement). We perform a comparative analysis of five machine learning models, namely, XGBoost, Random Forest, K-Nearest Neighbors, Stochastic Gradient Descent, and Native Bayes, by comparing these models based on accuracy, precision, recall, F1-score, and ROC-AUC. Results show that the best performance in detecting and mitigating different DDoS vectors comes from the ensemble-based approaches.</li>
</ul>

<h3>Title: The Logical Implication Steering Method for Conditional Interventions on Transformer Generation</h3>
<ul>
<li><strong>Authors: </strong>Damjan Kalajdzievski</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03618">https://arxiv.org/abs/2502.03618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03618">https://arxiv.org/pdf/2502.03618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03618]] The Logical Implication Steering Method for Conditional Interventions on Transformer Generation(https://arxiv.org/abs/2502.03618)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>The field of mechanistic interpretability in pre-trained transformer models has demonstrated substantial evidence supporting the ''linear representation hypothesis'', which is the idea that high level concepts are encoded as vectors in the space of activations of a model. Studies also show that model generation behavior can be steered toward a given concept by adding the concept's vector to the corresponding activations. We show how to leverage these properties to build a form of logical implication into models, enabling transparent and interpretable adjustments that induce a chosen generation behavior in response to the presence of any given concept. Our method, Logical Implication Model Steering (LIMS), unlocks new hand engineered reasoning capabilities by integrating neuro-symbolic logic into pre-trained transformer models.</li>
</ul>

<h3>Title: Swarm Characteristic Classification using Robust Neural Networks with Optimized Controllable Inputs</h3>
<ul>
<li><strong>Authors: </strong>Donald W. Peltier III, Isaac Kaminer, Abram Clark, Marko Orescanin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03619">https://arxiv.org/abs/2502.03619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03619">https://arxiv.org/pdf/2502.03619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03619]] Swarm Characteristic Classification using Robust Neural Networks with Optimized Controllable Inputs(https://arxiv.org/abs/2502.03619)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, robust</a></li>
<li><strong>Abstract: </strong>Having the ability to infer characteristics of autonomous agents would profoundly revolutionize defense, security, and civil applications. Our previous work was the first to demonstrate that supervised neural network time series classification (NN TSC) could rapidly predict the tactics of swarming autonomous agents in military contexts, providing intelligence to inform counter-maneuvers. However, most autonomous interactions, especially military engagements, are fraught with uncertainty, raising questions about the practicality of using a pretrained classifier. This article addresses that challenge by leveraging expected operational variations to construct a richer dataset, resulting in a more robust NN with improved inference performance in scenarios characterized by significant uncertainties. Specifically, diverse datasets are created by simulating variations in defender numbers, defender motions, and measurement noise levels. Key findings indicate that robust NNs trained on an enriched dataset exhibit enhanced classification accuracy and offer operational flexibility, such as reducing resources required and offering adherence to trajectory constraints. Furthermore, we present a new framework for optimally deploying a trained NN by the defenders. The framework involves optimizing defender trajectories that elicit adversary responses that maximize the probability of correct NN tactic classification while also satisfying operational constraints imposed on the defenders.</li>
</ul>

<h3>Title: DynVFX: Augmenting Real Videos with Dynamic Content</h3>
<ul>
<li><strong>Authors: </strong>Danah Yatim, Rafail Fridman, Omer Bar-Tal, Tali Dekel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03621">https://arxiv.org/abs/2502.03621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03621">https://arxiv.org/pdf/2502.03621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03621]] DynVFX: Augmenting Real Videos with Dynamic Content(https://arxiv.org/abs/2502.03621)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene. Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion.</li>
</ul>

<h3>Title: AdaPhish: AI-Powered Adaptive Defense and Education Resource Against Deceptive Emails</h3>
<ul>
<li><strong>Authors: </strong>Rei Meguro, Ng S. T. Chong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03622">https://arxiv.org/abs/2502.03622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03622">https://arxiv.org/pdf/2502.03622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03622]] AdaPhish: AI-Powered Adaptive Defense and Education Resource Against Deceptive Emails(https://arxiv.org/abs/2502.03622)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Phishing attacks remain a significant threat in the digital age, yet organizations lack effective methods to tackle phishing attacks without leaking sensitive information. Phish bowl initiatives are a vital part of cybersecurity efforts against these attacks. However, traditional phish bowls require manual anonymization and are often limited to internal use. To overcome these limitations, we introduce AdaPhish, an AI-powered phish bowl platform that automatically anonymizes and analyzes phishing emails using large language models (LLMs) and vector databases. AdaPhish achieves real-time detection and adaptation to new phishing tactics while enabling long-term tracking of phishing trends. Through automated reporting, adaptive analysis, and real-time alerts, AdaPhish presents a scalable, collaborative solution for phishing detection and cybersecurity education.</li>
</ul>

<h3>Title: Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach</h3>
<ul>
<li><strong>Authors: </strong>Yunuo Chen, Junli Cao, Anil Kag, Vidit Goel, Sergei Korolev, Chenfanfu Jiang, Sergey Tulyakov, Jian Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03639">https://arxiv.org/abs/2502.03639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03639">https://arxiv.org/pdf/2502.03639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03639]] Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach(https://arxiv.org/abs/2502.03639)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, \eg, nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to a lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as task-oriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion.</li>
</ul>

<h3>Title: Context-Preserving Gradient Modulation for Large Language Models: A Novel Approach to Semantic Consistency in Long-Form Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Nirola Kobanov, Edmund Weatherstone, Zachary Vanderpoel, Orlando Wetherby</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03643">https://arxiv.org/abs/2502.03643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03643">https://arxiv.org/pdf/2502.03643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03643]] Context-Preserving Gradient Modulation for Large Language Models: A Novel Approach to Semantic Consistency in Long-Form Text Generation(https://arxiv.org/abs/2502.03643)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Maintaining semantic consistency over extended text sequences remains a fundamental challenge in long-form text generation, where conventional training methodologies often struggle to prevent contextual drift and coherence degradation. A novel gradient modulation approach is introduced, designed to adjust parameter updates dynamically in response to contextual relevance, ensuring that generated text remains aligned with prior discourse. By integrating a modulation function that selectively amplifies or attenuates gradients based on learned contextual dependencies, the proposed method enhances the stability of model-generated narratives without imposing significant computational overhead. Comparative evaluations against baseline models reveal improvements in coherence, contextual retention, and long-range dependency tracking, demonstrating the effectiveness of modifying the learning process at the gradient level. The results indicate that sentence structure variability and lexical diversity benefit from this approach, mitigating repetitive phrasing and improving adaptability across diverse linguistic contexts. Statistical validation of coherence metrics further substantiates the observed enhancements, with a significant reduction in inconsistencies emerging as a direct consequence of the modulation mechanism. Computational efficiency assessments confirm that the framework achieves these gains without requiring substantial modifications to the underlying architecture, ensuring compatibility with existing optimization workflows.</li>
</ul>

<h3>Title: The Cost of Shuffling in Private Gradient Based Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shuli Jiang, Pranay Sharma, Zhiwei Steven Wu, Gauri Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03652">https://arxiv.org/abs/2502.03652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03652">https://arxiv.org/pdf/2502.03652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03652]] The Cost of Shuffling in Private Gradient Based Optimization(https://arxiv.org/abs/2502.03652)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We consider the problem of differentially private (DP) convex empirical risk minimization (ERM). While the standard DP-SGD algorithm is theoretically well-established, practical implementations often rely on shuffled gradient methods that traverse the training data sequentially rather than sampling with replacement in each iteration. Despite their widespread use, the theoretical privacy-accuracy trade-offs of private shuffled gradient methods (\textit{DP-ShuffleG}) remain poorly understood, leading to a gap between theory and practice. In this work, we leverage privacy amplification by iteration (PABI) and a novel application of Stein's lemma to provide the first empirical excess risk bound of \textit{DP-ShuffleG}. Our result shows that data shuffling results in worse empirical excess risk for \textit{DP-ShuffleG} compared to DP-SGD. To address this limitation, we propose \textit{Interleaved-ShuffleG}, a hybrid approach that integrates public data samples in private optimization. By alternating optimization steps that use private and public samples, \textit{Interleaved-ShuffleG} effectively reduces empirical excess risk. Our analysis introduces a new optimization framework with surrogate objectives, adaptive noise injection, and a dissimilarity metric, which can be of independent interest. Our experiments on diverse datasets and tasks demonstrate the superiority of \textit{Interleaved-ShuffleG} over several baselines.</li>
</ul>

<h3>Title: Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Indrashis Das, Mahmoud Safari, Steven Adriaensen, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03654">https://arxiv.org/abs/2502.03654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03654">https://arxiv.org/pdf/2502.03654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03654]] Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics(https://arxiv.org/abs/2502.03654)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Activation functions are fundamental elements of deep learning architectures as they significantly influence training dynamics. ReLU, while widely used, is prone to the dying neuron problem, which has been mitigated by variants such as LeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently, self-gated activations like GELU and Swish have emerged as state-of-the-art alternatives, leveraging their smoothness to ensure stable gradient flow and prevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit (GoLU), a novel self-gated activation function defined as $\mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)$, where $\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU activation leverages the asymmetry in the Gompertz function to reduce variance in the latent space more effectively compared to GELU and Swish, while preserving robust gradient flow. Extensive experiments across diverse tasks, including Image Classification, Language Modeling, Semantic Segmentation, Object Detection, Instance Segmentation, and Diffusion, highlight GoLU's superior performance relative to state-of-the-art activation functions, establishing GoLU as a robust alternative to existing activation functions.</li>
</ul>

<h3>Title: Privacy-Preserving Generative Models: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Debalina Padariya, Isabel Wagner, Aboozar Taherkhani, Eerke Boiten</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03668">https://arxiv.org/abs/2502.03668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03668">https://arxiv.org/pdf/2502.03668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03668]] Privacy-Preserving Generative Models: A Comprehensive Survey(https://arxiv.org/abs/2502.03668)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>Despite the generative model's groundbreaking success, the need to study its implications for privacy and utility becomes more urgent. Although many studies have demonstrated the privacy threats brought by GANs, no existing survey has systematically categorized the privacy and utility perspectives of GANs and VAEs. In this article, we comprehensively study privacy-preserving generative models, articulating the novel taxonomies for both privacy and utility metrics by analyzing 100 research publications. Finally, we discuss the current challenges and future research directions that help new researchers gain insight into the underlying concepts.</li>
</ul>

<h3>Title: Unrealized Expectations: Comparing AI Methods vs Classical Algorithms for Maximum Independent Set</h3>
<ul>
<li><strong>Authors: </strong>Yikai Wu, Haoyu Zhao, Sanjeev Arora</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DM, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03669">https://arxiv.org/abs/2502.03669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03669">https://arxiv.org/pdf/2502.03669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03669]] Unrealized Expectations: Comparing AI Methods vs Classical Algorithms for Maximum Independent Set(https://arxiv.org/abs/2502.03669)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI methods, such as generative models and reinforcement learning, have recently been applied to combinatorial optimization (CO) problems, especially NP-hard ones. This paper compares such GPU-based methods with classical CPU-based methods on Maximum Independent Set (MIS). Experiments on standard graph families show that AI-based algorithms fail to outperform and, in many cases, to match the solution quality of the state-of-art classical solver KaMIS running on a single CPU. Some GPU-based methods even perform similarly to the simplest heuristic, degree-based greedy. Even with post-processing techniques like local search, AI-based methods still perform worse than CPU-based solvers. We develop a new mode of analysis to reveal that non-backtracking AI methods, e.g. LTFT (which is based on GFlowNets), end up reasoning similarly to the simplest degree-based greedy approach, and thus worse than KaMIS. We also find that CPU-based algorithms, notably KaMIS, have strong performance on sparse random graphs, which appears to refute a well-known conjectured upper bound for efficient algorithms from Coja-Oghlan & Efthymiou (2015).</li>
</ul>

<h3>Title: Advancing Reasoning in Large Language Models: Promising Methods and Approaches</h3>
<ul>
<li><strong>Authors: </strong>Avinash Patil</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03671">https://arxiv.org/abs/2502.03671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03671">https://arxiv.org/pdf/2502.03671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03671]] Advancing Reasoning in Large Language Models: Promising Methods and Approaches(https://arxiv.org/abs/2502.03671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have succeeded remarkably in various natural language processing (NLP) tasks, yet their reasoning capabilities remain a fundamental challenge. While LLMs exhibit impressive fluency and factual recall, their ability to perform complex reasoning-spanning logical deduction, mathematical problem-solving, commonsense inference, and multi-step reasoning-often falls short of human expectations. This survey provides a comprehensive review of emerging techniques enhancing reasoning in LLMs. We categorize existing methods into key approaches, including prompting strategies (e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought reasoning), architectural innovations (e.g., retrieval-augmented models, modular reasoning networks, and neuro-symbolic integration), and learning paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement learning, and self-supervised reasoning objectives). Additionally, we explore evaluation frameworks used to assess reasoning in LLMs and highlight open challenges, such as hallucinations, robustness, and reasoning generalization across diverse tasks. By synthesizing recent advancements, this survey aims to provide insights into promising directions for future research and practical applications of reasoning-augmented LLMs.</li>
</ul>

<h3>Title: Reflection-Window Decoding: Text Generation with Selective Refinement</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Tang, Zhenhao Chen, Loka Li, Xiangchen Song, Yunlong Deng, Yifan Shen, Guangyi Chen, Peter Spirtes, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03678">https://arxiv.org/abs/2502.03678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03678">https://arxiv.org/pdf/2502.03678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03678]] Reflection-Window Decoding: Text Generation with Selective Refinement(https://arxiv.org/abs/2502.03678)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Towards Scalable Defenses against Intimate Partner Infiltrations</h3>
<ul>
<li><strong>Authors: </strong>Weisi Yang, Shinan Liu, Feng Xiao, Nick Feamster, Stephen Xia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03682">https://arxiv.org/abs/2502.03682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03682">https://arxiv.org/pdf/2502.03682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03682]] Towards Scalable Defenses against Intimate Partner Infiltrations(https://arxiv.org/abs/2502.03682)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Intimate Partner Infiltration (IPI)--a type of Intimate Partner Violence (IPV) that typically requires physical access to a victim's device--is a pervasive concern in the United States, often manifesting through digital surveillance, control, and monitoring. Unlike conventional cyberattacks, IPI perpetrators leverage close proximity and personal knowledge to circumvent standard protections, underscoring the need for targeted interventions. While security clinics and other human-centered approaches effectively tailor solutions for survivors, their scalability remains constrained by resource limitations and the need for specialized counseling. In this paper, we present AID, an Automated IPI Detection system that continuously monitors for unauthorized access and suspicious behaviors on smartphones. AID employs a two-stage architecture to process multimodal signals stealthily and preserve user privacy. A brief calibration phase upon installation enables AID to adapt to each user's behavioral patterns, achieving high accuracy with minimal false alarms. Our 27-participant user study demonstrates that AID achieves highly accurate detection of non-owner access and fine-grained IPI-related activities, attaining an end-to-end top-3 F1 score of 0.981 with a false positive rate of 4%. These findings suggest that AID can serve as a forensic tool within security clinics, scaling their ability to identify IPI tactics and deliver personalized, far-reaching support to survivors.</li>
</ul>

<h3>Title: Controlled LLM Decoding via Discrete Auto-regressive Biasing</h3>
<ul>
<li><strong>Authors: </strong>Patrick Pynadath, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03685">https://arxiv.org/abs/2502.03685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03685">https://arxiv.org/pdf/2502.03685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03685]] Controlled LLM Decoding via Discrete Auto-regressive Biasing(https://arxiv.org/abs/2502.03685)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Controlled text generation allows for enforcing user-defined constraints on large language model outputs, an increasingly important field as LLMs become more prevalent in everyday life. One common approach uses energy-based decoding, which defines a target distribution through an energy function that combines multiple constraints into a weighted average. However, these methods often struggle to balance fluency with constraint satisfaction, even with extensive tuning of the energy function's coefficients. In this paper, we identify that this suboptimal balance arises from sampling in continuous space rather than the natural discrete space of text tokens. To address this, we propose Discrete Auto-regressive Biasing, a controlled decoding algorithm that leverages gradients while operating entirely in the discrete text domain. Specifically, we introduce a new formulation for controlled text generation by defining a joint distribution over the generated sequence and an auxiliary bias sequence. To efficiently sample from this joint distribution, we propose a Langevin-within-Gibbs sampling algorithm using gradient-based discrete MCMC. Our method significantly improves constraint satisfaction while maintaining comparable or better fluency, all with even lower computational costs. We demonstrate the advantages of our controlled decoding method on sentiment control, language detoxification, and keyword-guided generation.</li>
</ul>

<h3>Title: Variational Control for Guidance in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kushagra Pandey, Farrin Marouf Sofian, Felix Draxler, Theofanis Karaletsos, Stephan Mandt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03686">https://arxiv.org/abs/2502.03686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03686">https://arxiv.org/pdf/2502.03686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03686]] Variational Control for Guidance in Diffusion Models(https://arxiv.org/abs/2502.03686)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models exhibit excellent sample quality, but existing guidance methods often require additional model training or are limited to specific tasks. We revisit guidance in diffusion models from the perspective of variational inference and control, introducing Diffusion Trajectory Matching (DTM) that enables guiding pretrained diffusion trajectories to satisfy a terminal cost. DTM unifies a broad class of guidance methods and enables novel instantiations. We introduce a new method within this framework that achieves state-of-the-art results on several linear and (blind) non-linear inverse problems without requiring additional model training or modifications. For instance, in ImageNet non-linear deblurring, our model achieves an FID score of 34.31, significantly improving over the best pretrained-method baseline (FID 78.07). We will make the code available in a future update.</li>
</ul>

<h3>Title: Conditional Diffusion Models are Medical Image Classifiers that Provide Explainability and Uncertainty for Free</h3>
<ul>
<li><strong>Authors: </strong>Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, Tal Arbel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03687">https://arxiv.org/abs/2502.03687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03687">https://arxiv.org/pdf/2502.03687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03687]] Conditional Diffusion Models are Medical Image Classifiers that Provide Explainability and Uncertainty for Free(https://arxiv.org/abs/2502.03687)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: this https URL</li>
</ul>

<h3>Title: A Comparison of DeepSeek and Other LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tianchen Gao, Jiashun Jin, Zheng Tracy Ke, Gabriel Moryoussef</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03688">https://arxiv.org/abs/2502.03688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03688">https://arxiv.org/pdf/2502.03688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03688]] A Comparison of DeepSeek and Other LLMs(https://arxiv.org/abs/2502.03688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, DeepSeek has been the focus of attention in and beyond the AI community. An interesting problem is how DeepSeek compares to other large language models (LLMs). There are many tasks an LLM can do, and in this paper, we use the task of predicting an outcome using a short text for comparison. We consider two settings, an authorship classification setting and a citation classification setting. In the first one, the goal is to determine whether a short text is written by human or AI. In the second one, the goal is to classify a citation to one of four types using the textual content. For each experiment, we compare DeepSeek with $4$ popular LLMs: Claude, Gemini, GPT, and Llama. We find that, in terms of classification accuracy, DeepSeek outperforms Gemini, GPT, and Llama in most cases, but underperforms Claude. We also find that DeepSeek is comparably slower than others but with a low cost to use, while Claude is much more expensive than all the others. Finally, we find that in terms of similarity, the output of DeepSeek is most similar to those of Gemini and Claude (and among all $5$ LLMs, Claude and Gemini have the most similar outputs). In this paper, we also present a fully-labeled dataset collected by ourselves, and propose a recipe where we can use the LLMs and a recent data set, MADStat, to generate new data sets. The datasets in our paper can be used as benchmarks for future study on LLMs.</li>
</ul>

<h3>Title: DocMIA: Document-Level Membership Inference Attacks against DocVQA Models</h3>
<ul>
<li><strong>Authors: </strong>Khanh Nguyen, Raouf Kerkouche, Mario Fritz, Dimosthenis Karatzas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03692">https://arxiv.org/abs/2502.03692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03692">https://arxiv.org/pdf/2502.03692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03692]] DocMIA: Document-Level Membership Inference Attacks against DocVQA Models(https://arxiv.org/abs/2502.03692)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Document Visual Question Answering (DocVQA) has introduced a new paradigm for end-to-end document understanding, and quickly became one of the standard benchmarks for multimodal LLMs. Automating document processing workflows, driven by DocVQA models, presents significant potential for many business sectors. However, documents tend to contain highly sensitive information, raising concerns about privacy risks associated with training such DocVQA models. One significant privacy vulnerability, exploited by the membership inference attack, is the possibility for an adversary to determine if a particular record was part of the model's training data. In this paper, we introduce two novel membership inference attacks tailored specifically to DocVQA models. These attacks are designed for two different adversarial scenarios: a white-box setting, where the attacker has full access to the model architecture and parameters, and a black-box setting, where only the model's outputs are available. Notably, our attacks assume the adversary lacks access to auxiliary datasets, which is more realistic in practice but also more challenging. Our unsupervised methods outperform existing state-of-the-art membership inference attacks across a variety of DocVQA models and datasets, demonstrating their effectiveness and highlighting the privacy risks in this domain.</li>
</ul>

<h3>Title: How vulnerable is my policy? Adversarial attacks on modern behavior cloning policies</h3>
<ul>
<li><strong>Authors: </strong>Basavasagar Patil, Akansha Kalra, Guanhong Tao, Daniel S. Brown</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03698">https://arxiv.org/abs/2502.03698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03698">https://arxiv.org/pdf/2502.03698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03698]] How vulnerable is my policy? Adversarial attacks on modern behavior cloning policies(https://arxiv.org/abs/2502.03698)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Learning from Demonstration (LfD) algorithms have shown promising results in robotic manipulation tasks, but their vulnerability to adversarial attacks remains underexplored. This paper presents a comprehensive study of adversarial attacks on both classic and recently proposed algorithms, including Behavior Cloning (BC), LSTM-GMM, Implicit Behavior Cloning (IBC), Diffusion Policy (DP), and VQ-Behavior Transformer (VQ-BET). We study the vulnerability of these methods to untargeted, targeted and universal adversarial perturbations. While explicit policies, such as BC, LSTM-GMM and VQ-BET can be attacked in the same manner as standard computer vision models, we find that attacks for implicit and denoising policy models are nuanced and require developing novel attack methods. Our experiments on several simulated robotic manipulation tasks reveal that most of the current methods are highly vulnerable to adversarial perturbations. We also show that these attacks are transferable across algorithms, architectures, and tasks, raising concerning security vulnerabilities with potentially a white-box threat model. In addition, we test the efficacy of a randomized smoothing, a widely used adversarial defense technique, and highlight its limitation in defending against attacks on complex and multi-modal action distribution common in complex control tasks. In summary, our findings highlight the vulnerabilities of modern BC algorithms, paving way for future work in addressing such limitations.</li>
</ul>

<h3>Title: LLM Alignment as Retriever Optimization: An Information Retrieval Perspective</h3>
<ul>
<li><strong>Authors: </strong>Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03699">https://arxiv.org/abs/2502.03699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03699">https://arxiv.org/pdf/2502.03699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03699]] LLM Alignment as Retriever Optimization: An Information Retrieval Perspective(https://arxiv.org/abs/2502.03699)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative. In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.</li>
</ul>

<h3>Title: Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers</h3>
<ul>
<li><strong>Authors: </strong>Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adser√†, Mikhail Belkin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03708">https://arxiv.org/abs/2502.03708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03708">https://arxiv.org/pdf/2502.03708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03708]] Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers(https://arxiv.org/abs/2502.03708)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A trained Large Language Model (LLM) contains much of human knowledge. Yet, it is difficult to gauge the extent or accuracy of that knowledge, as LLMs do not always ``know what they know'' and may even be actively misleading. In this work, we give a general method for detecting semantic concepts in the internal activations of LLMs. Furthermore, we show that our methodology can be easily adapted to steer LLMs toward desirable outputs. Our innovations are the following: (1) we use a nonlinear feature learning method to identify important linear directions for predicting concepts from each layer; (2) we aggregate features across layers to build powerful concept detectors and steering mechanisms. We showcase the power of our approach by attaining state-of-the-art results for detecting hallucinations, harmfulness, toxicity, and untruthful content on seven benchmarks. We highlight the generality of our approach by steering LLMs towards new concepts that, to the best of our knowledge, have not been previously considered in the literature, including: semantic disambiguation, human languages, programming languages, hallucinated responses, science subjects, poetic/Shakespearean English, and even multiple concepts simultaneously. Moreover, our method can steer concepts with numerical attributes such as product reviews. We provide our code (including a simple API for our methods) at this https URL .</li>
</ul>

<h3>Title: MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers</h3>
<ul>
<li><strong>Authors: </strong>Nicole Cho, William Watson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03711">https://arxiv.org/abs/2502.03711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03711">https://arxiv.org/pdf/2502.03711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03711]] MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers(https://arxiv.org/abs/2502.03711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>One critical challenge in the institutional adoption journey of Large Language Models (LLMs) stems from their propensity to hallucinate in generated responses. To address this, we propose MultiQ&A, a systematic approach for evaluating the robustness and consistency of LLM-generated answers. We demonstrate MultiQ&A's ability to crowdsource question perturbations and their respective answers through independent LLM agents at scale. Our experiments culminated in the examination of 1.9 million question perturbations and 2.3 million answers. Furthermore, MultiQ&A shows that ensembled LLMs, such as gpt-3.5-turbo, remain relatively robust and consistent under perturbations. MultiQ&A provides clarity in the response generation space, offering an effective method for inspecting disagreements and variability. Therefore, our system offers a potential framework for institutional LLM adoption with the ability to measure confidence, consistency, and the quantification of hallucinations.</li>
</ul>

<h3>Title: Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment</h3>
<ul>
<li><strong>Authors: </strong>Harrish Thasarathan, Julian Forsyth, Thomas Fel, Matthew Kowal, Konstantinos Derpanis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03714">https://arxiv.org/abs/2502.03714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03714">https://arxiv.org/pdf/2502.03714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03714]] Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment(https://arxiv.org/abs/2502.03714)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We present Universal Sparse Autoencoders (USAEs), a framework for uncovering and aligning interpretable concepts spanning multiple pretrained deep neural networks. Unlike existing concept-based interpretability methods, which focus on a single model, USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models at once. Our core insight is to train a single, overcomplete sparse autoencoder (SAE) that ingests activations from any model and decodes them to approximate the activations of any other model under consideration. By optimizing a shared objective, the learned dictionary captures common factors of variation-concepts-across different tasks, architectures, and datasets. We show that USAEs discover semantically coherent and important universal concepts across vision models; ranging from low-level features (e.g., colors and textures) to higher-level structures (e.g., parts and objects). Overall, USAEs provide a powerful new method for interpretable cross-model analysis and offers novel applications, such as coordinated activation maximization, that open avenues for deeper insights in multi-model AI systems</li>
</ul>

<h3>Title: Following Devils' Footprint: Towards Real-time Detection of Price Manipulation Attacks</h3>
<ul>
<li><strong>Authors: </strong>Bosi Zhang, Ningyu He, Xiaohui Hu, Kai Ma, Haoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03718">https://arxiv.org/abs/2502.03718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03718">https://arxiv.org/pdf/2502.03718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03718]] Following Devils' Footprint: Towards Real-time Detection of Price Manipulation Attacks(https://arxiv.org/abs/2502.03718)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Price manipulation attack is one of the notorious threats in decentralized finance (DeFi) applications, which allows attackers to exchange tokens at an extensively deviated price from the market. Existing efforts usually rely on reactive methods to identify such kind of attacks after they have happened, e.g., detecting attack transactions in the post-attack stage, which cannot mitigate or prevent price manipulation attacks timely. From the perspective of attackers, they usually need to deploy attack contracts in the pre-attack stage. Thus, if we can identify these attack contracts in a proactive manner, we can raise alarms and mitigate the threats. With the core idea in mind, in this work, we shift our attention from the victims to the attackers. Specifically, we propose SMARTCAT, a novel approach for identifying price manipulation attacks in the pre-attack stage proactively. For generality, it conducts analysis on bytecode and does not require any source code and transaction data. For accuracy, it depicts the control- and data-flow dependency relationships among function calls into a token flow graph. For scalability, it filters out those suspicious paths, in which it conducts inter-contract analysis as necessary. To this end, SMARTCAT can pinpoint attacks in real time once they have been deployed on a chain. The evaluation results illustrate that SMARTCAT significantly outperforms existing baselines with 91.6% recall and ~100% precision. Moreover, SMARTCAT also uncovers 616 attack contracts in-the-wild, accounting for \$9.25M financial losses, with only 19 cases publicly reported. By applying SMARTCAT as a real-time detector in Ethereum and Binance Smart Chain, it has raised 14 alarms 99 seconds after the corresponding deployment on average. These attacks have already led to $641K financial losses, and seven of them are still waiting for their ripe time.</li>
</ul>

<h3>Title: Detecting Backdoor Attacks via Similarity in Semantic Communication Systems</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Wei, Yili Jiang, Jiaqi Huang, Fangtian Zhong, Sohan Gyawali</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03721">https://arxiv.org/abs/2502.03721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03721">https://arxiv.org/pdf/2502.03721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03721]] Detecting Backdoor Attacks via Similarity in Semantic Communication Systems(https://arxiv.org/abs/2502.03721)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, generative</a></li>
<li><strong>Abstract: </strong>Semantic communication systems, which leverage Generative AI (GAI) to transmit semantic meaning rather than raw data, are poised to revolutionize modern communications. However, they are vulnerable to backdoor attacks, a type of poisoning manipulation that embeds malicious triggers into training datasets. As a result, Backdoor attacks mislead the inference for poisoned samples while clean samples remain unaffected. The existing defenses may alter the model structure (such as neuron pruning that potentially degrades inference performance on clean inputs, or impose strict requirements on data formats (such as ``Semantic Shield" that requires image-text pairs). To address these limitations, this work proposes a defense mechanism that leverages semantic similarity to detect backdoor attacks without modifying the model structure or imposing data format constraints. By analyzing deviations in semantic feature space and establishing a threshold-based detection framework, the proposed approach effectively identifies poisoned samples. The experimental results demonstrate high detection accuracy and recall across varying poisoning ratios, underlining the significant effectiveness of our proposed solution.</li>
</ul>

<h3>Title: DICE: Distilling Classifier-Free Guidance into Text Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen, Siwei Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03726">https://arxiv.org/abs/2502.03726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03726">https://arxiv.org/pdf/2502.03726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03726]] DICE: Distilling Classifier-Free Guidance into Text Embeddings(https://arxiv.org/abs/2502.03726)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models are capable of generating high-quality images, but these images often fail to align closely with the given text prompts. Classifier-free guidance (CFG) is a popular and effective technique for improving text-image alignment in the generative process. However, using CFG introduces significant computational overhead and deviates from the established theoretical foundations of diffusion models. In this paper, we present DIstilling CFG by enhancing text Embeddings (DICE), a novel approach that removes the reliance on CFG in the generative process while maintaining the benefits it provides. DICE distills a CFG-based text-to-image diffusion model into a CFG-free version by refining text embeddings to replicate CFG-based directions. In this way, we avoid the computational and theoretical drawbacks of CFG, enabling high-quality, well-aligned image generation at a fast sampling speed. Extensive experiments on multiple Stable Diffusion v1.5 variants, SDXL and PixArt-$\alpha$ demonstrate the effectiveness of our method. Furthermore, DICE supports negative prompts for image editing to improve image quality further. Code will be available soon.</li>
</ul>

<h3>Title: Mitigating the Participation Bias by Balancing Extreme Ratings</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Guo, Yuqing Kong, Jialiang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03737">https://arxiv.org/abs/2502.03737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03737">https://arxiv.org/pdf/2502.03737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03737]] Mitigating the Participation Bias by Balancing Extreme Ratings(https://arxiv.org/abs/2502.03737)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Rating aggregation plays a crucial role in various fields, such as product recommendations, hotel rankings, and teaching evaluations. However, traditional averaging methods can be affected by participation bias, where some raters do not participate in the rating process, leading to potential distortions. In this paper, we consider a robust rating aggregation task under the participation bias. We assume that raters may not reveal their ratings with a certain probability depending on their individual ratings, resulting in partially observed samples. Our goal is to minimize the expected squared loss between the aggregated ratings and the average of all underlying ratings (possibly unobserved) in the worst-case scenario. We focus on two settings based on whether the sample size (i.e. the number of raters) is known. In the first setting, where the sample size is known, we propose an aggregator, named as the Balanced Extremes Aggregator. It estimates unrevealed ratings with a balanced combination of extreme ratings. When the sample size is unknown, we derive another aggregator, the Polarizing-Averaging Aggregator, which becomes optimal as the sample size grows to infinity. Numerical results demonstrate the superiority of our proposed aggregators in mitigating participation bias, compared to simple averaging and the spectral method. Furthermore, we validate the effectiveness of our aggregators on a real-world dataset.</li>
</ul>

<h3>Title: Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More</h3>
<ul>
<li><strong>Authors: </strong>Feng Wang, Yaodong Yu, Guoyizhe Wei, Wei Shao, Yuyin Zhou, Alan Yuille, Cihang Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03738">https://arxiv.org/abs/2502.03738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03738">https://arxiv.org/pdf/2502.03738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03738]] Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More(https://arxiv.org/abs/2502.03738)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at this https URL.</li>
</ul>

<h3>Title: Brain Tumor Identification using Improved YOLOv8</h3>
<ul>
<li><strong>Authors: </strong>Rupesh Dulal, Rabin Dulal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03746">https://arxiv.org/abs/2502.03746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03746">https://arxiv.org/pdf/2502.03746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03746]] Brain Tumor Identification using Improved YOLOv8(https://arxiv.org/abs/2502.03746)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Identifying the extent of brain tumors is a significant challenge in brain cancer treatment. The main difficulty is in the approximate detection of tumor size. Magnetic resonance imaging (MRI) has become a critical diagnostic tool. However, manually detecting the boundaries of brain tumors from MRI scans is a labor-intensive task that requires extensive expertise. Deep learning and computer-aided detection techniques have led to notable advances in machine learning for this purpose. In this paper, we propose a modified You Only Look Once (YOLOv8) model to accurately detect the tumors within the MRI images. The proposed model replaced the Non-Maximum Suppression (NMS) algorithm with a Real-Time Detection Transformer (RT- DETR) in the detection head. NMS filters out redundant or overlapping bounding boxes in the detected tumors, but they are hand-designed and pre-set. RT-DETR removes hand-designed components. The second improvement was made by replacing the normal convolution block with ghost convolution. Ghost Convolution reduces computational and memory costs while maintaining high accuracy and enabling faster inference, making it ideal for resource-constrained environments and real-time applications. The third improvement was made by introducing a vision transformer block in the backbone of YOLOv8 to extract context-aware features. We used a publicly available dataset of brain tumors in the proposed model. The proposed model performed better than the original YOLOv8 model and also performed better than other object detectors (Faster R- CNN, Mask R-CNN, YOLO, YOLOv3, YOLOv4, YOLOv5, SSD, RetinaNet, EfficientDet, and DETR). The proposed model achieved 0.91 mAP (mean Average Precision)@0.5.</li>
</ul>

<h3>Title: Rethinking the Residual Distribution of Locate-then-Editing Methods in Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Xiaopeng Li, Shanwen Wang, Shasha Li, Shezheng Song, Bin Ji, Jun Ma, Jie Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03748">https://arxiv.org/abs/2502.03748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03748">https://arxiv.org/pdf/2502.03748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03748]] Rethinking the Residual Distribution of Locate-then-Editing Methods in Model Editing(https://arxiv.org/abs/2502.03748)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model editing is a powerful technique for updating the knowledge of Large Language Models (LLMs). Locate-then-edit methods are a popular class of approaches that first identify the critical layers storing knowledge, then compute the residual of the last critical layer based on the edited knowledge, and finally perform multi-layer updates using a least-squares solution by evenly distributing the residual from the first critical layer to the last. Although these methods achieve promising results, they have been shown to degrade the original knowledge of LLMs. We argue that residual distribution leads to this issue. To explore this, we conduct a comprehensive analysis of residual distribution in locate-then-edit methods from both empirical and theoretical perspectives, revealing that residual distribution introduces editing errors, leading to inaccurate edits. To address this issue, we propose the Boundary Layer UpdatE (BLUE) strategy to enhance locate-then-edit methods. Sequential batch editing experiments on three LLMs and two datasets demonstrate that BLUE not only delivers an average performance improvement of 35.59\%, significantly advancing the state of the art in model editing, but also enhances the preservation of LLMs' general capabilities. Our code is available at this https URL.</li>
</ul>

<h3>Title: PINS: Proximal Iterations with Sparse Newton and Sinkhorn for Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Di Wu, Ling Liang, Haizhao Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03749">https://arxiv.org/abs/2502.03749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03749">https://arxiv.org/pdf/2502.03749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03749]] PINS: Proximal Iterations with Sparse Newton and Sinkhorn for Optimal Transport(https://arxiv.org/abs/2502.03749)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Optimal transport (OT) is a critical problem in optimization and machine learning, where accuracy and efficiency are paramount. Although entropic regularization and the Sinkhorn algorithm improve scalability, they frequently encounter numerical instability and slow convergence, especially when the regularization parameter is small. In this work, we introduce Proximal Iterations with Sparse Newton and Sinkhorn methods (PINS) to efficiently compute highly accurate solutions for large-scale OT problems. A reduced computational complexity through overall sparsity and global convergence are guaranteed by rigorous theoretical analysis. Our approach offers three key advantages: it achieves accuracy comparable to exact solutions, progressively accelerates each iteration for greater efficiency, and enhances robustness by reducing sensitivity to regularization parameters. Extensive experiments confirm these advantages, demonstrating superior performance compared to related methods.</li>
</ul>

<h3>Title: PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations</h3>
<ul>
<li><strong>Authors: </strong>Sanghyeon Lee, Sangjun Bae, Yisak Park, Seungyul Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03752">https://arxiv.org/abs/2502.03752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03752">https://arxiv.org/pdf/2502.03752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03752]] PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations(https://arxiv.org/abs/2502.03752)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, resulting in unstable skill learning and degraded performance. To overcome this, we propose Prioritized Refinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates exploration near noisy data to generate online trajectories and combines them with offline data. Through prioritization, PRISM extracts high-quality data to learn task-relevant skills effectively. By addressing the impact of noise, our method ensures stable skill learning and achieves superior performance in long-horizon tasks, even with noisy and sub-optimal data.</li>
</ul>

<h3>Title: Improving Adversarial Robustness via Phase and Amplitude-aware Prompting</h3>
<ul>
<li><strong>Authors: </strong>Yibo Xu, Dawei Zhou, Decheng Liu, Nannan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03758">https://arxiv.org/abs/2502.03758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03758">https://arxiv.org/pdf/2502.03758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03758]] Improving Adversarial Robustness via Phase and Amplitude-aware Prompting(https://arxiv.org/abs/2502.03758)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks are found to be vulnerable to adversarial noises. The prompt-based defense has been increasingly studied due to its high efficiency. However, existing prompt-based defenses mainly exploited mixed prompt patterns, where critical patterns closely related to object semantics lack sufficient focus. The phase and amplitude spectra have been proven to be highly related to specific semantic patterns and crucial for robustness. To this end, in this paper, we propose a Phase and Amplitude-aware Prompting (PAP) defense. Specifically, we construct phase-level and amplitude-level prompts for each class, and adjust weights for prompting according to the model's robust performance under these prompts during training. During testing, we select prompts for each image using its predicted label to obtain the prompted image, which is inputted to the model to get the final prediction. Experimental results demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Meiquan Dong, Haoran Liu, Yan Huang, Zixuan Feng, Jianhong Tang, Ruoxi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03766">https://arxiv.org/abs/2502.03766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03766">https://arxiv.org/pdf/2502.03766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03766]] Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models(https://arxiv.org/abs/2502.03766)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The organization of latent token representations plays a crucial role in determining the stability, generalization, and contextual consistency of language models, yet conventional approaches to embedding refinement often rely on parameter modifications that introduce additional computational overhead. A hierarchical alignment method was introduced to restructure token embeddings without altering core model weights, ensuring that representational distributions maintained coherence across different linguistic contexts. Experimental evaluations demonstrated improvements in rare token retrieval, adversarial robustness, and long-range dependency tracking, highlighting the advantages of hierarchical structuring in mitigating inconsistencies in latent space organization. The comparative analysis against conventional fine-tuning and embedding perturbation methods revealed that hierarchical restructuring maintained computational efficiency while achieving measurable gains in representation quality. Structural refinements introduced through the alignment process resulted in improved contextual stability across varied linguistic tasks, reducing inconsistencies in token proximity relationships and enhancing interpretability in language generation. A detailed computational assessment confirmed that the realignment process introduced minimal inference overhead, ensuring that representational improvements did not compromise model efficiency. The findings reinforced the broader significance of structured representation learning, illustrating that hierarchical embedding modifications could serve as an effective strategy for refining latent space distributions while preserving pre-learned semantic associations.</li>
</ul>

<h3>Title: Adaptive Semantic Prompt Caching with VectorQ</h3>
<ul>
<li><strong>Authors: </strong>Luis Gaspar Schroeder, Shu Liu, Alejandro Cuadron, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03771">https://arxiv.org/abs/2502.03771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03771">https://arxiv.org/pdf/2502.03771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03771]] Adaptive Semantic Prompt Caching with VectorQ(https://arxiv.org/abs/2502.03771)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign a numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on a static threshold to classify whether the similarity score is sufficiently high to result in a cache hit. We show that this one-size-fits-all threshold is insufficient across different prompts. We propose VectorQ, a framework to learn embedding-specific threshold regions that adapt to the complexity and uncertainty of an embedding. Through evaluations on a combination of four diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 12x increases in cache hit rate and error rate reductions up to 92%.</li>
</ul>

<h3>Title: A Retrospective Systematic Study on Hierarchical Sparse Query Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma</h3>
<ul>
<li><strong>Authors: </strong>Chaoyin She, Ruifang Lu, Danni He, Jiayi Lv, Yadan Lin, Meiqing Cheng, Hui Huang, Lida Chen, Wei Wang, Qinghua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03772">https://arxiv.org/abs/2502.03772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03772">https://arxiv.org/pdf/2502.03772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03772]] A Retrospective Systematic Study on Hierarchical Sparse Query Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma(https://arxiv.org/abs/2502.03772)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Hepatocellular carcinoma (HCC) ranks as the third leading cause of cancer-related mortality worldwide, with early detection being crucial for improving patient survival rates. However, early screening for HCC using ultrasound suffers from insufficient sensitivity and is highly dependent on the expertise of radiologists for interpretation. Leveraging the latest advancements in artificial intelligence (AI) in medical imaging, this study proposes an innovative Hierarchical Sparse Query Transformer (HSQformer) model that combines the strengths of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to enhance the accuracy of HCC diagnosis in ultrasound screening. The HSQformer leverages sparse latent space representations to capture hierarchical details at various granularities without the need for complex adjustments, and adopts a modular, plug-and-play design philosophy, ensuring the model's versatility and ease of use. The HSQformer's performance was rigorously tested across three distinct clinical scenarios: single-center, multi-center, and high-risk patient testing. In each of these settings, it consistently outperformed existing state-of-the-art models, such as ConvNext and SwinTransformer. Notably, the HSQformer even matched the diagnostic capabilities of senior radiologists and comprehensively surpassed those of junior radiologists. The experimental results from this study strongly demonstrate the effectiveness and clinical potential of AI-assisted tools in HCC screening. The full code is available at this https URL.</li>
</ul>

<h3>Title: ExpProof : Operationalizing Explanations for Confidential Models with ZKPs</h3>
<ul>
<li><strong>Authors: </strong>Chhavi Yadav, Evan Monroe Laufer, Dan Boneh, Kamalika Chaudhuri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03773">https://arxiv.org/abs/2502.03773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03773">https://arxiv.org/pdf/2502.03773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03773]] ExpProof : Operationalizing Explanations for Confidential Models with ZKPs(https://arxiv.org/abs/2502.03773)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In principle, explanations are intended as a way to increase trust in machine learning models and are often obligated by regulations. However, many circumstances where these are demanded are adversarial in nature, meaning the involved parties have misaligned interests and are incentivized to manipulate explanations for their purpose. As a result, explainability methods fail to be operational in such settings despite the demand \cite{bordt2022post}. In this paper, we take a step towards operationalizing explanations in adversarial scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive. Specifically we explore ZKP-amenable versions of the popular explainability algorithm LIME and evaluate their performance on Neural Networks and Random Forests.</li>
</ul>

<h3>Title: StarMAP: Global Neighbor Embedding for Faithful Data Visualization</h3>
<ul>
<li><strong>Authors: </strong>Koshi Watanabe, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03776">https://arxiv.org/abs/2502.03776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03776">https://arxiv.org/pdf/2502.03776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03776]] StarMAP: Global Neighbor Embedding for Faithful Data Visualization(https://arxiv.org/abs/2502.03776)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Neighbor embedding is widely employed to visualize high-dimensional data; however, it frequently overlooks the global structure, e.g., intercluster similarities, thereby impeding accurate visualization. To address this problem, this paper presents Star-attracted Manifold Approximation and Projection (StarMAP), which incorporates the advantage of principal component analysis (PCA) in neighbor embedding. Inspired by the property of PCA embedding, which can be viewed as the largest shadow of the data, StarMAP introduces the concept of \textit{star attraction} by leveraging the PCA embedding. This approach yields faithful global structure preservation while maintaining the interpretability and computational efficiency of neighbor embedding. StarMAP was compared with existing methods in the visualization tasks of toy datasets, single-cell RNA sequencing data, and deep representation. The experimental results show that StarMAP is simple but effective in realizing faithful visualizations.</li>
</ul>

<h3>Title: Gaze-Assisted Human-Centric Domain Adaptation for Cardiac Ultrasound Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ruiyi Li, Yuting He, Rongjun Ge, Chong Wang, Daoqiang Zhang, Yang Chen, Shuo Li</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03781">https://arxiv.org/abs/2502.03781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03781">https://arxiv.org/pdf/2502.03781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03781]] Gaze-Assisted Human-Centric Domain Adaptation for Cardiac Ultrasound Image Segmentation(https://arxiv.org/abs/2502.03781)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Domain adaptation (DA) for cardiac ultrasound image segmentation is clinically significant and valuable. However, previous domain adaptation methods are prone to be affected by the incomplete pseudo-label and low-quality target to source images. Human-centric domain adaptation has great advantages of human cognitive guidance to help model adapt to target domain and reduce reliance on labels. Doctor gaze trajectories contains a large amount of cross-domain human guidance. To leverage gaze information and human cognition for guiding domain adaptation, we propose gaze-assisted human-centric domain adaptation (GAHCDA), which reliably guides the domain adaptation of cardiac ultrasound images. GAHCDA includes following modules: (1) Gaze Augment Alignment (GAA): GAA enables the model to obtain human cognition general features to recognize segmentation target in different domain of cardiac ultrasound images like humans. (2) Gaze Balance Loss (GBL): GBL fused gaze heatmap with outputs which makes the segmentation result structurally closer to the target domain. The experimental results illustrate that our proposed framework is able to segment cardiac ultrasound images more effectively in the target domain than GAN-based methods and other self-train based methods, showing great potential in clinical application.</li>
</ul>

<h3>Title: Iterate to Accelerate: A Unified Framework for Iterative Reasoning and Feedback Convergence</h3>
<ul>
<li><strong>Authors: </strong>Jacob Fein-Ashley</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03787">https://arxiv.org/abs/2502.03787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03787">https://arxiv.org/pdf/2502.03787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03787]] Iterate to Accelerate: A Unified Framework for Iterative Reasoning and Feedback Convergence(https://arxiv.org/abs/2502.03787)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a unified framework for iterative reasoning that leverages non-Euclidean geometry via Bregman divergences, higher-order operator averaging, and adaptive feedback mechanisms. Our analysis establishes that, under mild smoothness and contractivity assumptions, a generalized update scheme not only unifies classical methods such as mirror descent and dynamic programming but also captures modern chain-of-thought reasoning processes in large language models. In particular, we prove that our accelerated iterative update achieves an $O(1/t^2)$ convergence rate in the absence of persistent perturbations, and we further demonstrate that feedback (iterative) architectures are necessary to approximate certain fixed-point functions efficiently. These theoretical insights bridge classical acceleration techniques with contemporary applications in neural computation and optimization.</li>
</ul>

<h3>Title: It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Clavi√©, Nathan Cooper, Benjamin Warner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03793">https://arxiv.org/abs/2502.03793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03793">https://arxiv.org/pdf/2502.03793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03793]] It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers(https://arxiv.org/abs/2502.03793)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>While encoder-only models such as BERT and ModernBERT are ubiquitous in real-world NLP applications, their conventional reliance on task-specific classification heads can limit their applicability compared to decoder-based large language models (LLMs). In this work, we introduce ModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its masked language modelling (MLM) head for generative classification. Our approach employs an intentionally simple training loop and inference mechanism that requires no heavy pre-processing, heavily engineered prompting, or architectural modifications. ModernBERT-Large-Instruct exhibits strong zero-shot performance on both classification and knowledge-based tasks, outperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's MMLU performance with 60% less parameters. We also demonstrate that, when fine-tuned, the generative approach using the MLM head matches or even surpasses traditional classification-head methods across diverse NLU this http URL capability emerges specifically in models trained on contemporary, diverse data mixes, with models trained on lower volume, less-diverse data yielding considerably weaker performance. Although preliminary, these results demonstrate the potential of using the original generative masked language modelling head over traditional task-specific heads for downstream tasks. Our work suggests that further exploration into this area is warranted, highlighting many avenues for future improvements.</li>
</ul>

<h3>Title: Distribution learning via neural differential equations: minimal energy regularization and approximation theory</h3>
<ul>
<li><strong>Authors: </strong>Youssef Marzouk, Zhi Ren, Jakob Zech</a></li>
<li><strong>Subjects: </strong>cs.LG, math.CA, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03795">https://arxiv.org/abs/2502.03795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03795">https://arxiv.org/pdf/2502.03795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03795]] Distribution learning via neural differential equations: minimal energy regularization and approximation theory(https://arxiv.org/abs/2502.03795)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural ordinary differential equations (ODEs) provide expressive representations of invertible transport maps that can be used to approximate complex probability distributions, e.g., for generative modeling, density estimation, and Bayesian inference. We show that for a large class of transport maps $T$, there exists a time-dependent ODE velocity field realizing a straight-line interpolation $(1-t)x + tT(x)$, $t \in [0,1]$, of the displacement induced by the map. Moreover, we show that such velocity fields are minimizers of a training objective containing a specific minimum-energy regularization. We then derive explicit upper bounds for the $C^k$ norm of the velocity field that are polynomial in the $C^k$ norm of the corresponding transport map $T$; in the case of triangular (Knothe--Rosenblatt) maps, we also show that these bounds are polynomial in the $C^k$ norms of the associated source and target densities. Combining these results with stability arguments for distribution approximation via ODEs, we show that Wasserstein or Kullback--Leibler approximation of the target distribution to any desired accuracy $\epsilon > 0$ can be achieved by a deep neural network representation of the velocity field whose size is bounded explicitly in terms of $\epsilon$, the dimension, and the smoothness of the source and target densities. The same neural network ansatz yields guarantees on the value of the regularized training objective.</li>
</ul>

<h3>Title: Enhancing Hallucination Detection through Noise Injection</h3>
<ul>
<li><strong>Authors: </strong>Litian Liu, Reza Pourreza, Sunny Panchal, Apratim Bhattacharyya, Yao Qin, Roland Memisevic</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03799">https://arxiv.org/abs/2502.03799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03799">https://arxiv.org/pdf/2502.03799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03799]] Enhancing Hallucination Detection through Noise Injection(https://arxiv.org/abs/2502.03799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are prone to generating plausible yet incorrect responses, known as hallucinations. Effectively detecting hallucinations is therefore crucial for the safe deployment of LLMs. Recent research has linked hallucinations to model uncertainty, suggesting that hallucinations can be detected by measuring dispersion over answer distributions obtained from a set of samples drawn from a model. While drawing from the distribution over tokens defined by the model is a natural way to obtain samples, in this work, we argue that it is sub-optimal for the purpose of detecting hallucinations. We show that detection can be improved significantly by taking into account model uncertainty in the Bayesian sense. To this end, we propose a very simple and efficient approach that perturbs an appropriate subset of model parameters, or equivalently hidden unit activations, during sampling. We demonstrate its effectiveness across a wide range of datasets and model architectures.</li>
</ul>

<h3>Title: SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Heyi Zhang, Yule Liu, Xinlei He, Jun Wu, Tianshuo Cong, Xinyi Huang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03801">https://arxiv.org/abs/2502.03801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03801">https://arxiv.org/pdf/2502.03801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03801]] SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning(https://arxiv.org/abs/2502.03801)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables collaborative model training while preserving data privacy, but its decentralized nature exposes it to client-side data poisoning attacks (DPAs) and model poisoning attacks (MPAs) that degrade global model performance. While numerous proposed defenses claim substantial effectiveness, their evaluation is typically done in isolation with limited attack strategies, raising concerns about their validity. Additionally, existing studies overlook the mutual effectiveness of defenses against both DPAs and MPAs, causing fragmentation in this field. This paper aims to provide a unified benchmark and analysis of defenses against DPAs and MPAs, clarifying the distinction between these two similar but slightly distinct domains. We present a systematic taxonomy of poisoning attacks and defense strategies, outlining their design, strengths, and limitations. Then, a unified comparative evaluation across FL algorithms and data heterogeneity is conducted to validate their individual and mutual effectiveness and derive key insights for design principles and future research. Along with the analysis, we frame our work to a unified benchmark, FLPoison, with high modularity and scalability to evaluate 15 representative poisoning attacks and 17 defense strategies, facilitating future research in this domain. Code is available at this https URL.</li>
</ul>

<h3>Title: Graph Neural Network-Driven Hierarchical Mining for Complex Imbalanced Data</h3>
<ul>
<li><strong>Authors: </strong>Yijiashun Qi, Quanchao Lu, Shiyu Dou, Xiaoxuan Sun, Muqing Li, Yankaiqi Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03803">https://arxiv.org/abs/2502.03803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03803">https://arxiv.org/pdf/2502.03803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03803]] Graph Neural Network-Driven Hierarchical Mining for Complex Imbalanced Data(https://arxiv.org/abs/2502.03803)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>This study presents a hierarchical mining framework for high-dimensional imbalanced data, leveraging a depth graph model to address the inherent performance limitations of conventional approaches in handling complex, high-dimensional data distributions with imbalanced sample representations. By constructing a structured graph representation of the dataset and integrating graph neural network (GNN) embeddings, the proposed method effectively captures global interdependencies among samples. Furthermore, a hierarchical strategy is employed to enhance the characterization and extraction of minority class feature patterns, thereby facilitating precise and robust imbalanced data mining. Empirical evaluations across multiple experimental scenarios validate the efficacy of the proposed approach, demonstrating substantial improvements over traditional methods in key performance metrics, including pattern discovery count, average support, and minority class coverage. Notably, the method exhibits superior capabilities in minority-class feature extraction and pattern correlation analysis. These findings underscore the potential of depth graph models, in conjunction with hierarchical mining strategies, to significantly enhance the efficiency and accuracy of imbalanced data analysis. This research contributes a novel computational framework for high-dimensional complex data processing and lays the foundation for future extensions to dynamically evolving imbalanced data and multi-modal data applications, thereby expanding the applicability of advanced data mining methodologies to more intricate analytical domains.</li>
</ul>

<h3>Title: Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03805">https://arxiv.org/abs/2502.03805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03805">https://arxiv.org/pdf/2502.03805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03805]] Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective(https://arxiv.org/abs/2502.03805)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have revolutionized natural language processing but face significant challenges of high storage and runtime costs, due to the transformer architecture's reliance on self-attention, particularly the large Key-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV cache size by pruning less critical entries based on attention weights remain empirical and lack formal grounding. This paper presents a formal study on identifying critical KV cache entries by analyzing attention output perturbation. Our analysis reveals that, beyond attention weights, the value states within KV entries and pretrained parameter matrices are also crucial. Based on this, we propose a perturbation-constrained selection algorithm that optimizes the worst-case output perturbation to identify critical entries. Evaluations on the Needle-in-a-Haystack test and Longbench benchmark show our algorithm enhances state-of-the-art cache eviction methods. Further empirical analysis confirms that our algorithm achieves lower output perturbations in over 92% attention heads in Llama model, thereby providing a significant improvement over existing methods.</li>
</ul>

<h3>Title: DeblurDiff: Real-World Image Deblurring with Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Lingshun Kong, Jiawei Zhang, Dongqing Zou, Jimmy Ren, Xiaohe Wu, Jiangxin Dong, Jinshan Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03810">https://arxiv.org/abs/2502.03810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03810">https://arxiv.org/pdf/2502.03810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03810]] DeblurDiff: Real-World Image Deblurring with Generative Diffusion Models(https://arxiv.org/abs/2502.03810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved significant progress in image generation. The pre-trained Stable Diffusion (SD) models are helpful for image deblurring by providing clear image priors. However, directly using a blurry image or pre-deblurred one as a conditional control for SD will either hinder accurate structure extraction or make the results overly dependent on the deblurring network. In this work, we propose a Latent Kernel Prediction Network (LKPN) to achieve robust real-world image deblurring. Specifically, we co-train the LKPN in latent space with conditional diffusion. The LKPN learns a spatially variant kernel to guide the restoration of sharp images in the latent space. By applying element-wise adaptive convolution (EAC), the learned kernel is utilized to adaptively process the input feature, effectively preserving the structural information of the input. This process thereby more effectively guides the generative process of Stable Diffusion (SD), enhancing both the deblurring efficacy and the quality of detail reconstruction. Moreover, the results at each diffusion step are utilized to iteratively estimate the kernels in LKPN to better restore the sharp latent by EAC. This iterative refinement enhances the accuracy and robustness of the deblurring process. Extensive experimental results demonstrate that the proposed method outperforms state-of-the-art image deblurring methods on both benchmark and real-world images.</li>
</ul>

<h3>Title: Privacy Risks in Health Big Data: A Systematic Literature Review</h3>
<ul>
<li><strong>Authors: </strong>Zhang Si Yuan, Manmeet Mahinderjit Singh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03811">https://arxiv.org/abs/2502.03811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03811">https://arxiv.org/pdf/2502.03811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03811]] Privacy Risks in Health Big Data: A Systematic Literature Review(https://arxiv.org/abs/2502.03811)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>The digitization of health records has greatly improved the efficiency of the healthcare system and promoted the formulation of related research and policies. However, the widespread application of advanced technologies such as electronic health records, genomic data, and wearable devices in the field of health big data has also intensified the collection of personal sensitive data, bringing serious privacy and security issues. Based on a systematic literature review (SLR), this paper comprehensively outlines the key research in the field of health big data security. By analyzing existing research, this paper explores how cutting-edge technologies such as homomorphic encryption, blockchain, federated learning, and artificial immune systems can enhance data security while protecting personal privacy. This paper also points out the current challenges and proposes a future research framework in this key area.</li>
</ul>

<h3>Title: Optimized Unet with Attention Mechanism for Multi-Scale Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xuan Li, Quanchao Lu, Yankaiqi Li, Muqing Li, Yijiashun Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03813">https://arxiv.org/abs/2502.03813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03813">https://arxiv.org/pdf/2502.03813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03813]] Optimized Unet with Attention Mechanism for Multi-Scale Semantic Segmentation(https://arxiv.org/abs/2502.03813)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is one of the core tasks in the field of computer vision, and its goal is to accurately classify each pixel in an image. The traditional Unet model achieves efficient feature extraction and fusion through an encoder-decoder structure, but it still has certain limitations when dealing with complex backgrounds, long-distance dependencies, and multi-scale targets. To this end, this paper proposes an improved Unet model combined with an attention mechanism, introduces channel attention and spatial attention modules, enhances the model's ability to focus on important features, and optimizes skip connections through a multi-scale feature fusion strategy, thereby improving the combination of global semantic information and fine-grained features. The experiment is based on the Cityscapes dataset and compared with classic models such as FCN, SegNet, DeepLabv3+, and PSPNet. The improved model performs well in terms of mIoU and pixel accuracy (PA), reaching 76.5% and 95.3% respectively. The experimental results verify the superiority of this method in dealing with complex scenes and blurred target boundaries. In addition, this paper discusses the potential of the improved model in practical applications and future expansion directions, indicating that it has broad application value in fields such as autonomous driving, remote sensing image analysis, and medical image processing.</li>
</ul>

<h3>Title: PsyPlay: Personality-Infused Role-Playing Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Tao Yang, Yuhua Zhu, Xiaojun Quan, Cong Liu, Qifan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03821">https://arxiv.org/abs/2502.03821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03821">https://arxiv.org/pdf/2502.03821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03821]] PsyPlay: Personality-Infused Role-Playing Conversational Agents(https://arxiv.org/abs/2502.03821)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The current research on Role-Playing Conversational Agents (RPCAs) with Large Language Models (LLMs) primarily focuses on imitating specific speaking styles and utilizing character backgrounds, neglecting the depiction of deeper personality traits.~In this study, we introduce personality-infused role-playing for LLM agents, which encourages agents to accurately portray their designated personality traits during dialogues. We then propose PsyPlay, a dialogue generation framework that facilitates the expression of rich personalities among multiple LLM agents. Specifically, PsyPlay enables agents to assume roles with distinct personality traits and engage in discussions centered around specific topics, consistently exhibiting their designated personality traits throughout the interactions. Validation on generated dialogue data demonstrates that PsyPlay can accurately portray the intended personality traits, achieving an overall success rate of 80.31% on GPT-3.5. Notably, we observe that LLMs aligned with positive values are more successful in portraying positive personality roles compared to negative ones. Moreover, we construct a dialogue corpus for personality-infused role-playing, called PsyPlay-Bench. The corpus, which consists of 4745 instances of correctly portrayed dialogues using PsyPlay, aims to further facilitate research in personalized role-playing and dialogue personality detection.</li>
</ul>

<h3>Title: FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large Language Model-Assisted Detection and Attribute Rebalancing</h3>
<ul>
<li><strong>Authors: </strong>Jinya Sakurai, Issei Sato</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03826">https://arxiv.org/abs/2502.03826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03826">https://arxiv.org/pdf/2502.03826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03826]] FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large Language Model-Assisted Detection and Attribute Rebalancing(https://arxiv.org/abs/2502.03826)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of Text-to-Image (T2I) models has revolutionized content creation, providing powerful tools for diverse applications ranging from artistic expression to educational material development and marketing. Despite these technological advancements, significant ethical concerns arise from these models' reliance on large-scale datasets that often contain inherent societal biases. These biases are further amplified when AI-generated content is included in training data, potentially reinforcing and perpetuating stereotypes in the generated outputs. In this paper, we introduce FairT2I, a novel framework that harnesses large language models to detect and mitigate social biases in T2I generation. Our framework comprises two key components: (1) an LLM-based bias detection module that identifies potential social biases in generated images based on text prompts, and (2) an attribute rebalancing module that fine-tunes sensitive attributes within the T2I model to mitigate identified biases. Our extensive experiments across various T2I models and datasets show that FairT2I can significantly reduce bias while maintaining high-quality image generation. We conducted both qualitative user studies and quantitative non-parametric analyses in the generated image feature space, building upon the occupational dataset introduced in the Stable Bias study. Our results show that FairT2I successfully mitigates social biases and enhances the diversity of sensitive attributes in generated images. We further demonstrate, using the P2 dataset, that our framework can detect subtle biases that are challenging for human observers to perceive, extending beyond occupation-related prompts. On the basis of these findings, we introduce a new benchmark dataset for evaluating bias in T2I models.</li>
</ul>

<h3>Title: FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything Capability for Versatile Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Guohao Huo, Ruiting Dai, Ling Shao, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03829">https://arxiv.org/abs/2502.03829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03829">https://arxiv.org/pdf/2502.03829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03829]] FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything Capability for Versatile Image Segmentation(https://arxiv.org/abs/2502.03829)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Image segmentation is a critical task in visual understanding. Convolutional Neural Networks (CNNs) are predisposed to capture high-frequency features in images, while Transformers exhibit a contrasting focus on low-frequency features. In this paper, we experimentally quantify the contrast sensitivity function of CNNs and compare it with that of the human visual system, informed by the seminal experiments of Mannos and Sakrison. Leveraging these insights, we propose the Wavelet-Guided Spectral Pooling Module (WSPM) to enhance and balance image features across the frequency domain. To further emulate the human visual system, we introduce the Frequency Domain Enhanced Receptive Field Block (FE-RFB), which integrates WSPM to extract enriched features from the frequency domain. Building on these innovations, we develop FE-UNet, a model that utilizes SAM2 as its backbone and incorporates Hiera-Large as a pre-trained block, designed to enhance generalization capabilities while ensuring high segmentation accuracy. Experimental results demonstrate that FE-UNet achieves state-of-the-art performance in diverse tasks, including marine animal and polyp segmentation, underscoring its versatility and effectiveness.</li>
</ul>

<h3>Title: Single-Domain Generalized Object Detection by Balancing Domain Diversity and Invariance</h3>
<ul>
<li><strong>Authors: </strong>Zhenwei He, Hongsu Ni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03835">https://arxiv.org/abs/2502.03835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03835">https://arxiv.org/pdf/2502.03835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03835]] Single-Domain Generalized Object Detection by Balancing Domain Diversity and Invariance(https://arxiv.org/abs/2502.03835)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Single-domain generalization for object detection (S-DGOD) aims to transfer knowledge from a single source domain to unseen target domains. In recent years, many models have focused primarily on achieving feature invariance to enhance robustness. However, due to the inherent diversity across domains, an excessive emphasis on invariance can cause the model to overlook the actual differences between images. This overemphasis may complicate the training process and lead to a loss of valuable information. To address this issue, we propose the Diversity Invariance Detection Model (DIDM), which focuses on the balance between the diversity of domain-specific and invariance cross domains. Recognizing that domain diversity introduces variations in domain-specific features, we introduce a Diversity Learning Module (DLM). The DLM is designed to preserve the diversity of domain-specific information with proposed feature diversity loss while limiting the category semantics in the features. In addition, to maintain domain invariance, we incorporate a Weighted Aligning Module (WAM), which aligns features without compromising feature diversity. We conducted our model on five distinct datasets, which have illustrated the superior performance and effectiveness of the proposed model.</li>
</ul>

<h3>Title: Adapting Human Mesh Recovery with Vision-Language Feedback</h3>
<ul>
<li><strong>Authors: </strong>Chongyang Xu, Buzhen Huang, Chengfang Zhang, Ziliang Feng, Yangang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03836">https://arxiv.org/abs/2502.03836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03836">https://arxiv.org/pdf/2502.03836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03836]] Adapting Human Mesh Recovery with Vision-Language Feedback(https://arxiv.org/abs/2502.03836)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human mesh recovery can be approached using either regression-based or optimization-based methods. Regression models achieve high pose accuracy but struggle with model-to-image alignment due to the lack of explicit 2D-3D correspondences. In contrast, optimization-based methods align 3D models to 2D observations but are prone to local minima and depth ambiguity. In this work, we leverage large vision-language models (VLMs) to generate interactive body part descriptions, which serve as implicit constraints to enhance 3D perception and limit the optimization space. Specifically, we formulate monocular human mesh recovery as a distribution adaptation task by integrating both 2D observations and language descriptions. To bridge the gap between text and 3D pose signals, we first train a text encoder and a pose VQ-VAE, aligning texts to body poses in a shared latent space using contrastive learning. Subsequently, we employ a diffusion-based framework to refine the initial parameters guided by gradients derived from both 2D observations and text descriptions. Finally, the model can produce poses with accurate 3D perception and image consistency. Experimental results on multiple benchmarks validate its effectiveness. The code will be made publicly available.</li>
</ul>

<h3>Title: Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Lin Yuan, Jun Xu, Honghao Gui, Mengshu Sun, Zhiqiang Zhang, Lei Liang, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03843">https://arxiv.org/abs/2502.03843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03843">https://arxiv.org/pdf/2502.03843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03843]] Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis(https://arxiv.org/abs/2502.03843)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>High-quality, large-scale instructions are crucial for aligning large language models (LLMs), however, there is a severe shortage of instruction in the field of natural language understanding (NLU). Previous works on constructing NLU instructions mainly focus on information extraction (IE), neglecting tasks such as machine reading comprehension, question answering, and text classification. Furthermore, the lack of diversity in the data has led to a decreased generalization ability of trained LLMs in other NLU tasks and a noticeable decline in the fundamental model's general capabilities. To address this issue, we propose Hum, a large-scale, high-quality synthetic instruction corpus for NLU tasks, designed to enhance the NLU capabilities of LLMs. Specifically, Hum includes IE (either close IE or open IE), machine reading comprehension, text classification, and instruction generalist tasks, thereby enriching task diversity. Additionally, we introduce a human-LLMs collaborative mechanism to synthesize instructions, which enriches instruction diversity by incorporating guidelines, preference rules, and format variants. We conduct extensive experiments on 5 NLU tasks and 28 general capability evaluation datasets for LLMs. Experimental results show that Hum enhances the NLU capabilities of six LLMs by an average of 3.1\%, with no significant decline observed in other general capabilities.</li>
</ul>

<h3>Title: Taking A Closer Look at Interacting Objects: Interaction-Aware Open Vocabulary Scene Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Lin Li, Chuhan Zhang, Dong Zhang, Chong Sun, Chen Li, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03856">https://arxiv.org/abs/2502.03856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03856">https://arxiv.org/pdf/2502.03856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03856]] Taking A Closer Look at Interacting Objects: Interaction-Aware Open Vocabulary Scene Graph Generation(https://arxiv.org/abs/2502.03856)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Today's open vocabulary scene graph generation (OVSGG) extends traditional SGG by recognizing novel objects and relationships beyond predefined categories, leveraging the knowledge from pre-trained large-scale models. Most existing methods adopt a two-stage pipeline: weakly supervised pre-training with image captions and supervised fine-tuning (SFT) on fully annotated scene graphs. Nonetheless, they omit explicit modeling of interacting objects and treat all objects equally, resulting in mismatched relation pairs. To this end, we propose an interaction-aware OVSGG framework INOVA. During pre-training, INOVA employs an interaction-aware target generation strategy to distinguish interacting objects from non-interacting ones. In SFT, INOVA devises an interaction-guided query selection tactic to prioritize interacting objects during bipartite graph matching. Besides, INOVA is equipped with an interaction-consistent knowledge distillation to enhance the robustness by pushing interacting object pairs away from the background. Extensive experiments on two benchmarks (VG and GQA) show that INOVA achieves state-of-the-art performance, demonstrating the potential of interaction-aware mechanisms for real-world applications.</li>
</ul>

<h3>Title: BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation</h3>
<ul>
<li><strong>Authors: </strong>Bo Pang, Hanze Dong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou, Caiming Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03860">https://arxiv.org/abs/2502.03860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03860">https://arxiv.org/pdf/2502.03860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03860]] BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation(https://arxiv.org/abs/2502.03860)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.</li>
</ul>

<h3>Title: Time-based GNSS attack detection</h3>
<ul>
<li><strong>Authors: </strong>Marco Spanghero, Panos Papadimitratos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03868">https://arxiv.org/abs/2502.03868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03868">https://arxiv.org/pdf/2502.03868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03868]] Time-based GNSS attack detection(https://arxiv.org/abs/2502.03868)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack</a></li>
<li><strong>Abstract: </strong>To safeguard Civilian Global Navigation Satellite Systems (GNSS) external information available to the platform encompassing the GNSS receiver can be used to detect attacks. Cross-checking the GNSS-provided time against alternative multiple trusted time sources can lead to attack detection aiming at controlling the GNSS receiver time. Leveraging external, network-connected secure time providers and onboard clock references, we achieve detection even under fine-grained time attacks. We provide an extensive evaluation of our multi-layered defense against adversaries mounting attacks against the GNSS receiver along with controlling the network link. We implement adversaries spanning from simplistic spoofers to advanced ones synchronized with the GNSS constellation. We demonstrate attack detection is possible in all tested cases (sharp discontinuity, smooth take-over, and coordinated network manipulation) without changes to the structure of the GNSS receiver. Leveraging the diversity of the reference time sources, detection of take-over time push as low as 150us is possible. Smooth take-overs forcing variations as low as 30ns are also detected based on on-board precision oscillators. The method (and thus the evaluation) is largely agnostic to the satellite constellation and the attacker type, making time-based data validation of GNSS information compatible with existing receivers and readily deployable.</li>
</ul>

<h3>Title: Consumer INS Coupled with Carrier Phase Measurements for GNSS Spoofing Detection</h3>
<ul>
<li><strong>Authors: </strong>Tore Johansson, Marco Spanghero, Panos Papadimitratos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03870">https://arxiv.org/abs/2502.03870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03870">https://arxiv.org/pdf/2502.03870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03870]] Consumer INS Coupled with Carrier Phase Measurements for GNSS Spoofing Detection(https://arxiv.org/abs/2502.03870)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Global Navigation Satellite Systems enable precise localization and timing even for highly mobile devices, but legacy implementations provide only limited support for the new generation of security-enhanced signals. Inertial Measurement Units have proved successful in augmenting the accuracy and robustness of the GNSS-provided navigation solution, but effective navigation based on inertial techniques in denied contexts requires high-end sensors. However, commercially available mobile devices usually embed a much lower-grade inertial system. To counteract an attacker transmitting all the adversarial signals from a single antenna, we exploit carrier phase-based observations coupled with a low-end inertial sensor to identify spoofing and meaconing. By short-time integration with an inertial platform, which tracks the displacement of the GNSS antenna, the high-frequency movement at the receiver is correlated with the variation in the carrier phase. In this way, we identify legitimate transmitters, based on their geometrical diversity with respect to the antenna system movement. We introduce a platform designed to effectively compare different tiers of commercial INS platforms with a GNSS receiver. By characterizing different inertial sensors, we show that simple MEMS INS perform as well as high-end industrial-grade sensors. Sensors traditionally considered unsuited for navigation purposes offer great performance at the short integration times used to evaluate the carrier phase information consistency against the high-frequency movement. Results from laboratory evaluation and through field tests at Jammertest 2024 show that the detector is up to 90% accurate in correctly identifying spoofing (or the lack of it), without any modification to the receiver structure, and with mass-production grade INS typical for mobile phones.</li>
</ul>

<h3>Title: Hierarchical Entropic Diffusion for Ransomware Detection: A Probabilistic Approach to Behavioral Anomaly Isolation</h3>
<ul>
<li><strong>Authors: </strong>Vasili Iskorohodov, Maximilian Ravensdale, Matthias von Holstein, Hugo Petrovic, Adrian Yardley</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03882">https://arxiv.org/abs/2502.03882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03882">https://arxiv.org/pdf/2502.03882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03882]] Hierarchical Entropic Diffusion for Ransomware Detection: A Probabilistic Approach to Behavioral Anomaly Isolation(https://arxiv.org/abs/2502.03882)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>The increasing complexity of cryptographic extortion techniques has necessitated the development of adaptive detection frameworks capable of identifying adversarial encryption behaviors without reliance on predefined signatures. Hierarchical Entropic Diffusion (HED) introduces a structured entropy-based anomaly classification mechanism that systematically tracks fluctuations in entropy evolution to differentiate between benign cryptographic processes and unauthorized encryption attempts. The integration of hierarchical clustering, entropy profiling, and probabilistic diffusion modeling refines detection granularity, ensuring that encryption anomalies are identified despite obfuscation strategies or incremental execution methodologies. Experimental evaluations demonstrated that HED maintained high classification accuracy across diverse ransomware families, outperforming traditional heuristic-based and signature-driven approaches while reducing false positive occurrences. Comparative analysis highlighted that entropy-driven anomaly segmentation improved detection efficiency under variable system workload conditions, ensuring real-time classification feasibility. The computational overhead associated with entropy anomaly detection remained within operational constraints, reinforcing the suitability of entropy-driven classification for large-scale deployment. The ability to identify adversarial entropy manipulations before encryption completion contributes to broader cybersecurity defenses, offering a structured methodology for isolating unauthorized cryptographic activities within heterogeneous computing environments. The results further emphasized that entropy evolution modeling facilitates predictive anomaly detection, enhancing resilience against encryption evasion techniques designed to circumvent traditional detection mechanisms.</li>
</ul>

<h3>Title: Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Peizhuang Cong, Wenpu Liu, Wenhan Yu, Haochen Zhao, Tong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03884">https://arxiv.org/abs/2502.03884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03884">https://arxiv.org/pdf/2502.03884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03884]] Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning(https://arxiv.org/abs/2502.03884)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable success across various tasks, accompanied by a continuous increase in their parameter size. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address the challenges of fine-tuning LLMs by significantly reducing the number of trainable parameters. Recent studies have integrated LoRA with Mixture of Experts (MoE) architectures, leveraging multiple adapter experts and gating mechanisms to further improve fine-tuning performance. However, existing approaches primarily focus on adjusting the allocations of adapter experts per layer to optimize the introduced trainable parameter size, while neglecting a critical factor of adapters' rank. To this end, we propose a hierarchical scheme for expert allocation and rank configuration, HILO, which dynamically adjusts the number and rank of adapter experts across layers, matching the varying representational complexity of model layers in adapter-granularity. Extensive experiments on multiple benchmark tasks demonstrate that HILO outperforms existing methods in accuracy while introducing fewer trainable parameters, providing an efficient and practical solution for fine-tuning LLMs.</li>
</ul>

<h3>Title: Rule-Based Modeling of Low-Dimensional Data with PCA and Binary Particle Swarm Optimization (BPSO) in ANFIS</h3>
<ul>
<li><strong>Authors: </strong>Afnan Al-Ali, Uvais Qidwai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03895">https://arxiv.org/abs/2502.03895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03895">https://arxiv.org/pdf/2502.03895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03895]] Rule-Based Modeling of Low-Dimensional Data with PCA and Binary Particle Swarm Optimization (BPSO) in ANFIS(https://arxiv.org/abs/2502.03895)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Fuzzy rule-based systems interpret data in low-dimensional domains, providing transparency and interpretability. In contrast, deep learning excels in complex tasks like image and speech recognition but is prone to overfitting in sparse, unstructured, or low-dimensional data. This interpretability is crucial in fields like healthcare and finance. Traditional rule-based systems, especially ANFIS with grid partitioning, suffer from exponential rule growth as dimensionality increases. We propose a strategic rule-reduction model that applies Principal Component Analysis (PCA) on normalized firing strengths to obtain linearly uncorrelated components. Binary Particle Swarm Optimization (BPSO) selectively refines these components, significantly reducing the number of rules while preserving precision in decision-making. A custom parameter update mechanism fine-tunes specific ANFIS layers by dynamically adjusting BPSO parameters, avoiding local minima. We validated our approach on standard UCI respiratory, keel classification, regression datasets, and a real-world ischemic stroke dataset, demonstrating adaptability and practicality. Results indicate fewer rules, shorter training, and high accuracy, underscoring the methods effectiveness for low-dimensional interpretability and complex data scenarios. This synergy of fuzzy logic and optimization fosters robust solutions. Our method contributes a powerful framework for interpretable AI in multiple domains. It addresses dimensionality, ensuring a rule base.</li>
</ul>

<h3>Title: LeAP: Consistent multi-domain 3D labeling using Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Simon Gebraad, Andras Palffy, Holger Caesar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03901">https://arxiv.org/abs/2502.03901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03901">https://arxiv.org/pdf/2502.03901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03901]] LeAP: Consistent multi-domain 3D labeling using Foundation Models(https://arxiv.org/abs/2502.03901)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Availability of datasets is a strong driver for research on 3D semantic understanding, and whilst obtaining unlabeled 3D point cloud data is straightforward, manually annotating this data with semantic labels is time-consuming and costly. Recently, Vision Foundation Models (VFMs) enable open-set semantic segmentation on camera images, potentially aiding automatic labeling. However,VFMs for 3D data have been limited to adaptations of 2D models, which can introduce inconsistencies to 3D labels. This work introduces Label Any Pointcloud (LeAP), leveraging 2D VFMs to automatically label 3D data with any set of classes in any kind of application whilst ensuring label consistency. Using a Bayesian update, point labels are combined into voxels to improve spatio-temporal consistency. A novel 3D Consistency Network (3D-CN) exploits 3D information to further improve label quality. Through various experiments, we show that our method can generate high-quality 3D semantic labels across diverse fields without any manual labeling. Further, models adapted to new domains using our labels show up to a 34.2 mIoU increase in semantic segmentation tasks.</li>
</ul>

<h3>Title: No Free Lunch in Annotation either: An objective evaluation of foundation models for streamlining annotation in animal tracking</h3>
<ul>
<li><strong>Authors: </strong>Emil Mededovic, Valdy Laurentius, Yuli Wu, Marcin Kopaczka, Zhu Chen, Mareike Schulz, Ren√© Tolba, Johannes Stegmaier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03907">https://arxiv.org/abs/2502.03907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03907">https://arxiv.org/pdf/2502.03907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03907]] No Free Lunch in Annotation either: An objective evaluation of foundation models for streamlining annotation in animal tracking(https://arxiv.org/abs/2502.03907)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We analyze the capabilities of foundation models addressing the tedious task of generating annotations for animal tracking. Annotating a large amount of data is vital and can be a make-or-break factor for the robustness of a tracking model. Robustness is particularly crucial in animal tracking, as accurate tracking over long time horizons is essential for capturing the behavior of animals. However, generating additional annotations using foundation models can be counterproductive, as the quality of the annotations is just as important. Poorly annotated data can introduce noise and inaccuracies, ultimately compromising the performance and accuracy of the trained model. Over-reliance on automated annotations without ensuring precision can lead to diminished results, making careful oversight and quality control essential in the annotation process. Ultimately, we demonstrate that a thoughtful combination of automated annotations and manually annotated data is a valuable strategy, yielding an IDF1 score of 80.8 against blind usage of SAM2 video with an IDF1 score of 65.6.</li>
</ul>

<h3>Title: Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software</h3>
<ul>
<li><strong>Authors: </strong>Andreas Baumann, Peter Eberhard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03916">https://arxiv.org/abs/2502.03916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03916">https://arxiv.org/pdf/2502.03916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03916]] Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software(https://arxiv.org/abs/2502.03916)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly helpful in text generation, even writing code in programming languages based on user prompts written in natural language. They are even applied to generate simulation models for multibody systems from natural language. Research results suggest that LLMs surpass the mere replication of existing code examples, where some LLMs have been trained on an open-source multibody simulation code. However, for closed-source simulation software, such results are not to be expected as their ideas and concepts might differ from other publicly available ones. LLMs can hallucinate for knowledge-intensive tasks, such as model creation, which can lead to wrong responses. This is especially the case for the LLM unknown closed-source simulation software. The same applies to other internal knowledge kept private to protect intellectual property or data privacy. The Retrieval-Augmented Generation (RAG) approach might yield a solution for these knowledge-intensive tasks. This paper explores the application of RAG to closed-source simulation software and presents first experiments. After a brief introduction to LLMs, the RAG approach, and the simulation method applied by the close-source simulation software, several examples are provided to test LLMs' knowledge of the simulation software and the creation of simulation models using two RAG systems. The examples show promising results indicating the benefits of applying RAG systems to closed-source simulation software, helping to access their knowledge. Nevertheless, they also reveal gaps in the applied information and open questions for further research.</li>
</ul>

<h3>Title: HEP-JEPA: A foundation model for collider physics using joint embedding predictive architecture</h3>
<ul>
<li><strong>Authors: </strong>Jai Bardhan, Radhikesh Agrawal, Abhiram Tilak, Cyrin Neeraj, Subhadip Mitra</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex, hep-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03933">https://arxiv.org/abs/2502.03933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03933">https://arxiv.org/pdf/2502.03933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03933]] HEP-JEPA: A foundation model for collider physics using joint embedding predictive architecture(https://arxiv.org/abs/2502.03933)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a transformer architecture-based foundation model for tasks at high-energy particle colliders such as the Large Hadron Collider. We train the model to classify jets using a self-supervised strategy inspired by the Joint Embedding Predictive Architecture. We use the JetClass dataset containing 100M jets of various known particles to pre-train the model with a data-centric approach -- the model uses a fraction of the jet constituents as the context to predict the embeddings of the unseen target constituents. Our pre-trained model fares well with other datasets for standard classification benchmark tasks. We test our model on two additional downstream tasks: top tagging and differentiating light-quark jets from gluon jets. We also evaluate our model with task-specific metrics and baselines and compare it with state-of-the-art models in high-energy physics. Project site: this https URL</li>
</ul>

<h3>Title: Multimodal Data-Driven Classification of Mental Disorders: A Comprehensive Approach to Diagnosing Depression, Anxiety, and Schizophrenia</h3>
<ul>
<li><strong>Authors: </strong>Himanshi Singh, Sadhana Tiwari, Sonali Agarwal, Ritesh Chandra, Sanjay Kumar Sonbhadra, Vrijendra Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03943">https://arxiv.org/abs/2502.03943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03943">https://arxiv.org/pdf/2502.03943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03943]] Multimodal Data-Driven Classification of Mental Disorders: A Comprehensive Approach to Diagnosing Depression, Anxiety, and Schizophrenia(https://arxiv.org/abs/2502.03943)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study investigates the potential of multimodal data integration, which combines electroencephalogram (EEG) data with sociodemographic characteristics like age, sex, education, and intelligence quotient (IQ), to diagnose mental diseases like schizophrenia, depression, and anxiety. Using Apache Spark and convolutional neural networks (CNNs), a data-driven classification pipeline has been developed for big data environment to effectively analyze massive datasets. In order to evaluate brain activity and connection patterns associated with mental disorders, EEG parameters such as power spectral density (PSD) and coherence are examined. The importance of coherence features is highlighted by comparative analysis, which shows significant improvement in classification accuracy and robustness. This study emphasizes the significance of holistic approaches for efficient diagnostic tools by integrating a variety of data sources. The findings open the door for creative, data-driven approaches to treating psychiatric diseases by demonstrating the potential of utilizing big data, sophisticated deep learning methods, and multimodal datasets to enhance the precision, usability, and comprehension of mental health diagnostics.</li>
</ul>

<h3>Title: Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Mardhiyah Sanni, Tassallah Abdullahi, Devendra D. Kayande, Emmanuel Ayodele, Naome A. Etori, Michael S. Mollel, Moshood Yekini, Chibuzor Okocha, Lukman E. Ismaila, Folafunmi Omofoye, Boluwatife A. Adewale, Tobi Olatunji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03945">https://arxiv.org/abs/2502.03945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03945">https://arxiv.org/pdf/2502.03945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03945]] Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond(https://arxiv.org/abs/2502.03945)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speech technologies are transforming interactions across various sectors, from healthcare to call centers and robots, yet their performance on African-accented conversations remains underexplored. We introduce Afrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical African-accented English conversations, designed to evaluate automatic speech recognition (ASR) and related technologies. We assess state-of-the-art (SOTA) speaker diarization and ASR systems on long-form, accented speech, comparing their performance with native accents and discover a 10%+ performance degradation. Additionally, we explore medical conversation summarization capabilities of large language models (LLMs) to demonstrate the impact of ASR errors on downstream medical summaries, providing insights into the challenges and opportunities for speech technologies in the Global South. Our work highlights the need for more inclusive datasets to advance conversational AI in low-resource settings.</li>
</ul>

<h3>Title: CleanSurvival: Automated data preprocessing for time-to-event models using reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Yousef Koka, David Selby, Gerrit Gro√ümann, Sebastian Vollmer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03946">https://arxiv.org/abs/2502.03946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03946">https://arxiv.org/pdf/2502.03946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03946]] CleanSurvival: Automated data preprocessing for time-to-event models using reinforcement learning(https://arxiv.org/abs/2502.03946)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Data preprocessing is a critical yet frequently neglected aspect of machine learning, often paid little attention despite its potentially significant impact on model performance. While automated machine learning pipelines are starting to recognize and integrate data preprocessing into their solutions for classification and regression tasks, this integration is lacking for more specialized tasks like survival or time-to-event models. As a result, survival analysis not only faces the general challenges of data preprocessing but also suffers from the lack of tailored, automated solutions in this area. To address this gap, this paper presents 'CleanSurvival', a reinforcement-learning-based solution for optimizing preprocessing pipelines, extended specifically for survival analysis. The framework can handle continuous and categorical variables, using Q-learning to select which combination of data imputation, outlier detection and feature extraction techniques achieves optimal performance for a Cox, random forest, neural network or user-supplied time-to-event model. The package is available on GitHub: this https URL Experimental benchmarks on real-world datasets show that the Q-learning-based data preprocessing results in superior predictive performance to standard approaches, finding such a model up to 10 times faster than undirected random grid search. Furthermore, a simulation study demonstrates the effectiveness in different types and levels of missingness and noise in the data.</li>
</ul>

<h3>Title: LR0.FM: Low-Resolution Zero-shot Classification Benchmark For Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Priyank Pathak, Shyam Marjit, Shruti Vyas, Yogesh S Rawat</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03950">https://arxiv.org/abs/2502.03950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03950">https://arxiv.org/pdf/2502.03950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03950]] LR0.FM: Low-Resolution Zero-shot Classification Benchmark For Foundation Models(https://arxiv.org/abs/2502.03950)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual-language foundation Models (FMs) exhibit remarkable zero-shot generalization across diverse tasks, largely attributed to extensive pre-training on large-scale datasets. However, their robustness on low-resolution/pixelated (LR) images, a common challenge in real-world scenarios, remains underexplored. We introduce this http URL, a comprehensive benchmark evaluating the impact of low resolution on the zero-shot classification performance of 10 FM(s) across 66 backbones and 15 datasets. We propose a novel metric, Weighted Aggregated Robustness, to address the limitations of existing metrics and better evaluate model performance across resolutions and datasets. Our key findings show that: (i) model size positively correlates with robustness to resolution degradation, (ii) pre-training dataset quality is more important than its size, and (iii) fine-tuned and higher-resolution models are less robust against LR. Our analysis further reveals that the model makes semantically reasonable predictions at LR, and the lack of fine-grained details in input adversely impacts the model's initial layers more than the deeper layers. We use these insights and introduce a simple strategy, LR-TK0, to enhance the robustness of models without compromising their pre-trained weights. We demonstrate the effectiveness of LR-TK0 for robustness against low-resolution across several datasets and its generalization capability across backbones and other approaches. Code is available at this this https URL</li>
</ul>

<h3>Title: MAQInstruct: Instruction-based Unified Event Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jun Xu, Mengshu Sun, Zhiqiang Zhang, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03954">https://arxiv.org/abs/2502.03954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03954">https://arxiv.org/pdf/2502.03954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03954]] MAQInstruct: Instruction-based Unified Event Relation Extraction(https://arxiv.org/abs/2502.03954)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Extracting event relations that deviate from known schemas has proven challenging for previous methods based on multi-class classification, MASK prediction, or prototype matching. Recent advancements in large language models have shown impressive performance through instruction tuning. Nevertheless, in the task of event relation extraction, instruction-based methods face several challenges: there are a vast number of inference samples, and the relations between events are non-sequential. To tackle these challenges, we present an improved instruction-based event relation extraction framework named MAQInstruct. Firstly, we transform the task from extracting event relations using given event-event instructions to selecting events using given event-relation instructions, which reduces the number of samples required for inference. Then, by incorporating a bipartite matching loss, we reduce the dependency of the instruction-based method on the generation sequence. Our experimental results demonstrate that MAQInstruct significantly improves the performance of event relation extraction across multiple LLMs.</li>
</ul>

<h3>Title: Non-convex composite federated learning with heterogeneous data</h3>
<ul>
<li><strong>Authors: </strong>Jiaojiao Zhang, Jiang Hu, Mikael Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03958">https://arxiv.org/abs/2502.03958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03958">https://arxiv.org/pdf/2502.03958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03958]] Non-convex composite federated learning with heterogeneous data(https://arxiv.org/abs/2502.03958)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>We propose an innovative algorithm for non-convex composite federated learning that decouples the proximal operator evaluation and the communication between server and clients. Moreover, each client uses local updates to communicate less frequently with the server, sends only a single d-dimensional vector per communication round, and overcomes issues with client drift. In the analysis, challenges arise from the use of decoupling strategies and local updates in the algorithm, as well as from the non-convex and non-smooth nature of the problem. We establish sublinear and linear convergence to a bounded residual error under general non-convexity and the proximal Polyak-Lojasiewicz inequality, respectively. In the numerical experiments, we demonstrate the superiority of our algorithm over state-of-the-art methods on both synthetic and real datasets.</li>
</ul>

<h3>Title: Innovative Framework for Early Estimation of Mental Disorder Scores to Enable Timely Interventions</h3>
<ul>
<li><strong>Authors: </strong>Himanshi Singh, Sadhana Tiwari, Sonali Agarwal, Ritesh Chandra, Sanjay Kumar Sonbhadra, Vrijendra Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03965">https://arxiv.org/abs/2502.03965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03965">https://arxiv.org/pdf/2502.03965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03965]] Innovative Framework for Early Estimation of Mental Disorder Scores to Enable Timely Interventions(https://arxiv.org/abs/2502.03965)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Individual's general well-being is greatly impacted by mental health conditions including depression and Post-Traumatic Stress Disorder (PTSD), underscoring the importance of early detection and precise diagnosis in order to facilitate prompt clinical intervention. An advanced multimodal deep learning system for the automated classification of PTSD and depression is presented in this paper. Utilizing textual and audio data from clinical interview datasets, the method combines features taken from both modalities by combining the architectures of LSTM (Long Short Term Memory) and BiLSTM (Bidirectional Long Short-Term Memory).Although text features focus on speech's semantic and grammatical components; audio features capture vocal traits including rhythm, tone, and pitch. This combination of modalities enhances the model's capacity to identify minute patterns connected to mental health conditions. Using test datasets, the proposed method achieves classification accuracies of 92% for depression and 93% for PTSD, outperforming traditional unimodal approaches and demonstrating its accuracy and robustness.</li>
</ul>

<h3>Title: MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>YoonJe Kang, Yonghoon Jung, Wonseop Shin, Bumsoo Kim, Sanghyun Seo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03966">https://arxiv.org/abs/2502.03966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03966">https://arxiv.org/pdf/2502.03966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03966]] MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation(https://arxiv.org/abs/2502.03966)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we present synthetic data generation framework for flood hazard detection system. For high fidelity and quality, we characterize several real-world properties into virtual world and simulate the flood situation by controlling them. For the sake of efficiency, recent generative models in image-to-3D and urban city synthesis are leveraged to easily composite flood environments so that we avoid data bias due to the hand-crafted manner. Based on our framework, we build the flood synthetic dataset with 5 levels, dubbed MultiFloodSynth which contains rich annotation types like normal map, segmentation, 3D bounding box for a variety of downstream task. In experiments, our dataset demonstrate the enhanced performance of flood hazard detection with on-par realism compared with real dataset.</li>
</ul>

<h3>Title: Tight Bounds on Jensen's Gap: Novel Approach with Applications in Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Marcin Mazur, Piotr Ko≈õcielniak, ≈Åukasz Struski</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03988">https://arxiv.org/abs/2502.03988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03988">https://arxiv.org/pdf/2502.03988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03988]] Tight Bounds on Jensen's Gap: Novel Approach with Applications in Generative Modeling(https://arxiv.org/abs/2502.03988)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Among various mathematical tools of particular interest are those that provide a common basis for researchers in different scientific fields. One of them is Jensen's inequality, which states that the expectation of a convex function is greater than or equal to the function evaluated at the expectation. The resulting difference, known as Jensen's gap, became the subject of investigation by both the statistical and machine learning communities. Among many related topics, finding lower and upper bounds on Jensen's gap (under different assumptions on the underlying function and distribution) has recently become a problem of particular interest. In our paper, we take another step in this direction by providing a novel general and mathematically rigorous technique, motivated by the recent results of Struski et al. (2023). In addition, by studying in detail the case of the logarithmic function and the log-normal distribution, we explore a method for tightly estimating the log-likelihood of generative models trained on real-world datasets. Furthermore, we present both analytical and experimental arguments in support of the superiority of our approach in comparison to existing state-of-the-art solutions, contingent upon fulfillment of the criteria set forth by theoretical studies and corresponding experiments on synthetic data.</li>
</ul>

<h3>Title: Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Longquan Jiang, Junbo Huang, Cedric M√∂ller, Ricardo Usbeck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03992">https://arxiv.org/abs/2502.03992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03992">https://arxiv.org/pdf/2502.03992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03992]] Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering(https://arxiv.org/abs/2502.03992)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Most existing Knowledge Graph Question Answering (KGQA) approaches are designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the heterogeneity of the underlying graph schema, topology and assertions, most KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without resource-intensive training data. We present OntoSCPrompt, a novel Large Language Model (LLM)-based KGQA approach with a two-stage architecture that separates semantic parsing from KG-dependent interactions. OntoSCPrompt first generates a SPARQL query structure (including SPARQL keywords such as SELECT, ASK, WHERE and placeholders for missing tokens) and then fills them with KG-specific information. To enhance the understanding of the underlying KG, we present an ontology-guided, hybrid prompt learning strategy that integrates KG ontology into the learning process of hybrid prompts (e.g., discrete and continuous vectors). We also present several task-specific decoding strategies to ensure the correctness and executability of generated SPARQL queries in both stages. Experimental results demonstrate that OntoSCPrompt performs as well as SOTA approaches without retraining on a number of KGQA datasets such as CWQ, WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code: \href{this https URL}{this https URL}</li>
</ul>

<h3>Title: CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing</h3>
<ul>
<li><strong>Authors: </strong>Yu Yuan, Shizhao Sun, Qi Liu, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03997">https://arxiv.org/abs/2502.03997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03997">https://arxiv.org/pdf/2502.03997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03997]] CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing(https://arxiv.org/abs/2502.03997)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Computer Aided Design (CAD) is indispensable across various industries. \emph{Text-based CAD editing}, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce \emph{CAD-Editor}, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively.</li>
</ul>

<h3>Title: Online Learning of Counter Categories and Ratings in PvP Games</h3>
<ul>
<li><strong>Authors: </strong>Chiu-Chou Lin, I-Chen Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.03998">https://arxiv.org/abs/2502.03998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.03998">https://arxiv.org/pdf/2502.03998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.03998]] Online Learning of Counter Categories and Ratings in PvP Games(https://arxiv.org/abs/2502.03998)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In competitive games, strength ratings like Elo are widely used to quantify player skill and support matchmaking by accounting for skill disparities better than simple win rate statistics. However, scalar ratings cannot handle complex intransitive relationships, such as counter strategies seen in Rock-Paper-Scissors. To address this, recent work introduced Neural Rating Table and Neural Counter Table, which combine scalar ratings with discrete counter categories to model intransitivity. While effective, these methods rely on neural network training and cannot perform real-time updates. In this paper, we propose an online update algorithm that extends Elo principles to incorporate real-time learning of counter categories. Our method dynamically adjusts both ratings and counter relationships after each match, preserving the explainability of scalar ratings while addressing intransitivity. Experiments on zero-sum competitive games demonstrate its practicality, particularly in scenarios without complex team compositions.</li>
</ul>

<h3>Title: A Critical Analysis of Deployed Use Cases for Quantum Key Distribution and Comparison with Post-Quantum Cryptography</h3>
<ul>
<li><strong>Authors: </strong>Nick Aquina, Bruno Cimoli, Soumya Das, Kathrin H√∂velmanns, Fiona Johanna Weber, Chigo Okonkwo, Simon Rommel, Boris ≈†koriƒá, Idelfonso Tafur Monroy, Sebastian Verschoor</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04009">https://arxiv.org/abs/2502.04009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04009">https://arxiv.org/pdf/2502.04009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04009]] A Critical Analysis of Deployed Use Cases for Quantum Key Distribution and Comparison with Post-Quantum Cryptography(https://arxiv.org/abs/2502.04009)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Quantum Key Distribution (QKD) is currently being discussed as a technology to safeguard communication in a future where quantum computers compromise traditional public-key cryptosystems. In this paper, we conduct a comprehensive security evaluation of QKD-based solutions, focusing on real-world use cases sourced from academic literature and industry reports. We analyze these use cases, assess their security and identify the possible advantages of deploying QKD-based solutions. We further compare QKD-based solutions with Post-Quantum Cryptography (PQC), the alternative approach to achieving security when quantum computers compromise traditional public-key cryptosystems, evaluating their respective suitability for each scenario. Based on this comparative analysis, we critically discuss and comment on which use cases QKD is suited for, considering factors such as implementation complexity, scalability, and long-term security. Our findings contribute to a better understanding of the role QKD could play in future cryptographic infrastructures and offer guidance to decision-makers considering the deployment of QKD.</li>
</ul>

<h3>Title: Quantification of Biodiversity from Historical Survey Text with LLM-based Best-Worst Scaling</h3>
<ul>
<li><strong>Authors: </strong>Thomas Haider, Tobias Perschl, Malte Rehbein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04022">https://arxiv.org/abs/2502.04022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04022">https://arxiv.org/pdf/2502.04022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04022]] Quantification of Biodiversity from Historical Survey Text with LLM-based Best-Worst Scaling(https://arxiv.org/abs/2502.04022)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In this study, we evaluate methods to determine the frequency of species via quantity estimation from historical survey text. To that end, we formulate classification tasks and finally show that this problem can be adequately framed as a regression task using Best-Worst Scaling (BWS) with Large Language Models (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the latter two have reasonable agreement with humans and each other. We conclude that this approach is more cost-effective and similarly robust compared to a fine-grained multi-class approach, allowing automated quantity estimation across species.</li>
</ul>

<h3>Title: Generalize Drug Response Prediction by Latent Independent Projection for Asymmetric Constrained Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Ran Song, Yinpu Bai, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04034">https://arxiv.org/abs/2502.04034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04034">https://arxiv.org/pdf/2502.04034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04034]] Generalize Drug Response Prediction by Latent Independent Projection for Asymmetric Constrained Domain Generalization(https://arxiv.org/abs/2502.04034)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed panCancerDR, to address this challenge. We conceptualize each cancer type as a distinct source domain, with its cell lines serving as domain-specific samples. Our primary objective is to extract domain-invariant features from the expression profiles of cell lines across diverse cancer types, thereby generalize the predictive capacity to out-of-distribution samples. To enhance robustness, we introduce a latent independence projection (LIP) module that encourages the encoder to extract informative yet non-redundant features. Also, we propose an asymmetric adaptive clustering constraint, which clusters drug-sensitive samples into a compact group while drives resistant samples dispersed across separate clusters in the latent space. Our empirical experiments demonstrate that panCancerDR effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type during training. Furthermore, when evaluated on single-cell and patient-level prediction tasks, our model-trained solely on in vitro cell line data without access to target-domain information-consistently outperforms and matched current state-of-the-art methods. These findings highlights the potential of our method for real-world clinical applications.</li>
</ul>

<h3>Title: Exploring Imbalanced Annotations for Effective In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongfu Gao, Feipeng Zhang, Hao Zeng, Deyu Meng, Bingyi Jing, Hongxin Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04037">https://arxiv.org/abs/2502.04037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04037">https://arxiv.org/pdf/2502.04037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04037]] Exploring Imbalanced Annotations for Effective In-Context Learning(https://arxiv.org/abs/2502.04037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown impressive performance on downstream tasks through in-context learning (ICL), which heavily relies on the demonstrations selected from annotated datasets. Existing selection methods may hinge on the distribution of annotated datasets, which can often be long-tailed in real-world scenarios. In this work, we show that imbalanced class distributions in annotated datasets significantly degrade the performance of ICL across various tasks and selection methods. Moreover, traditional rebalance methods fail to ameliorate the issue of class imbalance in ICL. Our method is motivated by decomposing the distributional differences between annotated and test datasets into two-component weights: class-wise weights and conditional bias. The key idea behind our method is to estimate the conditional bias by minimizing the empirical error on a balanced validation dataset and to employ the two-component weights to modify the original scoring functions during selection. Our approach can prevent selecting too many demonstrations from a single class while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of our method, improving the average accuracy by up to 5.46 on common benchmarks with imbalanced datasets.</li>
</ul>

<h3>Title: Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Minhao Cheng, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04040">https://arxiv.org/abs/2502.04040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04040">https://arxiv.org/pdf/2502.04040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04040]] Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment(https://arxiv.org/abs/2502.04040)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Training safe LLMs is one of the most critical research challenge. However, the commonly used method, Refusal Training (RT), struggles to generalize against various OOD jailbreaking attacks. Many safety training methods have been proposed to address this issue. While they offer valuable insights, we aim to complement this line of research by investigating whether OOD attacks truly exceed the capability of RT model. Conducting evaluation with BoN, we observe significant improvements on generalization as N increases. This underscores that the model possesses sufficient safety-related latent knowledge, but RT fails to consistently elicit this knowledge when addressing OOD attacks. Further analysis based on domain adaptation reveals that training with direct refusal causes model to rely on superficial shortcuts, resulting in learning of non-robust representation mappings. Based on our findings, we propose training model to perform safety reasoning for each query. Reasoning supervision encourages model to perform more computations, explicitly eliciting and using latent knowledge through reasoning. To achieve this, we synthesize reasoning supervision based on pre-guidelines, training the model to reason in alignment with them, thereby effectively eliciting and utilizing latent knowledge from diverse perspectives. Extensive experiments show that our method significantly improves generalization performance against OOD attacks.</li>
</ul>

<h3>Title: Comparing privacy notions for protection against reconstruction attacks in machine learning</h3>
<ul>
<li><strong>Authors: </strong>Sayan Biswas, Mark Dras, Pedro Faustini, Natasha Fernandes, Annabelle McIver, Catuscia Palamidessi, Parastoo Sadeghi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04045">https://arxiv.org/abs/2502.04045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04045">https://arxiv.org/pdf/2502.04045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04045]] Comparing privacy notions for protection against reconstruction attacks in machine learning(https://arxiv.org/abs/2502.04045)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, federate, fair</a></li>
<li><strong>Abstract: </strong>Within the machine learning community, reconstruction attacks are a principal concern and have been identified even in federated learning (FL), which was designed with privacy preservation in mind. In response to these threats, the privacy community recommends the use of differential privacy (DP) in the stochastic gradient descent algorithm, termed DP-SGD. However, the proliferation of variants of DP in recent years\textemdash such as metric privacy\textemdash has made it challenging to conduct a fair comparison between different mechanisms due to the different meanings of the privacy parameters $\epsilon$ and $\delta$ across different variants. Thus, interpreting the practical implications of $\epsilon$ and $\delta$ in the FL context and amongst variants of DP remains ambiguous. In this paper, we lay a foundational framework for comparing mechanisms with differing notions of privacy guarantees, namely $(\epsilon,\delta)$-DP and metric privacy. We provide two foundational means of comparison: firstly, via the well-established $(\epsilon,\delta)$-DP guarantees, made possible through the R√©nyi differential privacy framework; and secondly, via Bayes' capacity, which we identify as an appropriate measure for reconstruction threats.</li>
</ul>

<h3>Title: PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Aleksandar Cvejic (KAUST), Abdelrahman Eldesokey (KAUST), Peter Wonka (KAUST)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04050">https://arxiv.org/abs/2502.04050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04050">https://arxiv.org/pdf/2502.04050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04050]] PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models(https://arxiv.org/abs/2502.04050)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present the first text-based image editing approach for object parts based on pre-trained diffusion models. Diffusion-based image editing approaches capitalized on the deep understanding of diffusion models of image semantics to perform a variety of edits. However, existing diffusion models lack sufficient understanding of many object parts, hindering fine-grained edits requested by users. To address this, we propose to expand the knowledge of pre-trained diffusion models to allow them to understand various object parts, enabling them to perform fine-grained edits. We achieve this by learning special textual tokens that correspond to different object parts through an efficient token optimization process. These tokens are optimized to produce reliable localization masks at each inference step to localize the editing region. Leveraging these masks, we design feature-blending and adaptive thresholding strategies to execute the edits seamlessly. To evaluate our approach, we establish a benchmark and an evaluation protocol for part editing. Experiments show that our approach outperforms existing editing methods on all metrics and is preferred by users 77-90% of the time in conducted user studies.</li>
</ul>

<h3>Title: Decision Trees That Remember: Gradient-Based Learning of Recurrent Decision Trees with Memory</h3>
<ul>
<li><strong>Authors: </strong>Sascha Marton, Moritz Schneider</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04052">https://arxiv.org/abs/2502.04052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04052">https://arxiv.org/pdf/2502.04052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04052]] Decision Trees That Remember: Gradient-Based Learning of Recurrent Decision Trees with Memory(https://arxiv.org/abs/2502.04052)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Neural architectures such as Recurrent Neural Networks (RNNs), Transformers, and State-Space Models have shown great success in handling sequential data by learning temporal dependencies. Decision Trees (DTs), on the other hand, remain a widely used class of models for structured tabular data but are typically not designed to capture sequential patterns directly. Instead, DT-based approaches for time-series data often rely on feature engineering, such as manually incorporating lag features, which can be suboptimal for capturing complex temporal dependencies. To address this limitation, we introduce ReMeDe Trees, a novel recurrent DT architecture that integrates an internal memory mechanism, similar to RNNs, to learn long-term dependencies in sequential data. Our model learns hard, axis-aligned decision rules for both output generation and state updates, optimizing them efficiently via gradient descent. We provide a proof-of-concept study on synthetic benchmarks to demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: TQ-DiT: Efficient Time-Aware Quantization for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Younghye Hwang, Hyojin Lee, Joonhyuk Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04056">https://arxiv.org/abs/2502.04056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04056">https://arxiv.org/pdf/2502.04056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04056]] TQ-DiT: Efficient Time-Aware Quantization for Diffusion Transformers(https://arxiv.org/abs/2502.04056)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Diffusion transformers (DiTs) combine transformer architectures with diffusion models. However, their computational complexity imposes significant limitations on real-time applications and sustainability of AI systems. In this study, we aim to enhance the computational efficiency through model quantization, which represents the weights and activation values with lower precision. Multi-region quantization (MRQ) is introduced to address the asymmetric distribution of network values in DiT blocks by allocating two scaling parameters to sub-regions. Additionally, time-grouping quantization (TGQ) is proposed to reduce quantization error caused by temporal variation in activations. The experimental results show that the proposed algorithm achieves performance comparable to the original full-precision model with only a 0.29 increase in FID at W8A8. Furthermore, it outperforms other baselines at W6A6, thereby confirming its suitability for low-bit quantization. These results highlight the potential of our method to enable efficient real-time generative models.</li>
</ul>

<h3>Title: Smart IoT Security: Lightweight Machine Learning Techniques for Multi-Class Attack Detection in IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Shahran Rahman Alve, Muhammad Zawad Mahmud, Samiha Islam, Md. Asaduzzaman Chowdhury, Jahirul Islam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04057">https://arxiv.org/abs/2502.04057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04057">https://arxiv.org/pdf/2502.04057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04057]] Smart IoT Security: Lightweight Machine Learning Techniques for Multi-Class Attack Detection in IoT Networks(https://arxiv.org/abs/2502.04057)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>In the growing terrain of the Internet of Things (IoT), it is vital that networks are secure to protect against a range of cyber threats. Based on the strong machine learning framework, this study proposes novel lightweight ensemble approaches for improving multi-class attack detection of IoT devices. Using the large CICIoT 2023 dataset with 34 attack types distributed amongst 10 attack categories, we systematically evaluated the performance of a wide variety of modern machine learning methods with the aim of establishing the best-performing algorithmic choice to secure IoT applications. In particular, we explore approaches based on ML classifiers to tackle the biocharges characterized by the challenging and heterogeneous nature of attack vectors in IoT environments. The method that performed best was the Decision Tree, with an accuracy of 99.56% and an F1 score of 99.62%, showing that this model is capable of accurately and reliably detecting this http URL Random Forest model was the next best-performing model with 98.22% and an F1 score of 98.24%, suggesting that ML methods are quite effective in a situation of high-dimensional data. Our results highlight the potential for using ML classifiers in bolstering security for IoT devices and also serve as motivations for future investigations targeting scalable, keystroke-based attack detection systems. We believe that our method provides a new path to develop complex machine learning algorithms for low-resource IoT devices, balancing both accuracy and time efficiency needs. In summary, these contributions enrich the state of the art of the IoT security literature, laying down solid ground and guidelines for the deployment of smart, adaptive security in IoT settings.</li>
</ul>

<h3>Title: Inteligencia artificial para la multi-clasificaci\'on de fauna en fotograf\'ias autom\'aticas utilizadas en investigaci\'on cient\'ifica</h3>
<ul>
<li><strong>Authors: </strong>Federico Gonzalez, Leonel Viera, Rosina Soler, Lucila Chiarvetto Peralta, Matias Gel, Gimena Bustamante, Abril Montaldo, Brian Rigoni, Ignacio Perez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04064">https://arxiv.org/abs/2502.04064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04064">https://arxiv.org/pdf/2502.04064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04064]] Inteligencia artificial para la multi-clasificaci\'on de fauna en fotograf\'ias autom\'aticas utilizadas en investigaci\'on cient\'ifica(https://arxiv.org/abs/2502.04064)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>The management of natural environments, whether for conservation or production, requires a deep understanding of wildlife. The number, location, and behavior of wild animals are among the main subjects of study in ecology and wildlife research. The use of camera traps offers the opportunity to quickly collect large quantities of photographs that capture wildlife in its natural habitat, avoiding factors that could alter their behavior. In Tierra del Fuego, Argentina, research is being conducted on forest use by different herbivores (guanacos, cows, sheep) to optimize management and protect these natural ecosystems. Although camera traps allow for the collection of millions of images, interpreting such photographs presents a scalability challenge for manual processing. As a result, much of the valuable knowledge stored in these vast data repositories remains untapped. Neural Networks and Deep Learning are areas of study within Artificial Intelligence. Over the past decade, these two disciplines have made significant contributions to image recognition on a global scale. Ecological and wildlife conservation studies can be combined with these new technologies to extract important information from the photographs obtained by camera traps, contributing to the understanding of various natural processes and improving the management of the involved wild areas. Our project aims to develop neural network models to classify animal species in photographs taken with camera traps, addressing large-scale challenges in scientific research.</li>
</ul>

<h3>Title: Predicting Large Language Model Capabilities on Closed-Book QA Tasks Using Only Information Available Prior to Training</h3>
<ul>
<li><strong>Authors: </strong>Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04066">https://arxiv.org/abs/2502.04066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04066">https://arxiv.org/pdf/2502.04066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04066]] Predicting Large Language Model Capabilities on Closed-Book QA Tasks Using Only Information Available Prior to Training(https://arxiv.org/abs/2502.04066)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The GPT-4 technical report from OpenAI suggests that model performance on specific tasks can be predicted prior to training, though methodologies remain unspecified. This approach is crucial for optimizing resource allocation and ensuring data alignment with target tasks. To achieve this vision, we focus on predicting performance on Closed-book Question Answering (CBQA) tasks, which are closely tied to pre-training data and knowledge retention. We address three major challenges: 1) mastering the entire pre-training process, especially data construction; 2) evaluating a model's knowledge retention; and 3) predicting task-specific knowledge retention using only information available prior to training. To tackle these challenges, we pre-train three large language models (i.e., 1.6B, 7B, and 13B) using 560k dollars and 520k GPU hours. We analyze the pre-training data with knowledge triples and assess knowledge retention using established methods. Additionally, we introduce the SMI metric, an information-theoretic measure that quantifies the relationship between pre-training data, model size, and task-specific knowledge retention. Our experiments reveal a strong linear correlation ($\text{R}^2 > 0.84$) between the SMI metric and the model's accuracy on CBQA tasks across models of varying sizes (i.e., 1.1B, 1.6B, 7B, and 13B). The dataset, model, and code are available at this https URL.</li>
</ul>

<h3>Title: AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04077">https://arxiv.org/abs/2502.04077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04077">https://arxiv.org/pdf/2502.04077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04077]] AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference(https://arxiv.org/abs/2502.04077)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16$\times$ KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.</li>
</ul>

<h3>Title: Automatic quantification of breast cancer biomarkers from multiple 18F-FDG PET image segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tewele W. Tareke (1), Neree Payan (1,2), Alexandre Cochet (1,2), Laurent Arnould (3), Benoit Presles (1), Jean-Marc Vrigneaud (1,2), Fabrice Meriaudeau (1), Alain Lalande (1,4) ((1) ICMUB laboratory, UMR CNRS 6302, Universite de Bourgogne Europe, Dijon, France, (2) Nuclear Medicine Department, Centre Georges-Francois Leclerc, Dijon, France, (3) Department of Biology and Pathology of the Tumors, Centre Georges-Francois Leclerc, Dijon, France, (4) Department of Medical Imaging, University Hospital of Dijon, Dijon, France)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04083">https://arxiv.org/abs/2502.04083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04083">https://arxiv.org/pdf/2502.04083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04083]] Automatic quantification of breast cancer biomarkers from multiple 18F-FDG PET image segmentation(https://arxiv.org/abs/2502.04083)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Neoadjuvant chemotherapy (NAC) has become a standard clinical practice for tumor downsizing in breast cancer with 18F-FDG Positron Emission Tomography (PET). Our work aims to leverage PET imaging for the segmentation of breast lesions. The focus is on developing an automated system that accurately segments primary tumor regions and extracts key biomarkers from these areas to provide insights into the evolution of breast cancer following the first course of NAC. 243 baseline 18F-FDG PET scans (PET_Bl) and 180 follow-up 18F-FDG PET scans (PET_Fu) were acquired before and after the first course of NAC, respectively. Firstly, a deep learning-based breast tumor segmentation method was developed. The optimal baseline model (model trained on baseline exams) was fine-tuned on 15 follow-up exams and adapted using active learning to segment tumor areas in PET_Fu. The pipeline computes biomarkers such as maximum standardized uptake value (SUVmax), metabolic tumor volume (MTV), and total lesion glycolysis (TLG) to evaluate tumor evolution between PET_Fu and PET_Bl. Quality control measures were employed to exclude aberrant outliers. The nnUNet deep learning model outperformed in tumor segmentation on PET_Bl, achieved a Dice similarity coefficient (DSC) of 0.89 and a Hausdorff distance (HD) of 3.52 mm. After fine-tuning, the model demonstrated a DSC of 0.78 and a HD of 4.95 mm on PET_Fu exams. Biomarkers analysis revealed very strong correlations whatever the biomarker between manually segmented and automatically predicted regions. The significant average decrease of SUVmax, MTV and TLG were 5.22, 11.79 cm3 and 19.23 cm3, respectively. The presented approach demonstrates an automated system for breast tumor segmentation from 18F-FDG PET. Thanks to the extracted biomarkers, our method enables the automatic assessment of cancer progression.</li>
</ul>

<h3>Title: LLMs to Support a Domain Specific Knowledge Assistant</h3>
<ul>
<li><strong>Authors: </strong>Maria-Flavia Lovin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04095">https://arxiv.org/abs/2502.04095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04095">https://arxiv.org/pdf/2502.04095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04095]] LLMs to Support a Domain Specific Knowledge Assistant(https://arxiv.org/abs/2502.04095)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work presents a custom approach to developing a domain specific knowledge assistant for sustainability reporting using the International Financial Reporting Standards (IFRS). In this domain, there is no publicly available question-answer dataset, which has impeded the development of a high-quality chatbot to support companies with IFRS reporting. The two key contributions of this project therefore are: (1) A high-quality synthetic question-answer (QA) dataset based on IFRS sustainability standards, created using a novel generation and evaluation pipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse QA pairs that address a wide spectrum of potential user queries in sustainability reporting. Various LLM-based techniques are employed to create the dataset, including chain-of-thought reasoning and few-shot prompting. A custom evaluation framework is developed to assess question and answer quality across multiple dimensions, including faithfulness, relevance, and domain specificity. The dataset averages a score range of 8.16 out of 10 on these metrics. (2) Two architectures for question-answering in the sustainability reporting domain - a RAG pipeline and a fully LLM-based pipeline. The architectures are developed by experimenting, fine-tuning, and training on the QA dataset. The final pipelines feature an LLM fine-tuned on domain specific data and an industry classification component to improve the handling of complex queries. The RAG architecture achieves an accuracy of 85.32% on single-industry and 72.15% on cross-industry multiple-choice questions, outperforming the baseline approach by 4.67 and 19.21 percentage points, respectively. The LLM-based pipeline achieves an accuracy of 93.45% on single-industry and 80.30% on cross-industry multiple-choice questions, an improvement of 12.80 and 27.36 percentage points over the baseline, respectively.</li>
</ul>

<h3>Title: Efficient Few-Shot Continual Learning in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aristeidis Panos, Rahaf Aljundi, Daniel Olmeda Reino, Richard E. Turner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04098">https://arxiv.org/abs/2502.04098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04098">https://arxiv.org/pdf/2502.04098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04098]] Efficient Few-Shot Continual Learning in Vision-Language Models(https://arxiv.org/abs/2502.04098)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) excel in tasks such as visual question answering and image captioning. However, VLMs are often limited by their use of pretrained image encoders, like CLIP, leading to image understanding errors that hinder overall performance. On top of that, real-world applications often require the model to be continuously adapted as new and often limited data continuously arrive. To address this, we propose LoRSU (Low-Rank Adaptation with Structured Updates), a robust and computationally efficient method for selectively updating image encoders within VLMs. LoRSU introduces structured and localized parameter updates, effectively correcting performance on previously error-prone data while preserving the model's general robustness. Our approach leverages theoretical insights to identify and update only the most critical parameters, achieving significant resource efficiency. Specifically, we demonstrate that LoRSU reduces computational overhead by over 25x compared to full VLM updates, without sacrificing performance. Experimental results on VQA tasks in the few-shot continual learning setting, validate LoRSU's scalability, efficiency, and effectiveness, making it a compelling solution for image encoder adaptation in resource-constrained environments.</li>
</ul>

<h3>Title: The Gradient Puppeteer: Adversarial Domination in Gradient Leakage Attacks through Model Poisoning</h3>
<ul>
<li><strong>Authors: </strong>Kunlan Xiang, Haomiao Yang, Meng Hao, Haoxin Wang, Shaofeng Li, Zikang Ding, Tianwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04106">https://arxiv.org/abs/2502.04106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04106">https://arxiv.org/pdf/2502.04106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04106]] The Gradient Puppeteer: Adversarial Domination in Gradient Leakage Attacks through Model Poisoning(https://arxiv.org/abs/2502.04106)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, steal, federate</a></li>
<li><strong>Abstract: </strong>In Federated Learning (FL), clients share gradients with a central server while keeping their data local. However, malicious servers could deliberately manipulate the models to reconstruct clients' data from shared gradients, posing significant privacy risks. Although such active gradient leakage attacks (AGLAs) have been widely studied, they suffer from several limitations including incomplete attack coverage and poor stealthiness. In this paper, we address these limitations with two core contributions. First, we introduce a new theoretical analysis approach, which uniformly models AGLAs as backdoor poisoning. This analysis approach reveals that the core principle of AGLAs is to bias the gradient space to prioritize the reconstruction of a small subset of samples while sacrificing the majority, which theoretically explains the above limitations of existing AGLAs. Second, we propose Enhanced Gradient Global Vulnerability (EGGV), the first AGLA that achieves complete attack coverage while evading client-side detection. In particular, EGGV employs a gradient projector and a jointly optimized discriminator to assess gradient vulnerability, steering the gradient space toward the point most prone to data leakage. Extensive experiments show that EGGV achieves complete attack coverage and surpasses SOTA with at least a 43% increase in reconstruction quality (PSNR) and a 45% improvement in stealthiness (D-SNR).</li>
</ul>

<h3>Title: Adaptive Margin Contrastive Learning for Ambiguity-aware 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yang Chen, Yueqi Duan, Runzhong Zhang, Yap-Peng Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04111">https://arxiv.org/abs/2502.04111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04111">https://arxiv.org/pdf/2502.04111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04111]] Adaptive Margin Contrastive Learning for Ambiguity-aware 3D Semantic Segmentation(https://arxiv.org/abs/2502.04111)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an adaptive margin contrastive learning method for 3D point cloud semantic segmentation, namely AMContrast3D. Most existing methods use equally penalized objectives, which ignore per-point ambiguities and less discriminated features stemming from transition regions. However, as highly ambiguous points may be indistinguishable even for humans, their manually annotated labels are less reliable, and hard constraints over these points would lead to sub-optimal models. To address this, we design adaptive objectives for individual points based on their ambiguity levels, aiming to ensure the correctness of low-ambiguity points while allowing mistakes for high-ambiguity points. Specifically, we first estimate ambiguities based on position embeddings. Then, we develop a margin generator to shift decision boundaries for contrastive feature embeddings, so margins are narrowed due to increasing ambiguities with even negative margins for extremely high-ambiguity points. Experimental results on large-scale datasets, S3DIS and ScanNet, demonstrate that our method outperforms state-of-the-art methods.</li>
</ul>

<h3>Title: Generative Adversarial Networks Bridging Art and Machine Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Junhao Song, Yichao Zhang, Ziqian Bi, Tianyang Wang, Keyu Chen, Ming Li, Qian Niu, Junyu Liu, Benji Peng, Sen Zhang, Ming Liu, Jiawei Xu, Xuanhe Pan, Jinlang Wang, Pohsun Feng, Yizhu Wen, Lawrence K.Q. Yan, Hong-Ming Tseng, Xinyuan Song, Jintao Ren, Silin Chen, Yunze Wang, Weiche Hsieh, Bowen Jing, Junjie Yang, Jun Zhou, Zheyu Yao, Chia Xin Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04116">https://arxiv.org/abs/2502.04116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04116">https://arxiv.org/pdf/2502.04116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04116]] Generative Adversarial Networks Bridging Art and Machine Intelligence(https://arxiv.org/abs/2502.04116)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>This book begins with a detailed introduction to the fundamental principles and historical development of GANs, contrasting them with traditional generative models and elucidating the core adversarial mechanisms through illustrative Python examples. The text systematically addresses the mathematical and theoretical underpinnings including probability theory, statistics, and game theory providing a solid framework for understanding the objectives, loss functions, and optimisation challenges inherent to GAN training. Subsequent chapters review classic variants such as Conditional GANs, DCGANs, InfoGAN, and LAPGAN before progressing to advanced training methodologies like Wasserstein GANs, GANs with gradient penalty, least squares GANs, and spectral normalisation techniques. The book further examines architectural enhancements and task-specific adaptations in generators and discriminators, showcasing practical implementations in high resolution image generation, artistic style transfer, video synthesis, text to image generation and other multimedia applications. The concluding sections offer insights into emerging research trends, including self-attention mechanisms, transformer-based generative models, and a comparative analysis with diffusion models, thus charting promising directions for future developments in both academic and applied settings.</li>
</ul>

<h3>Title: The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bryan Guan, Tanya Roosta, Peyman Passban, Mehdi Rezagholizadeh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04134">https://arxiv.org/abs/2502.04134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04134">https://arxiv.org/pdf/2502.04134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04134]] The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs(https://arxiv.org/abs/2502.04134)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become integral to diverse applications, ensuring their reliability under varying input conditions is crucial. One key issue affecting this reliability is order sensitivity, wherein slight variations in input arrangement can lead to inconsistent or biased outputs. Although recent advances have reduced this sensitivity, the problem remains unresolved. This paper investigates the extent of order sensitivity in closed-source LLMs by conducting experiments across multiple tasks, including paraphrasing, relevance judgment, and multiple-choice questions. Our results show that input order significantly affects performance across tasks, with shuffled inputs leading to measurable declines in output accuracy. Few-shot prompting demonstrates mixed effectiveness and offers partial mitigation, however, fails to fully resolve the problem. These findings highlight persistent risks, particularly in high-stakes applications, and point to the need for more robust LLMs or improved input-handling techniques in future development.</li>
</ul>

<h3>Title: Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Lu, Jiacheng Deng, Tianzhu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04139">https://arxiv.org/abs/2502.04139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04139">https://arxiv.org/pdf/2502.04139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04139]] Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation(https://arxiv.org/abs/2502.04139)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>3D instance segmentation aims to predict a set of object instances in a scene and represent them as binary foreground masks with corresponding semantic labels. Currently, transformer-based methods are gaining increasing attention due to their elegant pipelines, reduced manual selection of geometric properties, and superior performance. However, transformer-based methods fail to simultaneously maintain strong position and content information during query initialization. Additionally, due to supervision at each decoder layer, there exists a phenomenon of object disappearance with the deepening of layers. To overcome these hurdles, we introduce Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation (BFL). Specifically, an Agent-Interpolation Initialization Module is designed to generate resilient queries capable of achieving a balance between foreground coverage and content learning. Additionally, a Hierarchical Query Fusion Decoder is designed to retain low overlap queries, mitigating the decrease in recall with the deepening of layers. Extensive experiments on ScanNetV2, ScanNet200, ScanNet++ and S3DIS datasets demonstrate the superior performance of BFL.</li>
</ul>

<h3>Title: HD-EPIC: A Highly-Detailed Egocentric Video Dataset</h3>
<ul>
<li><strong>Authors: </strong>Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, Jacob Chalk, Zhifan Zhu, Rhodri Guerrier, Fahd Abdelazim, Bin Zhu, Davide Moltisanti, Michael Wray, Hazel Doughty, Dima Damen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04144">https://arxiv.org/abs/2502.04144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04144">https://arxiv.org/pdf/2502.04144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04144]] HD-EPIC: A Highly-Detailed Egocentric Video Dataset(https://arxiv.org/abs/2502.04144)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present a validation dataset of newly-collected kitchen-based egocentric videos, manually annotated with highly detailed and interconnected ground-truth labels covering: recipe steps, fine-grained actions, ingredients with nutritional values, moving objects, and audio annotations. Importantly, all annotations are grounded in 3D through digital twinning of the scene, fixtures, object locations, and primed with gaze. Footage is collected from unscripted recordings in diverse home environments, making HDEPIC the first dataset collected in-the-wild but with detailed annotations matching those in controlled lab environments. We show the potential of our highly-detailed annotations through a challenging VQA benchmark of 26K questions assessing the capability to recognise recipes, ingredients, nutrition, fine-grained actions, 3D perception, object motion, and gaze direction. The powerful long-context Gemini Pro only achieves 38.5% on this benchmark, showcasing its difficulty and highlighting shortcomings in current VLMs. We additionally assess action recognition, sound recognition, and long-term video-object segmentation on HD-EPIC. HD-EPIC is 41 hours of video in 9 kitchens with digital twins of 413 kitchen fixtures, capturing 69 recipes, 59K fine-grained actions, 51K audio events, 20K object movements and 37K object masks lifted to 3D. On average, we have 263 annotations per minute of our unscripted videos.</li>
</ul>

<h3>Title: UltraIF: Advancing Instruction Following from the Wild</h3>
<ul>
<li><strong>Authors: </strong>Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, Baobao Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04153">https://arxiv.org/abs/2502.04153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04153">https://arxiv.org/pdf/2502.04153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04153]] UltraIF: Advancing Instruction Following from the Wild(https://arxiv.org/abs/2502.04153)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at this https URL.</li>
</ul>

<h3>Title: MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented Multimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Qinhan Yu, Zhiyou Xiao, Binghui Li, Zhengren Wang, Chong Chen, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04176">https://arxiv.org/abs/2502.04176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04176">https://arxiv.org/pdf/2502.04176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04176]] MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented Multimodal Generation(https://arxiv.org/abs/2502.04176)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in Retrieval-Augmented Generation (RAG) have shown remarkable performance in enhancing response accuracy and relevance by integrating external knowledge into generative models. However, existing RAG methods primarily focus on providing text-only answers, even in multimodal retrieval-augmented generation scenarios. In this work, we introduce the Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims to generate answers that combine both text and images, fully leveraging the multimodal data within a corpus. Despite the importance of this task, there is a notable absence of a comprehensive benchmark to effectively evaluate MRAMG performance. To bridge this gap, we introduce the MRAMG-Bench, a carefully curated, human-annotated dataset comprising 4,346 documents, 14,190 images, and 4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, and Lifestyle. The dataset incorporates diverse difficulty levels and complex multi-image scenarios, providing a robust foundation for evaluating multimodal generation tasks. To facilitate rigorous evaluation, our MRAMG-Bench incorporates a comprehensive suite of both statistical and LLM-based metrics, enabling a thorough analysis of the performance of popular generative models in the MRAMG task. Besides, we propose an efficient multimodal answer generation framework that leverages both LLMs and MLLMs to generate multimodal responses. Our datasets are available at: this https URL.</li>
</ul>

<h3>Title: Multi-agent Architecture Search via Agentic Supernet</h3>
<ul>
<li><strong>Authors: </strong>Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04180">https://arxiv.org/abs/2502.04180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04180">https://arxiv.org/pdf/2502.04180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04180]] Multi-agent Architecture Search via Agentic Supernet(https://arxiv.org/abs/2502.04180)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \textbf{(I)} requires only $6\sim45\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \textbf{(II)} surpasses them by $0.54\%\sim11.82\%$, and \textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.</li>
</ul>

<h3>Title: PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?</h3>
<ul>
<li><strong>Authors: </strong>Mennatullah Siam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04192">https://arxiv.org/abs/2502.04192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04192">https://arxiv.org/pdf/2502.04192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04192]] PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?(https://arxiv.org/abs/2502.04192)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Multiple works have emerged to push the boundaries on multi-modal large language models (MLLMs) towards pixel-level understanding. Such approaches have shown strong performance on benchmarks for referring expression segmentation and grounded conversation generation. The current trend in pixel-level MLLMs is to train with pixel-level grounding supervision on large-scale labelled data. However, we show that such MLLMs when evaluated on recent challenging vision centric benchmarks, exhibit a weak ability in visual question answering. Surprisingly, some of these methods even downgrade the grounding ability of MLLMs that were never trained with such supervision. In this work, we propose two novel challenging benchmarks and show that MLLMs without pixel-level grounding supervision can outperform the state of the art in such tasks when evaluating both the pixel-level grounding and visual question answering. We propose simple baselines to extract the grounding information that can be plugged into any MLLM, which we call as PixFoundation. More importantly, we study the research question of ``When does grounding emerge in MLLMs that are not trained with pixel-level grounding supervision?'' We show that grounding can coincide with object parts or location/appearance information. Code repository is at this https URL.</li>
</ul>

<h3>Title: The Best Instruction-Tuning Data are Those That Fit</h3>
<ul>
<li><strong>Authors: </strong>Dylan Zhang, Qirun Dai, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04194">https://arxiv.org/abs/2502.04194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04194">https://arxiv.org/pdf/2502.04194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04194]] The Best Instruction-Tuning Data are Those That Fit(https://arxiv.org/abs/2502.04194)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>High-quality supervised fine-tuning (SFT) data are crucial for eliciting strong capabilities from pretrained large language models (LLMs). Typically, instructions are paired with multiple responses sampled from other LLMs, which are often out of the distribution of the target model to be fine-tuned. This, at scale, can lead to diminishing returns and even hurt the models' performance and robustness. We propose **GRAPE**, a novel SFT framework that accounts for the unique characteristics of the target model. For each instruction, it gathers responses from various LLMs and selects the one with the highest probability measured by the target model, indicating that it aligns most closely with the target model's pretrained distribution; it then proceeds with standard SFT training. We first evaluate GRAPE with a controlled experiment, where we sample various solutions for each question in UltraInteract from multiple models and fine-tune commonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on GRAPE-selected data. GRAPE significantly outperforms strong baselines, including distilling from the strongest model with an absolute gain of up to 13.8%, averaged across benchmarks, and training on 3x more data with a maximum performance improvement of 17.3%. GRAPE's strong performance generalizes to realistic settings. We experiment with the post-training data used for Tulu3 and Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data by 6.1% and a state-of-the-art data selection approach by 3% on average performance. Remarkably, using 1/3 of the data and half the number of epochs, GRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.</li>
</ul>

<h3>Title: Safeguarding connected autonomous vehicle communication: Protocols, intra- and inter-vehicular attacks and defenses</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Aledhari, Rehma Razzak, Mohamed Rahouti, Abbas Yazdinejad, Reza M. Parizi, Basheer Qolomany, Mohsen Guizani, Junaid Qadir, Ala Al-Fuqaha</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04201">https://arxiv.org/abs/2502.04201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04201">https://arxiv.org/pdf/2502.04201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04201]] Safeguarding connected autonomous vehicle communication: Protocols, intra- and inter-vehicular attacks and defenses(https://arxiv.org/abs/2502.04201)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The advancements in autonomous driving technology, coupled with the growing interest from automotive manufacturers and tech companies, suggest a rising adoption of Connected Autonomous Vehicles (CAVs) in the near future. Despite some evidence of higher accident rates in AVs, these incidents tend to result in less severe injuries compared to traditional vehicles due to cooperative safety measures. However, the increased complexity of CAV systems exposes them to significant security vulnerabilities, potentially compromising their performance and communication integrity. This paper contributes by presenting a detailed analysis of existing security frameworks and protocols, focusing on intra- and inter-vehicle communications. We systematically evaluate the effectiveness of these frameworks in addressing known vulnerabilities and propose a set of best practices for enhancing CAV communication security. The paper also provides a comprehensive taxonomy of attack vectors in CAV ecosystems and suggests future research directions for designing more robust security mechanisms. Our key contributions include the development of a new classification system for CAV security threats, the proposal of practical security protocols, and the introduction of use cases that demonstrate how these protocols can be integrated into real-world CAV applications. These insights are crucial for advancing secure CAV adoption and ensuring the safe integration of autonomous vehicles into intelligent transportation systems.</li>
</ul>

<h3>Title: "Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence</h3>
<ul>
<li><strong>Authors: </strong>Shaopeng Fu, Liang Ding, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04204">https://arxiv.org/abs/2502.04204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04204">https://arxiv.org/pdf/2502.04204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04204]] "Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence(https://arxiv.org/abs/2502.04204)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks against large language models (LLMs) aim to induce harmful behaviors in LLMs through carefully crafted adversarial prompts. To mitigate attacks, one way is to perform adversarial training (AT)-based alignment, i.e., training LLMs on some of the most adversarial prompts to help them learn how to behave safely under attacks. During AT, the length of adversarial prompts plays a critical role in the robustness of aligned LLMs. This paper focuses on adversarial suffix jailbreak attacks and unveils that to defend against a jailbreak attack with an adversarial suffix of length $\Theta(M)$, it is enough to align LLMs on prompts with adversarial suffixes of length $\Theta(\sqrt{M})$. Theoretically, we analyze the adversarial in-context learning of linear transformers on linear regression tasks and prove a robust generalization bound for trained transformers. The bound depends on the term $\Theta(\sqrt{M_{\text{test}}}/M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the number of adversarially perturbed in-context samples during training and testing. Empirically, we conduct AT on popular open-source LLMs and evaluate their robustness against jailbreak attacks of different adversarial suffix lengths. Results confirm a positive correlation between the attack success rate and the ratio of the square root of the adversarial suffix during jailbreaking to the length during AT. Our findings show that it is practical to defend "long-length" jailbreak attacks via efficient "short-length" AT. The code is available at this https URL.</li>
</ul>

<h3>Title: Enhanced Feature-based Image Stitching for Endoscopic Videos in Pediatric Eosinophilic Esophagitis</h3>
<ul>
<li><strong>Authors: </strong>Juming Xiong, Muyang Li, Ruining Deng, Tianyuan Yao, Regina N Tyree, Girish Hiremath, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04207">https://arxiv.org/abs/2502.04207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04207">https://arxiv.org/pdf/2502.04207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04207]] Enhanced Feature-based Image Stitching for Endoscopic Videos in Pediatric Eosinophilic Esophagitis(https://arxiv.org/abs/2502.04207)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video endoscopy represents a major advance in the investigation of gastrointestinal diseases. Reviewing endoscopy videos often involves frequent adjustments and reorientations to piece together a complete view, which can be both time-consuming and prone to errors. Image stitching techniques address this issue by providing a continuous and complete visualization of the examined area. However, endoscopic images, particularly those of the esophagus, present unique challenges. The smooth surface, lack of distinct feature points, and non-horizontal orientation complicate the stitching process, rendering traditional feature-based methods often ineffective for these types of images. In this paper, we propose a novel preprocessing pipeline designed to enhance endoscopic image stitching through advanced computational techniques. Our approach converts endoscopic video data into continuous 2D images by following four key steps: (1) keyframe selection, (2) image rotation adjustment to correct distortions, (3) surface unwrapping using polar coordinate transformation to generate a flat image, and (4) feature point matching enhanced by Adaptive Histogram Equalization for improved feature detection. We evaluate stitching quality through the assessment of valid feature point match pairs. Experiments conducted on 20 pediatric endoscopy videos demonstrate that our method significantly improves image alignment and stitching quality compared to traditional techniques, laying a robust foundation for more effective panoramic image creation.</li>
</ul>

<h3>Title: Algorithmic causal structure emerging through compression</h3>
<ul>
<li><strong>Authors: </strong>Liang Wendong, Simon Buchholz, Bernhard Sch√∂lkopf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CC, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04210">https://arxiv.org/abs/2502.04210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04210">https://arxiv.org/pdf/2502.04210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04210]] Algorithmic causal structure emerging through compression(https://arxiv.org/abs/2502.04210)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We explore the relationship between causality, symmetry, and compression. We build on and generalize the known connection between learning and compression to a setting where causal models are not identifiable. We propose a framework where causality emerges as a consequence of compressing data across multiple environments. We define algorithmic causality as an alternative definition of causality when traditional assumptions for causal identifiability do not hold. We demonstrate how algorithmic causal and symmetric structures can emerge from minimizing upper bounds on Kolmogorov complexity, without knowledge of intervention targets. We hypothesize that these insights may also provide a novel perspective on the emergence of causality in machine learning models, such as large language models, where causal relationships may not be explicitly identifiable.</li>
</ul>

<h3>Title: Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data</h3>
<ul>
<li><strong>Authors: </strong>Laura Biester</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04218">https://arxiv.org/abs/2502.04218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04218">https://arxiv.org/pdf/2502.04218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04218]] Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data(https://arxiv.org/abs/2502.04218)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been shown to be biased in prior work, as they generate text that is in line with stereotypical views of the world or that is not representative of the viewpoints and values of historically marginalized demographic groups. In this work, we propose using data from parallel men's and women's events at the Olympic Games to investigate different forms of gender bias in language models. We define three metrics to measure bias, and find that models are consistently biased against women when the gender is ambiguous in the prompt. In this case, the model frequently retrieves only the results of the men's event with or without acknowledging them as such, revealing pervasive gender bias in LLMs in the context of athletics.</li>
</ul>

<h3>Title: \'Eclair -- Extracting Content and Layout with Integrated Reading Order for Documents</h3>
<ul>
<li><strong>Authors: </strong>Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko, Timo Roman, Jarno Sepp√§nen, Jupinder Parmar, Joseph Jennings, Andrew Tao, Karan Sapra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04223">https://arxiv.org/abs/2502.04223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04223">https://arxiv.org/pdf/2502.04223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04223]] \'Eclair -- Extracting Content and Layout with Integrated Reading Order for Documents(https://arxiv.org/abs/2502.04223)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -- including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -- as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce √âclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, √âclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. √âclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate √âclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards.</li>
</ul>

<h3>Title: Provably Robust Explainable Graph Neural Networks against Graph Perturbation Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jiate Li, Meng Pang, Yun Dong, Jinyuan Jia, Binghui Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04224">https://arxiv.org/abs/2502.04224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04224">https://arxiv.org/pdf/2502.04224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04224]] Provably Robust Explainable Graph Neural Networks against Graph Perturbation Attacks(https://arxiv.org/abs/2502.04224)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Explaining Graph Neural Network (XGNN) has gained growing attention to facilitate the trust of using GNNs, which is the mainstream method to learn graph data. Despite their growing attention, Existing XGNNs focus on improving the explanation performance, and its robustness under attacks is largely unexplored. We noticed that an adversary can slightly perturb the graph structure such that the explanation result of XGNNs is largely changed. Such vulnerability of XGNNs could cause serious issues particularly in safety/security-critical applications. In this paper, we take the first step to study the robustness of XGNN against graph perturbation attacks, and propose XGNNCert, the first provably robust XGNN. Particularly, our XGNNCert can provably ensure the explanation result for a graph under the worst-case graph perturbation attack is close to that without the attack, while not affecting the GNN prediction, when the number of perturbed edges is bounded. Evaluation results on multiple graph datasets and GNN explainers show the effectiveness of XGNNCert.</li>
</ul>

<h3>Title: Keep It Light! Simplifying Image Clustering Via Text-Free Adapters</h3>
<ul>
<li><strong>Authors: </strong>Yicen Li, Haitz S√°ez de Oc√°riz Borde, Anastasis Kratsios, Paul D. McNicholas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.NE, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04226">https://arxiv.org/abs/2502.04226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04226">https://arxiv.org/pdf/2502.04226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04226]] Keep It Light! Simplifying Image Clustering Via Text-Free Adapters(https://arxiv.org/abs/2502.04226)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many competitive clustering pipelines have a multi-modal design, leveraging large language models (LLMs) or other text encoders, and text-image pairs, which are often unavailable in real-world downstream applications. Additionally, such frameworks are generally complicated to train and require substantial computational resources, making widespread adoption challenging. In this work, we show that in deep clustering, competitive performance with more complex state-of-the-art methods can be achieved using a text-free and highly simplified training pipeline. In particular, our approach, Simple Clustering via Pre-trained models (SCP), trains only a small cluster head while leveraging pre-trained vision model feature representations and positive data pairs. Experiments on benchmark datasets including CIFAR-10, CIFAR-20, CIFAR-100, STL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly competitive performance. Furthermore, we provide a theoretical result explaining why, at least under ideal conditions, additional text-based embeddings may not be necessary to achieve strong clustering performance in vision.</li>
</ul>

<h3>Title: Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks</h3>
<ul>
<li><strong>Authors: </strong>Andreas Happe, J√ºrgen Cito</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04227">https://arxiv.org/abs/2502.04227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04227">https://arxiv.org/pdf/2502.04227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04227]] Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks(https://arxiv.org/abs/2502.04227)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>We explore the feasibility and effectiveness of using LLM-driven autonomous systems for Assumed Breach penetration testing in enterprise networks. We introduce a novel prototype that, driven by Large Language Models (LLMs), can compromise accounts within a real-life Active Directory testbed. Our research provides a comprehensive evaluation of the prototype's capabilities, and highlights both strengths and limitations while executing attack. The evaluation uses a realistic simulation environment (Game of Active Directory, GOAD) to capture intricate interactions, stochastic outcomes, and timing dependencies that characterize live network scenarios. The study concludes that autonomous LLMs are able to conduct Assumed Breach simulations, potentially democratizing access to penetration testing for organizations facing budgetary constraints. The prototype's source code, traces, and analyzed logs are released as open-source to enhance collective cybersecurity and facilitate future research in LLM-driven cybersecurity automation.</li>
</ul>

<h3>Title: Dark Distillation: Backdooring Distilled Datasets without Accessing Raw Data</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Yang, Ming Yan, Yi Zhang, Joey Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04229">https://arxiv.org/abs/2502.04229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04229">https://arxiv.org/pdf/2502.04229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04229]] Dark Distillation: Backdooring Distilled Datasets without Accessing Raw Data(https://arxiv.org/abs/2502.04229)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Dataset distillation (DD) enhances training efficiency and reduces bandwidth by condensing large datasets into smaller synthetic ones. It enables models to achieve performance comparable to those trained on the raw full dataset and has become a widely adopted method for data sharing. However, security concerns in DD remain underexplored. Existing studies typically assume that malicious behavior originates from dataset owners during the initial distillation process, where backdoors are injected into raw datasets. In contrast, this work is the first to address a more realistic and concerning threat: attackers may intercept the dataset distribution process, inject backdoors into the distilled datasets, and redistribute them to users. While distilled datasets were previously considered resistant to backdoor attacks, we demonstrate that they remain vulnerable to such attacks. Furthermore, we show that attackers do not even require access to any raw data to inject the backdoors successfully. Specifically, our approach reconstructs conceptual archetypes for each class from the model trained on the distilled dataset. Backdoors are then injected into these archetypes to update the distilled dataset. Moreover, we ensure the updated dataset not only retains the backdoor but also preserves the original optimization trajectory, thus maintaining the knowledge of the raw dataset. To achieve this, a hybrid loss is designed to integrate backdoor information along the benign optimization trajectory, ensuring that previously learned information is not forgotten. Extensive experiments demonstrate that distilled datasets are highly vulnerable to backdoor attacks, with risks pervasive across various raw datasets, distillation methods, and downstream training strategies. Moreover, our attack method is efficient, capable of synthesizing a malicious distilled dataset in under one minute in certain cases.</li>
</ul>

<h3>Title: Graph machine learning for flight delay prediction due to holding manouver</h3>
<ul>
<li><strong>Authors: </strong>Jorge L. Franco, Manoel V. Machado Neto, Filipe A. N. Verri, Diego R. Amancio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04233">https://arxiv.org/abs/2502.04233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04233">https://arxiv.org/pdf/2502.04233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04233]] Graph machine learning for flight delay prediction due to holding manouver(https://arxiv.org/abs/2502.04233)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Flight delays due to holding maneuvers are a critical and costly phenomenon in aviation, driven by the need to manage air traffic congestion and ensure safety. Holding maneuvers occur when aircraft are instructed to circle in designated airspace, often due to factors such as airport congestion, adverse weather, or air traffic control restrictions. This study models the prediction of flight delays due to holding maneuvers as a graph problem, leveraging advanced Graph Machine Learning (Graph ML) techniques to capture complex interdependencies in air traffic networks. Holding maneuvers, while crucial for safety, cause increased fuel usage, emissions, and passenger dissatisfaction, making accurate prediction essential for operational efficiency. Traditional machine learning models, typically using tabular data, often overlook spatial-temporal relations within air traffic data. To address this, we model the problem of predicting holding as edge feature prediction in a directed (multi)graph where we apply both CatBoost, enriched with graph features capturing network centrality and connectivity, and Graph Attention Networks (GATs), which excel in relational data contexts. Our results indicate that CatBoost outperforms GAT in this imbalanced dataset, effectively predicting holding events and offering interpretability through graph-based feature importance. Additionally, we discuss the model's potential operational impact through a web-based tool that allows users to simulate real-time delay predictions. This research underscores the viability of graph-based approaches for predictive analysis in aviation, with implications for enhancing fuel efficiency, reducing delays, and improving passenger experience.</li>
</ul>

<h3>Title: A Classification System Approach in Predicting Chinese Censorship</h3>
<ul>
<li><strong>Authors: </strong>Matt Prodani, Tianchu Ze, Yushen Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04234">https://arxiv.org/abs/2502.04234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04234">https://arxiv.org/pdf/2502.04234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04234]] A Classification System Approach in Predicting Chinese Censorship(https://arxiv.org/abs/2502.04234)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper is dedicated to using a classifier to predict whether a Weibo post would be censored under the Chinese internet. Through randomized sampling from \citeauthor{Fu2021} and Chinese tokenizing strategies, we constructed a cleaned Chinese phrase dataset with binary censorship markings. Utilizing various probability-based information retrieval methods on the data, we were able to derive 4 logistic regression models for classification. Furthermore, we experimented with pre-trained transformers to perform similar classification tasks. After evaluating both the macro-F1 and ROC-AUC metrics, we concluded that the Fined-Tuned BERT model exceeds other strategies in performance.</li>
</ul>

<h3>Title: MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion</h3>
<ul>
<li><strong>Authors: </strong>Xintong Hao, Ke Shen, Chenggang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04235">https://arxiv.org/abs/2502.04235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04235">https://arxiv.org/pdf/2502.04235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04235]] MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion(https://arxiv.org/abs/2502.04235)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose \textbf{MA}ssive \textbf{G}enre-\textbf{A}udience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.</li>
</ul>

<h3>Title: TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Amaan Dhamaskar, Rasika Ransing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04245">https://arxiv.org/abs/2502.04245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04245">https://arxiv.org/pdf/2502.04245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04245]] TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi(https://arxiv.org/abs/2502.04245)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>India's rich cultural and linguistic diversity poses various challenges in the domain of Natural Language Processing (NLP), particularly in Named Entity Recognition (NER). NER is a NLP task that aims to identify and classify tokens into different entity groups like Person, Location, Organization, Number, etc. This makes NER very useful for downstream tasks like context-aware anonymization. This paper details our work to build a multilingual NER model for the three most spoken languages in India - Hindi, Bengali & Marathi. We train a custom transformer model and fine tune a few pretrained models, achieving an F1 Score of 92.11 for a total of 6 entity groups. Through this paper, we aim to introduce a single model to perform NER and significantly reduce the inconsistencies in entity groups and tag names, across the three languages.</li>
</ul>

<h3>Title: Adapting to Evolving Adversaries with Regularized Continual Robust Training</h3>
<ul>
<li><strong>Authors: </strong>Sihui Dai, Christian Cianfarani, Arjun Bhagoji, Vikash Sehwag, Prateek Mittal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04248">https://arxiv.org/abs/2502.04248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04248">https://arxiv.org/pdf/2502.04248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04248]] Adapting to Evolving Adversaries with Regularized Continual Robust Training(https://arxiv.org/abs/2502.04248)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Robust training methods typically defend against specific attack types, such as Lp attacks with fixed budgets, and rarely account for the fact that defenders may encounter new attacks over time. A natural solution is to adapt the defended model to new adversaries as they arise via fine-tuning, a method which we call continual robust training (CRT). However, when implemented naively, fine-tuning on new attacks degrades robustness on previous attacks. This raises the question: how can we improve the initial training and fine-tuning of the model to simultaneously achieve robustness against previous and new attacks? We present theoretical results which show that the gap in a model's robustness against different attacks is bounded by how far each attack perturbs a sample in the model's logit space, suggesting that regularizing with respect to this logit space distance can help maintain robustness against previous attacks. Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and ImageNette) and over 100 attack combinations demonstrate that the proposed regularization improves robust accuracy with little overhead in training time. Our findings and open-source code lay the groundwork for the deployment of models robust to evolving attacks.</li>
</ul>

<h3>Title: Realistic Image-to-Image Machine Unlearning via Decoupling and Knowledge Retention</h3>
<ul>
<li><strong>Authors: </strong>Ayush K. Varshney, Vicen√ß Torra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04260">https://arxiv.org/abs/2502.04260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04260">https://arxiv.org/pdf/2502.04260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04260]] Realistic Image-to-Image Machine Unlearning via Decoupling and Knowledge Retention(https://arxiv.org/abs/2502.04260)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, fair, generative</a></li>
<li><strong>Abstract: </strong>Machine Unlearning allows participants to remove their data from a trained machine learning model in order to preserve their privacy, and security. However, the machine unlearning literature for generative models is rather limited. The literature for image-to-image generative model (I2I model) considers minimizing the distance between Gaussian noise and the output of I2I model for forget samples as machine unlearning. However, we argue that the machine learning model performs fairly well on unseen data i.e., a retrained model will be able to catch generic patterns in the data and hence will not generate an output which is equivalent to Gaussian noise. In this paper, we consider that the model after unlearning should treat forget samples as out-of-distribution (OOD) data, i.e., the unlearned model should no longer recognize or encode the specific patterns found in the forget samples. To achieve this, we propose a framework which decouples the model parameters with gradient ascent, ensuring that forget samples are OOD for unlearned model with theoretical guarantee. We also provide $(\epsilon, \delta)$-unlearning guarantee for model updates with gradient ascent. The unlearned model is further fine-tuned on the remaining samples to maintain its performance. We also propose an attack model to ensure that the unlearned model has effectively removed the influence of forget samples. Extensive empirical evaluation on two large-scale datasets, ImageNet-1K and Places365 highlights the superiority of our approach. To show comparable performance with retrained model, we also show the comparison of a simple AutoEncoder on various baselines on CIFAR-10 dataset.</li>
</ul>

<h3>Title: Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances</h3>
<ul>
<li><strong>Authors: </strong>Yi Yu, Botao Ren, Peiyuan Zhang, Mingxin Liu, Junwei Luo, Shaofeng Zhang, Feipeng Da, Junchi Yan, Xue Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04268">https://arxiv.org/abs/2502.04268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04268">https://arxiv.org/pdf/2502.04268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04268]] Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances(https://arxiv.org/abs/2502.04268)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning OOD from point annotations has gained great attention. In this paper, we rethink this challenging task setting with the layout among instances and present Point2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. 2) Voronoi watershed loss. It learns a lower bound for each instance through watershed on Voronoi tessellation. 3) Consistency loss. It learns the size/rotation variation between two output sets with respect to an input image and its augmented view. Supplemented by a few devised techniques, e.g. edge loss and copy-paste, the detector is further this http URL our best knowledge, Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M. Code is available at this https URL.</li>
</ul>

<h3>Title: PILAF: Optimal Human Preference Sampling for Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yunzhen Feng, Ariel Kwiatkowski, Kunhao Zheng, Julia Kempe, Yaqi Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04270">https://arxiv.org/abs/2502.04270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04270">https://arxiv.org/pdf/2502.04270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04270]] PILAF: Optimal Human Preference Sampling for Reward Modeling(https://arxiv.org/abs/2502.04270)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.</li>
</ul>

<h3>Title: Orthogonal Representation Learning for Estimating Causal Quantities</h3>
<ul>
<li><strong>Authors: </strong>Valentyn Melnychuk, Dennis Frauen, Jonas Schweisthal, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04274">https://arxiv.org/abs/2502.04274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04274">https://arxiv.org/pdf/2502.04274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04274]] Orthogonal Representation Learning for Estimating Causal Quantities(https://arxiv.org/abs/2502.04274)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Representation learning is widely used for estimating causal quantities (e.g., the conditional average treatment effect) from observational data. While existing representation learning methods have the benefit of allowing for end-to-end learning, they do not have favorable theoretical properties of Neyman-orthogonal learners, such as double robustness and quasi-oracle efficiency. Also, such representation learning methods often employ additional constraints, like balancing, which may even lead to inconsistent estimation. In this paper, we propose a novel class of Neyman-orthogonal learners for causal quantities defined at the representation level, which we call OR-learners. Our OR-learners have several practical advantages: they allow for consistent estimation of causal quantities based on any learned representation, while offering favorable theoretical properties including double robustness and quasi-oracle efficiency. In multiple experiments, we show that, under certain regularity conditions, our OR-learners improve existing representation learning methods and achieve state-of-the-art performance. To the best of our knowledge, our OR-learners are the first work to offer a unified framework of representation learning methods and Neyman-orthogonal learners for causal quantities estimation.</li>
</ul>

<h3>Title: DECAF: Learning to be Fair in Multi-agent Resource Allocation</h3>
<ul>
<li><strong>Authors: </strong>Ashwin Kumar, William Yeoh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04281">https://arxiv.org/abs/2502.04281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04281">https://arxiv.org/pdf/2502.04281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04281]] DECAF: Learning to be Fair in Multi-agent Resource Allocation(https://arxiv.org/abs/2502.04281)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>A wide variety of resource allocation problems operate under resource constraints that are managed by a central arbitrator, with agents who evaluate and communicate preferences over these resources. We formulate this broad class of problems as Distributed Evaluation, Centralized Allocation (DECA) problems and propose methods to learn fair and efficient policies in centralized resource allocation. Our methods are applied to learning long-term fairness in a novel and general framework for fairness in multi-agent systems. We show three different methods based on Double Deep Q-Learning: (1) A joint weighted optimization of fairness and utility, (2) a split optimization, learning two separate Q-estimators for utility and fairness, and (3) an online policy perturbation to guide existing black-box utility functions toward fair solutions. Our methods outperform existing fair MARL approaches on multiple resource allocation domains, even when evaluated using diverse fairness functions, and allow for flexible online trade-offs between utility and fairness.</li>
</ul>

<h3>Title: Breaking the Vault: A Case Study of the 2022 LastPass Data Breach</h3>
<ul>
<li><strong>Authors: </strong>Jessica Gentles, Mason Fields, Garrett Goodman, Suman Bhunia</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04287">https://arxiv.org/abs/2502.04287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04287">https://arxiv.org/pdf/2502.04287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04287]] Breaking the Vault: A Case Study of the 2022 LastPass Data Breach(https://arxiv.org/abs/2502.04287)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Managing the security of employee work computers has become increasingly important as today's work model shifts to remote and hybrid work plans. In this paper, we explore the recent 2022 LastPass data breach, in which the attacker obtained sensitive customer data by exploiting a software vulnerability on a DevSecOps engineer's computer. We discuss the methodology of the attacker as well as the impact this incident had on LastPass and its customers. Next, we expand upon the impact the breach had on LastPass as well as its customers. From this, we propose solutions for preparing for and mitigating similar attacks in the future. The aim of this paper is to shed light on the LastPass incident and provide methods for companies to secure their employee base, both nationally and internationally. With a strong security structure, companies can vastly reduce the chances of falling victim to a similar attack.</li>
</ul>

<h3>Title: Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants</h3>
<ul>
<li><strong>Authors: </strong>Fares Fourati, Salma Kharrat, Vaneet Aggarwal, Mohamed-Slim Alouini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04290">https://arxiv.org/abs/2502.04290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04290">https://arxiv.org/pdf/2502.04290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04290]] Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants(https://arxiv.org/abs/2502.04290)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Optimizing expensive, non-convex, black-box Lipschitz continuous functions presents significant challenges, particularly when the Lipschitz constant of the underlying function is unknown. Such problems often demand numerous function evaluations to approximate the global optimum, which can be prohibitive in terms of time, energy, or resources. In this work, we introduce Every Call is Precious (ECP), a novel global optimization algorithm that minimizes unpromising evaluations by strategically focusing on potentially optimal regions. Unlike previous approaches, ECP eliminates the need to estimate the Lipschitz constant, thereby avoiding additional function evaluations. ECP guarantees no-regret performance for infinite evaluation budgets and achieves minimax-optimal regret bounds within finite budgets. Extensive ablation studies validate the algorithm's robustness, while empirical evaluations show that ECP outperforms 10 benchmark algorithms including Lipschitz, Bayesian, bandits, and evolutionary methods across 30 multi-dimensional non-convex synthetic and real-world optimization problems, which positions ECP as a competitive approach for global optimization.</li>
</ul>

<h3>Title: GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Weihang Li, Hongli Xu, Junwen Huang, Hyunjun Jung, Peter KT Yu, Nassir Navab, Benjamin Busam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04293">https://arxiv.org/abs/2502.04293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04293">https://arxiv.org/pdf/2502.04293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04293]] GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation(https://arxiv.org/abs/2502.04293)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>A key challenge in model-free category-level pose estimation is the extraction of contextual object features that generalize across varying instances within a specific category. Recent approaches leverage foundational features to capture semantic and geometry cues from data. However, these approaches fail under partial visibility. We overcome this with a first-complete-then-aggregate strategy for feature extraction utilizing class priors. In this paper, we present GCE-Pose, a method that enhances pose estimation for novel instances by integrating category-level global context prior. GCE-Pose performs semantic shape reconstruction with a proposed Semantic Shape Reconstruction (SSR) module. Given an unseen partial RGB-D object instance, our SSR module reconstructs the instance's global geometry and semantics by deforming category-specific 3D semantic prototypes through a learned deep Linear Shape Model. We further introduce a Global Context Enhanced (GCE) feature fusion module that effectively fuses features from partial RGB-D observations and the reconstructed global context. Extensive experiments validate the impact of our global context prior and the effectiveness of the GCE fusion module, demonstrating that GCE-Pose significantly outperforms existing methods on challenging real-world datasets HouseCat6D and NOCS-REAL275. Our project page is available at this https URL.</li>
</ul>

<h3>Title: Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuanye Liu, Jiahang Xu, Li Lyna Zhang, Qi Chen, Xuan Feng, Yang Chen, Zhongxin Guo, Yuqing Yang, Cheng Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04295">https://arxiv.org/abs/2502.04295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04295">https://arxiv.org/pdf/2502.04295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04295]] Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization(https://arxiv.org/abs/2502.04295)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at this https URL.</li>
</ul>

<h3>Title: Statistical guarantees for continuous-time policy evaluation: blessing of ellipticity and new tradeoffs</h3>
<ul>
<li><strong>Authors: </strong>Wenlong Mou</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, math.PR, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04297">https://arxiv.org/abs/2502.04297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04297">https://arxiv.org/pdf/2502.04297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04297]] Statistical guarantees for continuous-time policy evaluation: blessing of ellipticity and new tradeoffs(https://arxiv.org/abs/2502.04297)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We study the estimation of the value function for continuous-time Markov diffusion processes using a single, discretely observed ergodic trajectory. Our work provides non-asymptotic statistical guarantees for the least-squares temporal-difference (LSTD) method, with performance measured in the first-order Sobolev norm. Specifically, the estimator attains an $O(1 / \sqrt{T})$ convergence rate when using a trajectory of length $T$; notably, this rate is achieved as long as $T$ scales nearly linearly with both the mixing time of the diffusion and the number of basis functions employed. A key insight of our approach is that the ellipticity inherent in the diffusion process ensures robust performance even as the effective horizon diverges to infinity. Moreover, we demonstrate that the Markovian component of the statistical error can be controlled by the approximation error, while the martingale component grows at a slower rate relative to the number of basis functions. By carefully balancing these two sources of error, our analysis reveals novel trade-offs between approximation and statistical errors.</li>
</ul>

<h3>Title: MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, Feng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04299">https://arxiv.org/abs/2502.04299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04299">https://arxiv.org/pdf/2502.04299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04299]] MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation(https://arxiv.org/abs/2502.04299)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.</li>
</ul>

<h3>Title: The 23andMe Data Breach: Analyzing Credential Stuffing Attacks, Security Vulnerabilities, and Mitigation Strategies</h3>
<ul>
<li><strong>Authors: </strong>Ryan Holthouse, Serena Owens, Suman Bhunia</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04303">https://arxiv.org/abs/2502.04303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04303">https://arxiv.org/pdf/2502.04303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04303]] The 23andMe Data Breach: Analyzing Credential Stuffing Attacks, Security Vulnerabilities, and Mitigation Strategies(https://arxiv.org/abs/2502.04303)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>In October 2023, 23andMe, a prominent provider of personal genetic testing, ancestry, and health information services, suffered a significant data breach orchestrated by a cybercriminal known as ``Golem.'' Initially, approximately 14,000 user accounts were compromised by a credential smear attack, exploiting reused usernames and passwords from previous data leaks. However, due to the interconnected nature of 23andMe's DNA Relatives and Family Tree features, the breach expanded exponentially, exposing sensitive personal and genetic data of approximately 5.5 million users and 1.4 million additional profiles. The attack highlights the increasing threat of credential stuffing, exacerbated by poor password hygiene and the absence of robust security measures such as multi-factor authentication (MFA) and rate limiting. In response, 23andMe mandated password resets, implemented email-based two-step verification, and advised users to update passwords across other services. This paper critically analyzes the attack methodology, its impact on users and the company, and explores potential mitigation strategies, including enhanced authentication protocols, proactive breach detection, and improved cybersecurity practices. The findings underscore the necessity of stronger user authentication measures and corporate responsibility in safeguarding sensitive genetic and personal data.</li>
</ul>

<h3>Title: ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, Bryon Aragam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04306">https://arxiv.org/abs/2502.04306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04306">https://arxiv.org/pdf/2502.04306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04306]] ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization(https://arxiv.org/abs/2502.04306)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: this https URL</li>
</ul>

<h3>Title: HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiming Huang, Tolga Birdal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04308">https://arxiv.org/abs/2502.04308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04308">https://arxiv.org/pdf/2502.04308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04308]] HOG-Diff: Higher-Order Guided Diffusion for Graph Generation(https://arxiv.org/abs/2502.04308)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Although diffusion models have recently made significant achievements in graph generation, these models typically adapt from the frameworks designed for image generation, making them ill-suited for capturing the topological properties of graphs. In this work, we propose a novel Higher-order Guided Diffusion (HOG-Diff) model that follows a coarse-to-fine generation curriculum and is guided by higher-order information, enabling the progressive generation of plausible graphs with inherent topological structures. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at this https URL.</li>
</ul>

<h3>Title: Targeted Learning for Data Fairness</h3>
<ul>
<li><strong>Authors: </strong>Alexander Asemota, Giles Hooker</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04309">https://arxiv.org/abs/2502.04309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04309">https://arxiv.org/pdf/2502.04309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04309]] Targeted Learning for Data Fairness(https://arxiv.org/abs/2502.04309)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Data and algorithms have the potential to produce and perpetuate discrimination and disparate treatment. As such, significant effort has been invested in developing approaches to defining, detecting, and eliminating unfair outcomes in algorithms. In this paper, we focus on performing statistical inference for fairness. Prior work in fairness inference has largely focused on inferring the fairness properties of a given predictive algorithm. Here, we expand fairness inference by evaluating fairness in the data generating process itself, referred to here as data fairness. We perform inference on data fairness using targeted learning, a flexible framework for nonparametric inference. We derive estimators demographic parity, equal opportunity, and conditional mutual information. Additionally, we find that our estimators for probabilistic metrics exploit double robustness. To validate our approach, we perform several simulations and apply our estimators to real data.</li>
</ul>

<h3>Title: Consistency of augmentation graph and network approximability in contrastive learning</h3>
<ul>
<li><strong>Authors: </strong>Chenghui Li, A. Martina Neuman</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AP, math.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04312">https://arxiv.org/abs/2502.04312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04312">https://arxiv.org/pdf/2502.04312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04312]] Consistency of augmentation graph and network approximability in contrastive learning(https://arxiv.org/abs/2502.04312)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contrastive learning leverages data augmentation to develop feature representation without relying on large labeled datasets. However, despite its empirical success, the theoretical foundations of contrastive learning remain incomplete, with many essential guarantees left unaddressed, particularly the realizability assumption concerning neural approximability of an optimal spectral contrastive loss solution. In this work, we overcome these limitations by analyzing the pointwise and spectral consistency of the augmentation graph Laplacian. We establish that, under specific conditions for data generation and graph connectivity, as the augmented dataset size increases, the augmentation graph Laplacian converges to a weighted Laplace-Beltrami operator on the natural data manifold. These consistency results ensure that the graph Laplacian spectrum effectively captures the manifold geometry. Consequently, they give way to a robust framework for establishing neural approximability, directly resolving the realizability assumption in a current paradigm.</li>
</ul>

<h3>Title: ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters</h3>
<ul>
<li><strong>Authors: </strong>Kamer Ali Yuksel, Hassan Sawaf</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04315">https://arxiv.org/abs/2502.04315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04315">https://arxiv.org/pdf/2502.04315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04315]] ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters(https://arxiv.org/abs/2502.04315)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have shown remarkable performance across diverse tasks. However, these models are typically deployed with fixed weights, which limits their ability to adapt dynamically to the variability inherent in real-world data during inference. This paper introduces ChamaleonLLM, a novel framework that enables inference-time adaptation of LLMs by leveraging batch-aware clustering and on-the-fly generation of low-rank updates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation (LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable masks), our method dynamically generates adaptive modifications to the decoder weights based on the aggregated statistics of clustered batches. By intelligently grouping similar inputs and computing context-aware low-rank updates via a hyper-network, ChamaleonLLM achieves significant performance gains, outperforming conventional LoRA methods while eliminating the overhead of maintaining multiple expert models. Our experiments highlight the potential of our approach to serve as a versatile and highly adaptive solution for language model inference. ChamaleonLLM is open-sourced to ensure the reproducibility of our experiments: this https URL</li>
</ul>

<h3>Title: ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</h3>
<ul>
<li><strong>Authors: </strong>Alec Helbling, Tuna Han Salih Meral, Ben Hoover, Pinar Yanardag, Duen Horng Chau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04320">https://arxiv.org/abs/2502.04320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04320">https://arxiv.org/pdf/2502.04320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04320]] ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features(https://arxiv.org/abs/2502.04320)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP.</li>
</ul>

<h3>Title: Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</h3>
<ul>
<li><strong>Authors: </strong>Yik Siu Chan, Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04322">https://arxiv.org/abs/2502.04322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04322">https://arxiv.org/pdf/2502.04322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04322]] Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions(https://arxiv.org/abs/2502.04322)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.</li>
</ul>

<h3>Title: Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.MM, cs.SD, eess.AS, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.04328">https://arxiv.org/abs/2502.04328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.04328">https://arxiv.org/pdf/2502.04328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.04328]] Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment(https://arxiv.org/abs/2502.04328)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
