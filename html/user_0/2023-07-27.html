<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: A Novel Computationally Efficient Group Signature for Anonymous and Secure V2X Communications. (arXiv:2307.13759v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13759">http://arxiv.org/abs/2307.13759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13759] A Novel Computationally Efficient Group Signature for Anonymous and Secure V2X Communications](http://arxiv.org/abs/2307.13759) #secure</code></li>
<li>Summary: <p>The use of vehicle-to-everything (V2X) communication is expected to
significantly improve road safety and traffic management. We present an
efficient protocol, called the AEE protocol, for protecting data authenticity
and user privacy in V2X applications. Our protocol provides event-based
likability, which enables messages from a subject vehicle to be linked to a
specific event in order to prevent Sybil attacks. Messages on different events
are unlinkable to preserve the long-term privacy of vehicles. Moreover, our
protocol introduces a new method for generating temporary public keys to reduce
computing and transmission overheads. Such a temporary public key is bound with
a certain event and is automatically revoked when the event is over. We
describe how to apply our protocol in vehicular communications using two
exemplar use cases. To further reduce the real-time computational complexity,
our protocol enables us to decompose the cryptographic operations into offline
processes for complex operations and real-time processes for fast computations.
</p></li>
</ul>

<h3>Title: Determining the Optimal Frequencies for a Duplicated Randomized Clock SCA Countermeasure. (arXiv:2307.13834v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13834">http://arxiv.org/abs/2307.13834</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13834] Determining the Optimal Frequencies for a Duplicated Randomized Clock SCA Countermeasure](http://arxiv.org/abs/2307.13834) #secure</code></li>
<li>Summary: <p>Side-channel attacks pose significant challenges to the security of embedded
systems, often allowing attackers to circumvent encryption algorithms in
minutes compared to the trillions of years required for brute-force attacks. To
mitigate these vulnerabilities, various countermeasures have been developed.
This study focuses on two specific countermeasures: randomization of the
encryption algorithm's clock and the incorporation of a dummy core to disguise
power traces.
</p></li>
</ul>

<p>The objective of this research is to identify the optimal frequencies that
yield the highest level of randomness when these two countermeasures are
combined. By investigating the interplay between clock randomization and the
presence of dummy cores, we aim to enhance the overall security of embedded
systems. The insights gained from this study will contribute to the development
of more robust countermeasures against side-channel attacks, bolstering the
protection of sensitive information and systems.
</p>
<p>To achieve this, we conduct simulations and perform side-channel attacks on
an FPGA to establish the relationship between frequencies and the resulting
protection. We break the encryption on a non-duplicated circuit and note the
least amount of measured power traces necessary and the timing overhead. We do
this for all sets of frequencies considered which gives a good indication of
which sets of frequencies give good protection. By comparing the frequencies
generated with those from the duplicated circuit we use similar conclusions to
prove whether a frequency set is secure or not.
</p>
<p>Based on our results we argue that having one frequency lower than half of
the base frequency and the other frequencies being close but not higher than
the base gives the highest security compared to the timing overhead measured.
</p>

<h3>Title: TeleBTC: Trustless Wrapped Bitcoin. (arXiv:2307.13848v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13848">http://arxiv.org/abs/2307.13848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13848] TeleBTC: Trustless Wrapped Bitcoin](http://arxiv.org/abs/2307.13848) #secure</code></li>
<li>Summary: <p>This paper introduces TeleBTC, a fully decentralized protocol designed to
wrap Bitcoin (BTC) on programmable blockchains. The creation of a decentralized
wrapped BTC presents challenges due to the non-programmable nature of Bitcoin,
making it difficult to custody BTCs in a decentralized way. Existing solutions
have addressed this challenge by introducing an external layer of validators
who take custody of users' BTCs. However, the security and decentralization of
this layer are inferior to the underlying blockchains on which wrapped BTC is
built. Moreover, the process of joining or leaving for a validator has become
overly complex and expensive. To overcome these limitations, we propose a novel
approach that eliminates the need for such an external layer by leveraging the
light client bridge protocol. Additionally, we employ economic mechanisms such
as incentivization and slashing, resulting in a secure and trust-minimized
wrapped BTC solution. With TeleBTC, users can seamlessly transfer their BTC to
other blockchains and utilize it within decentralized applications.
Furthermore, they can unwrap their TeleBTC and reclaim the native BTC. To
address the high costs associated with light client bridges, we present an
optimistic approach that minimizes the cost. This approach significantly
reduces the operational expenses of running the protocol.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: A Comprehensive Analysis on the Leakage of Fuzzy Matchers. (arXiv:2307.13717v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13717">http://arxiv.org/abs/2307.13717</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13717] A Comprehensive Analysis on the Leakage of Fuzzy Matchers](http://arxiv.org/abs/2307.13717) #security</code></li>
<li>Summary: <p>The present paper presents a comprehensive analysis of potential information
leakage in distance evaluation, with a specific emphasis on threshold-based
obfuscated distance (i.e. Fuzzy Matcher). It includes detailed descriptions of
various situations related to potential information leakage and specific
attention is given to their consequences on security. Generic attacks
corresponding to each scenario are outlined, and their complexities are
assessed. The main contribution of this work lies in providing an upper bound
on the security of a fuzzy matcher in scenarios where there is additional
information leakage from the matcher, providing a straightforward understanding
of the maximum level of achievable security and its potential implications for
data privacy and security.
</p></li>
</ul>

<h3>Title: AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception. (arXiv:2307.13933v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13933">http://arxiv.org/abs/2307.13933</a></li>
<li>Code URL: <a href="https://github.com/ydk122024/aide">https://github.com/ydk122024/aide</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13933] AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception](http://arxiv.org/abs/2307.13933) #security</code></li>
<li>Summary: <p>Driver distraction has become a significant cause of severe traffic accidents
over the past decade. Despite the growing development of vision-driven driver
monitoring systems, the lack of comprehensive perception datasets restricts
road safety and traffic security. In this paper, we present an AssIstive
Driving pErception dataset (AIDE) that considers context information both
inside and outside the vehicle in naturalistic scenarios. AIDE facilitates
holistic driver monitoring through three distinctive characteristics, including
multi-view settings of driver and scene, multi-modal annotations of face, body,
posture, and gesture, and four pragmatic task designs for driving
understanding. To thoroughly explore AIDE, we provide experimental benchmarks
on three kinds of baseline frameworks via extensive methods. Moreover, two
fusion strategies are introduced to give new insights into learning effective
multi-stream/modal representations. We also systematically investigate the
importance and rationality of the key components in AIDE and benchmarks. The
project link is https://github.com/ydk122024/AIDE.
</p></li>
</ul>

<h3>Title: Enhanced Security against Adversarial Examples Using a Random Ensemble of Encrypted Vision Transformer Models. (arXiv:2307.13985v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13985">http://arxiv.org/abs/2307.13985</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13985] Enhanced Security against Adversarial Examples Using a Random Ensemble of Encrypted Vision Transformer Models](http://arxiv.org/abs/2307.13985) #security</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In addition, AEs have adversarial transferability, which means
AEs generated for a source model can fool another black-box model (target
model) with a non-trivial probability. In previous studies, it was confirmed
that the vision transformer (ViT) is more robust against the property of
adversarial transferability than convolutional neural network (CNN) models such
as ConvMixer, and moreover encrypted ViT is more robust than ViT without any
encryption. In this article, we propose a random ensemble of encrypted ViT
models to achieve much more robust models. In experiments, the proposed scheme
is verified to be more robust against not only black-box attacks but also
white-box ones than convention methods.
</p></li>
</ul>

<h3>Title: Security Weaknesses in IoT Management Platforms. (arXiv:2307.13952v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13952">http://arxiv.org/abs/2307.13952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13952] Security Weaknesses in IoT Management Platforms](http://arxiv.org/abs/2307.13952) #security</code></li>
<li>Summary: <p>A diverse set of Internet of Things (IoT) devices are becoming an integrated
part of daily lives, and playing an increasingly vital role in various
industry, enterprise and agricultural settings. The current IoT ecosystem
relies on several IoT management platforms to manage and operate a large number
of IoT devices, their data, and their connectivity. Considering their key role,
these platforms must be properly secured against cyber attacks. In this work,
we first explore the core operations/features of leading platforms to design a
framework to perform a systematic security evaluation of these platforms.
Subsequently, we use our framework to analyze a representative set of 52 IoT
management platforms, including 42 web-hosted and 10 locally-deployable
platforms. We discover a number of high severity unauthorized access
vulnerabilities in 9/52 evaluated IoT management platforms, which could be
abused to perform attacks such as remote IoT SIM deactivation, IoT SIM
overcharging and IoT device data forgery. More seriously, we also uncover
instances of broken authentication in 13/52 platforms, including complete
account takeover on 8/52 platforms along with remote code execution on 2/52
platforms. In effect, 17/52 platforms were affected by vulnerabilities that
could lead to platform-wide attacks. Overall, vulnerabilities were uncovered in
33 platforms, out of which 28 platforms responded to our responsible
disclosure. We were also assigned 11 CVEs and awarded bounty for our findings.
</p></li>
</ul>

<h3>Title: Unveiling Security, Privacy, and Ethical Concerns of ChatGPT. (arXiv:2307.14192v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14192">http://arxiv.org/abs/2307.14192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14192] Unveiling Security, Privacy, and Ethical Concerns of ChatGPT](http://arxiv.org/abs/2307.14192) #security</code></li>
<li>Summary: <p>This paper delves into the realm of ChatGPT, an AI-powered chatbot that
utilizes topic modeling and reinforcement learning to generate natural
responses. Although ChatGPT holds immense promise across various industries,
such as customer service, education, mental health treatment, personal
productivity, and content creation, it is essential to address its security,
privacy, and ethical implications. By exploring the upgrade path from GPT-1 to
GPT-4, discussing the model's features, limitations, and potential
applications, this study aims to shed light on the potential risks of
integrating ChatGPT into our daily lives. Focusing on security, privacy, and
ethics issues, we highlight the challenges these concerns pose for widespread
adoption. Finally, we analyze the open problems in these areas, calling for
concerted efforts to ensure the development of secure and ethically sound large
language models.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: SplitFed resilience to packet loss: Where to split, that is the question. (arXiv:2307.13851v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13851">http://arxiv.org/abs/2307.13851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13851] SplitFed resilience to packet loss: Where to split, that is the question](http://arxiv.org/abs/2307.13851) #privacy</code></li>
<li>Summary: <p>Decentralized machine learning has broadened its scope recently with the
invention of Federated Learning (FL), Split Learning (SL), and their hybrids
like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce
the computational power required by each client in FL and parallelize SL while
maintaining privacy. This paper investigates the robustness of SFL against
packet loss on communication links. The performance of various SFL aggregation
strategies is examined by splitting the model at two points -- shallow split
and deep split -- and testing whether the split point makes a statistically
significant difference to the accuracy of the final model. Experiments are
carried out on a segmentation model for human embryo images and indicate the
statistically significant advantage of a deeper split point.
</p></li>
</ul>

<h3>Title: RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition. (arXiv:2307.14016v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14016">http://arxiv.org/abs/2307.14016</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14016] RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition](http://arxiv.org/abs/2307.14016) #privacy</code></li>
<li>Summary: <p>Palmprint recently shows great potential in recognition applications as it is
a privacy-friendly and stable biometric. However, the lack of large-scale
public palmprint datasets limits further research and development of palmprint
recognition. In this paper, we propose a novel realistic pseudo-palmprint
generation (RPG) model to synthesize palmprints with massive identities. We
first introduce a conditional modulation generator to improve the intra-class
diversity. Then an identity-aware loss is proposed to ensure identity
consistency against unpaired training. We further improve the B\'ezier palm
creases generation strategy to guarantee identity independence. Extensive
experimental results demonstrate that synthetic pretraining significantly
boosts the recognition model performance. For example, our model improves the
state-of-the-art B\'ezierPalm by more than $5\%$ and $14\%$ in terms of
TAR@FAR=1e-6 under the $1:1$ and $1:3$ Open-set protocol. When accessing only
$10\%$ of the real training data, our method still outperforms ArcFace with
$100\%$ real training data, indicating that we are closer to real-data-free
palmprint recognition.
</p></li>
</ul>

<h3>Title: DisguisOR: Holistic Face Anonymization for the Operating Room. (arXiv:2307.14241v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14241">http://arxiv.org/abs/2307.14241</a></li>
<li>Code URL: <a href="https://github.com/wngtn/disguisor">https://github.com/wngtn/disguisor</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14241] DisguisOR: Holistic Face Anonymization for the Operating Room](http://arxiv.org/abs/2307.14241) #privacy</code></li>
<li>Summary: <p>Purpose: Recent advances in Surgical Data Science (SDS) have contributed to
an increase in video recordings from hospital environments. While methods such
as surgical workflow recognition show potential in increasing the quality of
patient care, the quantity of video data has surpassed the scale at which
images can be manually anonymized. Existing automated 2D anonymization methods
under-perform in Operating Rooms (OR), due to occlusions and obstructions. We
propose to anonymize multi-view OR recordings using 3D data from multiple
camera streams. Methods: RGB and depth images from multiple cameras are fused
into a 3D point cloud representation of the scene. We then detect each
individual's face in 3D by regressing a parametric human mesh model onto
detected 3D human keypoints and aligning the face mesh with the fused 3D point
cloud. The mesh model is rendered into every acquired camera view, replacing
each individual's face. Results: Our method shows promise in locating faces at
a higher rate than existing approaches. DisguisOR produces geometrically
consistent anonymizations for each camera view, enabling more realistic
anonymization that is less detrimental to downstream tasks. Conclusion:
Frequent obstructions and crowding in operating rooms leaves significant room
for improvement for off-the-shelf anonymization methods. DisguisOR addresses
privacy on a scene level and has the potential to facilitate further research
in SDS.
</p></li>
</ul>

<h3>Title: Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach. (arXiv:2307.13771v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13771">http://arxiv.org/abs/2307.13771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13771] Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach](http://arxiv.org/abs/2307.13771) #privacy</code></li>
<li>Summary: <p>Machine learning (ML) models can memorize training datasets. As a result,
training ML models over private datasets can violate the privacy of
individuals. Differential privacy (DP) is a rigorous privacy notion to preserve
the privacy of underlying training datasets in ML models. Yet, training ML
models in a DP framework usually degrades the accuracy of ML models. This paper
aims to boost the accuracy of a DP-ML model, specifically a logistic regression
model, via a pre-training module. In more detail, we initially pre-train our
model on a public training dataset that there is no privacy concern about it.
Then, we fine-tune our model via the DP logistic regression with the private
dataset. In the numerical results, we show that adding a pre-training module
significantly improves the accuracy of the DP logistic regression.
</p></li>
</ul>

<h3>Title: Random (Un)rounding : Vulnerabilities in Discrete Attribute Disclosure in the 2021 Canadian Census. (arXiv:2307.13859v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13859">http://arxiv.org/abs/2307.13859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13859] Random (Un)rounding : Vulnerabilities in Discrete Attribute Disclosure in the 2021 Canadian Census](http://arxiv.org/abs/2307.13859) #privacy</code></li>
<li>Summary: <p>The 2021 Canadian census is notable for using a unique form of privacy,
random rounding, which independently and probabilistically rounds discrete
numerical attribute values. In this work, we explore how hierarchical summative
correlation between discrete variables allows for both probabilistic and exact
solutions to attribute values in the 2021 Canadian Census disclosure. We
demonstrate that, in some cases, it is possible to "unround" and extract the
original private values before rounding, both in the presence and absence of
provided population invariants. Using these methods, we expose the exact value
of 624 previously private attributes in the 2021 Canadian census disclosure. We
also infer the potential values of more than 1000 private attributes with a
high probability of correctness. Finally, we propose how a simple solution
based on unbounded discrete noise can effectively negate exact unrounding while
maintaining high utility in the final product.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Defending Adversarial Patches via Joint Region Localizing and Inpainting. (arXiv:2307.14242v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14242">http://arxiv.org/abs/2307.14242</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14242] Defending Adversarial Patches via Joint Region Localizing and Inpainting](http://arxiv.org/abs/2307.14242) #defense</code></li>
<li>Summary: <p>Deep neural networks are successfully used in various applications, but show
their vulnerability to adversarial examples. With the development of
adversarial patches, the feasibility of attacks in physical scenes increases,
and the defenses against patch attacks are urgently needed. However, defending
such adversarial patch attacks is still an unsolved problem. In this paper, we
analyse the properties of adversarial patches, and find that: on the one hand,
adversarial patches will lead to the appearance or contextual inconsistency in
the target objects; on the other hand, the patch region will show abnormal
changes on the high-level feature maps of the objects extracted by a backbone
network. Considering the above two points, we propose a novel defense method
based on a <code>localizing and inpainting" mechanism to pre-process the input
examples. Specifically, we design an unified framework, where the</code>localizing"
sub-network utilizes a two-branch structure to represent the above two aspects
to accurately detect the adversarial patch region in the image. For the
<code>inpainting" sub-network, it utilizes the surrounding contextual cues to
recover the original content covered by the adversarial patch. The quality of
inpainted images is also evaluated by measuring the appearance consistency and
the effects of adversarial attacks. These two sub-networks are then jointly
trained via an iterative optimization manner. In this way, the</code>localizing"
and ``inpainting" modules can interact closely with each other, and thus learn
a better solution. A series of experiments versus traffic sign classification
and detection tasks are conducted to defend against various adversarial patch
attacks.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Foundational Models Defining a New Era in Vision: A Survey and Outlook. (arXiv:2307.13721v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13721">http://arxiv.org/abs/2307.13721</a></li>
<li>Code URL: <a href="https://github.com/awaisrauf/awesome-cv-foundational-models">https://github.com/awaisrauf/awesome-cv-foundational-models</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13721] Foundational Models Defining a New Era in Vision: A Survey and Outlook](http://arxiv.org/abs/2307.13721) #attack</code></li>
<li>Summary: <p>Vision systems to see and reason about the compositional nature of visual
scenes are fundamental to understanding our world. The complex relations
between objects and their locations, ambiguities, and variations in the
real-world environment can be better described in human language, naturally
governed by grammatical rules and other modalities such as audio and depth. The
models learned to bridge the gap between such modalities coupled with
large-scale training data facilitate contextual reasoning, generalization, and
prompt capabilities at test time. These models are referred to as foundational
models. The output of such models can be modified through human-provided
prompts without retraining, e.g., segmenting a particular object by providing a
bounding box, having interactive dialogues by asking questions about an image
or video scene or manipulating the robot's behavior through language
instructions. In this survey, we provide a comprehensive review of such
emerging foundational models, including typical architecture designs to combine
different modalities (vision, text, audio, etc), training objectives
(contrastive, generative), pre-training datasets, fine-tuning mechanisms, and
the common prompting patterns; textual, visual, and heterogeneous. We discuss
the open challenges and research directions for foundational models in computer
vision, including difficulties in their evaluations and benchmarking, gaps in
their real-world understanding, limitations of their contextual understanding,
biases, vulnerability to adversarial attacks, and interpretability issues. We
review recent developments in this field, covering a wide range of applications
of foundation models systematically and comprehensively. A comprehensive list
of foundational models studied in this work is available at
\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.
</p></li>
</ul>

<h3>Title: Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models. (arXiv:2307.14061v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14061">http://arxiv.org/abs/2307.14061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14061] Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models](http://arxiv.org/abs/2307.14061) #attack</code></li>
<li>Summary: <p>Vision-language pre-training (VLP) models have shown vulnerability to
adversarial examples in multimodal tasks. Furthermore, malicious adversaries
can be deliberately transferred to attack other black-box models. However,
existing work has mainly focused on investigating white-box attacks. In this
paper, we present the first study to investigate the adversarial
transferability of recent VLP models. We observe that existing methods exhibit
much lower transferability, compared to the strong attack performance in
white-box settings. The transferability degradation is partly caused by the
under-utilization of cross-modal interactions. Particularly, unlike unimodal
learning, VLP models rely heavily on cross-modal interactions and the
multimodal alignments are many-to-many, e.g., an image can be described in
various natural languages. To this end, we propose a highly transferable
Set-level Guidance Attack (SGA) that thoroughly leverages modality interactions
and incorporates alignment-preserving augmentation with cross-modal guidance.
Experimental results demonstrate that SGA could generate adversarial examples
that can strongly transfer across different VLP models on multiple downstream
vision-language tasks. On image-text retrieval, SGA significantly enhances the
attack success rate for transfer attacks from ALBEF to TCL by a large margin
(at least 9.78% and up to 30.21%), compared to the state-of-the-art.
</p></li>
</ul>

<h3>Title: The GANfather: Controllable generation of malicious activity to improve defence systems. (arXiv:2307.13787v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13787">http://arxiv.org/abs/2307.13787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13787] The GANfather: Controllable generation of malicious activity to improve defence systems](http://arxiv.org/abs/2307.13787) #attack</code></li>
<li>Summary: <p>Machine learning methods to aid defence systems in detecting malicious
activity typically rely on labelled data. In some domains, such labelled data
is unavailable or incomplete. In practice this can lead to low detection rates
and high false positive rates, which characterise for example anti-money
laundering systems. In fact, it is estimated that 1.7--4 trillion euros are
laundered annually and go undetected. We propose The GANfather, a method to
generate samples with properties of malicious activity, without label
requirements. We propose to reward the generation of malicious samples by
introducing an extra objective to the typical Generative Adversarial Networks
(GANs) loss. Ultimately, our goal is to enhance the detection of illicit
activity using the discriminator network as a novel and robust defence system.
Optionally, we may encourage the generator to bypass pre-existing detection
systems. This setup then reveals defensive weaknesses for the discriminator to
correct. We evaluate our method in two real-world use cases, money laundering
and recommendation systems. In the former, our method moves cumulative amounts
close to 350 thousand dollars through a network of accounts without being
detected by an existing system. In the latter, we recommend the target item to
a broad user base with as few as 30 synthetic attackers. In both cases, we
train a new defence system to capture the synthetic attacks.
</p></li>
</ul>

<h3>Title: Open Image Content Disarm And Reconstruction. (arXiv:2307.14057v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14057">http://arxiv.org/abs/2307.14057</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14057] Open Image Content Disarm And Reconstruction](http://arxiv.org/abs/2307.14057) #attack</code></li>
<li>Summary: <p>With the advance in malware technology, attackers create new ways to hide
their malicious code from antivirus services. One way to obfuscate an attack is
to use common files as cover to hide the malicious scripts, so the malware will
look like a legitimate file. Although cutting-edge Artificial Intelligence and
content signature exist, evasive malware successfully bypasses next-generation
malware detection using advanced methods like steganography. Some of the files
commonly used to hide malware are image files (e.g., JPEG). In addition, some
malware use steganography to hide malicious scripts or sensitive data in
images. Steganography in images is difficult to detect even with specialized
tools. Image-based attacks try to attack the user's device using malicious
payloads or utilize image steganography to hide sensitive data inside
legitimate images and leak it outside the user's device. Therefore in this
paper, we present a novel Image Content Disarm and Reconstruction (ICDR). Our
ICDR system removes potential malware, with a zero trust approach, while
maintaining high image quality and file usability. By extracting the image
data, removing it from the rest of the file, and manipulating the image pixels,
it is possible to disable or remove the hidden malware inside the file.
</p></li>
</ul>

<h3>Title: Risk Assessment Graphs: Utilizing Attack Graphs for Risk Assessment. (arXiv:2307.14114v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14114">http://arxiv.org/abs/2307.14114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14114] Risk Assessment Graphs: Utilizing Attack Graphs for Risk Assessment](http://arxiv.org/abs/2307.14114) #attack</code></li>
<li>Summary: <p>Risk assessment plays a crucial role in ensuring the security and resilience
of modern computer systems. Existing methods for conducting risk assessments
often suffer from tedious and time-consuming processes, making it challenging
to maintain a comprehensive overview of potential security issues. In this
paper, we propose a novel approach that leverages attack graphs to enhance the
efficiency and effectiveness of risk assessment. Attack graphs visually
represent the various attack paths that adversaries can exploit within a
system, enabling a systematic exploration of potential vulnerabilities. By
extending attack graphs with capabilities to include countermeasures and
consequences, they can be leveraged to constitute the complete risk assessment
process. Our method offers a more streamlined and comprehensive analysis of
system vulnerabilities, where system changes, or environment changes can easily
be adapted and the issues exposing the highest risk can easily be identified.
We demonstrate the effectiveness of our approach through a case study, as well
as the applicability by combining existing risk assessment standards with our
method. Our work aims to bridge the gap between risk assessment practices and
evolving threat landscapes, offering an improved methodology for managing and
mitigating risks in modern computer systems.
</p></li>
</ul>

<h3>Title: ICCPS: Impact discovery using causal inference for cyber attacks in CPSs. (arXiv:2307.14161v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14161">http://arxiv.org/abs/2307.14161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14161] ICCPS: Impact discovery using causal inference for cyber attacks in CPSs](http://arxiv.org/abs/2307.14161) #attack</code></li>
<li>Summary: <p>We propose a new method to quantify the impact of cyber attacks in Cyber
Physical Systems (CPSs). In particular, our method allows to identify the
Design Parameter (DPs) affected due to a cyber attack launched on a different
set of DPs in the same CPS. To achieve this, we adopt causal graphs to causally
link DPs with each other and quantify the impact of one DP on another. Using
SWaT, a real world testbed of a water treatment system, we demonstrate that
causal graphs can be build in two ways: i) using domain knowledge of the
control logic and the physical connectivity structure of the DPs, we call these
causal domain graphs and ii) learning from operational data logs, we call these
causal learnt graphs. We then compare these graphs when a same set of DPs is
used. Our analysis shows a common set of edges between the causal domain graphs
and the causal learnt graphs exists, which helps validate the causal learnt
graphs. Additionally, we show that the learnt graphs can discover new causal
relations, not initially considered in the domain graphs, that help
significantly characterising the impact of the attack. We use causal domain
graphs to estimate the parameters of the graphs, and the causal learnt graphs
for causal inference. To learn the structure of the causal learnt graphs in all
the six-stages of SWaT, we experiment with three learning algorithms: Peter
Clarke (PC), Hill Climb (HC) search and Chow-Lie (CH). Finally, we demonstrate
how causal graphs can be used to analyse the impact of cyber attacks by
analysing nine well known cyber attacks on the SWaT test bed. We find that by
using causal learnt graphs the DPs impacted by the attacks are correctly
discovered with a probability greater than 0.9.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: MAEA: Multimodal Attribution for Embodied AI. (arXiv:2307.13850v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13850">http://arxiv.org/abs/2307.13850</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13850] MAEA: Multimodal Attribution for Embodied AI](http://arxiv.org/abs/2307.13850) #robust</code></li>
<li>Summary: <p>Understanding multimodal perception for embodied AI is an open question
because such inputs may contain highly complementary as well as redundant
information for the task. A relevant direction for multimodal policies is
understanding the global trends of each modality at the fusion layer. To this
end, we disentangle the attributions for visual, language, and previous action
inputs across different policies trained on the ALFRED dataset. Attribution
analysis can be utilized to rank and group the failure scenarios, investigate
modeling and dataset biases, and critically analyze multimodal EAI policies for
robustness and user trust before deployment. We present MAEA, a framework to
compute global attributions per modality of any differentiable policy. In
addition, we show how attributions enable lower-level behavior analysis in EAI
policies for language and visual attributions.
</p></li>
</ul>

<h3>Title: Exploring the Sharpened Cosine Similarity. (arXiv:2307.13855v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13855">http://arxiv.org/abs/2307.13855</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13855] Exploring the Sharpened Cosine Similarity](http://arxiv.org/abs/2307.13855) #robust</code></li>
<li>Summary: <p>Convolutional layers have long served as the primary workhorse for image
classification. Recently, an alternative to convolution was proposed using the
Sharpened Cosine Similarity (SCS), which in theory may serve as a better
feature detector. While multiple sources report promising results, there has
not been to date a full-scale empirical analysis of neural network performance
using these new layers. In our work, we explore SCS's parameter behavior and
potential as a drop-in replacement for convolutions in multiple CNN
architectures benchmarked on CIFAR-10. We find that while SCS may not yield
significant increases in accuracy, it may learn more interpretable
representations. We also find that, in some circumstances, SCS may confer a
slight increase in adversarial robustness.
</p></li>
</ul>

<h3>Title: Centroid-aware feature recalibration for cancer grading in pathology images. (arXiv:2307.13947v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13947">http://arxiv.org/abs/2307.13947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13947] Centroid-aware feature recalibration for cancer grading in pathology images](http://arxiv.org/abs/2307.13947) #robust</code></li>
<li>Summary: <p>Cancer grading is an essential task in pathology. The recent developments of
artificial neural networks in computational pathology have shown that these
methods hold great potential for improving the accuracy and quality of cancer
diagnosis. However, the issues with the robustness and reliability of such
methods have not been fully resolved yet. Herein, we propose a centroid-aware
feature recalibration network that can conduct cancer grading in an accurate
and robust manner. The proposed network maps an input pathology image into an
embedding space and adjusts it by using centroids embedding vectors of
different cancer grades via attention mechanism. Equipped with the recalibrated
embedding vector, the proposed network classifiers the input pathology image
into a pertinent class label, i.e., cancer grade. We evaluate the proposed
network using colorectal cancer datasets that were collected under different
environments. The experimental results confirm that the proposed network is
able to conduct cancer grading in pathology images with high accuracy
regardless of the environmental changes in the datasets.
</p></li>
</ul>

<h3>Title: Visual Prompt Flexible-Modal Face Anti-Spoofing. (arXiv:2307.13958v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13958">http://arxiv.org/abs/2307.13958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13958] Visual Prompt Flexible-Modal Face Anti-Spoofing](http://arxiv.org/abs/2307.13958) #robust</code></li>
<li>Summary: <p>Recently, vision transformer based multimodal learning methods have been
proposed to improve the robustness of face anti-spoofing (FAS) systems.
However, multimodal face data collected from the real world is often imperfect
due to missing modalities from various imaging sensors. Recently,
flexible-modal FAS~\cite{yu2023flexible} has attracted more attention, which
aims to develop a unified multimodal FAS model using complete multimodal face
data but is insensitive to test-time missing modalities. In this paper, we
tackle one main challenge in flexible-modal FAS, i.e., when missing modality
occurs either during training or testing in real-world situations. Inspired by
the recent success of the prompt learning in language models, we propose
\textbf{V}isual \textbf{P}rompt flexible-modal \textbf{FAS} (VP-FAS), which
learns the modal-relevant prompts to adapt the frozen pre-trained foundation
model to downstream flexible-modal FAS task. Specifically, both vanilla visual
prompts and residual contextual prompts are plugged into multimodal
transformers to handle general missing-modality cases, while only requiring
less than 4\% learnable parameters compared to training the entire model.
Furthermore, missing-modality regularization is proposed to force models to
learn consistent multimodal feature embeddings when missing partial modalities.
Extensive experiments conducted on two multimodal FAS benchmark datasets
demonstrate the effectiveness of our VP-FAS framework that improves the
performance under various missing-modality cases while alleviating the
requirement of heavy model re-training.
</p></li>
</ul>

<h3>Title: Consensus-Adaptive RANSAC. (arXiv:2307.14030v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14030">http://arxiv.org/abs/2307.14030</a></li>
<li>Code URL: <a href="https://github.com/cavalli1234/ca-ransac">https://github.com/cavalli1234/ca-ransac</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14030] Consensus-Adaptive RANSAC](http://arxiv.org/abs/2307.14030) #robust</code></li>
<li>Summary: <p>RANSAC and its variants are widely used for robust estimation, however, they
commonly follow a greedy approach to finding the highest scoring model while
ignoring other model hypotheses. In contrast, Iteratively Reweighted Least
Squares (IRLS) techniques gradually approach the model by iteratively updating
the weight of each correspondence based on the residuals from previous
iterations. Inspired by these methods, we propose a new RANSAC framework that
learns to explore the parameter space by considering the residuals seen so far
via a novel attention layer. The attention mechanism operates on a batch of
point-to-model residuals, and updates a per-point estimation state to take into
account the consensus found through a lightweight one-step transformer. This
rich state then guides the minimal sampling between iterations as well as the
model refinement. We evaluate the proposed approach on essential and
fundamental matrix estimation on a number of indoor and outdoor datasets. It
outperforms state-of-the-art estimators by a significant margin adding only a
small runtime overhead. Moreover, we demonstrate good generalization properties
of our trained model, indicating its effectiveness across different datasets
and tasks. The proposed attention mechanism and one-step transformer provide an
adaptive behavior that enhances the performance of RANSAC, making it a more
effective tool for robust estimation. Code is available at
https://github.com/cavalli1234/CA-RANSAC.
</p></li>
</ul>

<h3>Title: PNT-Edge: Towards Robust Edge Detection with Noisy Labels by Learning Pixel-level Noise Transitions. (arXiv:2307.14070v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14070">http://arxiv.org/abs/2307.14070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14070] PNT-Edge: Towards Robust Edge Detection with Noisy Labels by Learning Pixel-level Noise Transitions](http://arxiv.org/abs/2307.14070) #robust</code></li>
<li>Summary: <p>Relying on large-scale training data with pixel-level labels, previous edge
detection methods have achieved high performance. However, it is hard to
manually label edges accurately, especially for large datasets, and thus the
datasets inevitably contain noisy labels. This label-noise issue has been
studied extensively for classification, while still remaining under-explored
for edge detection. To address the label-noise issue for edge detection, this
paper proposes to learn Pixel-level NoiseTransitions to model the
label-corruption process. To achieve it, we develop a novel Pixel-wise Shift
Learning (PSL) module to estimate the transition from clean to noisy labels as
a displacement field. Exploiting the estimated noise transitions, our model,
named PNT-Edge, is able to fit the prediction to clean labels. In addition, a
local edge density regularization term is devised to exploit local structure
information for better transition learning. This term encourages learning large
shifts for the edges with complex local structures. Experiments on SBD and
Cityscapes demonstrate the effectiveness of our method in relieving the impact
of label noise. Codes will be available at github.
</p></li>
</ul>

<h3>Title: Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching. (arXiv:2307.14071v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14071">http://arxiv.org/abs/2307.14071</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14071] Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching](http://arxiv.org/abs/2307.14071) #robust</code></li>
<li>Summary: <p>Correlation based stereo matching has achieved outstanding performance, which
pursues cost volume between two feature maps. Unfortunately, current methods
with a fixed model do not work uniformly well across various datasets, greatly
limiting their real-world applicability. To tackle this issue, this paper
proposes a new perspective to dynamically calculate correlation for robust
stereo matching. A novel Uncertainty Guided Adaptive Correlation (UGAC) module
is introduced to robustly adapt the same model for different scenarios.
Specifically, a variance-based uncertainty estimation is employed to adaptively
adjust the sampling area during warping operation. Additionally, we improve the
traditional non-parametric warping with learnable parameters, such that the
position-specific weights can be learned. We show that by empowering the
recurrent network with the UGAC module, stereo matching can be exploited more
robustly and effectively. Extensive experiments demonstrate that our method
achieves state-of-the-art performance over the ETH3D, KITTI, and Middlebury
datasets when employing the same fixed model over these datasets without any
retraining procedure. To target real-time applications, we further design a
lightweight model based on UGAC, which also outperforms other methods over
KITTI benchmarks with only 0.6 M parameters.
</p></li>
</ul>

<h3>Title: Large-scale Fully-Unsupervised Re-Identification. (arXiv:2307.14278v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14278">http://arxiv.org/abs/2307.14278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14278] Large-scale Fully-Unsupervised Re-Identification](http://arxiv.org/abs/2307.14278) #robust</code></li>
<li>Summary: <p>Fully-unsupervised Person and Vehicle Re-Identification have received
increasing attention due to their broad applicability in surveillance,
forensics, event understanding, and smart cities, without requiring any manual
annotation. However, most of the prior art has been evaluated in datasets that
have just a couple thousand samples. Such small-data setups often allow the use
of costly techniques in time and memory footprints, such as Re-Ranking, to
improve clustering results. Moreover, some previous work even pre-selects the
best clustering hyper-parameters for each dataset, which is unrealistic in a
large-scale fully-unsupervised scenario. In this context, this work tackles a
more realistic scenario and proposes two strategies to learn from large-scale
unlabeled data. The first strategy performs a local neighborhood sampling to
reduce the dataset size in each iteration without violating neighborhood
relationships. A second strategy leverages a novel Re-Ranking technique, which
has a lower time upper bound complexity and reduces the memory complexity from
O(n^2) to O(kn) with k << n. To avoid the pre-selection of specific
hyper-parameter values for the clustering algorithm, we also present a novel
scheduling algorithm that adjusts the density parameter during training, to
leverage the diversity of samples and keep the learning robust to noisy
labeling. Finally, due to the complementary knowledge learned by different
models, we also introduce a co-training strategy that relies upon the
permutation of predicted pseudo-labels, among the backbones, with no need for
any hyper-parameters or weighting optimization. The proposed methodology
outperforms the state-of-the-art methods in well-known benchmarks and in the
challenging large-scale Veri-Wild dataset, with a faster and memory-efficient
Re-Ranking strategy, and a large-scale, noisy-robust, and ensemble-based
learning approach.
</p></li>
</ul>

<h3>Title: Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models. (arXiv:2307.14134v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14134">http://arxiv.org/abs/2307.14134</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14134] Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models](http://arxiv.org/abs/2307.14134) #robust</code></li>
<li>Summary: <p>This study introduces and evaluates tiny, mini, small, and medium-sized
uncased Turkish BERT models, aiming to bridge the research gap in
less-resourced languages. We trained these models on a diverse dataset
encompassing over 75GB of text from multiple sources and tested them on several
tasks, including mask prediction, sentiment analysis, news classification, and,
zero-shot classification. Despite their smaller size, our models exhibited
robust performance, including zero-shot task, while ensuring computational
efficiency and faster execution times. Our findings provide valuable insights
into the development and application of smaller language models, especially in
the context of the Turkish language.
</p></li>
</ul>

<h3>Title: Gradient-Based Spectral Embeddings of Random Dot Product Graphs. (arXiv:2307.13818v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13818">http://arxiv.org/abs/2307.13818</a></li>
<li>Code URL: <a href="https://github.com/marfiori/efficient-ase">https://github.com/marfiori/efficient-ase</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13818] Gradient-Based Spectral Embeddings of Random Dot Product Graphs](http://arxiv.org/abs/2307.13818) #robust</code></li>
<li>Summary: <p>The Random Dot Product Graph (RDPG) is a generative model for relational
data, where nodes are represented via latent vectors in low-dimensional
Euclidean space. RDPGs crucially postulate that edge formation probabilities
are given by the dot product of the corresponding latent positions.
Accordingly, the embedding task of estimating these vectors from an observed
graph is typically posed as a low-rank matrix factorization problem. The
workhorse Adjacency Spectral Embedding (ASE) enjoys solid statistical
properties, but it is formally solving a surrogate problem and can be
computationally intensive. In this paper, we bring to bear recent advances in
non-convex optimization and demonstrate their impact to RDPG inference. We
advocate first-order gradient descent methods to better solve the embedding
problem, and to organically accommodate broader network embedding applications
of practical relevance. Notably, we argue that RDPG embeddings of directed
graphs loose interpretability unless the factor matrices are constrained to
have orthogonal columns. We thus develop a novel feasible optimization method
in the resulting manifold. The effectiveness of the graph representation
learning framework is demonstrated on reproducible experiments with both
synthetic and real network data. Our open-source algorithm implementations are
scalable, and unlike the ASE they are robust to missing edge data and can track
slowly-varying latent positions from streaming graphs.
</p></li>
</ul>

<h3>Title: ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis. (arXiv:2307.13883v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13883">http://arxiv.org/abs/2307.13883</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13883] ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis](http://arxiv.org/abs/2307.13883) #robust</code></li>
<li>Summary: <p>When writing programs, people have the ability to tackle a new complex task
by decomposing it into smaller and more familiar subtasks. While it is
difficult to measure whether neural program synthesis methods have similar
capabilities, we can measure whether they compositionally generalize, that is,
whether a model that has been trained on the simpler subtasks is subsequently
able to solve more complex tasks. In this paper, we characterize several
different forms of compositional generalization that are desirable in program
synthesis, forming a meta-benchmark which we use to create generalization tasks
for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a
novel decomposition-based synthesis strategy that predicts execution subgoals
to solve problems step-by-step informed by program execution at each step.
ExeDec has better synthesis performance and greatly improved compositional
generalization ability compared to baselines.
</p></li>
</ul>

<h3>Title: Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13885">http://arxiv.org/abs/2307.13885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13885] Efficient Estimation of the Local Robustness of Machine Learning Models](http://arxiv.org/abs/2307.13885) #robust</code></li>
<li>Summary: <p>Machine learning models often need to be robust to noisy input data. The
effect of real-world noise (which is often random) on model predictions is
captured by a model's local robustness, i.e., the consistency of model
predictions in a local region around an input. However, the na\"ive approach to
computing local robustness based on Monte-Carlo sampling is statistically
inefficient, leading to prohibitive computational costs for large-scale
applications. In this work, we develop the first analytical estimators to
efficiently compute local robustness of multi-class discriminative models using
local linear function approximation and the multivariate Normal CDF. Through
the derivation of these estimators, we show how local robustness is connected
to concepts such as randomized smoothing and softmax probability. We also
confirm empirically that these estimators accurately and efficiently compute
the local robustness of standard deep learning models. In addition, we
demonstrate these estimators' usefulness for various tasks involving local
robustness, such as measuring robustness bias and identifying examples that are
vulnerable to noise perturbation in a dataset. By developing these analytical
estimators, this work not only advances conceptual understanding of local
robustness, but also makes its computation practical, enabling the use of local
robustness in critical downstream applications.
</p></li>
</ul>

<h3>Title: Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13903">http://arxiv.org/abs/2307.13903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13903] Corruption-Robust Lipschitz Contextual Search](http://arxiv.org/abs/2307.13903) #robust</code></li>
<li>Summary: <p>I study the problem of learning a Lipschitz function with corrupted binary
signals. The learner tries to learn a Lipschitz function $f$ that the adversary
chooses. In each round, the adversary selects a context vector $x_t$ in the
input space, and the learner makes a guess to the true function value $f(x_t)$
and receives a binary signal indicating whether the guess was high or low. In a
total of $C$ rounds, the signal may be corrupted, though the value of $C$ is
unknown to the learner. The learner's goal is to incur a small cumulative loss.
I present a natural yet powerful technique sanity check, which proves useful in
designing corruption-robust algorithms. I design algorithms which (treating the
Lipschitz parameter $L$ as constant): for the symmetric loss, the learner
achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with
$d > 1$; for the pricing loss the learner achieves regret $\widetilde{O}
(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
</p></li>
</ul>

<h3>Title: Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input. (arXiv:2307.13907v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13907">http://arxiv.org/abs/2307.13907</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13907] Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input](http://arxiv.org/abs/2307.13907) #robust</code></li>
<li>Summary: <p>Data-driven, neural network (NN) based anomaly detection and predictive
maintenance are emerging research areas. NN-based analytics of time-series data
offer valuable insights into past behaviors and estimates of critical
parameters like remaining useful life (RUL) of equipment and state-of-charge
(SOC) of batteries. However, input time series data can be exposed to
intentional or unintentional noise when passing through sensors, necessitating
robust validation and verification of these NNs. This paper presents a case
study of the robustness verification approach for time series regression NNs
(TSRegNN) using set-based formal methods. It focuses on utilizing
variable-length input data to streamline input manipulation and enhance network
architecture generalizability. The method is applied to two data sets in the
Prognostics and Health Management (PHM) application areas: (1) SOC estimation
of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs'
robustness is checked using star-based reachability analysis, and several
performance measures evaluate the effect of bounded perturbations in the input
on network outputs, i.e., future outcomes. Overall, the paper offers a
comprehensive case study for validating and verifying NN-based analytics of
time-series data in real-world applications, emphasizing the importance of
robustness testing for accurate and reliable predictions, especially
considering the impact of noise on future outcomes.
</p></li>
</ul>

<h3>Title: Topology-aware Robust Optimization for Out-of-distribution Generalization. (arXiv:2307.13943v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13943">http://arxiv.org/abs/2307.13943</a></li>
<li>Code URL: <a href="https://github.com/joffery/tro">https://github.com/joffery/tro</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13943] Topology-aware Robust Optimization for Out-of-distribution Generalization](http://arxiv.org/abs/2307.13943) #robust</code></li>
<li>Summary: <p>Out-of-distribution (OOD) generalization is a challenging machine learning
problem yet highly desirable in many high-stake applications. Existing methods
suffer from overly pessimistic modeling with low generalization confidence. As
generalizing to arbitrary test distributions is impossible, we hypothesize that
further structure on the topology of distributions is crucial in developing
strong OOD resilience. To this end, we propose topology-aware robust
optimization (TRO) that seamlessly integrates distributional topology in a
principled optimization framework. More specifically, TRO solves two
optimization objectives: (1) Topology Learning which explores data manifold to
uncover the distributional topology; (2) Learning on Topology which exploits
the topology to constrain robust optimization for tightly-bounded
generalization risks. We theoretically demonstrate the effectiveness of our
approach and empirically show that it significantly outperforms the state of
the arts in a wide range of tasks including classification, regression, and
semantic segmentation. Moreover, we empirically find the data-driven
distributional topology is consistent with domain knowledge, enhancing the
explainability of our approach.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Periocular biometrics: databases, algorithms and directions. (arXiv:2307.14111v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14111">http://arxiv.org/abs/2307.14111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14111] Periocular biometrics: databases, algorithms and directions](http://arxiv.org/abs/2307.14111) #biometric</code></li>
<li>Summary: <p>Periocular biometrics has been established as an independent modality due to
concerns on the performance of iris or face systems in uncontrolled conditions.
Periocular refers to the facial region in the eye vicinity, including eyelids,
lashes and eyebrows. It is available over a wide range of acquisition
distances, representing a trade-off between the whole face (which can be
occluded at close distances) and the iris texture (which do not have enough
resolution at long distances). Since the periocular region appears in face or
iris images, it can be used also in conjunction with these modalities. Features
extracted from the periocular region have been also used successfully for
gender classification and ethnicity classification, and to study the impact of
gender transformation or plastic surgery in the recognition performance. This
paper presents a review of the state of the art in periocular biometric
research, providing an insight of the most relevant issues and giving a
thorough coverage of the existing literature. Future research trends are also
briefly discussed.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: DFR-Net: Density Feature Refinement Network for Image Dehazing Utilizing Haze Density Difference. (arXiv:2307.13927v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13927">http://arxiv.org/abs/2307.13927</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13927] DFR-Net: Density Feature Refinement Network for Image Dehazing Utilizing Haze Density Difference](http://arxiv.org/abs/2307.13927) #extraction</code></li>
<li>Summary: <p>In image dehazing task, haze density is a key feature and affects the
performance of dehazing methods. However, some of the existing methods lack a
comparative image to measure densities, and others create intermediate results
but lack the exploitation of their density differences, which can facilitate
perception of density. To address these deficiencies, we propose a
density-aware dehazing method named Density Feature Refinement Network
(DFR-Net) that extracts haze density features from density differences and
leverages density differences to refine density features. In DFR-Net, we first
generate a proposal image that has lower overall density than the hazy input,
bringing in global density differences. Additionally, the dehazing residual of
the proposal image reflects the level of dehazing performance and provides
local density differences that indicate localized hard dehazing or high density
areas. Subsequently, we introduce a Global Branch (GB) and a Local Branch (LB)
to achieve density-awareness. In GB, we use Siamese networks for feature
extraction of hazy inputs and proposal images, and we propose a Global Density
Feature Refinement (GDFR) module that can refine features by pushing features
with different global densities further away. In LB, we explore local density
features from the dehazing residuals between hazy inputs and proposal images
and introduce an Intermediate Dehazing Residual Feedforward (IDRF) module to
update local features and pull them closer to clear image features. Sufficient
experiments demonstrate that the proposed method achieves results beyond the
state-of-the-art methods on various datasets.
</p></li>
</ul>

<h3>Title: Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras. (arXiv:2307.14124v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14124">http://arxiv.org/abs/2307.14124</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14124] Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras](http://arxiv.org/abs/2307.14124) #extraction</code></li>
<li>Summary: <p>Recent advances in event camera research emphasize processing data in its
original sparse form, which allows the use of its unique features such as high
temporal resolution, high dynamic range, low latency, and resistance to image
blur. One promising approach for analyzing event data is through graph
convolutional networks (GCNs). However, current research in this domain
primarily focuses on optimizing computational costs, neglecting the associated
memory costs. In this paper, we consider both factors together in order to
achieve satisfying results and relatively low model complexity. For this
purpose, we performed a comparative analysis of different graph convolution
operations, considering factors such as execution time, the number of trainable
model parameters, data format requirements, and training outcomes. Our results
show a 450-fold reduction in the number of parameters for the feature
extraction module and a 4.5-fold reduction in the size of the data
representation while maintaining a classification accuracy of 52.3%, which is
6.3% higher compared to the operation used in state-of-the-art approaches. To
further evaluate performance, we implemented the object detection architecture
and evaluated its performance on the N-Caltech101 dataset. The results showed
an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per
second.
</p></li>
</ul>

<h3>Title: FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction. (arXiv:2307.13900v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13900">http://arxiv.org/abs/2307.13900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13900] FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction](http://arxiv.org/abs/2307.13900) #extraction</code></li>
<li>Summary: <p>We present FinTree, Financial Dataset Pretrain Transformer Encoder for
Relation Extraction. Utilizing an encoder language model, we further pretrain
FinTree on the financial dataset, adapting the model in financial domain tasks.
FinTree stands out with its novel structure that predicts a masked token
instead of the conventional [CLS] token, inspired by the Pattern Exploiting
Training methodology. This structure allows for more accurate relation
predictions between two given entities. The model is trained with a unique
input pattern to provide contextual and positional information about the
entities of interest, and a post-processing step ensures accurate predictions
in line with the entity types. Our experiments demonstrate that FinTree
outperforms on the REFinD, a large-scale financial relation extraction dataset.
The code and pretrained models are available at
https://github.com/HJ-Ok/FinTree.
</p></li>
</ul>

<h3>Title: Unsupervised extraction of local and global keywords from a single text. (arXiv:2307.14005v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14005">http://arxiv.org/abs/2307.14005</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14005] Unsupervised extraction of local and global keywords from a single text](http://arxiv.org/abs/2307.14005) #extraction</code></li>
<li>Summary: <p>We propose an unsupervised, corpus-independent method to extract keywords
from a single text. It is based on the spatial distribution of words and the
response of this distribution to a random permutation of words. As compared to
existing methods (such as e.g. YAKE) our method has three advantages. First, it
is significantly more effective at extracting keywords from long texts. Second,
it allows inference of two types of keywords: local and global. Third, it
uncovers basic themes in texts. Additionally, our method is
language-independent and applies to short texts. The results are obtained via
human annotators with previous knowledge of texts from our database of
classical literary works (the agreement between annotators is from moderate to
substantial). Our results are supported via human-independent arguments based
on the average length of extracted content words and on the average number of
nouns in extracted words. We discuss relations of keywords with higher-order
textual features and reveal a connection between keywords and chapter
divisions.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Integration of Digital Twin and Federated Learning for Securing Vehicular Internet of Things. (arXiv:2307.13794v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13794">http://arxiv.org/abs/2307.13794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13794] Integration of Digital Twin and Federated Learning for Securing Vehicular Internet of Things](http://arxiv.org/abs/2307.13794) #federate</code></li>
<li>Summary: <p>In the present era of advanced technology, the Internet of Things (IoT) plays
a crucial role in enabling smart connected environments. This includes various
domains such as smart homes, smart healthcare, smart cities, smart vehicles,
and many others.With ubiquitous smart connected devices and systems, a large
amount of data associated with them is at a prime risk from malicious entities
(e.g., users, devices, applications) in these systems. Innovative technologies,
including cloud computing, Machine Learning (ML), and data analytics, support
the development of anomaly detection models for the Vehicular Internet of
Things (V-IoT), which encompasses collaborative automatic driving and enhanced
transportation systems. However, traditional centralized anomaly detection
models fail to provide better services for connected vehicles due to issues
such as high latency, privacy leakage, performance overhead, and model drift.
Recently, Federated Learning (FL) has gained significant recognition for its
ability to address data privacy concerns in the IoT domain. Digital Twin (DT),
proves beneficial in addressing uncertain crises and data security issues by
creating a virtual replica that simulates various factors, including traffic
trajectories, city policies, and vehicle utilization. However, the
effectiveness of a V-IoT DT system heavily relies on the collection of
long-term and high-quality data to make appropriate decisions. This paper
introduces a Hierarchical Federated Learning (HFL) based anomaly detection
model for V-IoT, aiming to enhance the accuracy of the model. Our proposed
model integrates both DT and HFL approaches to create a comprehensive system
for detecting malicious activities using an anomaly detection model.
Additionally, real-world V-IoT use case scenarios are presented to demonstrate
the application of the proposed model.
</p></li>
</ul>

<h3>Title: FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13716">http://arxiv.org/abs/2307.13716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13716] FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning](http://arxiv.org/abs/2307.13716) #federate</code></li>
<li>Summary: <p>Traditional federated learning uses the number of samples to calculate the
weights of each client model and uses this fixed weight value to fusion the
global model. However, in practical scenarios, each client's device and data
heterogeneity leads to differences in the quality of each client's model. Thus
the contribution to the global model is not wholly determined by the sample
size. In addition, if clients intentionally upload low-quality or malicious
models, using these models for aggregation will lead to a severe decrease in
global model accuracy. Traditional federated learning algorithms do not address
these issues. To solve this probelm, we propose FedDRL, a model fusion approach
using reinforcement learning based on a two staged approach. In the first
stage, Our method could filter out malicious models and selects trusted client
models to participate in the model fusion. In the second stage, the FedDRL
algorithm adaptively adjusts the weights of the trusted client models and
aggregates the optimal global model. We also define five model fusion scenarios
and compare our method with two baseline algorithms in those scenarios. The
experimental results show that our algorithm has higher reliability than other
algorithms while maintaining accuracy.
</p></li>
</ul>

<h3>Title: Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space. (arXiv:2307.13995v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13995">http://arxiv.org/abs/2307.13995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13995] Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space](http://arxiv.org/abs/2307.13995) #federate</code></li>
<li>Summary: <p>Personalized federated learning (PFL) is a popular framework that allows
clients to have different models to address application scenarios where
clients' data are in different domains. The typical model of a client in PFL
features a global encoder trained by all clients to extract universal features
from the raw data and personalized layers (e.g., a classifier) trained using
the client's local data. Nonetheless, due to the differences between the data
distributions of different clients (aka, domain gaps), the universal features
produced by the global encoder largely encompass numerous components irrelevant
to a certain client's local task. Some recent PFL methods address the above
problem by personalizing specific parameters within the encoder. However, these
methods encounter substantial challenges attributed to the high dimensionality
and non-linearity of neural network parameter space. In contrast, the feature
space exhibits a lower dimensionality, providing greater intuitiveness and
interpretability as compared to the parameter space. To this end, we propose a
novel PFL framework named FedPick. FedPick achieves PFL in the low-dimensional
feature space by selecting task-relevant features adaptively for each client
from the features generated by the global encoder based on its local data
distribution. It presents a more accessible and interpretable implementation of
PFL compared to those methods working in the parameter space. Extensive
experimental results show that FedPick could effectively select task-relevant
features for each client and improve model performance in cross-domain FL.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: YOLOBench: Benchmarking Efficient Object Detectors on Embedded Systems. (arXiv:2307.13901v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13901">http://arxiv.org/abs/2307.13901</a></li>
<li>Code URL: <a href="https://github.com/deeplite/deeplite-torch-zoo">https://github.com/deeplite/deeplite-torch-zoo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13901] YOLOBench: Benchmarking Efficient Object Detectors on Embedded Systems](http://arxiv.org/abs/2307.13901) #fair</code></li>
<li>Summary: <p>We present YOLOBench, a benchmark comprised of 550+ YOLO-based object
detection models on 4 different datasets and 4 different embedded hardware
platforms (x86 CPU, ARM CPU, Nvidia GPU, NPU). We collect accuracy and latency
numbers for a variety of YOLO-based one-stage detectors at different model
scales by performing a fair, controlled comparison of these detectors with a
fixed training environment (code and training hyperparameters).
Pareto-optimality analysis of the collected data reveals that, if modern
detection heads and training techniques are incorporated into the learning
process, multiple architectures of the YOLO series achieve a good
accuracy-latency trade-off, including older models like YOLOv3 and YOLOv4. We
also evaluate training-free accuracy estimators used in neural architecture
search on YOLOBench and demonstrate that, while most state-of-the-art zero-cost
accuracy estimators are outperformed by a simple baseline like MAC count, some
of them can be effectively used to predict Pareto-optimal detection models. We
showcase that by using a zero-cost proxy to identify a YOLO architecture
competitive against a state-of-the-art YOLOv8 model on a Raspberry Pi 4 CPU.
The code and data are available at
https://github.com/Deeplite/deeplite-torch-zoo
</p></li>
</ul>

<h3>Title: Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13709">http://arxiv.org/abs/2307.13709</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13709] Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items](http://arxiv.org/abs/2307.13709) #fair</code></li>
<li>Summary: <p>Many properties in real world, such as desirability or strength in
competitive environment, can't be directly observed, which makes them difficult
to evaluate. To deal with this challenging problem, prior work has primarily
focused on estimating those properties of known items, especially the strength
of sports players, only of those who appears in paired comparison dataset. In
this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework
to evaluate any properties of unknown items, not necessarily present in
dataset. Our method seamlessly integrates traditional Bradley-Terry model with
a neural network structure. We also generalizes this architecture further for
asymmetric environment with unfairness, which is much more common in real world
settings. In our experimental analysis, DBTR successfully learned desired
quantification of those properties.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h3>Title: Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance. (arXiv:2307.13698v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13698">http://arxiv.org/abs/2307.13698</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13698] Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance](http://arxiv.org/abs/2307.13698) #explainability</code></li>
<li>Summary: <p>Discovering a high-performing sparse network within a massive neural network
is advantageous for deploying them on devices with limited storage, such as
mobile phones. Additionally, model explainability is essential to fostering
trust in AI. The Lottery Ticket Hypothesis (LTH) finds a network within a deep
network with comparable or superior performance to the original model. However,
limited study has been conducted on the success or failure of LTH in terms of
explainability. In this work, we examine why the performance of the pruned
networks gradually increases or decreases. Using Grad-CAM and Post-hoc concept
bottleneck models (PCBMs), respectively, we investigate the explainability of
pruned networks in terms of pixels and high-level concepts. We perform
extensive experiments across vision and medical imaging datasets. As more
weights are pruned, the performance of the network degrades. The discovered
concepts and pixels from the pruned networks are inconsistent with the original
network -- a possible reason for the drop in performance.
</p></li>
</ul>

<h2>watermark</h2>
<h3>Title: Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy. (arXiv:2307.13808v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13808">http://arxiv.org/abs/2307.13808</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13808] Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy](http://arxiv.org/abs/2307.13808) #watermark</code></li>
<li>Summary: <p>To mitigate potential risks associated with language models, recent AI
detection research proposes incorporating watermarks into machine-generated
text through random vocabulary restrictions and utilizing this information for
detection. While these watermarks only induce a slight deterioration in
perplexity, our empirical investigation reveals a significant detriment to the
performance of conditional text generation. To address this issue, we introduce
a simple yet effective semantic-aware watermarking algorithm that considers the
characteristics of conditional text generation and the input context.
Experimental results demonstrate that our proposed method yields substantial
improvements across various text generation models, including BART and Flan-T5,
in tasks such as summarization and data-to-text generation while maintaining
detection ability.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Composite Diffusion | whole >= \Sigma parts. (arXiv:2307.13720v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13720">http://arxiv.org/abs/2307.13720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13720] Composite Diffusion | whole >= \Sigma parts](http://arxiv.org/abs/2307.13720) #diffusion</code></li>
<li>Summary: <p>For an artist or a graphic designer, the spatial layout of a scene is a
critical design choice. However, existing text-to-image diffusion models
provide limited support for incorporating spatial information. This paper
introduces Composite Diffusion as a means for artists to generate high-quality
images by composing from the sub-scenes. The artists can specify the
arrangement of these sub-scenes through a flexible free-form segment layout.
They can describe the content of each sub-scene primarily using natural text
and additionally by utilizing reference images or control inputs such as line
art, scribbles, human pose, canny edges, and more.
</p></li>
</ul>

<p>We provide a comprehensive and modular method for Composite Diffusion that
enables alternative ways of generating, composing, and harmonizing sub-scenes.
Further, we wish to evaluate the composite image for effectiveness in both
image quality and achieving the artist's intent. We argue that existing image
quality metrics lack a holistic evaluation of image composites. To address
this, we propose novel quality criteria especially relevant to composite
generation.
</p>
<p>We believe that our approach provides an intuitive method of art creation.
Through extensive user surveys, quantitative and qualitative analysis, we show
how it achieves greater spatial, semantic, and creative control over image
generation. In addition, our methods do not need to retrain or modify the
architecture of the base diffusion models and can work in a plug-and-play
manner with the fine-tuned models.
</p>

<h3>Title: Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation. (arXiv:2307.13908v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13908">http://arxiv.org/abs/2307.13908</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13908] Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation](http://arxiv.org/abs/2307.13908) #diffusion</code></li>
<li>Summary: <p>Text-to-3D generation has recently garnered significant attention, fueled by
2D diffusion models trained on billions of image-text pairs. Existing methods
primarily rely on score distillation to leverage the 2D diffusion priors to
supervise the generation of 3D models, e.g., NeRF. However, score distillation
is prone to suffer the view inconsistency problem, and implicit NeRF modeling
can also lead to an arbitrary shape, thus leading to less realistic and
uncontrollable 3D generation. In this work, we propose a flexible framework of
Points-to-3D to bridge the gap between sparse yet freely available 3D points
and realistic shape-controllable 3D generation by distilling the knowledge from
both 2D and 3D diffusion models. The core idea of Points-to-3D is to introduce
controllable sparse 3D points to guide the text-to-3D generation. Specifically,
we use the sparse point cloud generated from the 3D diffusion model, Point-E,
as the geometric prior, conditioned on a single reference image. To better
utilize the sparse 3D points, we propose an efficient point cloud guidance loss
to adaptively drive the NeRF's geometry to align with the shape of the sparse
3D points. In addition to controlling the geometry, we propose to optimize the
NeRF for a more view-consistent appearance. To be specific, we perform score
distillation to the publicly available 2D image diffusion model ControlNet,
conditioned on text as well as depth map of the learned compact geometry.
Qualitative and quantitative comparisons demonstrate that Points-to-3D improves
view consistency and achieves good shape controllability for text-to-3D
generation. Points-to-3D provides users with a new way to improve and control
text-to-3D generation.
</p></li>
</ul>

<h3>Title: Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14066">http://arxiv.org/abs/2307.14066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14066] Pre-Training with Diffusion models for Dental Radiography segmentation](http://arxiv.org/abs/2307.14066) #diffusion</code></li>
<li>Summary: <p>Medical radiography segmentation, and specifically dental radiography, is
highly limited by the cost of labeling which requires specific expertise and
labor-intensive annotations. In this work, we propose a straightforward
pre-training method for semantic segmentation leveraging Denoising Diffusion
Probabilistic Models (DDPM), which have shown impressive results for generative
modeling. Our straightforward approach achieves remarkable performance in terms
of label efficiency and does not require architectural modifications between
pre-training and downstream tasks. We propose to first pre-train a Unet by
exploiting the DDPM training objective, and then fine-tune the resulting model
on a segmentation task. Our experimental results on the segmentation of dental
radiographs demonstrate that the proposed method is competitive with
state-of-the-art pre-training methods.
</p></li>
</ul>

<h3>Title: VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet. (arXiv:2307.14073v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14073">http://arxiv.org/abs/2307.14073</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14073] VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet](http://arxiv.org/abs/2307.14073) #diffusion</code></li>
<li>Summary: <p>Recently, diffusion models like StableDiffusion have achieved impressive
image generation results. However, the generation process of such diffusion
models is uncontrollable, which makes it hard to generate videos with
continuous and consistent content. In this work, by using the diffusion model
with ControlNet, we proposed a new motion-guided video-to-video translation
framework called VideoControlNet to generate various videos based on the given
prompts and the condition from the input video. Inspired by the video codecs
that use motion information for reducing temporal redundancy, our framework
uses motion information to prevent the regeneration of the redundant areas for
content consistency. Specifically, we generate the first frame (i.e., the
I-frame) by using the diffusion model with ControlNet. Then we generate other
key frames (i.e., the P-frame) based on the previous I/P-frame by using our
newly proposed motion-guided P-frame generation (MgPG) method, in which the
P-frames are generated based on the motion information and the occlusion areas
are inpainted by using the diffusion model. Finally, the rest frames (i.e., the
B-frame) are generated by using our motion-guided B-frame interpolation (MgBI)
module. Our experiments demonstrate that our proposed VideoControlNet inherits
the generation capability of the pre-trained large diffusion model and extends
the image diffusion model to the video diffusion model by using motion
information. More results are provided at our project page.
</p></li>
</ul>

<h3>Title: Visual Instruction Inversion: Image Editing via Visual Prompting. (arXiv:2307.14331v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14331">http://arxiv.org/abs/2307.14331</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14331] Visual Instruction Inversion: Image Editing via Visual Prompting](http://arxiv.org/abs/2307.14331) #diffusion</code></li>
<li>Summary: <p>Text-conditioned image editing has emerged as a powerful tool for editing
images. However, in many situations, language can be ambiguous and ineffective
in describing specific image edits. When faced with such challenges, visual
prompts can be a more informative and intuitive way to convey ideas. We present
a method for image editing via visual prompting. Given pairs of example that
represent the "before" and "after" images of an edit, our goal is to learn a
text-based editing direction that can be used to perform the same edit on new
images. We leverage the rich, pretrained editing capabilities of text-to-image
diffusion models by inverting visual prompts into editing instructions. Our
results show that with just one example pair, we can achieve competitive
results compared to state-of-the-art text-conditioned image editing frameworks.
</p></li>
</ul>

<h3>Title: How Does Diffusion Influence Pretrained Language Models on Out-of-Distribution Data?. (arXiv:2307.13949v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13949">http://arxiv.org/abs/2307.13949</a></li>
<li>Code URL: <a href="https://github.com/maybelizzy/diffusion_ood_robustness">https://github.com/maybelizzy/diffusion_ood_robustness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13949] How Does Diffusion Influence Pretrained Language Models on Out-of-Distribution Data?](http://arxiv.org/abs/2307.13949) #diffusion</code></li>
<li>Summary: <p>Transformer-based pretrained language models (PLMs) have achieved great
success in modern NLP. An important advantage of PLMs is good
out-of-distribution (OOD) robustness. Recently, diffusion models have attracted
a lot of work to apply diffusion to PLMs. It remains under-explored how
diffusion influences PLMs on OOD data. The core of diffusion models is a
forward diffusion process which gradually applies Gaussian noise to inputs, and
a reverse denoising process which removes noise. The noised input
reconstruction is a fundamental ability of diffusion models. We directly
analyze OOD robustness by measuring the reconstruction loss, including testing
the abilities to reconstruct OOD data, and to detect OOD samples. Experiments
are conducted by analyzing different training parameters and data statistical
features on eight datasets. It shows that finetuning PLMs with diffusion
degrades the reconstruction ability on OOD data. The comparison also shows that
diffusion models can effectively detect OOD samples, achieving state-of-the-art
performance in most of the datasets with an absolute accuracy improvement up to
18%. These results indicate that diffusion reduces OOD robustness of PLMs.
</p></li>
</ul>

<h3>Title: Founding a mathematical diffusion model in linguistics. The case study of German syntactic features in the North-Eastern Italian dialects. (arXiv:2307.14291v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14291">http://arxiv.org/abs/2307.14291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14291] Founding a mathematical diffusion model in linguistics](http://arxiv.org/abs/2307.14291) #diffusion</code></li>
<li>Summary: <p>We take as a case study the spread of Germanic syntactic features into
Romance dialects of North-Eastern Italy, which occurred after the immigration
of German people in the Tyrol during the High Middle Ages.
</p></li>
</ul>

<p>An interactive map is produced using tools of what is called Geographic Data
Science. A smooth two-dimensional surface $\mathcal{G}$ expresses locally which
fraction of territory uses a given German language feature: it is obtained by
interpolating a discrete function that says if at any surveyed locality that
feature is used or not.\newline
</p>
<p>This surface $\mathcal{G}$ is thought of as the value at the present time of
a function describing a diffusion-convection phenomenon in two dimensions (here
said \emph{tidal} mode), which is subjected in a very natural way to the same
equation, suitably contextualized, used in physics for a number of
phenomenological facts like the heat diffusion. It is shown that solutions of
this equation, evaluated at the present time, fit well with the data as
interpolated by $\mathcal{G}$, thus providing convincing pictures of
diffusion-convection of the linguistic features of the case study, albeit
simplifications and approximations.\newline
</p>
<p>Very importantly, it is shown that Schmidt's 'waves' can be counted among the
solutions of the diffusion equation: superimposing Schmidt 'waves' to a 'tidal
flooding' can reproduce complexities of real linguistic diffusion events.
</p>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: PlaneRecTR: Unified Query learning for 3D Plane Recovery from a Single View. (arXiv:2307.13756v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13756">http://arxiv.org/abs/2307.13756</a></li>
<li>Code URL: <a href="https://github.com/sjingjia/planerectr">https://github.com/sjingjia/planerectr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13756] PlaneRecTR: Unified Query learning for 3D Plane Recovery from a Single View](http://arxiv.org/abs/2307.13756) #transformer</code></li>
<li>Summary: <p>3D plane recovery from a single image can usually be divided into several
subtasks of plane detection, segmentation, parameter estimation and possibly
depth estimation. Previous works tend to solve this task by either extending
the RCNN-based segmentation network or the dense pixel embedding-based
clustering framework. However, none of them tried to integrate above related
subtasks into a unified framework but treat them separately and sequentially,
which we suspect is potentially a main source of performance limitation for
existing approaches. Motivated by this finding and the success of query-based
learning in enriching reasoning among semantic entities, in this paper, we
propose PlaneRecTR, a Transformer-based architecture, which for the first time
unifies all subtasks related to single-view plane recovery with a single
compact model. Extensive quantitative and qualitative experiments demonstrate
that our proposed unified learning achieves mutual benefits across subtasks,
obtaining a new state-of-the-art performance on public ScanNet and NYUv2-Plane
datasets. Codes are available at https://github.com/SJingjia/PlaneRecTR.
</p></li>
</ul>

<h3>Title: E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning. (arXiv:2307.13770v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13770">http://arxiv.org/abs/2307.13770</a></li>
<li>Code URL: <a href="https://github.com/chenghan111/e2vpt">https://github.com/chenghan111/e2vpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13770] E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning](http://arxiv.org/abs/2307.13770) #transformer</code></li>
<li>Summary: <p>As the size of transformer-based models continues to grow, fine-tuning these
large-scale pretrained vision models for new tasks has become increasingly
parameter-intensive. Parameter-efficient learning has been developed to reduce
the number of tunable parameters during fine-tuning. Although these methods
show promising results, there is still a significant performance gap compared
to full fine-tuning. To address this challenge, we propose an Effective and
Efficient Visual Prompt Tuning (E^2VPT) approach for large-scale
transformer-based model adaptation. Specifically, we introduce a set of
learnable key-value prompts and visual prompts into self-attention and input
layers, respectively, to improve the effectiveness of model fine-tuning.
Moreover, we design a prompt pruning procedure to systematically prune low
importance prompts while preserving model performance, which largely enhances
the model's efficiency. Empirical results demonstrate that our approach
outperforms several state-of-the-art baselines on two benchmarks, with
considerably low parameter usage (e.g., 0.32% of model parameters on VTAB-1k).
Our code is available at https://github.com/ChengHan111/E2VPT.
</p></li>
</ul>

<h3>Title: CosSIF: Cosine similarity-based image filtering to overcome low inter-class variation in synthetic medical image datasets. (arXiv:2307.13842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13842">http://arxiv.org/abs/2307.13842</a></li>
<li>Code URL: <a href="https://github.com/mominul-ssv/cossif">https://github.com/mominul-ssv/cossif</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13842] CosSIF: Cosine similarity-based image filtering to overcome low inter-class variation in synthetic medical image datasets](http://arxiv.org/abs/2307.13842) #transformer</code></li>
<li>Summary: <p>Crafting effective deep learning models for medical image analysis is a
complex task, particularly in cases where the medical image dataset lacks
significant inter-class variation. This challenge is further aggravated when
employing such datasets to generate synthetic images using generative
adversarial networks (GANs), as the output of GANs heavily relies on the input
data. In this research, we propose a novel filtering algorithm called Cosine
Similarity-based Image Filtering (CosSIF). We leverage CosSIF to develop two
distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering
After GAN Training (FAGT). FBGT involves the removal of real images that
exhibit similarities to images of other classes before utilizing them as the
training dataset for a GAN. On the other hand, FAGT focuses on eliminating
synthetic images with less discriminative features compared to real images used
for training the GAN. Experimental results reveal that employing either the
FAGT or FBGT method with modern transformer and convolutional-based networks
leads to substantial performance gains in various evaluation metrics. FAGT
implementation on the ISIC-2016 dataset surpasses the baseline method in terms
of sensitivity by 1.59\% and AUC by 1.88\%. Furthermore, for the HAM10000
dataset, applying FABT outperforms the baseline approach in terms of recall by
13.75\%, and with the sole implementation of FAGT, achieves a maximum accuracy
of 94.44\%.
</p></li>
</ul>

<h3>Title: On the unreasonable vulnerability of transformers for image restoration -- and an easy fix. (arXiv:2307.13856v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13856">http://arxiv.org/abs/2307.13856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13856] On the unreasonable vulnerability of transformers for image restoration -- and an easy fix](http://arxiv.org/abs/2307.13856) #transformer</code></li>
<li>Summary: <p>Following their success in visual recognition tasks, Vision
Transformers(ViTs) are being increasingly employed for image restoration. As a
few recent works claim that ViTs for image classification also have better
robustness properties, we investigate whether the improved adversarial
robustness of ViTs extends to image restoration. We consider the recently
proposed Restormer model, as well as NAFNet and the "Baseline network" which
are both simplified versions of a Restormer. We use Projected Gradient Descent
(PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise
prediction tasks for our robustness evaluation. Our experiments are performed
on real-world images from the GoPro dataset for image deblurring. Our analysis
indicates that contrary to as advocated by ViTs in image classification works,
these models are highly susceptible to adversarial attacks. We attempt to
improve their robustness through adversarial training. While this yields a
significant increase in robustness for Restormer, results on other networks are
less promising. Interestingly, the design choices in NAFNet and Baselines,
which were based on iid performance, and not on robust generalization, seem to
be at odds with the model robustness. Thus, we investigate this further and
find a fix.
</p></li>
</ul>

<h3>Title: Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT. (arXiv:2307.13865v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13865">http://arxiv.org/abs/2307.13865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13865] Pretrained Deep 2](http://arxiv.org/abs/2307.13865) #transformer</code></li>
<li>Summary: <p>In the field of medical imaging, 3D deep learning models play a crucial role
in building powerful predictive models of disease progression. However, the
size of these models presents significant challenges, both in terms of
computational resources and data requirements. Moreover, achieving high-quality
pretraining of 3D models proves to be even more challenging. To address these
issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D
volumetric data efficiently using 2D models. Combining 2D and 3D techniques
offers a promising avenue for optimizing performance while minimizing memory
requirements. In this paper, we explore 2.5D architectures based on a
combination of convolutional neural networks (CNNs), long short-term memory
(LSTM), and Transformers. In addition, leveraging the benefits of recent
non-contrastive pretraining approaches in 2D, we enhanced the performance and
data efficiency of 2.5D techniques even further. We demonstrate the
effectiveness of architectures and associated pretraining on a task of
predicting progression to wet age-related macular degeneration (AMD) within a
six-month period on two large longitudinal OCT datasets.
</p></li>
</ul>

<h3>Title: AViT: Adapting Vision Transformers for Small Skin Lesion Segmentation Datasets. (arXiv:2307.13897v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13897">http://arxiv.org/abs/2307.13897</a></li>
<li>Code URL: <a href="https://github.com/siyi-wind/avit">https://github.com/siyi-wind/avit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13897] AViT: Adapting Vision Transformers for Small Skin Lesion Segmentation Datasets](http://arxiv.org/abs/2307.13897) #transformer</code></li>
<li>Summary: <p>Skin lesion segmentation (SLS) plays an important role in skin lesion
analysis. Vision transformers (ViTs) are considered an auspicious solution for
SLS, but they require more training data compared to convolutional neural
networks (CNNs) due to their inherent parameter-heavy structure and lack of
some inductive biases. To alleviate this issue, current approaches fine-tune
pre-trained ViT backbones on SLS datasets, aiming to leverage the knowledge
learned from a larger set of natural images to lower the amount of skin
training data needed. However, fully fine-tuning all parameters of large
backbones is computationally expensive and memory intensive. In this paper, we
propose AViT, a novel efficient strategy to mitigate ViTs' data-hunger by
transferring any pre-trained ViTs to the SLS task. Specifically, we integrate
lightweight modules (adapters) within the transformer layers, which modulate
the feature representation of a ViT without updating its pre-trained weights.
In addition, we employ a shallow CNN as a prompt generator to create a prompt
embedding from the input image, which grasps fine-grained information and CNN's
inductive biases to guide the segmentation task on small datasets. Our
quantitative experiments on 4 skin lesion datasets demonstrate that AViT
achieves competitive, and at times superior, performance to SOTA but with
significantly fewer trainable parameters. Our code is available at
https://github.com/siyi-wind/AViT.
</p></li>
</ul>

<h3>Title: Adaptive Frequency Filters As Efficient Global Token Mixers. (arXiv:2307.14008v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14008">http://arxiv.org/abs/2307.14008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14008] Adaptive Frequency Filters As Efficient Global Token Mixers](http://arxiv.org/abs/2307.14008) #transformer</code></li>
<li>Summary: <p>Recent vision transformers, large-kernel CNNs and MLPs have attained
remarkable successes in broad vision tasks thanks to their effective
information fusion in the global scope. However, their efficient deployments,
especially on mobile devices, still suffer from noteworthy challenges due to
the heavy computational costs of self-attention mechanisms, large kernels, or
fully connected layers. In this work, we apply conventional convolution theorem
to deep learning for addressing this and reveal that adaptive frequency filters
can serve as efficient global token mixers. With this insight, we propose
Adaptive Frequency Filtering (AFF) token mixer. This neural operator transfers
a latent representation to the frequency domain via a Fourier transform and
performs semantic-adaptive frequency filtering via an elementwise
multiplication, which mathematically equals to a token mixing operation in the
original latent space with a dynamic convolution kernel as large as the spatial
resolution of this latent representation. We take AFF token mixers as primary
neural operators to build a lightweight neural network, dubbed AFFNet.
Extensive experiments demonstrate the effectiveness of our proposed AFF token
mixer and show that AFFNet achieve superior accuracy and efficiency trade-offs
compared to other lightweight network designs on broad visual tasks, including
visual recognition and dense prediction tasks.
</p></li>
</ul>

<h3>Title: ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution. (arXiv:2307.14010v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14010">http://arxiv.org/abs/2307.14010</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14010] ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2307.14010) #transformer</code></li>
<li>Summary: <p>Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a
high-resolution hyperspectral image from a low-resolution observation. However,
the prevailing CNN-based approaches have shown limitations in building
long-range dependencies and capturing interaction information between spectral
features. This results in inadequate utilization of spectral information and
artifacts after upsampling. To address this issue, we propose ESSAformer, an
ESSA attention-embedded Transformer network for single-HSI-SR with an iterative
refining structure. Specifically, we first introduce a robust and
spectral-friendly similarity metric, \ie, the spectral correlation coefficient
of the spectrum (SCC), to replace the original attention matrix and
incorporates inductive biases into the model to facilitate training. Built upon
it, we further utilize the kernelizable attention technique with theoretical
support to form a novel efficient SCC-kernel-based self-attention (ESSA) and
reduce attention computation to linear complexity. ESSA enlarges the receptive
field for features after upsampling without bringing much computation and
allows the model to effectively utilize spatial-spectral information from
different scales, resulting in the generation of more natural high-resolution
images. Without the need for pretraining on large-scale datasets, our
experiments demonstrate ESSA's effectiveness in both visual quality and
quantitative results.
</p></li>
</ul>

<h3>Title: Sparse Double Descent in Vision Transformers: real or phantom threat?. (arXiv:2307.14253v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14253">http://arxiv.org/abs/2307.14253</a></li>
<li>Code URL: <a href="https://github.com/vgcq/sdd_vit">https://github.com/vgcq/sdd_vit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14253] Sparse Double Descent in Vision Transformers: real or phantom threat?](http://arxiv.org/abs/2307.14253) #transformer</code></li>
<li>Summary: <p>Vision transformers (ViT) have been of broad interest in recent theoretical
and empirical works. They are state-of-the-art thanks to their attention-based
approach, which boosts the identification of key features and patterns within
images thanks to the capability of avoiding inductive bias, resulting in highly
accurate image analysis. Meanwhile, neoteric studies have reported a ``sparse
double descent'' phenomenon that can occur in modern deep-learning models,
where extremely over-parametrized models can generalize well. This raises
practical questions about the optimal size of the model and the quest over
finding the best trade-off between sparsity and performance is launched: are
Vision Transformers also prone to sparse double descent? Can we find a way to
avoid such a phenomenon? Our work tackles the occurrence of sparse double
descent on ViTs. Despite some works that have shown that traditional
architectures, like Resnet, are condemned to the sparse double descent
phenomenon, for ViTs we observe that an optimally-tuned $\ell_2$ regularization
relieves such a phenomenon. However, everything comes at a cost: optimal lambda
will sacrifice the potential compression of the ViT.
</p></li>
</ul>

<h3>Title: Event-based Vision for Early Prediction of Manipulation Actions. (arXiv:2307.14332v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14332">http://arxiv.org/abs/2307.14332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14332] Event-based Vision for Early Prediction of Manipulation Actions](http://arxiv.org/abs/2307.14332) #transformer</code></li>
<li>Summary: <p>Neuromorphic visual sensors are artificial retinas that output sequences of
asynchronous events when brightness changes occur in the scene. These sensors
offer many advantages including very high temporal resolution, no motion blur
and smart data compression ideal for real-time processing. In this study, we
introduce an event-based dataset on fine-grained manipulation actions and
perform an experimental study on the use of transformers for action prediction
with events. There is enormous interest in the fields of cognitive robotics and
human-robot interaction on understanding and predicting human actions as early
as possible. Early prediction allows anticipating complex stages for planning,
enabling effective and real-time interaction. Our Transformer network uses
events to predict manipulation actions as they occur, using online inference.
The model succeeds at predicting actions early on, building up confidence over
time and achieving state-of-the-art classification. Moreover, the
attention-based transformer architecture allows us to study the role of the
spatio-temporal patterns selected by the model. Our experiments show that the
Transformer network captures action dynamic features outperforming video-based
approaches and succeeding with scenarios where the differences between actions
lie in very subtle cues. Finally, we release the new event dataset, which is
the first in the literature for manipulation action recognition. Code will be
available at https://github.com/DaniDeniz/EventVisionTransformer.
</p></li>
</ul>

<h3>Title: This is not correct! Negation-aware Evaluation of Language Generation Systems. (arXiv:2307.13989v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13989">http://arxiv.org/abs/2307.13989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13989] This is not correct! Negation-aware Evaluation of Language Generation Systems](http://arxiv.org/abs/2307.13989) #transformer</code></li>
<li>Summary: <p>Large language models underestimate the impact of negations on how much they
change the meaning of a sentence. Therefore, learned evaluation metrics based
on these models are insensitive to negations. In this paper, we propose
NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that,
we designed a rule-based sentence negation tool and used it to create the
CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a
sentence transformer and an evaluation metric to improve their negation
sensitivity. Evaluating these models on existing benchmarks shows that our
fine-tuned models outperform existing metrics on the negated sentences by far
while preserving their base models' performances on other perturbations.
</p></li>
</ul>

<h3>Title: Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges, and Possible Future Directions. (arXiv:2307.14107v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14107">http://arxiv.org/abs/2307.14107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14107] Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges, and Possible Future Directions](http://arxiv.org/abs/2307.14107) #transformer</code></li>
<li>Summary: <p>Chat Generative Pre-trained Transformer (ChatGPT) has gained significant
interest and attention since its launch in November 2022. It has shown
impressive performance in various domains, including passing exams and creative
writing. However, challenges and concerns related to biases and trust persist.
In this work, we present a comprehensive review of over 100 Scopus-indexed
publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and
explore its applications. We critically analyze the existing literature,
identifying common approaches employed in the studies. Additionally, we
investigate diverse application areas where ChatGPT has found utility, such as
healthcare, marketing and financial services, software engineering, academic
and scientific writing, research and education, environmental science, and
natural language processing. Through examining these applications, we gain
valuable insights into the potential of ChatGPT in addressing real-world
challenges. We also discuss crucial issues related to ChatGPT, including biases
and trustworthiness, emphasizing the need for further research and development
in these areas. Furthermore, we identify potential future directions for
ChatGPT research, proposing solutions to current challenges and speculating on
expected advancements. By fully leveraging the capabilities of ChatGPT, we can
unlock its potential across various domains, leading to advancements in
conversational AI and transformative impacts in society.
</p></li>
</ul>

<h3>Title: Comparative Analysis of Libraries for the Sentimental Analysis. (arXiv:2307.14311v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14311">http://arxiv.org/abs/2307.14311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14311] Comparative Analysis of Libraries for the Sentimental Analysis](http://arxiv.org/abs/2307.14311) #transformer</code></li>
<li>Summary: <p>This study is main goal is to provide a comparative comparison of libraries
using machine learning methods. Experts in natural language processing (NLP)
are becoming more and more interested in sentiment analysis (SA) of text
changes. The objective of employing NLP text analysis techniques is to
recognize and categorize feelings related to twitter users utterances. In this
examination, issues with SA and the libraries utilized are also looked at.
provides a number of cooperative methods to classify emotional polarity. The
Naive Bayes Classifier, Decision Tree Classifier, Maxent Classifier, Sklearn
Classifier, Sklearn Classifier MultinomialNB, and other conjoint learning
algorithms, according to recent research, are very effective. In the project
will use Five Python and R libraries NLTK, TextBlob, Vader, Transformers (GPT
and BERT pretrained), and Tidytext will be used in the study to apply sentiment
analysis techniques. Four machine learning models Tree of Decisions (DT),
Support Vector Machine (SVM), Naive Bayes (NB), and K-Nearest Neighbor (KNN)
will also be used. To evaluate how well libraries for SA operate in the social
network environment, comparative study was also carried out. The measures to
assess the best algorithms in this experiment, which used a single data set for
each method, were precision, recall, and F1 score. We conclude that the BERT
transformer method with an Accuracy: 0.973 is recommended for sentiment
analysis.
</p></li>
</ul>

<h3>Title: Understanding Deep Neural Networks via Linear Separability of Hidden Layers. (arXiv:2307.13962v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13962">http://arxiv.org/abs/2307.13962</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13962] Understanding Deep Neural Networks via Linear Separability of Hidden Layers](http://arxiv.org/abs/2307.13962) #transformer</code></li>
<li>Summary: <p>In this paper, we measure the linear separability of hidden layer outputs to
study the characteristics of deep neural networks. In particular, we first
propose Minkowski difference based linear separability measures (MD-LSMs) to
evaluate the linear separability degree of two points sets. Then, we
demonstrate that there is a synchronicity between the linear separability
degree of hidden layer outputs and the network training performance, i.e., if
the updated weights can enhance the linear separability degree of hidden layer
outputs, the updated network will achieve a better training performance, and
vice versa. Moreover, we study the effect of activation function and network
size (including width and depth) on the linear separability of hidden layers.
Finally, we conduct the numerical experiments to validate our findings on some
popular deep networks including multilayer perceptron (MLP), convolutional
neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet,
vision transformer (ViT) and GoogLeNet.
</p></li>
</ul>

<h3>Title: Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?. (arXiv:2307.14023v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14023">http://arxiv.org/abs/2307.14023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14023] Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?](http://arxiv.org/abs/2307.14023) #transformer</code></li>
<li>Summary: <p>Existing analyses of the expressive capacity of Transformer models have
required excessively deep layers for data memorization, leading to a
discrepancy with the Transformers actually used in practice. This is primarily
due to the interpretation of the softmax function as an approximation of the
hardmax function. By clarifying the connection between the softmax function and
the Boltzmann operator, we prove that a single layer of self-attention with
low-rank weight matrices possesses the capability to perfectly capture the
context of an entire input sequence. As a consequence, we show that
single-layer Transformer has a memorization capacity for finite samples, and
that Transformers consisting of one self-attention layer with two feed-forward
neural networks are universal approximators for continuous functions on a
compact domain.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Regularizing Neural Networks with Meta-Learning Generative Models. (arXiv:2307.13899v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13899">http://arxiv.org/abs/2307.13899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13899] Regularizing Neural Networks with Meta-Learning Generative Models](http://arxiv.org/abs/2307.13899) #generative</code></li>
<li>Summary: <p>This paper investigates methods for improving generative data augmentation
for deep learning. Generative data augmentation leverages the synthetic samples
produced by generative models as an additional dataset for classification with
small dataset settings. A key challenge of generative data augmentation is that
the synthetic data contain uninformative samples that degrade accuracy. This is
because the synthetic samples do not perfectly represent class categories in
real data and uniform sampling does not necessarily provide useful samples for
tasks. In this paper, we present a novel strategy for generative data
augmentation called meta generative regularization (MGR). To avoid the
degradation of generative data augmentation, MGR utilizes synthetic samples in
the regularization term for feature extractors instead of in the loss function,
e.g., cross-entropy. These synthetic samples are dynamically determined to
minimize the validation losses through meta-learning. We observed that MGR can
avoid the performance degradation of na\"ive generative data augmentation and
boost the baselines. Experiments on six datasets showed that MGR is effective
particularly when datasets are smaller and stably outperforms baselines.
</p></li>
</ul>

<h3>Title: Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation. (arXiv:2307.13978v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13978">http://arxiv.org/abs/2307.13978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13978] Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation](http://arxiv.org/abs/2307.13978) #generative</code></li>
<li>Summary: <p>Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to
generate realistic outputs based on training datasets. However, the challenge
of exerting control over the generation process of GANs remains a significant
hurdle. In this paper, we propose a novel methodology to address this issue by
integrating a reinforcement learning (RL) agent with a latent-space GAN
(l-GAN), thereby facilitating the generation of desired outputs. More
specifically, we have developed an actor-critic RL agent with a meticulously
designed reward policy, enabling it to acquire proficiency in navigating the
latent space of the l-GAN and generating outputs based on specified tasks. To
substantiate the efficacy of our approach, we have conducted a series of
experiments employing the MNIST dataset, including arithmetic addition as an
illustrative task. The outcomes of these experiments serve to validate our
methodology. Our pioneering integration of an RL agent with a GAN model
represents a novel advancement, holding great potential for enhancing
generative networks in the future.
</p></li>
</ul>

<h3>Title: 3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability. (arXiv:2307.14051v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14051">http://arxiv.org/abs/2307.14051</a></li>
<li>Code URL: <a href="https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser">https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14051] 3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability](http://arxiv.org/abs/2307.14051) #generative</code></li>
<li>Summary: <p>Shape generation is the practice of producing 3D shapes as various
representations for 3D content creation. Previous studies on 3D shape
generation have focused on shape quality and structure, without or less
considering the importance of semantic information. Consequently, such
generative models often fail to preserve the semantic consistency of shape
structure or enable manipulation of the semantic attributes of shapes during
generation. In this paper, we proposed a novel semantic generative model named
3D Semantic Subspace Traverser that utilizes semantic attributes for
category-specific 3D shape generation and editing. Our method utilizes implicit
functions as the 3D shape representation and combines a novel latent-space GAN
with a linear subspace model to discover semantic dimensions in the local
latent space of 3D shapes. Each dimension of the subspace corresponds to a
particular semantic attribute, and we can edit the attributes of generated
shapes by traversing the coefficients of those dimensions. Experimental results
demonstrate that our method can produce plausible shapes with complex
structures and enable the editing of semantic attributes. The code and trained
models are available at
https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser
</p></li>
</ul>

<h3>Title: Towards Generalist Biomedical AI. (arXiv:2307.14334v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14334">http://arxiv.org/abs/2307.14334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14334] Towards Generalist Biomedical AI](http://arxiv.org/abs/2307.14334) #generative</code></li>
<li>Summary: <p>Medicine is inherently multimodal, with rich data modalities spanning text,
imaging, genomics, and more. Generalist biomedical artificial intelligence (AI)
systems that flexibly encode, integrate, and interpret this data at scale can
potentially enable impactful applications ranging from scientific discovery to
care delivery. To enable the development of these models, we first curate
MultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses
14 diverse tasks such as medical question answering, mammography and
dermatology image interpretation, radiology report generation and
summarization, and genomic variant calling. We then introduce Med-PaLM
Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI
system. Med-PaLM M is a large multimodal generative model that flexibly encodes
and interprets biomedical data including clinical language, imaging, and
genomics with the same set of model weights. Med-PaLM M reaches performance
competitive with or exceeding the state of the art on all MultiMedBench tasks,
often surpassing specialist models by a wide margin. We also report examples of
zero-shot generalization to novel medical concepts and tasks, positive transfer
learning across tasks, and emergent zero-shot medical reasoning. To further
probe the capabilities and limitations of Med-PaLM M, we conduct a radiologist
evaluation of model-generated (and human) chest X-ray reports and observe
encouraging performance across model scales. In a side-by-side ranking on 246
retrospective chest X-rays, clinicians express a pairwise preference for
Med-PaLM M reports over those produced by radiologists in up to 40.50% of
cases, suggesting potential clinical utility. While considerable work is needed
to validate these models in real-world use cases, our results represent a
milestone towards the development of generalist biomedical AI systems.
</p></li>
</ul>

<h3>Title: GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs. (arXiv:2307.14109v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14109">http://arxiv.org/abs/2307.14109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14109] GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs](http://arxiv.org/abs/2307.14109) #generative</code></li>
<li>Summary: <p>GraphRNN is a deep learning-based architecture proposed by You et al. for
learning generative models for graphs. We replicate the results of You et al.
using a reproduced implementation of the GraphRNN architecture and evaluate
this against baseline models using new metrics. Through an ablation study, we
find that the BFS traversal suggested by You et al. to collapse representations
of isomorphic graphs contributes significantly to model performance.
Additionally, we extend GraphRNN to generate directed acyclic graphs by
replacing the BFS traversal with a topological sort. We demonstrate that this
method improves significantly over a directed-multiclass variant of GraphRNN on
a real-world dataset.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Is GPT a Computational Model of Emotion? Detailed Analysis. (arXiv:2307.13779v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13779">http://arxiv.org/abs/2307.13779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13779] Is GPT a Computational Model of Emotion? Detailed Analysis](http://arxiv.org/abs/2307.13779) #large language model</code></li>
<li>Summary: <p>This paper investigates the emotional reasoning abilities of the GPT family
of large language models via a component perspective. The paper first examines
how the model reasons about autobiographical memories. Second, it
systematically varies aspects of situations to impact emotion intensity and
coping tendencies. Even without the use of prompt engineering, it is shown that
GPT's predictions align significantly with human-provided appraisals and
emotional labels. However, GPT faces difficulties predicting emotion intensity
and coping responses. GPT-4 showed the highest performance in the initial study
but fell short in the second, despite providing superior results after minor
prompt engineering. This assessment brings up questions on how to effectively
employ the strong points and address the weak areas of these models,
particularly concerning response variability. These studies underscore the
merits of evaluating models from a componential perspective.
</p></li>
</ul>

<h3>Title: GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning. (arXiv:2307.13923v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13923">http://arxiv.org/abs/2307.13923</a></li>
<li>Code URL: <a href="https://github.com/freedomintelligence/grammargpt">https://github.com/freedomintelligence/grammargpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13923] GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning](http://arxiv.org/abs/2307.13923) #large language model</code></li>
<li>Summary: <p>Grammatical error correction aims to correct ungrammatical sentences
automatically. Recently, some work has demonstrated the excellent capabilities
of closed-source Large Language Models (LLMs, e.g., ChatGPT) in grammatical
error correction. However, the potential of open-source LLMs remains
unexplored. In this paper, we introduced GrammarGPT, an open-source LLM, to
preliminary explore its potential for native Chinese grammatical error
correction. The core recipe of GrammarGPT is to leverage the hybrid dataset of
ChatGPT-generated and human-annotated. For grammatical errors with clues, we
proposed a heuristic method to guide ChatGPT to generate ungrammatical
sentences by providing those clues. For grammatical errors without clues, we
collected ungrammatical sentences from publicly available websites and manually
corrected them. In addition, we employed an error-invariant augmentation method
to enhance the ability of the model to correct native Chinese grammatical
errors. We ultimately constructed about 1k parallel data and utilized these
data to fine-tune open-source LLMs (e.g., Phoenix, released by The Chinese
University of Hong Kong, Shenzhen) with instruction tuning. The experimental
results show that GrammarGPT outperforms the existing SOTA system
significantly. Although model parameters are 20x larger than the SOTA baseline,
the required amount of data for instruction tuning is 1200x smaller,
illustrating the potential of open-source LLMs on native CGEC. Our GrammarGPT
ranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach's
effectiveness. The code and data are available at
\url{https://github.com/FreedomIntelligence/GrammarGPT}.
</p></li>
</ul>

<h3>Title: Evaluating the Moral Beliefs Encoded in LLMs. (arXiv:2307.14324v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14324">http://arxiv.org/abs/2307.14324</a></li>
<li>Code URL: <a href="https://github.com/ninodimontalcino/moralchoice">https://github.com/ninodimontalcino/moralchoice</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14324] Evaluating the Moral Beliefs Encoded in LLMs](http://arxiv.org/abs/2307.14324) #large language model</code></li>
<li>Summary: <p>This paper presents a case study on the design, administration,
post-processing, and evaluation of surveys on large language models (LLMs). It
comprises two components: (1) A statistical method for eliciting beliefs
encoded in LLMs. We introduce statistical measures and evaluation metrics that
quantify the probability of an LLM "making a choice", the associated
uncertainty, and the consistency of that choice. (2) We apply this method to
study what moral beliefs are encoded in different LLMs, especially in ambiguous
cases where the right choice is not obvious. We design a large-scale survey
comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white
lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a
pedestrian on the road?"). Each scenario includes a description, two possible
actions, and auxiliary labels indicating violated rules (e.g., "do not kill").
We administer the survey to 28 open- and closed-source LLMs. We find that (a)
in unambiguous scenarios, most models "choose" actions that align with
commonsense. In ambiguous cases, most models express uncertainty. (b) Some
models are uncertain about choosing the commonsense action because their
responses are sensitive to the question-wording. (c) Some models reflect clear
preferences in ambiguous scenarios. Specifically, closed-source models tend to
agree with each other.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: EasyNet: An Easy Network for 3D Industrial Anomaly Detection. (arXiv:2307.13925v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13925">http://arxiv.org/abs/2307.13925</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13925] EasyNet: An Easy Network for 3D Industrial Anomaly Detection](http://arxiv.org/abs/2307.13925) #segmentation</code></li>
<li>Summary: <p>3D anomaly detection is an emerging and vital computer vision task in
industrial manufacturing (IM). Recently many advanced algorithms have been
published, but most of them cannot meet the needs of IM. There are several
disadvantages: i) difficult to deploy on production lines since their
algorithms heavily rely on large pre-trained models; ii) hugely increase
storage overhead due to overuse of memory banks; iii) the inference speed
cannot be achieved in real-time. To overcome these issues, we propose an easy
and deployment-friendly network (called EasyNet) without using pre-trained
models and memory banks: firstly, we design a multi-scale multi-modality
feature encoder-decoder to accurately reconstruct the segmentation maps of
anomalous regions and encourage the interaction between RGB images and depth
images; secondly, we adopt a multi-modality anomaly segmentation network to
achieve a precise anomaly map; thirdly, we propose an attention-based
information entropy fusion module for feature fusion during inference, making
it suitable for real-time deployment. Extensive experiments show that EasyNet
achieves an anomaly detection AUROC of 92.6% without using pre-trained models
and memory banks. In addition, EasyNet is faster than existing methods, with a
high frame rate of 94.55 FPS on a Tesla V100 GPU.
</p></li>
</ul>

<h3>Title: Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network. (arXiv:2307.13938v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13938">http://arxiv.org/abs/2307.13938</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13938] Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network](http://arxiv.org/abs/2307.13938) #segmentation</code></li>
<li>Summary: <p>Semi-supervised semantic segmentation (SSS) is an important task that
utilizes both labeled and unlabeled data to reduce expenses on labeling
training examples. However, the effectiveness of SSS algorithms is limited by
the difficulty of fully exploiting the potential of unlabeled data. To address
this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise
contrastive learning. By aligning positive pairs with a pixel-wise contrastive
loss using strong augmented views in both low-level image space and high-level
feature space, the proposed DSSN is designed to maximize the utilization of
available unlabeled data. Additionally, we introduce a novel class-aware
pseudo-label selection strategy for weak-to-strong supervision, which addresses
the limitations of most existing methods that do not perform selection or apply
a predefined threshold for all classes. Specifically, our strategy selects the
top high-confidence prediction of the weak view for each class to generate
pseudo labels that supervise the strong augmented views. This strategy is
capable of taking into account the class imbalance and improving the
performance of long-tailed classes. Our proposed method achieves
state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes,
outperforming other SSS algorithms by a significant margin.
</p></li>
</ul>

<h3>Title: Tracking Anything in High Quality. (arXiv:2307.13974v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13974">http://arxiv.org/abs/2307.13974</a></li>
<li>Code URL: <a href="https://github.com/jiawen-zhu/hqtrack">https://github.com/jiawen-zhu/hqtrack</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13974] Tracking Anything in High Quality](http://arxiv.org/abs/2307.13974) #segmentation</code></li>
<li>Summary: <p>Visual object tracking is a fundamental video task in computer vision.
Recently, the notably increasing power of perception algorithms allows the
unification of single/multiobject and box/mask-based tracking. Among them, the
Segment Anything Model (SAM) attracts much attention. In this report, we
propose HQTrack, a framework for High Quality Tracking anything in videos.
HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask
refiner (MR). Given the object to be tracked in the initial frame of a video,
VMOS propagates the object masks to the current frame. The mask results at this
stage are not accurate enough since VMOS is trained on several closeset video
object segmentation (VOS) datasets, which has limited ability to generalize to
complex and corner scenes. To further improve the quality of tracking masks, a
pretrained MR model is employed to refine the tracking results. As a compelling
testament to the effectiveness of our paradigm, without employing any tricks
such as test-time data augmentations and model ensemble, HQTrack ranks the 2nd
place in the Visual Object Tracking and Segmentation (VOTS2023) challenge. Code
and models are available at https://github.com/jiawen-zhu/HQTrack.
</p></li>
</ul>

<h3>Title: Causal reasoning in typical computer vision tasks. (arXiv:2307.13992v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13992">http://arxiv.org/abs/2307.13992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13992] Causal reasoning in typical computer vision tasks](http://arxiv.org/abs/2307.13992) #segmentation</code></li>
<li>Summary: <p>Deep learning has revolutionized the field of artificial intelligence. Based
on the statistical correlations uncovered by deep learning-based methods,
computer vision technology has contributed to tremendous growth in areas such
as autonomous driving and robotics. Despite being the basis of deep learning,
such correlation is not stable and is susceptible to uncontrolled factors. In
the absence of the guidance of prior knowledge, statistical correlations can
easily turn into spurious correlations and cause confounders. As a result,
researchers are beginning to refine deep learning-based methods with causal
theory. Causal theory models the intrinsic causal structure unaffected by data
bias and is effective in avoiding spurious correlations. This paper aims to
comprehensively review the existing causal methods in typical vision and
vision-language tasks such as semantic segmentation, object detection, and
image captioning. The advantages of causality and the approaches for building
causal paradigms will be summarized. Future roadmaps are also proposed,
including facilitating the development of causal theory and its application in
other complex scenes and systems.
</p></li>
</ul>

<h3>Title: Unite-Divide-Unite: Joint Boosting Trunk and Structure for High-accuracy Dichotomous Image Segmentation. (arXiv:2307.14052v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14052">http://arxiv.org/abs/2307.14052</a></li>
<li>Code URL: <a href="https://github.com/pjlallen/udun">https://github.com/pjlallen/udun</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14052] Unite-Divide-Unite: Joint Boosting Trunk and Structure for High-accuracy Dichotomous Image Segmentation](http://arxiv.org/abs/2307.14052) #segmentation</code></li>
<li>Summary: <p>High-accuracy Dichotomous Image Segmentation (DIS) aims to pinpoint
category-agnostic foreground objects from natural scenes. The main challenge
for DIS involves identifying the highly accurate dominant area while rendering
detailed object structure. However, directly using a general encoder-decoder
architecture may result in an oversupply of high-level features and neglect the
shallow spatial information necessary for partitioning meticulous structures.
To fill this gap, we introduce a novel Unite-Divide-Unite Network (UDUN} that
restructures and bipartitely arranges complementary features to simultaneously
boost the effectiveness of trunk and structure identification. The proposed
UDUN proceeds from several strengths. First, a dual-size input feeds into the
shared backbone to produce more holistic and detailed features while keeping
the model lightweight. Second, a simple Divide-and-Conquer Module (DCM) is
proposed to decouple multiscale low- and high-level features into our structure
decoder and trunk decoder to obtain structure and trunk information
respectively. Moreover, we design a Trunk-Structure Aggregation module (TSA) in
our union decoder that performs cascade integration for uniform high-accuracy
segmentation. As a result, UDUN performs favorably against state-of-the-art
competitors in all six evaluation metrics on overall DIS-TE, i.e., achieving
0.772 weighted F-measure and 977 HCE. Using 1024*1024 input, our model enables
real-time inference at 65.3 fps with ResNet-18.
</p></li>
</ul>

<h3>Title: Multi-modal Learning with Missing Modality via Shared-Specific Feature Modelling. (arXiv:2307.14126v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14126">http://arxiv.org/abs/2307.14126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14126] Multi-modal Learning with Missing Modality via Shared-Specific Feature Modelling](http://arxiv.org/abs/2307.14126) #segmentation</code></li>
<li>Summary: <p>The missing modality issue is critical but non-trivial to be solved by
multi-modal models. Current methods aiming to handle the missing modality
problem in multi-modal tasks, either deal with missing modalities only during
evaluation or train separate models to handle specific missing modality
settings. In addition, these models are designed for specific tasks, so for
example, classification models are not easily adapted to segmentation tasks and
vice versa. In this paper, we propose the Shared-Specific Feature Modelling
(ShaSpec) method that is considerably simpler and more effective than competing
approaches that address the issues above. ShaSpec is designed to take advantage
of all available input modalities during training and evaluation by learning
shared and specific features to better represent the input data. This is
achieved from a strategy that relies on auxiliary tasks based on distribution
alignment and domain classification, in addition to a residual feature fusion
procedure. Also, the design simplicity of ShaSpec enables its easy adaptation
to multiple tasks, such as classification and segmentation. Experiments are
conducted on both medical image segmentation and computer vision
classification, with results indicating that ShaSpec outperforms competing
methods by a large margin. For instance, on BraTS2018, ShaSpec improves the
SOTA by more than 3% for enhancing tumour, 5% for tumour core and 3% for whole
tumour.
</p></li>
</ul>

<h3>Title: Creative Birds: Self-Supervised Single-View 3D Style Transfer. (arXiv:2307.14127v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14127">http://arxiv.org/abs/2307.14127</a></li>
<li>Code URL: <a href="https://github.com/wrk226/2d-to-3d-evolution-transfer">https://github.com/wrk226/2d-to-3d-evolution-transfer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14127] Creative Birds: Self-Supervised Single-View 3D Style Transfer](http://arxiv.org/abs/2307.14127) #segmentation</code></li>
<li>Summary: <p>In this paper, we propose a novel method for single-view 3D style transfer
that generates a unique 3D object with both shape and texture transfer. Our
focus lies primarily on birds, a popular subject in 3D reconstruction, for
which no existing single-view 3D transfer methods have been developed.The
method we propose seeks to generate a 3D mesh shape and texture of a bird from
two single-view images. To achieve this, we introduce a novel shape transfer
generator that comprises a dual residual gated network (DRGNet), and a
multi-layer perceptron (MLP). DRGNet extracts the features of source and target
images using a shared coordinate gate unit, while the MLP generates spatial
coordinates for building a 3D mesh. We also introduce a semantic UV texture
transfer module that implements textural style transfer using semantic UV
segmentation, which ensures consistency in the semantic meaning of the
transferred regions. This module can be widely adapted to many existing
approaches. Finally, our method constructs a novel 3D bird using a
differentiable renderer. Experimental results on the CUB dataset verify that
our method achieves state-of-the-art performance on the single-view 3D style
transfer task. Code is available in
https://github.com/wrk226/2D-to-3D-Evolution-Transfer.
</p></li>
</ul>

<h3>Title: Resolution-Aware Design of Atrous Rates for Semantic Segmentation Networks. (arXiv:2307.14179v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14179">http://arxiv.org/abs/2307.14179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14179] Resolution-Aware Design of Atrous Rates for Semantic Segmentation Networks](http://arxiv.org/abs/2307.14179) #segmentation</code></li>
<li>Summary: <p>DeepLab is a widely used deep neural network for semantic segmentation, whose
success is attributed to its parallel architecture called atrous spatial
pyramid pooling (ASPP). ASPP uses multiple atrous convolutions with different
atrous rates to extract both local and global information. However, fixed
values of atrous rates are used for the ASPP module, which restricts the size
of its field of view. In principle, atrous rate should be a hyperparameter to
change the field of view size according to the target task or dataset. However,
the manipulation of atrous rate is not governed by any guidelines. This study
proposes practical guidelines for obtaining an optimal atrous rate. First, an
effective receptive field for semantic segmentation is introduced to analyze
the inner behavior of segmentation networks. We observed that the use of ASPP
module yielded a specific pattern in the effective receptive field, which was
traced to reveal the module's underlying mechanism. Accordingly, we derive
practical guidelines for obtaining the optimal atrous rate, which should be
controlled based on the size of input image. Compared to other values, using
the optimal atrous rate consistently improved the segmentation results across
multiple datasets, including the STARE, CHASE_DB1, HRF, Cityscapes, and iSAID
datasets.
</p></li>
</ul>

<h3>Title: Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy. (arXiv:2307.14243v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14243">http://arxiv.org/abs/2307.14243</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14243] Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy](http://arxiv.org/abs/2307.14243) #segmentation</code></li>
<li>Summary: <p>Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy
images and the corresponding ground-truth annotations, designed to foster
innovative research in the domains of Life Sciences and Deep Learning. This
dataset encompasses three image collections in which rodent neuronal cells'
nuclei and cytoplasm are stained with diverse markers to highlight their
anatomical or functional characteristics. Alongside the images, we provide
ground-truth annotations for several learning tasks, including semantic
segmentation, object detection, and counting. The contribution is two-fold.
First, given the variety of annotations and their accessible formats, we
envision our work facilitating methodological advancements in computer vision
approaches for segmentation, detection, feature learning, unsupervised and
self-supervised learning, transfer learning, and related areas. Second, by
enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal
Cells v2 will catalyze breakthroughs in fluorescence microscopy analysis and
promote cutting-edge discoveries in life sciences. The data are available at:
https://amsacta.unibo.it/id/eprint/7347
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
