<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-13</h1>
<h3>Title: Orientation tracking method for anisotropic particles</h3>
<ul>
<li><strong>Authors: </strong>Mees M. Flapper, Elian Bernard, Sander G. Huisman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08694">https://arxiv.org/abs/2503.08694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08694">https://arxiv.org/pdf/2503.08694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08694]] Orientation tracking method for anisotropic particles(https://arxiv.org/abs/2503.08694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A method for particle orientation tracking is developed and demonstrated specifically for anisotropic particles. Using (high-speed) multi-camera recordings of anisotropic particles from different viewpoints, we reconstruct the 3D location and orientation of these particles using their known shape. This paper describes an algorithm which tracks the location and orientation of multiple anisotropic particles over time, enabling detailed investigations of location, orientation, and rotation statistics. The robustness and error of this method is quantified, and we explore the effects of noise, image size, the number of used cameras, and the camera arrangement by applying the algorithm to synthetic images. We showcase several use-cases of this method in several experiments (in both quiescent and turbulent fluids), demonstrating the effectiveness and broad applicability of the described tracking method. The proposed method is shown to work for widely different particle shapes, successfully tracks multiple particles simultaneously, and the method can distinguish between different types of particles.</li>
</ul>

<h3>Title: Out-of-Distribution Segmentation in Autonomous Driving: Problems and State of the Art</h3>
<ul>
<li><strong>Authors: </strong>Youssef Shoeb, Azarm Nowzad, Hanno Gottschalk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08695">https://arxiv.org/abs/2503.08695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08695">https://arxiv.org/pdf/2503.08695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08695]] Out-of-Distribution Segmentation in Autonomous Driving: Problems and State of the Art(https://arxiv.org/abs/2503.08695)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we review the state of the art in Out-of-Distribution (OoD) segmentation, with a focus on road obstacle detection in automated driving as a real-world application. We analyse the performance of existing methods on two widely used benchmarks, SegmentMeIfYouCan Obstacle Track and LostAndFound-NoKnown, highlighting their strengths, limitations, and real-world applicability. Additionally, we discuss key challenges and outline potential research directions to advance the field. Our goal is to provide researchers and practitioners with a comprehensive perspective on the current landscape of OoD segmentation and to foster further advancements toward safer and more reliable autonomous driving systems.</li>
</ul>

<h3>Title: Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net: A Comparison of CPU, GPU, and FPGA Workflows</h3>
<ul>
<li><strong>Authors: </strong>Julien Posso, Hugo Kieffer, Nicolas Menga, Omar Hlimi, Sébastien Tarris, Hubert Guerard, Guy Bois, Matthieu Couderc, Eric Jenn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.AR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08700">https://arxiv.org/abs/2503.08700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08700">https://arxiv.org/pdf/2503.08700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08700]] Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net: A Comparison of CPU, GPU, and FPGA Workflows(https://arxiv.org/abs/2503.08700)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This study introduces a lightweight U-Net model optimized for real-time semantic segmentation of aerial images, targeting the efficient utilization of Commercial Off-The-Shelf (COTS) embedded computing platforms. We maintain the accuracy of the U-Net on a real-world dataset while significantly reducing the model's parameters and Multiply-Accumulate (MAC) operations by a factor of 16. Our comprehensive analysis covers three hardware platforms (CPU, GPU, and FPGA) and five different toolchains (TVM, FINN, Vitis AI, TensorFlow GPU, and cuDNN), assessing each on metrics such as latency, power consumption, memory footprint, energy efficiency, and FPGA resource usage. The results highlight the trade-offs between these platforms and toolchains, with a particular focus on the practical deployment challenges in real-world applications. Our findings demonstrate that while the FPGA with Vitis AI emerges as the superior choice due to its performance, energy efficiency, and maturity, it requires specialized hardware knowledge, emphasizing the need for a balanced approach in selecting embedded computing solutions for semantic segmentation tasks</li>
</ul>

<h3>Title: Life-Cycle Routing Vulnerabilities of LLM Router</h3>
<ul>
<li><strong>Authors: </strong>Qiqi Lin, Xiaoyang Ji, Shengfang Zhai, Qingni Shen, Zhi Zhang, Yuejian Fang, Yansong Gao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08704">https://arxiv.org/abs/2503.08704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08704">https://arxiv.org/pdf/2503.08704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08704]] Life-Cycle Routing Vulnerabilities of LLM Router(https://arxiv.org/abs/2503.08704)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable success in natural language processing, yet their performance and computational costs vary significantly. LLM routers play a crucial role in dynamically balancing these trade-offs. While previous studies have primarily focused on routing efficiency, security vulnerabilities throughout the entire LLM router life cycle, from training to inference, remain largely unexplored. In this paper, we present a comprehensive investigation into the life-cycle routing vulnerabilities of LLM routers. We evaluate both white-box and black-box adversarial robustness, as well as backdoor robustness, across several representative routing models under extensive experimental settings. Our experiments uncover several key findings: 1) Mainstream DNN-based routers tend to exhibit the weakest adversarial and backdoor robustness, largely due to their strong feature extraction capabilities that amplify vulnerabilities during both training and inference; 2) Training-free routers demonstrate the strongest robustness across different attack types, benefiting from the absence of learnable parameters that can be manipulated. These findings highlight critical security risks spanning the entire life cycle of LLM routers and provide insights for developing more robust models.</li>
</ul>

<h3>Title: A Secure Blockchain-Assisted Framework for Real-Time Maritime Environmental Compliance Monitoring</h3>
<ul>
<li><strong>Authors: </strong>William C. Quigley, Mohamed Rahouti, Gary M. Weiss</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08707">https://arxiv.org/abs/2503.08707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08707">https://arxiv.org/pdf/2503.08707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08707]] A Secure Blockchain-Assisted Framework for Real-Time Maritime Environmental Compliance Monitoring(https://arxiv.org/abs/2503.08707)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, robust</a></li>
<li><strong>Abstract: </strong>The maritime industry is governed by stringent environmental regulations, most notably the International Convention for the Prevention of Pollution from Ships (MARPOL). Ensuring compliance with these regulations is difficult due to low inspection rates and the risk of data fabrication. To address these issues, this paper proposes a secure blockchain-assisted framework for real-time maritime environmental compliance monitoring. By integrating IoT and shipboard sensors with blockchain technology, the framework ensures immutable and transparent record-keeping of environmental data. Smart contracts automate compliance verification and notify relevant authorities in case of non-compliance. A proof-of-concept case study on sulfur emissions demonstrates the framework's efficacy in enhancing MARPOL enforcement through real-time data integrity and regulatory adherence. The proposed system leverages the Polygon blockchain for scalability and efficiency, providing a robust solution for maritime environmental protection. The evaluation results demonstrate that the proposed blockchain-enhanced compliance monitoring system effectively and securely ensures real-time regulatory adherence with high scalability, efficiency, and cost-effectiveness, leveraging the robust capabilities of the Polygon blockchain.</li>
</ul>

<h3>Title: TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on Machine-Generated Text Detectors</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Zheng, Junfeng Wang, Zhen Sun, Wenhan Dong, Yule Liu, Xinlei He</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08708">https://arxiv.org/abs/2503.08708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08708">https://arxiv.org/pdf/2503.08708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08708]] TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on Machine-Generated Text Detectors(https://arxiv.org/abs/2503.08708)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have become increasingly fluent, high-quality, and informative. Existing wide-range MGT detectors are designed to identify MGTs to prevent the spread of plagiarism and misinformation. However, adversaries attempt to humanize MGTs to evade detection (named evading attacks), which requires only minor modifications to bypass MGT detectors. Unfortunately, existing attacks generally lack a unified and comprehensive evaluation framework, as they are assessed using different experimental settings, model architectures, and datasets. To fill this gap, we introduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive benchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates attacks across three key dimensions: evading effectiveness, text quality, and computational overhead. Our extensive experiments evaluate 6 state-of-the-art attacks against 13 MGT detectors across 6 datasets, spanning 19 domains and generated by 11 widely used LLMs. Our findings reveal that no single evading attack excels across all three dimensions. Through in-depth analysis, we highlight the strengths and limitations of different attacks. More importantly, we identify a trade-off among three dimensions and propose two optimization insights. Through preliminary experiments, we validate their correctness and effectiveness, offering potential directions for future research.</li>
</ul>

<h3>Title: Versatile Multimodal Controls for Whole-Body Talking Human Animation</h3>
<ul>
<li><strong>Authors: </strong>Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Minghui Yang, Ming Yang, Le Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08714">https://arxiv.org/abs/2503.08714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08714">https://arxiv.org/pdf/2503.08714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08714]] Versatile Multimodal Controls for Whole-Body Talking Human Animation(https://arxiv.org/abs/2503.08714)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human animation from a single reference image shall be flexible to synthesize whole-body motion for either a headshot or whole-body portrait, where the motions are readily controlled by audio signal and text prompts. This is hard for most existing methods as they only support producing pre-specified head or half-body motion aligned with audio inputs. In this paper, we propose a versatile human animation method, i.e., VersaAnimator, which generates whole-body talking human from arbitrary portrait images, not only driven by audio signal but also flexibly controlled by text prompts. Specifically, we design a text-controlled, audio-driven motion generator that produces whole-body motion representations in 3D synchronized with audio inputs while following textual motion descriptions. To promote natural smooth motion, we propose a code-pose translation module to link VAE codebooks with 2D DWposes extracted from template videos. Moreover, we introduce a multi-modal video diffusion that generates photorealistic human animation from a reference image according to both audio inputs and whole-body motion representations. Extensive experiments show that VersaAnimator outperforms existing methods in visual quality, identity preservation, and audio-lip synchronization.</li>
</ul>

<h3>Title: AuthorMist: Evading AI Text Detectors with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Isaac David, Arthur Gervais</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08716">https://arxiv.org/abs/2503.08716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08716">https://arxiv.org/pdf/2503.08716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08716]] AuthorMist: Evading AI Text Detectors with Reinforcement Learning(https://arxiv.org/abs/2503.08716)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, fair</a></li>
<li><strong>Abstract: </strong>In the age of powerful AI-generated text, automatic detectors have emerged to identify machine-written content. This poses a threat to author privacy and freedom, as text authored with AI assistance may be unfairly flagged. We propose AuthorMist, a novel reinforcement learning-based system to transform AI-generated text into human-like writing. AuthorMist leverages a 3-billion-parameter language model as a backbone, fine-tuned with Group Relative Policy Optimization (GPRO) to paraphrase text in a way that evades AI detectors. Our framework establishes a generic approach where external detector APIs (GPTZero, WinstonAI, this http URL, etc.) serve as reward functions within the reinforcement learning loop, enabling the model to systematically learn outputs that these detectors are less likely to classify as AI-generated. This API-as-reward methodology can be applied broadly to optimize text against any detector with an accessible interface. Experiments on multiple datasets and detectors demonstrate that AuthorMist effectively reduces the detectability of AI-generated text while preserving the original meaning. Our evaluation shows attack success rates ranging from 78.6% to 96.2% against individual detectors, significantly outperforming baseline paraphrasing methods. AuthorMist maintains high semantic similarity (above 0.94) with the original text while successfully evading detection. These results highlight limitations in current AI text detection technologies and raise questions about the sustainability of the detection-evasion arms race.</li>
</ul>

<h3>Title: A Recipe for Improving Remote Sensing VLM Zero Shot Generalization</h3>
<ul>
<li><strong>Authors: </strong>Aviad Barzilai, Yotam Gigi, Vered Silverman, Yehonathan Refael, Bolous Jaber, Amr Helmy, Tomer Shekel, George Leifman, Genady Beryozkin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08722">https://arxiv.org/abs/2503.08722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08722">https://arxiv.org/pdf/2503.08722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08722]] A Recipe for Improving Remote Sensing VLM Zero Shot Generalization(https://arxiv.org/abs/2503.08722)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models have had a significant impact across various AI applications, enabling use cases that were previously impossible. Contrastive Visual Language Models (VLMs), in particular, have outperformed other techniques in many tasks. However, their prevalence in remote sensing (RS) is still limited, due to the scarcity of diverse remote-sensing visual-language datasets. In this work we introduce two novel image-caption datasets for training of remote sensing foundation models. The first dataset pairs aerial and satellite imagery with captions generated by Gemini using landmarks extracted from Google Maps. The second dataset utilizes public web images and their corresponding alt-text, filtered for the remote sensing domain, resulting in a diverse dataset with greater breadth in image styles and subject matter. These datasets are used to pre-train the MaMMUT~\citep{kuo2023mammutsimplearchitecturejoint} VLM architecture, resulting in state-of-the-art generalization performance in zero-shot cross-modal retrieval on well-known public benchmarks. Finally, we present our ongoing research to distill image-level knowledge gained in the VLM contrastive training procedure to enhance the model's localization ability. Specifically, we iteratively generate pseudo-labels for image regions based on the model's attention maps and use these labels for further training. To mitigate noisy attention maps and create robust segmentation masks, we introduce a novel attention-pooling mechanism called the Smooth-Attention-Operation.</li>
</ul>

<h3>Title: SIMAC: A Semantic-Driven Integrated Multimodal Sensing And Communication Framework</h3>
<ul>
<li><strong>Authors: </strong>Yubo Peng, Luping Xiang, Kun Yang, Feibo Jiang, Kezhi Wang, Dapeng Oliver Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08726">https://arxiv.org/abs/2503.08726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08726">https://arxiv.org/pdf/2503.08726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08726]] SIMAC: A Semantic-Driven Integrated Multimodal Sensing And Communication Framework(https://arxiv.org/abs/2503.08726)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional single-modality sensing faces limitations in accuracy and capability, and its decoupled implementation with communication systems increases latency in bandwidth-constrained environments. Additionally, single-task-oriented sensing systems fail to address users' diverse demands. To overcome these challenges, we propose a semantic-driven integrated multimodal sensing and communication (SIMAC) framework. This framework leverages a joint source-channel coding architecture to achieve simultaneous sensing decoding and transmission of sensing results. Specifically, SIMAC first introduces a multimodal semantic fusion (MSF) network, which employs two extractors to extract semantic information from radar signals and images, respectively. MSF then applies cross-attention mechanisms to fuse these unimodal features and generate multimodal semantic representations. Secondly, we present a large language model (LLM)-based semantic encoder (LSE), where relevant communication parameters and multimodal semantics are mapped into a unified latent space and input to the LLM, enabling channel-adaptive semantic encoding. Thirdly, a task-oriented sensing semantic decoder (SSD) is proposed, in which different decoded heads are designed according to the specific needs of tasks. Simultaneously, a multi-task learning strategy is introduced to train the SIMAC framework, achieving diverse sensing services. Finally, experimental simulations demonstrate that the proposed framework achieves diverse sensing services and higher accuracy.</li>
</ul>

<h3>Title: Preserving Product Fidelity in Large Scale Image Recontextualization with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ishaan Malhi, Praneet Dutta, Ellie Talius, Sally Ma, Brendan Driscoll, Krista Holden, Garima Pruthi, Arunachalam Narayanaswamy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08729">https://arxiv.org/abs/2503.08729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08729">https://arxiv.org/pdf/2503.08729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08729]] Preserving Product Fidelity in Large Scale Image Recontextualization with Diffusion Models(https://arxiv.org/abs/2503.08729)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a framework for high-fidelity product image recontextualization using text-to-image diffusion models and a novel data augmentation pipeline. This pipeline leverages image-to-video diffusion, in/outpainting & negatives to create synthetic training data, addressing limitations of real-world data collection for this task. Our method improves the quality and diversity of generated images by disentangling product representations and enhancing the model's understanding of product characteristics. Evaluation on the ABO dataset and a private product dataset, using automated metrics and human assessment, demonstrates the effectiveness of our framework in generating realistic and compelling product visualizations, with implications for applications such as e-commerce and virtual product showcasing.</li>
</ul>

<h3>Title: FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face Obfuscation Methods</h3>
<ul>
<li><strong>Authors: </strong>Seyyed Mohammad Sadegh Moosavi Khorzooghi, Poojitha Thota, Mohit Singhal, Abolfazl Asudeh, Gautam Das, Shirin Nilizadeh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08731">https://arxiv.org/abs/2503.08731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08731">https://arxiv.org/pdf/2503.08731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08731]] FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face Obfuscation Methods(https://arxiv.org/abs/2503.08731)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, fair</a></li>
<li><strong>Abstract: </strong>The lack of a common platform and benchmark datasets for evaluating face obfuscation methods has been a challenge, with every method being tested using arbitrary experiments, datasets, and metrics. While prior work has demonstrated that face recognition systems exhibit bias against some demographic groups, there exists a substantial gap in our understanding regarding the fairness of face obfuscation methods. Providing fair face obfuscation methods can ensure equitable protection across diverse demographic groups, especially since they can be used to preserve the privacy of vulnerable populations. To address these gaps, this paper introduces a comprehensive framework, named FairDeFace, designed to assess the adversarial robustness and fairness of face obfuscation methods. The framework introduces a set of modules encompassing data benchmarks, face detection and recognition algorithms, adversarial models, utility detection models, and fairness metrics. FairDeFace serves as a versatile platform where any face obfuscation method can be integrated, allowing for rigorous testing and comparison with other state-of-the-art methods. In its current implementation, FairDeFace incorporates 6 attacks, and several privacy, utility and fairness metrics. Using FairDeFace, and by conducting more than 500 experiments, we evaluated and compared the adversarial robustness of seven face obfuscation methods. This extensive analysis led to many interesting findings both in terms of the degree of robustness of existing methods and their biases against some gender or racial groups. FairDeFace also uses visualization of focused areas for both obfuscation and verification attacks to show not only which areas are mostly changed in the obfuscation process for some demographics, but also why they failed through focus area comparison of obfuscation and verification.</li>
</ul>

<h3>Title: Zero-to-One IDV: A Conceptual Model for AI-Powered Identity Verification</h3>
<ul>
<li><strong>Authors: </strong>Aniket Vaidya, Anurag Awasthi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08734">https://arxiv.org/abs/2503.08734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08734">https://arxiv.org/pdf/2503.08734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08734]] Zero-to-One IDV: A Conceptual Model for AI-Powered Identity Verification(https://arxiv.org/abs/2503.08734)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, biometric</a></li>
<li><strong>Abstract: </strong>In today's increasingly digital interactions, robust Identity Verification (IDV) is crucial for security and trust. Artificial Intelligence (AI) is transforming IDV, enhancing accuracy and fraud detection. This paper introduces ``Zero to One,'' a holistic conceptual framework for developing AI-powered IDV products. This paper outlines the foundational problem and research objectives that necessitate a new framework for IDV in the age of AI. It details the evolution of identity verification and the current regulatory landscape to contextualize the need for a robust conceptual model. The core of the paper is the presentation of the ``Zero to One'' framework itself, dissecting its four essential components: Document Verification, Biometric Verification, Risk Assessment, and Orchestration. The paper concludes by discussing the implications of this conceptual model and suggesting future research directions focused on the framework's further development and application. The framework addresses security, privacy, UX, and regulatory compliance, offering a structured approach to building effective IDV solutions. Successful IDV platforms require a balanced conceptual understanding of verification methods, risk management, and operational scalability, with AI as a key enabler. This paper presents the ``Zero to One'' framework as a refined conceptual model, detailing verification layers, and AI's transformative role in shaping next-generation IDV products.</li>
</ul>

<h3>Title: Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>In Cho, Youngbeom Yoo, Subin Jeon, Seon Joo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08737">https://arxiv.org/abs/2503.08737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08737">https://arxiv.org/pdf/2503.08737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08737]] Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models(https://arxiv.org/abs/2503.08737)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Constructing a compressed latent space through a variational autoencoder (VAE) is the key for efficient 3D diffusion models. This paper introduces COD-VAE, a VAE that encodes 3D shapes into a COmpact set of 1D latent vectors without sacrificing quality. COD-VAE introduces a two-stage autoencoder scheme to improve compression and decoding efficiency. First, our encoder block progressively compresses point clouds into compact latent vectors via intermediate point patches. Second, our triplane-based decoder reconstructs dense triplanes from latent vectors instead of directly decoding neural fields, significantly reducing computational overhead of neural fields decoding. Finally, we propose uncertainty-guided token pruning, which allocates resources adaptively by skipping computations in simpler regions and improves the decoder efficiency. Experimental results demonstrate that COD-VAE achieves 16x compression compared to the baseline while maintaining quality. This enables 20.8x speedup in generation, highlighting that a large number of latent vectors is not a prerequisite for high-quality reconstruction and generation.</li>
</ul>

<h3>Title: Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Letian Zhang, Quan Cui, Bingchen Zhao, Cheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08741">https://arxiv.org/abs/2503.08741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08741">https://arxiv.org/pdf/2503.08741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08741]] Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis(https://arxiv.org/abs/2503.08741)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The success of multi-modal large language models (MLLMs) has been largely attributed to the large-scale training data. However, the training data of many MLLMs is unavailable due to privacy concerns. The expensive and labor-intensive process of collecting multi-modal data further exacerbates the problem. Is it possible to synthesize multi-modal training data automatically without compromising diversity and quality? In this paper, we propose a new method, Oasis, to synthesize high-quality multi-modal data with only images. Oasis breaks through traditional methods by prompting only images to the MLLMs, thus extending the data diversity by a large margin. Our method features a delicate quality control method which ensures the data quality. We collected over 500k data and conducted incremental experiments on LLaVA-NeXT. Extensive experiments demonstrate that our method can significantly improve the performance of MLLMs. The image-based synthesis also allows us to focus on the specific-domain ability of MLLMs. Code and data will be publicly available.</li>
</ul>

<h3>Title: Exposing Product Bias in LLM Investment Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Zhi, Xiaoyu Zhang, Longtian Wang, Shumin Jiang, Shiqing Ma, Xiaohong Guan, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08750">https://arxiv.org/abs/2503.08750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08750">https://arxiv.org/pdf/2503.08750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08750]] Exposing Product Bias in LLM Investment Recommendation(https://arxiv.org/abs/2503.08750)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), as a new generation of recommendation engines, possess powerful summarization and data analysis capabilities, surpassing traditional recommendation systems in both scope and performance. One promising application is investment recommendation. In this paper, we reveal a novel product bias in LLM investment recommendation, where LLMs exhibit systematic preferences for specific products. Such preferences can subtly influence user investment decisions, potentially leading to inflated valuations of products and financial bubbles, posing risks to both individual investors and market stability. To comprehensively study the product bias, we develop an automated pipeline to create a dataset of 567,000 samples across five asset classes (stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this dataset, we present the bf first study on product bias in LLM investment recommendations. Our findings reveal that LLMs exhibit clear product preferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from Microsoft). Notably, this bias persists even after applying debiasing techniques. We urge AI researchers to take heed of the product bias in LLM investment recommendations and its implications, ensuring fairness and security in the digital space and market.</li>
</ul>

<h3>Title: Robust Multi-Objective Controlled Decoding of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seongho Son, William Bankes, Sangwoong Yoon, Shyam Sundhar Ramesh, Xiaohang Tang, Ilija Bogunovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08796">https://arxiv.org/abs/2503.08796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08796">https://arxiv.org/pdf/2503.08796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08796]] Robust Multi-Objective Controlled Decoding of Large Language Models(https://arxiv.org/abs/2503.08796)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Test-time alignment of Large Language Models (LLMs) to human preferences offers a flexible way to generate responses aligned to diverse objectives without extensive retraining of LLMs. Existing methods achieve alignment to multiple objectives simultaneously (e.g., instruction-following, helpfulness, conciseness) by optimizing their corresponding reward functions. However, they often rely on predefined weights or optimize for averages, sacrificing one objective for another and leading to unbalanced outcomes. To address this, we introduce Robust Multi-Objective Decoding (RMOD), a novel inference-time algorithm that optimizes for improving worst-case rewards. RMOD formalizes the robust decoding problem as a maximin two-player game between reward weights and the sampling policy, solving for the Nash equilibrium. We show that the game reduces to a convex optimization problem to find the worst-case weights, while the best response policy can be computed analytically. We also introduce a practical RMOD variant designed for efficient decoding with contemporary LLMs, incurring minimal computational overhead compared to non-robust Multi-Objective Decoding (MOD) methods. Our experimental results showcase the effectiveness of RMOD in generating responses equitably aligned with diverse objectives, outperforming baselines up to 20%.</li>
</ul>

<h3>Title: Enhanced Estimation Techniques for Certified Radii in Randomized Smoothing</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08801">https://arxiv.org/abs/2503.08801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08801">https://arxiv.org/pdf/2503.08801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08801]] Enhanced Estimation Techniques for Certified Radii in Randomized Smoothing(https://arxiv.org/abs/2503.08801)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents novel methods for estimating certified radii in randomized smoothing, a technique crucial for certifying the robustness of neural networks against adversarial perturbations. Our proposed techniques significantly improve the accuracy of certified test-set accuracy by providing tighter bounds on the certified radii. We introduce advanced algorithms for both discrete and continuous domains, demonstrating their effectiveness on CIFAR-10 and ImageNet datasets. The new methods show considerable improvements over existing approaches, particularly in reducing discrepancies in certified radii estimates. We also explore the impact of various hyperparameters, including sample size, standard deviation, and temperature, on the performance of these methods. Our findings highlight the potential for more efficient certification processes and pave the way for future research on tighter confidence sequences and improved theoretical frameworks. The study concludes with a discussion of potential future directions, including enhanced estimation techniques for discrete domains and further theoretical advancements to bridge the gap between empirical and theoretical performance in randomized smoothing.</li>
</ul>

<h3>Title: Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations</h3>
<ul>
<li><strong>Authors: </strong>Danielle Villa, Maria Chang, Keerthiram Murugesan, Rosario Uceda-Sosa, Karthikeyan Natesan Ramamurthy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08815">https://arxiv.org/abs/2503.08815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08815">https://arxiv.org/pdf/2503.08815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08815]] Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations(https://arxiv.org/abs/2503.08815)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are often asked to explain their outputs to enhance accuracy and transparency. However, evidence suggests that these explanations can misrepresent the models' true reasoning processes. One effective way to identify inaccuracies or omissions in these explanations is through consistency checking, which typically involves asking follow-up questions. This paper introduces, cross-examiner, a new method for generating follow-up questions based on a model's explanation of an initial question. Our method combines symbolic information extraction with language model-driven question generation, resulting in better follow-up questions than those produced by LLMs alone. Additionally, this approach is more flexible than other methods and can generate a wider variety of follow-up questions.</li>
</ul>

<h3>Title: Seal Your Backdoor with Variational Defense</h3>
<ul>
<li><strong>Authors: </strong>Ivan Sabolić, Matej Grcić, Siniša Šegvić</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08829">https://arxiv.org/abs/2503.08829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08829">https://arxiv.org/pdf/2503.08829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08829]] Seal Your Backdoor with Variational Defense(https://arxiv.org/abs/2503.08829)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>We propose VIBE, a model-agnostic framework that trains classifiers resilient to backdoor attacks. The key concept behind our approach is to treat malicious inputs and corrupted labels from the training dataset as observed random variables, while the actual clean labels are latent. VIBE then recovers the corresponding latent clean label posterior through variational inference. The resulting training procedure follows the expectation-maximization (EM) algorithm. The E-step infers the clean pseudolabels by solving an entropy-regularized optimal transport problem, while the M-step updates the classifier parameters via gradient descent. Being modular, VIBE can seamlessly integrate with recent advancements in self-supervised representation learning, which enhance its ability to resist backdoor attacks. We experimentally validate the method effectiveness against contemporary backdoor attacks on standard datasets, a large-scale setup with 1$k$ classes, and a dataset poisoned with multiple attacks. VIBE consistently outperforms previous defenses across all tested scenarios.</li>
</ul>

<h3>Title: evoBPE: Evolutionary Protein Sequence Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Burak Suyunu, Özdeniz Dolu, Arzucan Özgür</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08838">https://arxiv.org/abs/2503.08838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08838">https://arxiv.org/pdf/2503.08838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08838]] evoBPE: Evolutionary Protein Sequence Tokenization(https://arxiv.org/abs/2503.08838)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements in computational biology have drawn compelling parallels between protein sequences and linguistic structures, highlighting the need for sophisticated tokenization methods that capture the intricate evolutionary dynamics of protein sequences. Current subword tokenization techniques, primarily developed for natural language processing, often fail to represent protein sequences' complex structural and functional properties adequately. This study introduces evoBPE, a novel tokenization approach that integrates evolutionary mutation patterns into sequence segmentation, addressing critical limitations in existing methods. By leveraging established substitution matrices, evoBPE transcends traditional frequency-based tokenization strategies. The method generates candidate token pairs through biologically informed mutations, evaluating them based on pairwise alignment scores and frequency thresholds. Extensive experiments on human protein sequences show that evoBPE performs better across multiple dimensions. Domain conservation analysis reveals that evoBPE consistently outperforms standard Byte-Pair Encoding, particularly as vocabulary size increases. Furthermore, embedding similarity analysis using ESM-2 suggests that mutation-based token replacements preserve biological sequence properties more effectively than arbitrary substitutions. The research contributes to protein sequence representation by introducing a mutation-aware tokenization method that better captures evolutionary nuances. By bridging computational linguistics and molecular biology, evoBPE opens new possibilities for machine learning applications in protein function prediction, structural modeling, and evolutionary analysis.</li>
</ul>

<h3>Title: Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Sun, Kun Qian, Wenhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08842">https://arxiv.org/abs/2503.08842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08842">https://arxiv.org/pdf/2503.08842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08842]] Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation with LLMs(https://arxiv.org/abs/2503.08842)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Multi-party dialogue generation presents significant challenges due to the complex interplay of multiple speakers and interwoven conversational threads. Traditional approaches often fall short in capturing these complexities, particularly when relying on manually annotated dialogue relations. This paper introduces Speaker-Attentive LLM (SA-LLM), a novel generative model that leverages pre-trained Large Language Models (LLMs) and a speaker-aware contrastive learning strategy to address these challenges. SA-LLM incorporates a speaker-attributed input encoding and a contrastive learning objective to implicitly learn contextual coherence and speaker roles without explicit relation annotations. Extensive experiments on the Ubuntu IRC and Movie Dialogues datasets demonstrate that SA-LLM significantly outperforms state-of-the-art baselines in automatic and human evaluations, achieving superior performance in fluency, coherence, informativeness, and response diversity. Ablation studies and detailed error analyses further validate the effectiveness of the proposed speaker-attentive training approach, highlighting its robustness across different speaker roles and context lengths. The results underscore the potential of SA-LLM as a powerful and annotation-free solution for high-quality multi-party dialogue generation.</li>
</ul>

<h3>Title: Keypoint Semantic Integration for Improved Feature Matching in Outdoor Agricultural Environments</h3>
<ul>
<li><strong>Authors: </strong>Rajitha de Silva, Jonathan Cox, Marija Popovic, Cesar Cadena, Cyrill Stachniss, Riccardo Polvara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08843">https://arxiv.org/abs/2503.08843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08843">https://arxiv.org/pdf/2503.08843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08843]] Keypoint Semantic Integration for Improved Feature Matching in Outdoor Agricultural Environments(https://arxiv.org/abs/2503.08843)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust robot navigation in outdoor environments requires accurate perception systems capable of handling visual challenges such as repetitive structures and changing appearances. Visual feature matching is crucial to vision-based pipelines but remains particularly challenging in natural outdoor settings due to perceptual aliasing. We address this issue in vineyards, where repetitive vine trunks and other natural elements generate ambiguous descriptors that hinder reliable feature matching. We hypothesise that semantic information tied to keypoint positions can alleviate perceptual aliasing by enhancing keypoint descriptor distinctiveness. To this end, we introduce a keypoint semantic integration technique that improves the descriptors in semantically meaningful regions within the image, enabling more accurate differentiation even among visually similar local features. We validate this approach in two vineyard perception tasks: (i) relative pose estimation and (ii) visual localisation. Across all tested keypoint types and descriptors, our method improves matching accuracy by 12.6%, demonstrating its effectiveness over multiple months in challenging vineyard conditions.</li>
</ul>

<h3>Title: Interpretable and Robust Dialogue State Tracking via Natural Language Summarization with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rafael Carranza, Mateo Alejandro Rojas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08857">https://arxiv.org/abs/2503.08857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08857">https://arxiv.org/pdf/2503.08857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08857]] Interpretable and Robust Dialogue State Tracking via Natural Language Summarization with LLMs(https://arxiv.org/abs/2503.08857)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to Dialogue State Tracking (DST) that leverages Large Language Models (LLMs) to generate natural language descriptions of dialogue states, moving beyond traditional slot-value representations. Conventional DST methods struggle with open-domain dialogues and noisy inputs. Motivated by the generative capabilities of LLMs, our Natural Language DST (NL-DST) framework trains an LLM to directly synthesize human-readable state descriptions. We demonstrate through extensive experiments on MultiWOZ 2.1 and Taskmaster-1 datasets that NL-DST significantly outperforms rule-based and discriminative BERT-based DST baselines, as well as generative slot-filling GPT-2 DST models, in both Joint Goal Accuracy and Slot Accuracy. Ablation studies and human evaluations further validate the effectiveness of natural language state generation, highlighting its robustness to noise and enhanced interpretability. Our findings suggest that NL-DST offers a more flexible, accurate, and human-understandable approach to dialogue state tracking, paving the way for more robust and adaptable task-oriented dialogue systems.</li>
</ul>

<h3>Title: Smoothing ADMM for Non-convex and Non-smooth Hierarchical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Reza Mirzaeifard, Stefan Werner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08869">https://arxiv.org/abs/2503.08869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08869">https://arxiv.org/pdf/2503.08869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08869]] Smoothing ADMM for Non-convex and Non-smooth Hierarchical Federated Learning(https://arxiv.org/abs/2503.08869)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>This paper presents a hierarchical federated learning (FL) framework that extends the alternating direction method of multipliers (ADMM) with smoothing techniques, tailored for non-convex and non-smooth objectives. Unlike traditional hierarchical FL methods, our approach supports asynchronous updates and multiple updates per iteration, enhancing adaptability to heterogeneous data and system settings. Additionally, we introduce a flexible mechanism to leverage diverse regularization functions at each layer, allowing customization to the specific prior information within each cluster and accommodating (possibly) non-smooth penalty objectives. Depending on the learning goal, the framework supports both consensus and personalization: the total variation norm can be used to enforce consensus across layers, while non-convex penalties such as minimax concave penalty (MCP) or smoothly clipped absolute deviation (SCAD) enable personalized learning. Experimental results demonstrate the superior convergence rates and accuracy of our method compared to conventional approaches, underscoring its robustness and versatility for a wide range of FL scenarios.</li>
</ul>

<h3>Title: Comprehensive Benchmarking of Machine Learning Methods for Risk Prediction Modelling from Large-Scale Survival Data: A UK Biobank Study</h3>
<ul>
<li><strong>Authors: </strong>Rafael R. Oexner, Robin Schmitt, Hyunchan Ahn, Ravi A. Shah, Anna Zoccarato, Konstantinos Theofilatos, Ajay M. Shah</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08870">https://arxiv.org/abs/2503.08870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08870">https://arxiv.org/pdf/2503.08870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08870]] Comprehensive Benchmarking of Machine Learning Methods for Risk Prediction Modelling from Large-Scale Survival Data: A UK Biobank Study(https://arxiv.org/abs/2503.08870)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Predictive modelling is vital to guide preventive efforts. Whilst large-scale prospective cohort studies and a diverse toolkit of available machine learning (ML) algorithms have facilitated such survival task efforts, choosing the best-performing algorithm remains challenging. Benchmarking studies to date focus on relatively small-scale datasets and it is unclear how well such findings translate to large datasets that combine omics and clinical features. We sought to benchmark eight distinct survival task implementations, ranging from linear to deep learning (DL) models, within the large-scale prospective cohort study UK Biobank (UKB). We compared discrimination and computational requirements across heterogenous predictor matrices and endpoints. Finally, we assessed how well different architectures scale with sample sizes ranging from n = 5,000 to n = 250,000 individuals. Our results show that discriminative performance across a multitude of metrices is dependent on endpoint frequency and predictor matrix properties, with very robust performance of (penalised) COX Proportional Hazards (COX-PH) models. Of note, there are certain scenarios which favour more complex frameworks, specifically if working with larger numbers of observations and relatively simple predictor matrices. The observed computational requirements were vastly different, and we provide solutions in cases where current implementations were impracticable. In conclusion, this work delineates how optimal model choice is dependent on a variety of factors, including sample size, endpoint frequency and predictor matrix properties, thus constituting an informative resource for researchers working on similar datasets. Furthermore, we showcase how linear models still display a highly effective and scalable platform to perform risk modelling at scale and suggest that those are reported alongside non-linear ML models.</li>
</ul>

<h3>Title: Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing</h3>
<ul>
<li><strong>Authors: </strong>Cameron Redovian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08872">https://arxiv.org/abs/2503.08872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08872">https://arxiv.org/pdf/2503.08872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08872]] Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing(https://arxiv.org/abs/2503.08872)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We integrate a meta-reinforcement learning algorithm with the DreamerV3 architecture to improve load balancing in operating systems. This approach enables rapid adaptation to dynamic workloads with minimal retraining, outperforming the Advantage Actor-Critic (A2C) algorithm in standard and adaptive trials. It demonstrates robust resilience to catastrophic forgetting, maintaining high performance under varying workload distributions and sizes. These findings have important implications for optimizing resource management and performance in modern operating systems. By addressing the challenges posed by dynamic and heterogeneous workloads, our approach advances the adaptability and efficiency of reinforcement learning in real-world system management tasks.</li>
</ul>

<h3>Title: LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference</h3>
<ul>
<li><strong>Authors: </strong>Guangtao Wang, Shubhangi Upasani, Chen Wu, Darshan Gandhi, Jonathan Li, Changran Hu, Bo Li, Urmish Thakker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08879">https://arxiv.org/abs/2503.08879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08879">https://arxiv.org/pdf/2503.08879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08879]] LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference(https://arxiv.org/abs/2503.08879)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient long-context inference is critical as large language models (LLMs) adopt context windows of ranging from 128K to 1M tokens. However, the growing key-value (KV) cache and the high computational complexity of attention create significant bottlenecks in memory usage and latency. In this paper, we find that attention in diverse long-context tasks exhibits sparsity, and LLMs implicitly "know" which tokens can be dropped or evicted at the head level after the pre-filling stage. Based on this insight, we propose Self-Attention Guided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for long-context inference. After prefilling, our method performs a one-time top-k selection at both the token and head levels to compress the KV cache, enabling efficient inference with the reduced cache. Evaluations on LongBench and three long-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct, and Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable to full attention while significantly improving efficiency. Specifically, SAGE-KV achieves 4x higher memory efficiency with improved accuracy over the static KV cache selection method StreamLLM, and 2x higher memory efficiency with better accuracy than the dynamic KV cache selection method Quest.</li>
</ul>

<h3>Title: Seeing What's Not There: Spurious Correlation in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Parsa Hosseini, Sumit Nawathe, Mazda Moayeri, Sriram Balasubramanian, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08884">https://arxiv.org/abs/2503.08884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08884">https://arxiv.org/pdf/2503.08884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08884]] Seeing What's Not There: Spurious Correlation in Multimodal LLMs(https://arxiv.org/abs/2503.08884)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Unimodal vision models are known to rely on spurious correlations, but it remains unclear to what extent Multimodal Large Language Models (MLLMs) exhibit similar biases despite language supervision. In this paper, we investigate spurious bias in MLLMs and introduce SpurLens, a pipeline that leverages GPT-4 and open-set object detectors to automatically identify spurious visual cues without human supervision. Our findings reveal that spurious correlations cause two major failure modes in MLLMs: (1) over-reliance on spurious cues for object recognition, where removing these cues reduces accuracy, and (2) object hallucination, where spurious cues amplify the hallucination by over 10x. We validate our findings in various MLLMs and datasets. Beyond diagnosing these failures, we explore potential mitigation strategies, such as prompt ensembling and reasoning-based prompting, and conduct ablation studies to examine the root causes of spurious bias in MLLMs. By exposing the persistence of spurious correlations, our study calls for more rigorous evaluation methods and mitigation strategies to enhance the reliability of MLLMs.</li>
</ul>

<h3>Title: PlainQAFact: Automatic Factuality Evaluation Metric for Biomedical Plain Language Summaries Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen You, Yue Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08890">https://arxiv.org/abs/2503.08890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08890">https://arxiv.org/pdf/2503.08890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08890]] PlainQAFact: Automatic Factuality Evaluation Metric for Biomedical Plain Language Summaries Generation(https://arxiv.org/abs/2503.08890)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Hallucinated outputs from language models pose risks in the medical domain, especially for lay audiences making health-related decisions. Existing factuality evaluation methods, such as entailment- and question-answering-based (QA), struggle with plain language summary (PLS) generation due to elaborative explanation phenomenon, which introduces external content (e.g., definitions, background, examples) absent from the source document to enhance comprehension. To address this, we introduce PlainQAFact, a framework trained on a fine-grained, human-annotated dataset PlainFact, to evaluate the factuality of both source-simplified and elaboratively explained sentences. PlainQAFact first classifies factuality type and then assesses factuality using a retrieval-augmented QA-based scoring method. Our approach is lightweight and computationally efficient. Empirical results show that existing factuality metrics fail to effectively evaluate factuality in PLS, especially for elaborative explanations, whereas PlainQAFact achieves state-of-the-art performance. We further analyze its effectiveness across external knowledge sources, answer extraction strategies, overlap measures, and document granularity levels, refining its overall factuality assessment.</li>
</ul>

<h3>Title: Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks</h3>
<ul>
<li><strong>Authors: </strong>Stefano Riva, Carolina Introini, J. Nathan Kutz, Antonio Cammi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08904">https://arxiv.org/abs/2503.08904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08904">https://arxiv.org/pdf/2503.08904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08904]] Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks(https://arxiv.org/abs/2503.08904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The recent developments in data-driven methods have paved the way to new methodologies to provide accurate state reconstruction of engineering systems; nuclear reactors represent particularly challenging applications for this task due to the complexity of the strongly coupled physics involved and the extremely harsh and hostile environments, especially for new technologies such as Generation-IV reactors. Data-driven techniques can combine different sources of information, including computational proxy models and local noisy measurements on the system, to robustly estimate the state. This work leverages the novel Shallow Recurrent Decoder architecture to infer the entire state vector (including neutron fluxes, precursors concentrations, temperature, pressure and velocity) of a reactor from three out-of-core time-series neutron flux measurements alone. In particular, this work extends the standard architecture to treat parametric time-series data, ensuring the possibility of investigating different accidental scenarios and showing the capabilities of this approach to provide an accurate state estimation in various operating conditions. This paper considers as a test case the Molten Salt Fast Reactor (MSFR), a Generation-IV reactor concept, characterised by strong coupling between the neutronics and the thermal hydraulics due to the liquid nature of the fuel. The promising results of this work are further strengthened by the possibility of quantifying the uncertainty associated with the state estimation, due to the considerably low training cost. The accurate reconstruction of every characteristic field in real-time makes this approach suitable for monitoring and control purposes in the framework of a reactor digital twin.</li>
</ul>

<h3>Title: Interpreting the Repeated Token Phenomenon in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Itay Yona, Ilia Shumailov, Jamie Hayes, Federico Barbero, Yossi Gandelsman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08908">https://arxiv.org/abs/2503.08908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08908">https://arxiv.org/pdf/2503.08908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08908]] Interpreting the Repeated Token Phenomenon in Large Language Models(https://arxiv.org/abs/2503.08908)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite their impressive capabilities, often fail to accurately repeat a single word when prompted to, and instead output unrelated text. This unexplained failure mode represents a vulnerability, allowing even end-users to diverge models away from their intended behavior. We aim to explain the causes for this phenomenon and link it to the concept of ``attention sinks'', an emergent LLM behavior crucial for fluency, in which the initial token receives disproportionately high attention scores. Our investigation identifies the neural circuit responsible for attention sinks and shows how long repetitions disrupt this circuit. We extend this finding to other non-repeating sequences that exhibit similar circuit disruptions. To address this, we propose a targeted patch that effectively resolves the issue without negatively impacting the model's overall performance. This study provides a mechanistic explanation for an LLM vulnerability, demonstrating how interpretability can diagnose and address issues, and offering insights that pave the way for more secure and reliable models.</li>
</ul>

<h3>Title: Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data</h3>
<ul>
<li><strong>Authors: </strong>Dandan Zhao, Hongpeng Yin, Jintang Bian, Han Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08916">https://arxiv.org/abs/2503.08916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08916">https://arxiv.org/pdf/2503.08916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08916]] Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data(https://arxiv.org/abs/2503.08916)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional fault diagnosis methods struggle to handle fault data, with complex data characteristics such as high dimensions and large noise. Deep learning is a promising solution, which typically works well only when labeled fault data are available. To address these problems, a robust unsupervised fault diagnosis using machine learning is proposed in this paper. First, a special dimension reduction method for the high-dimensional fault data is designed. Second, the extracted features are enhanced by incorporating nonlinear information through the learning of a graph structure. Third, to alleviate the problem of reduced fault-diagnosis accuracy attributed to noise and outliers, $l_{2,1}$-norm and typicality-aware constraints are introduced from the perspective of model optimization, respectively. Finally, this paper provides comprehensive theoretical and experimental evidence supporting the effectiveness and robustness of the proposed method. The experiments on both the benchmark Tennessee-Eastman process and a real hot-steel milling process show that the proposed method exhibits better robustness compared to other methods, maintaining high diagnostic accuracy even in the presence of outliers or noise.</li>
</ul>

<h3>Title: Multilevel Generative Samplers for Investigating Critical Phenomena</h3>
<ul>
<li><strong>Authors: </strong>Ankur Singha, Elia Cellini, Kim A. Nicoli, Karl Jansen, Stefan Kühn, Shinichi Nakajima</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-lat, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08918">https://arxiv.org/abs/2503.08918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08918">https://arxiv.org/pdf/2503.08918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08918]] Multilevel Generative Samplers for Investigating Critical Phenomena(https://arxiv.org/abs/2503.08918)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Investigating critical phenomena or phase transitions is of high interest in physics and chemistry, for which Monte Carlo (MC) simulations, a crucial tool for numerically analyzing macroscopic properties of given systems, are often hindered by an emerging divergence of correlation length -- known as scale invariance at criticality (SIC) in the renormalization group theory. SIC causes the system to behave the same at any length scale, from which many existing sampling methods suffer: long-range correlations cause critical slowing down in Markov chain Monte Carlo (MCMC), and require intractably large receptive fields for generative samplers. In this paper, we propose a Renormalization-informed Generative Critical Sampler (RiGCS) -- a novel sampler specialized for near-critical systems, where SIC is leveraged as an advantage rather than a nuisance. Specifically, RiGCS builds on MultiLevel Monte Carlo (MLMC) with Heat Bath (HB) algorithms, which perform ancestral sampling from low-resolution to high-resolution lattice configurations with site-wise-independent conditional HB sampling. Although MLMC-HB is highly efficient under exact SIC, it suffers from a low acceptance rate under slight SIC violation. Notably, SIC violation always occurs in finite-size systems, and may induce long-range and higher-order interactions in the renormalized distributions, which are not considered by independent HB samplers. RiGCS enhances MLMC-HB by replacing a part of the conditional HB sampler with generative models that capture those residual interactions and improve the sampling efficiency. Our experiments show that the effective sample size of RiGCS is a few orders of magnitude higher than state-of-the-art generative model baselines in sampling configurations for 128x128 two-dimensional Ising systems.</li>
</ul>

<h3>Title: Backtracking for Safety</h3>
<ul>
<li><strong>Authors: </strong>Bilgehan Sel, Dingcheng Li, Phillip Wallis, Vaishakh Keshava, Ming Jin, Siddhartha Reddy Jonnalagadda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08919">https://arxiv.org/abs/2503.08919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08919">https://arxiv.org/pdf/2503.08919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08919]] Backtracking for Safety(https://arxiv.org/abs/2503.08919)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across various tasks, but ensuring their safety and alignment with human values remains crucial. Current safety alignment methods, such as supervised fine-tuning and reinforcement learning-based approaches, can exhibit vulnerabilities to adversarial attacks and often result in shallow safety alignment, primarily focusing on preventing harmful content in the initial tokens of the generated output. While methods like resetting can help recover from unsafe generations by discarding previous tokens and restarting the generation process, they are not well-suited for addressing nuanced safety violations like toxicity that may arise within otherwise benign and lengthy generations. In this paper, we propose a novel backtracking method designed to address these limitations. Our method allows the model to revert to a safer generation state, not necessarily at the beginning, when safety violations occur during generation. This approach enables targeted correction of problematic segments without discarding the entire generated text, thereby preserving efficiency. We demonstrate that our method dramatically reduces toxicity appearing through the generation process with minimal impact to efficiency.</li>
</ul>

<h3>Title: Enhancing Large Language Models for Hardware Verification: A Novel SystemVerilog Assertion Dataset</h3>
<ul>
<li><strong>Authors: </strong>Anand Menon, Samit S Miftah, Shamik Kundu, Souvik Kundu, Amisha Srivastava, Arnab Raha, Gabriel Theodor Sonnenschein, Suvadeep Banerjee, Deepak Mathaikutty, Kanad Basu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08923">https://arxiv.org/abs/2503.08923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08923">https://arxiv.org/pdf/2503.08923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08923]] Enhancing Large Language Models for Hardware Verification: A Novel SystemVerilog Assertion Dataset(https://arxiv.org/abs/2503.08923)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Hardware verification is crucial in modern SoC design, consuming around 70% of development time. SystemVerilog assertions ensure correct functionality. However, existing industrial practices rely on manual efforts for assertion generation, which becomes increasingly untenable as hardware systems become complex. Recent research shows that Large Language Models (LLMs) can automate this process. However, proprietary SOTA models like GPT-4o often generate inaccurate assertions and require expensive licenses, while smaller open-source LLMs need fine-tuning to manage HDL code complexities. To address these issues, we introduce **VERT**, an open-source dataset designed to enhance SystemVerilog assertion generation using LLMs. VERT enables researchers in academia and industry to fine-tune open-source models, outperforming larger proprietary ones in both accuracy and efficiency while ensuring data privacy through local fine-tuning and eliminating costly licenses. The dataset is curated by systematically augmenting variables from open-source HDL repositories to generate synthetic code snippets paired with corresponding assertions. Experimental results demonstrate that fine-tuned models like Deepseek Coder 6.7B and Llama 3.1 8B outperform GPT-4o, achieving up to 96.88% improvement over base models and 24.14% over GPT-4o on platforms including OpenTitan, CVA6, OpenPiton and Pulpissimo. VERT is available at this https URL.</li>
</ul>

<h3>Title: Near-Optimal Sample Complexity for Iterated CVaR Reinforcement Learning with a Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Zilong Deng, Simon Khan, Shaofeng Zou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08934">https://arxiv.org/abs/2503.08934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08934">https://arxiv.org/pdf/2503.08934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08934]] Near-Optimal Sample Complexity for Iterated CVaR Reinforcement Learning with a Generative Model(https://arxiv.org/abs/2503.08934)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>In this work, we study the sample complexity problem of risk-sensitive Reinforcement Learning (RL) with a generative model, where we aim to maximize the Conditional Value at Risk (CVaR) with risk tolerance level $\tau$ at each step, named Iterated CVaR. %We consider the sample complexity of obtaining an $\epsilon$-optimal policy in an infinite horizon discounted MDP, given access to a generative model. % We first build a connection between Iterated CVaR RL with $(s, a)$-rectangular distributional robust RL with the specific uncertainty set for CVaR. We develop nearly matching upper and lower bounds on the sample complexity for this problem. Specifically, we first prove that a value iteration-based algorithm, ICVaR-VI, achieves an $\epsilon$-optimal policy with at most $\tilde{O}\left(\frac{SA}{(1-\gamma)^4\tau^2\epsilon^2}\right)$ samples, where $\gamma$ is the discount factor, and $S, A$ are the sizes of the state and action spaces. Furthermore, if $\tau \geq \gamma$, then the sample complexity can be further improved to $\tilde{O}\left( \frac{SA}{(1-\gamma)^3\epsilon^2} \right)$. We further show a minimax lower bound of ${\tilde{O}}\left(\frac{(1-\gamma \tau)SA}{(1-\gamma)^4\tau\epsilon^2}\right)$. For a constant risk level $0<\tau\leq 1$, our upper and lower bounds match with each other, demonstrating the tightness and optimality of our analyses. We also investigate a limiting case with a small risk level $\tau$, called Worst-Path RL, where the objective is to maximize the minimum possible cumulative reward. We develop matching upper and lower bounds of $\tilde{O}\left(\frac{SA}{p_{\min}}\right)$, where $p_{\min}$ denotes the minimum non-zero reaching probability of the transition kernel.</li>
</ul>

<h3>Title: KAN-Mixers: a new deep learning architecture for image classification</h3>
<ul>
<li><strong>Authors: </strong>Jorge Luiz dos Santos Canuto, Linnyer Beatrys Ruiz Aylon, Rodrigo Clemente Thom de Souza</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08939">https://arxiv.org/abs/2503.08939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08939">https://arxiv.org/pdf/2503.08939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08939]] KAN-Mixers: a new deep learning architecture for image classification(https://arxiv.org/abs/2503.08939)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Due to their effective performance, Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures have become the standard for solving computer vision tasks. Such architectures require large data sets and rely on convolution and self-attention operations. In 2021, MLP-Mixer emerged, an architecture that relies only on Multilayer Perceptron (MLP) and achieves extremely competitive results when compared to CNNs and ViTs. Despite its good performance in computer vision tasks, the MLP-Mixer architecture may not be suitable for refined feature extraction in images. Recently, the Kolmogorov-Arnold Network (KAN) was proposed as a promising alternative to MLP models. KANs promise to improve accuracy and interpretability when compared to MLPs. Therefore, the present work aims to design a new mixer-based architecture, called KAN-Mixers, using KANs as main layers and evaluate its performance, in terms of several performance metrics, in the image classification task. As main results obtained, the KAN-Mixers model was superior to the MLP, MLP-Mixer and KAN models in the Fashion-MNIST and CIFAR-10 datasets, with 0.9030 and 0.6980 of average accuracy, respectively.</li>
</ul>

<h3>Title: Extragradient Preference Optimization (EGPO): Beyond Last-Iterate Convergence for Nash Learning from Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Runlong Zhou, Maryam Fazel, Simon S. Du</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08942">https://arxiv.org/abs/2503.08942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08942">https://arxiv.org/pdf/2503.08942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08942]] Extragradient Preference Optimization (EGPO): Beyond Last-Iterate Convergence for Nash Learning from Human Feedback(https://arxiv.org/abs/2503.08942)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has become essential for improving language model capabilities, but traditional approaches rely on the assumption that human preferences follow a transitive Bradley-Terry model. This assumption fails to capture the non-transitive nature of populational human preferences. Nash learning from human feedback (NLHF), targeting non-transitive preferences, is a problem of computing the Nash equilibrium (NE) of the two-player constant-sum game defined by the human preference. We introduce Extragradient preference optimization (EGPO), a novel algorithm for NLHF achieving last-iterate linear convergence to the NE of KL-regularized games and polynomial convergence to the NE of original games, while being robust to noise. Unlike previous approaches that rely on nested optimization, we derive an equivalent implementation using gradients of an online variant of the identity preference optimization (IPO) loss, enabling more faithful implementation for neural networks. Our empirical evaluations demonstrate EGPO's superior performance over baseline methods when training for the same number of epochs, as measured by pairwise win-rates using the ground truth preference. These results validate both the theoretical strengths and practical advantages of EGPO for language model alignment with non-transitive human preferences.</li>
</ul>

<h3>Title: Leaky Batteries: A Novel Set of Side-Channel Attacks on Electric Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Francesco Marchiori, Mauro Conti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08956">https://arxiv.org/abs/2503.08956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08956">https://arxiv.org/pdf/2503.08956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08956]] Leaky Batteries: A Novel Set of Side-Channel Attacks on Electric Vehicles(https://arxiv.org/abs/2503.08956)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack, extraction</a></li>
<li><strong>Abstract: </strong>Advancements in battery technology have accelerated the adoption of Electric Vehicles (EVs) due to their environmental benefits. However, their growing sophistication introduces security and privacy challenges. Often seen as mere operational data, battery consumption patterns can unintentionally reveal critical information exploitable for malicious purposes. These risks go beyond privacy, impacting vehicle security and regulatory compliance. Despite these concerns, current research has largely overlooked the broader implications of battery consumption data exposure. As EVs integrate further into smart transportation networks, addressing these gaps is crucial to ensure their safety, reliability, and resilience. In this work, we introduce a novel class of side-channel attacks that exploit EV battery data to extract sensitive user information. Leveraging only battery consumption patterns, we demonstrate a methodology to accurately identify the EV driver and their driving style, determine the number of occupants, and infer the vehicle's start and end locations when user habits are known. We utilize several machine learning models and feature extraction techniques to analyze EV power consumption patterns, validating our approach on simulated and real-world datasets collected from actual drivers. Our attacks achieve an average success rate of 95.4% across all attack objectives. Our findings highlight the privacy risks associated with EV battery data, emphasizing the need for stronger protections to safeguard user privacy and vehicle security.</li>
</ul>

<h3>Title: Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Jiaxin Zhang, Xiang Gao, Wendi Cui, Peng Li, Kamalika Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08963">https://arxiv.org/abs/2503.08963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08963">https://arxiv.org/pdf/2503.08963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08963]] Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation(https://arxiv.org/abs/2503.08963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In tasks like summarization and open-book question answering (QA), Large Language Models (LLMs) often encounter "contextual hallucination", where they produce irrelevant or incorrect responses despite having access to accurate source information. This typically occurs because these models tend to prioritize self-generated content over the input context, causing them to disregard pertinent details. To address this challenge, we introduce a novel method called "Guided Attention Map Editing" (GAME), which dynamically adjusts attention maps to improve contextual relevance. During inference, GAME employs a trained classifier to identify attention maps prone to inducing hallucinations and executes targeted interventions. These interventions, guided by gradient-informed "edit directions'', strategically redistribute attention weights across various heads to effectively reduce hallucination. Comprehensive evaluations on challenging summarization and open-book QA tasks show that GAME consistently reduces hallucinations across a variety of open-source models. Specifically, GAME reduces hallucinations by 10% in the XSum summarization task while achieving a 7X speed-up in computational efficiency compared to the state-of-the-art baselines.</li>
</ul>

<h3>Title: CIPHERMATCH: Accelerating Homomorphic Encryption-Based String Matching via Memory-Efficient Data Packing and In-Flash Processing</h3>
<ul>
<li><strong>Authors: </strong>Mayank Kabra, Rakesh Nadig, Harshita Gupta, Rahul Bera, Manos Frouzakis, Vamanan Arulchelvan, Yu Liang, Haiyu Mao, Mohammad Sadrosadati, Onur Mutlu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08968">https://arxiv.org/abs/2503.08968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08968">https://arxiv.org/pdf/2503.08968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08968]] CIPHERMATCH: Accelerating Homomorphic Encryption-Based String Matching via Memory-Efficient Data Packing and In-Flash Processing(https://arxiv.org/abs/2503.08968)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, biometric</a></li>
<li><strong>Abstract: </strong>Homomorphic encryption (HE) allows secure computation on encrypted data without revealing the original data, providing significant benefits for privacy-sensitive applications. Many cloud computing applications (e.g., DNA read mapping, biometric matching, web search) use exact string matching as a key operation. However, prior string matching algorithms that use homomorphic encryption are limited by high computational latency caused by the use of complex operations and data movement bottlenecks due to the large encrypted data size. In this work, we provide an efficient algorithm-hardware codesign to accelerate HE-based secure exact string matching. We propose CIPHERMATCH, which (i) reduces the increase in memory footprint after encryption using an optimized software-based data packing scheme, (ii) eliminates the use of costly homomorphic operations (e.g., multiplication and rotation), and (iii) reduces data movement by designing a new in-flash processing (IFP) architecture. We demonstrate the benefits of CIPHERMATCH using two case studies: (1) Exact DNA string matching and (2) encrypted database search. Our pure software-based CIPHERMATCH implementation that uses our memory-efficient data packing scheme improves performance and reduces energy consumption by 42.9X and 17.6X, respectively, compared to the state-of-the-art software baseline. Integrating CIPHERMATCH with IFP improves performance and reduces energy consumption by 136.9X and 256.4X, respectively, compared to the software-based CIPHERMATCH implementation.</li>
</ul>

<h3>Title: Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Idris Zakariyya, Ferheen Ayaz, Mounia Kharbouche-Harrari, Jeremy Singer, Sye Loong Keoh, Danilo Pau, José Cano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08973">https://arxiv.org/abs/2503.08973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08973">https://arxiv.org/pdf/2503.08973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08973]] Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial Attacks(https://arxiv.org/abs/2503.08973)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Reducing the memory footprint of Machine Learning (ML) models, especially Deep Neural Networks (DNNs), is imperative to facilitate their deployment on resource-constrained edge devices. However, a notable drawback of DNN models lies in their susceptibility to adversarial attacks, wherein minor input perturbations can deceive them. A primary challenge revolves around the development of accurate, resilient, and compact DNN models suitable for deployment on resource-constrained edge devices. This paper presents the outcomes of a compact DNN model that exhibits resilience against both black-box and white-box adversarial attacks. This work has achieved this resilience through training with the QKeras quantization-aware training framework. The study explores the potential of QKeras and an adversarial robustness technique, Jacobian Regularization (JR), to co-optimize the DNN architecture through per-layer JR methodology. As a result, this paper has devised a DNN model employing this co-optimization strategy based on Stochastic Ternary Quantization (STQ). Its performance was compared against existing DNN models in the face of various white-box and black-box attacks. The experimental findings revealed that, the proposed DNN model had small footprint and on average, it exhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T) benchmarks when challenged with white-box and black-box attacks, respectively, on the CIFAR-10 image and Google Speech Commands audio datasets.</li>
</ul>

<h3>Title: Beyond Overfitting: Doubly Adaptive Dropout for Generalizable AU Detection</h3>
<ul>
<li><strong>Authors: </strong>Yong Li, Yi Ren, Xuesong Niu, Yi Ding, Xiu-Shen Wei, Cuntai Guan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08974">https://arxiv.org/abs/2503.08974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08974">https://arxiv.org/pdf/2503.08974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08974]] Beyond Overfitting: Doubly Adaptive Dropout for Generalizable AU Detection(https://arxiv.org/abs/2503.08974)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Facial Action Units (AUs) are essential for conveying psychological states and emotional expressions. While automatic AU detection systems leveraging deep learning have progressed, they often overfit to specific datasets and individual features, limiting their cross-domain applicability. To overcome these limitations, we propose a doubly adaptive dropout approach for cross-domain AU detection, which enhances the robustness of convolutional feature maps and spatial tokens against domain shifts. This approach includes a Channel Drop Unit (CD-Unit) and a Token Drop Unit (TD-Unit), which work together to reduce domain-specific noise at both the channel and token levels. The CD-Unit preserves domain-agnostic local patterns in feature maps, while the TD-Unit helps the model identify AU relationships generalizable across domains. An auxiliary domain classifier, integrated at each layer, guides the selective omission of domain-sensitive features. To prevent excessive feature dropout, a progressive training strategy is used, allowing for selective exclusion of sensitive features at any model layer. Our method consistently outperforms existing techniques in cross-domain AU detection, as demonstrated by extensive experimental evaluations. Visualizations of attention maps also highlight clear and meaningful patterns related to both individual and combined AUs, further validating the approach's effectiveness.</li>
</ul>

<h3>Title: Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Zirui Gong, Yanjun Zhang, Leo Yu Zhang, Zhaoxi Zhang, Yong Xiang, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08976">https://arxiv.org/abs/2503.08976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08976">https://arxiv.org/pdf/2503.08976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08976]] Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning(https://arxiv.org/abs/2503.08976)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Ranking Learning (FRL) is a state-of-the-art FL framework that stands out for its communication efficiency and resilience to poisoning attacks. It diverges from the traditional FL framework in two ways: 1) it leverages discrete rankings instead of gradient updates, significantly reducing communication costs and limiting the potential space for malicious updates, and 2) it uses majority voting on the server side to establish the global ranking, ensuring that individual updates have minimal influence since each client contributes only a single vote. These features enhance the system's scalability and position FRL as a promising paradigm for FL training. However, our analysis reveals that FRL is not inherently robust, as certain edges are particularly vulnerable to poisoning attacks. Through a theoretical investigation, we prove the existence of these vulnerable edges and establish a lower bound and an upper bound for identifying them in each layer. Based on this finding, we introduce a novel local model poisoning attack against FRL, namely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on identifying and perturbing the most vulnerable edges in each layer and leveraging an optimization-based approach to maximize the attack's impact. Through extensive experiments on benchmark datasets, we demonstrate that our attack achieves an overall 53.23% attack impact and is 3.7x more impactful than existing methods. Our findings highlight significant vulnerabilities in ranking-based FL systems and underline the urgency for the development of new robust FL frameworks.</li>
</ul>

<h3>Title: I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Liu, Dong Gong, Erdun Gao, Zhen Zhang, Biwei Huang, Mingming Gong, Anton van den Hengel, Javen Qinfeng Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08980">https://arxiv.org/abs/2503.08980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08980">https://arxiv.org/pdf/2503.08980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08980]] I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?(https://arxiv.org/abs/2503.08980)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. To illuminate the distinction between these explanations, we introduce a novel generative model that generates tokens on the basis of human interpretable concepts represented as latent discrete variables. Under mild conditions, even when the mapping from the latent space to the observed space is non-invertible, we establish an identifiability result: the representations learned by LLMs through next-token prediction can be approximately modeled as the logarithm of the posterior probabilities of these latent discrete concepts, up to an invertible linear transformation. This theoretical finding not only provides evidence that LLMs capture underlying generative factors, but also strongly reinforces the linear representation hypothesis, which posits that LLMs learn linear representations of human-interpretable concepts. Empirically, we validate our theoretical results through evaluations on both simulation data and the Pythia, Llama, and DeepSeek model families.</li>
</ul>

<h3>Title: JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing</h3>
<ul>
<li><strong>Authors: </strong>Vasudev Gohil</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08990">https://arxiv.org/abs/2503.08990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08990">https://arxiv.org/pdf/2503.08990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08990]] JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing(https://arxiv.org/abs/2503.08990)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown great promise as language understanding and decision making tools, and they have permeated various aspects of our everyday life. However, their widespread availability also comes with novel risks, such as generating harmful, unethical, or offensive content, via an attack called jailbreaking. Despite extensive efforts from LLM developers to align LLMs using human feedback, they are still susceptible to jailbreak attacks. To tackle this issue, researchers often employ red-teaming to understand and investigate jailbreak prompts. However, existing red-teaming approaches lack effectiveness, scalability, or both. To address these issues, we propose JBFuzz, a novel effective, automated, and scalable red-teaming technique for jailbreaking LLMs. JBFuzz is inspired by the success of fuzzing for detecting bugs/vulnerabilities in software. We overcome three challenges related to effectiveness and scalability by devising novel seed prompts, a lightweight mutation engine, and a lightweight and accurate evaluator for guiding the fuzzer. Assimilating all three solutions results in a potent fuzzer that only requires black-box access to the target LLM. We perform extensive experimental evaluation of JBFuzz using nine popular and widely-used LLMs. We find that JBFuzz successfully jailbreaks all LLMs for various harmful/unethical questions, with an average attack success rate of 99%. We also find that JBFuzz is extremely efficient as it jailbreaks a given LLM for a given question in 60 seconds on average. Our work highlights the susceptibility of the state-of-the-art LLMs to jailbreak attacks even after safety alignment, and serves as a valuable red-teaming tool for LLM developers.</li>
</ul>

<h3>Title: From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches</h3>
<ul>
<li><strong>Authors: </strong>Wei Ruan, Tianze Yang, Yifan Zhou, Tianming Liu, Jin Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.08998">https://arxiv.org/abs/2503.08998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.08998">https://arxiv.org/pdf/2503.08998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.08998]] From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches(https://arxiv.org/abs/2503.08998)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Model merging has achieved significant success, with numerous innovative methods proposed to enhance capabilities by combining multiple models. However, challenges persist due to the lack of a unified framework for classification and systematic comparative analysis, leading to inconsistencies in terminologies and categorizations. Meanwhile, as an increasing number of fine-tuned models are publicly available, their original training data often remain inaccessible due to privacy concerns or intellectual property restrictions. This makes traditional multi-task learning based on shared training data impractical. In scenarios where direct access to training data is infeasible, merging model parameters to create a unified model with broad generalization across multiple domains becomes crucial, further underscoring the importance of model merging techniques. Despite the rapid progress in this field, a comprehensive taxonomy and survey summarizing recent advances and predicting future directions are still lacking. This paper addresses these gaps by establishing a new taxonomy of model merging methods, systematically comparing different approaches, and providing an overview of key developments. By offering a structured perspective on this evolving area, we aim to help newcomers quickly grasp the field's landscape and inspire further innovations.</li>
</ul>

<h3>Title: Towards Quantifying Long-Range Interactions in Graph Machine Learning: a Large Graph Dataset and a Measurement</h3>
<ul>
<li><strong>Authors: </strong>Huidong Liang, Haitz Sáez de Ocáriz Borde, Baskaran Sripathmanathan, Michael Bronstein, Xiaowen Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09008">https://arxiv.org/abs/2503.09008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09008">https://arxiv.org/pdf/2503.09008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09008]] Towards Quantifying Long-Range Interactions in Graph Machine Learning: a Large Graph Dataset and a Measurement(https://arxiv.org/abs/2503.09008)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Long-range dependencies are critical for effective graph representation learning, yet most existing datasets focus on small graphs tailored to inductive tasks, offering limited insight into long-range interactions. Current evaluations primarily compare models employing global attention (e.g., graph transformers) with those using local neighborhood aggregation (e.g., message-passing neural networks) without a direct measurement of long-range dependency. In this work, we introduce City-Networks, a novel large-scale transductive learning dataset derived from real-world city roads. This dataset features graphs with over $10^5$ nodes and significantly larger diameters than those in existing benchmarks, naturally embodying long-range information. We annotate the graphs using an eccentricity-based approach, ensuring that the classification task inherently requires information from distant nodes. Furthermore, we propose a model-agnostic measurement based on the Jacobians of neighbors from distant hops, offering a principled quantification of long-range dependencies. Finally, we provide theoretical justifications for both our dataset design and the proposed measurement - particularly by focusing on over-smoothing and influence score dilution - which establishes a robust foundation for further exploration of long-range interactions in graph neural networks.</li>
</ul>

<h3>Title: Prompt Inversion Attack against Collaborative Inference of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Qu, Yuguang Zhou, Yongji Wu, Tingsong Xiao, Binhang Yuan, Yiming Li, Jiaheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09022">https://arxiv.org/abs/2503.09022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09022">https://arxiv.org/pdf/2503.09022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09022]] Prompt Inversion Attack against Collaborative Inference of Large Language Models(https://arxiv.org/abs/2503.09022)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely applied for their remarkable capability of content generation. However, the practical use of open-source LLMs is hindered by high resource requirements, making deployment expensive and limiting widespread development. The collaborative inference is a promising solution for this problem, in which users collaborate by each hosting a subset of layers and transmitting intermediate activation. Many companies are building collaborative inference platforms to reduce LLM serving costs, leveraging users' underutilized GPUs. Despite widespread interest in collaborative inference within academia and industry, the privacy risks associated with LLM collaborative inference have not been well studied. This is largely because of the challenge posed by inverting LLM activation due to its strong non-linearity. In this paper, to validate the severity of privacy threats in LLM collaborative inference, we introduce the concept of prompt inversion attack (PIA), where a malicious participant intends to recover the input prompt through the activation transmitted by its previous participant. Extensive experiments show that our PIA method substantially outperforms existing baselines. For example, our method achieves an 88.4\% token accuracy on the Skytrax dataset with the Llama-65B model when inverting the maximum number of transformer layers, while the best baseline method only achieves 22.8\% accuracy. The results verify the effectiveness of our PIA attack and highlights its practical threat to LLM collaborative inference systems.</li>
</ul>

<h3>Title: Aligning to What? Limits to RLHF Based Alignment</h3>
<ul>
<li><strong>Authors: </strong>Logan Barnhart, Reza Akbarian Bafghi, Stephen Becker, Maziar Raissi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09025">https://arxiv.org/abs/2503.09025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09025">https://arxiv.org/pdf/2503.09025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09025]] Aligning to What? Limits to RLHF Based Alignment(https://arxiv.org/abs/2503.09025)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is increasingly used to align large language models (LLMs) with human preferences. However, the effectiveness of RLHF in addressing underlying biases remains unclear. This study investigates the relationship between RLHF and both covert and overt biases in LLMs, particularly focusing on biases against African Americans. We applied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and evaluated the covert and overt biases of the resulting models using matched-guise probing and explicit bias testing. We performed additional tests with DPO on different base models and datasets; among several implications, we found that SFT before RLHF calcifies model biases. Additionally, we extend the tools for measuring biases to multi-modal models. Through our experiments we collect evidence that indicates that current alignment techniques are inadequate for nebulous tasks such as mitigating covert biases, highlighting the need for capable datasets, data curating techniques, or alignment tools.</li>
</ul>

<h3>Title: DAST: Difficulty-Aware Self-Training on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Boyang Xue, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Hongling Xu, Fei Mi, Yasheng Wang, Lifeng Shang, Qun Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09029">https://arxiv.org/abs/2503.09029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09029">https://arxiv.org/pdf/2503.09029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09029]] DAST: Difficulty-Aware Self-Training on Large Language Models(https://arxiv.org/abs/2503.09029)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Present Large Language Models (LLM) self-training methods always under-sample on challenging queries, leading to inadequate learning on difficult problems which limits LLMs' ability. Therefore, this work proposes a difficulty-aware self-training (DAST) framework that focuses on improving both the quantity and quality of self-generated responses on challenging queries during self-training. DAST is specified in three components: 1) sampling-based difficulty level estimation, 2) difficulty-aware data augmentation, and 3) the self-training algorithm using SFT and DPO respectively. Experiments on mathematical tasks demonstrate the effectiveness and generalization of DAST, highlighting the critical role of difficulty-aware strategies in advancing LLM self-training.</li>
</ul>

<h3>Title: Teaching LLMs How to Learn with Contextual Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Younwoo Choi, Muhammad Adil Asif, Ziwen Han, John Willes, Rahul G. Krishnan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09032">https://arxiv.org/abs/2503.09032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09032">https://arxiv.org/pdf/2503.09032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09032]] Teaching LLMs How to Learn with Contextual Fine-Tuning(https://arxiv.org/abs/2503.09032)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human's learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, "can prompting help us teach LLMs how to learn". In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model's interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.</li>
</ul>

<h3>Title: Image Encryption Using DNA Encoding, Snake Permutation and Chaotic Substitution Techniques</h3>
<ul>
<li><strong>Authors: </strong>Waleed Ahmed Farooqui, Jawad Ahmad, Nadeem Kureshi, Fawad Ahmed, Aizaz Ahmad Khattak, Muhammad Shahbaz Khan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09038">https://arxiv.org/abs/2503.09038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09038">https://arxiv.org/pdf/2503.09038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09038]] Image Encryption Using DNA Encoding, Snake Permutation and Chaotic Substitution Techniques(https://arxiv.org/abs/2503.09038)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Securing image data in IoT networks and other insecure information channels is a matter of critical concern. This paper presents a new image encryption scheme using DNA encoding, snake permutation and chaotic substitution techniques that ensures robust security of the image data with reduced computational overhead. The DNA encoding and snake permutation modules ensure effective scrambling of the pixels and result in efficient diffusion in the plaintext image. For the confusion part, the chaotic substitution technique is implemented, which substitutes the pixel values chosen randomly from 3 S-boxes. Extensive security analysis validate the efficacy of the image encryption algorithm proposed in this paper and results demonstrate that the encrypted images have an ideal information entropy of 7.9895 and an almost zero correlation coefficient of -0.001660. These results indicate a high degree of randomness and no correlation in the encrypted image.</li>
</ul>

<h3>Title: Discovering Influential Neuron Path in Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Yifei Liu, Yingdong Shi, Changming Li, Anqi Pang, Sibei Yang, Jingyi Yu, Kan Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09046">https://arxiv.org/abs/2503.09046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09046">https://arxiv.org/pdf/2503.09046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09046]] Discovering Influential Neuron Path in Vision Transformers(https://arxiv.org/abs/2503.09046)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning. The project website including implementation code is available at this https URL.</li>
</ul>

<h3>Title: Performance Evaluation of Threshold Signing Schemes in Cryptography</h3>
<ul>
<li><strong>Authors: </strong>Faneela, Jawad Ahmad, Baraq Ghaleb, Imdad Ullah Khan, William J. Buchanan, Sana Ullah Jan, Muhammad Shahbaz Khan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09047">https://arxiv.org/abs/2503.09047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09047">https://arxiv.org/pdf/2503.09047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09047]] Performance Evaluation of Threshold Signing Schemes in Cryptography(https://arxiv.org/abs/2503.09047)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>Threshold Signature Scheme (TSS) protocols have gained significant attention over the past ten years due to their widespread adoption in cryptocurrencies. The adoption is mainly boosted by Gennaro and Goldfedder's TSS protocol. Since then, various TSS protocols have been introduced with different features, such as security and performance, etc. Large organizations are using TSS protocols to protect many digital assets, such as cryptocurrency. However, the adoption of these TSS protocols requires an understanding of state-of-the-art research in threshold signing. This study describes the holistic view of TSS protocols, evaluates cutting-edge TSS protocols, highlights their characteristics, and compares them in terms of security and performance. The evaluation of these TSS protocols will help the researchers address real-world problems by considering the relevant merits of different TSS protocols.</li>
</ul>

<h3>Title: Adaptive Backdoor Attacks with Reasonable Constraints on Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Xuewen Dong, Jiachen Li, Shujun Li, Zhichao You, Qiang Qu, Yaroslav Kholodov, Yulong Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09049">https://arxiv.org/abs/2503.09049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09049">https://arxiv.org/pdf/2503.09049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09049]] Adaptive Backdoor Attacks with Reasonable Constraints on Graph Neural Networks(https://arxiv.org/abs/2503.09049)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Recent studies show that graph neural networks (GNNs) are vulnerable to backdoor attacks. Existing backdoor attacks against GNNs use fixed-pattern triggers and lack reasonable trigger constraints, overlooking individual graph characteristics and rendering insufficient evasiveness. To tackle the above issues, we propose ABARC, the first Adaptive Backdoor Attack with Reasonable Constraints, applying to both graph-level and node-level tasks in GNNs. For graph-level tasks, we propose a subgraph backdoor attack independent of the graph's topology. It dynamically selects trigger nodes for each target graph and modifies node features with constraints based on graph similarity, feature range, and feature type. For node-level tasks, our attack begins with an analysis of node features, followed by selecting and modifying trigger features, which are then constrained by node similarity, feature range, and feature type. Furthermore, an adaptive edge-pruning mechanism is designed to reduce the impact of neighbors on target nodes, ensuring a high attack success rate (ASR). Experimental results show that even with reasonable constraints for attack evasiveness, our attack achieves a high ASR while incurring a marginal clean accuracy drop (CAD). When combined with the state-of-the-art defense randomized smoothing (RS) method, our attack maintains an ASR over 94%, surpassing existing attacks by more than 7%.</li>
</ul>

<h3>Title: TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction</h3>
<ul>
<li><strong>Authors: </strong>Shengyao Lu, Jiuding Yang, Baochun Li, Di Niu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09051">https://arxiv.org/abs/2503.09051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09051">https://arxiv.org/pdf/2503.09051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09051]] TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction(https://arxiv.org/abs/2503.09051)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The growing demand for transparency and interpretability in critical domains has driven increased interests in comprehending the explainability of Message-Passing (MP) Graph Neural Networks (GNNs). Although substantial research efforts have been made to generate explanations for individual graph instances, identifying global explaining concepts for a GNN still poses great challenges, especially when concepts are desired in a graphical form on the dataset level. While most prior works treat GNNs as black boxes, in this paper, we propose to unbox GNNs by analyzing and extracting critical subtrees incurred by the inner workings of message passing, which correspond to critical subgraphs in the datasets. By aggregating subtrees in an embedding space with an efficient algorithm, which does not require complex subgraph matching or search, we can make intuitive graphical explanations for Message-Passing GNNs on local, class and global levels. We empirically show that our proposed approach not only generates clean subgraph concepts on a dataset level in contrast to existing global explaining methods which generate non-graphical rules (e.g., language or embeddings) as explanations, but it is also capable of providing explanations for individual instances with a comparable or even superior performance as compared to leading local-level GNN explainers.</li>
</ul>

<h3>Title: Implicit Contrastive Representation Learning with Guided Stop-gradient</h3>
<ul>
<li><strong>Authors: </strong>Byeongchan Lee, Sehyun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09058">https://arxiv.org/abs/2503.09058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09058">https://arxiv.org/pdf/2503.09058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09058]] Implicit Contrastive Representation Learning with Guided Stop-gradient(https://arxiv.org/abs/2503.09058)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In self-supervised representation learning, Siamese networks are a natural architecture for learning transformation-invariance by bringing representations of positive pairs closer together. But it is prone to collapse into a degenerate solution. To address the issue, in contrastive learning, a contrastive loss is used to prevent collapse by moving representations of negative pairs away from each other. But it is known that algorithms with negative sampling are not robust to a reduction in the number of negative samples. So, on the other hand, there are algorithms that do not use negative pairs. Many positive-only algorithms adopt asymmetric network architecture consisting of source and target encoders as a key factor in coping with collapse. By exploiting the asymmetric architecture, we introduce a methodology to implicitly incorporate the idea of contrastive learning. As its implementation, we present a novel method guided stop-gradient. We apply our method to benchmark algorithms SimSiam and BYOL and show that our method stabilizes training and boosts performance. We also show that the algorithms with our method work well with small batch sizes and do not collapse even when there is no predictor. The code is available at this https URL.</li>
</ul>

<h3>Title: Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States</h3>
<ul>
<li><strong>Authors: </strong>Xin Wei Chia, Jonathan Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09066">https://arxiv.org/abs/2503.09066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09066">https://arxiv.org/pdf/2503.09066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09066]] Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States(https://arxiv.org/abs/2503.09066)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they remain vulnerable to adversarial manipulations such as jailbreaking via prompt injection attacks. These attacks bypass safety mechanisms to generate restricted or harmful content. In this study, we investigated the underlying latent subspaces of safe and jailbroken states by extracting hidden activations from a LLM. Inspired by attractor dynamics in neuroscience, we hypothesized that LLM activations settle into semi stable states that can be identified and perturbed to induce state transitions. Using dimensionality reduction techniques, we projected activations from safe and jailbroken responses to reveal latent subspaces in lower dimensional spaces. We then derived a perturbation vector that when applied to safe representations, shifted the model towards a jailbreak state. Our results demonstrate that this causal intervention results in statistically significant jailbreak responses in a subset of prompts. Next, we probed how these perturbations propagate through the model's layers, testing whether the induced state change remains localized or cascades throughout the network. Our findings indicate that targeted perturbations induced distinct shifts in activations and model responses. Our approach paves the way for potential proactive defenses, shifting from traditional guardrail based methods to preemptive, model agnostic techniques that neutralize adversarial states at the representation level.</li>
</ul>

<h3>Title: Theoretical Guarantees for High Order Trajectory Refinement in Generative Flows</h3>
<ul>
<li><strong>Authors: </strong>Chengyue Gong, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yu Tian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09069">https://arxiv.org/abs/2503.09069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09069">https://arxiv.org/pdf/2503.09069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09069]] Theoretical Guarantees for High Order Trajectory Refinement in Generative Flows(https://arxiv.org/abs/2503.09069)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow matching has emerged as a powerful framework for generative modeling, offering computational advantages over diffusion models by leveraging deterministic Ordinary Differential Equations (ODEs) instead of stochastic dynamics. While prior work established the worst case optimality of standard flow matching under Wasserstein distances, the theoretical guarantees for higher-order flow matching - which incorporates acceleration terms to refine sample trajectories - remain unexplored. In this paper, we bridge this gap by proving that higher-order flow matching preserves worst case optimality as a distribution estimator. We derive upper bounds on the estimation error for second-order flow matching, demonstrating that the convergence rates depend polynomially on the smoothness of the target distribution (quantified via Besov spaces) and key parameters of the ODE dynamics. Our analysis employs neural network approximations with carefully controlled depth, width, and sparsity to bound acceleration errors across both small and large time intervals, ultimately unifying these results into a general worst case optimal bound for all time steps.</li>
</ul>

<h3>Title: Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xiaowei Bi, Zheyuan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09081">https://arxiv.org/abs/2503.09081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09081">https://arxiv.org/pdf/2503.09081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09081]] Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment(https://arxiv.org/abs/2503.09081)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Long Video Question Answering (LVQA) is challenging due to the need for temporal reasoning and large-scale multimodal data processing. Existing methods struggle with retrieving cross-modal information from long videos, especially when relevant details are sparsely distributed. We introduce UMaT (Unified Multi-modal as Text), a retrieval-augmented generation (RAG) framework that efficiently processes extremely long videos while maintaining cross-modal coherence. UMaT converts visual and auditory data into a unified textual representation, ensuring semantic and temporal alignment. Short video clips are analyzed using a vision-language model, while automatic speech recognition (ASR) transcribes dialogue. These text-based representations are structured into temporally aligned segments, with adaptive filtering to remove redundancy and retain salient details. The processed data is embedded into a vector database, enabling precise retrieval of dispersed yet relevant content. Experiments on a benchmark LVQA dataset show that UMaT outperforms existing methods in multimodal integration, long-form video understanding, and sparse information retrieval. Its scalability and interpretability allow it to process videos over an hour long while maintaining semantic and temporal coherence. These findings underscore the importance of structured retrieval and multimodal synchronization for advancing LVQA and long-form AI systems.</li>
</ul>

<h3>Title: C^2 ATTACK: Towards Representation Backdoor on CLIP via Concept Confusion</h3>
<ul>
<li><strong>Authors: </strong>Lijie Hu, Junchi Liao, Weimin Lyu, Shaopeng Fu, Tianhao Huang, Shu Yang, Guimin Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09095">https://arxiv.org/abs/2503.09095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09095">https://arxiv.org/pdf/2503.09095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09095]] C^2 ATTACK: Towards Representation Backdoor on CLIP via Concept Confusion(https://arxiv.org/abs/2503.09095)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Backdoor attacks pose a significant threat to deep learning models, enabling adversaries to embed hidden triggers that manipulate the behavior of the model during inference. Traditional backdoor attacks typically rely on inserting explicit triggers (e.g., external patches, or perturbations) into input data, but they often struggle to evade existing defense mechanisms. To address this limitation, we investigate backdoor attacks through the lens of the reasoning process in deep learning systems, drawing insights from interpretable AI. We conceptualize backdoor activation as the manipulation of learned concepts within the model's latent representations. Thus, existing attacks can be seen as implicit manipulations of these activated concepts during inference. This raises interesting questions: why not manipulate the concepts explicitly? This idea leads to our novel backdoor attack framework, Concept Confusion Attack (C^2 ATTACK), which leverages internal concepts in the model's reasoning as "triggers" without introducing explicit external modifications. By avoiding the use of real triggers and directly activating or deactivating specific concepts in latent spaces, our approach enhances stealth, making detection by existing defenses significantly harder. Using CLIP as a case study, experimental results demonstrate the effectiveness of C^2 ATTACK, achieving high attack success rates while maintaining robustness against advanced defenses.</li>
</ul>

<h3>Title: The Shape of Attraction in UMAP: Exploring the Embedding Forces in Dimensionality Reduction</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Tariqul Islam, Jason W. Fleischer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09101">https://arxiv.org/abs/2503.09101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09101">https://arxiv.org/pdf/2503.09101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09101]] The Shape of Attraction in UMAP: Exploring the Embedding Forces in Dimensionality Reduction(https://arxiv.org/abs/2503.09101)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Uniform manifold approximation and projection (UMAP) is among the most popular neighbor embedding methods. The method relies on attractive and repulsive forces among high-dimensional data points to obtain a low-dimensional embedding. In this paper, we analyze the forces to reveal their effects on cluster formations and visualization. Repulsion emphasizes differences, controlling cluster boundaries and inter-cluster distance. Attraction is more subtle, as attractive tension between points can manifest simultaneously as attraction and repulsion in the lower-dimensional mapping. This explains the need for learning rate annealing and motivates the different treatments between attractive and repulsive terms. Moreover, by modifying attraction, we improve the consistency of cluster formation under random initialization. Overall, our analysis makes UMAP and similar embedding methods more interpretable, more robust, and more accurate.</li>
</ul>

<h3>Title: VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for Detecting LLM-Generated Vaccine Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Syed Talal Ahmad, Haohui Lu, Sidong Liu, Annie Lau, Amin Beheshti, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09103">https://arxiv.org/abs/2503.09103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09103">https://arxiv.org/pdf/2503.09103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09103]] VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for Detecting LLM-Generated Vaccine Misinformation(https://arxiv.org/abs/2503.09103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have significantly improved text generation capabilities. However, they also present challenges, particularly in generating vaccine-related misinformation, which poses risks to public health. Despite research on human-authored misinformation, a notable gap remains in understanding how LLMs contribute to vaccine misinformation and how best to detect it. Existing benchmarks often overlook vaccine-specific misinformation and the diverse roles of misinformation spreaders. This paper introduces VaxGuard, a novel dataset designed to address these challenges. VaxGuard includes vaccine-related misinformation generated by multiple LLMs and provides a comprehensive framework for detecting misinformation across various roles. Our findings show that GPT-3.5 and GPT-4o consistently outperform other LLMs in detecting misinformation, especially when dealing with subtle or emotionally charged narratives. On the other hand, PHI3 and Mistral show lower performance, struggling with precision and recall in fear-driven contexts. Additionally, detection performance tends to decline as input text length increases, indicating the need for improved methods to handle larger content. These results highlight the importance of role-specific detection strategies and suggest that VaxGuard can serve as a key resource for improving the detection of LLM-generated vaccine misinformation.</li>
</ul>

<h3>Title: Constraint-Guided Learning of Data-driven Health Indicator Models: An Application on the Pronostia Bearing Dataset</h3>
<ul>
<li><strong>Authors: </strong>Yonas Tefera, Quinten Van Baelen, Maarten Meire, Stijn Luca, Peter Karsmakers</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09113">https://arxiv.org/abs/2503.09113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09113">https://arxiv.org/pdf/2503.09113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09113]] Constraint-Guided Learning of Data-driven Health Indicator Models: An Application on the Pronostia Bearing Dataset(https://arxiv.org/abs/2503.09113)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a constraint-guided deep learning framework for developing physically consistent health indicators in bearing prognostics and health management. Conventional data-driven methods often lack physical plausibility, while physics-based models are limited by incomplete system knowledge. To address this, we integrate domain knowledge into deep learning using constraints to enforce monotonicity, bound output values between 1 and 0 (representing healthy to failed states), and ensure consistency between signal energy trends and health indicator estimates. This eliminates the need for complex loss term balancing. We implement constraint-guided gradient descent within an autoencoder architecture, creating a constrained autoencoder. However, the framework is adaptable to other architectures. Using time-frequency representations of accelerometer signals from the Pronostia dataset, our constrained model generates smoother, more reliable degradation profiles compared to conventional methods, aligning with expected physical behavior. Performance is assessed using three metrics: trendability, robustness, and consistency. Compared to a conventional baseline, the constrained model improves all three. Another baseline, incorporating monotonicity via a soft-ranking loss function, outperforms in trendability but falls short in robustness and consistency. An ablation study confirms that the monotonicity constraint enhances trendability, the boundary constraint ensures consistency, and the energy-health consistency constraint improves robustness. These findings highlight the effectiveness of constraint-guided deep learning in producing reliable, physically meaningful health indicators, offering a promising direction for future prognostic applications.</li>
</ul>

<h3>Title: Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Abstreiter, Sasu Tarkoma, Roberto Morabito</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09114">https://arxiv.org/abs/2503.09114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09114">https://arxiv.org/pdf/2503.09114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09114]] Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge(https://arxiv.org/abs/2503.09114)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>The rapid rise of Language Models (LMs) has expanded the capabilities of natural language processing, powering applications from text generation to complex decision-making. While state-of-the-art LMs often boast hundreds of billions of parameters and are primarily deployed in data centers, recent trends show a growing focus on compact models-typically under 10 billion parameters-enabled by techniques such as quantization and other model compression techniques. This shift paves the way for LMs on edge devices, offering potential benefits such as enhanced privacy, reduced latency, and improved data sovereignty. However, the inherent complexity of even these smaller models, combined with the limited computing resources of edge hardware, raises critical questions about the practical trade-offs in executing LM inference outside the cloud. To address these challenges, we present a comprehensive evaluation of generative LM inference on representative CPU-based and GPU-accelerated edge devices. Our study measures key performance indicators-including memory usage, inference speed, and energy consumption-across various device configurations. Additionally, we examine throughput-energy trade-offs, cost considerations, and usability, alongside an assessment of qualitative model performance. While quantization helps mitigate memory overhead, it does not fully eliminate resource bottlenecks, especially for larger models. Our findings quantify the memory and energy constraints that must be considered for practical real-world deployments, offering concrete insights into the trade-offs between model size, inference performance, and efficiency. The exploration of LMs at the edge is still in its early stages. We hope this study provides a foundation for future research, guiding the refinement of models, the enhancement of inference efficiency, and the advancement of edge-centric AI systems.</li>
</ul>

<h3>Title: Drift-Aware Federated Learning: A Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yunjie Fang, Sheng Wu, Tao Yang, Xiaofeng Wu, Bo Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09116">https://arxiv.org/abs/2503.09116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09116">https://arxiv.org/pdf/2503.09116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09116]] Drift-Aware Federated Learning: A Causal Perspective(https://arxiv.org/abs/2503.09116)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) facilitates collaborative model training among multiple clients while preserving data privacy, often resulting in enhanced performance compared to models trained by individual clients. However, factors such as communication frequency and data distribution can contribute to feature drift, hindering the attainment of optimal training performance. This paper examine the relationship between model update drift and global as well as local optimizer from causal perspective. The influence of the global optimizer on feature drift primarily arises from the participation frequency of certain clients in server updates, whereas the effect of the local optimizer is typically associated with imbalanced data this http URL mitigate this drift, we propose a novel framework termed Causal drift-Aware Federated lEarning (CAFE). CAFE exploits the causal relationship between feature-invariant components and classification outcomes to independently calibrate local client sample features and classifiers during the training phase. In the inference phase, it eliminated the drifts in the global model that favor frequently communicating this http URL results demonstrate that CAFE's integration of feature calibration, parameter calibration, and historical information effectively reduces both drift towards majority classes and tendencies toward frequently communicating nodes.</li>
</ul>

<h3>Title: GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, Bo Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09117">https://arxiv.org/abs/2503.09117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09117">https://arxiv.org/pdf/2503.09117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09117]] GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models(https://arxiv.org/abs/2503.09117)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) unlearning has demonstrated its essential role in removing privacy and copyright-related responses, crucial for their legal and safe applications. However, the pursuit of complete unlearning often comes with substantial costs due to its compromises in their general functionality, leading to a notorious trade-off between unlearning and retention. In examining the update process for unlearning dynamically, we find gradients hold essential information for revealing this trade-off. In particular, we look at the varying relationship between retention performance and directional disparities between gradients during unlearning. It motivates the sculpting of an update mechanism derived from gradients from two sources, i.e., harmful for retention and useful for unlearning. Accordingly, we propose Gradient Rectified Unlearning (GRU), an enhanced unlearning framework controlling the updating gradients in a geometry-focused and optimization-driven manner such that their side impacts on other, unrelated responses can be minimized. Specifically, GRU derives a closed-form solution to project the unlearning gradient onto the orthogonal space of that gradient harmful for retention, ensuring minimal deviation from its original direction under the condition that overall performance is retained. Comprehensive experiments are conducted to demonstrate that GRU, as a general framework, is straightforward to implement and efficiently enhances a range of baseline methods through its adaptable and compatible characteristics. Additionally, experimental results show its broad effectiveness across a diverse set of benchmarks for LLM unlearning.</li>
</ul>

<h3>Title: Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?</h3>
<ul>
<li><strong>Authors: </strong>Yuechen Xie, Jie Song, Huiqiong Wang, Mingli Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09122">https://arxiv.org/abs/2503.09122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09122">https://arxiv.org/pdf/2503.09122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09122]] Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?(https://arxiv.org/abs/2503.09122)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, diffusion, generative</a></li>
<li><strong>Abstract: </strong>High-quality open-source text-to-image models have lowered the threshold for obtaining photorealistic images significantly, but also face potential risks of misuse. Specifically, suspects may use synthetic data generated by these generative models to train models for specific tasks without permission, when lacking real data resources especially. Protecting these generative models is crucial for the well-being of their owners. In this work, we propose the first method to this important yet unresolved issue, called Training data Provenance Verification (TrainProVe). The rationale behind TrainProVe is grounded in the principle of generalization error bound, which suggests that, for two models with the same task, if the distance between their training data distributions is smaller, their generalization ability will be closer. We validate the efficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4, latent consistency model, PixArt-$\alpha$, and Stable Cascade). The results show that TrainProVe achieves a verification accuracy of over 99\% in determining the provenance of suspicious model training data, surpassing all previous methods. Code is available at this https URL.</li>
</ul>

<h3>Title: AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jin Li, Ziqiang He, Anwei Luo, Jian-Fang Hu, Z. Jane Wang, Xiangui Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09124">https://arxiv.org/abs/2503.09124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09124">https://arxiv.org/pdf/2503.09124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09124]] AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks(https://arxiv.org/abs/2503.09124)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible perturbation to the input data. Previous methods typically improve the imperceptibility of attacks by integrating common attack paradigms with specifically designed perception-based losses or the capabilities of generative models. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a novel modeling framework distinct from existing attack paradigms. AdvAD innovatively conceptualizes attacking as a non-parametric diffusion process by theoretically exploring basic modeling approach rather than using the denoising or generation abilities of regular diffusion models requiring neural networks. At each step, much subtler yet effective adversarial guidance is crafted using only the attacked model without any additional network, which gradually leads the end of diffusion process from the original image to a desired imperceptible adversarial example. Grounded in a solid theoretical foundation of the proposed non-parametric diffusion process, AdvAD achieves high attack efficacy and imperceptibility with intrinsically lower overall perturbation strength. Additionally, an enhanced version AdvAD-X is proposed to evaluate the extreme of our novel framework under an ideal scenario. Extensive experiments demonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with state-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9$\%$ (+17.3$\%$) ASR with 1.34 (-0.97) $l_2$ distance, 49.74 (+4.76) PSNR and 0.9971 (+0.0043) SSIM against four prevalent DNNs with three different architectures on the ImageNet-compatible dataset. Code is available at this https URL.</li>
</ul>

<h3>Title: MP-HSIR: A Multi-Prompt Framework for Universal Hyperspectral Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhehui Wu, Yong Chen, Naoto Yokoya, Wei He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09131">https://arxiv.org/abs/2503.09131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09131">https://arxiv.org/pdf/2503.09131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09131]] MP-HSIR: A Multi-Prompt Framework for Universal Hyperspectral Image Restoration(https://arxiv.org/abs/2503.09131)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Hyperspectral images (HSIs) often suffer from diverse and unknown degradations during imaging, leading to severe spectral and spatial distortions. Existing HSI restoration methods typically rely on specific degradation assumptions, limiting their effectiveness in complex scenarios. In this paper, we propose MP-HSIR, a novel multi-prompt framework that effectively integrates spectral, textual, and visual prompts to achieve universal HSI restoration across diverse degradation types and intensities. Specifically, we develop a prompt-guided spatial-spectral transformer, which incorporates spatial self-attention and a prompt-guided dual-branch spectral self-attention. Since degradations affect spectral features differently, we introduce spectral prompts in the local spectral branch to provide universal low-rank spectral patterns as prior knowledge for enhancing spectral reconstruction. Furthermore, the text-visual synergistic prompt fuses high-level semantic representations with fine-grained visual features to encode degradation information, thereby guiding the restoration process. Extensive experiments on 9 HSI restoration tasks, including all-in-one scenarios, generalization tests, and real-world cases, demonstrate that MP-HSIR not only consistently outperforms existing all-in-one methods but also surpasses state-of-the-art task-specific approaches across multiple tasks. The code and models will be released at this https URL.</li>
</ul>

<h3>Title: Investigation of Frame Differences as Motion Cues for Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sota Kawamura, Hirotada Honda, Shugo Nakamura, Takashi Sano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09132">https://arxiv.org/abs/2503.09132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09132">https://arxiv.org/pdf/2503.09132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09132]] Investigation of Frame Differences as Motion Cues for Video Object Segmentation(https://arxiv.org/abs/2503.09132)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Automatic Video Object Segmentation (AVOS) refers to the task of autonomously segmenting target objects in video sequences without relying on human-provided annotations in the first frames. In AVOS, the use of motion information is crucial, with optical flow being a commonly employed method for capturing motion cues. However, the computation of optical flow is resource-intensive, making it unsuitable for real-time applications, especially on edge devices with limited computational resources. In this study, we propose using frame differences as an alternative to optical flow for motion cue extraction. We developed an extended U-Net-like AVOS model that takes a frame on which segmentation is performed and a frame difference as inputs, and outputs an estimated segmentation map. Our experimental results demonstrate that the proposed model achieves performance comparable to the model with optical flow as an input, particularly when applied to videos captured by stationary cameras. Our results suggest the usefulness of employing frame differences as motion cues in cases with limited computational resources.</li>
</ul>

<h3>Title: Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Zhang, Qiaohui Chu, Meng Liu, Yunxiao Wang, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Yaowei Wang, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09143">https://arxiv.org/abs/2503.09143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09143">https://arxiv.org/pdf/2503.09143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09143]] Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding(https://arxiv.org/abs/2503.09143)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. Current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique aspects of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D. Our approach features a progressive training pipeline with three stages: Teacher Self-Preparation, Teacher-Student Guidance, and Student Self-Practice. Additionally, we propose an instruction-tuning data EgoIT from multiple sources to strengthen the model's instruction-following capabilities, along with the EgoBench benchmark comprising eight different tasks for thorough evaluation. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models.</li>
</ul>

<h3>Title: Efficient UAV Swarm-Based Multi-Task Federated Learning with Dynamic Task Knowledge Sharing</h3>
<ul>
<li><strong>Authors: </strong>Yubo Yang, Tao Yang, Xiaofeng Wu, Ziyu Guo, Bo Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09144">https://arxiv.org/abs/2503.09144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09144">https://arxiv.org/pdf/2503.09144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09144]] Efficient UAV Swarm-Based Multi-Task Federated Learning with Dynamic Task Knowledge Sharing(https://arxiv.org/abs/2503.09144)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>UAV swarms are widely used in emergency communications, area monitoring, and disaster relief. Coordinated by control centers, they are ideal for federated learning (FL) frameworks. However, current UAV-assisted FL methods primarily focus on single tasks, overlooking the need for multi-task training. In disaster relief scenarios, UAVs perform tasks such as crowd detection, road feasibility analysis, and disaster assessment, which exhibit time-varying demands and potential correlations. In order to meet the time-varying requirements of tasks and complete multiple tasks efficiently under resource constraints, in this paper, we propose a UAV swarm based multi-task FL framework, where ground emergency vehicles (EVs) collaborate with UAVs to accomplish multiple tasks efficiently under constrained energy and bandwidth resources. Through theoretical analysis, we identify key factors affecting task performance and introduce a task attention mechanism to dynamically evaluate task importance, thereby achieving efficient resource allocation. Additionally, we propose a task affinity (TA) metric to capture the dynamic correlation among tasks, thereby promoting task knowledge sharing to accelerate training and improve the generalization ability of the model in different scenarios. To optimize resource allocation, we formulate a two-layer optimization problem to jointly optimize UAV transmission power, computation frequency, bandwidth allocation, and UAV-EV associations. For the inner problem, we derive closed-form solutions for transmission power, computation frequency, and bandwidth allocation and apply a block coordinate descent method for optimization. For the outer problem, a two-stage algorithm is designed to determine optimal UAV-EV associations. Furthermore, theoretical analysis reveals a trade-off between UAV energy consumption and multi-task performance.</li>
</ul>

<h3>Title: Generative Frame Sampler for Long Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Linli Yao, Haoning Wu, Kun Ouyang, Yuanxing Zhang, Caiming Xiong, Bei Chen, Xu Sun, Junnan Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09146">https://arxiv.org/abs/2503.09146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09146">https://arxiv.org/pdf/2503.09146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09146]] Generative Frame Sampler for Long Video Understanding(https://arxiv.org/abs/2503.09146)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite recent advances in Video Large Language Models (VideoLLMs), effectively understanding long-form videos remains a significant challenge. Perceiving lengthy videos containing thousands of frames poses substantial computational burden. To mitigate this issue, this paper introduces Generative Frame Sampler (GenS), a plug-and-play module integrated with VideoLLMs to facilitate efficient lengthy video perception. Built upon a lightweight VideoLLM, GenS leverages its inherent vision-language capabilities to identify question-relevant frames. To facilitate effective retrieval, we construct GenS-Video-150K, a large-scale video instruction dataset with dense frame relevance annotations. Extensive experiments demonstrate that GenS consistently boosts the performance of various VideoLLMs, including open-source models (Qwen2-VL-7B, Aria-25B, VILA-40B, LLaVA-Video-7B/72B) and proprietary assistants (GPT-4o, Gemini). When equipped with GenS, open-source VideoLLMs achieve impressive state-of-the-art results on long-form video benchmarks: LLaVA-Video-72B reaches 66.8 (+4.3) on LongVideoBench and 77.0 (+2.7) on MLVU, while Aria obtains 39.2 on HourVideo surpassing the Gemini-1.5-pro by 1.9 points. We will release all datasets and models at this https URL.</li>
</ul>

<h3>Title: Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</h3>
<ul>
<li><strong>Authors: </strong>Hyeonho Jeong, Suhyeon Lee, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09151">https://arxiv.org/abs/2503.09151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09151">https://arxiv.org/pdf/2503.09151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09151]] Reangle-A-Video: 4D Video Generation as Video-to-Video Translation(https://arxiv.org/abs/2503.09151)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: this https URL</li>
</ul>

<h3>Title: FaVChat: Unlocking Fine-Grained Facail Video Understanding with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fufangchen Zhao, Ming Li, Linrui Xu, Wenhao Jiang, Jian Gao, Danfeng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09158">https://arxiv.org/abs/2503.09158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09158">https://arxiv.org/pdf/2503.09158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09158]] FaVChat: Unlocking Fine-Grained Facail Video Understanding with Multimodal Large Language Models(https://arxiv.org/abs/2503.09158)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video-based multimodal large language models (VMLLMs) have demonstrated remarkable potential in cross-modal video understanding. However, their abilities in fine-grained face comprehension remain largely underexplored. Given its pivotal role in human-centric intelligence, developing VMLLMs for facial understanding holds a fundamental problem. To address this gap, we propose FaVChat, the first VMLLM specifically designed for fine-grained facial video understanding. To facilitate its training, we construct a large-scale facial video dataset comprising over 60k videos, with the majority annotated with 83 fine-grained facial attributes. These attributes are incorporated to enrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs and an additional 170k fine-grained question-answering (QA) pairs. To effectively capture rich facial clues, we propose a hybrid model architecture composed of a general visual encoder, a dedicated facial encoder, and a mixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual features. To mitigate information loss during feature transformation, we extract multi-granularity representations from the facial encoder and integrate them into the subsequent LLM. This design enhances the model's ability to comprehend and respond to questions involving diverse levels of visual details. We employ a progressive training paradigm, transitioning from video summarization to a high-quality subset of video QA, gradually increasing task complexity to enhance the model's fine-grained visual perception. We conduct extensive zero-shot evaluation on a couple of public benchmarks, demonstrating that FaVChat consistently surpasses existing VMLLMs across multiple tasks.</li>
</ul>

<h3>Title: WonderVerse: Extendable 3D Scene Generation with Video Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Feng, Zhi Zuo, Jia-hui Pan, Ka-hei Hui, Yi-hua Shao, Qi Dou, Wei Xie, Zheng-zhe Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09160">https://arxiv.org/abs/2503.09160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09160">https://arxiv.org/pdf/2503.09160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09160]] WonderVerse: Extendable 3D Scene Generation with Video Generative Models(https://arxiv.org/abs/2503.09160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce \textit{WonderVerse}, a simple but effective framework for generating extendable 3D scenes. Unlike existing methods that rely on iterative depth estimation and image inpainting, often leading to geometric distortions and inconsistencies, WonderVerse leverages the powerful world-level priors embedded within video generative foundation models to create highly immersive and geometrically coherent 3D environments. Furthermore, we propose a new technique for controllable 3D scene extension to substantially increase the scale of the generated environments. Besides, we introduce a novel abnormal sequence detection module that utilizes camera trajectory to address geometric inconsistency in the generated videos. Finally, WonderVerse is compatible with various 3D reconstruction methods, allowing both efficient and high-quality generation. Extensive experiments on 3D scene generation demonstrate that our WonderVerse, with an elegant and simple pipeline, delivers extendable and highly-realistic 3D scenes, markedly outperforming existing works that rely on more complex architectures.</li>
</ul>

<h3>Title: Effective Feature Selection for Predicting Spreading Factor with ML in Large LoRaWAN-based Mobile IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Aman Prakash, Nikumani Choudhury, Anakhi Hazarika, Alekhya Gorrela</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09170">https://arxiv.org/abs/2503.09170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09170">https://arxiv.org/pdf/2503.09170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09170]] Effective Feature Selection for Predicting Spreading Factor with ML in Large LoRaWAN-based Mobile IoT Networks(https://arxiv.org/abs/2503.09170)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LoRaWAN is a low-power long-range protocol that enables reliable and robust communication. This paper addresses the challenge of predicting the spreading factor (SF) in LoRaWAN networks using machine learning (ML) techniques. Optimal SF allocation is crucial for optimizing data transmission in IoT-enabled mobile devices, yet it remains a challenging task due to the fluctuation in environment and network conditions. We evaluated ML model performance across a large publicly available dataset to explore the best feature across key LoRaWAN features such as RSSI, SNR, frequency, distance between end devices and gateways, and antenna height of the end device, further, we also experimented with 31 different combinations possible for 5 features. We trained and evaluated the model using k-nearest neighbors (k-NN), Decision Tree Classifier (DTC), Random Forest (RF), and Multinomial Logistic Regression (MLR) algorithms. The combination of RSSI and SNR was identified as the best feature set. The finding of this paper provides valuable information for reducing the overall cost of dataset collection for ML model training and extending the battery life of LoRaWAN devices. This work contributes to a more reliable LoRaWAN system by understanding the importance of specific feature sets for optimized SF allocation.</li>
</ul>

<h3>Title: Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs</h3>
<ul>
<li><strong>Authors: </strong>Aidan Ferguson, Perry Gibson, Lara D'Agata, Parker McLeod, Ferhat Yaman, Amitabh Das, Ian Colbert, José Cano</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09184">https://arxiv.org/abs/2503.09184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09184">https://arxiv.org/pdf/2503.09184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09184]] Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs(https://arxiv.org/abs/2503.09184)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The deployment of deep neural networks (DNNs) in privacy-sensitive environments is constrained by computational overheads in fully homomorphic encryption (FHE). This paper explores unstructured sparsity in FHE matrix multiplication schemes as a means of reducing this burden while maintaining model accuracy requirements. We demonstrate that sparsity can be exploited in arbitrary matrix multiplication, providing runtime benefits compared to a baseline naive algorithm at all sparsity levels. This is a notable departure from the plaintext domain, where there is a trade-off between sparsity and the overhead of the sparse multiplication algorithm. In addition, we propose three sparse multiplication schemes in FHE based on common plaintext sparse encodings. We demonstrate the performance gain is scheme-invariant; however, some sparse schemes vastly reduce the memory storage requirements of the encrypted matrix at high sparsity values. Our proposed sparse schemes yield an average performance gain of 2.5x at 50% unstructured sparsity, with our multi-threading scheme providing a 32.5x performance increase over the equivalent single-threaded sparse computation when utilizing 64 cores.</li>
</ul>

<h3>Title: Incomplete Multi-view Clustering via Diffusion Contrastive Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuanyang Zhang, Yijie Lin, Weiqing Yan, Li Yao, Xinhang Wan, Guangyuan Li, Chao Zhang, Guanzhou Ke, Jie Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09185">https://arxiv.org/abs/2503.09185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09185">https://arxiv.org/pdf/2503.09185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09185]] Incomplete Multi-view Clustering via Diffusion Contrastive Generation(https://arxiv.org/abs/2503.09185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Incomplete multi-view clustering (IMVC) has garnered increasing attention in recent years due to the common issue of missing data in multi-view datasets. The primary approach to address this challenge involves recovering the missing views before applying conventional multi-view clustering methods. Although imputation-based IMVC methods have achieved significant improvements, they still encounter notable limitations: 1) heavy reliance on paired data for training the data recovery module, which is impractical in real scenarios with high missing data rates; 2) the generated data often lacks diversity and discriminability, resulting in suboptimal clustering results. To address these shortcomings, we propose a novel IMVC method called Diffusion Contrastive Generation (DCG). Motivated by the consistency between the diffusion and clustering processes, DCG learns the distribution characteristics to enhance clustering by applying forward diffusion and reverse denoising processes to intra-view data. By performing contrastive learning on a limited set of paired multi-view samples, DCG can align the generated views with the real views, facilitating accurate recovery of views across arbitrary missing view scenarios. Additionally, DCG integrates instance-level and category-level interactive learning to exploit the consistent and complementary information available in multi-view data, achieving robust and end-to-end clustering. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches.</li>
</ul>

<h3>Title: Polygonizing Roof Segments from High-Resolution Aerial Images Using Yolov8-Based Edge Detection</h3>
<ul>
<li><strong>Authors: </strong>Qipeng Mei, Dimitri Bulatov, Dorota Iwaszczuk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09187">https://arxiv.org/abs/2503.09187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09187">https://arxiv.org/pdf/2503.09187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09187]] Polygonizing Roof Segments from High-Resolution Aerial Images Using Yolov8-Based Edge Detection(https://arxiv.org/abs/2503.09187)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>This study presents a novel approach for roof detail extraction and vectorization using remote sensing images. Unlike previous geometric-primitive-based methods that rely on the detection of corners, our method focuses on edge detection as the primary mechanism for roof reconstruction, while utilizing geometric relationships to define corners and faces. We adapt the YOLOv8 OBB model, originally designed for rotated object detection, to extract roof edges effectively. Our method demonstrates robustness against noise and occlusion, leading to precise vectorized representations of building roofs. Experiments conducted on the SGA and Melville datasets highlight the method's effectiveness. At the raster level, our model outperforms the state-of-the-art foundation segmentation model (SAM), achieving a mIoU between 0.85 and 1 for most samples and an ovIoU close to 0.97. At the vector level, evaluation using the Hausdorff distance, PolyS metric, and our raster-vector-metric demonstrates significant improvements after polygonization, with a close approximation to the reference data. The method successfully handles diverse roof structures and refines edge gaps, even on complex roof structures of new, excluded from training datasets. Our findings underscore the potential of this approach to address challenges in automatic roof structure vectorization, supporting various applications such as urban terrain reconstruction.</li>
</ul>

<h3>Title: Learning Appearance and Motion Cues for Panoptic Tracking</h3>
<ul>
<li><strong>Authors: </strong>Juana Valeria Hurtado, Sajad Marvi, Rohit Mohan, Abhinav Valada</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09191">https://arxiv.org/abs/2503.09191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09191">https://arxiv.org/pdf/2503.09191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09191]] Learning Appearance and Motion Cues for Panoptic Tracking(https://arxiv.org/abs/2503.09191)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Panoptic tracking enables pixel-level scene interpretation of videos by integrating instance tracking in panoptic segmentation. This provides robots with a spatio-temporal understanding of the environment, an essential attribute for their operation in dynamic environments. In this paper, we propose a novel approach for panoptic tracking that simultaneously captures general semantic information and instance-specific appearance and motion features. Unlike existing methods that overlook dynamic scene attributes, our approach leverages both appearance and motion cues through dedicated network heads. These interconnected heads employ multi-scale deformable convolutions that reason about scene motion offsets with semantic context and motion-enhanced appearance features to learn tracking embeddings. Furthermore, we introduce a novel two-step fusion module that integrates the outputs from both heads by first matching instances from the current time step with propagated instances from previous time steps and subsequently refines associations using motion-enhanced appearance embeddings, improving robustness in challenging scenarios. Extensive evaluations of our proposed \netname model on two benchmark datasets demonstrate that it achieves state-of-the-art performance in panoptic tracking accuracy, surpassing prior methods in maintaining object identities over time. To facilitate future research, we make the code available at this http URL</li>
</ul>

<h3>Title: Differential Privacy Personalized Federated Learning Based on Dynamically Sparsified Client Updates</h3>
<ul>
<li><strong>Authors: </strong>Chuanyin Wang, Yifei Zhang, Neng Gao, Qiang Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09192">https://arxiv.org/abs/2503.09192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09192">https://arxiv.org/pdf/2503.09192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09192]] Differential Privacy Personalized Federated Learning Based on Dynamically Sparsified Client Updates(https://arxiv.org/abs/2503.09192)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Personalized federated learning is extensively utilized in scenarios characterized by data heterogeneity, facilitating more efficient and automated local training on data-owning terminals. This includes the automated selection of high-performance model parameters for upload, thereby enhancing the overall training process. However, it entails significant risks of privacy leakage. Existing studies have attempted to mitigate these risks by utilizing differential privacy. Nevertheless, these studies present two major limitations: (1) The integration of differential privacy into personalized federated learning lacks sufficient personalization, leading to the introduction of excessive noise into the model. (2) It fails to adequately control the spatial scope of model update information, resulting in a suboptimal balance between data privacy and model effectiveness in differential privacy federated learning. In this paper, we propose a differentially private personalized federated learning approach that employs dynamically sparsified client updates through reparameterization and adaptive norm(DP-pFedDSU). Reparameterization training effectively selects personalized client update information, thereby reducing the quantity of updates. This approach minimizes the introduction of noise to the greatest extent possible. Additionally, dynamic adaptive norm refers to controlling the norm space of model updates during the training process, mitigating the negative impact of clipping on the update information. These strategies substantially enhance the effective integration of differential privacy and personalized federated learning. Experimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our proposed scheme achieves superior performance and is well-suited for more complex personalized federated learning scenarios.</li>
</ul>

<h3>Title: GENEOnet: Statistical analysis supporting explainability and trustworthiness</h3>
<ul>
<li><strong>Authors: </strong>Giovanni Bocchi, Patrizio Frosini, Alessandra Micheletti, Alessandro Pedretti, Carmen Gratteri, Filippo Lunghini, Andrea Rosario Beccari, Carmine Talarico</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09199">https://arxiv.org/abs/2503.09199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09199">https://arxiv.org/pdf/2503.09199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09199]] GENEOnet: Statistical analysis supporting explainability and trustworthiness(https://arxiv.org/abs/2503.09199)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Group Equivariant Non-Expansive Operators (GENEOs) have emerged as mathematical tools for constructing networks for Machine Learning and Artificial Intelligence. Recent findings suggest that such models can be inserted within the domain of eXplainable Artificial Intelligence (XAI) due to their inherent interpretability. In this study, we aim to verify this claim with respect to GENEOnet, a GENEO network developed for an application in computational biochemistry by employing various statistical analyses and experiments. Such experiments first allow us to perform a sensitivity analysis on GENEOnet's parameters to test their significance. Subsequently, we show that GENEOnet exhibits a significantly higher proportion of equivariance compared to other methods. Lastly, we demonstrate that GENEOnet is on average robust to perturbations arising from molecular dynamics. These results collectively serve as proof of the explainability, trustworthiness, and robustness of GENEOnet and confirm the beneficial use of GENEOs in the context of Trustworthy Artificial Intelligence.</li>
</ul>

<h3>Title: Time-EAPCR: A Deep Learning-Based Novel Approach for Anomaly Detection Applied to the Environmental Field</h3>
<ul>
<li><strong>Authors: </strong>Lei Liu, Yuchao Lu, Ling An, Huajie Liang, Chichun Zhou, Zhenyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09200">https://arxiv.org/abs/2503.09200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09200">https://arxiv.org/pdf/2503.09200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09200]] Time-EAPCR: A Deep Learning-Based Novel Approach for Anomaly Detection Applied to the Environmental Field(https://arxiv.org/abs/2503.09200)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As human activities intensify, environmental systems such as aquatic ecosystems and water treatment systems face increasingly complex pressures, impacting ecological balance, public health, and sustainable development, making intelligent anomaly monitoring essential. However, traditional monitoring methods suffer from delayed responses, insufficient data processing capabilities, and weak generalisation, making them unsuitable for complex environmental monitoring this http URL recent years, machine learning has been widely applied to anomaly detection, but the multi-dimensional features and spatiotemporal dynamics of environmental ecological data, especially the long-term dependencies and strong variability in the time dimension, limit the effectiveness of traditional this http URL learning, with its ability to automatically learn features, captures complex nonlinear relationships, improving detection performance. However, its application in environmental monitoring is still in its early stages and requires further this http URL paper introduces a new deep learning method, Time-EAPCR (Time-Embedding-Attention-Permutated CNN-Residual), and applies it to environmental science. The method uncovers feature correlations, captures temporal evolution patterns, and enables precise anomaly detection in environmental this http URL validated Time-EAPCR's high accuracy and robustness across four publicly available environmental datasets. Experimental results show that the method efficiently handles multi-source data, improves detection accuracy, and excels across various scenarios with strong adaptability and generalisation. Additionally, a real-world river monitoring dataset confirmed the feasibility of its deployment, providing reliable technical support for environmental monitoring.</li>
</ul>

<h3>Title: Token Weighting for Long-Range Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Falko Helm, Nico Daheim, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09202">https://arxiv.org/abs/2503.09202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09202">https://arxiv.org/pdf/2503.09202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09202]] Token Weighting for Long-Range Language Modeling(https://arxiv.org/abs/2503.09202)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many applications of large language models (LLMs) require long-context understanding, but models continue to struggle with such tasks. We hypothesize that conventional next-token prediction training could contribute to this, because each token is assigned equal weight. Yet, intuitively, the amount of context needed to predict the next token accurately varies greatly across different data. To reflect this, we propose various novel token-weighting schemes that assign different weights to each training token in the loss, thereby generalizing existing works. For this, we categorize token-weighting methods using a two-step framework which compares the confidences of a long-context and short-context model to score tokens. We evaluate all methods on multiple long-context understanding tasks and show that non-uniform loss weights are helpful to improve the long-context abilities of LLMs. Different short-context models can be used effectively for token scoring, including models that are much smaller than the long-context model that is trained. All in all, this work contributes to a better understanding of the trade-offs long-context language modeling faces and provides guidelines for model steering via loss-weighting based on empirical evidence. The code can be found on Github.</li>
</ul>

<h3>Title: Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients</h3>
<ul>
<li><strong>Authors: </strong>Xiuwen Fang, Mang Ye, Bo Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09206">https://arxiv.org/abs/2503.09206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09206">https://arxiv.org/pdf/2503.09206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09206]] Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients(https://arxiv.org/abs/2503.09206)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>This paper studies a challenging robust federated learning task with model heterogeneous and data corrupted clients, where the clients have different local model structures. Data corruption is unavoidable due to factors such as random noise, compression artifacts, or environmental conditions in real-world deployment, drastically crippling the entire federated system. To address these issues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated Learning (RAHFL) framework. We propose a Diversity-enhanced supervised Contrastive Learning technique to enhance the resilience and adaptability of local models on various data corruption patterns. Its basic idea is to utilize complex augmented samples obtained by the mixed-data augmentation strategy for supervised contrastive learning, thereby enhancing the ability of the model to learn robust and diverse feature representations. Furthermore, we design an Asymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback from external clients. The strategy allows clients to perform selective one-way learning during collaborative learning phase, enabling clients to refrain from incorporating lower-quality information from less robust or underperforming collaborators. Extensive experimental results demonstrate the effectiveness and robustness of our approach in diverse, challenging federated learning environments. Our code and models are public available at this https URL.</li>
</ul>

<h3>Title: Why LLMs Cannot Think and How to Fix It</h3>
<ul>
<li><strong>Authors: </strong>Marius Jahrens, Thomas Martinetz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09211">https://arxiv.org/abs/2503.09211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09211">https://arxiv.org/pdf/2503.09211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09211]] Why LLMs Cannot Think and How to Fix It(https://arxiv.org/abs/2503.09211)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper elucidates that current state-of-the-art Large Language Models (LLMs) are fundamentally incapable of making decisions or developing "thoughts" within the feature space due to their architectural constraints. We establish a definition of "thought" that encompasses traditional understandings of that term and adapt it for application to LLMs. We demonstrate that the architectural design and language modeling training methodology of contemporary LLMs inherently preclude them from engaging in genuine thought processes. Our primary focus is on this theoretical realization rather than practical insights derived from experimental data. Finally, we propose solutions to enable thought processes within the feature space and discuss the broader implications of these architectural modifications.</li>
</ul>

<h3>Title: Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latant Space</h3>
<ul>
<li><strong>Authors: </strong>Jian Zhu, Zhengyu Jia, Tian Gao, Jiaxin Deng, Shidi Li, Fu Liu, Peng Jia, Xianpeng Lang, Xiaolong Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09215">https://arxiv.org/abs/2503.09215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09215">https://arxiv.org/pdf/2503.09215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09215]] Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latant Space(https://arxiv.org/abs/2503.09215)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Advanced end-to-end autonomous driving systems predict other vehicles' motions and plan ego vehicle's trajectory. The world model that can foresee the outcome of the trajectory has been used to evaluate the end-to-end autonomous driving system. However, existing world models predominantly emphasize the trajectory of the ego vehicle and leave other vehicles uncontrollable. This limitation hinders their ability to realistically simulate the interaction between the ego vehicle and the driving scenario. In addition, it remains a challenge to match multiple trajectories with each vehicle in the video to control the video generation. To address above issues, a driving \textbf{W}orld \textbf{M}odel named EOT-WM is proposed in this paper, unifying \textbf{E}go-\textbf{O}ther vehicle \textbf{T}rajectories in videos. Specifically, we first project ego and other vehicle trajectories in the BEV space into the image coordinate to match each trajectory with its corresponding vehicle in the video. Then, trajectory videos are encoded by the Spatial-Temporal Variational Auto Encoder to align with driving video latents spatially and temporally in the unified visual space. A trajectory-injected diffusion Transformer is further designed to denoise the noisy video latents for video generation with the guidance of ego-other vehicle trajectories. In addition, we propose a metric based on control latent similarity to evaluate the controllability of trajectories. Extensive experiments are conducted on the nuScenes dataset, and the proposed model outperforms the state-of-the-art method by 30\% in FID and 55\% in FVD. The model can also predict unseen driving scenes with self-produced trajectories.</li>
</ul>

<h3>Title: Rethinking Prompt-based Debiasing in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Yang, Runzhe Zhan, Derek F. Wong, Shu Yang, Junchao Wu, Lidia S. Chao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09219">https://arxiv.org/abs/2503.09219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09219">https://arxiv.org/pdf/2503.09219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09219]] Rethinking Prompt-based Debiasing in Large Language Models(https://arxiv.org/abs/2503.09219)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Investigating bias in large language models (LLMs) is crucial for developing trustworthy AI. While prompt-based through prompt engineering is common, its effectiveness relies on the assumption that models inherently understand biases. Our study systematically analyzed this assumption using the BBQ and StereoSet benchmarks on both open-source models as well as commercial GPT model. Experimental results indicate that prompt-based is often superficial; for instance, the Llama2-7B-Chat model misclassified over 90% of unbiased content as biased, despite achieving high accuracy in identifying bias issues on the BBQ dataset. Additionally, specific evaluation and question settings in bias benchmarks often lead LLMs to choose "evasive answers", disregarding the core of the question and the relevance of the response to the context. Moreover, the apparent success of previous methods may stem from flawed evaluation metrics. Our research highlights a potential "false prosperity" in prompt-base efforts and emphasizes the need to rethink bias metrics to ensure truly trustworthy AI.</li>
</ul>

<h3>Title: Active Learning Inspired ControlNet Guidance for Augmenting Semantic Segmentation Datasets</h3>
<ul>
<li><strong>Authors: </strong>Hannah Kniesel, Pedro Hermosilla, Timo Ropinski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09221">https://arxiv.org/abs/2503.09221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09221">https://arxiv.org/pdf/2503.09221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09221]] Active Learning Inspired ControlNet Guidance for Augmenting Semantic Segmentation Datasets(https://arxiv.org/abs/2503.09221)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in conditional image generation from diffusion models have shown great potential in achieving impressive image quality while preserving the constraints introduced by the user. In particular, ControlNet enables precise alignment between ground truth segmentation masks and the generated image content, allowing the enhancement of training datasets in segmentation tasks. This raises a key question: Can ControlNet additionally be guided to generate the most informative synthetic samples for a specific task? Inspired by active learning, where the most informative real-world samples are selected based on sample difficulty or model uncertainty, we propose the first approach to integrate active learning-based selection metrics into the backward diffusion process for sample generation. Specifically, we explore uncertainty, query by committee, and expected model change, which are commonly used in active learning, and demonstrate their application for guiding the sample generation process through gradient approximation. Our method is training-free, modifying only the backward diffusion process, allowing it to be used on any pretrained ControlNet. Using this process, we show that segmentation models trained with guided synthetic data outperform those trained on non-guided synthetic data. Our work underscores the need for advanced control mechanisms for diffusion-based models, which are not only aligned with image content but additionally downstream task performance, highlighting the true potential of synthetic data generation.</li>
</ul>

<h3>Title: NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Ma, Bo Cheng, Shanyuan Liu, Ao Ma, Xiaoyu Wu, Liebucha Wu, Dawei Leng, Yuhui Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09242">https://arxiv.org/abs/2503.09242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09242">https://arxiv.org/pdf/2503.09242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09242]] NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers(https://arxiv.org/abs/2503.09242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Flow-based transformer models for image generation have achieved state-of-the-art performance with larger model parameters, but their inference deployment cost remains high. To enhance inference performance while maintaining generation quality, we propose progressive rectified flow transformers. We divide the rectified flow into different stages according to resolution, using fewer transformer layers at the low-resolution stages to generate image layouts and concept contours, and progressively adding more layers as the resolution increases. Experiments demonstrate that our approach achieves fast convergence and reduces inference time while ensuring generation quality. The main contributions of this paper are summarized as follows: (1) We introduce progressive rectified flow transformers that enable multi-resolution training, accelerating model convergence; (2) NAMI leverages piecewise flow and spatial cascading of Diffusion Transformer (DiT) to rapidly generate images, reducing inference time by 40% to generate a 1024 resolution image; (3) We propose NAMI-1K benchmark to evaluate human preference performance, aiming to mitigate distributional bias and prevent data leakage from open-source benchmarks. The results show that our model is competitive with state-of-the-art models.</li>
</ul>

<h3>Title: How To Make Your Cell Tracker Say "I dunno!"</h3>
<ul>
<li><strong>Authors: </strong>Richard D. Paul, Johannes Seiffarth, David Rügamer, Hanno Scharr, Katharina Nöh</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09244">https://arxiv.org/abs/2503.09244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09244">https://arxiv.org/pdf/2503.09244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09244]] How To Make Your Cell Tracker Say "I dunno!"(https://arxiv.org/abs/2503.09244)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cell tracking is a key computational task in live-cell microscopy, but fully automated analysis of high-throughput imaging requires reliable and, thus, uncertainty-aware data analysis tools, as the amount of data recorded within a single experiment exceeds what humans are able to overlook. We here propose and benchmark various methods to reason about and quantify uncertainty in linear assignment-based cell tracking algorithms. Our methods take inspiration from statistics and machine learning, leveraging two perspectives on the cell tracking problem explored throughout this work: Considering it as a Bayesian inference problem and as a classification problem. Our methods admit a framework-like character in that they equip any frame-to-frame tracking method with uncertainty quantification. We demonstrate this by applying it to various existing tracking algorithms including the recently presented Transformer-based trackers. We demonstrate empirically that our methods yield useful and well-calibrated tracking uncertainties.</li>
</ul>

<h3>Title: UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Wang, Jinlong Peng, Qingdong He, Hao Yang, Ying Jin, Jiafu Wu, Xiaobin Hu, Yanjie Pan, Zhenye Gan, Mingmin Chi, Bo Peng, Yabiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09277">https://arxiv.org/abs/2503.09277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09277">https://arxiv.org/pdf/2503.09277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09277]] UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer(https://arxiv.org/abs/2503.09277)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>With the rapid development of diffusion models in image generation, the demand for more powerful and flexible controllable frameworks is increasing. Although existing methods can guide generation beyond text prompts, the challenge of effectively combining multiple conditional inputs while maintaining consistency with all of them remains unsolved. To address this, we introduce UniCombine, a DiT-based multi-conditional controllable generative framework capable of handling any combination of conditions, including but not limited to text prompts, spatial maps, and subject images. Specifically, we introduce a novel Conditional MMDiT Attention mechanism and incorporate a trainable LoRA module to build both the training-free and training-based versions. Additionally, we propose a new pipeline to construct SubjectSpatial200K, the first dataset designed for multi-conditional generative tasks covering both the subject-driven and spatially-aligned conditions. Extensive experimental results on multi-conditional generation demonstrate the outstanding universality and powerful capability of our approach with state-of-the-art performance.</li>
</ul>

<h3>Title: Unmask It! AI-Generated Product Review Detection in Dravidian Languages</h3>
<ul>
<li><strong>Authors: </strong>Somsubhra De, Advait Vats</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09289">https://arxiv.org/abs/2503.09289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09289">https://arxiv.org/pdf/2503.09289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09289]] Unmask It! AI-Generated Product Review Detection in Dravidian Languages(https://arxiv.org/abs/2503.09289)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>The rise of Generative AI has led to a surge in AI-generated reviews, often posing a serious threat to the credibility of online platforms. Reviews serve as the primary source of information about products and services. Authentic reviews play a vital role in consumer decision-making. The presence of fabricated content misleads consumers, undermines trust and facilitates potential fraud in digital marketplaces. This study focuses on detecting AI-generated product reviews in Tamil and Malayalam, two low-resource languages where research in this domain is relatively under-explored. We worked on a range of approaches - from traditional machine learning methods to advanced transformer-based models such as Indic-BERT, IndicSBERT, MuRIL, XLM-RoBERTa and MalayalamBERT. Our findings highlight the effectiveness of leveraging the state-of-the-art transformers in accurately identifying AI-generated content, demonstrating the potential in enhancing the detection of fake reviews in low-resource language settings.</li>
</ul>

<h3>Title: Prompt Inference Attack on Distributed Large Language Model Inference Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Xinjian Luo, Ting Yu, Xiaokui Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09291">https://arxiv.org/abs/2503.09291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09291">https://arxiv.org/pdf/2503.09291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09291]] Prompt Inference Attack on Distributed Large Language Model Inference Frameworks(https://arxiv.org/abs/2503.09291)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, large language model</a></li>
<li><strong>Abstract: </strong>The inference process of modern large language models (LLMs) demands prohibitive computational resources, rendering them infeasible for deployment on consumer-grade devices. To address this limitation, recent studies propose distributed LLM inference frameworks, which employ split learning principles to enable collaborative LLM inference on resource-constrained hardware. However, distributing LLM layers across participants requires the transmission of intermediate outputs, which may introduce privacy risks to the original input prompts - a critical issue that has yet to be thoroughly explored in the literature. In this paper, we rigorously examine the privacy vulnerabilities of distributed LLM inference frameworks by designing and evaluating three prompt inference attacks aimed at reconstructing input prompts from intermediate LLM outputs. These attacks are developed under various query and data constraints to reflect diverse real-world LLM service scenarios. Specifically, the first attack assumes an unlimited query budget and access to an auxiliary dataset sharing the same distribution as the target prompts. The second attack also leverages unlimited queries but uses an auxiliary dataset with a distribution differing from the target prompts. The third attack operates under the most restrictive scenario, with limited query budgets and no auxiliary dataset available. We evaluate these attacks on a range of LLMs, including state-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used models like GPT-2 and BERT for comparative analysis. Our experiments show that the first two attacks achieve reconstruction accuracies exceeding 90%, while the third achieves accuracies typically above 50%, even under stringent constraints. These findings highlight privacy risks in distributed LLM inference frameworks, issuing a strong alert on their deployment in real-world applications.</li>
</ul>

<h3>Title: IQPFR: An Image Quality Prior for Blind Face Restoration and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Peng Hu, Chunming He, Lei Xu, Jingduo Tian, Sina Farsiu, Yulun Zhang, Pei Liu, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09294">https://arxiv.org/abs/2503.09294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09294">https://arxiv.org/pdf/2503.09294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09294]] IQPFR: An Image Quality Prior for Blind Face Restoration and Beyond(https://arxiv.org/abs/2503.09294)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Blind Face Restoration (BFR) addresses the challenge of reconstructing degraded low-quality (LQ) facial images into high-quality (HQ) outputs. Conventional approaches predominantly rely on learning feature representations from ground-truth (GT) data; however, inherent imperfections in GT datasets constrain restoration performance to the mean quality level of the training data, rather than attaining maximally attainable visual quality. To overcome this limitation, we propose a novel framework that incorporates an Image Quality Prior (IQP) derived from No-Reference Image Quality Assessment (NR-IQA) models to guide the restoration process toward optimal HQ reconstructions. Our methodology synergizes this IQP with a learned codebook prior through two critical innovations: (1) During codebook learning, we devise a dual-branch codebook architecture that disentangles feature extraction into universal structural components and HQ-specific attributes, ensuring comprehensive representation of both common and high-quality facial characteristics. (2) In the codebook lookup stage, we implement a quality-conditioned Transformer-based framework. NR-IQA-derived quality scores act as dynamic conditioning signals to steer restoration toward the highest feasible quality standard. This score-conditioned paradigm enables plug-and-play enhancement of existing BFR architectures without modifying the original structure. We also formulate a discrete representation-based quality optimization strategy that circumvents over-optimization artifacts prevalent in continuous latent space approaches. Extensive experiments demonstrate that our method outperforms state-of-the-art techniques across multiple benchmarks. Besides, our quality-conditioned framework demonstrates consistent performance improvements when integrated with prior BFR models. The code will be released.</li>
</ul>

<h3>Title: Detecting and Preventing Data Poisoning Attacks on AI Models</h3>
<ul>
<li><strong>Authors: </strong>Halima I. Kure, Pradipta Sarkar, Ahmed B. Ndanusa, Augustine O. Nwajana</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09302">https://arxiv.org/abs/2503.09302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09302">https://arxiv.org/pdf/2503.09302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09302]] Detecting and Preventing Data Poisoning Attacks on AI Models(https://arxiv.org/abs/2503.09302)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>This paper investigates the critical issue of data poisoning attacks on AI models, a growing concern in the ever-evolving landscape of artificial intelligence and cybersecurity. As advanced technology systems become increasingly prevalent across various sectors, the need for robust defence mechanisms against adversarial attacks becomes paramount. The study aims to develop and evaluate novel techniques for detecting and preventing data poisoning attacks, focusing on both theoretical frameworks and practical applications. Through a comprehensive literature review, experimental validation using the CIFAR-10 and Insurance Claims datasets, and the development of innovative algorithms, this paper seeks to enhance the resilience of AI models against malicious data manipulation. The study explores various methods, including anomaly detection, robust optimization strategies, and ensemble learning, to identify and mitigate the effects of poisoned data during model training. Experimental results indicate that data poisoning significantly degrades model performance, reducing classification accuracy by up to 27% in image recognition tasks (CIFAR-10) and 22% in fraud detection models (Insurance Claims dataset). The proposed defence mechanisms, including statistical anomaly detection and adversarial training, successfully mitigated poisoning effects, improving model robustness and restoring accuracy levels by an average of 15-20%. The findings further demonstrate that ensemble learning techniques provide an additional layer of resilience, reducing false positives and false negatives caused by adversarial data injections.</li>
</ul>

<h3>Title: Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Siavashi, Faezeh Keshmiri Dindarloo, Dejan Kostic, Marco Chiesa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09304">https://arxiv.org/abs/2503.09304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09304">https://arxiv.org/pdf/2503.09304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09304]] Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference(https://arxiv.org/abs/2503.09304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have revolutionized natural language processing, yet serving them efficiently in data centers remains challenging due to mixed workloads comprising latency-sensitive (LS) and best-effort (BE) jobs. Existing inference systems employ iteration-level first-come-first-served scheduling, causing head-of-line blocking when BE jobs delay LS jobs. We introduce QLLM, a novel inference system designed for Mixture of Experts (MoE) models, featuring a fine-grained, priority-aware preemptive scheduler. QLLM enables expert-level preemption, deferring BE job execution while minimizing LS time-to-first-token (TTFT). Our approach removes iteration-level scheduling constraints, enabling the scheduler to preempt jobs at any layer based on priority. Evaluations on an Nvidia A100 GPU show that QLLM significantly improves performance. It reduces LS TTFT by an average of $65.5\times$ and meets the SLO at up to $7$ requests/sec, whereas the baseline fails to do so under the tested workload. Additionally, it cuts LS turnaround time by up to $12.8\times$ without impacting throughput. QLLM is modular, extensible, and seamlessly integrates with Hugging Face MoE models.</li>
</ul>

<h3>Title: Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions</h3>
<ul>
<li><strong>Authors: </strong>Fynn Bachmann, Daan van der Weijden, Lucien Heitz, Cristina Sarasua, Abraham Bernstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09311">https://arxiv.org/abs/2503.09311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09311">https://arxiv.org/pdf/2503.09311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09311]] Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions(https://arxiv.org/abs/2503.09311)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Adaptive questionnaires dynamically select the next question for a survey participant based on their previous answers. Due to digitalisation, they have become a viable alternative to traditional surveys in application areas such as political science. One limitation, however, is their dependency on data to train the model for question selection. Often, such training data (i.e., user interactions) are unavailable a priori. To address this problem, we (i) test whether Large Language Models (LLM) can accurately generate such interaction data and (ii) explore if these synthetic data can be used to pre-train the statistical model of an adaptive political survey. To evaluate this approach, we utilise existing data from the Swiss Voting Advice Application (VAA) Smartvote in two ways: First, we compare the distribution of LLM-generated synthetic data to the real distribution to assess its similarity. Second, we compare the performance of an adaptive questionnaire that is randomly initialised with one pre-trained on synthetic data to assess their suitability for training. We benchmark these results against an "oracle" questionnaire with perfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately generates answers to the Smartvote questionnaire from the perspective of different Swiss parties. Furthermore, we demonstrate that initialising the statistical model with synthetic data can (i) significantly reduce the error in predicting user responses and (ii) increase the candidate recommendation accuracy of the VAA. Our work emphasises the considerable potential of LLMs to create training data to improve the data collection process in adaptive questionnaires in LLM-affine areas such as political surveys.</li>
</ul>

<h3>Title: xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using Self-Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Elio Musacchio, Lucia Siciliani, Pierpaolo Basile, Giovanni Semeraro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09313">https://arxiv.org/abs/2503.09313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09313">https://arxiv.org/pdf/2503.09313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09313]] xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using Self-Knowledge Distillation(https://arxiv.org/abs/2503.09313)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In the current literature, most embedding models are based on the encoder-only transformer architecture to extract a dense and meaningful representation of the given input, which can be a text, an image, and more. With the recent advances in language modeling thanks to the introduction of Large Language Models, the possibility of extracting embeddings from these large and extensively trained models has been explored. However, current studies focus on textual embeddings in English, which is also the main language on which these models have been trained. Furthermore, there are very few models that consider multimodal and multilingual input. In light of this, we propose an adaptation methodology for Large Vision-Language Models trained on English language data to improve their performance in extracting multilingual and multimodal embeddings. Finally, we design and introduce a benchmark to evaluate the effectiveness of multilingual and multimodal embedding models.</li>
</ul>

<h3>Title: Revealing the Implicit Noise-based Imprint of Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Xinghan Li, Jingjing Chen, Yue Yu, Xue Song, Haijun Shan, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09314">https://arxiv.org/abs/2503.09314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09314">https://arxiv.org/pdf/2503.09314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09314]] Revealing the Implicit Noise-based Imprint of Generative Models(https://arxiv.org/abs/2503.09314)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of vision generation models, the potential security risks stemming from synthetic visual content have garnered increasing attention, posing significant challenges for AI-generated image detection. Existing methods suffer from inadequate generalization capabilities, resulting in unsatisfactory performance on emerging generative models. To address this issue, this paper presents a novel framework that leverages noise-based model-specific imprint for the detection task. Specifically, we propose a novel noise-based imprint simulator to capture intrinsic patterns imprinted in images generated by different models. By aggregating imprints from various generative models, imprints of future models can be extrapolated to expand training data, thereby enhancing generalization and robustness. Furthermore, we design a new pipeline that pioneers the use of noise patterns, derived from a noise-based imprint extractor, alongside other visual features for AI-generated image detection, resulting in a significant improvement in performance. Our approach achieves state-of-the-art performance across three public benchmarks including GenImage, Synthbuster and Chameleon.</li>
</ul>

<h3>Title: RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract Execution Architecture</h3>
<ul>
<li><strong>Authors: </strong>Keyu Zhang, Andrew Martin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09317">https://arxiv.org/abs/2503.09317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09317">https://arxiv.org/pdf/2503.09317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09317]] RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract Execution Architecture(https://arxiv.org/abs/2503.09317)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Decentralized on-chain smart contracts enable trustless collaboration, yet their inherent data transparency and execution overhead hinder widespread adoption. Existing cryptographic approaches incur high computational costs and lack generality. Meanwhile, prior TEE-based solutions suffer from practical limitations, such as the inability to support inter-contract interactions, reliance on unbreakable TEEs, and compromised usability. We introduce RaceTEE, a practical and privacy-preserving off-chain execution architecture for smart contracts that leverages Trusted Execution Environments (TEEs). RaceTEE decouples transaction ordering (on-chain) from execution (off-chain), with computations performed competitively in TEEs, ensuring confidentiality and minimizing overhead. It further enhances practicality through three key improvements: supporting secure inter-contract interactions, providing a key rotation scheme that enforces forward and backward secrecy even in the event of TEE breaches, and enabling full compatibility with existing blockchains without altering the user interaction model. To validate its feasibility, we prototype RaceTEE using Intel SGX and Ethereum, demonstrating its applicability across various use cases and evaluating its performance.</li>
</ul>

<h3>Title: 2HandedAfforder: Learning Precise Actionable Bimanual Affordances from Human Videos</h3>
<ul>
<li><strong>Authors: </strong>Marvin Heidinger, Snehal Jauhri, Vignesh Prasad, Georgia Chalvatzaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09320">https://arxiv.org/abs/2503.09320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09320">https://arxiv.org/pdf/2503.09320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09320]] 2HandedAfforder: Learning Precise Actionable Bimanual Affordances from Human Videos(https://arxiv.org/abs/2503.09320)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>When interacting with objects, humans effectively reason about which regions of objects are viable for an intended action, i.e., the affordance regions of the object. They can also account for subtle differences in object regions based on the task to be performed and whether one or two hands need to be used. However, current vision-based affordance prediction methods often reduce the problem to naive object part segmentation. In this work, we propose a framework for extracting affordance data from human activity video datasets. Our extracted 2HANDS dataset contains precise object affordance region segmentations and affordance class-labels as narrations of the activity performed. The data also accounts for bimanual actions, i.e., two hands co-ordinating and interacting with one or more objects. We present a VLM-based affordance prediction model, 2HandedAfforder, trained on the dataset and demonstrate superior performance over baselines in affordance region segmentation for various activities. Finally, we show that our predicted affordance regions are actionable, i.e., can be used by an agent performing a task, through demonstration in robotic manipulation scenarios.</li>
</ul>

<h3>Title: DAVE: Diagnostic benchmark for Audio Visual Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Gorjan Radevski, Teodora Popordanoska, Matthew B. Blaschko, Tinne Tuytelaars</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09321">https://arxiv.org/abs/2503.09321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09321">https://arxiv.org/pdf/2503.09321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09321]] DAVE: Diagnostic benchmark for Audio Visual Evaluation(https://arxiv.org/abs/2503.09321)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Audio-visual understanding is a rapidly evolving field that seeks to integrate and interpret information from both auditory and visual modalities. Despite recent advances in multi-modal learning, existing benchmarks often suffer from strong visual bias -- where answers can be inferred from visual data alone -- and provide only aggregate scores that conflate multiple sources of error. This makes it difficult to determine whether models struggle with visual understanding, audio interpretation, or audio-visual alignment. In this work, we introduce DAVE (Diagnostic Audio Visual Evaluation), a novel benchmark dataset designed to systematically evaluate audio-visual models across controlled challenges. DAVE alleviates existing limitations by (i) ensuring both modalities are necessary to answer correctly and (ii) decoupling evaluation into atomic subcategories. Our detailed analysis of state-of-the-art models reveals specific failure modes and provides targeted insights for improvement. By offering this standardized diagnostic framework, we aim to facilitate more robust development of audio-visual models. The dataset is released: this https URL</li>
</ul>

<h3>Title: A Survey on Enhancing Causal Reasoning Ability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Zhuo Cai, Shoujin Wang, Kun Yu, Fang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09326">https://arxiv.org/abs/2503.09326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09326">https://arxiv.org/pdf/2503.09326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09326]] A Survey on Enhancing Causal Reasoning Ability of Large Language Models(https://arxiv.org/abs/2503.09326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently shown remarkable performance in language tasks and beyond. However, due to their limited inherent causal reasoning ability, LLMs still face challenges in handling tasks that require robust causal reasoning ability, such as health-care and economic analysis. As a result, a growing body of research has focused on enhancing the causal reasoning ability of LLMs. Despite the booming research, there lacks a survey to well review the challenges, progress and future directions in this area. To bridge this significant gap, we systematically review literature on how to strengthen LLMs' causal reasoning ability in this paper. We start from the introduction of background and motivations of this topic, followed by the summarisation of key challenges in this area. Thereafter, we propose a novel taxonomy to systematically categorise existing methods, together with detailed comparisons within and between classes of methods. Furthermore, we summarise existing benchmarks and evaluation metrics for assessing LLMs' causal reasoning ability. Finally, we outline future research directions for this emerging field, offering insights and inspiration to researchers and practitioners in the area.</li>
</ul>

<h3>Title: Heuristic-Based Address Clustering in Cardano Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Mostafa Chegenizadeh, Sina Rafati Niya, Claudio J. Tessone</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09327">https://arxiv.org/abs/2503.09327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09327">https://arxiv.org/pdf/2503.09327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09327]] Heuristic-Based Address Clustering in Cardano Blockchain(https://arxiv.org/abs/2503.09327)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Blockchain technology has recently gained widespread popularity as a practical method of storing immutable data while preserving the privacy of users by anonymizing their real identities. This anonymization approach, however, significantly complicates the analysis of blockchain data. To address this problem, heuristic-based clustering algorithms as an effective way of linking all addresses controlled by the same entity have been presented in the literature. In this paper, considering the particular features of the Extended Unspent Transaction Outputs accounting model introduced by the Cardano blockchain, two new clustering heuristics are proposed for clustering the Cardano payment addresses. Applying these heuristics and employing the UnionFind algorithm, we efficiently cluster all the addresses that have appeared on the Cardano blockchain from September 2017 to January 2023, where each cluster represents a distinct entity. The results show that each medium-sized entity in the Cardano network owns and controls 9.67 payment addresses on average. The results also confirm that a power law distribution is fitted to the distribution of entity sizes recognized using our proposed heuristics.</li>
</ul>

<h3>Title: Group-robust Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Thomas De Min, Subhankar Roy, Stéphane Lathuilière, Elisa Ricci, Massimiliano Mancini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09330">https://arxiv.org/abs/2503.09330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09330">https://arxiv.org/pdf/2503.09330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09330]] Group-robust Machine Unlearning(https://arxiv.org/abs/2503.09330)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at this https URL.</li>
</ul>

<h3>Title: CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data</h3>
<ul>
<li><strong>Authors: </strong>Adel ElZemity, Budi Arief, Shujun Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09334">https://arxiv.org/abs/2503.09334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09334">https://arxiv.org/pdf/2503.09334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09334]] CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data(https://arxiv.org/abs/2503.09334)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The integration of large language models (LLMs) into cyber security applications presents significant opportunities, such as enhancing threat analysis and malware detection, but can also introduce critical risks and safety concerns, including personal data leakage and automated generation of new malware. To address these challenges, we developed CyberLLMInstruct, a dataset of 54,928 instruction-response pairs spanning cyber security tasks such as malware analysis, phishing simulations, and zero-day vulnerabilities. The dataset was constructed through a multi-stage process. This involved sourcing data from multiple resources, filtering and structuring it into instruction-response pairs, and aligning it with real-world scenarios to enhance its applicability. Seven open-source LLMs were chosen to test the usefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we rigorously assess the safety of fine-tuned models using the OWASP top 10 framework, finding that fine-tuning reduces safety resilience across all tested LLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B against prompt injection drops from 0.95 to 0.15). In our second example, we show that these same fine-tuned models can also achieve up to 92.50 percent accuracy on the CyberMetric benchmark. These findings highlight a trade-off between performance and safety, showing the importance of adversarial testing and further research into fine-tuning methodologies that can mitigate safety risks while still improving performance across diverse datasets and domains. All scripts required to reproduce the dataset, along with examples and relevant resources for replicating our results, will be made available upon the paper's acceptance.</li>
</ul>

<h3>Title: Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature Awareness</h3>
<ul>
<li><strong>Authors: </strong>Yu Feng, Dingxin Zhang, Runkai Zhao, Yong Xia, Heng Huang, Weidong Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09336">https://arxiv.org/abs/2503.09336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09336">https://arxiv.org/pdf/2503.09336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09336]] Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature Awareness(https://arxiv.org/abs/2503.09336)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>Backdoor attacks pose a severe threat to deep neural networks (DNN) by implanting hidden backdoors that can be activated with predefined triggers to manipulate model behaviors maliciously. Existing 3D point cloud backdoor attacks primarily rely on sample-wise global modifications, resulting in suboptimal stealthiness. To address this limitation, we propose Stealthy Patch-Wise Backdoor Attack (SPBA), which employs the first patch-wise trigger for 3D point clouds and restricts perturbations to local regions, significantly enhancing stealthiness. Specifically, SPBA decomposes point clouds into local patches and evaluates their geometric complexity using a curvature-based patch imperceptibility score, ensuring that the trigger remains less perceptible to the human eye by strategically applying it across multiple geometrically complex patches with lower visual sensitivity. By leveraging the Graph Fourier Transform (GFT), SPBA optimizes a patch-wise spectral trigger that perturbs the spectral features of selected patches, enhancing attack effectiveness while preserving the global geometric structure of the point cloud. Extensive experiments on ModelNet40 and ShapeNetPart demonstrate that SPBA consistently achieves an attack success rate (ASR) exceeding 96.5% across different models while achieving state-of-the-art imperceptibility compared to existing backdoor attack methods.</li>
</ul>

<h3>Title: Investigating User Perspectives on Differentially Private Text Privatization</h3>
<ul>
<li><strong>Authors: </strong>Stephen Meisenbacher, Alexandra Klymenko, Alexander Karpp, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09338">https://arxiv.org/abs/2503.09338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09338">https://arxiv.org/pdf/2503.09338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09338]] Investigating User Perspectives on Differentially Private Text Privatization(https://arxiv.org/abs/2503.09338)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Recent literature has seen a considerable uptick in $\textit{Differentially Private Natural Language Processing}$ (DP NLP). This includes DP text privatization, where potentially sensitive input texts are transformed under DP to achieve privatized output texts that ideally mask sensitive information $\textit{and}$ maintain original semantics. Despite continued work to address the open challenges in DP text privatization, there remains a scarcity of work addressing user perceptions of this technology, a crucial aspect which serves as the final barrier to practical adoption. In this work, we conduct a survey study with 721 laypersons around the globe, investigating how the factors of $\textit{scenario}$, $\textit{data sensitivity}$, $\textit{mechanism type}$, and $\textit{reason for data collection}$ impact user preferences for text privatization. We learn that while all these factors play a role in influencing privacy decisions, users are highly sensitive to the utility and coherence of the private output texts. Our findings highlight the socio-technical factors that must be considered in the study of DP NLP, opening the door to further user-based investigations going forward.</li>
</ul>

<h3>Title: Unified Dense Prediction of Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Lehan Yang, Lu Qi, Xiangtai Li, Sheng Li, Varun Jampani, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09344">https://arxiv.org/abs/2503.09344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09344">https://arxiv.org/pdf/2503.09344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09344]] Unified Dense Prediction of Video Diffusion(https://arxiv.org/abs/2503.09344)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>We present a unified network for simultaneously generating videos and their corresponding entity segmentation and depth maps from text prompts. We utilize colormap to represent entity masks and depth maps, tightly integrating dense prediction with RGB video generation. Introducing dense prediction information improves video generation's consistency and motion smoothness without increasing computational costs. Incorporating learnable task embeddings brings multiple dense prediction tasks into a single model, enhancing flexibility and further boosting performance. We further propose a large-scale dense prediction video dataset~\datasetname, addressing the issue that existing datasets do not concurrently contain captions, videos, segmentation, or depth maps. Comprehensive experiments demonstrate the high efficiency of our method, surpassing the state-of-the-art in terms of video quality, consistency, and motion smoothness.</li>
</ul>

<h3>Title: Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Chen, Seraphina Goldfarb-Tarrant</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09347">https://arxiv.org/abs/2503.09347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09347">https://arxiv.org/pdf/2503.09347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09347]] Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts(https://arxiv.org/abs/2503.09347)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.</li>
</ul>

<h3>Title: GIGP: A Global Information Interacting and Geometric Priors Focusing Framework for Semi-supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Lianyuan Yu, Xiuzhen Guo, Ji Shi, Hongxiao Wang, Hongwei Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09355">https://arxiv.org/abs/2503.09355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09355">https://arxiv.org/pdf/2503.09355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09355]] GIGP: A Global Information Interacting and Geometric Priors Focusing Framework for Semi-supervised Medical Image Segmentation(https://arxiv.org/abs/2503.09355)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning enhances medical image segmentation by leveraging unlabeled data, reducing reliance on extensive labeled datasets. On the one hand, the distribution discrepancy between limited labeled data and abundant unlabeled data can hinder model generalization. Most existing methods rely on local similarity matching, which may introduce bias. In contrast, Mamba effectively models global context with linear complexity, learning more comprehensive data representations. On the other hand, medical images usually exhibit consistent anatomical structures defined by geometric features. Most existing methods fail to fully utilize global geometric priors, such as volumes, moments etc. In this work, we introduce a global information interaction and geometric priors focus framework (GIGP). Firstly, we present a Global Information Interaction Mamba module to reduce distribution discrepancy between labeled and unlabeled data. Secondly, we propose a Geometric Moment Attention Mechanism to extract richer global geometric features. Finally, we propose Global Geometric Perturbation Consistency to simulate organ dynamics and geometric variations, enhancing the ability of the model to learn generalized features. The superior performance on the NIH Pancreas and Left Atrium datasets demonstrates the effectiveness of our approach.</li>
</ul>

<h3>Title: RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports</h3>
<ul>
<li><strong>Authors: </strong>Jiushen Cai, Weihang Zhang, Hanruo Liu, Ningli Wang, Huiqi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09358">https://arxiv.org/abs/2503.09358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09358">https://arxiv.org/pdf/2503.09358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09358]] RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports(https://arxiv.org/abs/2503.09358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Standardization of clinical reports is crucial for improving the quality of healthcare and facilitating data integration. The lack of unified standards, including format, terminology, and style, is a great challenge in clinical fundus diagnostic reports, which increases the difficulty for large language models (LLMs) to understand the data. To address this, we construct a bilingual standard terminology, containing fundus clinical terms and commonly used descriptions in clinical diagnosis. Then, we establish two models, RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented dataset simulating clinical scenarios, demonstrates powerful standardization behaviors. However, it encounters a challenge of limitation to cover a wider range of diseases. To further enhance standardization performance, we build RetSTA-7B, which integrates a substantial amount of standardized data generated by RetSTA-7B-Zero along with corresponding English data, covering diverse complex clinical scenarios and achieving report-level standardization for the first time. Experimental results demonstrate that RetSTA-7B outperforms other compared LLMs in bilingual standardization task, which validates its superior performance and generalizability. The checkpoints are available at this https URL.</li>
</ul>

<h3>Title: Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity</h3>
<ul>
<li><strong>Authors: </strong>Daniel Jiménez-López, Nuria Rodríguez-Barroso, M. Victoria Luzón, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09365">https://arxiv.org/abs/2503.09365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09365">https://arxiv.org/pdf/2503.09365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09365]] Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity(https://arxiv.org/abs/2503.09365)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, steal, membership infer, interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning models have an intrinsic privacy issue as they memorize parts of their training data, creating a privacy leakage. Membership Inference Attacks (MIA) exploit it to obtain confidential information about the data used for training, aiming to steal information. They can be repurposed as a measurement of data integrity by inferring whether it was used to train a machine learning model. While state-of-the-art attacks achieve a significant privacy leakage, their requirements are not feasible enough, hindering their role as practical tools to assess the magnitude of the privacy risk. Moreover, the most appropriate evaluation metric of MIA, the True Positive Rate at low False Positive Rate lacks interpretability. We claim that the incorporation of Few-Shot Learning techniques to the MIA field and a proper qualitative and quantitative privacy evaluation measure should deal with these issues. In this context, our proposal is twofold. We propose a Few-Shot learning based MIA, coined as the FeS-MIA model, which eases the evaluation of the privacy breach of a deep learning model by significantly reducing the number of resources required for the purpose. Furthermore, we propose an interpretable quantitative and qualitative measure of privacy, referred to as Log-MIA measure. Jointly, these proposals provide new tools to assess the privacy leakage and to ease the evaluation of the training data integrity of deep learning models, that is, to analyze the privacy breach of a deep learning model. Experiments carried out with MIA over image classification and language modeling tasks and its comparison to the state-of-the-art show that our proposals excel at reporting the privacy leakage of a deep learning model with little extra information.</li>
</ul>

<h3>Title: Post-interactive Multimodal Trajectory Prediction for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Huang, Yang Li, Dushuai Li, Yao Mu, Hongmao Qin, Nan Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09366">https://arxiv.org/abs/2503.09366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09366">https://arxiv.org/pdf/2503.09366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09366]] Post-interactive Multimodal Trajectory Prediction for Autonomous Driving(https://arxiv.org/abs/2503.09366)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modeling the interactions among agents for trajectory prediction of autonomous driving has been challenging due to the inherent uncertainty in agents' behavior. The interactions involved in the predicted trajectories of agents, also called post-interactions, have rarely been considered in trajectory prediction models. To this end, we propose a coarse-to-fine Transformer for multimodal trajectory prediction, i.e., Pioformer, which explicitly extracts the post-interaction features to enhance the prediction accuracy. Specifically, we first build a Coarse Trajectory Network to generate coarse trajectories based on the observed trajectories and lane segments, in which the low-order interaction features are extracted with the graph neural networks. Next, we build a hypergraph neural network-based Trajectory Proposal Network to generate trajectory proposals, where the high-order interaction features are learned by the hypergraphs. Finally, the trajectory proposals are sent to the Proposal Refinement Network for further refinement. The observed trajectories and trajectory proposals are concatenated together as the inputs of the Proposal Refinement Network, in which the post-interaction features are learned by combining the previous interaction features and trajectory consistency features. Moreover, we propose a three-stage training scheme to facilitate the learning process. Extensive experiments on the Argoverse 1 dataset demonstrate the superiority of our method. Compared with the baseline HiVT-64, our model has reduced the prediction errors by 4.4%, 8.4%, 14.4%, 5.7% regarding metrics minADE6, minFDE6, MR6, and brier-minFDE6, respectively.</li>
</ul>

<h3>Title: PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling</h3>
<ul>
<li><strong>Authors: </strong>Nikolai Körber, Eduard Kromer, Andreas Siebert, Sascha Hauke, Daniel Mueller-Gritschneder, Björn Schuller</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09368">https://arxiv.org/abs/2503.09368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09368">https://arxiv.org/pdf/2503.09368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09368]] PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling(https://arxiv.org/abs/2503.09368)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at this https URL.</li>
</ul>

<h3>Title: Revisiting Medical Image Retrieval via Knowledge Consolidation</h3>
<ul>
<li><strong>Authors: </strong>Yang Nan, Huichi Zhou, Xiaodan Xing, Giorgos Papanastasiou, Lei Zhu, Zhifan Gao, Alejandro F Fangi, Guang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09370">https://arxiv.org/abs/2503.09370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09370">https://arxiv.org/pdf/2503.09370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09370]] Revisiting Medical Image Retrieval via Knowledge Consolidation(https://arxiv.org/abs/2503.09370)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, robust</a></li>
<li><strong>Abstract: </strong>As artificial intelligence and digital medicine increasingly permeate healthcare systems, robust governance frameworks are essential to ensure ethical, secure, and effective implementation. In this context, medical image retrieval becomes a critical component of clinical data management, playing a vital role in decision-making and safeguarding patient information. Existing methods usually learn hash functions using bottleneck features, which fail to produce representative hash codes from blended embeddings. Although contrastive hashing has shown superior performance, current approaches often treat image retrieval as a classification task, using category labels to create positive/negative pairs. Moreover, many methods fail to address the out-of-distribution (OOD) issue when models encounter external OOD queries or adversarial attacks. In this work, we propose a novel method to consolidate knowledge of hierarchical features and optimisation functions. We formulate the knowledge consolidation by introducing Depth-aware Representation Fusion (DaRF) and Structure-aware Contrastive Hashing (SCH). DaRF adaptively integrates shallow and deep representations into blended features, and SCH incorporates image fingerprints to enhance the adaptability of positive/negative pairings. These blended features further facilitate OOD detection and content-based recommendation, contributing to a secure AI-driven healthcare environment. Moreover, we present a content-guided ranking to improve the robustness and reproducibility of retrieval results. Our comprehensive assessments demonstrate that the proposed method could effectively recognise OOD samples and significantly outperform existing approaches in medical image retrieval (p<0.05). In particular, our method achieves a 5.6-38.9% improvement in mean Average Precision on the anatomical radiology dataset.</li>
</ul>

<h3>Title: Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning</h3>
<ul>
<li><strong>Authors: </strong>Suryansh Upadhyay, Koustubh Phalak, Jungeun Lee, Kathleen Mitchell Hill, Swaroop Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09375">https://arxiv.org/abs/2503.09375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09375">https://arxiv.org/pdf/2503.09375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09375]] Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning(https://arxiv.org/abs/2503.09375)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Quantum computing is an emerging paradigm with the potential to transform numerous application areas by addressing problems considered intractable in the classical domain. However, its integration into cyberspace introduces significant security and privacy challenges. The exponential rise in cyber attacks, further complicated by quantum capabilities, poses serious risks to financial systems and national security. The scope of quantum threats extends beyond traditional software, operating system, and network vulnerabilities, necessitating a shift in cybersecurity education. Traditional cybersecurity education, often reliant on didactic methods, lacks hands on, student centered learning experiences necessary to prepare students for these evolving challenges. There is an urgent need for curricula that address both classical and quantum security threats through experiential learning. In this work, we present the design and evaluation of EE 597: Introduction to Hardware Security, a graduate level course integrating hands-on quantum security learning with classical security concepts through simulations and cloud-based quantum hardware. Unlike conventional courses focused on quantum threats to cryptographic systems, EE 597 explores security challenges specific to quantum computing itself. We employ a mixed-methods evaluation using pre and post surveys to assess student learning outcomes and engagement. Results indicate significant improvements in students' understanding of quantum and hardware security, with strong positive feedback on course structure and remote instruction (mean scores: 3.33 to 3.83 on a 4 point scale).</li>
</ul>

<h3>Title: Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition</h3>
<ul>
<li><strong>Authors: </strong>Fangzheng Qi, Zhenjie Hou, En Lin, Xing Li, iuzhen Liang, Xinwen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09378">https://arxiv.org/abs/2503.09378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09378">https://arxiv.org/pdf/2503.09378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09378]] Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition(https://arxiv.org/abs/2503.09378)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The recognition of pig behavior plays a crucial role in smart farming and welfare assurance for pigs. Currently, in the field of pig behavior recognition, the lack of publicly available behavioral datasets not only limits the development of innovative algorithms but also hampers model robustness and algorithm this http URL paper proposes a dataset containing 13 pig behaviors that significantly impact this http URL on this dataset, this paper proposes a spatial-temporal perception and enhancement networks based on the attention mechanism to model the spatiotemporal features of pig behaviors and their associated interaction areas in video data. The network is composed of a spatiotemporal perception network and a spatiotemporal feature enhancement network. The spatiotemporal perception network is responsible for establishing connections between the pigs and the key regions of their behaviors in the video data. The spatiotemporal feature enhancement network further strengthens the important spatial features of individual pigs and captures the long-term dependencies of the spatiotemporal features of individual behaviors by remodeling these connections, thereby enhancing the model's perception of spatiotemporal changes in pig behaviors. Experimental results demonstrate that on the dataset established in this paper, our proposed model achieves a MAP score of 75.92%, which is an 8.17% improvement over the best-performing traditional model. This study not only improces the accuracy and generalizability of individual pig behavior recognition but also provides new technological tools for modern smart farming. The dataset and related code will be made publicly available alongside this paper.</li>
</ul>

<h3>Title: Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaozhen Qiao, Peng Huang, Jiakang Yuan, Xianda Guo, Bowen Ye, Zhe Sun, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09394">https://arxiv.org/abs/2503.09394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09394">https://arxiv.org/pdf/2503.09394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09394]] Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of Vision-Language Models(https://arxiv.org/abs/2503.09394)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Test-time adaptation (TTA) is crucial in maintaining Vision-Language Models (VLMs) performance when facing real-world distribution shifts, particularly when the source data or target labels are inaccessible. Existing TTA methods rely on CLIP's output probability distribution for feature evaluation, which can introduce biases under domain shifts. This misalignment may cause features to be misclassified due to text priors or incorrect textual associations. To address these limitations, we propose Bidirectional Prototype-Reward co-Evolution (BPRE), a novel TTA framework for VLMs that integrates feature quality assessment with prototype evolution through a synergistic feedback loop. BPRE first employs a Multi-Dimensional Quality-Aware Reward Module to evaluate feature quality and guide prototype refinement precisely. The continuous refinement of prototype quality through Prototype-Reward Interactive Evolution will subsequently enhance the computation of more robust Multi-Dimensional Quality-Aware Reward Scores. Through the bidirectional interaction, the precision of rewards and the evolution of prototypes mutually reinforce each other, forming a self-evolving cycle. Extensive experiments are conducted across 15 diverse recognition datasets encompassing natural distribution shifts and cross-dataset generalization scenarios. Results demonstrate that BPRE consistently achieves superior average performance compared to state-of-the-art methods across different model architectures, such as ResNet-50 and ViT-B/16. By emphasizing comprehensive feature evaluation and bidirectional knowledge refinement, BPRE advances VLM generalization capabilities, offering a new perspective on TTA.</li>
</ul>

<h3>Title: Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training</h3>
<ul>
<li><strong>Authors: </strong>Jiatong Xia, Lingqiao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09396">https://arxiv.org/abs/2503.09396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09396">https://arxiv.org/pdf/2503.09396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09396]] Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training(https://arxiv.org/abs/2503.09396)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has demonstrated impressive performance in synthesizing novel views after training on a given set of viewpoints. However, its rendering quality deteriorates when the synthesized view deviates significantly from the training views. This decline occurs due to (1) the model's difficulty in generalizing to out-of-distribution scenarios and (2) challenges in interpolating fine details caused by substantial resolution changes and occlusions. A notable case of this limitation is close-up view generation--producing views that are significantly closer to the object than those in the training set. To tackle this issue, we propose a novel approach for close-up view generation based by progressively training the 3DGS model with self-generated data. Our solution is based on three key ideas. First, we leverage the See3D model, a recently introduced 3D-aware generative model, to enhance the details of rendered views. Second, we propose a strategy to progressively expand the ``trust regions'' of the 3DGS model and update a set of reference views for See3D. Finally, we introduce a fine-tuning strategy to carefully update the 3DGS model with training data generated from the above schemes. We further define metrics for close-up views evaluation to facilitate better research on this problem. By conducting evaluations on specifically selected scenarios for close-up views, our proposed approach demonstrates a clear advantage over competitive solutions.</li>
</ul>

<h3>Title: ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Tobias Christian Nauen, Brian Moser, Federico Raue, Stanislav Frolov, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09399">https://arxiv.org/abs/2503.09399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09399">https://arxiv.org/pdf/2503.09399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09399]] ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation(https://arxiv.org/abs/2503.09399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation scheme that addresses these challenges and explicitly includes inductive biases, which commonly are part of the neural network architecture, into the training data. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds, enabling fine-grained control over image composition during training. It thus increases the data diversity and effective number of training samples. We demonstrate that training on ForNet, the application of ForAug to ImageNet, significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks. Importantly, ForAug enables novel ways of analyzing model behavior and quantifying biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that training on ForNet substantially reduces these biases compared to training on ImageNet. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at this https URL.</li>
</ul>

<h3>Title: VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary</h3>
<ul>
<li><strong>Authors: </strong>Kevin Qinghong Lin, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09402">https://arxiv.org/abs/2503.09402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09402">https://arxiv.org/pdf/2503.09402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09402]] VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary(https://arxiv.org/abs/2503.09402)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at this https URL.</li>
</ul>

<h3>Title: Diff-CL: A Novel Cross Pseudo-Supervision Method for Semi-supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiuzhen Guo, Lianyuan Yu, Ji Shi, Na Lei, Hongxiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09408">https://arxiv.org/abs/2503.09408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09408">https://arxiv.org/pdf/2503.09408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09408]] Diff-CL: A Novel Cross Pseudo-Supervision Method for Semi-supervised Medical Image Segmentation(https://arxiv.org/abs/2503.09408)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning utilizes insights from unlabeled data to improve model generalization, thereby reducing reliance on large labeled datasets. Most existing studies focus on limited samples and fail to capture the overall data distribution. We contend that combining distributional information with detailed information is crucial for achieving more robust and accurate segmentation results. On the one hand, with its robust generative capabilities, diffusion models (DM) learn data distribution effectively. However, it struggles with fine detail capture, leading to generated images with misleading details. Combining DM with convolutional neural networks (CNNs) enables the former to learn data distribution while the latter corrects fine details. While capturing complete high-frequency details by CNNs requires substantial computational resources and is susceptible to local noise. On the other hand, given that both labeled and unlabeled data come from the same distribution, we believe that regions in unlabeled data similar to overall class semantics to labeled data are likely to belong to the same class, while regions with minimal similarity are less likely to. This work introduces a semi-supervised medical image segmentation framework from the distribution perspective (Diff-CL). Firstly, we propose a cross-pseudo-supervision learning mechanism between diffusion and convolution segmentation networks. Secondly, we design a high-frequency mamba module to capture boundary and detail information globally. Finally, we apply contrastive learning for label propagation from labeled to unlabeled data. Our method achieves state-of-the-art (SOTA) performance across three datasets, including left atrium, brain tumor, and NIH pancreas datasets.</li>
</ul>

<h3>Title: Monte Carlo Diffusion for Generalizable Learning-Based RANSAC</h3>
<ul>
<li><strong>Authors: </strong>Jiale Wang, Chen Zhao, Wei Ke, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09410">https://arxiv.org/abs/2503.09410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09410">https://arxiv.org/pdf/2503.09410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09410]] Monte Carlo Diffusion for Generalizable Learning-Based RANSAC(https://arxiv.org/abs/2503.09410)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Random Sample Consensus (RANSAC) is a fundamental approach for robustly estimating parametric models from noisy data. Existing learning-based RANSAC methods utilize deep learning to enhance the robustness of RANSAC against outliers. However, these approaches are trained and tested on the data generated by the same algorithms, leading to limited generalization to out-of-distribution data during inference. Therefore, in this paper, we introduce a novel diffusion-based paradigm that progressively injects noise into ground-truth data, simulating the noisy conditions for training learning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo sampling into the diffusion paradigm, approximating diverse data distributions by introducing different types of randomness at multiple stages. We evaluate our approach in the context of feature matching through comprehensive experiments on the ScanNet and MegaDepth datasets. The experimental results demonstrate that our Monte Carlo diffusion mechanism significantly improves the generalization ability of learning-based RANSAC. We also develop extensive ablation studies that highlight the effectiveness of key components in our framework.</li>
</ul>

<h3>Title: Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Amit Attia, Tomer Koren</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09411">https://arxiv.org/abs/2503.09411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09411">https://arxiv.org/pdf/2503.09411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09411]] Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic Optimization(https://arxiv.org/abs/2503.09411)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The learning rate in stochastic gradient methods is a critical hyperparameter that is notoriously costly to tune via standard grid search, especially for training modern large-scale models with billions of parameters. We identify a theoretical advantage of learning rate annealing schemes that decay the learning rate to zero at a polynomial rate, such as the widely-used cosine schedule, by demonstrating their increased robustness to initial parameter misspecification due to a coarse grid search. We present an analysis in a stochastic convex optimization setup demonstrating that the convergence rate of stochastic gradient descent with annealed schedules depends sublinearly on the multiplicative misspecification factor $\rho$ (i.e., the grid resolution), achieving a rate of $O(\rho^{1/(2p+1)}/\sqrt{T})$ where $p$ is the degree of polynomial decay and $T$ is the number of steps, in contrast to the $O(\rho/\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear dependence on $\rho$. Experiments confirm the increased robustness compared to tuning with a fixed stepsize, that has significant implications for the computational overhead of hyperparameter search in practical training scenarios.</li>
</ul>

<h3>Title: Mitigating Membership Inference Vulnerability in Personalized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Kangsoo Jung, Sayan Biswas, Catuscia Palamidessi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09414">https://arxiv.org/abs/2503.09414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09414">https://arxiv.org/pdf/2503.09414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09414]] Mitigating Membership Inference Vulnerability in Personalized Federated Learning(https://arxiv.org/abs/2503.09414)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a promising paradigm for collaborative model training without the need to share clients' personal data, thereby preserving privacy. However, the non-IID nature of the clients' data introduces major challenges for FL, highlighting the importance of personalized federated learning (PFL) methods. In PFL, models are trained to cater to specific feature distributions present in the population data. A notable method for PFL is the Iterative Federated Clustering Algorithm (IFCA), which mitigates the concerns associated with the non-IID-ness by grouping clients with similar data distributions. While it has been shown that IFCA enhances both accuracy and fairness, its strategy of dividing the population into smaller clusters increases vulnerability to Membership Inference Attacks (MIA), particularly among minorities with limited training samples. In this paper, we introduce IFCA-MIR, an improved version of IFCA that integrates MIA risk assessment into the clustering process. Allowing clients to select clusters based on both model performance and MIA vulnerability, IFCA-MIR achieves an improved performance with respect to accuracy, fairness, and privacy. We demonstrate that IFCA-MIR significantly reduces MIA risk while maintaining comparable model accuracy and fairness as the original IFCA.</li>
</ul>

<h3>Title: Efficient dynamic modal load reconstruction using physics-informed Gaussian processes based on frequency-sparse Fourier basis functions</h3>
<ul>
<li><strong>Authors: </strong>Gledson Rodrigo Tondo, Igor Kavrakov, Guido Morgenthal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09418">https://arxiv.org/abs/2503.09418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09418">https://arxiv.org/pdf/2503.09418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09418]] Efficient dynamic modal load reconstruction using physics-informed Gaussian processes based on frequency-sparse Fourier basis functions(https://arxiv.org/abs/2503.09418)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Knowledge of the force time history of a structure is essential to assess its behaviour, ensure safety and maintain reliability. However, direct measurement of external forces is often challenging due to sensor limitations, unknown force characteristics, or inaccessible load points. This paper presents an efficient dynamic load reconstruction method using physics-informed Gaussian processes (GP) based on frequency-sparse Fourier basis functions. The GP's covariance matrices are built using the description of the system dynamics, and the model is trained using structural response measurements. This provides support and interpretability to the machine learning model, in contrast to purely data-driven methods. In addition, the model filters out irrelevant components in the Fourier basis function by leveraging the sparsity of structural responses in the frequency domain, thereby reducing computational complexity during optimization. The trained model for structural responses is then integrated with the differential equation for a harmonic oscillator, creating a probabilistic dynamic load model that predicts load patterns without requiring force data during training. The model's effectiveness is validated through two case studies: a numerical model of a wind-excited 76-story building and an experiment using a physical scale model of the Lillebælt Bridge in Denmark, excited by a servo motor. For both cases, validation of the reconstructed forces is provided using comparison metrics for several signal properties. The developed model holds potential for applications in structural health monitoring, damage prognosis, and load model validation.</li>
</ul>

<h3>Title: Alias-Free Latent Diffusion Models:Improving Fractional Shift Equivariance of Diffusion Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhou, Zeqi Xiao, Shuai Yang, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09419">https://arxiv.org/abs/2503.09419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09419">https://arxiv.org/pdf/2503.09419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09419]] Alias-Free Latent Diffusion Models:Improving Fractional Shift Equivariance of Diffusion Latent Space(https://arxiv.org/abs/2503.09419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: this https URL</li>
</ul>

<h3>Title: Multimodal Language Modeling for High-Accuracy Single Cell Transcriptomics Analysis and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yaorui Shi, Jiaqi Yang, Sihang Li, Junfeng Fang, Xiang Wang, Zhiyuan Liu, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09427">https://arxiv.org/abs/2503.09427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09427">https://arxiv.org/pdf/2503.09427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09427]] Multimodal Language Modeling for High-Accuracy Single Cell Transcriptomics Analysis and Generation(https://arxiv.org/abs/2503.09427)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\% relative improvement of textual discrepancy for cell description generation, 20.5\% higher accuracy for cell type annotation, and 4\% improvement in $k$-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.</li>
</ul>

<h3>Title: CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection</h3>
<ul>
<li><strong>Authors: </strong>Richard A. Dubniczky, Krisztofer Zoltán Horvát, Tamás Bisztray, Mohamed Amine Ferrag, Lucas C. Cordeiro, Norbert Tihanyi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09433">https://arxiv.org/abs/2503.09433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09433">https://arxiv.org/pdf/2503.09433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09433]] CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection(https://arxiv.org/abs/2503.09433)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, fair, large language model</a></li>
<li><strong>Abstract: </strong>Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at this https URL.</li>
</ul>

<h3>Title: SuperCarver: Texture-Consistent 3D Geometry Super-Resolution for High-Fidelity Surface Detail Generation</h3>
<ul>
<li><strong>Authors: </strong>Qijian Zhang, Xiaozheng Jian, Xuan Zhang, Wenping Wang, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09439">https://arxiv.org/abs/2503.09439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09439">https://arxiv.org/pdf/2503.09439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09439]] SuperCarver: Texture-Consistent 3D Geometry Super-Resolution for High-Fidelity Surface Detail Generation(https://arxiv.org/abs/2503.09439)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Traditional production workflow of high-precision 3D mesh assets necessitates a cumbersome and laborious process of manual sculpting by specialized modelers. The recent years have witnessed remarkable advances in AI-empowered 3D content creation. However, although the latest state-of-the-arts are already capable of generating plausible structures and intricate appearances from images or text prompts, the actual mesh surfaces are typically over-smoothing and lack geometric details. This paper introduces SuperCarver, a 3D geometry super-resolution framework particularly tailored for adding texture-consistent surface details to given coarse meshes. Technically, we start by rendering the original textured mesh into the image domain from multiple viewpoints. To achieve geometric detail generation, we develop a deterministic prior-guided normal diffusion model fine-tuned on a carefully curated dataset of paired low-poly and high-poly normal renderings. To optimize mesh structures from potentially imperfect normal map predictions, we design a simple yet effective noise-resistant inverse rendering scheme based on distance field deformation. Extensive experiments show that SuperCarver generates realistic and expressive surface details as depicted by specific texture appearances, making it a powerful tool for automatically upgrading massive outdated low-quality assets and shortening the iteration cycle of high-quality mesh production in practical applications.</li>
</ul>

<h3>Title: Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Julian Spravil, Sebastian Houben, Sven Behnke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09443">https://arxiv.org/abs/2503.09443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09443">https://arxiv.org/pdf/2503.09443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09443]] Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models(https://arxiv.org/abs/2503.09443)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task performance for multilingual capabilities, struggling with lexical ambiguities, and falling behind recent advances. In this work, we study the scaling laws of systematic generalization with monolingual VLMs for multilingual tasks, focusing on the impact of model size and seen training samples. We propose Florenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters combining the pre-trained VLM Florence-2 and the large language model Gemma-2. Florenz is trained with varying compute budgets on a synthetic dataset that features intentionally incomplete language coverage for image captioning, thus, testing generalization from the fully covered translation task. We show that not only does indirectly learning unseen task-language pairs adhere to a scaling law, but also that with our data generation pipeline and the proposed Florenz model family, image captioning abilities can emerge in a specific language even when only data for the translation task is available. Fine-tuning on a mix of downstream datasets yields competitive performance and demonstrates promising scaling trends in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).</li>
</ul>

<h3>Title: Astrea: A MOE-based Visual Understanding Model with Progressive Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xiaoda Yang, JunYu Lu, Hongshun Qiu, Sijing Li, Hao Li, Shengpeng Ji, Xudong Tang, Jiayang Xu, Jiaqi Duan, Ziyue Jiang, Cong Lin, Sihang Cai, Zejian Xie, Zhuoyang Song, Songxin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09445">https://arxiv.org/abs/2503.09445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09445">https://arxiv.org/pdf/2503.09445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09445]] Astrea: A MOE-based Visual Understanding Model with Progressive Alignment(https://arxiv.org/abs/2503.09445)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.</li>
</ul>

<h3>Title: Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihua Tian, Sirun Nan, Ming Xu, Shengfang Zhai, Wenjie Qu, Jian Liu, Kui Ren, Ruoxi Jia, Jiaheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09446">https://arxiv.org/abs/2503.09446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09446">https://arxiv.org/pdf/2503.09446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09446]] Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models(https://arxiv.org/abs/2503.09446)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have achieved remarkable progress in generating high-quality images but also raise people's concerns about generating harmful or misleading content. While extensive approaches have been proposed to erase unwanted concepts without requiring retraining from scratch, they inadvertently degrade performance on normal generation tasks. In this work, we propose Interpret then Deactivate (ItD), a novel framework to enable precise concept removal in T2I diffusion models while preserving overall performance. ItD first employs a sparse autoencoder (SAE) to interpret each concept as a combination of multiple features. By permanently deactivating the specific features associated with target concepts, we repurpose SAE as a zero-shot classifier that identifies whether the input prompt includes target concepts, allowing selective concept erasure in diffusion models. Moreover, we demonstrate that ItD can be easily extended to erase multiple concepts without requiring further training. Comprehensive experiments across celebrity identities, artistic styles, and explicit content demonstrate ItD's effectiveness in eliminating targeted concepts without interfering with normal concept generation. Additionally, ItD is also robust against adversarial prompts designed to circumvent content filters. Code is available at: this https URL.</li>
</ul>

<h3>Title: How Well Does Your Tabular Generator Learn the Structure of Tabular Data?</h3>
<ul>
<li><strong>Authors: </strong>Xiangjian Jiang, Nikola Simidjievski, Mateja Jamnik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09453">https://arxiv.org/abs/2503.09453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09453">https://arxiv.org/pdf/2503.09453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09453]] How Well Does Your Tabular Generator Learn the Structure of Tabular Data?(https://arxiv.org/abs/2503.09453)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Heterogeneous tabular data poses unique challenges in generative modelling due to its fundamentally different underlying data structure compared to homogeneous modalities, such as images and text. Although previous research has sought to adapt the successes of generative modelling in homogeneous modalities to the tabular domain, defining an effective generator for tabular data remains an open problem. One major reason is that the evaluation criteria inherited from other modalities often fail to adequately assess whether tabular generative models effectively capture or utilise the unique structural information encoded in tabular data. In this paper, we carefully examine the limitations of the prevailing evaluation framework and introduce $\textbf{TabStruct}$, a novel evaluation benchmark that positions structural fidelity as a core evaluation dimension. Specifically, TabStruct evaluates the alignment of causal structures in real and synthetic data, providing a direct measure of how effectively tabular generative models learn the structure of tabular data. Through extensive experiments using generators from eight categories on seven datasets with expert-validated causal graphical structures, we show that structural fidelity offers a task-independent, domain-agnostic evaluation dimension. Our findings highlight the importance of tabular data structure and offer practical guidance for developing more effective and robust tabular generative models. Code is available at this https URL.</li>
</ul>

<h3>Title: Explicit Learning and the LLM in Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Malik Marmonier, Rachel Bawden, Benoît Sagot</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09454">https://arxiv.org/abs/2503.09454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09454">https://arxiv.org/pdf/2503.09454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09454]] Explicit Learning and the LLM in Machine Translation(https://arxiv.org/abs/2503.09454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study explores the capacity of large language models (LLMs) for explicit learning, a process involving the assimilation of metalinguistic explanations to carry out language tasks. Using constructed languages generated by cryptographic means as controlled test environments, we designed experiments to assess an LLM's ability to explicitly learn and apply grammar rules. Our results demonstrate that while LLMs possess a measurable capacity for explicit learning, this ability diminishes as the complexity of the linguistic phenomena at hand increases. Supervised fine-tuning on chains of thought significantly enhances LLM performance but struggles to generalize to typologically novel or more complex linguistic features. These findings point to the need for more diverse training sets and alternative fine-tuning strategies to further improve explicit learning by LLMs.</li>
</ul>

<h3>Title: Automatic Association of Quality Requirements and Quantifiable Metrics for Cloud Security Certification</h3>
<ul>
<li><strong>Authors: </strong>John Bianchi, Shuya Dong, Luca Petrillo, Marinella Petrocchi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09460">https://arxiv.org/abs/2503.09460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09460">https://arxiv.org/pdf/2503.09460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09460]] Automatic Association of Quality Requirements and Quantifiable Metrics for Cloud Security Certification(https://arxiv.org/abs/2503.09460)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, transformer</a></li>
<li><strong>Abstract: </strong>The European Cybersecurity Certification Scheme for Cloud Services (EUCS) is one of the first cybersecurity schemes in Europe, defined by the European Union Agency for Cybersecurity (ENISA). It aims to encourage cloud providers to strengthen their cybersecurity policies in order to receive an official seal of approval from European authorities. EUCS defines a set of security requirements that the cloud provider must meet, in whole or in part, in order to achieve the security certification. The requirements are written in natural language and cover every aspect of security in the cloud environment, from logging access to protecting the system with anti-malware tools to training staff. Operationally, each requirement is associated with one or more evaluable metrics. For example, a requirement to monitor access attempts to a service will have associated metrics that take into account the number of accesses, the number of access attempts, who is accessing, and what resources are being used. Partners in the European project Medina, which ended in October 2023, defined 163 metrics and manually mapped them to 70 EUCS requirements. Manual mapping is intuitively a long and costly process in terms of human resources. This paper proposes an approach based on Sentence Transformers to automatically associate requirements and metrics. In terms of correctness of associations, the proposed method achieves a Normalized Discounted Cumulative Gain of 0.640, improving a previous experiment by 0.146 points.</li>
</ul>

<h3>Title: SurgicalVLM-Agent: Towards an Interactive AI Co-Pilot for Pituitary Surgery</h3>
<ul>
<li><strong>Authors: </strong>Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos Mazomenos, Danail Stoyanov, Hani J. Marcus, Matthew J. Clarkson, Mobarakol Islam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09474">https://arxiv.org/abs/2503.09474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09474">https://arxiv.org/pdf/2503.09474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09474]] SurgicalVLM-Agent: Towards an Interactive AI Co-Pilot for Pituitary Surgery(https://arxiv.org/abs/2503.09474)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance. Large vision-language models (VLMs) offer a promising solution by enabling dynamic task planning and predictive decision support. We introduce SurgicalVLM-Agent, an AI co-pilot for image-guided pituitary surgery, capable of conversation, planning, and task execution. The agent dynamically processes surgeon queries and plans the tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA). To enable structured task planning, we develop the PitAgent dataset, a surgical context-aware dataset covering segmentation, overlaying, instrument localization, tool tracking, tool-tissue interactions, phase identification, and surgical activity recognition. Additionally, we propose FFT-GaLore, a fast Fourier transform (FFT)-based gradient projection technique for efficient low-rank adaptation, optimizing fine-tuning for LLaMA 3.2 in surgical environments. We validate SurgicalVLM-Agent by assessing task planning and prompt generation on our PitAgent dataset and evaluating zero-shot VQA using a public pituitary dataset. Results demonstrate state-of-the-art performance in task planning and query interpretation, with highly semantically meaningful VQA responses, advancing AI-driven surgical assistance.</li>
</ul>

<h3>Title: BAMBI: Developing Baby Language Models for Italian</h3>
<ul>
<li><strong>Authors: </strong>Alice Suozzi, Luca Capone, Gianluca E. Lebani, Alessandro Lenci</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09481">https://arxiv.org/abs/2503.09481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09481">https://arxiv.org/pdf/2503.09481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09481]] BAMBI: Developing Baby Language Models for Italian(https://arxiv.org/abs/2503.09481)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents BAMBI (BAby language Models Boostrapped for Italian), a series of Baby Language Models (BabyLMs) trained on data that mimic the linguistic input received by a five-year-old Italian-speaking child. The BAMBI models are tested using a benchmark specifically designed to evaluate language models, which takes into account the amount of training input the models received. The BAMBI models are compared against a large language model (LLM) and a multimodal language model (VLM) to study the contribution of extralinguistic information for language acquisition. The results of our evaluation align with the existing literature on English language models, confirming that while reduced training data support the development of relatively robust syntactic competence, they are insufficient for fostering semantic understanding. However, the gap between the training resources (data and computation) of the BAMBI models and the LLMs is not fully reflected in their performance: despite LLMs' massive training, their performance is not much better than that of BAMBI models. This suggests that strategies beyond scaling training resources, such as data curation, inclusion of multimodal input, and other training strategies such as curriculum learning, could play a crucial role in shaping model performance.</li>
</ul>

<h3>Title: A Novel Approach for Intrinsic Dimension Estimation</h3>
<ul>
<li><strong>Authors: </strong>Kadir Özçoban, Murat Manguoğlu, Emrullah Fatih Yetkin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09485">https://arxiv.org/abs/2503.09485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09485">https://arxiv.org/pdf/2503.09485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09485]] A Novel Approach for Intrinsic Dimension Estimation(https://arxiv.org/abs/2503.09485)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The real-life data have a complex and non-linear structure due to their nature. These non-linearities and the large number of features can usually cause problems such as the empty-space phenomenon and the well-known curse of dimensionality. Finding the nearly optimal representation of the dataset in a lower-dimensional space (i.e. dimensionality reduction) offers an applicable mechanism for improving the success of machine learning tasks. However, estimating the required data dimension for the nearly optimal representation (intrinsic dimension) can be very costly, particularly if one deals with big data. We propose a highly efficient and robust intrinsic dimension estimation approach that only relies on matrix-vector products for dimensionality reduction methods. An experimental study is also conducted to compare the performance of proposed method with state of the art approaches.</li>
</ul>

<h3>Title: Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness</h3>
<ul>
<li><strong>Authors: </strong>Beier Zhu, Jiequan Cui, Hanwang Zhang, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09487">https://arxiv.org/abs/2503.09487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09487">https://arxiv.org/pdf/2503.09487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09487]] Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness(https://arxiv.org/abs/2503.09487)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While image-text foundation models have succeeded across diverse downstream tasks, they still face challenges in the presence of spurious correlations between the input and label. To address this issue, we propose a simple three-step approach,Project-Probe-Aggregate (PPA), that enables parameter-efficient fine-tuning for foundation models without relying on group annotations. Building upon the failure-based debiasing scheme, our method, PPA, improves its two key components: minority samples identification and the robust training algorithm. Specifically, we first train biased classifiers by projecting image features onto the nullspace of class proxies from text encoders. Next, we infer group labels using the biased classifier and probe group targets with prior correction. Finally, we aggregate group weights of each class to produce the debiased classifier. Our theoretical analysis shows that our PPA enhances minority group identification and is Bayes optimal for minimizing the balanced group error, mitigating spurious correlations. Extensive experimental results confirm the effectiveness of our PPA: it outperforms the state-of-the-art by an average worst-group accuracy while requiring less than 0.01% tunable parameters without training group labels.</li>
</ul>

<h3>Title: DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction</h3>
<ul>
<li><strong>Authors: </strong>Junjie Zhou, Shouju Wang, Yuxia Tang, Qi Zhu, Daoqiang Zhang, Wei Shao</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09491">https://arxiv.org/abs/2503.09491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09491">https://arxiv.org/pdf/2503.09491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09491]] DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction(https://arxiv.org/abs/2503.09491)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The prediction of nanoparticles (NPs) distribution is crucial for the diagnosis and treatment of tumors. Recent studies indicate that the heterogeneity of tumor microenvironment (TME) highly affects the distribution of NPs across tumors. Hence, it has become a research hotspot to generate the NPs distribution by the aid of multi-modal TME components. However, the distribution divergence among multi-modal TME components may cause side effects i.e., the best uni-modal model may outperform the joint generative model. To address the above issues, we propose a \textbf{D}ivergence-\textbf{A}ware \textbf{M}ulti-\textbf{M}odal \textbf{Diffusion} model (i.e., \textbf{DAMM-Diffusion}) to adaptively generate the prediction results from uni-modal and multi-modal branches in a unified network. In detail, the uni-modal branch is composed of the U-Net architecture while the multi-modal branch extends it by introducing two novel fusion modules i.e., Multi-Modal Fusion Module (MMFM) and Uncertainty-Aware Fusion Module (UAFM). Specifically, the MMFM is proposed to fuse features from multiple modalities, while the UAFM module is introduced to learn the uncertainty map for cross-attention computation. Following the individual prediction results from each branch, the Divergence-Aware Multi-Modal Predictor (DAMMP) module is proposed to assess the consistency of multi-modal data with the uncertainty map, which determines whether the final prediction results come from multi-modal or uni-modal predictions. We predict the NPs distribution given the TME components of tumor vessels and cell nuclei, and the experimental results show that DAMM-Diffusion can generate the distribution of NPs with higher accuracy than the comparing methods. Additional results on the multi-modal brain image synthesis task further validate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection</h3>
<ul>
<li><strong>Authors: </strong>Romain Thoreau, Valerio Marsocci, Dawa Derksen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09493">https://arxiv.org/abs/2503.09493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09493">https://arxiv.org/pdf/2503.09493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09493]] Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection(https://arxiv.org/abs/2503.09493)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>As large-scale heterogeneous data sets become increasingly available, adapting foundation models at low cost has become a key issue. Seminal works in natural language processing, e.g. Low-Rank Adaptation (LoRA), leverage the low "intrinsic rank" of parameter updates during adaptation. In this paper, we argue that incorporating stronger inductive biases in both data and models can enhance the adaptation of Geospatial Foundation Models (GFMs), pretrained on RGB satellite images, to other types of optical satellite data. Specifically, the pretrained parameters of GFMs serve as a strong prior for the spatial structure of multispectral images. For this reason, we introduce DEFLECT (Deflecting Embeddings for Finetuning Latent representations for Earth and Climate Tasks), a novel strategy for adapting GFMs to multispectral satellite imagery with very few additional parameters. DEFLECT improves the representation capabilities of the extracted features, particularly enhancing spectral information, which is essential for geoscience and environmental-related tasks. We demonstrate the effectiveness of our method across three different GFMs and five diverse datasets, ranging from forest monitoring to marine environment segmentation. Compared to competing methods, DEFLECT achieves on-par or higher accuracy with 5-10$\times$ fewer parameters for classification and segmentation tasks. The code will be made publicly available.</li>
</ul>

<h3>Title: Robust Multimodal Survival Prediction with the Latent Differentiation Conditional Variational AutoEncoder</h3>
<ul>
<li><strong>Authors: </strong>Junjie Zhou, Jiao Tang, Yingli Zuo, Peng Wan, Daoqiang Zhang, Wei Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09496">https://arxiv.org/abs/2503.09496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09496">https://arxiv.org/pdf/2503.09496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09496]] Robust Multimodal Survival Prediction with the Latent Differentiation Conditional Variational AutoEncoder(https://arxiv.org/abs/2503.09496)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>The integrative analysis of histopathological images and genomic data has received increasing attention for survival prediction of human cancers. However, the existing studies always hold the assumption that full modalities are available. As a matter of fact, the cost for collecting genomic data is high, which sometimes makes genomic data unavailable in testing samples. A common way of tackling such incompleteness is to generate the genomic representations from the pathology images. Nevertheless, such strategy still faces the following two challenges: (1) The gigapixel whole slide images (WSIs) are huge and thus hard for representation. (2) It is difficult to generate the genomic embeddings with diverse function categories in a unified generative framework. To address the above challenges, we propose a Conditional Latent Differentiation Variational AutoEncoder (LD-CVAE) for robust multimodal survival prediction, even with missing genomic data. Specifically, a Variational Information Bottleneck Transformer (VIB-Trans) module is proposed to learn compressed pathological representations from the gigapixel WSIs. To generate different functional genomic features, we develop a novel Latent Differentiation Variational AutoEncoder (LD-VAE) to learn the common and specific posteriors for the genomic embeddings with diverse functions. Finally, we use the product-of-experts technique to integrate the genomic common posterior and image posterior for the joint latent distribution estimation in LD-CVAE. We test the effectiveness of our method on five different cancer datasets, and the experimental results demonstrate its superiority in both complete and missing modality scenarios.</li>
</ul>

<h3>Title: Federated Smoothing ADMM for Localization</h3>
<ul>
<li><strong>Authors: </strong>Reza Mirzaeifard, Ashkan Moradi, Masahiro Yukawa, Stefan Werner</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09497">https://arxiv.org/abs/2503.09497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09497">https://arxiv.org/pdf/2503.09497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09497]] Federated Smoothing ADMM for Localization(https://arxiv.org/abs/2503.09497)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of localization in federated settings, which are characterized by distributed data, non-convexity, and non-smoothness. To tackle the scalability and outlier issues inherent in such environments, we propose a robust algorithm that employs an $\ell_1$-norm formulation within a novel federated ADMM framework. This approach addresses the problem by integrating an iterative smooth approximation for the total variation consensus term and employing a Moreau envelope approximation for the convex function that appears in a subtracted form. This transformation ensures that the problem is smooth and weakly convex in each iteration, which results in enhanced computational efficiency and improved estimation accuracy. The proposed algorithm supports asynchronous updates and multiple client updates per iteration, which ensures its adaptability to real-world federated systems. To validate the reliability of the proposed algorithm, we show that the method converges to a stationary point, and numerical simulations highlight its superior performance in convergence speed and outlier resilience compared to existing state-of-the-art localization methods.</li>
</ul>

<h3>Title: Towards Robust Multimodal Representation: A Unified Approach with Adaptive Experts and Alignment</h3>
<ul>
<li><strong>Authors: </strong>Nazanin Moradinasab, Saurav Sengupta, Jiebei Liu, Sana Syed, Donald E. Brown</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09498">https://arxiv.org/abs/2503.09498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09498">https://arxiv.org/pdf/2503.09498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09498]] Towards Robust Multimodal Representation: A Unified Approach with Adaptive Experts and Alignment(https://arxiv.org/abs/2503.09498)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Healthcare relies on multiple types of data, such as medical images, genetic information, and clinical records, to improve diagnosis and treatment. However, missing data is a common challenge due to privacy restrictions, cost, and technical issues, making many existing multi-modal models unreliable. To address this, we propose a new multi-model model called Mixture of Experts, Symmetric Aligning, and Reconstruction (MoSARe), a deep learning framework that handles incomplete multimodal data while maintaining high accuracy. MoSARe integrates expert selection, cross-modal attention, and contrastive learning to improve feature representation and decision-making. Our results show that MoSARe outperforms existing models in situations when the data is complete. Furthermore, it provides reliable predictions even when some data are missing. This makes it especially useful in real-world healthcare settings, including resource-limited environments. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging Questions</h3>
<ul>
<li><strong>Authors: </strong>Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, Ying Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09499">https://arxiv.org/abs/2503.09499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09499">https://arxiv.org/pdf/2503.09499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09499]] MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging Questions(https://arxiv.org/abs/2503.09499)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large vision-language models (VLMs) face challenges in achieving robust, transferable reasoning abilities due to reliance on labor-intensive manual instruction datasets or computationally expensive self-supervised methods. To address these issues, we introduce MindGYM, a framework that enhances VLMs through synthetic self-challenging questions, consisting of three stages: (1) Seed Single-Hop Question Synthesis, generating cognitive questions across textual (e.g., logical deduction) and multimodal contexts (e.g., diagram-based queries) spanning eight semantic areas like ethical analysis; (2) Challenging Multi-Hop Question Synthesis, combining seed questions via diverse principles like bridging, visual-textual alignment, to create multi-step problems demanding deeper reasoning; and (3) Thinking-Induced Curriculum Fine-Tuning, a structured pipeline that progressively trains the model from scaffolded reasoning to standalone inference. By leveraging the model's self-synthesis capability, MindGYM achieves high data efficiency (e.g., +16% gains on MathVision-Mini with only 400 samples), computational efficiency (reducing both training and inference costs), and robust generalization across tasks. Extensive evaluations on seven benchmarks demonstrate superior performance over strong baselines, with notable improvements (+15.77% win rates) in reasoning depth and breadth validated via GPT-based scoring. MindGYM underscores the viability of self-challenging for refining VLM capabilities while minimizing human intervention and resource demands. Code and data are released to advance multimodal reasoning research.</li>
</ul>

<h3>Title: Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework</h3>
<ul>
<li><strong>Authors: </strong>Bakary Badjie, José Cecílio, António Casimiro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09504">https://arxiv.org/abs/2503.09504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09504">https://arxiv.org/pdf/2503.09504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09504]] Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework(https://arxiv.org/abs/2503.09504)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL). However, its complex architecture and advantages over dense models in image classification remain unclear. In previous studies, MoE performance has often been affected by noise and outliers in the input space. Some approaches incorporate input clustering for training MoE models, but most clustering algorithms lack access to labeled data, limiting their effectiveness. This paper introduces the Double-stage Feature-level Clustering and Pseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists of input feature extraction, feature-level clustering, and a computationally efficient pseudo-labeling strategy. This approach reduces the impact of noise and outliers while leveraging a small subset of labeled data to label a large portion of unlabeled inputs. We propose a conditional end-to-end joint training method that improves expert specialization by training the MoE model on well-labeled, clustered inputs. Unlike traditional MoE and dense models, the DFCP-MoE framework effectively captures input space diversity, leading to competitive inference results. We validate our approach on three benchmark datasets for multi-class classification tasks.</li>
</ul>

<h3>Title: ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba</h3>
<ul>
<li><strong>Authors: </strong>Juncan Deng, Shuaiting Li, Zeyu Wang, Kedong Xu, Hong Gu, Kejie Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09509">https://arxiv.org/abs/2503.09509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09509">https://arxiv.org/pdf/2503.09509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09509]] ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba(https://arxiv.org/abs/2503.09509)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Visual Mamba networks (ViMs) extend the selective space state model (Mamba) to various vision tasks and demonstrate significant potential. Vector quantization (VQ), on the other hand, decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency to enable ViMs deployment on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.</li>
</ul>

<h3>Title: RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment</h3>
<ul>
<li><strong>Authors: </strong>Md Morshed Alam, Lokesh Chandra Das, Sandip Roy, Sachin Shetty, Weichao Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09513">https://arxiv.org/abs/2503.09513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09513">https://arxiv.org/pdf/2503.09513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09513]] RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment(https://arxiv.org/abs/2503.09513)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>Internet of Things (IoT) platforms with trigger-action capability allow event conditions to trigger actions in IoT devices autonomously by creating a chain of interactions. Adversaries exploit this chain of interactions to maliciously inject fake event conditions into IoT hubs, triggering unauthorized actions on target IoT devices to implement remote injection attacks. Existing defense mechanisms focus mainly on the verification of event transactions using physical event fingerprints to enforce the security policies to block unsafe event transactions. These approaches are designed to provide offline defense against injection attacks. The state-of-the-art online defense mechanisms offer real-time defense, but extensive reliability on the inference of attack impacts on the IoT network limits the generalization capability of these approaches. In this paper, we propose a platform-independent multi-agent online defense system, namely RESTRAIN, to counter remote injection attacks at runtime. RESTRAIN allows the defense agent to profile attack actions at runtime and leverages reinforcement learning to optimize a defense policy that complies with the security requirements of the IoT network. The experimental results show that the defense agent effectively takes real-time defense actions against complex and dynamic remote injection attacks and maximizes the security gain with minimal computational overhead.</li>
</ul>

<h3>Title: CM-Diff: A Single Generative Network for Bidirectional Cross-Modality Translation Diffusion Model Between Infrared and Visible Images</h3>
<ul>
<li><strong>Authors: </strong>Bin Hu, Chenqiang Gao, Shurui Liu, Junjie Guo, Fang Chen, Fangcen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09514">https://arxiv.org/abs/2503.09514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09514">https://arxiv.org/pdf/2503.09514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09514]] CM-Diff: A Single Generative Network for Bidirectional Cross-Modality Translation Diffusion Model Between Infrared and Visible Images(https://arxiv.org/abs/2503.09514)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The image translation method represents a crucial approach for mitigating information deficiencies in the infrared and visible modalities, while also facilitating the enhancement of modality-specific datasets. However, existing methods for infrared and visible image translation either achieve unidirectional modality translation or rely on cycle consistency for bidirectional modality translation, which may result in suboptimal performance. In this work, we present the cross-modality translation diffusion model (CM-Diff) for simultaneously modeling data distributions in both the infrared and visible modalities. We address this challenge by combining translation direction labels for guidance during training with cross-modality feature control. Specifically, we view the establishment of the mapping relationship between the two modalities as the process of learning data distributions and understanding modality differences, achieved through a novel Bidirectional Diffusion Training (BDT) strategy. Additionally, we propose a Statistical Constraint Inference (SCI) strategy to ensure the generated image closely adheres to the data distribution of the target modality. Experimental results demonstrate the superiority of our CM-Diff over state-of-the-art methods, highlighting its potential for generating dual-modality datasets.</li>
</ul>

<h3>Title: Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09516">https://arxiv.org/abs/2503.09516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09516">https://arxiv.org/pdf/2503.09516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09516]] Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning(https://arxiv.org/abs/2503.09516)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at this https URL.</li>
</ul>

<h3>Title: SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Matthew Wearden, Arthur Conmy, Samuel Marks, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09532">https://arxiv.org/abs/2503.09532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09532">https://arxiv.org/pdf/2503.09532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09532]] SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability(https://arxiv.org/abs/2503.09532)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) are a popular technique for interpreting language model activations, and there is extensive recent work on improving SAE effectiveness. However, most prior work evaluates progress using unsupervised proxy metrics with unclear practical relevance. We introduce SAEBench, a comprehensive evaluation suite that measures SAE performance across seven diverse metrics, spanning interpretability, feature disentanglement and practical applications like unlearning. To enable systematic comparison, we open-source a suite of over 200 SAEs across eight recently proposed SAE architectures and training algorithms. Our evaluation reveals that gains on proxy metrics do not reliably translate to better practical performance. For instance, while Matryoshka SAEs slightly underperform on existing proxy metrics, they substantially outperform other architectures on feature disentanglement metrics; moreover, this advantage grows with SAE scale. By providing a standardized framework for measuring progress in SAE development, SAEBench enables researchers to study scaling trends and make nuanced comparisons between different SAE architectures and training methodologies. Our interactive interface enables researchers to flexibly visualize relationships between metrics across hundreds of open-source SAEs at: this https URL</li>
</ul>

<h3>Title: Large Language Models for Multi-Facility Location Mechanism Design</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Thach, Fei Liu, Houyu Zhou, Hau Chan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09533">https://arxiv.org/abs/2503.09533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09533">https://arxiv.org/pdf/2503.09533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09533]] Large Language Models for Multi-Facility Location Mechanism Design(https://arxiv.org/abs/2503.09533)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Designing strategyproof mechanisms for multi-facility location that optimize social costs based on agent preferences had been challenging due to the extensive domain knowledge required and poor worst-case guarantees. Recently, deep learning models have been proposed as alternatives. However, these models require some domain knowledge and extensive hyperparameter tuning as well as lacking interpretability, which is crucial in practice when transparency of the learned mechanisms is mandatory. In this paper, we introduce a novel approach, named LLMMech, that addresses these limitations by incorporating large language models (LLMs) into an evolutionary framework for generating interpretable, hyperparameter-free, empirically strategyproof, and nearly optimal mechanisms. Our experimental results, evaluated on various problem settings where the social cost is arbitrarily weighted across agents and the agent preferences may not be uniformly distributed, demonstrate that the LLM-generated mechanisms generally outperform existing handcrafted baselines and deep learning models. Furthermore, the mechanisms exhibit impressive generalizability to out-of-distribution agent preferences and to larger instances with more agents.</li>
</ul>

<h3>Title: Evaluating Visual Explanations of Attention Maps for Transformer-based Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Minjae Chung, Jong Bum Won, Ganghyun Kim, Yujin Kim, Utku Ozbulak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09535">https://arxiv.org/abs/2503.09535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09535">https://arxiv.org/pdf/2503.09535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09535]] Evaluating Visual Explanations of Attention Maps for Transformer-based Medical Imaging(https://arxiv.org/abs/2503.09535)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability, transformer</a></li>
<li><strong>Abstract: </strong>Although Vision Transformers (ViTs) have recently demonstrated superior performance in medical imaging problems, they face explainability issues similar to previous architectures such as convolutional neural networks. Recent research efforts suggest that attention maps, which are part of decision-making process of ViTs can potentially address the explainability issue by identifying regions influencing predictions, especially in models pretrained with self-supervised learning. In this work, we compare the visual explanations of attention maps to other commonly used methods for medical imaging problems. To do so, we employ four distinct medical imaging datasets that involve the identification of (1) colonic polyps, (2) breast tumors, (3) esophageal inflammation, and (4) bone fractures and hardware implants. Through large-scale experiments on the aforementioned datasets using various supervised and self-supervised pretrained ViTs, we find that although attention maps show promise under certain conditions and generally surpass GradCAM in explainability, they are outperformed by transformer-specific interpretability methods. Our findings indicate that the efficacy of attention maps as a method of interpretability is context-dependent and may be limited as they do not consistently provide the comprehensive insights required for robust medical decision-making.</li>
</ul>

<h3>Title: GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals</h3>
<ul>
<li><strong>Authors: </strong>Shuokang Huang, Julie A. McCann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09537">https://arxiv.org/abs/2503.09537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09537">https://arxiv.org/pdf/2503.09537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09537]] GenHPE: Generative Counterfactuals for 3D Human Pose Estimation with Radio Frequency Signals(https://arxiv.org/abs/2503.09537)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Human pose estimation (HPE) detects the positions of human body joints for various applications. Compared to using cameras, HPE using radio frequency (RF) signals is non-intrusive and more robust to adverse conditions, exploiting the signal variations caused by human interference. However, existing studies focus on single-domain HPE confined by domain-specific confounders, which cannot generalize to new domains and result in diminished HPE performance. Specifically, the signal variations caused by different human body parts are entangled, containing subject-specific confounders. RF signals are also intertwined with environmental noise, involving environment-specific confounders. In this paper, we propose GenHPE, a 3D HPE approach that generates counterfactual RF signals to eliminate domain-specific confounders. GenHPE trains generative models conditioned on human skeleton labels, learning how human body parts and confounders interfere with RF signals. We manipulate skeleton labels (i.e., removing body parts) as counterfactual conditions for generative models to synthesize counterfactual RF signals. The differences between counterfactual signals approximately eliminate domain-specific confounders and regularize an encoder-decoder model to learn domain-independent representations. Such representations help GenHPE generalize to new subjects/environments for cross-domain 3D HPE. We evaluate GenHPE on three public datasets from WiFi, ultra-wideband, and millimeter wave. Experimental results show that GenHPE outperforms state-of-the-art methods and reduces estimation errors by up to 52.2mm for cross-subject HPE and 10.6mm for cross-environment HPE.</li>
</ul>

<h3>Title: TPDiff: Temporal Pyramid Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Lingmin Ran, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09566">https://arxiv.org/abs/2503.09566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09566">https://arxiv.org/pdf/2503.09566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09566]] TPDiff: Temporal Pyramid Video Diffusion Model(https://arxiv.org/abs/2503.09566)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The development of video diffusion models unveils a significant challenge: the substantial computational demands. To mitigate this challenge, we note that the reverse process of diffusion exhibits an inherent entropy-reducing nature. Given the inter-frame redundancy in video modality, maintaining full frame rates in high-entropy stages is unnecessary. Based on this insight, we propose TPDiff, a unified framework to enhance training and inference efficiency. By dividing diffusion into several stages, our framework progressively increases frame rate along the diffusion process with only the last stage operating on full frame rate, thereby optimizing computational efficiency. To train the multi-stage diffusion model, we introduce a dedicated training framework: stage-wise diffusion. By solving the partitioned probability flow ordinary differential equations (ODE) of diffusion under aligned data and noise, our training strategy is applicable to various diffusion forms and further enhances training efficiency. Comprehensive experimental evaluations validate the generality of our method, demonstrating 50% reduction in training cost and 1.5x improvement in inference efficiency.</li>
</ul>

<h3>Title: Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks</h3>
<ul>
<li><strong>Authors: </strong>Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, Amir Gholami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09572">https://arxiv.org/abs/2503.09572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09572">https://arxiv.org/pdf/2503.09572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09572]] Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks(https://arxiv.org/abs/2503.09572)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of the-art 54% success rate on the WebArena-Lite benchmark.</li>
</ul>

<h3>Title: Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Marianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, Volodymyr Kuleshov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09573">https://arxiv.org/abs/2503.09573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09573">https://arxiv.org/pdf/2503.09573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09573]] Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models(https://arxiv.org/abs/2503.09573)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: this https URL</li>
</ul>

<h3>Title: Cost-Optimal Grouped-Query Attention for Long-Context LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yingfa Chen, Yutong Wu, Xu Han, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09579">https://arxiv.org/abs/2503.09579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09579">https://arxiv.org/pdf/2503.09579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09579]] Cost-Optimal Grouped-Query Attention for Long-Context LLMs(https://arxiv.org/abs/2503.09579)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.</li>
</ul>

<h3>Title: Minimax Optimality of the Probability Flow ODE for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Changxiao Cai, Gen Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09583">https://arxiv.org/abs/2503.09583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09583">https://arxiv.org/pdf/2503.09583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09583]] Minimax Optimality of the Probability Flow ODE for Diffusion Models(https://arxiv.org/abs/2503.09583)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Score-based diffusion models have become a foundational paradigm for modern generative modeling, demonstrating exceptional capability in generating samples from complex high-dimensional distributions. Despite the dominant adoption of probability flow ODE-based samplers in practice due to their superior sampling efficiency and precision, rigorous statistical guarantees for these methods have remained elusive in the literature. This work develops the first end-to-end theoretical framework for deterministic ODE-based samplers that establishes near-minimax optimal guarantees under mild assumptions on target data distributions. Specifically, focusing on subgaussian distributions with $\beta$-Hölder smooth densities for $\beta\leq 2$, we propose a smooth regularized score estimator that simultaneously controls both the $L^2$ score error and the associated mean Jacobian error. Leveraging this estimator within a refined convergence analysis of the ODE-based sampling process, we demonstrate that the resulting sampler achieves the minimax rate in total variation distance, modulo logarithmic factors. Notably, our theory comprehensively accounts for all sources of error in the sampling process and does not require strong structural conditions such as density lower bounds or Lipschitz/smooth scores on target distributions, thereby covering a broad range of practical data distributions.</li>
</ul>

<h3>Title: BIMBA: Selective-Scan Compression for Long-Range Video Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, Lorenzo Torresani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09590">https://arxiv.org/abs/2503.09590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09590">https://arxiv.org/pdf/2503.09590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09590]] BIMBA: Selective-Scan Compression for Long-Range Video Question Answering(https://arxiv.org/abs/2503.09590)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at this https URL.</li>
</ul>

<h3>Title: SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment</h3>
<ul>
<li><strong>Authors: </strong>Katrin Renz, Long Chen, Elahe Arani, Oleg Sinavski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09594">https://arxiv.org/abs/2503.09594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09594">https://arxiv.org/pdf/2503.09594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09594]] SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment(https://arxiv.org/abs/2503.09594)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Integrating large language models (LLMs) into autonomous driving has attracted significant attention with the hope of improving generalization and explainability. However, existing methods often focus on either driving or vision-language understanding but achieving both high driving performance and extensive language understanding remains challenging. In addition, the dominant approach to tackle vision-language understanding is using visual question answering. However, for autonomous driving, this is only useful if it is aligned with the action space. Otherwise, the model's answers could be inconsistent with its behavior. Therefore, we propose a model that can handle three different tasks: (1) closed-loop driving, (2) vision-language understanding, and (3) language-action alignment. Our model SimLingo is based on a vision language model (VLM) and works using only camera, excluding expensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on the widely used CARLA simulator on the Bench2Drive benchmark and is the winning entry at the CARLA challenge 2024. Additionally, we achieve strong results in a wide variety of language-related tasks while maintaining high driving performance.</li>
</ul>

<h3>Title: PISA Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Li, Oscar Michel, Xichen Pan, Sainan Liu, Mike Roberts, Saining Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09595">https://arxiv.org/abs/2503.09595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09595">https://arxiv.org/pdf/2503.09595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09595]] PISA Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop(https://arxiv.org/abs/2503.09595)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large-scale pre-trained video generation models excel in content creation but are not reliable as physically accurate world simulators out of the box. This work studies the process of post-training these models for accurate world modeling through the lens of the simple, yet fundamental, physics task of modeling object freefall. We show state-of-the-art video generation models struggle with this basic task, despite their visually impressive outputs. To remedy this problem, we find that fine-tuning on a relatively small amount of simulated videos is effective in inducing the dropping behavior in the model, and we can further improve results through a novel reward modeling procedure we introduce. Our study also reveals key limitations of post-training in generalization and distribution modeling. Additionally, we release a benchmark for this task that may serve as a useful diagnostic tool for tracking physical accuracy in large-scale video generative model development.</li>
</ul>

<h3>Title: How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Ruohao Guo, Wei Xu, Alan Ritter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09598">https://arxiv.org/abs/2503.09598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09598">https://arxiv.org/pdf/2503.09598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09598]] How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation(https://arxiv.org/abs/2503.09598)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are widely deployed in diverse scenarios, the extent to which they could tacitly spread misinformation emerges as a critical safety concern. Current research primarily evaluates LLMs on explicit false statements, overlooking how misinformation often manifests subtly as unchallenged premises in real-world user interactions. We curated ECHOMIST, the first comprehensive benchmark for implicit misinformation, where the misinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based on rigorous selection criteria and carefully curated data from diverse sources, including real-world human-AI conversations and social media interactions. We also introduce a new evaluation metric to measure whether LLMs can recognize and counter false information rather than amplify users' misconceptions. Through an extensive empirical study on a wide range of LLMs, including GPT-4, Claude, and Llama, we find that current models perform alarmingly poorly on this task, often failing to detect false premises and generating misleading explanations. Our findings underscore the critical need for an increased focus on implicit misinformation in LLM safety research.</li>
</ul>

<h3>Title: MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System</h3>
<ul>
<li><strong>Authors: </strong>Jihao Zhao, Zhiyuan Ji, Zhaoxin Fan, Hanyu Wang, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09600">https://arxiv.org/abs/2503.09600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09600">https://arxiv.org/pdf/2503.09600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09600]] MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System(https://arxiv.org/abs/2503.09600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.</li>
</ul>

<h3>Title: RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling</h3>
<ul>
<li><strong>Authors: </strong>Itay Chachy, Guy Yariv, Sagie Benaim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.09601">https://arxiv.org/abs/2503.09601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.09601">https://arxiv.org/pdf/2503.09601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.09601]] RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling(https://arxiv.org/abs/2503.09601)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Score Distillation Sampling (SDS) has emerged as an effective technique for leveraging 2D diffusion priors for tasks such as text-to-3D generation. While powerful, SDS struggles with achieving fine-grained alignment to user intent. To overcome this, we introduce RewardSDS, a novel approach that weights noise samples based on alignment scores from a reward model, producing a weighted SDS loss. This loss prioritizes gradients from noise samples that yield aligned high-reward output. Our approach is broadly applicable and can extend SDS-based methods. In particular, we demonstrate its applicability to Variational Score Distillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and RewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks, showing significant improvements over SDS and VSD on a diverse set of metrics measuring generation quality and alignment to desired reward models, enabling state-of-the-art performance. Project page is available at https://itaychachy. this http URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
