<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Combining Blockchain and IOT for Decentralized Healthcare Data Management. (arXiv:2304.00127v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00127">http://arxiv.org/abs/2304.00127</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00127] Combining Blockchain and IOT for Decentralized Healthcare Data Management](http://arxiv.org/abs/2304.00127) #secure</code></li>
<li>Summary: <p>The emergence of the Internet of Things (IoT) has resulted in a significant
increase in research on e-health. As the amount of patient data grows, it has
become increasingly challenging to protect patients' privacy. Patient data is
commonly stored in the cloud, making it difficult for users to control and
protect their information. Moreover, the recent rise in security and
surveillance breaches in the healthcare industry has highlighted the need for a
better approach to data storage and protection. Traditional models that rely on
third-party control over patients' healthcare data are no longer reliable, as
they have proven vulnerable to security breaches. To address these issues,
blockchain technology has emerged as a promising solution. Blockchain-based
protocols have the potential to provide a secure and efficient system for
e-health applications that does not require trust in third-party
intermediaries. The proposed protocol outlined in this paper uses a
blockchain-based approach to manage patient data securely and efficiently.
Unlike Bitcoin, which is primarily used for financial transactions, the
protocol described here is designed specifically for e-health applications. It
employs a consensus mechanism that is more suitable for resource constrained
IoT devices, thereby reducing network costs and increasing efficiency. The
proposed protocol also provides a privacy-preserving access control mechanism
that enables patients to have more control over their healthcare data. By
leveraging blockchain technology, the protocol ensures that only authorized
individuals can access the patient's data, which helps prevent data breaches
and other security issues. Finally, the security and privacy of the proposed
protocol are analysed to ensure that it meets the necessary standards for data
protection.
</p></li>
</ul>

<h3>Title: Secure Federated Learning against Model Poisoning Attacks via Client Filtering. (arXiv:2304.00160v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00160">http://arxiv.org/abs/2304.00160</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00160] Secure Federated Learning against Model Poisoning Attacks via Client Filtering](http://arxiv.org/abs/2304.00160) #secure</code></li>
<li>Summary: <p>Given the distributed nature, detecting and defending against the backdoor
attack under federated learning (FL) systems is challenging. In this paper, we
observe that the cosine similarity of the last layer's weight between the
global model and each local update could be used effectively as an indicator of
malicious model updates. Therefore, we propose CosDefense, a
cosine-similarity-based attacker detection algorithm. Specifically, under
CosDefense, the server calculates the cosine similarity score of the last
layer's weight between the global model and each client update, labels
malicious clients whose score is much higher than the average, and filters them
out of the model aggregation in each round. Compared to existing defense
schemes, CosDefense does not require any extra information besides the received
model updates to operate and is compatible with client sampling. Experiment
results on three real-world datasets demonstrate that CosDefense could provide
robust performance under the state-of-the-art FL poisoning attack.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00083">http://arxiv.org/abs/2304.00083</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00083] Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE](http://arxiv.org/abs/2304.00083) #security</code></li>
<li>Summary: <p>The growing popularity of Machine Learning (ML) has led to its deployment in
various sensitive domains, which has resulted in significant research focused
on ML security and privacy. However, in some applications, such as autonomous
driving, integrity verification of the outsourced ML workload is more
critical-a facet that has not received much attention. Existing solutions, such
as multi-party computation and proof-based systems, impose significant
computation overhead, which makes them unfit for real-time applications. We
propose Fides, a novel framework for real-time validation of outsourced ML
workloads. Fides features a novel and efficient distillation technique-Greedy
Distillation Transfer Learning-that dynamically distills and fine-tunes a space
and compute-efficient verification model for verifying the corresponding
service model while running inside a trusted execution environment. Fides
features a client-side attack detection model that uses statistical analysis
and divergence measurements to identify, with a high likelihood, if the service
model is under attack. Fides also offers a re-classification functionality that
predicts the original class whenever an attack is identified. We devised a
generative adversarial network framework for training the attack detection and
re-classification models. The extensive evaluation shows that Fides achieves an
accuracy of up to 98% for attack detection and 94% for re-classification.
</p></li>
</ul>

<h3>Title: Pointcheval-Sanders Signature-Based Synchronized Aggregate Signature. (arXiv:2304.00265v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00265">http://arxiv.org/abs/2304.00265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00265] Pointcheval-Sanders Signature-Based Synchronized Aggregate Signature](http://arxiv.org/abs/2304.00265) #security</code></li>
<li>Summary: <p>Synchronized aggregate signature is a special type of signature that all
signers have a synchronized time period and allows aggregating signatures which
are generated in the same period. This signature has a wide range of
applications for systems that have a natural reporting period such as log and
sensor data, or blockchain protocol. In CT-RSA 2016, Pointcheval and Sanders
proposed the new randomizable signature scheme. Since this signature scheme is
based on type-3 pairing, this signature achieves a short signature size and
efficient signature verification. In this paper, we design the
Pointchcval-Sanders signature-based synchronized aggregate signature scheme and
prove its security under the generalized Pointcheval-Sanders assumption in the
random oracle model. Our scheme offers the most efficient aggregate signature
verification among synchronized aggregate signature schemes based on bilinear
groups.
</p></li>
</ul>

<h3>Title: DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. (arXiv:2304.00409v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00409">http://arxiv.org/abs/2304.00409</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00409] DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection](http://arxiv.org/abs/2304.00409) #security</code></li>
<li>Summary: <p>We propose and release a new vulnerable source code dataset. We curate the
dataset by crawling security issue websites, extracting vulnerability-fixing
commits and source codes from the corresponding projects. Our new dataset
contains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerable
functions extracted from 7,861 commits. Our dataset covers 305 more projects
than all previous datasets combined. We show that increasing the diversity and
volume of training data improves the performance of deep learning models for
vulnerability detection.
</p></li>
</ul>

<p>Combining our new dataset with previous datasets, we present an analysis of
the challenges and promising research directions of using deep learning for
detecting software vulnerabilities. We study 11 model architectures belonging
to 4 families. Our results show that deep learning is still not ready for
vulnerability detection, due to high false positive rate, low F1 score, and
difficulty of detecting hard CWEs. In particular, we demonstrate an important
generalization challenge for the deployment of deep learning-based models.
</p>
<p>However, we also identify hopeful future research directions. We demonstrate
that large language models (LLMs) are the future for vulnerability detection,
outperforming Graph Neural Networks (GNNs) with manual feature engineering.
Moreover, developing source code specific pre-training objectives is a
promising research direction to improve the vulnerability detection
performance.
</p>

<h2>privacy</h2>
<h3>Title: When Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus. (arXiv:2304.00350v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00350">http://arxiv.org/abs/2304.00350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00350] When Crowd Meets Persona: Creating a Large-Scale Open-Domain Persona Dialogue Corpus](http://arxiv.org/abs/2304.00350) #privacy</code></li>
<li>Summary: <p>Building a natural language dataset requires caution since word semantics is
vulnerable to subtle text change or the definition of the annotated concept.
Such a tendency can be seen in generative tasks like question-answering and
dialogue generation and also in tasks that create a categorization-based
corpus, like topic classification or sentiment analysis. Open-domain
conversations involve two or more crowdworkers freely conversing about any
topic, and collecting such data is particularly difficult for two reasons: 1)
the dataset should be <code>crafted" rather than</code>obtained" due to privacy
concerns, and 2) paid creation of such dialogues may differ from how
crowdworkers behave in real-world settings. In this study, we tackle these
issues when creating a large-scale open-domain persona dialogue corpus, where
persona implies that the conversation is performed by several actors with a
fixed persona and user-side workers from an unspecified crowd.
</p></li>
</ul>

<h3>Title: PEOPL: Characterizing Privately Encoded Open Datasets with Public Labels. (arXiv:2304.00047v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00047">http://arxiv.org/abs/2304.00047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00047] PEOPL: Characterizing Privately Encoded Open Datasets with Public Labels](http://arxiv.org/abs/2304.00047) #privacy</code></li>
<li>Summary: <p>Allowing organizations to share their data for training of machine learning
(ML) models without unintended information leakage is an open problem in
practice. A promising technique for this still-open problem is to train models
on the encoded data. Our approach, called Privately Encoded Open Datasets with
Public Labels (PEOPL), uses a certain class of randomly constructed transforms
to encode sensitive data. Organizations publish their randomly encoded data and
associated raw labels for ML training, where training is done without knowledge
of the encoding realization. We investigate several important aspects of this
problem: We introduce information-theoretic scores for privacy and utility,
which quantify the average performance of an unfaithful user (e.g., adversary)
and a faithful user (e.g., model developer) that have access to the published
encoded data. We then theoretically characterize primitives in building
families of encoding schemes that motivate the use of random deep neural
networks. Empirically, we compare the performance of our randomized encoding
scheme and a linear scheme to a suite of computational attacks, and we also
show that our scheme achieves competitive prediction accuracy to raw-sample
baselines. Moreover, we demonstrate that multiple institutions, using
independent random encoders, can collaborate to train improved ML models.
</p></li>
</ul>

<h3>Title: Scalable and Privacy-Preserving Federated Principal Component Analysis. (arXiv:2304.00129v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00129">http://arxiv.org/abs/2304.00129</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00129] Scalable and Privacy-Preserving Federated Principal Component Analysis](http://arxiv.org/abs/2304.00129) #privacy</code></li>
<li>Summary: <p>Principal component analysis (PCA) is an essential algorithm for
dimensionality reduction in many data science domains. We address the problem
of performing a federated PCA on private data distributed among multiple data
providers while ensuring data confidentiality. Our solution, SF-PCA, is an
end-to-end secure system that preserves the confidentiality of both the
original data and all intermediate results in a passive-adversary model with up
to all-but-one colluding parties. SF-PCA jointly leverages multiparty
homomorphic encryption, interactive protocols, and edge computing to
efficiently interleave computations on local cleartext data with operations on
collectively encrypted data. SF-PCA obtains results as accurate as non-secure
centralized solutions, independently of the data distribution among the
parties. It scales linearly or better with the dataset dimensions and with the
number of data providers. SF-PCA is more precise than existing approaches that
approximate the solution by combining local analysis results, and between 3x
and 250x faster than privacy-preserving alternatives based solely on secure
multiparty computation or homomorphic encryption. Our work demonstrates the
practical applicability of secure and federated PCA on private distributed
datasets.
</p></li>
</ul>

<h3>Title: Data Privacy Preservation on the Internet of Things. (arXiv:2304.00258v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00258">http://arxiv.org/abs/2304.00258</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00258] Data Privacy Preservation on the Internet of Things](http://arxiv.org/abs/2304.00258) #privacy</code></li>
<li>Summary: <p>Recent developments in hardware and information technology have enabled the
emergence of billions of connected, intelligent devices around the world
exchanging information with minimal human involvement. This paradigm, known as
the Internet of Things (IoT) is progressing quickly with an estimated 27
billion devices by 2025. This growth in the number of IoT devices and
successful IoT services has generated a tremendous amount of data. However,
this humongous volume of data poses growing concerns for user privacy. This
introductory chapter has presented a brief survey of some of the existing data
privacy-preservation schemes proposed by researchers in the field of the
Internet of Things.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00252">http://arxiv.org/abs/2304.00252</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00252] Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning](http://arxiv.org/abs/2304.00252) #protect</code></li>
<li>Summary: <p>A backdoor attack allows a malicious user to manipulate the environment or
corrupt the training data, thus inserting a backdoor into the trained agent.
Such attacks compromise the RL system's reliability, leading to potentially
catastrophic results in various key fields. In contrast, relatively limited
research has investigated effective defenses against backdoor attacks in RL.
This paper proposes the Recovery Triggered States (RTS) method, a novel
approach that effectively protects the victim agents from backdoor attacks. RTS
involves building a surrogate network to approximate the dynamics model.
Developers can then recover the environment from the triggered state to a clean
state, thereby preventing attackers from activating backdoors hidden in the
agent by presenting the trigger. When training the surrogate to predict states,
we incorporate agent action information to reduce the discrepancy between the
actions taken by the agent on predicted states and the actions taken on real
states. RTS is the first approach to defend against backdoor attacks in a
single-agent setting. Our results show that using RTS, the cumulative reward
only decreased by 1.41% under the backdoor attack.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Improving Fast Adversarial Training with Prior-Guided Knowledge. (arXiv:2304.00202v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00202">http://arxiv.org/abs/2304.00202</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00202] Improving Fast Adversarial Training with Prior-Guided Knowledge](http://arxiv.org/abs/2304.00202) #attack</code></li>
<li>Summary: <p>Fast adversarial training (FAT) is an efficient method to improve robustness.
However, the original FAT suffers from catastrophic overfitting, which
dramatically and suddenly reduces robustness after a few training epochs.
Although various FAT variants have been proposed to prevent overfitting, they
require high training costs. In this paper, we investigate the relationship
between adversarial example quality and catastrophic overfitting by comparing
the training processes of standard adversarial training and FAT. We find that
catastrophic overfitting occurs when the attack success rate of adversarial
examples becomes worse. Based on this observation, we propose a positive
prior-guided adversarial initialization to prevent overfitting by improving
adversarial example quality without extra training costs. This initialization
is generated by using high-quality adversarial perturbations from the
historical training process. We provide theoretical analysis for the proposed
initialization and propose a prior-guided regularization method that boosts the
smoothness of the loss function. Additionally, we design a prior-guided
ensemble FAT method that averages the different model weights of historical
models using different decay rates. Our proposed method, called FGSM-PGK,
assembles the prior-guided knowledge, i.e., the prior-guided initialization and
model weights, acquired during the historical training process. Evaluations of
four datasets demonstrate the superiority of the proposed method.
</p></li>
</ul>

<h3>Title: Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space. (arXiv:2304.00436v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00436">http://arxiv.org/abs/2304.00436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00436] Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space](http://arxiv.org/abs/2304.00436) #attack</code></li>
<li>Summary: <p>Malicious perturbations embedded in input data, known as Trojan attacks, can
cause neural networks to misbehave. However, the impact of a Trojan attack is
reduced during fine-tuning of the model, which involves transferring knowledge
from a pretrained large-scale model like visual question answering (VQA) to the
target model. To mitigate the effects of a Trojan attack, replacing and
fine-tuning multiple layers of the pretrained model is possible. This research
focuses on sample efficiency, stealthiness and variation, and robustness to
model fine-tuning. To address these challenges, we propose an instance-level
Trojan attack that generates diverse Trojans across input samples and
modalities. Adversarial learning establishes a correlation between a specified
perturbation layer and the misbehavior of the fine-tuned model. We conducted
extensive experiments on the VQA-v2 dataset using a range of metrics. The
results show that our proposed method can effectively adapt to a fine-tuned
model with minimal samples. Specifically, we found that a model with a single
fine-tuning layer can be compromised using a single shot of adversarial
samples, while a model with more fine-tuning layers can be compromised using
only a few shots.
</p></li>
</ul>

<h3>Title: Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias. (arXiv:2304.00010v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00010">http://arxiv.org/abs/2304.00010</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00010] Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias](http://arxiv.org/abs/2304.00010) #attack</code></li>
<li>Summary: <p>It has become cognitive inertia to employ cross-entropy loss function in
classification related tasks. In the untargeted attacks on graph structure, the
gradients derived from the attack objective are the attacker's basis for
evaluating a perturbation scheme. Previous methods use negative cross-entropy
loss as the attack objective in attacking node-level classification models.
However, the suitability of the cross-entropy function for constructing the
untargeted attack objective has yet been discussed in previous works. This
paper argues about the previous unreasonable attack objective from the
perspective of budget allocation. We demonstrate theoretically and empirically
that negative cross-entropy tends to produce more significant gradients from
nodes with lower confidence in the labeled classes, even if the predicted
classes of these nodes have been misled. To free up these inefficient attack
budgets, we propose a simple attack model for untargeted attacks on graph
structure based on a novel attack objective which generates unweighted
gradients on graph structures that are not affected by the node confidence. By
conducting experiments in gray-box poisoning attack scenarios, we demonstrate
that a reasonable budget allocation can significantly improve the effectiveness
of gradient-based edge perturbations without any extra hyper-parameter.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Weakly-Supervised Text-driven Contrastive Learning for Facial Behavior Understanding. (arXiv:2304.00058v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00058">http://arxiv.org/abs/2304.00058</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00058] Weakly-Supervised Text-driven Contrastive Learning for Facial Behavior Understanding](http://arxiv.org/abs/2304.00058) #robust</code></li>
<li>Summary: <p>Contrastive learning has shown promising potential for learning robust
representations by utilizing unlabeled data. However, constructing effective
positive-negative pairs for contrastive learning on facial behavior datasets
remains challenging. This is because such pairs inevitably encode the
subject-ID information, and the randomly constructed pairs may push similar
facial images away due to the limited number of subjects in facial behavior
datasets. To address this issue, we propose to utilize activity descriptions,
coarse-grained information provided in some datasets, which can provide
high-level semantic information about the image sequences but is often
neglected in previous studies. More specifically, we introduce a two-stage
Contrastive Learning with Text-Embeded framework for Facial behavior
understanding (CLEF). The first stage is a weakly-supervised contrastive
learning method that learns representations from positive-negative pairs
constructed using coarse-grained activity information. The second stage aims to
train the recognition of facial expressions or facial action units by
maximizing the similarity between image and the corresponding text label names.
The proposed CLEF achieves state-of-the-art performance on three in-the-lab
datasets for AU recognition and three in-the-wild datasets for facial
expression recognition.
</p></li>
</ul>

<h3>Title: SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail. (arXiv:2304.00101v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00101">http://arxiv.org/abs/2304.00101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00101] SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail](http://arxiv.org/abs/2304.00101) #robust</code></li>
<li>Summary: <p>Modern image classifiers perform well on populated classes, while degrading
considerably on tail classes with only a few instances. Humans, by contrast,
effortlessly handle the long-tailed recognition challenge, since they can learn
the tail representation based on different levels of semantic abstraction,
making the learned tail features more discriminative. This phenomenon motivated
us to propose SuperDisco, an algorithm that discovers super-class
representations for long-tailed recognition using a graph model. We learn to
construct the super-class graph to guide the representation learning to deal
with long-tailed distributions. Through message passing on the super-class
graph, image representations are rectified and refined by attending to the most
relevant entities based on the semantic similarity among their super-classes.
Moreover, we propose to meta-learn the super-class graph under the supervision
of a prototype graph constructed from a small amount of imbalanced data. By
doing so, we obtain a more robust super-class graph that further improves the
long-tailed recognition performance. The consistent state-of-the-art
experiments on the long-tailed CIFAR-100, ImageNet, Places and iNaturalist
demonstrate the benefit of the discovered super-class graph for dealing with
long-tailed distributions.
</p></li>
</ul>

<h3>Title: Deep Factor Model: A Novel Approach for Motion Compensated Multi-Dimensional MRI. (arXiv:2304.00102v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00102">http://arxiv.org/abs/2304.00102</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00102] Deep Factor Model: A Novel Approach for Motion Compensated Multi-Dimensional MRI](http://arxiv.org/abs/2304.00102) #robust</code></li>
<li>Summary: <p>Recent quantitative parameter mapping methods including MR fingerprinting
(MRF) collect a time series of images that capture the evolution of
magnetization. The focus of this work is to introduce a novel approach termed
as Deep Factor Model(DFM), which offers an efficient representation of the
multi-contrast image time series. The higher efficiency of the representation
enables the acquisition of the images in a highly undersampled fashion, which
translates to reduced scan time in 3D high-resolution multi-contrast
applications. The approach integrates motion estimation and compensation,
making the approach robust to subject motion during the scan.
</p></li>
</ul>

<h3>Title: GLT-T++: Global-Local Transformer for 3D Siamese Tracking with Ranking Loss. (arXiv:2304.00242v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00242">http://arxiv.org/abs/2304.00242</a></li>
<li>Code URL: <a href="https://github.com/haooozi/glt-t">https://github.com/haooozi/glt-t</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00242] GLT-T++: Global-Local Transformer for 3D Siamese Tracking with Ranking Loss](http://arxiv.org/abs/2304.00242) #robust</code></li>
<li>Summary: <p>Siamese trackers based on 3D region proposal network (RPN) have shown
remarkable success with deep Hough voting. However, using a single seed point
feature as the cue for voting fails to produce high-quality 3D proposals.
Additionally, the equal treatment of seed points in the voting process,
regardless of their significance, exacerbates this limitation. To address these
challenges, we propose a novel transformer-based voting scheme to generate
better proposals. Specifically, a global-local transformer (GLT) module is
devised to integrate object- and patch-aware geometric priors into seed point
features, resulting in robust and accurate cues for offset learning of seed
points. To train the GLT module, we introduce an importance prediction branch
that learns the potential importance weights of seed points as a training
constraint. Incorporating this transformer-based voting scheme into 3D RPN, a
novel Siamese method dubbed GLT-T is developed for 3D single object tracking on
point clouds. Moreover, we identify that the highest-scored proposal in the
Siamese paradigm may not be the most accurate proposal, which limits tracking
performance. Towards this concern, we approach the binary score prediction task
as a ranking problem, and design a target-aware ranking loss and a
localization-aware ranking loss to produce accurate ranking of proposals. With
the ranking losses, we further present GLT-T++, an enhanced version of GLT-T.
Extensive experiments on multiple benchmarks demonstrate that our GLT-T and
GLT-T++ outperform state-of-the-art methods in terms of tracking accuracy while
maintaining a real-time inference speed. The source code will be made available
at https://github.com/haooozi/GLT-T.
</p></li>
</ul>

<h3>Title: Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case. (arXiv:2304.00025v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00025">http://arxiv.org/abs/2304.00025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00025] Demo Alleviate: Demonstrating Artificial Intelligence Enabled Virtual Assistance for Telehealth: The Mental Health Case](http://arxiv.org/abs/2304.00025) #robust</code></li>
<li>Summary: <p>After the pandemic, artificial intelligence (AI) powered support for mental
health care has become increasingly important. The breadth and complexity of
significant challenges required to provide adequate care involve: (a)
Personalized patient understanding, (b) Safety-constrained and medically
validated chatbot patient interactions, and (c) Support for continued
feedback-based refinements in design using chatbot-patient interactions. We
propose Alleviate, a chatbot designed to assist patients suffering from mental
health challenges with personalized care and assist clinicians with
understanding their patients better. Alleviate draws from an array of publicly
available clinically valid mental-health texts and databases, allowing
Alleviate to make medically sound and informed decisions. In addition,
Alleviate's modular design and explainable decision-making lends itself to
robust and continued feedback-based refinements to its design. In this paper,
we explain the different modules of Alleviate and submit a short video
demonstrating Alleviate's capabilities to help patients and clinicians
understand each other better to facilitate optimal care strategies.
</p></li>
</ul>

<h3>Title: A robust deep learning-based damage identification approach for SHM considering missing data. (arXiv:2304.00040v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00040">http://arxiv.org/abs/2304.00040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00040] A robust deep learning-based damage identification approach for SHM considering missing data](http://arxiv.org/abs/2304.00040) #robust</code></li>
<li>Summary: <p>Data-driven method for Structural Health Monitoring (SHM), that mine the
hidden structural performance from the correlations among monitored time series
data, has received widely concerns recently. However, missing data
significantly impacts the conduction of this method. Missing data is a
frequently encountered issue in time series data in SHM and many other
real-world applications, that harms to the standardized data mining and
downstream tasks, such as condition assessment. Imputation approaches based on
spatiotemporal relations among monitoring data are developed to handle this
issue, however, no additional information is added during imputation. This
paper thus develops a robust method for damage identification that considers
the missing data occasions, based on long-short term memory (LSTM) model and
dropout mechanism in the autoencoder (AE) framework. Inputs channels are
randomly dropped to simulate the missing data in training, and reconstruction
errors are used as the loss function and the damage indicator. Quasi-static
response (cable tension) of a cable-stayed bridge released in 1st IPC-SHM is
employed to verify this proposed method, and results show that the missing data
imputation and damage identification can be implemented together in a unified
way.
</p></li>
</ul>

<h3>Title: To be Robust and to be Fair: Aligning Fairness with Robustness. (arXiv:2304.00061v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00061">http://arxiv.org/abs/2304.00061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00061] To be Robust and to be Fair: Aligning Fairness with Robustness](http://arxiv.org/abs/2304.00061) #robust</code></li>
<li>Summary: <p>Adversarial training has been shown to be reliable in improving robustness
against adversarial samples. However, the problem of adversarial training in
terms of fairness has not yet been properly studied, and the relationship
between fairness and accuracy attack still remains unclear. Can we
simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle
this topic, in this paper, we study the problem of adversarial training and
adversarial attack w.r.t. both metrics. We propose a unified structure for
fairness attack which brings together common notions in group fairness, and we
theoretically prove the equivalence of fairness attack against different
notions. Moreover, we show the alignment of fairness and accuracy attack, and
theoretically demonstrate that robustness w.r.t. one metric benefits from
robustness w.r.t. the other metric. Our study suggests a novel way to unify
adversarial training and attack w.r.t. fairness and accuracy, and experimental
results show that our proposed method achieves better performance in terms of
robustness w.r.t. both metrics.
</p></li>
</ul>

<h3>Title: On Context Distribution Shift in Task Representation Learning for Offline Meta RL. (arXiv:2304.00354v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00354">http://arxiv.org/abs/2304.00354</a></li>
<li>Code URL: <a href="https://github.com/zjlab-ammi/hs-omrl">https://github.com/zjlab-ammi/hs-omrl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00354] On Context Distribution Shift in Task Representation Learning for Offline Meta RL](http://arxiv.org/abs/2304.00354) #robust</code></li>
<li>Summary: <p>Offline meta reinforcement learning (OMRL) aims to learn transferrable
knowledge from offline datasets to facilitate the learning process for new
target tasks. Context-based RL employs a context encoder to rapidly adapt the
agent to new tasks by inferring about the task representation, and then
adjusting the acting policy based on the inferred task representation. Here we
consider context-based OMRL, in particular, the issue of task representation
learning for OMRL. We empirically demonstrate that the context encoder trained
on offline datasets could suffer from distribution shift between the contexts
used for training and testing. To tackle this issue, we propose a hard sampling
based strategy for learning a robust task context encoder. Experimental
results, based on distinct continuous control tasks, demonstrate that the
utilization of our technique results in more robust task representations and
better testing performance in terms of accumulated returns, compared with
baseline methods. Our code is available at
https://github.com/ZJLAB-AMMI/HS-OMRL.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Identifying Symptoms of Delirium from Clinical Narratives Using Natural Language Processing. (arXiv:2304.00111v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00111">http://arxiv.org/abs/2304.00111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00111] Identifying Symptoms of Delirium from Clinical Narratives Using Natural Language Processing](http://arxiv.org/abs/2304.00111) #extraction</code></li>
<li>Summary: <p>Delirium is an acute decline or fluctuation in attention, awareness, or other
cognitive function that can lead to serious adverse outcomes. Despite the
severe outcomes, delirium is frequently unrecognized and uncoded in patients'
electronic health records (EHRs) due to its transient and diverse nature.
Natural language processing (NLP), a key technology that extracts medical
concepts from clinical narratives, has shown great potential in studies of
delirium outcomes and symptoms. To assist in the diagnosis and phenotyping of
delirium, we formed an expert panel to categorize diverse delirium symptoms,
composed annotation guidelines, created a delirium corpus with diverse delirium
symptoms, and developed NLP methods to extract delirium symptoms from clinical
notes. We compared 5 state-of-the-art transformer models including 2 models
(BERT and RoBERTa) from the general domain and 3 models (BERT_MIMIC,
RoBERTa_MIMIC, and GatorTron) from the clinical domain. GatorTron achieved the
best strict and lenient F1 scores of 0.8055 and 0.8759, respectively. We
conducted an error analysis to identify challenges in annotating delirium
symptoms and developing NLP systems. To the best of our knowledge, this is the
first large language model-based delirium symptom extraction system. Our study
lays the foundation for the future development of computable phenotypes and
diagnosis methods for delirium.
</p></li>
</ul>

<h3>Title: Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using Transformer-based Natural Language Processing Methods. (arXiv:2304.00115v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00115">http://arxiv.org/abs/2304.00115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00115] Extracting Thyroid Nodules Characteristics from Ultrasound Reports Using Transformer-based Natural Language Processing Methods](http://arxiv.org/abs/2304.00115) #extraction</code></li>
<li>Summary: <p>The ultrasound characteristics of thyroid nodules guide the evaluation of
thyroid cancer in patients with thyroid nodules. However, the characteristics
of thyroid nodules are often documented in clinical narratives such as
ultrasound reports. Previous studies have examined natural language processing
(NLP) methods in extracting a limited number of characteristics (<9) using
rule-based NLP systems. In this study, a multidisciplinary team of NLP experts
and thyroid specialists, identified thyroid nodule characteristics that are
important for clinical care, composed annotation guidelines, developed a
corpus, and compared 5 state-of-the-art transformer-based NLP methods,
including BERT, RoBERTa, LongFormer, DeBERTa, and GatorTron, for extraction of
thyroid nodule characteristics from ultrasound reports. Our GatorTron model, a
transformer-based large language model trained using over 90 billion words of
text, achieved the best strict and lenient F1-score of 0.8851 and 0.9495 for
the extraction of a total number of 16 thyroid nodule characteristics, and
0.9321 for linking characteristics to nodules, outperforming other clinical
transformer models. To the best of our knowledge, this is the first study to
systematically categorize and apply transformer-based NLP models to extract a
large number of clinical relevant thyroid nodule characteristics from
ultrasound reports. This study lays ground for assessing the documentation
quality of thyroid ultrasound reports and examining outcomes of patients with
thyroid nodules using electronic health records.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Fair-CDA: Continuous and Directional Augmentation for Group Fairness. (arXiv:2304.00295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00295">http://arxiv.org/abs/2304.00295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00295] Fair-CDA: Continuous and Directional Augmentation for Group Fairness](http://arxiv.org/abs/2304.00295) #fair</code></li>
<li>Summary: <p>In this work, we propose {\it Fair-CDA}, a fine-grained data augmentation
strategy for imposing fairness constraints. We use a feature disentanglement
method to extract the features highly related to the sensitive attributes. Then
we show that group fairness can be achieved by regularizing the models on
transition paths of sensitive features between groups. By adjusting the
perturbation strength in the direction of the paths, our proposed augmentation
is controllable and auditable. To alleviate the accuracy degradation caused by
fairness constraints, we further introduce a calibrated model to impute labels
for the augmented data. Our proposed method does not assume any data generative
model and ensures good generalization for both accuracy and fairness.
Experimental results show that Fair-CDA consistently outperforms
state-of-the-art methods on widely-used benchmarks, e.g., Adult, CelebA and
MovieLens. Especially, Fair-CDA obtains an 86.3\% relative improvement for
fairness while maintaining the accuracy on the Adult dataset. Moreover, we
evaluate Fair-CDA in an online recommendation system to demonstrate the
effectiveness of our method in terms of accuracy and fairness.
</p></li>
</ul>

<h3>Title: Predictive Heterogeneity: Measures and Applications. (arXiv:2304.00305v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00305">http://arxiv.org/abs/2304.00305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00305] Predictive Heterogeneity: Measures and Applications](http://arxiv.org/abs/2304.00305) #fair</code></li>
<li>Summary: <p>As an intrinsic and fundamental property of big data, data heterogeneity
exists in a variety of real-world applications, such as precision medicine,
autonomous driving, financial applications, etc. For machine learning
algorithms, the ignorance of data heterogeneity will greatly hurt the
generalization performance and the algorithmic fairness, since the prediction
mechanisms among different sub-populations are likely to differ from each
other. In this work, we focus on the data heterogeneity that affects the
prediction of machine learning models, and firstly propose the \emph{usable
predictive heterogeneity}, which takes into account the model capacity and
computational constraints. We prove that it can be reliably estimated from
finite data with probably approximately correct (PAC) bounds. Additionally, we
design a bi-level optimization algorithm to explore the usable predictive
heterogeneity from data. Empirically, the explored heterogeneity provides
insights for sub-population divisions in income prediction, crop yield
prediction and image classification tasks, and leveraging such heterogeneity
benefits the out-of-distribution generalization performance.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps. (arXiv:2304.00133v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00133">http://arxiv.org/abs/2304.00133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00133] DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate Decision Stumps](http://arxiv.org/abs/2304.00133) #interpretability</code></li>
<li>Summary: <p>As the complexity of machine learning (ML) models increases and the
applications in different (and critical) domains grow, there is a strong demand
for more interpretable and trustworthy ML. One straightforward and
model-agnostic way to interpret complex ML models is to train surrogate models,
such as rule sets and decision trees, that sufficiently approximate the
original ones while being simpler and easier-to-explain. Yet, rule sets can
become very lengthy, with many if-else statements, and decision tree depth
grows rapidly when accurately emulating complex ML models. In such cases, both
approaches can fail to meet their core goal, providing users with model
interpretability. We tackle this by proposing DeforestVis, a visual analytics
tool that offers user-friendly summarization of the behavior of complex ML
models by providing surrogate decision stumps (one-level decision trees)
generated with the adaptive boosting (AdaBoost) technique. Our solution helps
users to explore the complexity vs fidelity trade-off by incrementally
generating more stumps, creating attribute-based explanations with weighted
stumps to justify decision making, and analyzing the impact of rule overriding
on training instance allocation between one or more stumps. An independent test
set allows users to monitor the effectiveness of manual rule changes and form
hypotheses based on case-by-case investigations. We show the applicability and
usefulness of DeforestVis with two use cases and expert interviews with data
analysts and model developers.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability. (arXiv:2304.00320v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.00320">http://arxiv.org/abs/2304.00320</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.00320] Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability](http://arxiv.org/abs/2304.00320) #diffusion</code></li>
<li>Summary: <p>Random label noises (or observational noises) widely exist in practical
machine learning settings. While previous studies primarily focus on the
affects of label noises to the performance of learning, our work intends to
investigate the implicit regularization effects of the label noises, under
mini-batch sampling settings of stochastic gradient descent (SGD), with
assumptions that label noises are unbiased. Specifically, we analyze the
learning dynamics of SGD over the quadratic loss with unbiased label noises,
where we model the dynamics of SGD as a stochastic differentiable equation
(SDE) with two diffusion terms (namely a Doubly Stochastic Model). While the
first diffusion term is caused by mini-batch sampling over the
(label-noiseless) loss gradients as many other works on SGD, our model
investigates the second noise term of SGD dynamics, which is caused by
mini-batch sampling over the label noises, as an implicit regularizer. Our
theoretical analysis finds such implicit regularizer would favor some
convergence points that could stabilize model outputs against perturbation of
parameters (namely inference stability). Though similar phenomenon have been
investigated, our work doesn't assume SGD as an Ornstein-Uhlenbeck like process
and achieve a more generalizable result with convergence of approximation
proved. To validate our analysis, we design two sets of empirical studies to
analyze the implicit regularizer of SGD with unbiased random label noises for
deep neural networks training and linear regression.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
