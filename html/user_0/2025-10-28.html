<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-28</h1>
<h3>Title: Proportion and Perspective Control for Flow-Based Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Julien Boudier, Hugo Caselles-Dupr√©</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21763">https://arxiv.org/abs/2510.21763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21763">https://arxiv.org/pdf/2510.21763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21763]] Proportion and Perspective Control for Flow-Based Image Generation(https://arxiv.org/abs/2510.21763)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While modern text-to-image diffusion models generate high-fidelity images, they offer limited control over the spatial and geometric structure of the output. To address this, we introduce and evaluate two ControlNets specialized for artistic control: (1) a proportion ControlNet that uses bounding boxes to dictate the position and scale of objects, and (2) a perspective ControlNet that employs vanishing lines to control the 3D geometry of the scene. We support the training of these modules with data pipelines that leverage vision-language models for annotation and specialized algorithms for conditioning image synthesis. Our experiments demonstrate that both modules provide effective control but exhibit limitations with complex constraints. Both models are released on HuggingFace: this https URL</li>
</ul>

<h3>Title: H2OFlow: Grounding Human-Object Affordances with 3D Generative Models and Dense Diffused Flows</h3>
<ul>
<li><strong>Authors: </strong>Harry Zhang, Luca Carlone</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21769">https://arxiv.org/abs/2510.21769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21769">https://arxiv.org/pdf/2510.21769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21769]] H2OFlow: Grounding Human-Object Affordances with 3D Generative Models and Dense Diffused Flows(https://arxiv.org/abs/2510.21769)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Understanding how humans interact with the surrounding environment, and specifically reasoning about object interactions and affordances, is a critical challenge in computer vision, robotics, and AI. Current approaches often depend on labor-intensive, hand-labeled datasets capturing real-world or simulated human-object interaction (HOI) tasks, which are costly and time-consuming to produce. Furthermore, most existing methods for 3D affordance understanding are limited to contact-based analysis, neglecting other essential aspects of human-object interactions, such as orientation (\eg, humans might have a preferential orientation with respect certain objects, such as a TV) and spatial occupancy (\eg, humans are more likely to occupy certain regions around an object, like the front of a microwave rather than its back). To address these limitations, we introduce \emph{H2OFlow}, a novel framework that comprehensively learns 3D HOI affordances -- encompassing contact, orientation, and spatial occupancy -- using only synthetic data generated from 3D generative models. H2OFlow employs a dense 3D-flow-based representation, learned through a dense diffusion process operating on point clouds. This learned flow enables the discovery of rich 3D affordances without the need for human annotations. Through extensive quantitative and qualitative evaluations, we demonstrate that H2OFlow generalizes effectively to real-world objects and surpasses prior methods that rely on manual annotations or mesh-based representations in modeling 3D affordance.</li>
</ul>

<h3>Title: Numerical Fragility in Transformers: A Layer-wise Theory for Explaining, Forecasting, and Mitigating Instability</h3>
<ul>
<li><strong>Authors: </strong>Jinwoo Baek</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21770">https://arxiv.org/abs/2510.21770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21770">https://arxiv.org/pdf/2510.21770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21770]] Numerical Fragility in Transformers: A Layer-wise Theory for Explaining, Forecasting, and Mitigating Instability(https://arxiv.org/abs/2510.21770)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers trained in low precision can suffer forward-error amplification. We give a first-order, module-wise theory that predicts when and where errors grow. For self-attention we derive a per-layer bound that factorizes into three interpretable diagnostics: a score-scale ratio $\kappa_{\rm score}$, a rowwise softmax sensitivity $\kappa_{\rm softmax}$, and value conditioning $\kappa(V)$. We prove a residual relaxation inequality showing that residual blocks attenuate depth-wise accumulation, and we introduce a precision- and width-aware LayerNorm indicator $\rho_{\rm LN}$ with a matching first-order bound in the $\epsilon$-dominated regime. These pieces yield a unified forward-stability bound whose right-hand side is directly estimable during training. On Tiny-ViT/CIFAR-10 we evaluate the bound and components. (1) The combined predictor $\kappa_{\rm softmax},(1+\kappa_{\rm score}),\kappa(V),|W_O|2+\kappa{\rm eff}+C_{\rm LN}$ tracks FP32$\leftrightarrow$LP mismatches across seeds, widths, and precisions; scaling by $\epsilon_{\rm mach}$ collapses mixed-precision points. (2) The time-series maximum of $\kappa_{\rm softmax}$ acts as an early-warning signal, leading error spikes by 16-24 steps (corr. 0.65-0.82; permutation $p!\approx!10^{-3}$; Precision@K 0.89-1.00). (3) Guided by $\rho_{\rm LN}$, a small LayerNorm-$\epsilon$ tweak targeting $\rho_\star$ gives consistent stabilization (mean tail-loss $\downarrow\ \approx0.010$ at $\rho_\star!=!0.6$, cap$=10^{-2}$) with negligible overhead. Overall, our theory supplies actionable, unitless diagnostics that (i) explain when self-attention is fragile, (ii) forecast instability, and (iii) motivate a minimally invasive mitigation.</li>
</ul>

<h3>Title: OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yulong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21774">https://arxiv.org/abs/2510.21774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21774">https://arxiv.org/pdf/2510.21774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21774]] OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment(https://arxiv.org/abs/2510.21774)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We present OCR-Quality, a comprehensive human-annotated dataset designed for evaluating and developing OCR quality assessment methods. The dataset consists of 1,000 PDF pages converted to PNG images at 300 DPI, sampled from diverse real-world scenarios, including academic papers, textbooks, e-books, and multilingual documents. Each document has been processed using state-of-the-art Vision-Language Models (VLMs) and manually annotated with quality scores using a 4-level scoring system (1: Excellent, 2: Good, 3: Fair, 4: Poor). The dataset includes detailed source information, annotation guidelines, and representative cases across various difficulty levels. OCR-Quality addresses the critical need for reliable OCR quality assessment in real-world applications and provides a valuable benchmark for training and evaluating OCR verification systems. The dataset is publicly available at this https URL .</li>
</ul>

<h3>Title: Bridging Accuracy and Interpretability: Deep Learning with XAI for Breast Cancer Detection</h3>
<ul>
<li><strong>Authors: </strong>Bishal Chhetri, B.V. Rathish Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21780">https://arxiv.org/abs/2510.21780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21780">https://arxiv.org/pdf/2510.21780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21780]] Bridging Accuracy and Interpretability: Deep Learning with XAI for Breast Cancer Detection(https://arxiv.org/abs/2510.21780)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In this study, we present an interpretable deep learning framework for the early detection of breast cancer using quantitative features extracted from digitized fine needle aspirate (FNA) images of breast masses. Our deep neural network, using ReLU activations, the Adam optimizer, and a binary cross-entropy loss, delivers state-of-the-art classification performance, achieving an accuracy of 0.992, precision of 1.000, recall of 0.977, and an F1 score of 0.988. These results substantially exceed the benchmarks reported in the literature. We evaluated the model under identical protocols against a suite of well-established algorithms (logistic regression, decision trees, random forests, stochastic gradient descent, K-nearest neighbors, and XGBoost) and found the deep model consistently superior on the same metrics. Recognizing that high predictive accuracy alone is insufficient for clinical adoption due to the black-box nature of deep learning models, we incorporated model-agnostic Explainable AI techniques such as SHAP and LIME to produce feature-level attributions and human-readable visualizations. These explanations quantify the contribution of each feature to individual predictions, support error analysis, and increase clinician trust, thus bridging the gap between performance and interpretability for real-world clinical use. The concave points feature of the cell nuclei is found to be the most influential feature positively impacting the classification task. This insight can be very helpful in improving the diagnosis and treatment of breast cancer by highlighting the key characteristics of breast tumor.</li>
</ul>

<h3>Title: Promptable Fire Segmentation: Unleashing SAM2's Potential for Real-Time Mobile Deployment with Strategic Bounding Box Guidance</h3>
<ul>
<li><strong>Authors: </strong>Emmanuel U. Ugwu, Zhang Xinming</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21782">https://arxiv.org/abs/2510.21782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21782">https://arxiv.org/pdf/2510.21782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21782]] Promptable Fire Segmentation: Unleashing SAM2's Potential for Real-Time Mobile Deployment with Strategic Bounding Box Guidance(https://arxiv.org/abs/2510.21782)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Fire segmentation remains a critical challenge in computer vision due to flames' irregular boundaries, translucent edges, and highly variable intensities. While the Segment Anything Models (SAM and SAM2) have demonstrated impressive cross-domain generalization capabilities, their effectiveness in fire segmentation -- particularly under mobile deployment constraints -- remains largely unexplored. This paper presents the first comprehensive evaluation of SAM2 variants for fire segmentation, focusing on bounding box prompting strategies to enhance deployment feasibility. We systematically evaluate four SAM2.1 variants (tiny, small, base_plus, large) alongside mobile-oriented variants (TinySAM, MobileSAM) across three fire datasets using multiple prompting strategies: automatic, single positive point (SP), single positive point + single negative point (SP+SN), multiple positive points (MP), bounding box (Box), and hybrid variants (Box+SP and Box+MP). Our experimental results demonstrate that bounding box prompts consistently outperform automatic and single point-based approaches, with Box+MP achieving the highest mean IoU (0.64) and Dice coefficient (0.75) on the Khan dataset. Lightweight variants such as TinySAM and MobileSAM further reduce memory and computational costs, making them more suitable for latency-tolerant edge scenarios. Overall, this work provides critical insights for deploying promptable segmentation models in fire monitoring systems and establishes benchmarks for future research in domain-specific SAM applications. Code is available at: this https URL</li>
</ul>

<h3>Title: Noise Aggregation Analysis Driven by Small-Noise Injection: Efficient Membership Inference for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Guo Li, Yuyang Yu, Xuemiao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21783">https://arxiv.org/abs/2510.21783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21783">https://arxiv.org/pdf/2510.21783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21783]] Noise Aggregation Analysis Driven by Small-Noise Injection: Efficient Membership Inference for Diffusion Models(https://arxiv.org/abs/2510.21783)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated powerful performance in generating high-quality images. A typical example is text-to-image generator like Stable Diffusion. However, their widespread use also poses potential privacy risks. A key concern is membership inference attacks, which attempt to determine whether a particular data sample was used in the model training process. We propose an efficient membership inference attack method against diffusion models. This method is based on the injection of slight noise and the evaluation of the aggregation degree of the noise distribution. The intuition is that the noise prediction patterns of diffusion models for training set samples and non-training set samples exhibit distinguishable this http URL, we suppose that member images exhibit higher aggregation of predicted noise around a certain time step of the diffusion process. In contrast, the predicted noises of non-member images exhibit a more discrete characteristic around the certain time step. Compared with other existing methods, our proposed method requires fewer visits to the target diffusion model. We inject slight noise into the image under test and then determine its membership by analyzing the aggregation degree of the noise distribution predicted by the model. Empirical findings indicate that our method achieves superior performance across multiple datasets. At the same time, our method can also show better attack effects in ASR and AUC when facing large-scale text-to-image diffusion models, proving the scalability of our method.</li>
</ul>

<h3>Title: EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction</h3>
<ul>
<li><strong>Authors: </strong>Qile Su, Shoutai Zhu, Shuai Zhang, Baoyu Liang, Chao Tong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21786">https://arxiv.org/abs/2510.21786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21786">https://arxiv.org/pdf/2510.21786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21786]] EventFormer: A Node-graph Hierarchical Attention Transformer for Action-centric Video Event Prediction(https://arxiv.org/abs/2510.21786)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Script event induction, which aims to predict the subsequent event based on the context, is a challenging task in NLP, achieving remarkable success in practical applications. However, human events are mostly recorded and presented in the form of videos rather than scripts, yet there is a lack of related research in the realm of vision. To address this problem, we introduce AVEP (Action-centric Video Event Prediction), a task that distinguishes itself from existing video prediction tasks through its incorporation of more complex logic and richer semantic information. We present a large structured dataset, which consists of about $35K$ annotated videos and more than $178K$ video clips of event, built upon existing video event datasets to support this task. The dataset offers more fine-grained annotations, where the atomic unit is represented as a multimodal event argument node, providing better structured representations of video events. Due to the complexity of event structures, traditional visual models that take patches or frames as input are not well-suited for AVEP. We propose EventFormer, a node-graph hierarchical attention based video event prediction model, which can capture both the relationships between events and their arguments and the coreferencial relationships between arguments. We conducted experiments using several SOTA video prediction models as well as LVLMs on AVEP, demonstrating both the complexity of the task and the value of the dataset. Our approach outperforms all these video prediction models. We will release the dataset and code for replicating the experiments and annotations.</li>
</ul>

<h3>Title: Mismatch reconstruction theory for unknown measurement matrix in imaging through multimode fiber bending</h3>
<ul>
<li><strong>Authors: </strong>Le Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21787">https://arxiv.org/abs/2510.21787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21787">https://arxiv.org/pdf/2510.21787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21787]] Mismatch reconstruction theory for unknown measurement matrix in imaging through multimode fiber bending(https://arxiv.org/abs/2510.21787)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimode fiber imaging requires strict matching between measurement value and measurement matrix to achieve image reconstruction. However, in practical applications, the measurement matrix often cannot be obtained due to unknown system configuration or difficulty in real-time alignment after arbitrary fiber bending, resulting in the failure of traditional reconstruction algorithms. This paper presents a novel mismatch reconstruction theory for solving the problem of image reconstruction when measurement matrix is unknown. We first propose mismatch equation and design matched and calibration solution algorithms to construct a new measurement matrix. In addition, we also provide a detailed proof of these equations and algorithms in the appendix. The experimental results show that under low noise levels, constructed matrix can be used for matched pair in traditional reconstruction algorithms, and reconstruct the original image successfully. Then, we analyze the impact of noise, computational precision and orthogonality on reconstruction performance. The results show that proposed algorithms have a certain degree of robustness. Finally, we discuss the limitations and potential applications of this theory. The code is available: this https URL.</li>
</ul>

<h3>Title: Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Larkin Liu, Jalal Etesami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21788">https://arxiv.org/abs/2510.21788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21788">https://arxiv.org/pdf/2510.21788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21788]] Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making(https://arxiv.org/abs/2510.21788)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We explore the use of expert-guided bandit learning, which we refer to as online mixture-of-experts (OMoE). In this setting, given a context, a candidate committee of experts must determine how to aggregate their outputs to achieve optimal results in terms of aggregate accuracy. We propose two algorithms to address this problem. The first algorithm combines aggregate voting with UCB-driven successive elimination, efficiently pruning suboptimal exploration actions. The second algorithm employs an online weighted-majority-voting mechanism, leveraging the respective voting power of each expert proportional to their predictive power. We derive theoretical guarantees for the regret properties in the bandit setting under ideal circumstances, and empirical results are provided accordingly. As a modern study on applications, these methods are applied to the online fine-tuning of a set of expert large language models (LLMs), where after each response, the generative LLM dynamically reweighs its set of experts and/or selects the optimal committee of experts to generate the most accurate response. Our results introduce new methodologies and no-regret guarantees for combining multiple experts to improve on the performance of the an aggregate model overall.</li>
</ul>

<h3>Title: Exploring the design space of diffusion and flow models for data fusion</h3>
<ul>
<li><strong>Authors: </strong>Niraj Chaudhari, Manmeet Singh, Naveen Sudharsan, Amit Kumar Srivastava, Harsh Kamath, Dushyant Mahajan, Ayan Paul</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.ins-det</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21791">https://arxiv.org/abs/2510.21791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21791">https://arxiv.org/pdf/2510.21791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21791]] Exploring the design space of diffusion and flow models for data fusion(https://arxiv.org/abs/2510.21791)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data fusion is an essential task in various domains, enabling the integration of multi-source information to enhance data quality and insights. One key application is in satellite remote sensing, where fusing multi-sensor observations can improve spatial and temporal resolution. In this study, we explore the design space of diffusion and flow models for data fusion, focusing on the integration of Defense Meteorological Satellite Program's Operational Linescan System (DMSP-OLS) and Visible Infrared Imaging Radiometer Suite (VIIRS) nighttime lights data. Our approach leverages a diverse set of 2D image-to-image generative models, including UNET, diffusion, and flow modeling architectures. We evaluate the effectiveness of these architectures in satellite remote sensing data fusion, identifying diffusion models based on UNet as particularly adept at preserving fine-grained spatial details and generating high-fidelity fused images. We also provide guidance on the selection of noise schedulers in diffusion-based models, highlighting the trade-offs between iterative solvers for faster inference and discrete schedulers for higher-quality reconstructions. Additionally, we explore quantization techniques to optimize memory efficiency and computational cost without compromising performance. Our findings offer practical insights into selecting the most effective diffusion and flow model architectures for data fusion tasks, particularly in remote sensing applications, and provide recommendations for leveraging noise scheduling strategies to enhance fusion quality.</li>
</ul>

<h3>Title: Variance-Reduction Guidance: Sampling Trajectory Optimization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shifeng Xu, Yanzhu Liu, Adams Wai-Kin Kong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21792">https://arxiv.org/abs/2510.21792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21792">https://arxiv.org/pdf/2510.21792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21792]] Variance-Reduction Guidance: Sampling Trajectory Optimization for Diffusion Models(https://arxiv.org/abs/2510.21792)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have become emerging generative models. Their sampling process involves multiple steps, and in each step the models predict the noise from a noisy sample. When the models make prediction, the output deviates from the ground truth, and we call such a deviation as \textit{prediction error}. The prediction error accumulates over the sampling process and deteriorates generation quality. This paper introduces a novel technique for statistically measuring the prediction error and proposes the Variance-Reduction Guidance (VRG) method to mitigate this error. VRG does not require model fine-tuning or modification. Given a predefined sampling trajectory, it searches for a new trajectory which has the same number of sampling steps but produces higher quality results. VRG is applicable to both conditional and unconditional generation. Experiments on various datasets and baselines demonstrate that VRG can significantly improve the generation quality of diffusion models. Source code is available at this https URL.</li>
</ul>

<h3>Title: 2D_3D Feature Fusion via Cross-Modal Latent Synthesis and Attention Guided Restoration for Industrial Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Usman Ali, Ali Zia, Abdul Rehman, Umer Ramzan, Zohaib Hassan, Talha Sattar, Jing Wang, Wei Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21793">https://arxiv.org/abs/2510.21793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21793">https://arxiv.org/pdf/2510.21793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21793]] 2D_3D Feature Fusion via Cross-Modal Latent Synthesis and Attention Guided Restoration for Industrial Anomaly Detection(https://arxiv.org/abs/2510.21793)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Industrial anomaly detection (IAD) increasingly benefits from integrating 2D and 3D data, but robust cross-modal fusion remains challenging. We propose a novel unsupervised framework, Multi-Modal Attention-Driven Fusion Restoration (MAFR), which synthesises a unified latent space from RGB images and point clouds using a shared fusion encoder, followed by attention-guided, modality-specific decoders. Anomalies are localised by measuring reconstruction errors between input features and their restored counterparts. Evaluations on the MVTec 3D-AD and Eyecandies benchmarks demonstrate that MAFR achieves state-of-the-art results, with a mean I-AUROC of 0.972 and 0.901, respectively. The framework also exhibits strong performance in few-shot learning settings, and ablation studies confirm the critical roles of the fusion architecture and composite loss. MAFR offers a principled approach for fusing visual and geometric information, advancing the robustness and accuracy of industrial anomaly detection. Code is available at this https URL</li>
</ul>

<h3>Title: MARS-M: When Variance Reduction Meets Matrices</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Liu, Angela Yuan, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21800">https://arxiv.org/abs/2510.21800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21800">https://arxiv.org/pdf/2510.21800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21800]] MARS-M: When Variance Reduction Meets Matrices(https://arxiv.org/abs/2510.21800)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, a new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to a first-order stationary point at a rate of $\tilde{\mathcal{O}}(T^{-1/3})$, which improves upon $\tilde{\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at this https URL.</li>
</ul>

<h3>Title: Morphology-Aware KOA Classification: Integrating Graph Priors with Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Marouane Tliba, Mohamed Amine Kerkouri, Yassine Nasser, Nour Aburaed, Aladine Chetouani, Ulas Bagci, Rachid Jennane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21801">https://arxiv.org/abs/2510.21801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21801">https://arxiv.org/pdf/2510.21801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21801]] Morphology-Aware KOA Classification: Integrating Graph Priors with Vision Models(https://arxiv.org/abs/2510.21801)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Knee osteoarthritis (KOA) diagnosis from radiographs remains challenging due to the subtle morphological details that standard deep learning models struggle to capture effectively. We propose a novel multimodal framework that combines anatomical structure with radiographic features by integrating a morphological graph representation - derived from Segment Anything Model (SAM) segmentations - with a vision encoder. Our approach enforces alignment between geometry-informed graph embeddings and radiographic features through mutual information maximization, significantly improving KOA classification accuracy. By constructing graphs from anatomical features, we introduce explicit morphological priors that mirror clinical assessment criteria, enriching the feature space and enhancing the model's inductive bias. Experiments on the Osteoarthritis Initiative dataset demonstrate that our approach surpasses single-modality baselines by up to 10\% in accuracy (reaching nearly 80\%), while outperforming existing state-of-the-art methods by 8\% in accuracy and 11\% in F1 score. These results underscore the critical importance of incorporating anatomical structure into radiographic analysis for accurate KOA severity grading.</li>
</ul>

<h3>Title: It Takes Two to Tango: Two Parallel Samplers Improve Quality in Diffusion Models for Limited Steps</h3>
<ul>
<li><strong>Authors: </strong>Pedro Cisneros-Velarde</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21802">https://arxiv.org/abs/2510.21802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21802">https://arxiv.org/pdf/2510.21802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21802]] It Takes Two to Tango: Two Parallel Samplers Improve Quality in Diffusion Models for Limited Steps(https://arxiv.org/abs/2510.21802)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We consider the situation where we have a limited number of denoising steps, i.e., of evaluations of a diffusion model. We show that two parallel processors or samplers under such limitation can improve the quality of the sampled image. Particularly, the two samplers make denoising steps at successive times, and their information is appropriately integrated in the latent image. Remarkably, our method is simple both conceptually and to implement: it is plug-&-play, model agnostic, and does not require any additional fine-tuning or external models. We test our method with both automated and human evaluations for different diffusion models. We also show that a naive integration of the information from the two samplers lowers sample quality. Finally, we find that adding more parallel samplers does not necessarily improve sample quality.</li>
</ul>

<h3>Title: Residual-guided AI-CFD hybrid method enables stable and scalable simulations: from 2D benchmarks to 3D applications</h3>
<ul>
<li><strong>Authors: </strong>Shilaj Baral, Youngkyu Lee, Sangam Khanal, Joongoo Jeon</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21804">https://arxiv.org/abs/2510.21804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21804">https://arxiv.org/pdf/2510.21804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21804]] Residual-guided AI-CFD hybrid method enables stable and scalable simulations: from 2D benchmarks to 3D applications(https://arxiv.org/abs/2510.21804)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Purely data-driven surrogates for fluid dynamics often fail catastrophically from error accumulation, while existing hybrid methods have lacked the automation and robustness for practical use. To solve this, we developed XRePIT, a novel hybrid simulation strategy that synergizes machine learning (ML) acceleration with solver-based correction. We specifically designed our method to be fully automated and physics-aware, ensuring the stability and practical applicability that previous approaches lacked. We demonstrate that this new design overcomes long-standing barriers, achieving the first stable, accelerated rollouts for over 10,000 timesteps. The method also generalizes robustly to unseen boundary conditions and, crucially, scales to 3D flows. Our approach delivers speedups up to 4.98$\times$ while maintaining high physical fidelity, resolving thermal fields with relative errors of ~1E-3 and capturing low magnitude velocity dynamics with errors below 1E-2 ms-1. This work thus establishes a mature and scalable hybrid method, paving the way for its use in real-world engineering.</li>
</ul>

<h3>Title: Activating Visual Context and Commonsense Reasoning through Masked Prediction in VLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaao Yu, Shenwei Li, Mingjie Han, Yifei Yin, Wenzheng Song, Chenghao Jia, Man Lan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21807">https://arxiv.org/abs/2510.21807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21807">https://arxiv.org/pdf/2510.21807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21807]] Activating Visual Context and Commonsense Reasoning through Masked Prediction in VLMs(https://arxiv.org/abs/2510.21807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in reasoning models have markedly advanced the reasoning capabilities of large language models, particularly via training on tasks with verifiable rewards. Yet, a significant gap persists in their adaptation to real world multimodal scenarios, most notably, vision language tasks, due to a heavy focus on single modal language settings. While efforts to transplant reinforcement learning techniques from NLP to VLMs have emerged, these approaches often remain confined to perception centric tasks or reduce images to textual summaries, failing to fully exploit visual context and commonsense knowledge, ultimately constraining the generalization of reasoning capabilities across diverse multimodal environments. To address this limitation, we introduce a novel fine tuning task, Masked Prediction via Context and Commonsense, which forces models to integrate visual context and commonsense reasoning by reconstructing semantically meaningful content from occluded images, thereby laying the foundation for generalized reasoning. To systematically evaluate the model performance in generalized reasoning, we developed a specialized evaluation benchmark, MPCC Eval, and employed various fine tuning strategies to guide reasoning. Among these, we introduced an innovative training method, Reinforcement Fine tuning with Prior Sampling, which not only enhances model performance but also improves its generalized reasoning capabilities in OOD and cross task scenarios.</li>
</ul>

<h3>Title: Hybrid Deep Learning Framework for Enhanced Diabetic Retinopathy Detection: Integrating Traditional Features with AI-driven Insights</h3>
<ul>
<li><strong>Authors: </strong>Arpan Maity, Aviroop Pal, MD. Samiul Islam, Tamal Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21810">https://arxiv.org/abs/2510.21810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21810">https://arxiv.org/pdf/2510.21810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21810]] Hybrid Deep Learning Framework for Enhanced Diabetic Retinopathy Detection: Integrating Traditional Features with AI-driven Insights(https://arxiv.org/abs/2510.21810)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Diabetic Retinopathy (DR), a vision-threatening complication of Dia-betes Mellitus (DM), is a major global concern, particularly in India, which has one of the highest diabetic populations. Prolonged hyperglycemia damages reti-nal microvasculature, leading to DR symptoms like microaneurysms, hemor-rhages, and fluid leakage, which, if undetected, cause irreversible vision loss. Therefore, early screening is crucial as DR is asymptomatic in its initial stages. Fundus imaging aids precise diagnosis by detecting subtle retinal lesions. This paper introduces a hybrid diagnostic framework combining traditional feature extraction and deep learning (DL) to enhance DR detection. While handcrafted features capture key clinical markers, DL automates hierarchical pattern recog-nition, improving early diagnosis. The model synergizes interpretable clinical data with learned features, surpassing standalone DL approaches that demon-strate superior classification and reduce false negatives. This multimodal AI-driven approach enables scalable, accurate DR screening, crucial for diabetes-burdened regions.</li>
</ul>

<h3>Title: Comparative Analysis of Object Detection Algorithms for Surface Defect Detection</h3>
<ul>
<li><strong>Authors: </strong>Arpan Maity, Tamal Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21811">https://arxiv.org/abs/2510.21811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21811">https://arxiv.org/pdf/2510.21811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21811]] Comparative Analysis of Object Detection Algorithms for Surface Defect Detection(https://arxiv.org/abs/2510.21811)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>This article compares the performance of six prominent object detection algorithms, YOLOv11, RetinaNet, Fast R-CNN, YOLOv8, RT-DETR, and DETR, on the NEU-DET surface defect detection dataset, comprising images representing various metal surface defects, a crucial application in industrial quality control. Each model's performance was assessed regarding detection accuracy, speed, and robustness across different defect types such as scratches, inclusions, and rolled-in scales. YOLOv11, a state-of-the-art real-time object detection algorithm, demonstrated superior performance compared to the other methods, achieving a remarkable 70% higher accuracy on average. This improvement can be attributed to YOLOv11s enhanced feature extraction capabilities and ability to process the entire image in a single forward pass, making it faster and more efficient in detecting minor surface defects. Additionally, YOLOv11's architecture optimizations, such as improved anchor box generation and deeper convolutional layers, contributed to more precise localization of defects. In conclusion, YOLOv11's outstanding performance in accuracy and speed solidifies its position as the most effective model for surface defect detection on the NEU dataset, surpassing competing algorithms by a substantial margin.</li>
</ul>

<h3>Title: SITS-DECO: A Generative Decoder Is All You Need For Multitask Satellite Image Time Series Modelling</h3>
<ul>
<li><strong>Authors: </strong>Samuel J. Barrett, Docko Sow</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21813">https://arxiv.org/abs/2510.21813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21813">https://arxiv.org/pdf/2510.21813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21813]] SITS-DECO: A Generative Decoder Is All You Need For Multitask Satellite Image Time Series Modelling(https://arxiv.org/abs/2510.21813)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Earth Observation (EO) Foundation Modelling (FM) holds great promise for simplifying and improving the use of EO data for diverse real-world tasks. However, most existing models require additional adaptation before they can be used and are structured rigidly around particular data sources or training approaches. To address this, we take inspiration from large language models, where diverse tasks, both pre-training and downstream, are implicitly captured through next-token prediction over unified token sequences, leveraging the structure and diversity of the training data. We introduce SITS-DECO (Satellite Image Time Series-DECoder Only), a proof-of-concept generative model that applies this unified-sequence framing to EO data. Using a simple GPT-style decoder-only architecture, and demonstrate its ability to perform useful EO tasks (pixel-wise, multi-temporal, multi-modal crop-type classification) in a purely generative framework. Through symbolic prompting, we show that the model can perform multiple supervised and self-supervised tasks within a single unified architecture, without task- or modality-specific adaptation. Despite its simplicity and lack of spatial context, SITS-DECO outperforms much larger EO foundation models on crop-type classification (PASTIS-R) demonstrating that dense temporal sequence modelling is a critical missing ingredient in the current paradigm. This work exemplifies a data-centric modelling paradigm in which capability arises from the diversity and structure of the training data rather than from architectural complexity. SITS-DECO provides a lightweight, practical route to multi-modal, multi-task EO modelling, and a conceptual bridge toward future generative EO foundation models.</li>
</ul>

<h3>Title: Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhuoming Li, Aitong Liu, Mengxi Jia, Tengxiang Zhang, Dell Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21814">https://arxiv.org/abs/2510.21814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21814">https://arxiv.org/pdf/2510.21814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21814]] Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding(https://arxiv.org/abs/2510.21814)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Free-form gesture understanding is highly appealing for human-computer interaction, as it liberates users from the constraints of predefined gesture categories. However, the sole existing solution GestureGPT suffers from limited recognition accuracy and slow response times. In this paper, we propose Gestura, an end-to-end system for free-form gesture understanding. Gestura harnesses a pre-trained Large Vision-Language Model (LVLM) to align the highly dynamic and diverse patterns of free-form gestures with high-level semantic concepts. To better capture subtle hand movements across different styles, we introduce a Landmark Processing Module that compensate for LVLMs' lack of fine-grained domain knowledge by embedding anatomical hand priors. Further, a Chain-of-Thought (CoT) reasoning strategy enables step-by-step semantic inference, transforming shallow knowledge into deep semantic understanding and significantly enhancing the model's ability to interpret ambiguous or unconventional gestures. Together, these components allow Gestura to achieve robust and adaptable free-form gesture comprehension. Additionally, we have developed the first open-source dataset for free-form gesture intention reasoning and understanding with over 300,000 annotated QA pairs.</li>
</ul>

<h3>Title: Unlocking Biomedical Insights: Hierarchical Attention Networks for High-Dimensional Data Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Rekha R Nair, Tina Babu, Alavikunhu Panthakkan, Hussain Al-Ahmad, Balamurugan Balusamy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21820">https://arxiv.org/abs/2510.21820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21820">https://arxiv.org/pdf/2510.21820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21820]] Unlocking Biomedical Insights: Hierarchical Attention Networks for High-Dimensional Data Interpretation(https://arxiv.org/abs/2510.21820)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The proliferation of high-dimensional datasets in fields such as genomics, healthcare, and finance has created an urgent need for machine learning models that are both highly accurate and inherently interpretable. While traditional deep learning approaches deliver strong predictive performance, their lack of transparency often impedes their deployment in critical, decision-sensitive applications. In this work, we introduce the Hierarchical Attention-based Interpretable Network (HAIN), a novel architecture that unifies multi-level attention mechanisms, dimensionality reduction, and explanation-driven loss functions to deliver interpretable and robust analysis of complex biomedical data. HAIN provides feature-level interpretability via gradientweighted attention and offers global model explanations through prototype-based representations. Comprehensive evaluation on The Cancer Genome Atlas (TCGA) dataset demonstrates that HAIN achieves a classification accuracy of 94.3%, surpassing conventional post-hoc interpretability approaches such as SHAP and LIME in both transparency and explanatory power. Furthermore, HAIN effectively identifies biologically relevant cancer biomarkers, supporting its utility for clinical and research applications. By harmonizing predictive accuracy with interpretability, HAIN advances the development of transparent AI solutions for precision medicine and regulatory compliance.</li>
</ul>

<h3>Title: Wavelet-based GAN Fingerprint Detection using ResNet50</h3>
<ul>
<li><strong>Authors: </strong>Sai Teja Erukude, Suhasnadh Reddy Veluru, Viswa Chaitanya Marella</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21822">https://arxiv.org/abs/2510.21822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21822">https://arxiv.org/pdf/2510.21822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21822]] Wavelet-based GAN Fingerprint Detection using ResNet50(https://arxiv.org/abs/2510.21822)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Identifying images generated by Generative Adversarial Networks (GANs) has become a significant challenge in digital image forensics. This research presents a wavelet-based detection method that uses discrete wavelet transform (DWT) preprocessing and a ResNet50 classification layer to differentiate the StyleGAN-generated images from real ones. Haar and Daubechies wavelet filters are applied to convert the input images into multi-resolution representations, which will then be fed to a ResNet50 network for classification, capitalizing on subtle artifacts left by the generative process. Moreover, the wavelet-based models are compared to an identical ResNet50 model trained on spatial data. The Haar and Daubechies preprocessed models achieved a greater accuracy of 93.8 percent and 95.1 percent, much higher than the model developed in the spatial domain (accuracy rate of 81.5 percent). The Daubechies-based model outperforms Haar, showing that adding layers of descriptive frequency patterns can lead to even greater distinguishing power. These results indicate that the GAN-generated images have unique wavelet-domain artifacts or "fingerprints." The method proposed illustrates the effectiveness of wavelet-domain analysis to detect GAN images and emphasizes the potential of further developing the capabilities of future deepfake detection systems.</li>
</ul>

<h3>Title: Explainable Deep Learning in Medical Imaging: Brain Tumor and Pneumonia Detection</h3>
<ul>
<li><strong>Authors: </strong>Sai Teja Erukude, Viswa Chaitanya Marella, Suhasnadh Reddy Veluru</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21823">https://arxiv.org/abs/2510.21823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21823">https://arxiv.org/pdf/2510.21823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21823]] Explainable Deep Learning in Medical Imaging: Brain Tumor and Pneumonia Detection(https://arxiv.org/abs/2510.21823)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Deep Learning (DL) holds enormous potential for improving medical imaging diagnostics, yet the lack of interpretability in most models hampers clinical trust and adoption. This paper presents an explainable deep learning framework for detecting brain tumors in MRI scans and pneumonia in chest X-ray images using two leading Convolutional Neural Networks, ResNet50 and DenseNet121. These models were trained on publicly available Kaggle datasets comprising 7,023 brain MRI images and 5,863 chest X-ray images, achieving high classification performance. DenseNet121 consistently outperformed ResNet50 with 94.3 percent vs. 92.5 percent accuracy for brain tumors and 89.1 percent vs. 84.4 percent accuracy for pneumonia. For better explainability, Gradient-weighted Class Activation Mapping (Grad-CAM) was integrated to create heatmap visualizations superimposed on the test images, indicating the most influential image regions in the decision-making process. Interestingly, while both models produced accurate results, Grad-CAM showed that DenseNet121 consistently focused on core pathological regions, whereas ResNet50 sometimes scattered attention to peripheral or non-pathological areas. Combining deep learning and explainable AI offers a promising path toward reliable, interpretable, and clinically useful diagnostic tools.</li>
</ul>

<h3>Title: Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Zhuo Chen, Lingbing Guo, Lei Liang, Wen Zhang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21828">https://arxiv.org/abs/2510.21828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21828">https://arxiv.org/pdf/2510.21828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21828]] Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images(https://arxiv.org/abs/2510.21828)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding and reasoning with abstractive information from the visual modality presents significant challenges for current multi-modal large language models (MLLMs). Among the various forms of abstractive information, Multi-Modal Relational Knowledge (MMRK), which represents abstract relational structures between multi-modal entities using node-edge formats, remains largely under-explored. In particular, STructured and Abstractive Reasoning (STAR) on such data has received little attention from the research community. To bridge the dual gaps in large-scale high-quality data and capability enhancement methodologies, this paper makes the following key contributions: (i). An automatic STAR data engine capable of synthesizing images with MMRK to build multi-modal instruction data with reliable chain-of-thought thinking for various STAR tasks and (ii). A comprehsive two-stage capability enhancement training framework, accompanied by a suite of evaluation protocols tailored to different STAR tasks. Based upon these contributions, we introduce STAR-64K, a dataset comprising 64K high-quality multi-modal instruction samples, and conduct experiments across 5 open-source MLLMs. Experimental results show that our two-stage enhancement framework enables smaller 3B/7B models to significantly outperform GPT-4o in STAR. Additionally, we provide in-depth analysis regarding the effectiveness of various designs, data transferability, and scalability.</li>
</ul>

<h3>Title: A Flow Model with Low-Rank Transformers for Incomplete Multimodal Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yi Yin, Yuntao Shou, Zao Dai, Yun Peng, Tao Meng, Wei Ai, Keqin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21829">https://arxiv.org/abs/2510.21829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21829">https://arxiv.org/pdf/2510.21829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21829]] A Flow Model with Low-Rank Transformers for Incomplete Multimodal Survival Analysis(https://arxiv.org/abs/2510.21829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>In recent years, multimodal medical data-based survival analysis has attracted much attention. However, real-world datasets often suffer from the problem of incomplete modality, where some patient modality information is missing due to acquisition limitations or system failures. Existing methods typically infer missing modalities directly from observed ones using deep neural networks, but they often ignore the distributional discrepancy across modalities, resulting in inconsistent and unreliable modality reconstruction. To address these challenges, we propose a novel framework that combines a low-rank Transformer with a flow-based generative model for robust and flexible multimodal survival prediction. Specifically, we first formulate the concerned problem as incomplete multimodal survival analysis using the multi-instance representation of whole slide images (WSIs) and genomic profiles. To realize incomplete multimodal survival analysis, we propose a class-specific flow for cross-modal distribution alignment. Under the condition of class labels, we model and transform the cross-modal distribution. By virtue of the reversible structure and accurate density modeling capabilities of the normalizing flow model, the model can effectively construct a distribution-consistent latent space of the missing modality, thereby improving the consistency between the reconstructed data and the true distribution. Finally, we design a lightweight Transformer architecture to model intra-modal dependencies while alleviating the overfitting problem in high-dimensional modality fusion by virtue of the low-rank Transformer. Extensive experiments have demonstrated that our method not only achieves state-of-the-art performance under complete modality settings, but also maintains robust and superior accuracy under the incomplete modalities scenario.</li>
</ul>

<h3>Title: GAPO: Group Adaptive Policy Optimization for Real-World Code Edit</h3>
<ul>
<li><strong>Authors: </strong>Jianqing Zhang, Zhezheng Hao, Wei Xia, Hande Dong, Hong Wang, Chenxing Wei, Yuyan Zhou, Yubin Qi, Qiang Lin, Jian Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21830">https://arxiv.org/abs/2510.21830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21830">https://arxiv.org/pdf/2510.21830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21830]] GAPO: Group Adaptive Policy Optimization for Real-World Code Edit(https://arxiv.org/abs/2510.21830)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods like GRPO are popular for their critic-free, normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable outliers, leading to distorted advantage computation and increased noise. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an outlier-free highest-density interval (HDI) per prompt and then uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation. This adaptive Q robustly handles skewed distributions while remaining plug-and-play and efficient. We validate GAPO on nine instruction-tuned LLMs (3B-14B) using a large internal dataset of 51,844 real-world, history-aware code-editing tasks across 10 languages, demonstrating consistent improvements in exact match accuracy over GRPO and its variant DAPO. Code is publicly available.</li>
</ul>

<h3>Title: Towards Accurate and Efficient Waste Image Classification: A Hybrid Deep Learning and Machine Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Ngoc-Bao-Quang Nguyen, Tuan-Minh Do, Cong-Tam Phan, Thi-Thu-Hong Phan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21833">https://arxiv.org/abs/2510.21833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21833">https://arxiv.org/pdf/2510.21833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21833]] Towards Accurate and Efficient Waste Image Classification: A Hybrid Deep Learning and Machine Learning Approach(https://arxiv.org/abs/2510.21833)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Automated image-based garbage classification is a critical component of global waste management; however, systematic benchmarks that integrate Machine Learning (ML), Deep Learning (DL), and efficient hybrid solutions remain underdeveloped. This study provides a comprehensive comparison of three paradigms: (1) machine learning algorithms using handcrafted features, (2) deep learning architectures, including ResNet variants and EfficientNetV2S, and (3) a hybrid approach that utilizes deep models for feature extraction combined with classical classifiers such as Support Vector Machine and Logistic Regression to identify the most effective strategy. Experiments on three public datasets - TrashNet, Garbage Classification, and a refined Household Garbage Dataset (with 43 corrected mislabels)- demonstrate that the hybrid method consistently outperforms the others, achieving up to 100% accuracy on TrashNet and the refined Household set, and 99.87% on Garbage Classification, thereby surpassing state-of-the-art benchmarks. Furthermore, feature selection reduces feature dimensionality by over 95% without compromising accuracy, resulting in faster training and inference. This work establishes more reliable benchmarks for waste classification and introduces an efficient hybrid framework that achieves high accuracy while reducing inference cost, making it suitable for scalable deployment in resource-constrained environments.</li>
</ul>

<h3>Title: Restoring Pruned Large Language Models via Lost Component Compensation</h3>
<ul>
<li><strong>Authors: </strong>Zijian Feng, Hanzhang Zhou, Zixiao Zhu, Tianjiao Li, Jia Jim Deryl Chua, Lee Onn Mak, Gee Wah Ng, Kezhi Mao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21834">https://arxiv.org/abs/2510.21834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21834">https://arxiv.org/pdf/2510.21834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21834]] Restoring Pruned Large Language Models via Lost Component Compensation(https://arxiv.org/abs/2510.21834)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pruning is a widely used technique to reduce the size and inference cost of large language models (LLMs), but it often causes performance degradation. To mitigate this, existing restoration methods typically employ parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned model's performance. However, most PEFT methods are designed for dense models and overlook the distinct properties of pruned models, often resulting in suboptimal recovery. In this work, we propose a targeted restoration strategy for pruned models that restores performance while preserving their low cost and high efficiency. We observe that pruning-induced information loss is reflected in attention activations, and selectively reintroducing components of this information can significantly recover model performance. Based on this insight, we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component Compensation), a plug-and-play method that contrastively probes critical attention heads via activation editing, extracts lost components from activation differences, and finally injects them back into the corresponding pruned heads for compensation and recovery. RestoreLCC is compatible with structured, semi-structured, and unstructured pruning schemes. Extensive experiments demonstrate that RestoreLCC consistently outperforms state-of-the-art baselines in both general and task-specific performance recovery, without compromising the sparsity or inference efficiency of pruned models.</li>
</ul>

<h3>Title: A Multimodal, Multitask System for Generating E Commerce Text Listings from Images</h3>
<ul>
<li><strong>Authors: </strong>Nayan Kumar Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21835">https://arxiv.org/abs/2510.21835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21835">https://arxiv.org/pdf/2510.21835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21835]] A Multimodal, Multitask System for Generating E Commerce Text Listings from Images(https://arxiv.org/abs/2510.21835)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Manually generating catchy descriptions and names is labor intensive and a slow process for retailers. Although generative AI provides an automation solution in form of Vision to Language Models (VLM), the current VLMs are prone to factual "hallucinations". Siloed, single task models are not only inefficient but also fail to capture interdependent relationships between features. To address these challenges, we propose an end to end, multi task system that generates factually grounded textual listings from a single image. The contributions of this study are two proposals for the model architecture. First, application of multi task learning approach for fine tuning a vision encoder where a single vision backbone is jointly trained on attribute prediction such as color, hemline and neck style and price regression. Second, introduction of a hierarchical generation process where the model's own predicted attributes are embedded in a prompt and fed to the text decoder to improve factual consistency. The experiments demonstrate the superiority of this architecture. The multi tasking approach outperforms both the independent price regression, with a 3.6% better R2 Value and attribute classification, with a 6.6% improvement F1 score. Critically, the hierarchical generation process proves highly effective, slashing the factual hallucination rate from 12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical ablation. The hierarchical approach also reduces the latency of the autoregressive text generation process by a factor of 3.5 when compared to direct vision to language model of similar size. One minor caveat is that the model does perform 3.5% worse than direct vision-to-language model on ROUGE-L score.</li>
</ul>

<h3>Title: COLA: Continual Learning via Autoencoder Retrieval of Adapters</h3>
<ul>
<li><strong>Authors: </strong>Jaya Krishna Mandivarapu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21836">https://arxiv.org/abs/2510.21836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21836">https://arxiv.org/pdf/2510.21836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21836]] COLA: Continual Learning via Autoencoder Retrieval of Adapters(https://arxiv.org/abs/2510.21836)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Learning a set of tasks over time, also known as continual learning (CL), is one of the most challenging problems in artificial intelligence due to catastrophic forgetting. Large language models (LLMs) are often impractical to frequent re-training and continual learning , due to high cost of computational resources for training. Moreover, LLM are not suitable for continual learning as updating these models over time for acquiring new knowledge leads to overwrites existing knowledge leading to common phenomenon know as \textit{catastrophic forgetting}. In this paper, we aim to address these concerns using a novel framework , COLA that employs an autoencoder to learn capture low-dimensional embeddings of the weights associated with various tasks. Our approach facilitates the transfer of knowledge to new tasks while preventing catastrophic forgetting, all without using data replay or a substantial set of task-specific parameters. Our approach, COLA, makes the LLM efficiently learn new tasks with minimal training, insignificant performance degradation on previous tasks, and eliminates the need for retaining earlier training data. Empirical evaluation on different datasets ranging from task oriented dialouge system to intent classsfication datasets showcases that our method not only overcomes catastrophic forgetting but also achieves significant reduction in parameter usage and memory size, across multiple tasks and outperforming the existing state of the art methods across multiple datasets.</li>
</ul>

<h3>Title: Improving the Physics of Video Generation with VJEPA-2 Reward Signal</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Yuan, Xiaofeng Zhang, Felix Friedrich, Nicolas Beltran-Velez, Melissa Hall, Reyhane Askari-Hemmat, Xiaochuang Han, Nicolas Ballas, Michal Drozdzal, Adriana Romero-Soriano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21840">https://arxiv.org/abs/2510.21840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21840">https://arxiv.org/pdf/2510.21840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21840]] Improving the Physics of Video Generation with VJEPA-2 Reward Signal(https://arxiv.org/abs/2510.21840)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This is a short technical report describing the winning entry of the PhysicsIQ Challenge, presented at the Perception Test Workshop at ICCV 2025. State-of-the-art video generative models exhibit severely limited physical understanding, and often produce implausible videos. The Physics IQ benchmark has shown that visual realism does not imply physics understanding. Yet, intuitive physics understanding has shown to emerge from SSL pretraining on natural videos. In this report, we investigate whether we can leverage SSL-based video world models to improve the physics plausibility of video generative models. In particular, we build ontop of the state-of-the-art video generative model MAGI-1 and couple it with the recently introduced Video Joint Embedding Predictive Architecture 2 (VJEPA-2) to guide the generation process. We show that by leveraging VJEPA-2 as reward signal, we can improve the physics plausibility of state-of-the-art video generative models by ~6%.</li>
</ul>

<h3>Title: RatioWaveNet: A Learnable RDWT Front-End for Robust and Interpretable EEG Motor-Imagery Classification</h3>
<ul>
<li><strong>Authors: </strong>Marco Siino, Giuseppe Bonomo, Rosario Sorbello, Ilenia Tinnirello</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21841">https://arxiv.org/abs/2510.21841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21841">https://arxiv.org/pdf/2510.21841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21841]] RatioWaveNet: A Learnable RDWT Front-End for Robust and Interpretable EEG Motor-Imagery Classification(https://arxiv.org/abs/2510.21841)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Brain-computer interfaces (BCIs) based on motor imagery (MI) translate covert movement intentions into actionable commands, yet reliable decoding from non-invasive EEG remains challenging due to nonstationarity, low SNR, and subject variability. We present RatioWaveNet, which augments a strong temporal CNN-Transformer backbone (TCFormer) with a trainable, Rationally-Dilated Wavelet Transform (RDWT) front end. The RDWT performs an undecimated, multi-resolution subband decomposition that preserves temporal length and shift-invariance, enhancing sensorimotor rhythms while mitigating jitter and mild artifacts; subbands are fused via lightweight grouped 1-D convolutions and passed to a multi-kernel CNN for local temporal-spatial feature extraction, a grouped-query attention encoder for long-range context, and a compact TCN head for causal temporal integration. Our goal is to test whether this principled wavelet front end improves robustness precisely where BCIs typically fail - on the hardest subjects - and whether such gains persist on average across seeds under both intra- and inter-subject protocols. On BCI-IV-2a and BCI-IV-2b, across five seeds, RatioWaveNet improves worst-subject accuracy over the Transformer backbone by +0.17 / +0.42 percentage points (Sub-Dependent / LOSO) on 2a and by +1.07 / +2.54 percentage points on 2b, with consistent average-case gains and modest computational overhead. These results indicate that a simple, trainable wavelet front end is an effective plug-in to strengthen Transformer-based BCIs, improving worst-case reliability without sacrificing efficiency.</li>
</ul>

<h3>Title: KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group</h3>
<ul>
<li><strong>Authors: </strong>Azree Nazri</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21844">https://arxiv.org/abs/2510.21844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21844">https://arxiv.org/pdf/2510.21844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21844]] KARIPAP: Quantum-Inspired Tensor Network Compression of Large Language Models Using Infinite Projected Entangled Pair States and Tensor Renormalization Group(https://arxiv.org/abs/2510.21844)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) like ChatGPT and LLaMA drive rapid progress in generative AI, yet their huge parameter scales create severe computational and environmental burdens. High training costs, energy use, and limited device deployment hinder accessibility. Existing compression - pruning, distillation, low-rank, and quantization - reduces size but ignores complex inter-layer correlations. We propose KARIPAP, a quantum-inspired tensor network compression using Infinite Projected Entangled Pair States (iPEPS) and Tensor Renormalization Group (TRG) contraction. Unlike 1D Matrix Product States, iPEPS captures multi-directional entanglement in attention and deep transformer layers. TRG ensures polynomial-time contraction, making tensorization feasible while preserving key correlation geometry. Experiments on LLaMA-2 7B show up to 93% memory and 70% parameter reduction, with 50% faster training, 25% faster inference, and only 2-3% accuracy loss. Layer-wise entanglement profiling reveals redundancy in deeper layers, confirming their suitability for tensor factorization. KARIPAP demonstrates that modern LLMs occupy low-dimensional entanglement manifolds, enabling scalable, energy-efficient, and quantum-aware AI architectures.</li>
</ul>

<h3>Title: Training data membership inference via Gaussian process meta-modeling: a post-hoc analysis approach</h3>
<ul>
<li><strong>Authors: </strong>Yongchao Huang, Pengfei Zhang, Shahzad Mumtaz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21846">https://arxiv.org/abs/2510.21846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21846">https://arxiv.org/pdf/2510.21846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21846]] Training data membership inference via Gaussian process meta-modeling: a post-hoc analysis approach(https://arxiv.org/abs/2510.21846)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) test whether a data point was part of a model's training set, posing serious privacy risks. Existing methods often depend on shadow models or heavy query access, which limits their practicality. We propose GP-MIA, an efficient and interpretable approach based on Gaussian process (GP) meta-modeling. Using post-hoc metrics such as accuracy, entropy, dataset statistics, and optional sensitivity features (e.g. gradients, NTK measures) from a single trained model, GP-MIA trains a GP classifier to distinguish members from non-members while providing calibrated uncertainty estimates. Experiments on synthetic data, real-world fraud detection data, CIFAR-10, and WikiText-2 show that GP-MIA achieves high accuracy and generalizability, offering a practical alternative to existing MIAs.</li>
</ul>

<h3>Title: SynCast: Synergizing Contradictions in Precipitation Nowcasting via Diffusion Sequential Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kaiyi Xu, Junchao Gong, Wenlong Zhang, Ben Fei, Lei Bai, Wanli Ouyang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21847">https://arxiv.org/abs/2510.21847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21847">https://arxiv.org/pdf/2510.21847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21847]] SynCast: Synergizing Contradictions in Precipitation Nowcasting via Diffusion Sequential Preference Optimization(https://arxiv.org/abs/2510.21847)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Precipitation nowcasting based on radar echoes plays a crucial role in monitoring extreme weather and supporting disaster prevention. Although deep learning approaches have achieved significant progress, they still face notable limitations. For example, deterministic models tend to produce over-smoothed predictions, which struggle to capture extreme events and fine-scale precipitation patterns. Probabilistic generative models, due to their inherent randomness, often show fluctuating performance across different metrics and rarely achieve consistently optimal results. Furthermore, precipitation nowcasting is typically evaluated using multiple metrics, some of which are inherently conflicting. For instance, there is often a trade-off between the Critical Success Index (CSI) and the False Alarm Ratio (FAR), making it challenging for existing models to deliver forecasts that perform well on both metrics simultaneously. To address these challenges, we introduce preference optimization into precipitation nowcasting for the first time, motivated by the success of reinforcement learning from human feedback in large language models. Specifically, we propose SynCast, a method that employs the two-stage post-training framework of Diffusion Sequential Preference Optimization (Diffusion-SPO), to progressively align conflicting metrics and consistently achieve superior performance. In the first stage, the framework focuses on reducing FAR, training the model to effectively suppress false alarms. Building on this foundation, the second stage further optimizes CSI with constraints that preserve FAR alignment, thereby achieving synergistic improvements across these conflicting metrics.</li>
</ul>

<h3>Title: Policy Optimization Prefers The Path of Least Resistance</h3>
<ul>
<li><strong>Authors: </strong>Debdeep Sanyal, Aakash Sen Sharma, Dhruv Kumar, Saurabh Deshpande, Murari Mandal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21853">https://arxiv.org/abs/2510.21853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21853">https://arxiv.org/pdf/2510.21853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21853]] Policy Optimization Prefers The Path of Least Resistance(https://arxiv.org/abs/2510.21853)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Policy optimization (PO) algorithms are used to refine Large Language Models for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a strict think-then-answer format to elicit chain-of-thought (CoT); however, the behavior of PO when these rigid constraints are relaxed into an open-ended CoT structure remains an under-studied question. We investigate this gap with an extensive suite of controlled experiments and identify a consistent principle: \textit{policy optimization consistently follows the path of least resistance}. When afforded the flexibility to interleave reasoning and response, policy optimization consistently learns to discard explicit reasoning, causing the policy to degenerate to a direct \texttt{<answer>}-only format. This outcome holds true across various models and algorithms. We find that this collapse in format is persistent even when the complex \texttt{<think><answer>} format is assigned up to 4x larger reward weights. We formalize this principle through a series of controlled reward decomposition experiments, demonstrating a clear hierarchy: PO systematically optimizes for the simplest reward component first, a preference that holds even when faced with mutually exclusive choices or strong incentives for more complex behaviors. Finally, we show that successful convergence on the high-reward shortcut is not a low-effort drift but is driven by the optimization process that requires the KL-regularized policy to have sufficient freedom to make a significant shift from its initial prior. Our findings reveal that granting policies the freedom to diverge is a double-edged sword: while necessary for discovering high-reward shortcuts, it also creates a powerful incentive to game the simplest aspects of the reward function, posing a critical challenge for reward hacking under alignment.</li>
</ul>

<h3>Title: Poisson Flow Consistency Training</h3>
<ul>
<li><strong>Authors: </strong>Anthony Zhang, Mahmut Gokmen, Dennis Hein, Rongjun Ge, Wenjun Xia, Ge Wang, Jin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21857">https://arxiv.org/abs/2510.21857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21857">https://arxiv.org/pdf/2510.21857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21857]] Poisson Flow Consistency Training(https://arxiv.org/abs/2510.21857)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The Poisson Flow Consistency Model (PFCM) is a consistency-style model based on the robust Poisson Flow Generative Model++ (PFGM++) which has achieved success in unconditional image generation and CT image denoising. Yet the PFCM can only be trained in distillation which limits the potential of the PFCM in many data modalities. The objective of this research was to create a method to train the PFCM in isolation called Poisson Flow Consistency Training (PFCT). The perturbation kernel was leveraged to remove the pretrained PFGM++, and the sinusoidal discretization schedule and Beta noise distribution were introduced in order to facilitate adaptability and improve sample quality. The model was applied to the task of low dose computed tomography image denoising and improved the low dose image in terms of LPIPS and SSIM. It also displayed similar denoising effectiveness as models like the Consistency Model. PFCT is established as a valid method of training the PFCM from its effectiveness in denoising CT images, showing potential with competitive results to other generative models. Further study is needed in the precise optimization of PFCT and in its applicability to other generative modeling tasks. The framework of PFCT creates more flexibility for the ways in which a PFCM can be created and can be applied to the field of generative modeling.</li>
</ul>

<h3>Title: Privacy-preserving Decision-focused Learning for Multi-energy Systems</h3>
<ul>
<li><strong>Authors: </strong>Yangze Zhou, Ruiyang Yao, Dalin Qin, Yixiong Jia, Yi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21858">https://arxiv.org/abs/2510.21858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21858">https://arxiv.org/pdf/2510.21858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21858]] Privacy-preserving Decision-focused Learning for Multi-energy Systems(https://arxiv.org/abs/2510.21858)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Decision-making for multi-energy system (MES) dispatch depends on accurate load forecasting. Traditionally, load forecasting and decision-making for MES are implemented separately. Forecasting models are typically trained to minimize forecasting errors, overlooking their impact on downstream decision-making. To address this, decision-focused learning (DFL) has been studied to minimize decision-making costs instead. However, practical adoption of DFL in MES faces significant challenges: the process requires sharing sensitive load data and model parameters across multiple sectors, raising serious privacy issues. To this end, we propose a privacy-preserving DFL framework tailored for MES. Our approach introduces information masking to safeguard private data while enabling recovery of decision variables and gradients required for model training. To further enhance security for DFL, we design a safety protocol combining matrix decomposition and homomorphic encryption, effectively preventing collusion and unauthorized data access. Additionally, we developed a privacy-preserving load pattern recognition algorithm, enabling the training of specialized DFL models for heterogeneous load patterns. Theoretical analysis and comprehensive case studies, including real-world MES data, demonstrate that our framework not only protects privacy but also consistently achieves lower average daily dispatch costs compared to existing methods.</li>
</ul>

<h3>Title: The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems</h3>
<ul>
<li><strong>Authors: </strong>Bentley DeVilling (Course Correct Labs, Independent Research Group)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21861">https://arxiv.org/abs/2510.21861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21861">https://arxiv.org/pdf/2510.21861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21861]] The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems(https://arxiv.org/abs/2510.21861)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are often described as capable of reflective reasoning, yet recursive self-evaluation without external feedback frequently yields reformulation rather than progress. We test this prediction in a cross-provider study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini, Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families (arithmetic, code, explanation, reflection), each iterated ten times under two conditions: ungrounded self-critique and a minimal grounding intervention (a single verification step at iteration three). Mean informational change (delta I, measured via normalized edit distance) declined by 55% from early (0.193) to late (0.087) iterations in ungrounded runs, with consistent patterns across all three providers. Grounded runs showed a +28% rebound in informational change immediately after the intervention and sustained non-zero variance thereafter. Complementary measures-n-gram novelty, embedding drift, and character-level entropy-converged on the same pattern: reflection without contact tends toward informational closure. We interpret this as evidence for a structural limit on self-correction in generative reasoning: without an exchange of information with an independent verifier or environment, recursive inference approaches an attractor state of epistemic stasis. Minimal grounding functions as dissipative coupling, reintroducing informational flux. The cross-architecture consistency suggests the mirror loop arises from shared autoregressive training objectives rather than provider-specific alignment schemes. The results delineate when reflection is performative rather than epistemic and motivate design principles for grounded, cooperative reasoning. Materials and code are publicly available.</li>
</ul>

<h3>Title: A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Tayyab Khan, Zane Yong, Lequn Chen, Wenhe Feng, Nicholas Yew Jin Tan, Seung Ki Moon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21862">https://arxiv.org/abs/2510.21862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21862">https://arxiv.org/pdf/2510.21862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21862]] A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model(https://arxiv.org/abs/2510.21862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Engineering drawings are fundamental to manufacturing communication, serving as the primary medium for conveying design intent, tolerances, and production details. However, interpreting complex multi-view drawings with dense annotations remains challenging using manual methods, generic optical character recognition (OCR) systems, or traditional deep learning approaches, due to varied layouts, orientations, and mixed symbolic-textual content. To address these challenges, this paper proposes a three-stage hybrid framework for the automated interpretation of 2D multi-view engineering drawings using modern detection and vision language models (VLMs). In the first stage, YOLOv11-det performs layout segmentation to localize key regions such as views, title blocks, and notes. The second stage uses YOLOv11-obb for orientation-aware, fine-grained detection of annotations, including measures, GD&T symbols, and surface roughness indicators. The third stage employs two Donut-based, OCR-free VLMs for semantic content parsing: the Alphabetical VLM extracts textual and categorical information from title blocks and notes, while the Numerical VLM interprets quantitative data such as measures, GD&T frames, and surface roughness. Two specialized datasets were developed to ensure robustness and generalization: 1,000 drawings for layout detection and 1,406 for annotation-level training. The Alphabetical VLM achieved an overall F1 score of 0.672, while the Numerical VLM reached 0.963, demonstrating strong performance in textual and quantitative interpretation, respectively. The unified JSON output enables seamless integration with CAD and manufacturing databases, providing a scalable solution for intelligent engineering drawing analysis.</li>
</ul>

<h3>Title: LSF-Animation: Label-Free Speech-Driven Facial Animation via Implicit Feature Representation</h3>
<ul>
<li><strong>Authors: </strong>Xin Lu, Chuanqing Zhuang, Chenxi Jin, Zhengda Lu, Yiqun Wang, Wu Liu, Jun Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21864">https://arxiv.org/abs/2510.21864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21864">https://arxiv.org/pdf/2510.21864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21864]] LSF-Animation: Label-Free Speech-Driven Facial Animation via Implicit Feature Representation(https://arxiv.org/abs/2510.21864)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Speech-driven 3D facial animation has attracted increasing interest since its potential to generate expressive and temporally synchronized digital humans. While recent works have begun to explore emotion-aware animation, they still depend on explicit one-hot encodings to represent identity and emotion with given emotion and identity labels, which limits their ability to generalize to unseen speakers. Moreover, the emotional cues inherently present in speech are often neglected, limiting the naturalness and adaptability of generated animations. In this work, we propose LSF-Animation, a novel framework that eliminates the reliance on explicit emotion and identity feature representations. Specifically, LSF-Animation implicitly extracts emotion information from speech and captures the identity features from a neutral facial mesh, enabling improved generalization to unseen speakers and emotional states without requiring manual labels. Furthermore, we introduce a Hierarchical Interaction Fusion Block (HIFB), which employs a fusion token to integrate dual transformer features and more effectively integrate emotional, motion-related and identity-related cues. Extensive experiments conducted on the 3DMEAD dataset demonstrate that our method surpasses recent state-of-the-art approaches in terms of emotional expressiveness, identity generalization, and animation realism. The source code will be released at: this https URL.</li>
</ul>

<h3>Title: Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Haicheng Liao, Bonan Wang, Junxian Yang, Chengyue Wang, Zhengbin He, Guohui Zhang, Chengzhong Xu, Zhenning Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21867">https://arxiv.org/abs/2510.21867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21867">https://arxiv.org/pdf/2510.21867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21867]] Addressing Corner Cases in Autonomous Driving: A World Model-based Approach with Mixture of Experts and LLMs(https://arxiv.org/abs/2510.21867)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Accurate and reliable motion forecasting is essential for the safe deployment of autonomous vehicles (AVs), particularly in rare but safety-critical scenarios known as corner cases. Existing models often underperform in these situations due to an over-representation of common scenes in training data and limited generalization capabilities. To address this limitation, we present WM-MoE, the first world model-based motion forecasting framework that unifies perception, temporal memory, and decision making to address the challenges of high-risk corner-case scenarios. The model constructs a compact scene representation that explains current observations, anticipates future dynamics, and evaluates the outcomes of potential actions. To enhance long-horizon reasoning, we leverage large language models (LLMs) and introduce a lightweight temporal tokenizer that maps agent trajectories and contextual cues into the LLM's feature space without additional training, enriching temporal context and commonsense priors. Furthermore, a mixture-of-experts (MoE) is introduced to decompose complex corner cases into subproblems and allocate capacity across scenario types, and a router assigns scenes to specialized experts that infer agent intent and perform counterfactual rollouts. In addition, we introduce nuScenes-corner, a new benchmark that comprises four real-world corner-case scenarios for rigorous evaluation. Extensive experiments on four benchmark datasets (nuScenes, NGSIM, HighD, and MoCAD) showcase that WM-MoE consistently outperforms state-of-the-art (SOTA) baselines and remains robust under corner-case and data-missing conditions, indicating the promise of world model-based architectures for robust and generalizable motion forecasting in fully AVs.</li>
</ul>

<h3>Title: Language Ranker: A Lightweight Ranking framework for LLM Decoding</h3>
<ul>
<li><strong>Authors: </strong>Chenheng Zhang, Tianqi Du, Jizhe Zhang, Mingqing Xiao, Yifei Wang, Yisen Wang, Zhouchen Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21883">https://arxiv.org/abs/2510.21883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21883">https://arxiv.org/pdf/2510.21883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21883]] Language Ranker: A Lightweight Ranking framework for LLM Decoding(https://arxiv.org/abs/2510.21883)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conventional research on large language models (LLMs) has primarily focused on refining output distributions, while paying less attention to the decoding process that transforms these distributions into final responses. Recent advances, such as scaling the computation of inference time with reward models, have underscored the importance of decoding, but these methods often suffer from high computational costs and limited applicability. In this paper, we revisit LLM generation through the lens of recommender systems, conceptualizing the decoding process as analogous to the ranking stage in recommendation pipelines. From this perspective, we observe that both traditional decoding methods and reward models exhibit clear limitations such as redundancy. Motivated by this insight, we propose Language Ranker, a novel framework that introduces a lightweight module to rerank candidate responses using features extracted by the base model. Experiments across a wide range of tasks show that Language Ranker achieves performance comparable to large-scale reward models, while requiring only <0.5M additional parameters, significantly reducing the computational overhead during both training and inference stages. This highlights the efficiency and effectiveness of our method, showcasing its potential to fully unlock the capabilities of LLMs.</li>
</ul>

<h3>Title: Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks</h3>
<ul>
<li><strong>Authors: </strong>Avinash Patil</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21884">https://arxiv.org/abs/2510.21884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21884">https://arxiv.org/pdf/2510.21884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21884]] Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks(https://arxiv.org/abs/2510.21884)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing adoption of machine learning (ML) in sensitive domains has heightened the demand for transparent and interpretable artificial intelligence. Large Language Models (LLMs) are increasingly capable of producing natural language explanations, yet it remains unclear whether these rationales faithfully capture the predictive signals that underlie decisions. This paper introduces RACE-Reasoning Alignment for Completeness of Explanations, a systematic framework to evaluate the alignment between LLM-generated explanations and interpretable feature importance scores derived from a logistic regression baseline. We analyze four widely used text classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and compare LLM rationales against top-ranked supporting and contradicting lexical features. To capture alignment at multiple levels of granularity, RACE implements token-aware, exact string, and edit-distance matching techniques. Empirical results reveal a consistent asymmetry: correct predictions exhibit higher coverage of supporting features, while incorrect predictions are associated with elevated coverage of contradicting features. Edit-distance matching further uncovers paraphrastic overlaps, boosting coverage while preserving this asymmetry. These findings demonstrate that LLM rationales combine both surface-level and flexible evidence reuse, yet can also amplify misleading cues in error cases. RACE provides new insights into the faithfulness of LLM explanations and establishes a quantitative basis for evaluating reasoning completeness in neural language models.</li>
</ul>

<h3>Title: Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Anh Pham, Mihir Thalanki, Michael Sun, Aditya Chaloo, Ankita Gupta, Tian Xia, Aditya Mate, Ehimwenma Nosakhare, Soundararajan Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21885">https://arxiv.org/abs/2510.21885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21885">https://arxiv.org/pdf/2510.21885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21885]] Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning(https://arxiv.org/abs/2510.21885)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models often lose previously aligned safety behaviors when fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior work shows that adding random safety examples can mitigate this effect, but it remains unclear which examples are most effective. We propose a behavior-aware sampling framework that selects safety examples based on two complementary factors: instruction-response behavior (e.g., refusal versus compliance) and semantic diversity across harm categories. Systematic evaluation shows that this approach substantially reduces harmful outputs while maintaining helpfulness, achieving up to a 41% reduction in harmfulness with only 0.5% additional training data. These results highlight how targeted data selection can improve the safety and efficiency of fine-tuning at scale.</li>
</ul>

<h3>Title: Generative AI in Depth: A Survey of Recent Advances, Model Variants, and Real-World Applications</h3>
<ul>
<li><strong>Authors: </strong>Shamim Yazdani, Akansha Singh, Nripsuta Saxena, Zichong Wang, Avash Palikhe, Deng Pan, Umapada Pal, Jie Yang, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21887">https://arxiv.org/abs/2510.21887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21887">https://arxiv.org/pdf/2510.21887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21887]] Generative AI in Depth: A Survey of Recent Advances, Model Variants, and Real-World Applications(https://arxiv.org/abs/2510.21887)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, deep learning based generative models, particularly Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion Models (DMs), have been instrumental in in generating diverse, high-quality content across various domains, such as image and video synthesis. This capability has led to widespread adoption of these models and has captured strong public interest. As they continue to advance at a rapid pace, the growing volume of research, expanding application areas, and unresolved technical challenges make it increasingly difficult to stay current. To address this need, this survey introduces a comprehensive taxonomy that organizes the literature and provides a cohesive framework for understanding the development of GANs, VAEs, and DMs, including their many variants and combined approaches. We highlight key innovations that have improved the quality, diversity, and controllability of generated outputs, reflecting the expanding potential of generative artificial intelligence. In addition to summarizing technical progress, we examine rising ethical concerns, including the risks of misuse and the broader societal impact of synthetic media. Finally, we outline persistent challenges and propose future research directions, offering a structured and forward looking perspective for researchers in this fast evolving field.</li>
</ul>

<h3>Title: The Principles of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chieh-Hsin Lai, Yang Song, Dongjun Kim, Yuki Mitsufuji, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21890">https://arxiv.org/abs/2510.21890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21890">https://arxiv.org/pdf/2510.21890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21890]] The Principles of Diffusion Models(https://arxiv.org/abs/2510.21890)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions. The goal is to learn a reverse process that transforms noise back into data while recovering the same intermediates. We describe three complementary views. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, indicating how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following a smooth path that moves samples from noise to data under a learned velocity field. These perspectives share a common backbone: a time-dependent velocity field whose flow transports a simple prior to the data. Sampling then amounts to solving a differential equation that evolves noise into data along a continuous trajectory. On this foundation, the monograph discusses guidance for controllable generation, efficient numerical solvers, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times. It provides a conceptual and mathematically grounded understanding of diffusion models for readers with basic deep-learning knowledge.</li>
</ul>

<h3>Title: Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Dhrupad Bhardwaj, Julia Kempe, Tim G. J. Rudner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21891">https://arxiv.org/abs/2510.21891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21891">https://arxiv.org/pdf/2510.21891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21891]] Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation(https://arxiv.org/abs/2510.21891)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To deploy large language models (LLMs) in high-stakes application domains that require substantively accurate responses to open-ended prompts, we need reliable, computationally inexpensive methods that assess the trustworthiness of long-form responses generated by LLMs. However, existing approaches often rely on claim-by-claim fact-checking, which is computationally expensive and brittle in long-form responses to open-ended prompts. In this work, we introduce semantic isotropy -- the degree of uniformity across normalized text embeddings on the unit sphere -- and use it to assess the trustworthiness of long-form responses generated by LLMs. To do so, we generate several long-form responses, embed them, and estimate the level of semantic isotropy of these responses as the angular dispersion of the embeddings on the unit sphere. We find that higher semantic isotropy -- that is, greater embedding dispersion -- reliably signals lower factual consistency across samples. Our approach requires no labeled data, no fine-tuning, and no hyperparameter selection, and can be used with open- or closed-weight embedding models. Across multiple domains, our method consistently outperforms existing approaches in predicting nonfactuality in long-form responses using only a handful of samples -- offering a practical, low-cost approach for integrating trust assessment into real-world LLM workflows.</li>
</ul>

<h3>Title: Understanding Network Behaviors through Natural Language Question-Answering</h3>
<ul>
<li><strong>Authors: </strong>Mingzhe Xing, Chang Tian, Jianan Zhang, Lichen Pan, Peipei Liu, Zhaoteng Yan, Yinliang Yue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21894">https://arxiv.org/abs/2510.21894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21894">https://arxiv.org/pdf/2510.21894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21894]] Understanding Network Behaviors through Natural Language Question-Answering(https://arxiv.org/abs/2510.21894)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models. While effective, they suffer from a steep learning curve and limited flexibility. In contrast, natural language (NL) offers a more accessible and interpretable interface, motivating recent research on NL-guided network behavior understanding. Recent advances in large language models (LLMs) further enhance this direction, leveraging their extensive prior knowledge of network concepts and strong reasoning capabilities. However, three key challenges remain: 1) numerous router devices with lengthy configuration files challenge LLM's long-context understanding ability; 2) heterogeneity across devices and protocols impedes scalability; and 3) complex network topologies and protocols demand advanced reasoning abilities beyond the current capabilities of LLMs. To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL. Our approach introduces a tree-based configuration chunking strategy to preserve semantic coherence while enabling efficient partitioning. We then construct a unified fact graph as an intermediate representation to normalize vendor-specific configurations. Finally, we design a hybrid imperative-declarative language to reduce the reasoning burden on LLMs and enhance precision. We contribute a benchmark consisting of NL question-answer pairs paired with network configurations. Experiments demonstrate that NetMind achieves accurate and scalable network behavior understanding, outperforming existing baselines.</li>
</ul>

<h3>Title: A supervised discriminant data representation: application to pattern classification</h3>
<ul>
<li><strong>Authors: </strong>Fadi Dornaika, Ahmad Khoder, Abdelmalik Moujahid, Wassim Khoder</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21898">https://arxiv.org/abs/2510.21898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21898">https://arxiv.org/pdf/2510.21898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21898]] A supervised discriminant data representation: application to pattern classification(https://arxiv.org/abs/2510.21898)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>The performance of machine learning and pattern recognition algorithms generally depends on data representation. That is why, much of the current effort in performing machine learning algorithms goes into the design of preprocessing frameworks and data transformations able to support effective machine learning. The method proposed in this work consists of a hybrid linear feature extraction scheme to be used in supervised multi-class classification problems. Inspired by two recent linear discriminant methods: robust sparse linear discriminant analysis (RSLDA) and inter-class sparsitybased discriminative least square regression (ICS_DLSR), we propose a unifying criterion that is able to retain the advantages of these two powerful methods. The resulting transformation relies on sparsity-promoting techniques both to select the features that most accurately represent the data and to preserve the row-sparsity consistency property of samples from the same class. The linear transformation and the orthogonal matrix are estimated using an iterative alternating minimization scheme based on steepest descent gradient method and different initialization schemes. The proposed framework is generic in the sense that it allows the combination and tuning of other linear discriminant embedding methods. According to the experiments conducted on several datasets including faces, objects, and digits, the proposed method was able to outperform competing methods in most cases.</li>
</ul>

<h3>Title: Adversarial D√©j√† Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks</h3>
<ul>
<li><strong>Authors: </strong>Mahavir Dabas, Tran Huynh, Nikhil Reddy Billa, Jiachen T. Wang, Peng Gao, Charith Peris, Yao Ma, Rahul Gupta, Ming Jin, Prateek Mittal, Ruoxi Jia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21910">https://arxiv.org/abs/2510.21910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21910">https://arxiv.org/pdf/2510.21910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21910]] Adversarial D√©j√† Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks(https://arxiv.org/abs/2510.21910)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models remain vulnerable to jailbreak attacks that bypass safety guardrails to elicit harmful outputs. Defending against novel jailbreaks represents a critical challenge in AI safety. Adversarial training -- designed to make models robust against worst-case perturbations -- has been the dominant paradigm for adversarial robustness. However, due to optimization challenges and difficulties in defining realistic threat models, adversarial training methods often fail on newly developed jailbreaks in practice. This paper proposes a new paradigm for improving robustness against unseen jailbreaks, centered on the Adversarial D√©j√† Vu hypothesis: novel jailbreaks are not fundamentally new, but largely recombinations of adversarial skills from previous attacks. We study this hypothesis through a large-scale analysis of 32 attack papers published over two years. Using an automated pipeline, we extract and compress adversarial skills into a sparse dictionary of primitives, with LLMs generating human-readable descriptions. Our analysis reveals that unseen attacks can be effectively explained as sparse compositions of earlier skills, with explanatory power increasing monotonically as skill coverage grows. Guided by this insight, we introduce Adversarial Skill Compositional Training (ASCoT), which trains on diverse compositions of skill primitives rather than isolated attack instances. ASCoT substantially improves robustness to unseen attacks, including multi-turn jailbreaks, while maintaining low over-refusal rates. We also demonstrate that expanding adversarial skill coverage, not just data scale, is key to defending against novel attacks. \textcolor{red}{\textbf{Warning: This paper contains content that may be harmful or offensive in nature.</li>
</ul>

<h3>Title: AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing</h3>
<ul>
<li><strong>Authors: </strong>Samuel Bright-Thonney, Christina Reissel, Gaia Grosso, Nathaniel Woodward, Katya Govorkova, Andrzej Novak, Sang Eon Park, Eric Moreno, Philip Harris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21935">https://arxiv.org/abs/2510.21935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21935">https://arxiv.org/pdf/2510.21935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21935]] AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing(https://arxiv.org/abs/2510.21935)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Novelty detection in large scientific datasets faces two key challenges: the noisy and high-dimensional nature of experimental data, and the necessity of making statistically robust statements about any observed outliers. While there is a wealth of literature on anomaly detection via dimensionality reduction, most methods do not produce outputs compatible with quantifiable claims of scientific discovery. In this work we directly address these challenges, presenting the first step towards a unified pipeline for novelty detection adapted for the rigorous statistical demands of science. We introduce AutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive Testing), a general-purpose pipeline for detecting novelty in scientific data. AutoSciDACT begins by creating expressive low-dimensional data representations using a contrastive pre-training, leveraging the abundance of high-quality simulated data in many scientific domains alongside expertise that can guide principled data augmentation strategies. These compact embeddings then enable an extremely sensitive machine learning-based two-sample test using the New Physics Learning Machine (NPLM) framework, which identifies and statistically quantifies deviations in observed data relative to a reference distribution (null hypothesis). We perform experiments across a range of astronomical, physical, biological, image, and synthetic datasets, demonstrating strong sensitivity to small injections of anomalous data across all domains.</li>
</ul>

<h3>Title: $Œ¥$-STEAL: LLM Stealing Attack with Local Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Kieu Dang, Phung Lai, NhatHai Phan, Yelong Shen, Ruoming Jin, Abdallah Khreishah</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21946">https://arxiv.org/abs/2510.21946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21946">https://arxiv.org/pdf/2510.21946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21946]] $Œ¥$-STEAL: LLM Stealing Attack with Local Differential Privacy(https://arxiv.org/abs/2510.21946)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, steal, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable capabilities across various tasks. However, their deployment introduces significant risks related to intellectual property. In this context, we focus on model stealing attacks, where adversaries replicate the behaviors of these models to steal services. These attacks are highly relevant to proprietary LLMs and pose serious threats to revenue and financial stability. To mitigate these risks, the watermarking solution embeds imperceptible patterns in LLM outputs, enabling model traceability and intellectual property verification. In this paper, we study the vulnerability of LLM service providers by introducing $\delta$-STEAL, a novel model stealing attack that bypasses the service provider's watermark detectors while preserving the adversary's model utility. $\delta$-STEAL injects noise into the token embeddings of the adversary's model during fine-tuning in a way that satisfies local differential privacy (LDP) guarantees. The adversary queries the service provider's model to collect outputs and form input-output training pairs. By applying LDP-preserving noise to these pairs, $\delta$-STEAL obfuscates watermark signals, making it difficult for the service provider to determine whether its outputs were used, thereby preventing claims of model theft. Our experiments show that $\delta$-STEAL with lightweight modifications achieves attack success rates of up to $96.95\%$ without significantly compromising the adversary's model utility. The noise scale in LDP controls the trade-off between attack effectiveness and model utility. This poses a significant risk, as even robust watermarks can be bypassed, allowing adversaries to deceive watermark detectors and undermine current intellectual property protection methods.</li>
</ul>

<h3>Title: Model-Aware Tokenizer Transfer</h3>
<ul>
<li><strong>Authors: </strong>Mykola Haltiuk, Aleksander Smywi≈Ñski-Pohl</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21954">https://arxiv.org/abs/2510.21954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21954">https://arxiv.org/pdf/2510.21954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21954]] Model-Aware Tokenizer Transfer(https://arxiv.org/abs/2510.21954)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained to support an increasing number of languages, yet their predefined tokenizers remain a bottleneck for adapting models to lower-resource or distinct-script languages. Existing tokenizer transfer methods typically rely on semantic heuristics to initialize new embeddings, ignoring higher-layer model dynamics and limiting transfer quality. We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates model internals into the tokenizer transfer process. MATT introduces an Attention Influence Modeling (AIM) objective that distills inter-token communication patterns from a source model into a target model with a new tokenizer, providing an efficient warm-up before standard language modeling. Unlike approaches that focus solely on embedding similarity, MATT leverages attention behavior to guide embedding initialization and adaptation. Experiments across diverse linguistic settings show that MATT recovers a large fraction of the original model's performance within a few GPU hours, outperforming heuristic baselines. These results demonstrate that incorporating model-level signals offers a practical and effective path toward robust tokenizer transfer in multilingual LLMs.</li>
</ul>

<h3>Title: Transformer Based Linear Attention with Optimized GPU Kernel Implementation</h3>
<ul>
<li><strong>Authors: </strong>Armin Gerami, Ramani Duraiswami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21956">https://arxiv.org/abs/2510.21956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21956">https://arxiv.org/pdf/2510.21956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21956]] Transformer Based Linear Attention with Optimized GPU Kernel Implementation(https://arxiv.org/abs/2510.21956)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The original softmax-based attention mechanism (regular attention) in the extremely successful Transformer architecture computes attention between $N$ tokens, each embedded in a $D$-dimensional head, with a time complexity of $O(N^2D)$. Given the success of Transformers, improving their runtime during both training and inference is a popular research area. One such approach is the introduction of the linear attention (LA) mechanisms, which offers a linear time complexity of $O(ND^2)$ and have demonstrated comparable accuracy to regular attention. However, LA in practice lags behind its theoretical efficiency. We propose a novel method for LA's forward and backward passes, along with a highly-optimized CUDA implementation. Our approach outperforms the state-of-the-art by 3.3 times in speed and reduces memory consumption by 3.6 times. We validate these improvements in both single-layer and end-to-end settings by training a 1.4 billion parameter language model, which demonstrates similar expressivity to regular attention on major reasoning benchmarks.</li>
</ul>

<h3>Title: Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhixin Pan, Ziyu Shu, Amberbir Alemayoh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21957">https://arxiv.org/abs/2510.21957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21957">https://arxiv.org/pdf/2510.21957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21957]] Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning(https://arxiv.org/abs/2510.21957)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Ransomware has become a critical threat to cybersecurity due to its rapid evolution, the necessity for early detection, and growing diversity, posing significant challenges to traditional detection methods. While AI-based approaches had been proposed by prior works to assist ransomware detection, existing methods suffer from three major limitations, ad-hoc feature dependencies, delayed response, and limited adaptability to unseen variants. In this paper, we propose a framework that integrates self-supervised contrastive learning with neural architecture search (NAS) to address these challenges. Specifically, this paper offers three important contributions. (1) We design a contrastive learning framework that incorporates hardware performance counters (HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a customized loss function that encourages early-stage detection of malicious activity, and significantly reduces the detection latency. (3) We deploy a neural architecture search (NAS) framework to automatically construct adaptive model architectures, allowing the detector to flexibly align with unseen ransomware variants. Experimental results show that our proposed method achieves significant improvements in both detection accuracy (up to 16.1%) and response time (up to 6x) compared to existing approaches while maintaining robustness under evasive attacks.</li>
</ul>

<h3>Title: A Stylometric Application of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Harrison F. Stropkay, Jiayi Chen, Mohammad J. Latifi, Daniel N. Rockmore, Jeremy R. Manning</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21958">https://arxiv.org/abs/2510.21958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21958">https://arxiv.org/pdf/2510.21958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21958]] A Stylometric Application of Large Language Models(https://arxiv.org/abs/2510.21958)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We show that large language models (LLMs) can be used to distinguish the writings of different authors. Specifically, an individual GPT-2 model, trained from scratch on the works of one author, will predict held-out text from that author more accurately than held-out text from other authors. We suggest that, in this way, a model trained on one author's works embodies the unique writing style of that author. We first demonstrate our approach on books written by eight different (known) authors. We also use this approach to confirm R. P. Thompson's authorship of the well-studied 15th book of the Oz series, originally attributed to F. L. Baum.</li>
</ul>

<h3>Title: Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing</h3>
<ul>
<li><strong>Authors: </strong>Iskander Azangulov, Teodora Pandeva, Niranjani Prasad, Javier Zazo, Sushrut Karmalkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21961">https://arxiv.org/abs/2510.21961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21961">https://arxiv.org/pdf/2510.21961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21961]] Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing(https://arxiv.org/abs/2510.21961)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) offer a compelling alternative to autoregressive models (ARMs) for discrete text generation because they enable parallel token sampling, rather than sequential, left-to-right generation. This means potentially much faster inference. However, effective parallel sampling faces two competing requirements: (i) simultaneously updated tokens must be conditionally independent, and (ii) updates should prioritise high-confidence predictions. These goals conflict because high-confidence predictions often cluster and depend on each other, opportunities for parallel updates. We present PUNT, a model-agnostic sampler that reconciles this trade-off. Our method identifies token dependencies and removes lower-confidence tokens from conflicting groups. This produces sets of indices for unmasking that satisfy both independence and confidence criteria. Our approach ensures improved parallel unmasking through approximate conditional independence testing. Our experiments show that PUNT delivers a superior trade-off between accuracy and compute when compared to other strong training-free baselines, especially for generation of longer sequences. On the IFEval benchmark, it achieves up to 16\% higher accuracy over baseline methods, including sequential generation (one-by-one). These gains hold across different values of hyperparameters, mitigating the need for brittle hyperparameter tuning. Moreover, we observe that PUNT induces an emergent hierarchical generation strategy, where the model first establishes high-level paragraph structure before local refinement, suggesting a planning-like generation process that contributes to strong alignment performance.</li>
</ul>

<h3>Title: Boltzmann Graph Ensemble Embeddings for Aptamer Libraries</h3>
<ul>
<li><strong>Authors: </strong>Starlika Bauskar, Jade Jiao, Narayanan Kannan, Alexander Kimm, Justin M. Baker, Matthew J. Tyler, Andrea L. Bertozzi, Anne M. Andrews</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR, q-bio.QM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21980">https://arxiv.org/abs/2510.21980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21980">https://arxiv.org/pdf/2510.21980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21980]] Boltzmann Graph Ensemble Embeddings for Aptamer Libraries(https://arxiv.org/abs/2510.21980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine-learning methods in biochemistry commonly represent molecules as graphs of pairwise intermolecular interactions for property and structure predictions. Most methods operate on a single graph, typically the minimal free energy (MFE) structure, for low-energy ensembles (conformations) representative of structures at thermodynamic equilibrium. We introduce a thermodynamically parameterized exponential-family random graph (ERGM) embedding that models molecules as Boltzmann-weighted ensembles of interaction graphs. We evaluate this embedding on SELEX datasets, where experimental biases (e.g., PCR amplification or sequencing noise) can obscure true aptamer-ligand affinity, producing anomalous candidates whose observed abundance diverges from their actual binding strength. We show that the proposed embedding enables robust community detection and subgraph-level explanations for aptamer ligand affinity, even in the presence of biased observations. This approach may be used to identify low-abundance aptamer candidates for further experimental evaluation.</li>
</ul>

<h3>Title: Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</h3>
<ul>
<li><strong>Authors: </strong>Havva Alizadeh Noughabi, Julien Serbanescu, Fattane Zarrinkalam, Ali Dehghantanha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21983">https://arxiv.org/abs/2510.21983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21983">https://arxiv.org/pdf/2510.21983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21983]] Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks(https://arxiv.org/abs/2510.21983)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Despite recent advances, Large Language Models remain vulnerable to jailbreak attacks that bypass alignment safeguards and elicit harmful outputs. While prior research has proposed various attack strategies differing in human readability and transferability, little attention has been paid to the linguistic and psychological mechanisms that may influence a model's susceptibility to such attacks. In this paper, we examine an interdisciplinary line of research that leverages foundational theories of persuasion from the social sciences to craft adversarial prompts capable of circumventing alignment constraints in LLMs. Drawing on well-established persuasive strategies, we hypothesize that LLMs, having been trained on large-scale human-generated text, may respond more compliantly to prompts with persuasive structures. Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive fingerprints that emerge in their jailbreak responses. Empirical evaluations across multiple aligned LLMs reveal that persuasion-aware prompts significantly bypass safeguards, demonstrating their potential to induce jailbreak behaviors. This work underscores the importance of cross-disciplinary insight in addressing the evolving challenges of LLM safety. The code and data are available.</li>
</ul>

<h3>Title: Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Dogyun Park, Moayed Haji-Ali, Yanyu Li, Willi Menapace, Sergey Tulyakov, Hyunwoo J. Kim, Aliaksandr Siarohin, Anil Kag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21986">https://arxiv.org/abs/2510.21986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21986">https://arxiv.org/pdf/2510.21986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21986]] Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers(https://arxiv.org/abs/2510.21986)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet na√Øve strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training.</li>
</ul>

<h3>Title: From Black-box to Causal-box: Towards Building More Interpretable Models</h3>
<ul>
<li><strong>Authors: </strong>Inwoo Hwang, Yushu Pan, Elias Bareinboim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.21998">https://arxiv.org/abs/2510.21998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.21998">https://arxiv.org/pdf/2510.21998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.21998]] From Black-box to Causal-box: Towards Building More Interpretable Models(https://arxiv.org/abs/2510.21998)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding the predictions made by deep learning models remains a central challenge, especially in high-stakes applications. A promising approach is to equip models with the ability to answer counterfactual questions -- hypothetical ``what if?'' scenarios that go beyond the observed data and provide insight into a model reasoning. In this work, we introduce the notion of causal interpretability, which formalizes when counterfactual queries can be evaluated from a specific class of models and observational data. We analyze two common model classes -- blackbox and concept-based predictors -- and show that neither is causally interpretable in general. To address this gap, we develop a framework for building models that are causally interpretable by design. Specifically, we derive a complete graphical criterion that determines whether a given model architecture supports a given counterfactual query. This leads to a fundamental tradeoff between causal interpretability and predictive accuracy, which we characterize by identifying the unique maximal set of features that yields an interpretable model with maximal predictive expressiveness. Experiments corroborate the theoretical findings.</li>
</ul>

<h3>Title: LiteDiff</h3>
<ul>
<li><strong>Authors: </strong>Ruchir Namjoshi, Nagasai Thadishetty, Vignesh Kumar, Hemanth Venkateshwara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22004">https://arxiv.org/abs/2510.22004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22004">https://arxiv.org/pdf/2510.22004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22004]] LiteDiff(https://arxiv.org/abs/2510.22004)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, diffusion models have demonstrated remarkable success in high-fidelity image synthesis. However, fine-tuning these models for specialized domains, such as medical imaging, remains challenging due to limited domain-specific data and the high computational cost of full model adaptation. In this paper, we introduce Lite-Diff (Lightweight Diffusion Model Adaptation), a novel finetuning approach that integrates lightweight adaptation layers into a frozen diffusion U-Net while enhancing training with a latent morphological autoencoder (for domain-specific latent consistency) and a pixel level discriminator(for adversarial alignment). By freezing weights of the base model and optimizing only small residual adapter modules, LiteDiff significantly reduces the computational overhead and mitigates overfitting, even in minimal-data settings. Additionally, we conduct ablation studies to analyze the effects of selectively integrating adaptation layers in different U-Net blocks, revealing an optimal balance between efficiency and performance. Experiments on three chest X-ray datasets - (1) Kaggle Chest X-Ray Pneumonia, (2) NIH Chest X-ray14 and (3) VinBigData Chest X_ray demonstrate that LiteDiff achieves superior adaptation efficiency compared to naive full fine-tuning. Our framework provides a promising direction for transfer learning in diffusion models, facilitating their deployment in diverse low data domains.</li>
</ul>

<h3>Title: Optimal Detection for Language Watermarks with Pseudorandom Collision</h3>
<ul>
<li><strong>Authors: </strong>T. Tony Cai, Xiang Li, Qi Long, Weijie J. Su, Garrett G. Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22007">https://arxiv.org/abs/2510.22007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22007">https://arxiv.org/pdf/2510.22007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22007]] Optimal Detection for Language Watermarks with Pseudorandom Collision(https://arxiv.org/abs/2510.22007)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>Text watermarking plays a crucial role in ensuring the traceability and accountability of large language model (LLM) outputs and mitigating misuse. While promising, most existing methods assume perfect pseudorandomness. In practice, repetition in generated text induces collisions that create structured dependence, compromising Type I error control and invalidating standard analyses. We introduce a statistical framework that captures this structure through a hierarchical two-layer partition. At its core is the concept of minimal units -- the smallest groups treatable as independent across units while permitting dependence within. Using minimal units, we define a non-asymptotic efficiency measure and cast watermark detection as a minimax hypothesis testing problem. Applied to Gumbel-max and inverse-transform watermarks, our framework produces closed-form optimal rules. It explains why discarding repeated statistics often improves performance and shows that within-unit dependence must be addressed unless degenerate. Both theory and experiments confirm improved detection power with rigorous Type I error control. These results provide the first principled foundation for watermark detection under imperfect pseudorandomness, offering both theoretical insight and practical guidance for reliable tracing of model outputs.</li>
</ul>

<h3>Title: A Multimodal Human Protein Embeddings Database: DeepDrug Protein Embeddings Bank (DPEB)</h3>
<ul>
<li><strong>Authors: </strong>Md Saiful Islam Sajol, Magesh Rajasekaran, Hayden Gemeinhardt, Adam Bess, Chris Alvin, Supratik Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.MN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22008">https://arxiv.org/abs/2510.22008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22008">https://arxiv.org/pdf/2510.22008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22008]] A Multimodal Human Protein Embeddings Database: DeepDrug Protein Embeddings Bank (DPEB)(https://arxiv.org/abs/2510.22008)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Computationally predicting protein-protein interactions (PPIs) is challenging due to the lack of integrated, multimodal protein representations. DPEB is a curated collection of 22,043 human proteins that integrates four embedding types: structural (AlphaFold2), transformer-based sequence (BioEmbeddings), contextual amino acid patterns (ESM-2: Evolutionary Scale Modeling), and sequence-based n-gram statistics (ProtVec]). AlphaFold2 protein structures are available through public databases (e.g., AlphaFold2 Protein Structure Database), but the internal neural network embeddings are not. DPEB addresses this gap by providing AlphaFold2-derived embeddings for computational modeling. Our benchmark evaluations show GraphSAGE with BioEmbedding achieved the highest PPI prediction performance (87.37% AUROC, 79.16% accuracy). The framework also achieved 77.42% accuracy for enzyme classification and 86.04% accuracy for protein family classification. DPEB supports multiple graph neural network methods for PPI prediction, enabling applications in systems biology, drug target identification, pathway analysis, and disease mechanism studies.</li>
</ul>

<h3>Title: FlowOpt: Fast Optimization Through Whole Flow Processes for Training-Free Editing</h3>
<ul>
<li><strong>Authors: </strong>Or Ronai, Vladimir Kulikov, Tomer Michaeli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22010">https://arxiv.org/abs/2510.22010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22010">https://arxiv.org/pdf/2510.22010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22010]] FlowOpt: Fast Optimization Through Whole Flow Processes for Training-Free Editing(https://arxiv.org/abs/2510.22010)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The remarkable success of diffusion and flow-matching models has ignited a surge of works on adapting them at test time for controlled generation tasks. Examples range from image editing to restoration, compression and personalization. However, due to the iterative nature of the sampling process in those models, it is computationally impractical to use gradient-based optimization to directly control the image generated at the end of the process. As a result, existing methods typically resort to manipulating each timestep separately. Here we introduce FlowOpt - a zero-order (gradient-free) optimization framework that treats the entire flow process as a black box, enabling optimization through the whole sampling path without backpropagation through the model. Our method is both highly efficient and allows users to monitor the intermediate optimization results and perform early stopping if desired. We prove a sufficient condition on FlowOpt's step-size, under which convergence to the global optimum is guaranteed. We further show how to empirically estimate this upper bound so as to choose an appropriate step-size. We demonstrate how FlowOpt can be used for image editing, showcasing two options: (i) inversion (determining the initial noise that generates a given image), and (ii) directly steering the edited image to be similar to the source image while conforming to a target text prompt. In both cases, FlowOpt achieves state-of-the-art results while using roughly the same number of neural function evaluations (NFEs) as existing methods. Code and examples are available on the project's webpage.</li>
</ul>

<h3>Title: Reconnaissance Automatique des Langues des Signes : Une Approche Hybrid√©e CNN-LSTM Bas√©e sur Mediapipe</h3>
<ul>
<li><strong>Authors: </strong>Fraisse Sacr√© Takouchouang, Ho Tuong Vinh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22011">https://arxiv.org/abs/2510.22011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22011">https://arxiv.org/pdf/2510.22011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22011]] Reconnaissance Automatique des Langues des Signes : Une Approche Hybrid√©e CNN-LSTM Bas√©e sur Mediapipe(https://arxiv.org/abs/2510.22011)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Sign languages play a crucial role in the communication of deaf communities, but they are often marginalized, limiting access to essential services such as healthcare and education. This study proposes an automatic sign language recognition system based on a hybrid CNN-LSTM architecture, using Mediapipe for gesture keypoint extraction. Developed with Python, TensorFlow and Streamlit, the system provides real-time gesture translation. The results show an average accuracy of 92\%, with very good performance for distinct gestures such as ``Hello'' and ``Thank you''. However, some confusions remain for visually similar gestures, such as ``Call'' and ``Yes''. This work opens up interesting perspectives for applications in various fields such as healthcare, education and public services.</li>
</ul>

<h3>Title: Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sarah Ball, Niki Hasrati, Alexander Robey, Avi Schwarzschild, Frauke Kreuter, Zico Kolter, Andrej Risteski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22014">https://arxiv.org/abs/2510.22014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22014">https://arxiv.org/pdf/2510.22014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22014]] Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models(https://arxiv.org/abs/2510.22014)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Discrete optimization-based jailbreaking attacks on large language models aim to generate short, nonsensical suffixes that, when appended onto input prompts, elicit disallowed content. Notably, these suffixes are often transferable -- succeeding on prompts and models for which they were never optimized. And yet, despite the fact that transferability is surprising and empirically well-established, the field lacks a rigorous analysis of when and why transfer occurs. To fill this gap, we identify three statistical properties that strongly correlate with transfer success across numerous experimental settings: (1) how much a prompt without a suffix activates a model's internal refusal direction, (2) how strongly a suffix induces a push away from this direction, and (3) how large these shifts are in directions orthogonal to refusal. On the other hand, we find that prompt semantic similarity only weakly correlates with transfer success. These findings lead to a more fine-grained understanding of transferability, which we use in interventional experiments to showcase how our statistical analysis can translate into practical improvements in attack success.</li>
</ul>

<h3>Title: Cost-Sensitive Evaluation for Binary Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Pierangelo Lombardo, Antonio Casoli, Cristian Cingolani, Shola Oshodi, Michele Zanatta</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22016">https://arxiv.org/abs/2510.22016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22016">https://arxiv.org/pdf/2510.22016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22016]] Cost-Sensitive Evaluation for Binary Classifiers(https://arxiv.org/abs/2510.22016)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Selecting an appropriate evaluation metric for classifiers is crucial for model comparison and parameter optimization, yet there is not consensus on a universally accepted metric that serves as a definitive standard. Moreover, there is often a misconception about the perceived need to mitigate imbalance in datasets used to train classification models. Since the final goal in classifier optimization is typically maximizing the return of investment or, equivalently, minimizing the Total Classification Cost (TCC), we define Weighted Accuracy (WA), an evaluation metric for binary classifiers with a straightforward interpretation as a weighted version of the well-known accuracy metric, coherent with the need of minimizing TCC. We clarify the conceptual framework for handling class imbalance in cost-sensitive scenarios, providing an alternative to rebalancing techniques. This framework can be applied to any metric that, like WA, can be expressed as a linear combination of example-dependent quantities and allows for comparing the results obtained in different datasets and for addressing discrepancies between the development dataset, used to train and validate the model, and the target dataset, where the model will be deployed. It also specifies in which scenarios using UCCs-unaware class rebalancing techniques or rebalancing metrics aligns with TCC minimization and when it is instead counterproductive. Finally, we propose a procedure to estimate the WA weight parameter in the absence of fully specified UCCs and demonstrate the robustness of WA by analyzing its correlation with TCC in example-dependent scenarios.</li>
</ul>

<h3>Title: Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies</h3>
<ul>
<li><strong>Authors: </strong>Naina Balepur, Xingrui Pei, Hari Sundaram</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22017">https://arxiv.org/abs/2510.22017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22017">https://arxiv.org/pdf/2510.22017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22017]] Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies(https://arxiv.org/abs/2510.22017)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Many governmental bodies are adopting AI policies for decision-making. In particular, Reinforcement Learning has been used to design policies that citizens would be expected to follow if implemented. Much RL work assumes that citizens follow these policies, and evaluate them with this in mind. However, we know from prior work that without institutional trust, citizens will not follow policies put in place by governments. In this work, we develop a trust-aware RL algorithm for resource allocation in communities. We consider the case of humanitarian engineering, where the organization is aiming to distribute some technology or resource to community members. We use a Deep Deterministic Policy Gradient approach to learn a resource allocation that fits the needs of the organization. Then, we simulate resource allocation according to the learned policy, and model the changes in institutional trust of community members. We investigate how this incorporation of institutional trust affects outcomes, and ask how effectively an organization can learn policies if trust values are private. We find that incorporating trust into RL algorithms can lead to more successful policies, specifically when the organization's goals are less certain. We find more conservative trust estimates lead to increased fairness and average community trust, though organization success suffers. Finally, we explore a strategy to prevent unfair outcomes to communities. We implement a quota system by an external entity which decreases the organization's utility when it does not serve enough community members. We find this intervention can improve fairness and trust among communities in some cases, while decreasing the success of the organization. This work underscores the importance of institutional trust in algorithm design and implementation, and identifies a tension between organization success and community well-being.</li>
</ul>

<h3>Title: K-DAREK: Distance Aware Error for Kurkova Kolmogorov Networks</h3>
<ul>
<li><strong>Authors: </strong>Masoud Ataei, Vikas Dhiman, Mohammad Javad Khojasteh</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22021">https://arxiv.org/abs/2510.22021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22021">https://arxiv.org/pdf/2510.22021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22021]] K-DAREK: Distance Aware Error for Kurkova Kolmogorov Networks(https://arxiv.org/abs/2510.22021)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Neural networks are parametric and powerful tools for function approximation, and the choice of architecture heavily influences their interpretability, efficiency, and generalization. In contrast, Gaussian processes (GPs) are nonparametric probabilistic models that define distributions over functions using a kernel to capture correlations among data points. However, these models become computationally expensive for large-scale problems, as they require inverting a large covariance matrix. Kolmogorov- Arnold networks (KANs), semi-parametric neural architectures, have emerged as a prominent approach for modeling complex functions with structured and efficient representations through spline layers. Kurkova Kolmogorov-Arnold networks (KKANs) extend this idea by reducing the number of spline layers in KAN and replacing them with Chebyshev layers and multi-layer perceptrons, thereby mapping inputs into higher-dimensional spaces before applying spline-based transformations. Compared to KANs, KKANs perform more stable convergence during training, making them a strong architecture for estimating operators and system modeling in dynamical systems. By enhancing the KKAN architecture, we develop a novel learning algorithm, distance-aware error for Kurkova-Kolmogorov networks (K-DAREK), for efficient and interpretable function approximation with uncertainty quantification. Our approach establishes robust error bounds that are distance-aware; this means they reflect the proximity of a test point to its nearest training points. Through case studies on a safe control task, we demonstrate that K-DAREK is about four times faster and ten times higher computationally efficiency than Ensemble of KANs, 8.6 times more scalable than GP by increasing the data size, and 50% safer than our previous work distance-aware error for Kolmogorov networks (DAREK).</li>
</ul>

<h3>Title: Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla</h3>
<ul>
<li><strong>Authors: </strong>Evangelos Bitsikas, Jason Veara, Aanjhan Ranganathan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22024">https://arxiv.org/abs/2510.22024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22024">https://arxiv.org/pdf/2510.22024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22024]] Security Analysis of LTE Connectivity in Connected Cars: A Case Study of Tesla(https://arxiv.org/abs/2510.22024)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Modern connected vehicles rely on persistent LTE connectivity to enable remote diagnostics, over-the-air (OTA) updates, and critical safety services. While mobile network vulnerabilities are well documented in the smartphone ecosystem, their impact in safety-critical automotive settings remains insufficiently examined. In this work, we conduct a black-box, non-invasive security analysis of LTE connectivity in Tesla vehicles, including the Model 3 and Cybertruck, revealing systemic protocol weaknesses and architectural misconfigurations. We find that Tesla's telematics stack is susceptible to IMSI catching, rogue base station hijacking, and insecure fallback mechanisms that may silently degrade service availability. Furthermore, legacy control-plane configurations allow for silent SMS injection and broadcast message spoofing without driver awareness. These vulnerabilities have implications beyond a single vendor as they challenge core assumptions in regulatory frameworks like ISO/SAE 21434 and UN R155/R156, which require secure, traceable, and resilient telematics for type approval of modern vehicles.</li>
</ul>

<h3>Title: Normalization in Attention Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Nikita Karagodin, Shu Ge, Yury Polyanskiy, Philippe Rigollet</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22026">https://arxiv.org/abs/2510.22026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22026">https://arxiv.org/pdf/2510.22026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22026]] Normalization in Attention Dynamics(https://arxiv.org/abs/2510.22026)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We study the effect of normalization schemes on token representations in deep transformers. Modeling their evolution as interacting particles on the sphere, we show that normalization acts as a form of speed regulation. This perspective enables a unified analysis of several schemes -- including Post-LN, Pre-LN, Mix-LN, Peri-LN, nGPT, and LN-Scaling -- revealing how they influence clustering dynamics and representation collapse. Our framework clarifies how different schemes shape token representations across layers and provides a principled basis for comparing them, identifying Peri-LN as a particularly effective choice.</li>
</ul>

<h3>Title: Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics</h3>
<ul>
<li><strong>Authors: </strong>Yilin Zhang, Wenda Xu, Zhongtao Liu, Tetsuji Nakagawa, Markus Freitag</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22028">https://arxiv.org/abs/2510.22028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22028">https://arxiv.org/pdf/2510.22028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22028]] Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics(https://arxiv.org/abs/2510.22028)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Quality Estimation (QE) metrics are vital in machine translation for reference-free evaluation and as a reward signal in tasks like reinforcement learning. However, the prevalence and impact of length bias in QE have been underexplored. Through a systematic study of top-performing regression-based and LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two critical length biases: First, QE metrics consistently over-predict errors with increasing translation length, even for high-quality, error-free texts. Second, they exhibit a preference for shorter translations when multiple candidates are available for the same source text. These inherent length biases risk unfairly penalizing longer, correct translations and can lead to sub-optimal decision-making in applications such as QE reranking and QE guided reinforcement learning. To mitigate this, we propose two strategies: (a) applying length normalization during model training, and (b) incorporating reference texts during evaluation. Both approaches were found to effectively reduce the identified length bias.</li>
</ul>

<h3>Title: Differentiable Constraint-Based Causal Discovery</h3>
<ul>
<li><strong>Authors: </strong>Jincheng Zhou, Mengbo Wang, Anqi He, Yumeng Zhou, Hessam Olya, Murat Kocaoglu, Bruno Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22031">https://arxiv.org/abs/2510.22031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22031">https://arxiv.org/pdf/2510.22031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22031]] Differentiable Constraint-Based Causal Discovery(https://arxiv.org/abs/2510.22031)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Causal discovery from observational data is a fundamental task in artificial intelligence, with far-reaching implications for decision-making, predictions, and interventions. Despite significant advances, existing methods can be broadly categorized as constraint-based or score-based approaches. Constraint-based methods offer rigorous causal discovery but are often hindered by small sample sizes, while score-based methods provide flexible optimization but typically forgo explicit conditional independence testing. This work explores a third avenue: developing differentiable $d$-separation scores, obtained through a percolation theory using soft logic. This enables the implementation of a new type of causal discovery method: gradient-based optimization of conditional independence constraints. Empirical evaluations demonstrate the robust performance of our approach in low-sample regimes, surpassing traditional constraint-based and score-based baselines on a real-world dataset. Code and data of the proposed method are publicly available at https://github$.$com/PurdueMINDS/DAGPA.</li>
</ul>

<h3>Title: Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data</h3>
<ul>
<li><strong>Authors: </strong>Tianxiang Wang, Yingtong Ke, Dhananjay Bhaskar, Smita Krishnaswamy, Alexander Cloninger</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22033">https://arxiv.org/abs/2510.22033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22033">https://arxiv.org/pdf/2510.22033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22033]] Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data(https://arxiv.org/abs/2510.22033)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Single-cell technologies generate high-dimensional point clouds of cells, enabling detailed characterization of complex patient states and treatment responses. Yet each patient is represented by an irregular point cloud rather than a simple vector, making it difficult to directly quantify and compare biological differences between individuals. Nonlinear methods such as kernels and neural networks achieve predictive accuracy but act as black boxes, offering little biological interpretability. To address these limitations, we adapt the Linear Optimal Transport (LOT) framework to this setting, embedding irregular point clouds into a fixed-dimensional Euclidean space while preserving distributional structure. This embedding provides a principled linear representation that preserves optimal transport geometry while enabling downstream analysis. It also forms a registration between any two patients, enabling direct comparison of their cellular distributions. Within this space, LOT enables: (i) \textbf{accurate and interpretable classification} of COVID-19 patient states, where classifier weights map back to specific markers and spatial regions driving predictions; and (ii) \textbf{synthetic data generation} for patient-derived organoids, exploiting the linearity of the LOT embedding. LOT barycenters yield averaged cellular profiles representing combined conditions or samples, supporting drug interaction testing. Together, these results establish LOT as a unified framework that bridges predictive performance, interpretability, and generative modeling. By transforming heterogeneous point clouds into structured embeddings directly traceable to the original data, LOT opens new opportunities for understanding immune variation and treatment effects in high-dimensional biological systems.</li>
</ul>

<h3>Title: Caption-Driven Explainability: Probing CNNs for Bias via CLIP</h3>
<ul>
<li><strong>Authors: </strong>Patrick Koller (Northwestern University, Evanston, Illinois, United States), Amil V. Dravid (University of California, Berkeley, California, United States), Guido M. Schuster (Eastern Switzerland University of Applied Sciences, Rapperswil, St. Gallen, Switzerland), Aggelos K. Katsaggelos (Northwestern University, Evanston, Illinois, United States)</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22035">https://arxiv.org/abs/2510.22035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22035">https://arxiv.org/pdf/2510.22035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22035]] Caption-Driven Explainability: Probing CNNs for Bias via CLIP(https://arxiv.org/abs/2510.22035)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Robustness has become one of the most critical problems in machine learning (ML). The science of interpreting ML models to understand their behavior and improve their robustness is referred to as explainable artificial intelligence (XAI). One of the state-of-the-art XAI methods for computer vision problems is to generate saliency maps. A saliency map highlights the pixel space of an image that excites the ML model the most. However, this property could be misleading if spurious and salient features are present in overlapping pixel spaces. In this paper, we propose a caption-based XAI method, which integrates a standalone model to be explained into the contrastive language-image pre-training (CLIP) model using a novel network surgery approach. The resulting caption-based XAI model identifies the dominant concept that contributes the most to the models prediction. This explanation minimizes the risk of the standalone model falling for a covariate shift and contributes significantly towards developing robust ML models.</li>
</ul>

<h3>Title: Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Reichman, Adar Avsian, Larry Heck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22042">https://arxiv.org/abs/2510.22042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22042">https://arxiv.org/pdf/2510.22042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22042]] Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models(https://arxiv.org/abs/2510.22042)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work investigates how large language models (LLMs) internally represent emotion by analyzing the geometry of their hidden-state space. The paper identifies a low-dimensional emotional manifold and shows that emotional representations are directionally encoded, distributed across layers, and aligned with interpretable dimensions. These structures are stable across depth and generalize to eight real-world emotion datasets spanning five languages. Cross-domain alignment yields low error and strong linear probe performance, indicating a universal emotional subspace. Within this space, internal emotion perception can be steered while preserving semantics using a learned intervention module, with especially strong control for basic emotions across languages. These findings reveal a consistent and manipulable affective geometry in LLMs and offer insight into how they internalize and process emotion.</li>
</ul>

<h3>Title: VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT</h3>
<ul>
<li><strong>Authors: </strong>Hyeonsu Kang, Emily Bao, Anjan Goswami</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22045">https://arxiv.org/abs/2510.22045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22045">https://arxiv.org/pdf/2510.22045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22045]] VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT(https://arxiv.org/abs/2510.22045)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) are increasingly used to evaluate multimodal content, including presentation slides, yet their slide-specific understanding remains underexplored {despite their growing role as critics in agentic, model-forward pipelines}. We introduce VLM-SlideEval, an evaluation framework that probes VLMs along three axes: (1) element-level extraction from slide images aligned to ground truth; (2) robustness to controlled perturbations in geometry, style, and text; and (3) higher-level comprehension, such as recovering a deck's narrative order from shuffled slides. Using publicly available decks from Zenodo (this https URL), we standardize ground-truth element metadata from PowerPoint XML and live renderings into a unified, verifiable schema. Empirically, VLMs underperform on pixel-accurate extraction and show non-trivial agreement, fidelity, and consistency under controlled perturbations, while performing better on single-slide content understanding; however, they do not reliably capture narrative structure across slides. These results highlight the limits of current VLMs for slide evaluation and motivate calibrated, critic-in-the-loop evaluators that drive iterative refinement and selection in agentic pipelines.</li>
</ul>

<h3>Title: PF$Œî$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations</h3>
<ul>
<li><strong>Authors: </strong>Ana K. Rivera, Anvita Bhagavathula, Alvaro Carbonero, Priya Donti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22048">https://arxiv.org/abs/2510.22048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22048">https://arxiv.org/pdf/2510.22048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22048]] PF$Œî$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations(https://arxiv.org/abs/2510.22048)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Power flow (PF) calculations are the backbone of real-time grid operations, across workflows such as contingency analysis (where repeated PF evaluations assess grid security under outages) and topology optimization (which involves PF-based searches over combinatorially large action spaces). Running these calculations at operational timescales or across large evaluation spaces remains a major computational bottleneck. Additionally, growing uncertainty in power system operations from the integration of renewables and climate-induced extreme weather also calls for tools that can accurately and efficiently simulate a wide range of scenarios and operating conditions. Machine learning methods offer a potential speedup over traditional solvers, but their performance has not been systematically assessed on benchmarks that capture real-world variability. This paper introduces PF$\Delta$, a benchmark dataset for power flow that captures diverse variations in load, generation, and topology. PF$\Delta$ contains 859,800 solved power flow instances spanning six different bus system sizes, capturing three types of contingency scenarios (N , N -1, and N -2), and including close-to-infeasible cases near steady-state voltage stability limits. We evaluate traditional solvers and GNN-based methods, highlighting key areas where existing approaches struggle, and identifying open problems for future research. Our dataset is available at this https URL and our code with data generation scripts and model implementations is at this https URL.</li>
</ul>

<h3>Title: Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Ali Etemadi Naeen, Hoda Mohammadzade, Saeed Bagheri Shouraki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22056">https://arxiv.org/abs/2510.22056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22056">https://arxiv.org/pdf/2510.22056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22056]] Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning(https://arxiv.org/abs/2510.22056)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Anomaly detection in surveillance videos remains a challenging task due to the diversity of abnormal events, class imbalance, and scene-dependent visual clutter. To address these issues, we propose a robust deep learning framework that integrates human-centric preprocessing with spatio-temporal modeling for multi-class anomaly classification. Our pipeline begins by applying YOLO-World - an open-vocabulary vision-language detector - to identify human instances in raw video clips, followed by ByteTrack for consistent identity-aware tracking. Background regions outside detected bounding boxes are suppressed via Gaussian blurring, effectively reducing scene-specific distractions and focusing the model on behaviorally relevant foreground content. The refined frames are then processed by an ImageNet-pretrained InceptionV3 network for spatial feature extraction, and temporal dynamics are captured using a bidirectional LSTM (BiLSTM) for sequence-level classification. Evaluated on a five-class subset of the UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), our method achieves a mean test accuracy of 92.41% across three independent trials, with per-class F1-scores consistently exceeding 0.85. Comprehensive evaluation metrics - including confusion matrices, ROC curves, and macro/weighted averages - demonstrate strong generalization and resilience to class imbalance. The results confirm that foreground-focused preprocessing significantly enhances anomaly discrimination in real-world surveillance scenarios.</li>
</ul>

<h3>Title: Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model</h3>
<ul>
<li><strong>Authors: </strong>James Thiering, Tarun Sethupat Radha Krishna, Dylan Zelkin, Ashis Kumer Biswas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22057">https://arxiv.org/abs/2510.22057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22057">https://arxiv.org/pdf/2510.22057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22057]] Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model(https://arxiv.org/abs/2510.22057)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>With the rise of online and virtual learning, monitoring and enhancing student engagement have become an important aspect of effective education. Traditional methods of assessing a student's involvement might not be applicable directly to virtual environments. In this study, we focused on this problem and addressed the need to develop an automated system to detect student engagement levels during online learning. We proposed a novel training method which can discourage a model from leveraging sensitive features like gender for its predictions. The proposed method offers benefits not only in the enforcement of ethical standards, but also to enhance interpretability of the model predictions. We applied an attribute-orthogonal regularization technique to a split-model classifier, which uses multiple transfer learning strategies to achieve effective results in reducing disparity in the distribution of prediction for sensitivity groups from a Pearson correlation coefficient of 0.897 for the unmitigated model, to 0.999 for the mitigated model. The source code for this project is available on this https URL .</li>
</ul>

<h3>Title: Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Zheng Qi, Chao Shang, Evangelia Spiliopoulou, Nikolaos Pappas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22067">https://arxiv.org/abs/2510.22067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22067">https://arxiv.org/pdf/2510.22067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22067]] Capturing Gaze Shifts for Guidance: Cross-Modal Fusion Enhancement for VLM Hallucination Mitigation(https://arxiv.org/abs/2510.22067)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision language models (VLMs) often generate hallucination, i.e., content that cannot be substantiated by either textual or visual inputs. Prior work primarily attributes this to over-reliance on linguistic prior knowledge rather than visual inputs. Some methods attempt to mitigate hallucination by amplifying visual token attention proportionally to their attention scores. However, these methods overlook the visual attention sink problem, where attention is frequently misallocated to task-irrelevant visual regions, and neglect cross-modal fusion balance by enhancing only visual attention without adjusting attention to the user query. This can result in amplifying incorrect areas while failing to properly interpret the user query. To address these challenges, we propose a simple yet effective method called Gaze Shift-Guided Cross-modal Fusion Enhancement (GIFT). GIFT pre-computes a holistic visual saliency map by tracking positive changes in visual attention, or "gaze shifts", during user query comprehension, and leverages this map to amplify attention to both salient visual information and the user query at each decoding step. This reduces the impact of visual attention sink, as irrelevant tokens exhibit minimal shifts, while ensuring balanced cross-modal fusion for well-integrated representation. Extensive experiments show that GIFT effectively mitigates hallucination in VLMs across both generative and classification tasks, achieving up to 20.7% improvement over greedy decoding, while maintaining general vision-language performance with low computational overhead.</li>
</ul>

<h3>Title: MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification</h3>
<ul>
<li><strong>Authors: </strong>Luca Caldera, Giacomo Bottacini, Lara Cavinato</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22070">https://arxiv.org/abs/2510.22070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22070">https://arxiv.org/pdf/2510.22070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22070]] MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification(https://arxiv.org/abs/2510.22070)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling has emerged as a powerful paradigm for representation learning, but its direct applicability to challenging fields like medical imaging remains limited: mere generation, without task alignment, fails to provide a robust foundation for clinical use. We propose MAGIC-Flow, a conditional multiscale normalizing flow architecture that performs generation and classification within a single modular framework. The model is built as a hierarchy of invertible and differentiable bijections, where the Jacobian determinant factorizes across sub-transformations. We show how this ensures exact likelihood computation and stable optimization, while invertibility enables explicit visualization of sample likelihoods, providing an interpretable lens into the model's reasoning. By conditioning on class labels, MAGIC-Flow supports controllable sample synthesis and principled class-probability estimation, effectively aiding both generative and discriminative objectives. We evaluate MAGIC-Flow against top baselines using metrics for similarity, fidelity, and diversity. Across multiple datasets, it addresses generation and classification under scanner noise, and modality-specific synthesis and identification. Results show MAGIC-Flow creates realistic, diverse samples and improves classification. MAGIC-Flow is an effective strategy for generation and classification in data-limited domains, with direct benefits for privacy-preserving augmentation, robust generalization, and trustworthy medical AI.</li>
</ul>

<h3>Title: Scanner-Agnostic MRI Harmonization via SSIM-Guided Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Luca Caldera, Lara Cavinato, Francesca Ieva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22073">https://arxiv.org/abs/2510.22073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22073">https://arxiv.org/pdf/2510.22073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22073]] Scanner-Agnostic MRI Harmonization via SSIM-Guided Disentanglement(https://arxiv.org/abs/2510.22073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The variability introduced by differences in MRI scanner models, acquisition protocols, and imaging sites hinders consistent analysis and generalizability across multicenter studies. We present a novel image-based harmonization framework for 3D T1-weighted brain MRI, which disentangles anatomical content from scanner- and site-specific variations. The model incorporates a differentiable loss based on the Structural Similarity Index (SSIM) to preserve biologically meaningful features while reducing inter-site variability. This loss enables separate evaluation of image luminance, contrast, and structural components. Training and validation were performed on multiple publicly available datasets spanning diverse scanners and sites, with testing on both healthy and clinical populations. Harmonization using multiple style targets, including style-agnostic references, produced consistent and high-quality outputs. Visual comparisons, voxel intensity distributions, and SSIM-based metrics demonstrated that harmonized images achieved strong alignment across acquisition settings while maintaining anatomical fidelity. Following harmonization, structural SSIM reached 0.97, luminance SSIM ranged from 0.98 to 0.99, and Wasserstein distances between mean voxel intensity distributions decreased substantially. Downstream tasks showed substantial improvements: mean absolute error for brain age prediction decreased from 5.36 to 3.30 years, and Alzheimer's disease classification AUC increased from 0.78 to 0.85. Overall, our framework enhances cross-site image consistency, preserves anatomical fidelity, and improves downstream model performance, providing a robust and generalizable solution for large-scale multicenter neuroimaging studies.</li>
</ul>

<h3>Title: Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds</h3>
<ul>
<li><strong>Authors: </strong>Atij Mahesh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22084">https://arxiv.org/abs/2510.22084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22084">https://arxiv.org/pdf/2510.22084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22084]] Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds(https://arxiv.org/abs/2510.22084)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) still produce gender-stereotyped language even in occupation-neutral contexts that reflect deep societal biases (Rudinger et al., 2018). To address this, prior work has proposed prompting, constrained decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022). However, the comparative efficacy and learning dynamics remain little understood. We report a comparative analysis of six control techniques for bias mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Iterative Nullspace Projection (INLP). We evaluate each method on a compositional constraint task. This task requires generating sentences that contain at least one agentic and one communal descriptor for each of the twenty Winogender-derived occupations. We quantify trade-offs between control strength and naturalness with evaluations of constraint compliance, lexical diversity, and fluency. Our results reveal key contrasts among the methods: SFT achieves 99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect compliance, but at the cost of severely reduced fluency and diversity. Preference-based learning fundamentally differs: it cannot satisfy compositional constraints, as binary preference signals encode ranking, not logical conjunctions. Only explicit positive supervision enables mitigation of compositional biases; preference-based alignment fails to generalize logical structures, underscoring the limitations of preference learning and the necessity of explicit supervision for fair and fluent controlled generation.</li>
</ul>

<h3>Title: Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pavlos Ntais</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22085">https://arxiv.org/abs/2510.22085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22085">https://arxiv.org/pdf/2510.22085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22085]] Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models(https://arxiv.org/abs/2510.22085)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) remain vulnerable to sophisticated prompt engineering attacks that exploit contextual framing to bypass safety mechanisms, posing significant risks in cybersecurity applications. We introduce Jailbreak Mimicry, a systematic methodology for training compact attacker models to automatically generate narrative-based jailbreak prompts in a one-shot manner. Our approach transforms adversarial prompt discovery from manual craftsmanship into a reproducible scientific process, enabling proactive vulnerability assessment in AI-driven security systems. Developed for the OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench, achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out test set of 200 items. Cross-model evaluation reveals significant variation in vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad applicability and model-specific defensive strengths in cybersecurity contexts. This represents a 54x improvement over direct prompting (1.5% ASR) and demonstrates systematic vulnerabilities in current safety alignment approaches. Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable, highlighting threats to AI-integrated threat detection, malware analysis, and secure systems, while physical harm categories show greater resistance (55.6% ASR). We employ automated harmfulness evaluation using Claude Sonnet 4, cross-validated with human expert assessment, ensuring reliable and scalable evaluation for cybersecurity red-teaming. Finally, we analyze failure mechanisms and discuss defensive strategies to mitigate these vulnerabilities in AI for cybersecurity.</li>
</ul>

<h3>Title: Generalization or Memorization: Dynamic Decoding for Mode Steering</h3>
<ul>
<li><strong>Authors: </strong>Xuanming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22099">https://arxiv.org/abs/2510.22099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22099">https://arxiv.org/pdf/2510.22099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22099]] Generalization or Memorization: Dynamic Decoding for Mode Steering(https://arxiv.org/abs/2510.22099)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.</li>
</ul>

<h3>Title: Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Saif E. Nouma, Attila A. Yavuz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22100">https://arxiv.org/abs/2510.22100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22100">https://arxiv.org/pdf/2510.22100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22100]] Lightweight and Breach-Resilient Authenticated Encryption Framework for Internet of Things(https://arxiv.org/abs/2510.22100)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The Internet of Things (IoT) relies heavily on resource-limited devices to communicate critical (e.g., military data) information under low-energy adversarial environments and low-latency wireless channels. Authenticated Encryption (AE) guarantees confidentiality, authenticity, and integrity, making it a vital security service for IoT. However, current deployed (lightweight) AE standards lack essential features like key compromise resiliency and compact authentication tags, as well as performance enhancements such as offline-online cryptography. To address these gaps, we propose Graphene, the first (to our knowledge) symmetric Forward-secure and Aggregate Authenticated Encryption (FAAE) framework designed for the performance and security demands of low-end IoT infrastructures. Graphene innovates by synergizing key evolution strategies and offline-online cryptographic processing with Universal Message Authentication Codes (UMACs) to guarantee breach-resiliency, near-optimal online latency, and compactness. We demonstrate Graphene efficiency through two distinct instantiations, each balancing unique performance trade-offs with extensibility for diverse MACs. Our experimental evaluation on commodity hardware and 32-bit ARM Cortex-M4 microcontroller shows Graphene significant performance gains over existing alternatives. Graphene is also backward compatible with standard-compliant cryptographic implementations. We release our implementation as open source for public testing and adaptation.</li>
</ul>

<h3>Title: Mitigating Coordinate Prediction Bias from Positional Encoding Failures</h3>
<ul>
<li><strong>Authors: </strong>Xingjian Tao, Yiwei Wang, Yujun Cai, Yihong Luo, Jing Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22102">https://arxiv.org/abs/2510.22102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22102">https://arxiv.org/pdf/2510.22102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22102]] Mitigating Coordinate Prediction Bias from Positional Encoding Failures(https://arxiv.org/abs/2510.22102)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) excel at vision-language tasks such as VQA and document understanding, yet precise coordinate prediction remains challenging. High-resolution inputs exacerbate this difficulty by producing long token sequences that weaken positional encodings and introduce directional biases in coordinate outputs. We investigate this phenomenon by analyzing how MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed through shuffling. Our analysis reveals that such perturbations induce predictable, non-random coordinate biases rather than random errors, suggesting that models rely on internal positional priors when spatial grounding signals are degraded. Crucially, we observe similar directional error patterns in natural high-resolution datasets, indicating that positional encoding failures are a key bottleneck for accurate coordinate prediction at scale. To address this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free test-time method that leverages the directional nature of these biases for correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate position-unconditioned tendencies, then uses this as negative evidence to guide digit prediction while preserving coordinate format through a lightweight finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable improvements, highlighting positional encoding robustness as a critical factor for spatial reasoning in MLLMs.</li>
</ul>

<h3>Title: Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Bailey Trang, Parham Saremi, Alan Q. Wang, Fangrui Huang, Zahra TehraniNasab, Amar Kumar, Tal Arbel, Li Fei-Fei, Ehsan Adeli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22107">https://arxiv.org/abs/2510.22107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22107">https://arxiv.org/pdf/2510.22107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22107]] Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation(https://arxiv.org/abs/2510.22107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Capturing diversity is crucial in conditional and prompt-based image generation, particularly when conditions contain uncertainty that can lead to multiple plausible outputs. To generate diverse images reflecting this diversity, traditional methods often modify random seeds, making it difficult to discern meaningful differences between samples, or diversify the input prompt, which is limited in verbally interpretable diversity. We propose Rainbow, a novel conditional image generation framework, applicable to any pretrained conditional generative model, that addresses inherent condition/prompt uncertainty and generates diverse plausible images. Rainbow is based on a simple yet effective idea: decomposing the input condition into diverse latent representations, each capturing an aspect of the uncertainty and generating a distinct image. First, we integrate a latent graph, parameterized by Generative Flow Networks (GFlowNets), into the prompt representation computation. Second, leveraging GFlowNets' advanced graph sampling capabilities to capture uncertainty and output diverse trajectories over the graph, we produce multiple trajectories that collectively represent the input condition, leading to diverse condition representations and corresponding output images. Evaluations on natural image and medical image datasets demonstrate Rainbow's improvement in both diversity and fidelity across image synthesis, image generation, and counterfactual generation tasks.</li>
</ul>

<h3>Title: Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows</h3>
<ul>
<li><strong>Authors: </strong>Billy Dickson, Zoran Tiganj</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22109">https://arxiv.org/abs/2510.22109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22109">https://arxiv.org/pdf/2510.22109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22109]] Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows(https://arxiv.org/abs/2510.22109)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Most approaches to long-context processing increase the complexity of the transformer's internal architecture by integrating mechanisms such as recurrence or auxiliary memory modules. In this work, we introduce an alternative approach that modifies the input representation itself, rather than the transformer architecture. Inspired by cognitive models of human memory, our method applies a scale-invariant logarithmic compression to the input tokens. The resulting compressed representation is processed by a standard, unmodified transformer, preserving architectural simplicity. We evaluate this approach on the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in perplexity compared to uncompressed baselines. Moreover, performance improves consistently with longer compressed temporal contexts, showing that input-level logarithmic compression is a simple and effective way to extend a transformer's long-range memory.</li>
</ul>

<h3>Title: GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Karim Elmaaroufi, Liheng Lai, Justin Svegliato, Yutong Bai, Sanjit A. Seshia, Matei Zaharia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22118">https://arxiv.org/abs/2510.22118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22118">https://arxiv.org/pdf/2510.22118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22118]] GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation(https://arxiv.org/abs/2510.22118)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) achieve strong performance on many vision-language tasks but often struggle with spatial reasoning\textemdash{}a prerequisite for many applications. Empirically, we find that a dataset produced by a current training data generation pipeline has a 57.6\% human validation rate. These rates stem from current limitations: single-image 3D reconstruction introduces cascading modeling errors and requires wide answer tolerances, while caption-based methods require hyper-detailed annotations and suffer from generative hallucinations. We present GRAID, built on the key insight that qualitative spatial relationships can be reliably determined from 2D geometric primitives alone. By operating exclusively on 2D bounding boxes from standard object detectors, GRAID avoids both 3D reconstruction errors and generative hallucinations, resulting in datasets that are of higher quality than existing tools that produce similar datasets as validated by human evaluations. We apply our framework to the BDD100k, NuImages, and Waymo datasets, generating over 8.5 million high-quality VQA pairs creating questions spanning spatial relations, counting, ranking, and size comparisons. We evaluate one of the datasets and find it achieves 91.16\% human-validated accuracy\textemdash{}compared to 57.6\% on a dataset generated by recent work. % or recent work Critically, we demonstrate that when trained on GRAID data, models learn spatial reasoning concepts that generalize: models fine-tuned on 6 question types improve on over 10 held-out types, with accuracy gains of 47.5\% on BDD and 37.9\% on NuImages for Llama 3.2B 11B, and when trained on all questions types, achieve improvements on several existing benchmarks such as BLINK. The GRAID framework, datasets, and additional information can be found on our \href{this https URL}{project page}.</li>
</ul>

<h3>Title: Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jinzhe Liu, Junshu Sun, Shufan Shen, Chenxue Yang, Shuhui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22139">https://arxiv.org/abs/2510.22139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22139">https://arxiv.org/pdf/2510.22139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22139]] Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs(https://arxiv.org/abs/2510.22139)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Lifelong knowledge editing enables continuous, precise updates to outdated knowledge in large language models (LLMs) without computationally expensive full retraining. However, existing methods often accumulate errors throughout the editing process, causing a gradual decline in both editing accuracy and generalization. To tackle this problem, we propose Neuron-Specific Masked Knowledge Editing (NMKE), a novel fine-grained editing framework that combines neuron-level attribution with dynamic sparse masking. Leveraging neuron functional attribution, we identify two key types of knowledge neurons, with knowledge-general neurons activating consistently across prompts and knowledge-specific neurons activating to specific prompts. NMKE further introduces an entropy-guided dynamic sparse mask, locating relevant neurons to the target knowledge. This strategy enables precise neuron-level knowledge editing with fewer parameter modifications. Experimental results from thousands of sequential edits demonstrate that NMKE outperforms existing methods in maintaining high editing success rates and preserving model general capabilities in lifelong editing.</li>
</ul>

<h3>Title: Power to the Clients: Federated Learning in a Dictatorship Setting</h3>
<ul>
<li><strong>Authors: </strong>Mohammadsajad Alipour, Mohammad Mohammadi Amiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR, cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22149">https://arxiv.org/abs/2510.22149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22149">https://arxiv.org/pdf/2510.22149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22149]] Power to the Clients: Federated Learning in a Dictatorship Setting(https://arxiv.org/abs/2510.22149)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple clients to collaboratively learn a shared model without exchanging their local data. However, the decentralized nature of FL also introduces vulnerabilities, as malicious clients can compromise or manipulate the training process. In this work, we introduce dictator clients, a novel, well-defined, and analytically tractable class of malicious participants capable of entirely erasing the contributions of all other clients from the server model, while preserving their own. We propose concrete attack strategies that empower such clients and systematically analyze their effects on the learning process. Furthermore, we explore complex scenarios involving multiple dictator clients, including cases where they collaborate, act independently, or form an alliance in order to ultimately betray one another. For each of these settings, we provide a theoretical analysis of their impact on the global model's convergence. Our theoretical algorithms and findings about the complex scenarios including multiple dictator clients are further supported by empirical evaluations on both computer vision and natural language processing benchmarks.</li>
</ul>

<h3>Title: SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language</h3>
<ul>
<li><strong>Authors: </strong>Rahul Ranjan, Mahendra Kumar Gurve, Anuj, Nitin, Yamuna Prasad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22160">https://arxiv.org/abs/2510.22160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22160">https://arxiv.org/pdf/2510.22160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22160]] SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language(https://arxiv.org/abs/2510.22160)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, transformer</a></li>
<li><strong>Abstract: </strong>Developing benchmark datasets for low-resource languages poses significant challenges, primarily due to the limited availability of native linguistic experts and the substantial time and cost involved in annotation. Given these challenges, Maithili is still underrepresented in natural language processing research. It is an Indo-Aryan language spoken by more than 13 million people in the Purvanchal region of India, valued for its rich linguistic structure and cultural significance. While sentiment analysis has achieved remarkable progress in high-resource languages, resources for low-resource languages, such as Maithili, remain scarce, often restricted to coarse-grained annotations and lacking interpretability mechanisms. To address this limitation, we introduce a novel dataset comprising 3,221 Maithili sentences annotated for sentiment polarity and accompanied by natural language justifications. Moreover, the dataset is carefully curated and validated by linguistic experts to ensure both label reliability and contextual fidelity. Notably, the justifications are written in Maithili, thereby promoting culturally grounded interpretation and enhancing the explainability of sentiment models. Furthermore, extensive experiments using both classical machine learning and state-of-the-art transformer architectures demonstrate the dataset's effectiveness for interpretable sentiment analysis. Ultimately, this work establishes the first benchmark for explainable affective computing in Maithili, thus contributing a valuable resource to the broader advancement of multilingual NLP and explainable AI.</li>
</ul>

<h3>Title: I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions</h3>
<ul>
<li><strong>Authors: </strong>Shuhong Liu, Lin Gu, Ziteng Cui, Xuangeng Chu, Tatsuya Harada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22161">https://arxiv.org/abs/2510.22161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22161">https://arxiv.org/pdf/2510.22161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22161]] I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions(https://arxiv.org/abs/2510.22161)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Participating in efforts to endow generative AI with the 3D physical world perception, we propose I2-NeRF, a novel neural radiance field framework that enhances isometric and isotropic metric perception under media degradation. While existing NeRF models predominantly rely on object-centric sampling, I2-NeRF introduces a reverse-stratified upsampling strategy to achieve near-uniform sampling across 3D space, thereby preserving isometry. We further present a general radiative formulation for media degradation that unifies emission, absorption, and scattering into a particle model governed by the Beer-Lambert attenuation law. By composing the direct and media-induced in-scatter radiance, this formulation extends naturally to complex media environments such as underwater, haze, and even low-light scenes. By treating light propagation uniformly in both vertical and horizontal directions, I2-NeRF enables isotropic metric perception and can even estimate medium properties such as water depth. Experiments on real-world datasets demonstrate that our method significantly improves both reconstruction fidelity and physical plausibility compared to existing approaches.</li>
</ul>

<h3>Title: TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation</h3>
<ul>
<li><strong>Authors: </strong>Qi Sheng</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22191">https://arxiv.org/abs/2510.22191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22191">https://arxiv.org/pdf/2510.22191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22191]] TPPR: APT Tactic / Technique Pattern Guided Attack Path Reasoning for Attack Investigation(https://arxiv.org/abs/2510.22191)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Provenance analysis based on system audit data has emerged as a fundamental approach for investigating Advanced Persistent Threat (APT) attacks. Due to the high concealment and long-term persistence of APT attacks, they are only represented as a minimal part of the critical path in the provenance graph. While existing techniques employ behavioral pattern matching and data flow feature matching to uncover latent associations in attack sequences through provenance graph path reasoning, their inability to establish effective attack context associations often leads to the conflation of benign system operations with real attack entities, that fail to accurately characterize real APT behaviors. We observe that while the causality of entities in the provenance graph exhibit substantial complexity, attackers often follow specific attack patterns-specifically, clear combinations of tactics and techniques to achieve their goals. Based on these insights, we propose TPPR, a novel framework that first extracts anomaly subgraphs through abnormal node detection, TTP-annotation and graph pruning, then performs attack path reasoning using mined TTP sequential pattern, and finally reconstructs attack scenarios through confidence-based path scoring and merging. Extensive evaluation on real enterprise logs (more than 100 million events) and DARPA TC dataset demonstrates TPPR's capability to achieve 99.9% graph simplification (700,000 to 20 edges) while preserving 91% of critical attack nodes, outperforming state-of-the-art solutions (SPARSE, DepImpact) by 63.1% and 67.9% in reconstruction precision while maintaining attack scenario integrity.</li>
</ul>

<h3>Title: Scaling Non-Parametric Sampling with Representation</h3>
<ul>
<li><strong>Authors: </strong>Vincent Lu, Aaron Truong, Zeyu Yun, Yubei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22196">https://arxiv.org/abs/2510.22196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22196">https://arxiv.org/pdf/2510.22196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22196]] Scaling Non-Parametric Sampling with Representation(https://arxiv.org/abs/2510.22196)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Scaling and architectural advances have produced strikingly photorealistic image generative models, yet their mechanisms still remain opaque. Rather than advancing scaling, our goal is to strip away complicated engineering tricks and propose a simple, non-parametric generative model. Our design is grounded in three principles of natural images-(i) spatial non-stationarity, (ii) low-level regularities, and (iii) high-level semantics-and defines each pixel's distribution from its local context window. Despite its minimal architecture and no training, the model produces high-fidelity samples on MNIST and visually compelling CIFAR-10 images. This combination of simplicity and strong empirical performance points toward a minimal theory of natural-image structure. The model's white-box nature also allows us to have a mechanistic understanding of how the model generalizes and generates diverse images. We study it by tracing each generated pixel back to its source images. These analyses reveal a simple, compositional procedure for "part-whole generalization", suggesting a hypothesis for how large neural network generative models learn to generalize.</li>
</ul>

<h3>Title: Multi-dataset Joint Pre-training of Emotional EEG Enables Generalizable Affective Computing</h3>
<ul>
<li><strong>Authors: </strong>Qingzhu Zhang, Jiani Zhong, Zongsheng Li, Xinke Shen, Quanying Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22197">https://arxiv.org/abs/2510.22197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22197">https://arxiv.org/pdf/2510.22197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22197]] Multi-dataset Joint Pre-training of Emotional EEG Enables Generalizable Affective Computing(https://arxiv.org/abs/2510.22197)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Task-specific pre-training is essential when task representations diverge from generic pre-training features. Existing task-general pre-training EEG models struggle with complex tasks like emotion recognition due to mismatches between task-specific features and broad pre-training approaches. This work aims to develop a task-specific multi-dataset joint pre-training framework for cross-dataset emotion recognition, tackling problems of large inter-dataset distribution shifts, inconsistent emotion category definitions, and substantial inter-subject variability. We introduce a cross-dataset covariance alignment loss to align second-order statistical properties across datasets, enabling robust generalization without the need for extensive labels or per-subject calibration. To capture the long-term dependency and complex dynamics of EEG, we propose a hybrid encoder combining a Mamba-like linear attention channel encoder and a spatiotemporal dynamics model. Our method outperforms state-of-the-art large-scale EEG models by an average of 4.57% in AUROC for few-shot emotion recognition and 11.92% in accuracy for zero-shot generalization to a new dataset. Performance scales with the increase of datasets used in pre-training. Multi-dataset joint pre-training achieves a performance gain of 8.55% over single-dataset training. This work provides a scalable framework for task-specific pre-training and highlights its benefit in generalizable affective computing. Our code is available at this https URL.</li>
</ul>

<h3>Title: LongCat-Video Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Meituan LongCat Team: Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, Shijun Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22200">https://arxiv.org/abs/2510.22200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22200">https://arxiv.org/pdf/2510.22200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22200]] LongCat-Video Technical Report(https://arxiv.org/abs/2510.22200)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Video generation is a critical pathway toward world models, with efficient long video inference as a key capability. Toward this end, we introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across multiple video generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models. Key features include: Unified architecture for multiple tasks: Built on the Diffusion Transformer (DiT) framework, LongCat-Video supports Text-to-Video, Image-to-Video, and Video-Continuation tasks with a single model; Long video generation: Pretraining on Video-Continuation tasks enables LongCat-Video to maintain high quality and temporal coherence in the generation of minutes-long videos; Efficient inference: LongCat-Video generates 720p, 30fps videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions; Strong performance with multi-reward RLHF: Multi-reward RLHF training enables LongCat-Video to achieve performance on par with the latest closed-source and leading open-source models. Code and model weights are publicly available to accelerate progress in the field.</li>
</ul>

<h3>Title: TrajGATFormer: A Graph-Based Transformer Approach for Worker and Obstacle Trajectory Prediction in Off-site Construction Environments</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Alduais, Xinming Li, Qipei Mei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22205">https://arxiv.org/abs/2510.22205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22205">https://arxiv.org/pdf/2510.22205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22205]] TrajGATFormer: A Graph-Based Transformer Approach for Worker and Obstacle Trajectory Prediction in Off-site Construction Environments(https://arxiv.org/abs/2510.22205)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As the demand grows within the construction industry for processes that are not only faster but also safer and more efficient, offsite construction has emerged as a solution, though it brings new safety risks due to the close interaction between workers, machinery, and moving obstacles. Predicting the future trajectories of workers and taking into account social and environmental factors is a crucial step for developing collision-avoidance systems to mitigate such risks. Traditional methods often struggle to adapt to the dynamic and unpredictable nature of construction environments. Many rely on simplified assumptions or require hand-crafted features, limiting their ability to respond to complex, real-time interactions between workers and moving obstacles. While recent data-driven methods have improved the modeling of temporal patterns, they still face challenges in capturing long-term behavior and accounting for the spatial and social context crucial to collision risk assessment. To address these limitations, this paper proposes a framework integrating YOLOv10n and DeepSORT for precise detection and tracking, along with two novel trajectory prediction models: TrajGATFormer and TrajGATFormer-Obstacle. YOLOv10n serves as the backbone for object detection, accurately identifying workers and obstacles in diverse scenes, while DeepSORT efficiently tracks them over time with unique IDs for continuity. Both models employ a transformer encoder-decoder with Graph Attention Networks (GAT) to capture temporal and spatial interactions. TrajGATFormer predicts worker trajectories with an ADE of 1.25 m and FDE of 2.3 m over a 4.8 s horizon, while TrajGATFormer-Obstacle extends prediction to both workers and obstacles, achieving higher accuracy (ADE 1.15 m, FDE 2.2 m). Comparative analysis shows both models outperform traditional methods, reducing ADE and FDE by up to 35% and 38%, respectively.</li>
</ul>

<h3>Title: The Lossy Horizon: Error-Bounded Predictive Coding for Lossy Text Compression (Episode I)</h3>
<ul>
<li><strong>Authors: </strong>Nnamdi Aghanya, Jun Li, Kewei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22207">https://arxiv.org/abs/2510.22207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22207">https://arxiv.org/pdf/2510.22207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22207]] The Lossy Horizon: Error-Bounded Predictive Coding for Lossy Text Compression (Episode I)(https://arxiv.org/abs/2510.22207)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can achieve near-optimal lossless compression by acting as powerful probability models. We investigate their use in the lossy domain, where reconstruction fidelity is traded for higher compression ratios. This paper introduces Error-Bounded Predictive Coding (EPC), a lossy text codec that leverages a Masked Language Model (MLM) as a decompressor. Instead of storing a subset of original tokens, EPC allows the model to predict masked content and stores minimal, rank-based corrections only when the model's top prediction is incorrect. This creates a residual channel that offers continuous rate-distortion control. We compare EPC to a simpler Predictive Masking (PM) baseline and a transform-based Vector Quantisation with a Residual Patch (VQ+RE) approach. Through an evaluation that includes precise bit accounting and rate-distortion analysis, we demonstrate that EPC consistently dominates PM, offering superior fidelity at a significantly lower bit rate by more efficiently utilising the model's intrinsic knowledge.</li>
</ul>

<h3>Title: Simplifying Knowledge Transfer in Pretrained Models</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Jain, Shyamgopal Karthik, Vineet Gandhi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22208">https://arxiv.org/abs/2510.22208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22208">https://arxiv.org/pdf/2510.22208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22208]] Simplifying Knowledge Transfer in Pretrained Models(https://arxiv.org/abs/2510.22208)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Pretrained models are ubiquitous in the current deep learning landscape, offering strong results on a broad range of tasks. Recent works have shown that models differing in various design choices exhibit categorically diverse generalization behavior, resulting in one model grasping distinct data-specific insights unavailable to the other. In this paper, we propose to leverage large publicly available model repositories as an auxiliary source of model improvements. We introduce a data partitioning strategy where pretrained models autonomously adopt either the role of a student, seeking knowledge, or that of a teacher, imparting knowledge. Experiments across various tasks demonstrate the effectiveness of our proposed approach. In image classification, we improved the performance of ViT-B by approximately 1.4% through bidirectional knowledge transfer with ViT-T. For semantic segmentation, our method boosted all evaluation metrics by enabling knowledge transfer both within and across backbone architectures. In video saliency prediction, our approach achieved a new state-of-the-art. We further extend our approach to knowledge transfer between multiple models, leading to considerable performance improvements for all model participants.</li>
</ul>

<h3>Title: Visual Model Selection using Feature Importance Clusters in Fairness-Performance Similarity Optimized Space</h3>
<ul>
<li><strong>Authors: </strong>Sofoklis Kitharidis, Cor J. Veenman, Thomas B√§ck, Niki van Stein</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22209">https://arxiv.org/abs/2510.22209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22209">https://arxiv.org/pdf/2510.22209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22209]] Visual Model Selection using Feature Importance Clusters in Fairness-Performance Similarity Optimized Space(https://arxiv.org/abs/2510.22209)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In the context of algorithmic decision-making, fair machine learning methods often yield multiple models that balance predictive fairness and performance in varying degrees. This diversity introduces a challenge for stakeholders who must select a model that aligns with their specific requirements and values. To address this, we propose an interactive framework that assists in navigating and interpreting the trade-offs across a portfolio of models. Our approach leverages weakly supervised metric learning to learn a Mahalanobis distance that reflects similarity in fairness and performance outcomes, effectively structuring the feature importance space of the models according to stakeholder-relevant criteria. We then apply clustering technique (k-means) to group models based on their transformed representations of feature importances, allowing users to explore clusters of models with similar predictive behaviors and fairness characteristics. This facilitates informed decision-making by helping users understand how models differ not only in their fairness-performance balance but also in the features that drive their predictions.</li>
</ul>

<h3>Title: DETECT: Determining Ease and Textual Clarity of German Text Simplifications</h3>
<ul>
<li><strong>Authors: </strong>Maria Korobeynikova, Alessia Battisti, Lukas Fischer, Yingqiang Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22212">https://arxiv.org/abs/2510.22212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22212">https://arxiv.org/pdf/2510.22212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22212]] DETECT: Determining Ease and Textual Clarity of German Text Simplifications(https://arxiv.org/abs/2510.22212)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current evaluation of German automatic text simplification (ATS) relies on general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently capture simplification quality in terms of simplicity, meaning preservation, and fluency. While specialized metrics like LENS have been developed for English, corresponding efforts for German have lagged behind due to the absence of human-annotated corpora. To close this gap, we introduce DETECT, the first German-specific metric that holistically evaluates ATS quality across all three dimensions of simplicity, meaning preservation, and fluency, and is trained entirely on synthetic large language model (LLM) responses. Our approach adapts the LENS framework to German and extends it with (i) a pipeline for generating synthetic quality scores via LLMs, enabling dataset creation without human annotation, and (ii) an LLM-based refinement step for aligning grading criteria with simplification requirements. To the best of our knowledge, we also construct the largest German human evaluation dataset for text simplification to validate our metric directly. Experimental results show that DETECT achieves substantially higher correlations with human judgments than widely used ATS metrics, with particularly strong gains in meaning preservation and fluency. Beyond ATS, our findings highlight both the potential and the limitations of LLMs for automatic evaluation and provide transferable guidelines for general language accessibility tasks.</li>
</ul>

<h3>Title: Estimating the Error of Large Language Models at Pairwise Text Comparison</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22219">https://arxiv.org/abs/2510.22219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22219">https://arxiv.org/pdf/2510.22219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22219]] Estimating the Error of Large Language Models at Pairwise Text Comparison(https://arxiv.org/abs/2510.22219)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We measure LLMs' output error at pairwise text comparison, noting the probability of error in their preferences. Our method does not rely on the ground truth and supports two scenarios: (i) uniform error rate regardless of the order of comparison, estimated with two comparisons for each text pair with either text placed first; (ii) binary positional bias assuming distinct error rates for the two orders of comparison, estimated with repeated comparisons between the texts. The Copeland counting constructs a ranking over the compared texts from pairwise preferences; the ranking reveals the poor scalability of LLM-based pairwise comparison and helps yield the estimates for LLMs' error rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini, Grok, Qwen) with five types of text input and obtain consistent estimates of LLMs' error. In general, the measured two positional bias terms are similar, close to the uniform error. Considering both the error rates and the robustness to the variation of prompts, Claude obtained the most desirable performance in this experiment. Our model outperforms the biased Bradley-Terry model and the commutativity score in indicating LLMs' error at this task.</li>
</ul>

<h3>Title: When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Keyu Wang, Tian Lyu, Guinan Su, Jonas Geiping, Lu Yin, Marco Canini, Shiwei Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22228">https://arxiv.org/abs/2510.22228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22228">https://arxiv.org/pdf/2510.22228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22228]] When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs(https://arxiv.org/abs/2510.22228)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Layer pruning has emerged as a widely adopted technique for improving the efficiency of large language models (LLMs). Although existing methods demonstrate strong performance retention on general knowledge tasks, their effect on long-chain reasoning, a more brittle yet crucial capability, remains largely unexplored. In this work, we study the impact of layer pruning on long-chain reasoning through the lens of test-time scaling, a key mechanism in modern LLMs that enables strong reasoning capacity by allocating more computation at inference time. With extensive experiments, we demonstrate that pruning even one or two layers can severely impair test-time scaling, with performance collapsing drastically on long reasoning benchmarks even when performance on knowledge-intensive and shallow reasoning tasks remains stable. Furthermore, we find that standard supervised fine-tuning remedies fail to recover test-time scaling once it has deteriorated. Through in-depth analyses, we identify the mechanisms underlying this fragility of test-time scaling and highlight the fundamental risks of applying layer pruning to reasoning-intensive LLMs. These findings call for a rethinking of layer pruning strategies and provide insights for developing methods that preserve the robustness of reasoning. We open-source the codebase in \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Diffusion-Driven Two-Stage Active Learning for Low-Budget Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jeongin Kim, Wonho Bae, YouLee Han, Giyeong Oh, Youngjae Yu, Danica J. Sutherland, Junhyug Noh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22229">https://arxiv.org/abs/2510.22229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22229">https://arxiv.org/pdf/2510.22229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22229]] Diffusion-Driven Two-Stage Active Learning for Low-Budget Semantic Segmentation(https://arxiv.org/abs/2510.22229)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation demands dense pixel-level annotations, which can be prohibitively expensive - especially under extremely constrained labeling budgets. In this paper, we address the problem of low-budget active learning for semantic segmentation by proposing a novel two-stage selection pipeline. Our approach leverages a pre-trained diffusion model to extract rich multi-scale features that capture both global structure and fine details. In the first stage, we perform a hierarchical, representation-based candidate selection by first choosing a small subset of representative pixels per image using MaxHerding, and then refining these into a diverse global pool. In the second stage, we compute an entropy-augmented disagreement score (eDALD) over noisy multi-scale diffusion features to capture both epistemic uncertainty and prediction confidence, selecting the most informative pixels for annotation. This decoupling of diversity and uncertainty lets us achieve high segmentation accuracy with only a tiny fraction of labeled pixels. Extensive experiments on four benchmarks (CamVid, ADE-Bed, Cityscapes, and Pascal-Context) demonstrate that our method significantly outperforms existing baselines under extreme pixel-budget regimes. Our code is available at this https URL.</li>
</ul>

<h3>Title: DiffusionLane: Diffusion Model for Lane Detection</h3>
<ul>
<li><strong>Authors: </strong>Kunyang Zhou, Yeqin Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22236">https://arxiv.org/abs/2510.22236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22236">https://arxiv.org/pdf/2510.22236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22236]] DiffusionLane: Diffusion Model for Lane Detection(https://arxiv.org/abs/2510.22236)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present a novel diffusion-based model for lane detection, called DiffusionLane, which treats the lane detection task as a denoising diffusion process in the parameter space of the lane. Firstly, we add the Gaussian noise to the parameters (the starting point and the angle) of ground truth lanes to obtain noisy lane anchors, and the model learns to refine the noisy lane anchors in a progressive way to obtain the target lanes. Secondly, we propose a hybrid decoding strategy to address the poor feature representation of the encoder, resulting from the noisy lane anchors. Specifically, we design a hybrid diffusion decoder to combine global-level and local-level decoders for high-quality lane anchors. Then, to improve the feature representation of the encoder, we employ an auxiliary head in the training stage to adopt the learnable lane anchors for enriching the supervision on the encoder. Experimental results on four benchmarks, Carlane, Tusimple, CULane, and LLAMAS, show that DiffusionLane possesses a strong generalization ability and promising detection performance compared to the previous state-of-the-art methods. For example, DiffusionLane with ResNet18 surpasses the existing methods by at least 1\% accuracy on the domain adaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets 81.32\% F1 score on CULane, 96.89\% accuracy on Tusimple with ResNet34, and 97.59\% F1 score on LLAMAS with ResNet101. Code will be available at this https URL.</li>
</ul>

<h3>Title: Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework</h3>
<ul>
<li><strong>Authors: </strong>Amir Mohammad Khadem Hosseini, Sattar Mirzakuchaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22243">https://arxiv.org/abs/2510.22243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22243">https://arxiv.org/pdf/2510.22243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22243]] Real-Time Semantic Segmentation on FPGA for Autonomous Vehicles Using LMIINet with the CGRA4ML Framework(https://arxiv.org/abs/2510.22243)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation has emerged as a fundamental problem in computer vision, gaining particular importance in real-time applications such as autonomous driving. The main challenge is achieving high accuracy while operating under computational and hardware constraints. In this research, we present an FPGA-based implementation of real-time semantic segmentation leveraging the lightweight LMIINet architecture and the Coarse-Grained Reconfigurable Array for Machine Learning (CGRA4ML) hardware framework. The model was trained using Quantization-Aware Training (QAT) with 8-bit precision on the Cityscapes dataset, reducing memory footprint by a factor of four while enabling efficient fixed-point computations. Necessary modifications were applied to adapt the model to CGRA4ML constraints, including simplifying skip connections, employing hardware-friendly operations such as depthwise-separable and 1A-1 convolutions, and redesigning parts of the Flatten Transformer. Our implementation achieves approximately 90% pixel accuracy and 45% mean Intersection-over-Union (mIoU), operating in real-time at 20 frames per second (FPS) with 50.1 ms latency on the ZCU104 FPGA board. The results demonstrate the potential of CGRA4ML, with its flexibility in mapping modern layers and off-chip memory utilization for skip connections, provides a path for implementing advanced semantic segmentation networks on FPGA for real-time applications to outperform traditional GPU solutions in terms of power efficiency while maintaining competitive accuracy. The code for this project is publicly available at this https URL cgra4ml_semantic_segmentation</li>
</ul>

<h3>Title: SteerX: Disentangled Steering for LLM Personalization</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Zhao, Ming Yan, Yilun Qiu, Haoting Ni, Yang Zhang, Fuli Feng, Hong Cheng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22256">https://arxiv.org/abs/2510.22256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22256">https://arxiv.org/pdf/2510.22256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22256]] SteerX: Disentangled Steering for LLM Personalization(https://arxiv.org/abs/2510.22256)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable success in recent years, enabling a wide range of applications, including intelligent assistants that support users' daily life and work. A critical factor in building such assistants is personalizing LLMs, as user preferences and needs vary widely. Activation steering, which directly leverages directions representing user preference in the LLM activation space to adjust its behavior, offers a cost-effective way to align the model's outputs with individual users. However, existing methods rely on all historical data to compute the steering vector, ignoring that not all content reflects true user preferences, which undermines the personalization signal. To address this, we propose SteerX, a disentangled steering method that isolates preference-driven components from preference-agnostic components. Grounded in causal inference theory, SteerX estimates token-level causal effects to identify preference-driven tokens, transforms these discrete signals into a coherent description, and then leverages them to steer personalized LLM generation. By focusing on the truly preference-driven information, SteerX produces more accurate activation steering vectors and enhances personalization. Experiments on two representative steering backbone methods across real-world datasets demonstrate that SteerX consistently enhances steering vector quality, offering a practical solution for more effective LLM personalization.</li>
</ul>

<h3>Title: LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis</h3>
<ul>
<li><strong>Authors: </strong>Berkay D√∂ner, Thorir Mar Ingolfsson, Luca Benini, Yawei Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22257">https://arxiv.org/abs/2510.22257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22257">https://arxiv.org/pdf/2510.22257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22257]] LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis(https://arxiv.org/abs/2510.22257)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Electroencephalography (EEG) offers a non-invasive lens into human brain activity, but building large-scale models is hampered by topological heterogeneity: each public EEG data defines its own electrode layout, limiting generalization. We introduce LUNA (Latent Unified Network Architecture), a self-supervised foundation model that reconciles disparate electrode geometries while scaling linearly -- not quadratically -- with channel count. LUNA compresses multi-channel EEG into a fixed-size, topology-agnostic latent space via learned queries and cross-attention. Downstream transformer blocks then operate exclusively on this latent representation using patch-wise temporal self-attention, decoupling computation from electrode count. Pre-trained on TUEG and Siena (over 21,000 hours of raw EEG across diverse montages) using a masked-patch reconstruction objective, LUNA transfers effectively to four downstream tasks: abnormality detection, artifact rejection, slowing classification, and emotion recognition. It demonstrates highly competitive performance across several benchmarks, achieving state-of-the-art results on TUAR and TUSL, e.g., 0.921 AUROC on TUAR, while reducing FLOPs by 300x and trimming GPU memory use by up to 10x. Critically, these gains are consistent across all evaluated electrode configurations. Code is available at this https URL</li>
</ul>

<h3>Title: Accident Anticipation via Temporal Occurrence Prediction</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Zhao, Yiyang Zou, Zihao Mao, Peilun Xiao, Yulin Huang, Hongda Yang, Yuxuan Li, Qun Li, Guobin Wu, Yutian Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22260">https://arxiv.org/abs/2510.22260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22260">https://arxiv.org/pdf/2510.22260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22260]] Accident Anticipation via Temporal Occurrence Prediction(https://arxiv.org/abs/2510.22260)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accident anticipation aims to predict potential collisions in an online manner, enabling timely alerts to enhance road safety. Existing methods typically predict frame-level risk scores as indicators of hazard. However, these approaches rely on ambiguous binary supervision (labeling all frames in accident videos as positive) despite the fact that risk varies continuously over time, leading to unreliable learning and false alarms. To address this, we propose a novel paradigm that shifts the prediction target from current-frame risk scoring to directly estimating accident scores at multiple future time steps (e.g., 0.1s-2.0s ahead), leveraging precisely annotated accident timestamps as supervision. Our method employs a snippet-level encoder to jointly model spatial and temporal dynamics, and a Transformer-based temporal decoder that predicts accident scores for all future horizons simultaneously using dedicated temporal queries. Furthermore, we introduce a refined evaluation protocol that reports Time-to-Accident (TTA) and recall (evaluated at multiple pre-accident intervals (0.5s, 1.0s, and 1.5s)) only when the false alarm rate (FAR) remains within an acceptable range, ensuring practical relevance. Experiments show that our method achieves superior performance in both recall and TTA under realistic FAR constraints.</li>
</ul>

<h3>Title: Epistemic Deep Learning: Enabling Machine Learning Models to Know When They Do Not Know</h3>
<ul>
<li><strong>Authors: </strong>Shireen Kudukkil Manchingal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22261">https://arxiv.org/abs/2510.22261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22261">https://arxiv.org/pdf/2510.22261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22261]] Epistemic Deep Learning: Enabling Machine Learning Models to Know When They Do Not Know(https://arxiv.org/abs/2510.22261)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Machine learning has achieved remarkable successes, yet its deployment in safety-critical domains remains hindered by an inherent inability to manage uncertainty, resulting in overconfident and unreliable predictions when models encounter out-of-distribution data, adversarial perturbations, or naturally fluctuating environments. This thesis, titled Epistemic Deep Learning: Enabling Machine Learning Models to 'Know When They Do Not Know', addresses these critical challenges by advancing the paradigm of Epistemic Artificial Intelligence, which explicitly models and quantifies epistemic uncertainty: the uncertainty arising from limited, biased, or incomplete training data, as opposed to the irreducible randomness of aleatoric uncertainty, thereby empowering models to acknowledge their limitations and refrain from overconfident decisions when uncertainty is high. Central to this work is the development of the Random-Set Neural Network (RS-NN), a novel methodology that leverages random set theory to predict belief functions over sets of classes, capturing the extent of epistemic uncertainty through the width of associated credal sets, applications of RS-NN, including its adaptation to Large Language Models (LLMs) and its deployment in weather classification for autonomous racing. In addition, the thesis proposes a unified evaluation framework for uncertainty-aware classifiers. Extensive experiments validate that integrating epistemic awareness into deep learning not only mitigates the risks associated with overconfident predictions but also lays the foundation for a paradigm shift in artificial intelligence, where the ability to 'know when it does not know' becomes a hallmark of robust and dependable systems. The title encapsulates the core philosophy of this work, emphasizing that true intelligence involves recognizing and managing the limits of one's own knowledge.</li>
</ul>

<h3>Title: From Slides to Chatbots: Enhancing Large Language Models with University Course Materials</h3>
<ul>
<li><strong>Authors: </strong>Tu Anh Dinh, Philipp Nicolas Schumacher, Jan Niehues</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22272">https://arxiv.org/abs/2510.22272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22272">https://arxiv.org/pdf/2510.22272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22272]] From Slides to Chatbots: Enhancing Large Language Models with University Course Materials(https://arxiv.org/abs/2510.22272)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have advanced rapidly in recent years. One application of LLMs is to support student learning in educational settings. However, prior work has shown that LLMs still struggle to answer questions accurately within university-level computer science courses. In this work, we investigate how incorporating university course materials can enhance LLM performance in this setting. A key challenge lies in leveraging diverse course materials such as lecture slides and transcripts, which differ substantially from typical textual corpora: slides also contain visual elements like images and formulas, while transcripts contain spoken, less structured language. We compare two strategies, Retrieval-Augmented Generation (RAG) and Continual Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture slides, we further explore a multi-modal RAG approach, where we present the retrieved content to the generator in image form. Our experiments reveal that, given the relatively small size of university course materials, RAG is more effective and efficient than CPT. Moreover, incorporating slides as images in the multi-modal setting significantly improves performance over text-only retrieval. These findings highlight practical strategies for developing AI assistants that better support learning and teaching, and we hope they inspire similar efforts in other educational contexts.</li>
</ul>

<h3>Title: SecureLearn - An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks</h3>
<ul>
<li><strong>Authors: </strong>Anum Paracha, Junaid Arshad, Mohamed Ben Farah, Khalid Ismail</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22274">https://arxiv.org/abs/2510.22274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22274">https://arxiv.org/pdf/2510.22274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22274]] SecureLearn - An Attack-agnostic Defense for Multiclass Machine Learning Against Data Poisoning Attacks(https://arxiv.org/abs/2510.22274)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Data poisoning attacks are a potential threat to machine learning (ML) models, aiming to manipulate training datasets to disrupt their performance. Existing defenses are mostly designed to mitigate specific poisoning attacks or are aligned with particular ML algorithms. Furthermore, most defenses are developed to secure deep neural networks or binary classifiers. However, traditional multiclass classifiers need attention to be secure from data poisoning attacks, as these models are significant in developing multi-modal applications. Therefore, this paper proposes SecureLearn, a two-layer attack-agnostic defense to defend multiclass models from poisoning attacks. It comprises two components of data sanitization and a new feature-oriented adversarial training. To ascertain the effectiveness of SecureLearn, we proposed a 3D evaluation matrix with three orthogonal dimensions: data poisoning attack, data sanitization and adversarial training. Benchmarking SecureLearn in a 3D matrix, a detailed analysis is conducted at different poisoning levels (10%-20%), particularly analysing accuracy, recall, F1-score, detection and correction rates, and false discovery rate. The experimentation is conducted for four ML algorithms, namely Random Forest (RF), Decision Tree (DT), Gaussian Naive Bayes (GNB) and Multilayer Perceptron (MLP), trained with three public datasets, against three poisoning attacks and compared with two existing mitigations. Our results highlight that SecureLearn is effective against the provided attacks. SecureLearn has strengthened resilience and adversarial robustness of traditional multiclass models and neural networks, confirming its generalization beyond algorithm-specific defenses. It consistently maintained accuracy above 90%, recall and F1-score above 75%. For neural networks, SecureLearn achieved 97% recall and F1-score against all selected poisoning attacks.</li>
</ul>

<h3>Title: Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study</h3>
<ul>
<li><strong>Authors: </strong>Devon A. Kelly, Christiana Chamon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, eess.SY, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22283">https://arxiv.org/abs/2510.22283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22283">https://arxiv.org/pdf/2510.22283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22283]] Adapting Noise-Driven PUF and AI for Secure WBG ICS: A Proof-of-Concept Study(https://arxiv.org/abs/2510.22283)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Wide-bandgap (WBG) technologies offer unprecedented improvements in power system efficiency, size, and performance, but also introduce unique sensor corruption and cybersecurity risks in industrial control systems (ICS), particularly due to high-frequency noise and sophisticated cyber-physical threats. This proof-of-concept (PoC) study demonstrates the adaptation of a noise-driven physically unclonable function (PUF) and machine learning (ML)-assisted anomaly detection framework to the demanding environment of WBG-based ICS sensor pathways. By extracting entropy from unavoidable WBG switching noise (up to 100 kHz) as a PUF source, and simultaneously using this noise as a real-time threat indicator, the proposed system unites hardware-level authentication and anomaly detection. Our approach integrates hybrid machine learning (ML) models with adaptive Bayesian filtering, providing robust and low-latency detection capabilities resilient to both natural electromagnetic interference (EMI) and active adversarial manipulation. Through detailed simulations of WBG modules under benign and attack scenarios--including EMI injection, signal tampering, and node impersonation--we achieve 95% detection accuracy and sub-millisecond processing latency. These results demonstrate the feasibility of physics-driven, dual-use noise exploitation as a scalable ICS defense primitive. Our findings lay the groundwork for next-generation security strategies that leverage inherent device characteristics, bridging hardware and artificial intelligence (AI) for enhanced protection of critical ICS infrastructure.</li>
</ul>

<h3>Title: Does Homophily Help in Robust Test-time Node Classification?</h3>
<ul>
<li><strong>Authors: </strong>Yan Jiang, Ruihong Qiu, Zi Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22289">https://arxiv.org/abs/2510.22289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22289">https://arxiv.org/pdf/2510.22289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22289]] Does Homophily Help in Robust Test-time Node Classification?(https://arxiv.org/abs/2510.22289)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Homophily, the tendency of nodes from the same class to connect, is a fundamental property of real-world graphs, underpinning structural and semantic patterns in domains such as citation networks and social networks. Existing methods exploit homophily through designing homophily-aware GNN architectures or graph structure learning strategies, yet they primarily focus on GNN learning with training graphs. However, in real-world scenarios, test graphs often suffer from data quality issues and distribution shifts, such as domain shifts across users from different regions in social networks and temporal evolution shifts in citation network graphs collected over varying time periods. These factors significantly compromise the pre-trained model's robustness, resulting in degraded test-time performance. With empirical observations and theoretical analysis, we reveal that transforming the test graph structure by increasing homophily in homophilic graphs or decreasing it in heterophilic graphs can significantly improve the robustness and performance of pre-trained GNNs on node classifications, without requiring model training or update. Motivated by these insights, a novel test-time graph structural transformation method grounded in homophily, named GrapHoST, is proposed. Specifically, a homophily predictor is developed to discriminate test edges, facilitating adaptive test-time graph structural transformation by the confidence of predicted homophily scores. Extensive experiments on nine benchmark datasets under a range of test-time data quality issues demonstrate that GrapHoST consistently achieves state-of-the-art performance, with improvements of up to 10.92%. Our code has been released at this https URL.</li>
</ul>

<h3>Title: Predicting Metabolic Dysfunction-Associated Steatotic Liver Disease using Machine Learning Methods</h3>
<ul>
<li><strong>Authors: </strong>Mary E. An, Paul Griffin, Jonathan G. Stine, Ramakrishna Balakrishnan, Ram Sriram, Soundar Kumara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22293">https://arxiv.org/abs/2510.22293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22293">https://arxiv.org/pdf/2510.22293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22293]] Predicting Metabolic Dysfunction-Associated Steatotic Liver Disease using Machine Learning Methods(https://arxiv.org/abs/2510.22293)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Background: Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD) affects ~33% of U.S. adults and is the most common chronic liver disease. Although often asymptomatic, progression can lead to cirrhosis. Early detection is important, as lifestyle interventions can prevent disease progression. We developed a fair, rigorous, and reproducible MASLD prediction model and compared it to prior methods using a large electronic health record database. Methods: We evaluated LASSO logistic regression, random forest, XGBoost, and a neural network for MASLD prediction using clinical feature subsets, including the top 10 SHAP-ranked features. To reduce disparities in true positive rates across racial and ethnic subgroups, we applied an equal opportunity postprocessing method. Results: This study included 59,492 patients in the training data, 24,198 in the validating data, and 25,188 in the testing data. The LASSO logistic regression model with the top 10 features was selected for its interpretability and comparable performance. Before fairness adjustment, the model achieved AUROC of 0.84, accuracy of 78%, sensitivity of 72%, specificity of 79%, and F1-score of 0.617. After equal opportunity postprocessing, accuracy modestly increased to 81% and specificity to 94%, while sensitivity decreased to 41% and F1-score to 0.515, reflecting the fairness trade-off. Conclusions: We developed the MASER prediction model (MASLD Static EHR Risk Prediction), a LASSO logistic regression model which achieved competitive performance for MASLD prediction (AUROC 0.836, accuracy 77.6%), comparable to previously reported ensemble and tree-based models. Overall, this approach demonstrates that interpretable models can achieve a balance of predictive performance and fairness in diverse patient populations.</li>
</ul>

<h3>Title: T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Zhang, Tairen Zhang, Lanjun Wang, Ruidong Chen, Wenhui Li, Anan Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22300">https://arxiv.org/abs/2510.22300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22300">https://arxiv.org/pdf/2510.22300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22300]] T2I-RiskyPrompt: A Benchmark for Safety Evaluation, Attack, and Defense on Text-to-Image Model(https://arxiv.org/abs/2510.22300)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Using risky text prompts, such as pornography and violent prompts, to test the safety of text-to-image (T2I) models is a critical task. However, existing risky prompt datasets are limited in three key areas: 1) limited risky categories, 2) coarse-grained annotation, and 3) low effectiveness. To address these limitations, we introduce T2I-RiskyPrompt, a comprehensive benchmark designed for evaluating safety-related tasks in T2I models. Specifically, we first develop a hierarchical risk taxonomy, which consists of 6 primary categories and 14 fine-grained subcategories. Building upon this taxonomy, we construct a pipeline to collect and annotate risky prompts. Finally, we obtain 6,432 effective risky prompts, where each prompt is annotated with both hierarchical category labels and detailed risk reasons. Moreover, to facilitate the evaluation, we propose a reason-driven risky image detection method that explicitly aligns the MLLM with safety annotations. Based on T2I-RiskyPrompt, we conduct a comprehensive evaluation of eight T2I models, nine defense methods, five safety filters, and five attack strategies, offering nine key insights into the strengths and limitations of T2I model safety. Finally, we discuss potential applications of T2I-RiskyPrompt across various research fields. The dataset and code are provided in this https URL.</li>
</ul>

<h3>Title: AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals</h3>
<ul>
<li><strong>Authors: </strong>Yujie Xiao, Gongzhen Tang, Wenhui Liu, Jun Li, Guangkun Nie, Zhuoran Kan, Deyun Zhang, Qinghao Zhao, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22301">https://arxiv.org/abs/2510.22301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22301">https://arxiv.org/pdf/2510.22301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22301]] AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals(https://arxiv.org/abs/2510.22301)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Timely access to laboratory values is critical for clinical decision-making, yet current approaches rely on invasive venous sampling and are intrinsically delayed. Electrocardiography (ECG), as a non-invasive and widely available signal, offers a promising modality for rapid laboratory estimation. Recent progress in deep learning has enabled the extraction of latent hematological signatures from ECGs. However, existing models are constrained by low signal-to-noise ratios, substantial inter-individual variability, limited data diversity, and suboptimal generalization, especially when adapted to low-lead wearable devices. In this work, we conduct an exploratory study leveraging transfer learning to fine-tune ECGFounder, a large-scale pre-trained ECG foundation model, on the Multimodal Clinical Monitoring in the Emergency Department (MC-MED) dataset from Stanford. We generated a corpus of more than 20 million standardized ten-second ECG segments to enhance sensitivity to subtle biochemical correlates. On internal validation, the model demonstrated strong predictive performance (area under the curve above 0.65) for thirty-three laboratory indicators, moderate performance (between 0.55 and 0.65) for fifty-nine indicators, and limited performance (below 0.55) for sixteen indicators. This study provides an efficient artificial-intelligence driven solution and establishes the feasibility scope for real-time, non-invasive estimation of laboratory values.</li>
</ul>

<h3>Title: LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22312">https://arxiv.org/abs/2510.22312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22312">https://arxiv.org/pdf/2510.22312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22312]] LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery(https://arxiv.org/abs/2510.22312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Analogical reasoning, the transfer of relational structures across contexts (e.g., planet is to sun as electron is to nucleus), is fundamental to scientific discovery. Yet human insight is often constrained by domain expertise and surface-level biases, limiting access to deeper, structure-driven analogies both within and across disciplines. Large language models (LLMs), trained on vast cross-domain data, present a promising yet underexplored tool for analogical reasoning in science. Here, we demonstrate that LLMs can generate novel battery materials by (1) retrieving cross-domain analogs and analogy-guided exemplars to steer exploration beyond conventional dopant substitutions, and (2) constructing in-domain analogical templates from few labeled examples to guide targeted exploitation. These explicit analogical reasoning strategies yield candidates outside established compositional spaces and outperform standard prompting baselines. Our findings position LLMs as interpretable, expert-like hypothesis generators that leverage analogy-driven generalization for scientific innovation.</li>
</ul>

<h3>Title: Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Antal van den Bosch, Ainhoa Risco Pat√≥n, Teun Buijse, Peter Berck, Maarten van Gompel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22317">https://arxiv.org/abs/2510.22317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22317">https://arxiv.org/pdf/2510.22317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22317]] Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling(https://arxiv.org/abs/2510.22317)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present memory-based language modeling as an efficient, eco-friendly alternative to deep neural network-based language modeling. It offers log-linearly scalable next-token prediction performance and strong memorization capabilities. Implementing fast approximations of k-nearest neighbor classification, memory-based language modeling leaves a relatively small ecological footprint both in training and in inference mode, as it relies fully on CPUs and attains low token latencies. Its internal workings are simple and fully transparent. We compare our implementation of memory-based language modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy, estimated emissions and speeds, and offer some deeper analyses of the model.</li>
</ul>

<h3>Title: GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping</h3>
<ul>
<li><strong>Authors: </strong>Jing Wang, Jiajun Liang, Jie Liu, Henglin Liu, Gongye Liu, Jun Zheng, Wanyuan Pang, Ao Ma, Zhenyu Xie, Xintao Wang, Meng Wang, Pengfei Wan, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22319">https://arxiv.org/abs/2510.22319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22319">https://arxiv.org/pdf/2510.22319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22319]] GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping(https://arxiv.org/abs/2510.22319)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.</li>
</ul>

<h3>Title: Transformer Key-Value Memories Are Nearly as Interpretable as Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Mengyu Ye, Jun Suzuki, Tatsuro Inaba, Tatsuki Kuribayashi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22332">https://arxiv.org/abs/2510.22332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22332">https://arxiv.org/pdf/2510.22332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22332]] Transformer Key-Value Memories Are Nearly as Interpretable as Sparse Autoencoders(https://arxiv.org/abs/2510.22332)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent interpretability work on large language models (LLMs) has been increasingly dominated by a feature-discovery approach with the help of proxy modules. Then, the quality of features learned by, e.g., sparse auto-encoders (SAEs), is evaluated. This paradigm naturally raises a critical question: do such learned features have better properties than those already represented within the original model parameters, and unfortunately, only a few studies have made such comparisons systematically so far. In this work, we revisit the interpretability of feature vectors stored in feed-forward (FF) layers, given the perspective of FF as key-value memories, with modern interpretability benchmarks. Our extensive evaluation revealed that SAE and FFs exhibits a similar range of interpretability, although SAEs displayed an observable but minimal improvement in some aspects. Furthermore, in certain aspects, surprisingly, even vanilla FFs yielded better interpretability than the SAEs, and features discovered in SAEs and FFs diverged. These bring questions about the advantage of SAEs from both perspectives of feature quality and faithfulness, compared to directly interpreting FF feature vectors, and FF key-value parameters serve as a strong baseline in modern interpretability research.</li>
</ul>

<h3>Title: Multilingual Target-Stance Extraction</h3>
<ul>
<li><strong>Authors: </strong>Ethan Mines, Bonnie Dorr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22334">https://arxiv.org/abs/2510.22334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22334">https://arxiv.org/pdf/2510.22334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22334]] Multilingual Target-Stance Extraction(https://arxiv.org/abs/2510.22334)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Social media enables data-driven analysis of public opinion on contested issues. Target-Stance Extraction (TSE) is the task of identifying the target discussed in a document and the document's stance towards that target. Many works classify stance towards a given target in a multilingual setting, but all prior work in TSE is English-only. This work introduces the first multilingual TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and Spanish corpora. It manages to extend the original TSE pipeline to a multilingual setting without requiring separate models for each language. Our model pipeline achieves a modest F1 score of 12.78, underscoring the increased difficulty of the multilingual task relative to English-only setups and highlighting target prediction as the primary bottleneck. We are also the first to demonstrate the sensitivity of TSE's F1 score to different target verbalizations. Together these serve as a much-needed baseline for resources, algorithms, and evaluation criteria in multilingual TSE.</li>
</ul>

<h3>Title: Moving Beyond Diffusion: Hierarchy-to-Hierarchy Autoregression for fMRI-to-Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Ruijie Quan, Wenguan Wang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22335">https://arxiv.org/abs/2510.22335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22335">https://arxiv.org/pdf/2510.22335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22335]] Moving Beyond Diffusion: Hierarchy-to-Hierarchy Autoregression for fMRI-to-Image Reconstruction(https://arxiv.org/abs/2510.22335)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reconstructing visual stimuli from fMRI signals is a central challenge bridging machine learning and neuroscience. Recent diffusion-based methods typically map fMRI activity to a single high-level embedding, using it as fixed guidance throughout the entire generation process. However, this fixed guidance collapses hierarchical neural information and is misaligned with the stage-dependent demands of image reconstruction. In response, we propose MindHier, a coarse-to-fine fMRI-to-image reconstruction framework built on scale-wise autoregressive modeling. MindHier introduces three components: a Hierarchical fMRI Encoder to extract multi-level neural embeddings, a Hierarchy-to-Hierarchy Alignment scheme to enforce layer-wise correspondence with CLIP features, and a Scale-Aware Coarse-to-Fine Neural Guidance strategy to inject these embeddings into autoregression at matching scales. These designs make MindHier an efficient and cognitively-aligned alternative to diffusion-based methods by enabling a hierarchical reconstruction process that synthesizes global semantics before refining local details, akin to human visual perception. Extensive experiments on the NSD dataset show that MindHier achieves superior semantic fidelity, 4.67x faster inference, and more deterministic results than the diffusion-based baselines.</li>
</ul>

<h3>Title: GeoDiffusion: A Training-Free Framework for Accurate 3D Geometric Conditioning in Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Phillip Mueller, Talip Uenlue, Sebastian Schmidt, Marcel Kollovieh, Jiajie Fan, Stephan Guennemann, Lars Mikelsons</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22337">https://arxiv.org/abs/2510.22337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22337">https://arxiv.org/pdf/2510.22337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22337]] GeoDiffusion: A Training-Free Framework for Accurate 3D Geometric Conditioning in Image Generation(https://arxiv.org/abs/2510.22337)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Precise geometric control in image generation is essential for engineering \& product design and creative industries to control 3D object features accurately in image space. Traditional 3D editing approaches are time-consuming and demand specialized skills, while current image-based generative methods lack accuracy in geometric conditioning. To address these challenges, we propose GeoDiffusion, a training-free framework for accurate and efficient geometric conditioning of 3D features in image generation. GeoDiffusion employs a class-specific 3D object as a geometric prior to define keypoints and parametric correlations in 3D space. We ensure viewpoint consistency through a rendered image of a reference 3D object, followed by style transfer to meet user-defined appearance specifications. At the core of our framework is GeoDrag, improving accuracy and speed of drag-based image editing on geometry guidance tasks and general instructions on DragBench. Our results demonstrate that GeoDiffusion enables precise geometric modifications across various iterative design workflows.</li>
</ul>

<h3>Title: FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Aghajani Asl, Majid Asgari-Bidhendi, Behrooz Minaei-Bidgoli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22344">https://arxiv.org/abs/2510.22344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22344">https://arxiv.org/pdf/2510.22344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22344]] FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation(https://arxiv.org/abs/2510.22344)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.</li>
</ul>

<h3>Title: Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fiaz Ahmad, Nisar Hussain, Amna Qasim, Momina Hafeez, Muhammad Usman Grigori Sidorov, Alexander Gelbukh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22356">https://arxiv.org/abs/2510.22356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22356">https://arxiv.org/pdf/2510.22356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22356]] Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models(https://arxiv.org/abs/2510.22356)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Ironic identification is a challenging task in Natural Language Processing, particularly when dealing with languages that differ in syntax and cultural context. In this work, we aim to detect irony in Urdu by translating an English Ironic Corpus into the Urdu language. We evaluate ten state-of-the-art machine learning algorithms using GloVe and Word2Vec embeddings, and compare their performance with classical methods. Additionally, we fine-tune advanced transformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B), and Mistral, to assess the effectiveness of large-scale models in irony detection. Among machine learning models, Gradient Boosting achieved the best performance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3 (8B) achieved the highest performance with an F1-score of 94.61%. These results demonstrate that combining transliteration techniques with modern NLP models enables robust irony detection in Urdu, a historically low-resource language.</li>
</ul>

<h3>Title: Bias Begins with Data: The FairGround Corpus for Robust and Reproducible Research on Algorithmic Fairness</h3>
<ul>
<li><strong>Authors: </strong>Jan Simson, Alessandro Fabris, Cosima Fr√∂hner, Frauke Kreuter, Christoph Kern</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22363">https://arxiv.org/abs/2510.22363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22363">https://arxiv.org/pdf/2510.22363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22363]] Bias Begins with Data: The FairGround Corpus for Robust and Reproducible Research on Algorithmic Fairness(https://arxiv.org/abs/2510.22363)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>As machine learning (ML) systems are increasingly adopted in high-stakes decision-making domains, ensuring fairness in their outputs has become a central challenge. At the core of fair ML research are the datasets used to investigate bias and develop mitigation strategies. Yet, much of the existing work relies on a narrow selection of datasets--often arbitrarily chosen, inconsistently processed, and lacking in diversity--undermining the generalizability and reproducibility of results. To address these limitations, we present FairGround: a unified framework, data corpus, and Python package aimed at advancing reproducible research and critical data studies in fair ML classification. FairGround currently comprises 44 tabular datasets, each annotated with rich fairness-relevant metadata. Our accompanying Python package standardizes dataset loading, preprocessing, transformation, and splitting, streamlining experimental workflows. By providing a diverse and well-documented dataset corpus along with robust tooling, FairGround enables the development of fairer, more reliable, and more reproducible ML models. All resources are publicly available to support open and collaborative research.</li>
</ul>

<h3>Title: T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jindong Yang, Han Fang, Weiming Zhang, Nenghai Yu, Kejiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22366">https://arxiv.org/abs/2510.22366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22366">https://arxiv.org/pdf/2510.22366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22366]] T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models(https://arxiv.org/abs/2510.22366)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, extraction, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have advanced rapidly in recent years, producing high-fidelity images while raising concerns about intellectual property protection and the misuse of generative AI. Image watermarking for diffusion models, particularly Noise-as-Watermark (NaW) methods, encode watermark as specific standard Gaussian noise vector for image generation, embedding the infomation seamlessly while maintaining image quality. For detection, the generation process is inverted to recover the initial noise vector containing the watermark before extraction. However, existing NaW methods struggle to balance watermark robustness with generation diversity. Some methods achieve strong robustness by heavily constraining initial noise sampling, which degrades user experience, while others preserve diversity but prove too fragile for real-world deployment. To address this issue, we propose T2SMark, a two-stage watermarking scheme based on Tail-Truncated Sampling (TTS). Unlike prior methods that simply map bits to positive or negative values, TTS enhances robustness by embedding bits exclusively in the reliable tail regions while randomly sampling the central zone to preserve the latent distribution. Our two-stage framework then ensures sampling diversity by integrating a randomly generated session key into both encryption pipelines. We evaluate T2SMark on diffusion models with both U-Net and DiT backbones. Extensive experiments show that it achieves an optimal balance between robustness and diversity. Our code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: GigaEmbeddings: Efficient Russian Language Embedding Model</h3>
<ul>
<li><strong>Authors: </strong>Egor Kolodin, Daria Khomich, Nikita Savushkin, Anastasia Ianina, Fyodor Minkin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22369">https://arxiv.org/abs/2510.22369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22369">https://arxiv.org/pdf/2510.22369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22369]] GigaEmbeddings: Efficient Russian Language Embedding Model(https://arxiv.org/abs/2510.22369)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We introduce GigaEmbeddings, a novel framework for training high-performance Russian-focused text embeddings through hierarchical instruction tuning of the decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our three-stage pipeline, comprising large-scale contrastive pre-training in web-scale corpora, fine-tuning with hard negatives, and multitask generalization across retrieval, classification, and clustering tasks, addresses key limitations of existing methods by unifying diverse objectives and leveraging synthetic data generation. Architectural innovations include bidirectional attention for contextual modeling, latent attention pooling for robust sequence aggregation, and strategic pruning of 25% of transformer layers to enhance efficiency without compromising performance. Evaluated on the ruMTEB benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves state-of-the-art results (69.1 avg. score), outperforming strong baselines with a larger number of parameters.</li>
</ul>

<h3>Title: VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Xie, Zhiyang Zhang, Yifan Wu, Sirong Lu, Jiayi Zhang, Zhaoyang Yu, Jinlin Wang, Sirui Hong, Bang Liu, Chenglin Wu, Yuyu Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22373">https://arxiv.org/abs/2510.22373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22373">https://arxiv.org/pdf/2510.22373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22373]] VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations(https://arxiv.org/abs/2510.22373)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at this https URL.</li>
</ul>

<h3>Title: Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization</h3>
<ul>
<li><strong>Authors: </strong>David Freire-Obreg√≥n, Jos√© Salas-C√°ceres, Modesto Castrill√≥n-Santana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22383">https://arxiv.org/abs/2510.22383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22383">https://arxiv.org/pdf/2510.22383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22383]] Dynamic Dropout: Leveraging Conway's Game of Life for Neural Networks Regularization(https://arxiv.org/abs/2510.22383)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Regularization techniques play a crucial role in preventing overfitting and improving the generalization performance of neural networks. Dropout, a widely used regularization technique, randomly deactivates units during training to introduce redundancy and prevent co-adaptation among neurons. Despite its effectiveness, dropout has limitations, such as its static nature and lack of interpretability. In this paper, we propose a novel approach to regularization by substituting dropout with Conway's Game of Life (GoL), a cellular automata with simple rules that govern the evolution of a grid of cells. We introduce dynamic unit deactivation during training by representing neural network units as cells in a GoL grid and applying the game's rules to deactivate units. This approach allows for the emergence of spatial patterns that adapt to the training data, potentially enhancing the network's ability to generalize. We demonstrate the effectiveness of our approach on the CIFAR-10 dataset, showing that dynamic unit deactivation using GoL achieves comparable performance to traditional dropout techniques while offering insights into the network's behavior through the visualization of evolving patterns. Furthermore, our discussion highlights the applicability of our proposal in deeper architectures, demonstrating how it enhances the performance of different dropout techniques.</li>
</ul>

<h3>Title: Privacy-Aware Federated nnU-Net for ECG Page Digitization</h3>
<ul>
<li><strong>Authors: </strong>Nader Nemati</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22387">https://arxiv.org/abs/2510.22387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22387">https://arxiv.org/pdf/2510.22387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22387]] Privacy-Aware Federated nnU-Net for ECG Page Digitization(https://arxiv.org/abs/2510.22387)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Deep neural networks can convert ECG page images into analyzable waveforms, yet centralized training often conflicts with cross-institutional privacy and deployment constraints. A cross-silo federated digitization framework is presented that trains a full-model nnU-Net segmentation backbone without sharing images and aggregates updates across sites under realistic non-IID heterogeneity (layout, grid style, scanner profile, noise). The protocol integrates three standard server-side aggregators--FedAvg, FedProx, and FedAdam--and couples secure aggregation with central, user-level differential privacy to align utility with formal guarantees. Key features include: (i) end-to-end full-model training and synchronization across clients; (ii) secure aggregation so the server only observes a clipped, weighted sum once a participation threshold is met; (iii) central Gaussian DP with Renyi accounting applied post-aggregation for auditable user-level privacy; and (iv) a calibration-aware digitization pipeline comprising page normalization, trace segmentation, grid-leakage suppression, and vectorization to twelve-lead signals. Experiments on ECG pages rendered from PTB-XL show consistently faster convergence and higher late-round plateaus with adaptive server updates (FedAdam) relative to FedAvg and FedProx, while approaching centralized performance. The privacy mechanism maintains competitive accuracy while preventing exposure of raw images or per-client updates, yielding deployable, auditable guarantees suitable for multi-institution settings.</li>
</ul>

<h3>Title: Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Federica Gamba, Aman Sinha, Timothee Mickus, Raul Vazquez, Patanjali Bhamidipati, Claudio Savelli, Ahana Chattopadhyay, Laura A. Zanella, Yash Kankanampati, Binesh Arakkal Remesh, Aryan Ashok Chandramania, Rohit Agarwal, Chuyuan Li, Ioana Buhnila, Radhika Mamidi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22395">https://arxiv.org/abs/2510.22395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22395">https://arxiv.org/pdf/2510.22395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22395]] Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection(https://arxiv.org/abs/2510.22395)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce the CAP (Confabulations from ACL Publications) dataset, a multilingual resource for studying hallucinations in large language models (LLMs) within scientific text generation. CAP focuses on the scientific domain, where hallucinations can distort factual knowledge, as they frequently do. In this domain, however, the presence of specialized terminology, statistical reasoning, and context-dependent interpretations further exacerbates these distortions, particularly given LLMs' lack of true comprehension, limited contextual understanding, and bias toward surface-level generalization. CAP operates in a cross-lingual setting covering five high-resource languages (English, French, Hindi, Italian, and Spanish) and four low-resource languages (Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated scientific questions and over 7000 LLM-generated answers from 16 publicly available models, provided as question-answer pairs along with token sequences and corresponding logits. Each instance is annotated with a binary label indicating the presence of a scientific hallucination, denoted as a factuality error, and a fluency label, capturing issues in the linguistic quality or naturalness of the text. CAP is publicly released to facilitate advanced research on hallucination detection, multilingual evaluation of LLMs, and the development of more reliable scientific NLP systems.</li>
</ul>

<h3>Title: PortGPT: Towards Automated Backporting Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Li, Zheng Yu, Jingyi Song, Meng Xu, Yuxuan Luo, Dongliang Mu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22396">https://arxiv.org/abs/2510.22396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22396">https://arxiv.org/pdf/2510.22396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22396]] PortGPT: Towards Automated Backporting Using Large Language Models(https://arxiv.org/abs/2510.22396)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Patch backporting, the process of migrating mainline security patches to older branches, is an essential task in maintaining popular open-source projects (e.g., Linux kernel). However, manual backporting can be labor-intensive, while existing automated methods, which heavily rely on predefined syntax or semantic rules, often lack agility for complex patches. In this paper, we introduce PORTGPT, an LLM-agent for end-to-end automation of patch backporting in real-world scenarios. PORTGPT enhances an LLM with tools to access code on-demand, summarize Git history, and revise patches autonomously based on feedback (e.g., from compilers), hence, simulating human-like reasoning and verification. PORTGPT achieved an 89.15% success rate on existing datasets (1815 cases), and 62.33% on our own dataset of 146 complex cases, both outperforms state-of-the-art of backporting tools. We contributed 9 backported patches from PORTGPT to the Linux kernel community and all patches are now merged.</li>
</ul>

<h3>Title: ProGQL: A Provenance Graph Query System for Cyber Attack Investigation</h3>
<ul>
<li><strong>Authors: </strong>Fei Shao, Jia Zou, Zhichao Cao, Xusheng Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22400">https://arxiv.org/abs/2510.22400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22400">https://arxiv.org/pdf/2510.22400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22400]] ProGQL: A Provenance Graph Query System for Cyber Attack Investigation(https://arxiv.org/abs/2510.22400)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Provenance analysis (PA) has recently emerged as an important solution for cyber attack investigation. PA leverages system monitoring to monitor system activities as a series of system audit events and organizes these events as a provenance graph to show the dependencies among system activities, which can reveal steps of cyber attacks. Despite their potential, existing PA techniques face two critical challenges: (1) they are inflexible and non-extensible, making it difficult to incorporate analyst expertise, and (2) they are memory inefficient, often requiring>100GB of RAM to hold entire event streams, which fundamentally limits scalability and deployment in real-world environments. To address these limitations, we propose the PROGQL framework, which provides a domain-specific graph search language with a well-engineered query engine, allowing PA over system audit events and expert knowledge to be jointly expressed as a graph search query and thereby facilitating the investigation of complex cyberattacks. In particular, to support dependency searches from a starting edge required in PA, PROGQL introduces new language constructs for constrained graph traversal, edge weight computation, value propagation along weighted edges, and graph merging to integrate multiple searches. Moreover, the PROGQL query engine is optimized for efficient incremental graph search across heterogeneous database backends, eliminating the need for full in-memory materialization and reducing memory overhead. Our evaluations on real attacks demonstrate the effectiveness of the PROGQL language in expressing a diverse set of complex attacks compared with the state-of-the-art graph query language Cypher, and the comparison with the SOTA PA technique DEPIMPACT further demonstrates the significant improvement of the scalability brought by our PROGQL framework's design.</li>
</ul>

<h3>Title: Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents</h3>
<ul>
<li><strong>Authors: </strong>Vijay Veerabadran, Fanyi Xiao, Nitin Kamra, Pedro Matias, Joy Chen, Caley Drooff, Brett D Roads, Riley Williams, Ethan Henderson, Xuanyi Zhao, Kevin Carlberg, Joseph Tighe, Karl Ridgeway</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22443">https://arxiv.org/abs/2510.22443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22443">https://arxiv.org/pdf/2510.22443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22443]] Benchmarking Egocentric Multimodal Goal Inference for Assistive Wearable Agents(https://arxiv.org/abs/2510.22443)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>There has been a surge of interest in assistive wearable agents: agents embodied in wearable form factors (e.g., smart glasses) who take assistive actions toward a user's goal/query (e.g. "Where did I leave my keys?"). In this work, we consider the important complementary problem of inferring that goal from multi-modal contextual observations. Solving this "goal inference" problem holds the promise of eliminating the effort needed to interact with such an agent. This work focuses on creating WAGIBench, a strong benchmark to measure progress in solving this problem using vision-language models (VLMs). Given the limited prior work in this area, we collected a novel dataset comprising 29 hours of multimodal data from 348 participants across 3,477 recordings, featuring ground-truth goals alongside accompanying visual, audio, digital, and longitudinal contextual observations. We validate that human performance exceeds model performance, achieving 93% multiple-choice accuracy compared with 84% for the best-performing VLM. Generative benchmark results that evaluate several families of modern vision-language models show that larger models perform significantly better on the task, yet remain far from practical usefulness, as they produce relevant goals only 55% of the time. Through a modality ablation, we show that models benefit from extra information in relevant modalities with minimal performance degradation from irrelevant modalities.</li>
</ul>

<h3>Title: Backward-Friendly Optimization: Training Large Language Models with Approximate Gradients under Memory Constraints</h3>
<ul>
<li><strong>Authors: </strong>Jing Yang, Kaitong Cai, Yijia Fan, Yufeng Yang, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22467">https://arxiv.org/abs/2510.22467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22467">https://arxiv.org/pdf/2510.22467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22467]] Backward-Friendly Optimization: Training Large Language Models with Approximate Gradients under Memory Constraints(https://arxiv.org/abs/2510.22467)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Full fine-tuning of Large Language Models (LLMs) is notoriously memory-intensive, primarily because conventional optimizers such as SGD or Adam assume access to exact gradients derived from cached activations. Existing solutions either alter the model architecture (e.g., reversible networks) or trade memory for computation (e.g., activation checkpointing), but the optimizer itself remains untouched. In this work, we introduce GradLite, a backward-friendly optimizer that relaxes the requirement of exact gradients, enabling efficient training even when intermediate activations are aggressively discarded or approximated. GradLite leverages two key techniques: (i) low-rank Jacobian approximation, which reduces the dimensionality of backpropagated error signals, and (ii) error-feedback correction, which accumulates and compensates approximation errors across iterations to preserve convergence guarantees. We provide a theoretical analysis showing that GradLite maintains unbiased gradient estimates with bounded variance, ensuring convergence rates comparable to Adam. Empirically, GradLite reduces optimizer-state and activation memory consumption by up to 50\% without architectural changes, and achieves on-par or superior downstream performance on reasoning (MMLU, GSM8K), multilingual, and dialogue benchmarks compared to checkpointing and optimizer-centric baselines (LoMo, GaLore).</li>
</ul>

<h3>Title: DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss</h3>
<ul>
<li><strong>Authors: </strong>Jing Yang, Yufeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22473">https://arxiv.org/abs/2510.22473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22473">https://arxiv.org/pdf/2510.22473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22473]] DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss(https://arxiv.org/abs/2510.22473)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in 2D and 3D generative models have expanded the capabilities of computer vision. However, generating high-quality 4D dynamic content from a single static image remains a significant challenge. Traditional methods have limitations in modeling temporal dependencies and accurately capturing dynamic geometry changes, especially when considering variations in camera perspective. To address this issue, we propose DynaPose4D, an innovative solution that integrates 4D Gaussian Splatting (4DGS) techniques with Category-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D Gaussian Splatting to construct a 3D model from single images, then predicts multi-view pose keypoints based on one-shot support from a chosen view, leveraging supervisory signals to enhance motion consistency. Experimental results show that DynaPose4D achieves excellent coherence, consistency, and fluidity in dynamic motion generation. These findings not only validate the efficacy of the DynaPose4D framework but also indicate its potential applications in the domains of computer vision and animation production.</li>
</ul>

<h3>Title: CHOIR: Collaborative Harmonization fOr Inference Robustness</h3>
<ul>
<li><strong>Authors: </strong>Xiangjue Dong, Cong Wang, Maria Teleki, Millennium Bismay, James Caverlee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22475">https://arxiv.org/abs/2510.22475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22475">https://arxiv.org/pdf/2510.22475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22475]] CHOIR: Collaborative Harmonization fOr Inference Robustness(https://arxiv.org/abs/2510.22475)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Persona-assigned Large Language Models (LLMs) can adopt diverse roles, enabling personalized and context-aware reasoning. However, even minor demographic perturbations in personas, such as simple pronoun changes, can alter reasoning trajectories, leading to divergent sets of correct answers. Instead of treating these variations as biases to be mitigated, we explore their potential as a constructive resource to improve reasoning robustness. We propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a test-time framework that harmonizes multiple persona-conditioned reasoning signals into a unified prediction. CHOIR orchestrates a collaborative decoding process among counterfactual personas, dynamically balancing agreement and divergence in their reasoning paths. Experiments on various reasoning benchmarks demonstrate that CHOIR consistently enhances performance across demographics, model architectures, scales, and tasks - without additional training. Improvements reach up to 26.4% for individual demographic groups and 19.2% on average across five demographics. It remains effective even when base personas are suboptimal. By reframing persona variation as a constructive signal, CHOIR provides a scalable and generalizable approach to more reliable LLM reasoning.</li>
</ul>

<h3>Title: Frustratingly Easy Task-aware Pruning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuanhe Tian, Junjie Liu, Xican Yang, Haishan Ye, Yan Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22489">https://arxiv.org/abs/2510.22489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22489">https://arxiv.org/pdf/2510.22489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22489]] Frustratingly Easy Task-aware Pruning for Large Language Models(https://arxiv.org/abs/2510.22489)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pruning provides a practical solution to reduce the resources required to run large language models (LLMs) to benefit from their effective capabilities as well as control their cost for training and inference. Research on LLM pruning often ranks the importance of LLM parameters using their magnitudes and calibration-data activations and removes (or masks) the less important ones, accordingly reducing LLMs' size. However, these approaches primarily focus on preserving the LLM's ability to generate fluent sentences, while neglecting performance on specific domains and tasks. In this paper, we propose a simple yet effective pruning approach for LLMs that preserves task-specific capabilities while shrinking their parameter space. We first analyze how conventional pruning minimizes loss perturbation under general-domain calibration and extend this formulation by incorporating task-specific feature distributions into the importance computation of existing pruning algorithms. Thus, our framework computes separate importance scores using both general and task-specific calibration data, partitions parameters into shared and exclusive groups based on activation-norm differences, and then fuses their scores to guide the pruning process. This design enables our method to integrate seamlessly with various foundation pruning techniques and preserve the LLM's specialized abilities under compression. Experiments on widely used benchmarks demonstrate that our approach is effective and consistently outperforms the baselines with identical pruning ratios and different settings.</li>
</ul>

<h3>Title: LAMP: Data-Efficient Linear Affine Weight-Space Models for Parameter-Controlled 3D Shape Generation and Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Ghadi Nehme, Yanxia Zhang, Dule Shu, Matt Klenk, Faez Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22491">https://arxiv.org/abs/2510.22491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22491">https://arxiv.org/pdf/2510.22491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22491]] LAMP: Data-Efficient Linear Affine Weight-Space Models for Parameter-Controlled 3D Shape Generation and Extrapolation(https://arxiv.org/abs/2510.22491)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generating high-fidelity 3D geometries that satisfy specific parameter constraints has broad applications in design and engineering. However, current methods typically rely on large training datasets and struggle with controllability and generalization beyond the training distributions. To overcome these limitations, we introduce LAMP (Linear Affine Mixing of Parametric shapes), a data-efficient framework for controllable and interpretable 3D generation. LAMP first aligns signed distance function (SDF) decoders by overfitting each exemplar from a shared initialization, then synthesizes new geometries by solving a parameter-constrained mixing problem in the aligned weight space. To ensure robustness, we further propose a safety metric that detects geometry validity via linearity mismatch. We evaluate LAMP on two 3D parametric benchmarks: DrivAerNet++ and BlendedNet. We found that LAMP enables (i) controlled interpolation within bounds with as few as 100 samples, (ii) safe extrapolation by up to 100% parameter difference beyond training ranges, (iii) physics performance-guided optimization under fixed parameters. LAMP significantly outperforms conditional autoencoder and Deep Network Interpolation (DNI) baselines in both extrapolation and data efficiency. Our results demonstrate that LAMP advances controllable, data-efficient, and safe 3D generation for design exploration, dataset generation, and performance-driven optimization.</li>
</ul>

<h3>Title: Scalable Oversight via Partitioned Human Supervision</h3>
<ul>
<li><strong>Authors: </strong>Ren Yin, Takashi Ishida, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22500">https://arxiv.org/abs/2510.22500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22500">https://arxiv.org/pdf/2510.22500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22500]] Scalable Oversight via Partitioned Human Supervision(https://arxiv.org/abs/2510.22500)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As artificial intelligence (AI) systems approach and surpass expert human performance across a broad range of tasks, obtaining high-quality human supervision for evaluation and training becomes increasingly challenging. Our focus is on tasks that require deep knowledge and skills of multiple domains. Unfortunately, even the best human experts are knowledgeable only in a single narrow area, and will not be able to evaluate the correctness of advanced AI systems on such superhuman tasks. However, based on their narrow expertise, humans may provide a weak signal, i.e., a complementary label indicating an option that is incorrect. For example, a cardiologist could state that "this is not related to cardiology,'' even if they cannot identify the true disease. Based on this weak signal, we propose a scalable oversight framework that enables us to evaluate frontier AI systems without the need to prepare the ground truth. We derive an unbiased estimator of top-1 accuracy from complementary labels and quantify how many complementary labels are needed to match the variance of ordinary labels. We further introduce two estimators to combine scarce ordinary labels with abundant complementary labels. We provide finite-sample deviation guarantees for both complementary-only and the mixed estimators. Empirically, we show that we can evaluate the output of large language models without the ground truth, if we have complementary labels. We further show that we can train an AI system with such weak signals: we show how we can design an agentic AI system automatically that can perform better with this partitioned human supervision. Our code is available at this https URL.</li>
</ul>

<h3>Title: Accelerating Materials Design via LLM-Guided Evolutionary Search</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Abhyankar, Sanchit Kabra, Saaketh Desai, Chandan K. Reddy</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22503">https://arxiv.org/abs/2510.22503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22503">https://arxiv.org/pdf/2510.22503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22503]] Accelerating Materials Design via LLM-Guided Evolutionary Search(https://arxiv.org/abs/2510.22503)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Materials discovery requires navigating vast chemical and structural spaces while satisfying multiple, often conflicting, objectives. We present LLM-guided Evolution for MAterials design (LLEMA), a unified framework that couples the scientific knowledge embedded in large language models with chemistry-informed evolutionary rules and memory-based refinement. At each iteration, an LLM proposes crystallographically specified candidates under explicit property constraints; a surrogate-augmented oracle estimates physicochemical properties; and a multi-objective scorer updates success/failure memories to guide subsequent generations. Evaluated on 14 realistic tasks spanning electronics, energy, coatings, optics, and aerospace, LLEMA discovers candidates that are chemically plausible, thermodynamically stable, and property-aligned, achieving higher hit-rates and stronger Pareto fronts than generative and LLM-only baselines. Ablation studies confirm the importance of rule-guided generation, memory-based refinement, and surrogate prediction. By enforcing synthesizability and multi-objective trade-offs, LLEMA delivers a principled pathway to accelerate practical materials discovery. Code: this https URL</li>
</ul>

<h3>Title: CANDI: Hybrid Discrete-Continuous Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Pynadath, Jiaxin Shi, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22510">https://arxiv.org/abs/2510.22510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22510">https://arxiv.org/pdf/2510.22510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22510]] CANDI: Hybrid Discrete-Continuous Diffusion Models(https://arxiv.org/abs/2510.22510)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While continuous diffusion has shown remarkable success in continuous domains such as image generation, its direct application to discrete data has underperformed compared to purely discrete formulations. This gap is counterintuitive, given that continuous diffusion learns score functions that enable joint evolution across multiple positions. To understand this gap, we introduce token identifiability as an analytical framework for understanding how Gaussian noise corrupts discrete data through two mechanisms: discrete identity corruption and continuous rank degradation. We reveal that these mechanisms scale differently with vocabulary size, creating a temporal dissonance: at noise levels where discrete corruption preserves enough structure for conditional learning, continuous denoising is trivial; at noise levels where continuous denoising is meaningful, discrete corruption destroys nearly all conditional structure. To solve this, we propose CANDI (Continuous ANd DIscrete diffusion), a hybrid framework that decouples discrete and continuous corruption, enabling simultaneous learning of both conditional structure and continuous geometry. We empirically validate the temporal dissonance phenomenon and demonstrate that CANDI successfully avoids it. This unlocks the benefits of continuous diffusion for discrete spaces: on controlled generation, CANDI enables classifier-based guidance with off-the-shelf classifiers through simple gradient addition; on text generation, CANDI outperforms masked diffusion at low NFE, demonstrating the value of learning continuous gradients for discrete spaces.</li>
</ul>

<h3>Title: Toward Robust Signed Graph Learning through Joint Input-Target Denoising</h3>
<ul>
<li><strong>Authors: </strong>Junran Wu, Beng Chin Ooi, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22513">https://arxiv.org/abs/2510.22513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22513">https://arxiv.org/pdf/2510.22513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22513]] Toward Robust Signed Graph Learning through Joint Input-Target Denoising(https://arxiv.org/abs/2510.22513)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Signed Graph Neural Networks (SGNNs) are widely adopted to analyze complex patterns in signed graphs with both positive and negative links. Given the noisy nature of real-world connections, the robustness of SGNN has also emerged as a pivotal research area. Under the supervision of empirical properties, graph structure learning has shown its robustness on signed graph representation learning, however, there remains a paucity of research investigating a robust SGNN with theoretical guidance. Inspired by the success of graph information bottleneck (GIB) in information extraction, we propose RIDGE, a novel framework for Robust sI gned graph learning through joint Denoising of Graph inputs and supervision targEts. Different from the basic GIB, we extend the GIB theory with the capability of target space denoising as the co-existence of noise in both input and target spaces. In instantiation, RIDGE effectively cleanses input data and supervision targets via a tractable objective function produced by reparameterization mechanism and variational approximation. We extensively validate our method on four prevalent signed graph datasets, and the results show that RIDGE clearly improves the robustness of popular SGNN models under various levels of noise.</li>
</ul>

<h3>Title: A Scalable Global Optimization Algorithm For Constrained Clustering</h3>
<ul>
<li><strong>Authors: </strong>Pedro Chumpitaz-Flores, My Duong, Cristobal Heredia, Kaixun Hua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22519">https://arxiv.org/abs/2510.22519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22519">https://arxiv.org/pdf/2510.22519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22519]] A Scalable Global Optimization Algorithm For Constrained Clustering(https://arxiv.org/abs/2510.22519)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Constrained clustering leverages limited domain knowledge to improve clustering performance and interpretability, but incorporating pairwise must-link and cannot-link constraints is an NP-hard challenge, making global optimization intractable. Existing mixed-integer optimization methods are confined to small-scale datasets, limiting their utility. We propose Sample-Driven Constrained Group-Based Branch-and-Bound (SDC-GBB), a decomposable branch-and-bound (BB) framework that collapses must-linked samples into centroid-based pseudo-samples and prunes cannot-link through geometric rules, while preserving convergence and guaranteeing global optimality. By integrating grouped-sample Lagrangian decomposition and geometric elimination rules for efficient lower and upper bounds, the algorithm attains highly scalable pairwise k-Means constrained clustering via parallelism. Experimental results show that our approach handles datasets with 200,000 samples with cannot-link constraints and 1,500,000 samples with must-link constraints, which is 200 - 1500 times larger than the current state-of-the-art under comparable constraint settings, while reaching an optimality gap of less than 3%. In providing deterministic global guarantees, our method also avoids the search failures that off-the-shelf heuristics often encounter on large datasets.</li>
</ul>

<h3>Title: AesCrop: Aesthetic-driven Cropping Guided by Composition</h3>
<ul>
<li><strong>Authors: </strong>Yen-Hong Wong, Lai-Kuan Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22528">https://arxiv.org/abs/2510.22528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22528">https://arxiv.org/pdf/2510.22528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22528]] AesCrop: Aesthetic-driven Cropping Guided by Composition(https://arxiv.org/abs/2510.22528)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Aesthetic-driven image cropping is crucial for applications like view recommendation and thumbnail generation, where visual appeal significantly impacts user engagement. A key factor in visual appeal is composition--the deliberate arrangement of elements within an image. Some methods have successfully incorporated compositional knowledge through evaluation-based and regression-based paradigms. However, evaluation-based methods lack globality while regression-based methods lack diversity. Recently, hybrid approaches that integrate both paradigms have emerged, bridging the gap between these two to achieve better diversity and globality. Notably, existing hybrid methods do not incorporate photographic composition guidance, a key attribute that defines photographic aesthetics. In this work, we introduce AesCrop, a composition-aware hybrid image-cropping model that integrates a VMamba image encoder, augmented with a novel Mamba Composition Attention Bias (MCAB) and a transformer decoder to perform end-to-end rank-based image cropping, generating multiple crops along with the corresponding quality scores. By explicitly encoding compositional cues into the attention mechanism, MCAB directs AesCrop to focus on the most compositionally salient regions. Extensive experiments demonstrate that AesCrop outperforms current state-of-the-art methods, delivering superior quantitative metrics and qualitatively more pleasing crops.</li>
</ul>

<h3>Title: Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing</h3>
<ul>
<li><strong>Authors: </strong>Xiang Fei, Tina Tian, Howie Choset, Lu Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22529">https://arxiv.org/abs/2510.22529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22529">https://arxiv.org/pdf/2510.22529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22529]] Bag-of-Word-Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing(https://arxiv.org/abs/2510.22529)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Loop closure is critical in Simultaneous Localization and Mapping (SLAM) systems to reduce accumulative drift and ensure global mapping consistency. However, conventional methods struggle in perceptually aliased environments, such as narrow pipes, due to vector quantization, feature sparsity, and repetitive textures, while existing solutions often incur high computational costs. This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure detection method that achieves superior precision-recall, robustness, and computational efficiency. The core innovation lies in the introduction of word groups, which captures the spatial co-occurrence and proximity of visual words to construct an online dictionary. Additionally, drawing inspiration from probabilistic transition models, we incorporate temporal consistency directly into similarity computation with an adaptive scheme, substantially improving precision-recall performance. The method is further strengthened by a feature distribution analysis module and dedicated post-verification mechanisms. To evaluate the effectiveness of our method, we conduct experiments on both public datasets and a confined-pipe dataset we constructed. Results demonstrate that BoWG surpasses state-of-the-art methods, including both traditional and learning-based approaches, in terms of precision-recall and computational efficiency. Our approach also exhibits excellent scalability, achieving an average processing time of 16 ms per image across 17,565 images in the Bicocca25b dataset.</li>
</ul>

<h3>Title: Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection</h3>
<ul>
<li><strong>Authors: </strong>Noshitha Padma Pratyusha Juttu, Sahithi Singireddy, Sravani Gona, Sujal Timilsina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22531">https://arxiv.org/abs/2510.22531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22531">https://arxiv.org/pdf/2510.22531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22531]] Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection(https://arxiv.org/abs/2510.22531)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have transformed text understanding, yet their adaptation to specialized legal domains remains constrained by the cost of full fine-tuning. This study provides a systematic evaluation of fine tuning, parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting strategies for unfair clause detection in Terms of Service (ToS) documents, a key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that full fine-tuning achieves the strongest precision recall balance, while LoRA-based models provide competitive recall with up to 3x lower memory cost. These findings highlight practical design trade-offs for efficient and domain-adapted LLMs, contributing open baselines for fine-tuning research in legal text processing.</li>
</ul>

<h3>Title: SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution with Spatially Re-Focused Text-Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Majid Abdolshah, Violetta Shevchenko, Hongdong Li, Chang Xu, Pulak Purkait</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22534">https://arxiv.org/abs/2510.22534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22534">https://arxiv.org/pdf/2510.22534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22534]] SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution with Spatially Re-Focused Text-Conditioning(https://arxiv.org/abs/2510.22534)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Existing diffusion-based super-resolution approaches often exhibit semantic ambiguities due to inaccuracies and incompleteness in their text conditioning, coupled with the inherent tendency for cross-attention to divert towards irrelevant pixels. These limitations can lead to semantic misalignment and hallucinated details in the generated high-resolution outputs. To address these, we propose a novel, plug-and-play spatially re-focused super-resolution (SRSR) framework that consists of two core components: first, we introduce Spatially Re-focused Cross-Attention (SRCA), which refines text conditioning at inference time by applying visually-grounded segmentation masks to guide cross-attention. Second, we introduce a Spatially Targeted Classifier-Free Guidance (STCFG) mechanism that selectively bypasses text influences on ungrounded pixels to prevent hallucinations. Extensive experiments on both synthetic and real-world datasets demonstrate that SRSR consistently outperforms seven state-of-the-art baselines in standard fidelity metrics (PSNR and SSIM) across all datasets, and in perceptual quality measures (LPIPS and DISTS) on two real-world benchmarks, underscoring its effectiveness in achieving both high semantic fidelity and perceptual quality in super-resolution.</li>
</ul>

<h3>Title: ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole</h3>
<ul>
<li><strong>Authors: </strong>Jotaro Yano</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22536">https://arxiv.org/abs/2510.22536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22536">https://arxiv.org/pdf/2510.22536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22536]] ZK Coprocessor Bridge: Replay-Safe Private Execution from Solana to Aztec via Wormhole(https://arxiv.org/abs/2510.22536)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We formalize a cross-domain "ZK coprocessor bridge" that lets Solana programs request private execution on Aztec L2 (via Ethereum) using Wormhole Verifiable Action Approvals (VAAs) as authenticated transport. The system comprises: (i) a Solana program that posts messages to Wormhole Core with explicit finality; (ii) an EVM Portal that verifies VAAs, enforces a replay lock, parses a bound payload secretHash||m from the attested VAA, derives a domain-separated field commitment, and enqueues an L1->L2 message into the Aztec Inbox (our reference implementation v0.1.0 currently uses consumeWithSecret(vaa, secretHash); we provide migration guidance to the payload-bound interface); (iii) a minimal Aztec contract that consumes the message privately; and (iv) an off-chain relayer that ferries VAAs and can record receipts on Solana. We present state machines, message formats, and proof sketches for replay-safety, origin authenticity, finality alignment, parameter binding (no relayer front-running of Aztec parameters), privacy, idempotence, and liveness. Finally, we include a concise Reproducibility note with pinned versions and artifacts to replicate a public testnet run.</li>
</ul>

<h3>Title: FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuyang Ding, Chi Zhang, Juntao Li, Haibin Lin, Xin Liu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22543">https://arxiv.org/abs/2510.22543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22543">https://arxiv.org/pdf/2510.22543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22543]] FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning(https://arxiv.org/abs/2510.22543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models (LLMs). In this context, models explore reasoning trajectories and exploit rollouts with correct answers as positive signals for policy optimization. However, these rollouts might involve flawed patterns such as answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are rewarded identically to fully correct ones, causing policy models to internalize these unreliable reasoning patterns. In this work, we first conduct a systematic study of flawed-positive rollouts in RL and find that they enable rapid capability gains during the early optimization stage, while constraining reasoning capability later by reinforcing unreliable patterns. Building on these insights, we propose Flawed-Aware Policy Optimization (FAPO), which presents a parameter-free reward penalty for flawed-positive rollouts, enabling the policy to leverage them as useful shortcuts in the warm-up stage, securing stable early gains, while gradually shifting optimization toward reliable reasoning in the later refinement stage. To accurately and comprehensively detect flawed-positive rollouts, we introduce a generative reward model (GenRM) with a process-level reward that precisely localizes reasoning errors. Experiments show that FAPO is effective in broad domains, improving outcome correctness, process reliability, and training stability without increasing the token budget.</li>
</ul>

<h3>Title: LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan He, Yuxuan Wang, Jiaqi Li, Kexin Liang, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22548">https://arxiv.org/abs/2510.22548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22548">https://arxiv.org/pdf/2510.22548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22548]] LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?(https://arxiv.org/abs/2510.22548)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are equipped with increasingly extended context windows recently, yet their long context understanding capabilities over long dependency tasks remain fundamentally limited and underexplored. This gap is especially significant in many real-world long-context applications that were rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark designed to evaluate LLMs' long context ability in real-world applications and scenarios. Our benchmark consists of automatically collected real-world long texts, ranging from 16k to 2M tokens, encompassing domains in law, finance, game and code. Accordingly, we delicately design 10 types of domain-specific long-dependency tasks and generate 1,934 QA instances with various diversity and complexity in a scalable data curation pipeline for further practical needs. We conduct a comprehensive assessment of 6 locally deployed and 4 API-based LLMs. The evaluation results show that even the best-performing model achieves only a 59.2% overall score on our benchmark. Despite the extensive context windows, popular LLMs are only capable of understanding a much shorter length of context than they claim to be, revealing significant limitations in their ability to handle real-world tasks with long dependencies and highlighting substantial room for model improvement in practical long-context understanding.</li>
</ul>

<h3>Title: DDTR: Diffusion Denoising Trace Recovery</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Matyash, Avigdor Gal, Arik Senderovich</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22553">https://arxiv.org/abs/2510.22553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22553">https://arxiv.org/pdf/2510.22553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22553]] DDTR: Diffusion Denoising Trace Recovery(https://arxiv.org/abs/2510.22553)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>With recent technological advances, process logs, which were traditionally deterministic in nature, are being captured from non-deterministic sources, such as uncertain sensors or machine learning models (that predict activities using cameras). In the presence of stochastically-known logs, logs that contain probabilistic information, the need for stochastic trace recovery increases, to offer reliable means of understanding the processes that govern such systems. We design a novel deep learning approach for stochastic trace recovery, based on Diffusion Denoising Probabilistic Models (DDPM), which makes use of process knowledge (either implicitly by discovering a model or explicitly by injecting process knowledge in the training phase) to recover traces by denoising. We conduct an empirical evaluation demonstrating state-of-the-art performance with up to a 25% improvement over existing methods, along with increased robustness under high noise levels.</li>
</ul>

<h3>Title: Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers</h3>
<ul>
<li><strong>Authors: </strong>Dongyi Liu, Jiangtong Li, Dawei Cheng, Changjun Jiang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22555">https://arxiv.org/abs/2510.22555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22555">https://arxiv.org/pdf/2510.22555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22555]] Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers(https://arxiv.org/abs/2510.22555)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, where adversaries implant malicious triggers to manipulate model predictions. Existing trigger generators are often simplistic in structure and overly reliant on specific features, confining them to a single graph learning paradigm, such as graph supervised learning, graph contrastive learning, or graph prompt learning. This specialized design, which aligns the trigger with one learning objective, results in poor transferability when applied to other learning paradigms. For instance, triggers generated for the graph supervised learning paradigm perform poorly when tested within graph contrastive learning or graph prompt learning environments. Furthermore, these simple generators often fail to utilize complex structural information or node diversity within the graph data. These constraints limit the attack success rates of such methods in general testing scenarios. Therefore, to address these limitations, we propose Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferable graph backdoor attack that employs graph prompt learning(GPL) to train a set of universal subgraph triggers. First, we distill a compact yet expressive trigger set from target graphs, which is structured as a queryable repository, by jointly enforcing class-awareness, feature richness, and structural fidelity. Second, we conduct the first exploration of the theoretical transferability of GPL to train these triggers under prompt-based objectives, enabling effective generalization to diverse and unseen test-time paradigms. Extensive experiments across multiple real-world datasets and defense scenarios show that CP-GBA achieves state-of-the-art attack success rates.</li>
</ul>

<h3>Title: SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size</h3>
<ul>
<li><strong>Authors: </strong>Jinhan Chen, Jianchun Liu, Hongli Xu, Xianjun Gao, Shilong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22556">https://arxiv.org/abs/2510.22556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22556">https://arxiv.org/pdf/2510.22556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22556]] SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size(https://arxiv.org/abs/2510.22556)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>The growing memory footprint of the Key-Value (KV) cache poses a severe scalability bottleneck for long-context Large Language Model (LLM) inference. While KV cache eviction has emerged as an effective solution by discarding less critical tokens, existing token-, block-, and sentence-level compression methods struggle to balance semantic coherence and memory efficiency. To this end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction framework with \underline{a}daptive \underline{block} sizes. Specifically, SABlock first performs semantic segmentation to align compression boundaries with linguistic structures, then applies segment-guided token scoring to refine token importance estimation. Finally, for each segment, a budget-driven search strategy adaptively determines the optimal block size that preserves semantic integrity while improving compression efficiency under a given cache budget. Extensive experiments on long-context benchmarks demonstrate that SABlock consistently outperforms state-of-the-art baselines under the same memory budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9% retrieval accuracy with only 96 KV entries, nearly matching the performance of the full-cache baseline that retains up to 8K entries. Under a fixed cache budget of 1,024, SABlock further reduces peak memory usage by 46.28% and achieves up to 9.5x faster decoding on a 128K context length.</li>
</ul>

<h3>Title: A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback</h3>
<ul>
<li><strong>Authors: </strong>Zhifeng Wang, Xinyue Zheng, Chunyan Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22559">https://arxiv.org/abs/2510.22559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22559">https://arxiv.org/pdf/2510.22559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22559]] A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback(https://arxiv.org/abs/2510.22559)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As information technology advances, education is moving from one-size-fits-all instruction toward personalized learning. However, most methods handle modeling, item selection, and feedback in isolation rather than as a closed loop. This leads to coarse or opaque student models, assumption-bound adaptivity that ignores diagnostic posteriors, and generic, non-actionable feedback. To address these limitations, this paper presents an end-to-end personalized learning agent, EduLoop-Agent, which integrates a Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation Computerized Adaptive Testing strategy (BECAT), and large language models (LLMs). The NCD module provides fine-grained estimates of students' mastery at the knowledge-point level; BECAT dynamically selects subsequent items to maximize relevance and learning efficiency; and LLMs convert diagnostic signals into structured, actionable feedback. Together, these components form a closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments on the ASSISTments dataset show that the NCD module achieves strong performance on response prediction while yielding interpretable mastery assessments. The adaptive recommendation strategy improves item relevance and personalization, and the LLM-based feedback offers targeted study guidance aligned with identified weaknesses. Overall, the results indicate that the proposed design is effective and practically deployable, providing a feasible pathway to generating individualized learning trajectories in intelligent education.</li>
</ul>

<h3>Title: Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study</h3>
<ul>
<li><strong>Authors: </strong>Kaveri Banerjee, Sajal Saha</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22561">https://arxiv.org/abs/2510.22561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22561">https://arxiv.org/pdf/2510.22561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22561]] Blockchain Signatures to Ensure Information Integrity and Non-Repudiation in the Digital Era: A comprehensive study(https://arxiv.org/abs/2510.22561)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Blockchain systems rely on decentralized ledgers and strong security guarantees. A key requirement is non-repudiation, which prevents denial of transaction authorship and supports integrity of recorded data. This work surveys digital signature schemes used in blockchain platforms and analyzes how they deliver non-repudiation and contribute to overall system security. We examine representative scheme families and their cryptographic foundations, security assumptions, and properties relevant to deployment, including unforgeability, resistance to malleability, support for aggregation and multisignature or threshold settings, key and signature sizes, and verification cost. Using these criteria, we compare the suitability of different designs for consensus protocols, smart contract constraints, and resource limits. We highlight practical tradeoffs that affect throughput, storage, scalability, and attack surfaces, and summarize benefits and limitations of each scheme in blockchain contexts. The study underscores that carefully chosen digital signatures are central to achieving non-repudiation and preserving information integrity, and it outlines implementation considerations and open directions such as interoperability and post-quantum readiness.</li>
</ul>

<h3>Title: FAARM: Firmware Attestation and Authentication Framework for Mali GPUs</h3>
<ul>
<li><strong>Authors: </strong>Md. Mehedi Hasan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22566">https://arxiv.org/abs/2510.22566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22566">https://arxiv.org/pdf/2510.22566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22566]] FAARM: Firmware Attestation and Authentication Framework for Mali GPUs(https://arxiv.org/abs/2510.22566)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Recent work has revealed MOLE, the first practical attack to compromise GPU Trusted Execution Environments (TEEs), by injecting malicious firmware into the embedded Microcontroller Unit (MCU) of Arm Mali GPUs. By exploiting the absence of cryptographic verification during initialization, adversaries with kernel privileges can bypass memory protections, exfiltrate sensitive data at over 40 MB/s, and tamper with inference results, all with negligible runtime overhead. This attack surface affects commodity mobile SoCs and cloud accelerators, exposing a critical firmware-level trust gap in existing GPU TEE designs. To address this gap, this paper presents FAARM, a lightweight Firmware Attestation and Authentication framework that prevents MOLE-style firmware subversion. FAARM integrates digital signature verification at the EL3 secure monitor using vendor-signed firmware bundles and an on-device public key anchor. At boot, EL3 verifies firmware integrity and authenticity, enforces version checks, and locks the firmware region, eliminating both pre-verification and time-of-check-to-time-of-use (TOCTOU) attack vectors. We implement FAARM as a software-only prototype on a Mali GPU testbed, using a Google Colab-based emulation framework that models the firmware signing process, the EL1 to EL3 load path, and secure memory configuration. FAARM reliably detects and blocks malicious firmware injections, rejecting tampered images before use and denying overwrite attempts after attestation. Firmware verification incurs only 1.34 ms latency on average, demonstrating that strong security can be achieved with negligible overhead. FAARM thus closes a fundamental gap in shim-based GPU TEEs, providing a practical, deployable defense that raises the security baseline for both mobile and cloud GPU deployments.</li>
</ul>

<h3>Title: Combining Deep Learning and Explainable AI for Toxicity Prediction of Chemical Compounds</h3>
<ul>
<li><strong>Authors: </strong>Eduard Popescu, Adrian Groza, Andreea Cernat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22572">https://arxiv.org/abs/2510.22572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22572">https://arxiv.org/pdf/2510.22572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22572]] Combining Deep Learning and Explainable AI for Toxicity Prediction of Chemical Compounds(https://arxiv.org/abs/2510.22572)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The task here is to predict the toxicological activity of chemical compounds based on the Tox21 dataset, a benchmark in computational toxicology. After a domain-specific overview of chemical toxicity, we discuss current computational strategies, focusing on machine learning and deep learning. Several architectures are compared in terms of performance, robustness, and interpretability. This research introduces a novel image-based pipeline based on DenseNet121, which processes 2D graphical representations of chemical structures. Additionally, we employ Grad-CAM visualizations, an explainable AI technique, to interpret the model's predictions and highlight molecular regions contributing to toxicity classification. The proposed architecture achieves competitive results compared to traditional models, demonstrating the potential of deep convolutional networks in cheminformatics. Our findings emphasize the value of combining image-based representations with explainable AI methods to improve both predictive accuracy and model transparency in toxicology.</li>
</ul>

<h3>Title: Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems</h3>
<ul>
<li><strong>Authors: </strong>Kaushal Kumar Maurya, Ekaterina Kochmar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22581">https://arxiv.org/abs/2510.22581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22581">https://arxiv.org/pdf/2510.22581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22581]] Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems(https://arxiv.org/abs/2510.22581)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>The interdisciplinary research domain of Artificial Intelligence in Education (AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by integrating insights from technological advancements, educational theories, and cognitive psychology. The remarkable success of generative AI (GenAI) models has accelerated the development of large language model (LLM)-powered ITSs, which have potential to imitate human-like, pedagogically rich, and cognitively demanding tutoring. However, the progress and impact of these systems remain largely untraceable due to the absence of reliable, universally accepted, and pedagogy-driven evaluation frameworks and benchmarks. Most existing educational dialogue-based ITS evaluations rely on subjective protocols and non-standardized benchmarks, leading to inconsistencies and limited generalizability. In this work, we take a step back from mainstream ITS development and provide comprehensive state-of-the-art evaluation practices, highlighting associated challenges through real-world case studies from careful and caring AIED research. Finally, building on insights from previous interdisciplinary AIED research, we propose three practical, feasible, and theoretically grounded research directions, rooted in learning science principles and aimed at establishing fair, unified, and scalable evaluation methodologies for ITSs.</li>
</ul>

<h3>Title: Cross-View UAV Geo-Localization with Precision-Focused Efficient Design: A Hierarchical Distillation Approach with Multi-view Refinement</h3>
<ul>
<li><strong>Authors: </strong>Jian Sun, Kangdao Liu, Chi Zhang, Chuangquan Chen, Junge Shen, Chi-Man Vong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22582">https://arxiv.org/abs/2510.22582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22582">https://arxiv.org/pdf/2510.22582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22582]] Cross-View UAV Geo-Localization with Precision-Focused Efficient Design: A Hierarchical Distillation Approach with Multi-view Refinement(https://arxiv.org/abs/2510.22582)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Cross-view geo-localization (CVGL) enables UAV localization by matching aerial images to geo-tagged satellite databases, which is critical for autonomous navigation in GNSS-denied environments. However, existing methods rely on resource-intensive fine-grained feature extraction and alignment, where multiple branches and modules significantly increase inference costs, limiting their deployment on edge devices. We propose Precision-Focused Efficient Design (PFED), a resource-efficient framework combining hierarchical knowledge transfer and multi-view representation refinement. This innovative method comprises two key components: 1) During training, Hierarchical Distillation paradigm for fast and accurate CVGL (HD-CVGL), coupled with Uncertainty-Aware Prediction Alignment (UAPA) to distill essential information and mitigate the data imbalance without incurring additional inference overhead. 2) During inference, an efficient Multi-view Refinement Module (MRM) leverages mutual information to filter redundant samples and effectively utilize the multi-view data. Extensive experiments show that PFED achieves state-of-the-art performance in both accuracy and efficiency, reaching 97.15\% Recall@1 on University-1652 while being over $5 \times$ more efficient in FLOPs and $3 \times$ faster than previous top methods. Furthermore, PFED runs at 251.5 FPS on the AGX Orin edge device, demonstrating its practical viability for real-time UAV applications. The project is available at this https URL</li>
</ul>

<h3>Title: PSScreen V2: Partially Supervised Multiple Retinal Disease Screening</h3>
<ul>
<li><strong>Authors: </strong>Boyi Zheng, Yalin Zheng, Hrvoje Bogunoviƒá, Qing Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22589">https://arxiv.org/abs/2510.22589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22589">https://arxiv.org/pdf/2510.22589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22589]] PSScreen V2: Partially Supervised Multiple Retinal Disease Screening(https://arxiv.org/abs/2510.22589)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we propose PSScreen V2, a partially supervised self-training framework for multiple retinal disease screening. Unlike previous methods that rely on fully labelled or single-domain datasets, PSScreen V2 is designed to learn from multiple partially labelled datasets with different distributions, addressing both label absence and domain shift challenges. To this end, PSScreen V2 adopts a three-branch architecture with one teacher and two student networks. The teacher branch generates pseudo labels from weakly augmented images to address missing labels, while the two student branches introduce novel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout), which enhances domain robustness by randomly discarding domain-related low-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which estimates uncertain domain variability via adversarially learned Gaussian perturbations of low-frequency statistics. Extensive experiments on multiple in-domain and out-of-domain fundus datasets demonstrate that PSScreen V2 achieves state-of-the-art performance and superior domain generalization ability. Furthermore, compatibility tests with diverse backbones, including the vision foundation model DINOv2, as well as evaluations on chest X-ray datasets, highlight the universality and adaptability of the proposed framework. The codes are available at this https URL.</li>
</ul>

<h3>Title: AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment</h3>
<ul>
<li><strong>Authors: </strong>Dario Loi, Elena Maria Mui√†, Federico Siciliano, Giovanni Trappolini, Vincenzo Cris√†, Peter Kruger, Fabrizio Silvestri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22593">https://arxiv.org/abs/2510.22593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22593">https://arxiv.org/pdf/2510.22593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22593]] AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment(https://arxiv.org/abs/2510.22593)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present AutoBench, a fully automated and self-sustaining framework for evaluating Large Language Models (LLMs) through reciprocal peer assessment. This paper provides a rigorous scientific validation of the AutoBench methodology, originally developed as an open-source project by eZecute S.R.L.. Unlike static benchmarks that suffer from test-set contamination and limited adaptability, AutoBench dynamically generates novel evaluation tasks while models alternately serve as question generators, contestants, and judges across diverse domains. An iterative weighting mechanism amplifies the influence of consistently reliable evaluators, aggregating peer judgments into consensus-based rankings that reflect collective model agreement. Our experiments demonstrate strong correlations with established benchmarks including MMLU-Pro and GPQA (respectively 78\% and 63\%), validating this peer-driven evaluation paradigm. The multi-judge design significantly outperforms single-judge baselines, confirming that distributed evaluation produces more robust and human-consistent assessments. AutoBench offers a scalable, contamination-resistant alternative to static benchmarks for the continuous evaluation of evolving language models.</li>
</ul>

<h3>Title: Projection Embedded Diffusion Bridge for CT Reconstruction from Incomplete Data</h3>
<ul>
<li><strong>Authors: </strong>Yuang Wang, Pengfei Jin, Siyeop Yoon, Matthew Tivnan, Shaoyang Zhang, Li Zhang, Quanzheng Li, Zhiqiang Chen, Dufan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22605">https://arxiv.org/abs/2510.22605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22605">https://arxiv.org/pdf/2510.22605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22605]] Projection Embedded Diffusion Bridge for CT Reconstruction from Incomplete Data(https://arxiv.org/abs/2510.22605)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reconstructing CT images from incomplete projection data remains challenging due to the ill-posed nature of the problem. Diffusion bridge models have recently shown promise in restoring clean images from their corresponding Filtered Back Projection (FBP) reconstructions, but incorporating data consistency into these models remains largely underexplored. Incorporating data consistency can improve reconstruction fidelity by aligning the reconstructed image with the observed projection data, and can enhance detail recovery by integrating structural information contained in the projections. In this work, we propose the Projection Embedded Diffusion Bridge (PEDB). PEDB introduces a novel reverse stochastic differential equation (SDE) to sample from the distribution of clean images conditioned on both the FBP reconstruction and the incomplete projection data. By explicitly conditioning on the projection data in sampling the clean images, PEDB naturally incorporates data consistency. We embed the projection data into the score function of the reverse SDE. Under certain assumptions, we derive a tractable expression for the posterior score. In addition, we introduce a free parameter to control the level of stochasticity in the reverse process. We also design a discretization scheme for the reverse SDE to mitigate discretization error. Extensive experiments demonstrate that PEDB achieves strong performance in CT reconstruction from three types of incomplete data, including sparse-view, limited-angle, and truncated projections. For each of these types, PEDB outperforms evaluated state-of-the-art diffusion bridge models across standard, noisy, and domain-shift evaluations.</li>
</ul>

<h3>Title: PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion</h3>
<ul>
<li><strong>Authors: </strong>Morteza Alikhani, Mohammadtaha Bagherifard, Erfan Zinvandi, Mehran Sarmadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22616">https://arxiv.org/abs/2510.22616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22616">https://arxiv.org/pdf/2510.22616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22616]] PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion(https://arxiv.org/abs/2510.22616)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale Persian benchmark for commonsense reasoning. PerCoR contains 106K multiple-choice sentence-completion problems drawn from more than forty news, cultural, and other web sources. We introduce a novel conjunction-based segmentation strategy to generate coherent sentence-completion pairs, enabling broad topical and structural diversity. To create challenging distractors, we propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and Adversarial Filtering), a generation-free adversarial filtering method that selects distractors from the pool of gold continuations while maximising model confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%). The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both the dataset's difficulty and the remaining performance gap in Persian commonsense reasoning. We further show that DRESS-AF transfers to the English HellaSwag benchmark, increasing its difficulty without hurting human solvability. The dataset is available at this https URL.</li>
</ul>

<h3>Title: Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Mackenzie Tapp, Sibi Chakravarthy Parivendan, Kashfia Sailunaz, Suresh Neethirajan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22618">https://arxiv.org/abs/2510.22618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22618">https://arxiv.org/pdf/2510.22618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22618]] Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation(https://arxiv.org/abs/2510.22618)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Pose estimation serves as a cornerstone of computer vision for understanding animal posture, behavior, and welfare. Yet, agricultural applications remain constrained by the scarcity of large, annotated datasets for livestock, especially dairy cattle. This study evaluates the potential and limitations of cross-species transfer learning by adapting ZebraPose - a vision transformer-based model trained on synthetic zebra imagery - for 27-keypoint detection in dairy cows under real barn conditions. Using three configurations - a custom on-farm dataset (375 images, Sussex, New Brunswick, Canada), a subset of the APT-36K benchmark dataset, and their combination, we systematically assessed model accuracy and generalization across environments. While the combined model achieved promising performance (AP = 0.86, AR = 0.87, PCK 0.5 = 0.869) on in-distribution data, substantial generalization failures occurred when applied to unseen barns and cow populations. These findings expose the synthetic-to-real domain gap as a major obstacle to agricultural AI deployment and emphasize that morphological similarity between species is insufficient for cross-domain transfer. The study provides practical insights into dataset diversity, environmental variability, and computational constraints that influence real-world deployment of livestock monitoring systems. We conclude with a call for agriculture-first AI design, prioritizing farm-level realism, cross-environment robustness, and open benchmark datasets to advance trustworthy and scalable animal-centric technologies.</li>
</ul>

<h3>Title: CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Songhan Zhang, Yuanhao Lai, Pengfei Zheng, Boxi Yu, Xiaoying Tang, Qiuai Fu, Pinjia He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22619">https://arxiv.org/abs/2510.22619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22619">https://arxiv.org/pdf/2510.22619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22619]] CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate Time Series(https://arxiv.org/abs/2510.22619)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multivariate time series (MTS) anomaly detection is essential for maintaining the reliability of industrial systems, yet real-world deployment is hindered by two critical challenges: training data contamination (noises and hidden anomalies) and inefficient model inference. Existing unsupervised methods assume clean training data, but contamination distorts learned patterns and degrades detection accuracy. Meanwhile, complex deep models often overfit to contamination and suffer from high latency, limiting practical use. To address these challenges, we propose CLEANet, a robust and efficient anomaly detection framework in contaminated multivariate time series. CLEANet introduces a Contamination-Resilient Training Framework (CRTF) that mitigates the impact of corrupted samples through an adaptive reconstruction weighting strategy combined with clustering-guided contrastive learning, thereby enhancing robustness. To further avoid overfitting on contaminated data and improve computational efficiency, we design a lightweight conjugate MLP that disentangles temporal and cross-feature dependencies. Across five public datasets, CLEANet achieves up to 73.04% higher F1 and 81.28% lower runtime compared with ten state-of-the-art baselines. Furthermore, integrating CRTF into three advanced models yields an average 5.35% F1 gain, confirming its strong generalizability.</li>
</ul>

<h3>Title: Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Julia Bazinska, Max Mathys, Francesco Casucci, Mateo Rojas-Carulla, Xander Davies, Alexandra Souly, Niklas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22620">https://arxiv.org/abs/2510.22620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22620">https://arxiv.org/pdf/2510.22620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22620]] Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents(https://arxiv.org/abs/2510.22620)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>AI agents powered by large language models (LLMs) are being deployed at scale, yet we lack a systematic understanding of how the choice of backbone LLM affects agent security. The non-deterministic sequential nature of AI agents complicates security modeling, while the integration of traditional software with AI components entangles novel LLM vulnerabilities with conventional security risks. Existing frameworks only partially address these challenges as they either capture specific vulnerabilities only or require modeling of complete agents. To address these limitations, we introduce threat snapshots: a framework that isolates specific states in an agent's execution flow where LLM vulnerabilities manifest, enabling the systematic identification and categorization of security risks that propagate from the LLM to the agent level. We apply this framework to construct the $\operatorname{b}^3$ benchmark, a security benchmark based on 194331 unique crowdsourced adversarial attacks. We then evaluate 31 popular LLMs with it, revealing, among other insights, that enhanced reasoning capabilities improve security, while model size does not correlate with security. We release our benchmark, dataset, and evaluation code to facilitate widespread adoption by LLM providers and practitioners, offering guidance for agent developers and incentivizing model developers to prioritize backbone security improvements.</li>
</ul>

<h3>Title: DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Kangran Zhao, Yupeng Chen, Xiaoyu Zhang, Yize Chen, Weinan Guan, Baicheng Chen, Chengzhe Sun, Soumyya Kanti Datta, Qingshan Liu, Siwei Lyu, Baoyuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22622">https://arxiv.org/abs/2510.22622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22622">https://arxiv.org/pdf/2510.22622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22622]] DeepfakeBench-MM: A Comprehensive Benchmark for Multimodal Deepfake Detection(https://arxiv.org/abs/2510.22622)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The misuse of advanced generative AI models has resulted in the widespread proliferation of falsified data, particularly forged human-centric audiovisual content, which poses substantial societal risks (e.g., financial fraud and social instability). In response to this growing threat, several works have preliminarily explored countermeasures. However, the lack of sufficient and diverse training data, along with the absence of a standardized benchmark, hinder deeper exploration. To address this challenge, we first build Mega-MMDF, a large-scale, diverse, and high-quality dataset for multimodal deepfake detection. Specifically, we employ 21 forgery pipelines through the combination of 10 audio forgery methods, 12 visual forgery methods, and 6 audio-driven face reenactment methods. Mega-MMDF currently contains 0.1 million real samples and 1.1 million forged samples, making it one of the largest and most diverse multimodal deepfake datasets, with plans for continuous expansion. Building on it, we present DeepfakeBench-MM, the first unified benchmark for multimodal deepfake detection. It establishes standardized protocols across the entire detection pipeline and serves as a versatile platform for evaluating existing methods as well as exploring novel approaches. DeepfakeBench-MM currently supports 5 datasets and 11 multimodal deepfake detectors. Furthermore, our comprehensive evaluations and in-depth analyses uncover several key findings from multiple perspectives (e.g., augmentation, stacked forgery). We believe that DeepfakeBench-MM, together with our large-scale Mega-MMDF, will serve as foundational infrastructures for advancing multimodal deepfake detection.</li>
</ul>

<h3>Title: Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks</h3>
<ul>
<li><strong>Authors: </strong>Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22628">https://arxiv.org/abs/2510.22628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22628">https://arxiv.org/pdf/2510.22628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22628]] Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks(https://arxiv.org/abs/2510.22628)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a real-time modular defense system named Sentra-Guard. The system detects and mitigates jailbreak and prompt injection attacks targeting large language models (LLMs). The framework uses a hybrid architecture with FAISS-indexed SBERT embedding representations that capture the semantic meaning of prompts, combined with fine-tuned transformer classifiers, which are machine learning models specialized for distinguishing between benign and adversarial language inputs. It identifies adversarial prompts in both direct and obfuscated attack vectors. A core innovation is the classifier-retriever fusion module, which dynamically computes context-aware risk scores that estimate how likely a prompt is to be adversarial based on its content and context. The framework ensures multilingual resilience with a language-agnostic preprocessing layer. This component automatically translates non-English prompts into English for semantic evaluation, enabling consistent detection across over 100 languages. The system includes a HITL feedback loop, where decisions made by the automated system are reviewed by human experts for continual learning and rapid adaptation under adversarial pressure. Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and malicious prompts, enhancing detection reliability and reducing false positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 = 1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible with diverse LLM backends. Its modular design supports scalable deployment in both commercial and open-source environments. The system establishes a new state-of-the-art in adversarial LLM defense.</li>
</ul>

<h3>Title: Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal</h3>
<ul>
<li><strong>Authors: </strong>Ambalika Guha, Sajal Saha, Debanjan Ballav, Soumi Mitra, Hritwick Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22629">https://arxiv.org/abs/2510.22629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22629">https://arxiv.org/pdf/2510.22629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22629]] Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal(https://arxiv.org/abs/2510.22629)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Preserving linguistic diversity is necessary as every language offers a distinct perspective on the world. There have been numerous global initiatives to preserve endangered languages through documentation. This paper is a part of a project which aims to develop a trilingual (Toto-Bangla-English) language learning application to digitally archive and promote the endangered Toto language of West Bengal, India. This application, designed for both native Toto speakers and non-native learners, aims to revitalize the language by ensuring accessibility and usability through Unicode script integration and a structured language corpus. The research includes detailed linguistic documentation collected via fieldwork, followed by the creation of a morpheme-tagged, trilingual corpus used to train a Small Language Model (SLM) and a Transformer-based translation engine. The analysis covers inflectional morphology such as person-number-gender agreement, tense-aspect-mood distinctions, and case marking, alongside derivational strategies that reflect word-class changes. Script standardization and digital literacy tools were also developed to enhance script usage. The study offers a sustainable model for preserving endangered languages by incorporating traditional linguistic methodology with AI. This bridge between linguistic research with technological innovation highlights the value of interdisciplinary collaboration for community-based language revitalization.</li>
</ul>

<h3>Title: Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware Augmentation and Hybrid Loss for Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Adinath Dukre, Ankan Deria, Yutong Xie, Imran Razzak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22630">https://arxiv.org/abs/2510.22630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22630">https://arxiv.org/pdf/2510.22630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22630]] Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware Augmentation and Hybrid Loss for Domain Generalization(https://arxiv.org/abs/2510.22630)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Atypical mitotic figures are important biomarkers of tumor aggressiveness in histopathology, yet reliable recognition remains challenging due to severe class imbalance and variability across imaging domains. We present a DenseNet-121-based framework tailored for atypical mitosis classification in the MIDOG 2025 (Track 2) setting. Our method integrates stain-aware augmentation (Macenko), geometric and intensity transformations, and imbalance-aware learning via weighted sampling with a hybrid objective combining class-weighted binary cross-entropy and focal loss. Trained end-to-end with AdamW and evaluated across multiple independent domains, the model demonstrates strong generalization under scanner and staining shifts, achieving balanced accuracy 85.0%, AUROC 0.927, sensitivity 89.2%, and specificity 80.9% on the official test set. These results indicate that combining DenseNet-121 with stain-aware augmentation and imbalance-adaptive objectives yields a robust, domain-generalizable framework for atypical mitosis classification suitable for real-world computational pathology workflows.</li>
</ul>

<h3>Title: Enhancing Graph Classification Robustness with Singular Pooling</h3>
<ul>
<li><strong>Authors: </strong>Sofiane Ennadir, Oleg Smirnov, Yassine Abbahaddou, Lele Cao, Johannes F. Lutzeyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22643">https://arxiv.org/abs/2510.22643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22643">https://arxiv.org/pdf/2510.22643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22643]] Enhancing Graph Classification Robustness with Singular Pooling(https://arxiv.org/abs/2510.22643)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved strong performance across a range of graph representation learning tasks, yet their adversarial robustness in graph classification remains underexplored compared to node classification. While most existing defenses focus on the message-passing component, this work investigates the overlooked role of pooling operations in shaping robustness. We present a theoretical analysis of standard flat pooling methods (sum, average and max), deriving upper bounds on their adversarial risk and identifying their vulnerabilities under different attack scenarios and graph structures. Motivated by these insights, we propose \textit{Robust Singular Pooling (RS-Pool)}, a novel pooling strategy that leverages the dominant singular vector of the node embedding matrix to construct a robust graph-level representation. We theoretically investigate the robustness of RS-Pool and interpret the resulting bound leading to improved understanding of our proposed pooling operator. While our analysis centers on Graph Convolutional Networks (GCNs), RS-Pool is model-agnostic and can be implemented efficiently via power iteration. Empirical results on real-world benchmarks show that RS-Pool provides better robustness than the considered pooling methods when subject to state-of-the-art adversarial attacks while maintaining competitive clean accuracy. Our code is publicly available at:\href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: A Critical Study on Tea Leaf Disease Detection using Deep Learning Techniques</h3>
<ul>
<li><strong>Authors: </strong>Nabajyoti Borah, Raju Moni Borah, Bandan Boruah, Purnendu Bikash Acharjee, Sajal Saha, Ripjyoti Hazarika</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22647">https://arxiv.org/abs/2510.22647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22647">https://arxiv.org/pdf/2510.22647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22647]] A Critical Study on Tea Leaf Disease Detection using Deep Learning Techniques(https://arxiv.org/abs/2510.22647)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The proposed solution is Deep Learning Technique that will be able classify three types of tea leaves diseases from which two diseases are caused by the pests and one due to pathogens (infectious organisms) and environmental conditions and also show the area damaged by a disease in leaves. Namely Red Rust, Helopeltis and Red spider mite respectively. In this paper we have evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%. While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95 and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than SSD. Also used Mask R-CNN for Object Instance Segmentation where we have implemented our custom method to calculate the damaged diseased portion of leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.</li>
</ul>

<h3>Title: Self-Attention Decomposition For Training Free Diffusion Editing</h3>
<ul>
<li><strong>Authors: </strong>Tharun Anand, Mohammad Hassan Vali, Arno Solin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22650">https://arxiv.org/abs/2510.22650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22650">https://arxiv.org/pdf/2510.22650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22650]] Self-Attention Decomposition For Training Free Diffusion Editing(https://arxiv.org/abs/2510.22650)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models achieve remarkable fidelity in image synthesis, yet precise control over their outputs for targeted editing remains challenging. A key step toward controllability is to identify interpretable directions in the model's latent representations that correspond to semantic attributes. Existing approaches for finding interpretable directions typically rely on sampling large sets of images or training auxiliary networks, which limits efficiency. We propose an analytical method that derives semantic editing directions directly from the pretrained parameters of diffusion models, requiring neither additional data nor fine-tuning. Our insight is that self-attention weight matrices encode rich structural information about the data distribution learned during training. By computing the eigenvectors of these weight matrices, we obtain robust and interpretable editing directions. Experiments demonstrate that our method produces high-quality edits across multiple datasets while reducing editing time significantly by 60% over current benchmarks.</li>
</ul>

<h3>Title: Variational Polya Tree</h3>
<ul>
<li><strong>Authors: </strong>Lu Xu, Tsai Hor Chan, Kwok Fai Lam, Lequan Yu, Guosheng Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22651">https://arxiv.org/abs/2510.22651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22651">https://arxiv.org/pdf/2510.22651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22651]] Variational Polya Tree(https://arxiv.org/abs/2510.22651)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Density estimation is essential for generative modeling, particularly with the rise of modern neural networks. While existing methods capture complex data distributions, they often lack interpretability and uncertainty quantification. Bayesian nonparametric methods, especially the \polya tree, offer a robust framework that addresses these issues by accurately capturing function behavior over small intervals. Traditional techniques like Markov chain Monte Carlo (MCMC) face high computational complexity and scalability limitations, hindering the use of Bayesian nonparametric methods in deep learning. To tackle this, we introduce the variational \polya tree (VPT) model, which employs stochastic variational inference to compute posterior distributions. This model provides a flexible, nonparametric Bayesian prior that captures latent densities and works well with stochastic gradient optimization. We also leverage the joint distribution likelihood for a more precise variational posterior approximation than traditional mean-field methods. We evaluate the model performance on both real data and images, and demonstrate its competitiveness with other state-of-the-art deep density estimation methods. We also explore its ability in enhancing interpretability and uncertainty quantification. Code is available at this https URL.</li>
</ul>

<h3>Title: If You Want to Be Robust, Be Wary of Initialization</h3>
<ul>
<li><strong>Authors: </strong>Sofiane Ennadir, Johannes F. Lutzeyer, Michalis Vazirgiannis, El Houcine Bergou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22652">https://arxiv.org/abs/2510.22652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22652">https://arxiv.org/pdf/2510.22652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22652]] If You Want to Be Robust, Be Wary of Initialization(https://arxiv.org/abs/2510.22652)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations. While prevailing defense strategies focus primarily on pre-processing techniques and adaptive message-passing schemes, this study delves into an under-explored dimension: the impact of weight initialization and associated hyper-parameters, such as training epochs, on a model's robustness. We introduce a theoretical framework bridging the connection between initialization strategies and a network's resilience to adversarial perturbations. Our analysis reveals a direct relationship between initial weights, number of training epochs and the model's vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms. While our primary focus is on GNNs, we extend our theoretical framework, providing a general upper-bound applicable to Deep Neural Networks. Extensive experiments, spanning diverse models and real-world datasets subjected to various adversarial attacks, validate our findings. We illustrate that selecting appropriate initialization not only ensures performance on clean datasets but also enhances model robustness against adversarial perturbations, with observed gaps of up to 50\% compared to alternative initialization approaches.</li>
</ul>

<h3>Title: Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion</h3>
<ul>
<li><strong>Authors: </strong>Zilong Wang, Qingtian Zeng, Hua Duan, Cheng Cheng, Minghao Zou, Ziyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22656">https://arxiv.org/abs/2510.22656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22656">https://arxiv.org/pdf/2510.22656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22656]] Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion(https://arxiv.org/abs/2510.22656)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Few-shot Knowledge Graph Completion (FKGC) infers missing triples from limited support samples, tackling long-tail distribution challenges. Existing methods, however, struggle to capture complex relational patterns and mitigate data sparsity. To address these challenges, we propose a novel FKGC framework for conjugate relation modeling (CR-FKGC). Specifically, it employs a neighborhood aggregation encoder to integrate higher-order neighbor information, a conjugate relation learner combining an implicit conditional diffusion relation module with a stable relation module to capture stable semantics and uncertainty offsets, and a manifold conjugate decoder for efficient evaluation and inference of missing triples in manifold space. Experiments on three benchmarks demonstrate that our method achieves superior performance over state-of-the-art methods.</li>
</ul>

<h3>Title: RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography</h3>
<ul>
<li><strong>Authors: </strong>Malik Imran, Safiullah Khan, Zain Ul Abideen, Ciara Rafferty, Ayesha Khalid, Muhammad Rashid, Maire O'Neill</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22661">https://arxiv.org/abs/2510.22661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22661">https://arxiv.org/pdf/2510.22661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22661]] RejSCore: Rejection Sampling Core for Multivariate-based Public key Cryptography(https://arxiv.org/abs/2510.22661)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Post-quantum multivariate public key cryptography (MPKC) schemes resist quantum threats but require heavy operations, such as rejection sampling, which challenge resource-limited devices. Prior hardware designs have addressed various aspects of MPKC signature generation. However, rejection sampling remains largely unexplored in such contexts. This paper presents RejSCore, a lightweight hardware accelerator for rejection sampling in post-quantum cryptography. It specifically targets the QR-UOV scheme, which is a prominent candidate under the second-round of the National Institute of Standards and Technology (NIST) additional digital signature standardization process. The architecture includes an AES-CTR-128-based pseudorandom number generator. Moreover, a lightweight iterative method is employed in rejection sampling, offering reduced resource consumption and area overhead while slightly increasing latency. The performance of RejSCore is comprehensively evaluated on Artix-7 FPGAs and 65 nm CMOS technology using the Area-Delay Product (ADP) and Power-Delay Product (PDP). On Artix-7 and 65 nm CMOS, RejSCore achieves an area of 2042 slices and 464,866~$\mu m^2$, with operating frequencies of 222 MHz and 565 MHz, respectively. Using the QR-UOV parameters for security level I ($q = 127$, $v = 156$, $m = 54$, $l = 3$), the core completes its operation in 8525 clock cycles. The ADP and PDP evaluations confirm RejSCore's suitability for deployment in resource-constrained and security-critical environments.</li>
</ul>

<h3>Title: SARCLIP: A Vision Language Foundation Model for Semantic Understanding and Target Recognition in SAR Imagery</h3>
<ul>
<li><strong>Authors: </strong>Qiwei Ma, Zhiyu Wang, Wang Liu, Xukun Lu, Bin Deng, Puhong Duan, Xudong Kang, Shutao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22665">https://arxiv.org/abs/2510.22665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22665">https://arxiv.org/pdf/2510.22665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22665]] SARCLIP: A Vision Language Foundation Model for Semantic Understanding and Target Recognition in SAR Imagery(https://arxiv.org/abs/2510.22665)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Synthetic Aperture Radar (SAR) has emerged as a crucial imaging modality due to its all-weather capabilities. While recent advancements in self-supervised learning and Masked Image Modeling (MIM) have paved the way for SAR foundation models, these approaches primarily focus on low-level visual features, often overlooking multimodal alignment and zero-shot target recognition within SAR imagery. To address this limitation, we construct SARCLIP-1M, a large-scale vision language dataset comprising over one million text-image pairs aggregated from existing datasets. We further introduce SARCLIP, the first vision language foundation model tailored for the SAR domain. Our SARCLIP model is trained using a contrastive vision language learning approach by domain transferring strategy, enabling it to bridge the gap between SAR imagery and textual descriptions. Extensive experiments on image-text retrieval and zero-shot classification tasks demonstrate the superior performance of SARCLIP in feature extraction and interpretation, significantly outperforming state-of-the-art foundation models and advancing the semantic understanding of SAR imagery. The code and datasets will be released soon.</li>
</ul>

<h3>Title: LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Zhu, Xu Li, Qimin Xu, Benwu Wang, Kun Wei, Yiming Peng, Zihang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22669">https://arxiv.org/abs/2510.22669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22669">https://arxiv.org/pdf/2510.22669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22669]] LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering(https://arxiv.org/abs/2510.22669)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting SLAM has emerged as a widely used technique for high-fidelity mapping in spatial intelligence. However, existing methods often rely on a single representation scheme, which limits their performance in large-scale dynamic outdoor scenes and leads to cumulative pose errors and scale ambiguity. To address these challenges, we propose \textbf{LVD-GS}, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human chain-of-thought process for information seeking, we introduce a hierarchical collaborative representation module that facilitates mutual reinforcement for mapping optimization, effectively mitigating scale drift and enhancing reconstruction robustness. Furthermore, to effectively eliminate the influence of dynamic objects, we propose a joint dynamic modeling module that generates fine-grained dynamic masks by fusing open-world segmentation with implicit residual constraints, guided by uncertainty estimates from DINO-Depth features. Extensive evaluations on KITTI, nuScenes, and self-collected datasets demonstrate that our approach achieves state-of-the-art performance compared to existing methods.</li>
</ul>

<h3>Title: Alias-Free ViT: Fractional Shift Invariance via Linear Attention</h3>
<ul>
<li><strong>Authors: </strong>Hagay Michaeli, Daniel Soudry</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22673">https://arxiv.org/abs/2510.22673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22673">https://arxiv.org/pdf/2510.22673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22673]] Alias-Free ViT: Fractional Shift Invariance via Linear Attention(https://arxiv.org/abs/2510.22673)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Transformers have emerged as a competitive alternative to convnets in vision tasks, yet they lack the architectural inductive bias of convnets, which may hinder their potential performance. Specifically, Vision Transformers (ViTs) are not translation-invariant and are more sensitive to minor image translations than standard convnets. Previous studies have shown, however, that convnets are also not perfectly shift-invariant, due to aliasing in downsampling and nonlinear layers. Consequently, anti-aliasing approaches have been proposed to certify convnets' translation robustness. Building on this line of work, we propose an Alias-Free ViT, which combines two main components. First, it uses alias-free downsampling and nonlinearities. Second, it uses linear cross-covariance attention that is shift-equivariant to both integer and fractional translations, enabling a shift-invariant global representation. Our model maintains competitive performance in image classification and outperforms similar-sized models in terms of robustness to adversarial translations.</li>
</ul>

<h3>Title: Estimation of Fireproof Structure Class and Construction Year for Disaster Risk Assessment</h3>
<ul>
<li><strong>Authors: </strong>Hibiki Ayabe, Kazushi Okamoto, Koki Karube, Atsushi Shibata, Kei Harada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22683">https://arxiv.org/abs/2510.22683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22683">https://arxiv.org/pdf/2510.22683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22683]] Estimation of Fireproof Structure Class and Construction Year for Disaster Risk Assessment(https://arxiv.org/abs/2510.22683)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Structural fireproof classification is vital for disaster risk assessment and insurance pricing in Japan. However, key building metadata such as construction year and structure type are often missing or outdated, particularly in the second-hand housing market. This study proposes a multi-task learning model that predicts these attributes from facade images. The model jointly estimates the construction year, building structure, and property type, from which the structural fireproof class - defined as H (non-fireproof), T (semi-fireproof), or M (fireproof) - is derived via a rule-based mapping based on official insurance criteria. We trained and evaluated the model using a large-scale dataset of Japanese residential images, applying rigorous filtering and deduplication. The model achieved high accuracy in construction-year regression and robust classification across imbalanced categories. Qualitative analyses show that it captures visual cues related to building age and materials. Our approach demonstrates the feasibility of scalable, interpretable, image-based risk-profiling systems, offering potential applications in insurance, urban planning, and disaster preparedness.</li>
</ul>

<h3>Title: FlowCritic: Bridging Value Estimation with Flow Matching in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shan Zhong, Shutong Ding, He Diao, Xiangyu Wang, Kah Chan Teh, Bei Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22686">https://arxiv.org/abs/2510.22686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22686">https://arxiv.org/pdf/2510.22686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22686]] FlowCritic: Bridging Value Estimation with Flow Matching in Reinforcement Learning(https://arxiv.org/abs/2510.22686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reliable value estimation serves as the cornerstone of reinforcement learning (RL) by evaluating long-term returns and guiding policy improvement, significantly influencing the convergence speed and final performance. Existing works improve the reliability of value function estimation via multi-critic ensembles and distributional RL, yet the former merely combines multi point estimation without capturing distributional information, whereas the latter relies on discretization or quantile regression, limiting the expressiveness of complex value distributions. Inspired by flow matching's success in generative modeling, we propose a generative paradigm for value estimation, named FlowCritic. Departing from conventional regression for deterministic value prediction, FlowCritic leverages flow matching to model value distributions and generate samples for value estimation.</li>
</ul>

<h3>Title: Rule-Based Explanations for Retrieval-Augmented LLM Systems</h3>
<ul>
<li><strong>Authors: </strong>Joel Rorseth, Parke Godfrey, Lukasz Golab, Divesh Srivastava, Jarek Szlichta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22689">https://arxiv.org/abs/2510.22689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22689">https://arxiv.org/pdf/2510.22689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22689]] Rule-Based Explanations for Retrieval-Augmented LLM Systems(https://arxiv.org/abs/2510.22689)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>If-then rules are widely used to explain machine learning models; e.g., "if employed = no, then loan application = rejected." We present the first proposal to apply rules to explain the emerging class of large language models (LLMs) with retrieval-augmented generation (RAG). Since RAG enables LLM systems to incorporate retrieved information sources at inference time, rules linking the presence or absence of sources can explain output provenance; e.g., "if a Times Higher Education ranking article is retrieved, then the LLM ranks Oxford first." To generate such rules, a brute force approach would probe the LLM with all source combinations and check if the presence or absence of any sources leads to the same output. We propose optimizations to speed up rule generation, inspired by Apriori-like pruning from frequent itemset mining but redefined within the scope of our novel problem. We conclude with qualitative and quantitative experiments demonstrating our solutions' value and efficiency.</li>
</ul>

<h3>Title: SALSA: Single-pass Autoregressive LLM Structured Classification</h3>
<ul>
<li><strong>Authors: </strong>Ruslan Berdichevsky, Shai Nahum-Gefen, Elad Ben Zaken</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22691">https://arxiv.org/abs/2510.22691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22691">https://arxiv.org/pdf/2510.22691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22691]] SALSA: Single-pass Autoregressive LLM Structured Classification(https://arxiv.org/abs/2510.22691)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite their impressive generalization capabilities, instruction-tuned Large Language Models often underperform on text classification benchmarks. We introduce SALSA, a coherent pipeline that combines structured prompting, class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding cold-start training. Each class label is mapped to a distinct output token, and prompts are constructed to elicit a single-token response. During inference, the model's output is projected only onto the logits of the relevant class tokens, enabling efficient and accurate classification in a single forward pass. SALSA achieves state-of-the-art results across diverse benchmarks, demonstrating its robustness and scalability for LLM-based classification applications.</li>
</ul>

<h3>Title: VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree</h3>
<ul>
<li><strong>Authors: </strong>Wenlong Li, Yifei Xu, Yuan Rao, Zhenhua Wang, Shuiguang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22693">https://arxiv.org/abs/2510.22693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22693">https://arxiv.org/pdf/2510.22693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22693]] VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree(https://arxiv.org/abs/2510.22693)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video anomaly detection (VAD) focuses on identifying anomalies in videos. Supervised methods demand substantial in-domain training data and fail to deliver clear explanations for anomalies. In contrast, training-free methods leverage the knowledge reserves and language interactivity of large pre-trained models to detect anomalies. However, the current fixed-length temporal window sampling approaches struggle to accurately capture anomalies with varying temporal spans. Therefore, we propose VADTree that utilizes a Hierarchical Granularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree leverages the knowledge embedded in a pre-trained Generic Event Boundary Detection (GEBD) model to characterize potential anomaly event boundaries. Specifically, VADTree decomposes the video into generic event nodes based on boundary confidence, and performs adaptive coarse-fine hierarchical structuring and redundancy removal to construct the HGTree. Then, the multi-dimensional priors are injected into the visual language models (VLMs) to enhance the node-wise anomaly perception, and anomaly reasoning for generic event nodes is achieved via large language models (LLMs). Finally, an inter-cluster node correlation method is used to integrate the multi-granularity anomaly scores. Extensive experiments on three challenging datasets demonstrate that VADTree achieves state-of-the-art performance in training-free settings while drastically reducing the number of sampled video segments. The code will be available at this https URL.</li>
</ul>

<h3>Title: Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Shu Zhao, Tianyi Shen, Nilesh Ahuja, Omesh Tickoo, Vijaykrishnan Narayanan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22694">https://arxiv.org/abs/2510.22694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22694">https://arxiv.org/pdf/2510.22694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22694]] Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation(https://arxiv.org/abs/2510.22694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising method to generate factual and up-to-date responses of Multimodal Large Language Models (MLLMs) by incorporating non-parametric knowledge from external knowledge bases. However, existing MRAG approaches suffer from static retrieval strategies, inflexible modality selection, and suboptimal utilization of retrieved information, leading to three critical challenges: determining when to retrieve, what modality to incorporate, and how to utilize retrieved information effectively. To address these challenges, we introduce Windsock, a query-dependent module making decisions on retrieval necessity and modality selection, effectively reducing computational overhead and improving response quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize retrieved information while maintaining robustness against noise. Moreover, we adopt a self-assessment approach leveraging knowledge within MLLMs to convert question-answering datasets to MRAG training datasets. Extensive experiments demonstrate that our proposed method significantly improves the generation quality by 17.07% while reducing 8.95% retrieval times.</li>
</ul>

<h3>Title: WaveMAE: Wavelet decomposition Masked Auto-Encoder for Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Vittorio Bernuzzi, Leonardo Rossi, Tomaso Fontanini, Massimo Bertozzi, Andrea Prati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22697">https://arxiv.org/abs/2510.22697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22697">https://arxiv.org/pdf/2510.22697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22697]] WaveMAE: Wavelet decomposition Masked Auto-Encoder for Remote Sensing(https://arxiv.org/abs/2510.22697)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, segmentation</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has recently emerged as a key strategy for building foundation models in remote sensing, where the scarcity of annotated data limits the applicability of fully supervised approaches. In this work, we introduce WaveMAE, a masked autoencoding framework tailored for multispectral satellite imagery. Unlike conventional pixel-based reconstruction, WaveMAE leverages a multi-level Discrete Wavelet Transform (DWT) to disentangle frequency components and guide the encoder toward learning scale-aware high-frequency representations. We further propose a Geo-conditioned Positional Encoding (GPE), which incorporates geographical priors via Spherical Harmonics, encouraging embeddings that respect both semantic and geospatial structure. To ensure fairness in evaluation, all methods are pretrained on the same dataset (fMoW-S2) and systematically evaluated on the diverse downstream tasks of the PANGAEA benchmark, spanning semantic segmentation, regression, change detection, and multilabel classification. Extensive experiments demonstrate that WaveMAE achieves consistent improvements over prior state-of-the-art approaches, with substantial gains on segmentation and regression benchmarks. The effectiveness of WaveMAE pretraining is further demonstrated by showing that even a lightweight variant, containing only 26.4% of the parameters, achieves state-of-the-art performance. Our results establish WaveMAE as a strong and geographically informed foundation model for multispectral remote sensing imagery.</li>
</ul>

<h3>Title: IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22706">https://arxiv.org/abs/2510.22706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22706">https://arxiv.org/pdf/2510.22706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22706]] IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction(https://arxiv.org/abs/2510.22706)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline.</li>
</ul>

<h3>Title: LRW-Persian: Lip-reading in the Wild Dataset for Persian Language</h3>
<ul>
<li><strong>Authors: </strong>Zahra Taghizadeh, Mohammad Shahverdikondori, Arian Noori, Alireza Dadgarnia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22716">https://arxiv.org/abs/2510.22716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22716">https://arxiv.org/pdf/2510.22716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22716]] LRW-Persian: Lip-reading in the Wild Dataset for Persian Language(https://arxiv.org/abs/2510.22716)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lipreading has emerged as an increasingly important research area for developing robust speech recognition systems and assistive technologies for the hearing-impaired. However, non-English resources for visual speech recognition remain limited. We introduce LRW-Persian, the largest in-the-wild Persian word-level lipreading dataset, comprising $743$ target words and over $414{,}000$ video samples extracted from more than $1{,}900$ hours of footage across $67$ television programs. Designed as a benchmark-ready resource, LRW-Persian provides speaker-disjoint training and test splits, wide regional and dialectal coverage, and rich per-clip metadata including head pose, age, and gender. To ensure large-scale data quality, we establish a fully automated end-to-end curation pipeline encompassing transcription based on Automatic Speech Recognition(ASR), active-speaker localization, quality filtering, and pose/mask screening. We further fine-tune two widely used lipreading architectures on LRW-Persian, establishing reference performance and demonstrating the difficulty of Persian visual speech recognition. By filling a critical gap in low-resource languages, LRW-Persian enables rigorous benchmarking, supports cross-lingual transfer, and provides a foundation for advancing multimodal speech research in underrepresented linguistic contexts. The dataset is publicly available at: this https URL.</li>
</ul>

<h3>Title: SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Van Le, Tan Le</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22726">https://arxiv.org/abs/2510.22726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22726">https://arxiv.org/pdf/2510.22726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22726]] SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking(https://arxiv.org/abs/2510.22726)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>SpoofTrackBench is a reproducible, modular benchmark for evaluating adversarial robustness in real-time localization and tracking (RTLS) systems under radar spoofing. Leveraging the Hampton University Skyler Radar Sensor dataset, we simulate drift, ghost, and mirror-type spoofing attacks and evaluate tracker performance using both Joint Probabilistic Data Association (JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separates clean and spoofed detection streams, visualizes spoof-induced trajectory divergence, and quantifies assignment errors via direct drift-from-truth metrics. Clustering overlays, injection-aware timelines, and scenario-adaptive visualizations enable interpretability across spoof types and configurations. Evaluation figures and logs are auto-exported for reproducible comparison. SpoofTrackBench sets a new standard for open, ethical benchmarking of spoof-aware tracking pipelines, enabling rigorous cross-architecture analysis and community validation.</li>
</ul>

<h3>Title: S-Chain: Structured Visual Chain-of-Thought For Medicine</h3>
<ul>
<li><strong>Authors: </strong>Khai Le-Duc, Duy M. H. Nguyen, Phuong T. H. Trinh, Tien-Phat Nguyen, Nghiem T. Diep, An Ngo, Tung Vu, Trinh Vuong, Anh-Tien Nguyen, Mau Nguyen, Van Trung Hoang, Khai-Nguyen Nguyen, Hy Nguyen, Chris Ngo, Anji Liu, Nhat Ho, Anne-Christin Hauschild, Khanh Xuan Nguyen, Thanh Nguyen-Tang, Pengtao Xie, Daniel Sonntag, James Zou, Mathias Niepert, Anh Totti Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22728">https://arxiv.org/abs/2510.22728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22728">https://arxiv.org/pdf/2510.22728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22728]] S-Chain: Structured Visual Chain-of-Thought For Medicine(https://arxiv.org/abs/2510.22728)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Faithful reasoning in medical vision-language models (VLMs) requires not only accurate predictions but also transparent alignment between textual rationales and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise in medical visual question answering (VQA), no large-scale expert-level dataset has captured stepwise reasoning with precise visual grounding. We introduce S-Chain, the first large-scale dataset of 12,000 expert-annotated medical images with bounding boxes and structured visual CoT (SV-CoT), explicitly linking visual regions to reasoning steps. The dataset further supports 16 languages, totaling over 700k VQA pairs for broad multilingual applicability. Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med, LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that SV-CoT supervision significantly improves interpretability, grounding fidelity, and robustness. Beyond benchmarking, we study its synergy with retrieval-augmented generation, revealing how domain knowledge and visual grounding interact during autoregressive reasoning. Finally, we propose a new mechanism that strengthens the alignment between visual evidence and reasoning, improving both reliability and efficiency. S-Chain establishes a new benchmark for grounded medical reasoning and paves the way toward more trustworthy and explainable medical VLMs.</li>
</ul>

<h3>Title: Cross-view Localization and Synthesis - Datasets, Challenges and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Ningli Xu, Rongjun Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22736">https://arxiv.org/abs/2510.22736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22736">https://arxiv.org/pdf/2510.22736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22736]] Cross-view Localization and Synthesis - Datasets, Challenges and Opportunities(https://arxiv.org/abs/2510.22736)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Cross-view localization and synthesis are two fundamental tasks in cross-view visual understanding, which deals with cross-view datasets: overhead (satellite or aerial) and ground-level imagery. These tasks have gained increasing attention due to their broad applications in autonomous navigation, urban planning, and augmented reality. Cross-view localization aims to estimate the geographic position of ground-level images based on information provided by overhead imagery while cross-view synthesis seeks to generate ground-level images based on information from the overhead imagery. Both tasks remain challenging due to significant differences in viewing perspective, resolution, and occlusion, which are widely embedded in cross-view datasets. Recent years have witnessed rapid progress driven by the availability of large-scale datasets and novel approaches. Typically, cross-view localization is formulated as an image retrieval problem where ground-level features are matched with tiled overhead images feature, extracted by convolutional neural networks (CNNs) or vision transformers (ViTs) for cross-view feature embedding. Cross-view synthesis, on the other hand, seeks to generate ground-level views based on information from overhead imagery, generally using generative adversarial networks (GANs) or diffusion models. This paper presents a comprehensive survey of advances in cross-view localization and synthesis, reviewing widely used datasets, highlighting key challenges, and providing an organized overview of state-of-the-art techniques. Furthermore, it discusses current limitations, offers comparative analyses, and outlines promising directions for future research. We also include the project page via this https URL.</li>
</ul>

<h3>Title: ConMatFormer: A Multi-attention and Transformer Integrated ConvNext based Deep Learning Model for Enhanced Diabetic Foot Ulcer Classification</h3>
<ul>
<li><strong>Authors: </strong>Raihan Ahamed Rifat, Fuyad Hasan Bhoyan, Md Humaion Kabir Mehedi, Md Kaviul Hossain, Md. Jakir Hossen, M. F. Mridha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22743">https://arxiv.org/abs/2510.22743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22743">https://arxiv.org/pdf/2510.22743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22743]] ConMatFormer: A Multi-attention and Transformer Integrated ConvNext based Deep Learning Model for Enhanced Diabetic Foot Ulcer Classification(https://arxiv.org/abs/2510.22743)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Diabetic foot ulcer (DFU) detection is a clinically significant yet challenging task due to the scarcity and variability of publicly available datasets. To solve these problems, we propose ConMatFormer, a new hybrid deep learning architecture that combines ConvNeXt blocks, multiple attention mechanisms convolutional block attention module (CBAM) and dual attention network (DANet), and transformer modules in a way that works together. This design facilitates the extraction of better local features and understanding of the global context, which allows us to model small skin patterns across different types of DFU very accurately. To address the class imbalance, we used data augmentation methods. A ConvNeXt block was used to obtain detailed local features in the initial stages. Subsequently, we compiled the model by adding a transformer module to enhance long-range dependency. This enabled us to pinpoint the DFU classes that were underrepresented or constituted minorities. Tests on the DS1 (DFUC2021) and DS2 (diabetic foot ulcer (DFU)) datasets showed that ConMatFormer outperformed state-of-the-art (SOTA) convolutional neural network (CNN) and Vision Transformer (ViT) models in terms of accuracy, reliability, and flexibility. The proposed method achieved an accuracy of 0.8961 and a precision of 0.9160 in a single experiment, which is a significant improvement over the current standards for classifying DFUs. In addition, by 4-fold cross-validation, the proposed model achieved an accuracy of 0.9755 with a standard deviation of only 0.0031. We further applied explainable artificial intelligence (XAI) methods, such as Grad-CAM, Grad-CAM++, and LIME, to consistently monitor the transparency and trustworthiness of the decision-making process.. Our findings set a new benchmark for DFU classification and provide a hybrid attention transformer framework for medical image analysis.</li>
</ul>

<h3>Title: Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study</h3>
<ul>
<li><strong>Authors: </strong>Eeham Khan, Firas Saidani, Owen Van Esbroeck, Richard Khoury, Leila Kosseim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22747">https://arxiv.org/abs/2510.22747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22747">https://arxiv.org/pdf/2510.22747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22747]] Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study(https://arxiv.org/abs/2510.22747)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the widespread adoption of large language models (LLMs), their strongest capabilities remain largely confined to a small number of high-resource languages for which there is abundant training data. Recently, continual pre-training (CPT) has emerged as a means to fine-tune these models to low-resource regional dialects. In this paper, we study the use of CPT for dialect learning under tight data and compute budgets. Using low-rank adaptation (LoRA) and compute-efficient continual pre-training, we adapt three LLMs to the Qu√©bec French dialect using a very small dataset and benchmark them on the COLE suite. Our experiments demonstrate an improvement on the minority dialect benchmarks with minimal regression on the prestige language benchmarks with under 1% of model parameters updated. Analysis of the results demonstrate that gains are highly contingent on corpus composition. These findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can narrow the dialect gap by providing cost-effective and sustainable language resource creation, expanding high-quality LLM access to minority linguistic communities. We release the first Qu√©bec French LLMs on HuggingFace.</li>
</ul>

<h3>Title: Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models</h3>
<ul>
<li><strong>Authors: </strong>Anooshka Bajaj, Deven Mahesh Mistry, Sahaj Singh Maini, Yash Aggarwal, Zoran Tiganj</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22752">https://arxiv.org/abs/2510.22752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22752">https://arxiv.org/pdf/2510.22752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22752]] Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models(https://arxiv.org/abs/2510.22752)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In-context learning is governed by both temporal and semantic relationships, shaping how Large Language Models (LLMs) retrieve contextual information. Analogous to human episodic memory, where the retrieval of specific events is enabled by separating events that happened at different times, this work probes the ability of various pretrained LLMs, including transformer and state-space models, to differentiate and retrieve temporally separated events. Specifically, we prompted models with sequences containing multiple presentations of the same token, which reappears at the sequence end. By fixing the positions of these repeated tokens and permuting all others, we removed semantic confounds and isolated temporal effects on next-token prediction. Across diverse sequences, models consistently placed the highest probabilities on tokens following a repeated token, but with a notable bias for those nearest the beginning or end of the input. An ablation experiment linked this phenomenon in transformers to induction heads. Extending the analysis to unique semantic contexts with partial overlap further demonstrated that memories embedded in the middle of a prompt are retrieved less reliably. Despite architectural differences, state-space and transformer models showed comparable temporal biases. Our findings deepen the understanding of temporal biases in in-context learning and offer an illustration of how these biases can enable temporal separation and episodic retrieval.</li>
</ul>

<h3>Title: Distributionally Robust Optimization via Diffusion Ambiguity Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wen, Jianyi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22757">https://arxiv.org/abs/2510.22757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22757">https://arxiv.org/pdf/2510.22757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22757]] Distributionally Robust Optimization via Diffusion Ambiguity Modeling(https://arxiv.org/abs/2510.22757)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent with the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose a diffusion-based ambiguity set design that captures various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this ambiguity modeling, we propose Diffusion-based DRO (D-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized diffusion model space. We formally establish the stationary convergence performance of D-DRO and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in a ML prediction task.</li>
</ul>

<h3>Title: Iterative Layer Pruning for Efficient Translation Inference</h3>
<ul>
<li><strong>Authors: </strong>Yasmin Moslem, Muhammad Hazim Al Farouq, John D. Kelleher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22763">https://arxiv.org/abs/2510.22763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22763">https://arxiv.org/pdf/2510.22763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22763]] Iterative Layer Pruning for Efficient Translation Inference(https://arxiv.org/abs/2510.22763)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have transformed many areas of natural language processing, including machine translation. However, efficient deployment of LLMs remains challenging due to their intensive computational requirements. In this paper, we address this challenge and present our submissions to the Model Compression track at the Conference on Machine Translation (WMT 2025). In our experiments, we investigate iterative layer pruning guided by layer importance analysis. We evaluate this method using the Aya-Expanse-8B model for translation from Czech to German, and from English to Egyptian Arabic. Our approach achieves substantial reductions in model size and inference time, while maintaining the translation quality of the baseline models.</li>
</ul>

<h3>Title: TELL-TALE: Task Efficient LLMs with Task Aware Layer Elimination</h3>
<ul>
<li><strong>Authors: </strong>Omar Naim, Krish Sharma, Nicholas Asher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22767">https://arxiv.org/abs/2510.22767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22767">https://arxiv.org/pdf/2510.22767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22767]] TELL-TALE: Task Efficient LLMs with Task Aware Layer Elimination(https://arxiv.org/abs/2510.22767)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>In this paper we introduce Tale, Task-Aware Layer Elimination, an inference-time algorithm that prunes entire transformer layers in an LLM by directly optimizing task-specific validation performance. We evaluate TALE on 9 tasks and 5 models, including LLaMA 3.1 8B, Qwen 2.5 7B, Qwen 2.5 0.5B, Mistral 7B, and Lucie 7B, under both zero-shot and few-shot settings. Unlike prior approaches, TALE requires no retraining and consistently improves accuracy while reducing computational cost across all benchmarks. Furthermore, applying TALE during finetuning leads to additional performance gains. Finally, TALE provides flexible user control over trade-offs between accuracy and efficiency. Mutual information analysis shows that certain layers act as bottlenecks, degrading task-relevant representations. Tale's selective layer removal remedies this problem, producing smaller, faster, and more accurate models that are also faster to fine-tune while offering new insights into transformer interpretability.</li>
</ul>

<h3>Title: MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion</h3>
<ul>
<li><strong>Authors: </strong>Haoyi Qiu, Yilun Zhou, Pranav Narayanan Venkit, Kung-Hsiang Huang, Jiaxin Zhang, Nanyun Peng, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22768">https://arxiv.org/abs/2510.22768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22768">https://arxiv.org/pdf/2510.22768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22768]] MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion(https://arxiv.org/abs/2510.22768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As Large Vision-Language Models (LVLMs) are increasingly deployed in domains such as shopping, health, and news, they are exposed to pervasive persuasive content. A critical question is how these models function as persuadees-how and why they can be influenced by persuasive multimodal inputs. Understanding both their susceptibility to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, override user preferences, or generate unethical or unsafe outputs when exposed to manipulative messages. We introduce MMPersuade, a unified framework for systematically studying multimodal persuasion dynamics in LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs images and videos with established persuasion principles across commercial, subjective and behavioral, and adversarial contexts, and (ii) an evaluation framework that quantifies both persuasion effectiveness and model susceptibility via third-party agreement scoring and self-estimated token probabilities on conversation histories. Our study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs substantially increase persuasion effectiveness-and model susceptibility-compared to text alone, especially in misinformation scenarios; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across contexts, with reciprocity being most potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts. By jointly analyzing persuasion effectiveness and susceptibility, MMPersuade provides a principled foundation for developing models that are robust, preference-consistent, and ethically aligned when engaging with persuasive multimodal content.</li>
</ul>

<h3>Title: Scalable Supervising Software Agents with Patch Reasoner</h3>
<ul>
<li><strong>Authors: </strong>Junjielong Xu, Boyin Tan, Xiaoyuan Liu, Chao Peng, Pengfei Gao, Pinjia He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22775">https://arxiv.org/abs/2510.22775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22775">https://arxiv.org/pdf/2510.22775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22775]] Scalable Supervising Software Agents with Patch Reasoner(https://arxiv.org/abs/2510.22775)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language model agents have advanced software engineering tasks, the unscalable nature of existing test-based supervision is limiting the potential improvement of data scaling. The reason is twofold: (1) building and running test sandbox is rather heavy and fragile, and (2) data with high-coverage tests is naturally rare and threatened by test hacking via edge cases. In this paper, we propose R4P, a patch verifier model to provide scalable rewards for training and testing SWE agents via reasoning. We consider that patch verification is fundamentally a reasoning task, mirroring how human repository maintainers review patches without writing and running new reproduction tests. To obtain sufficient reference and reduce the risk of reward hacking, R4P uses a group-wise objective for RL training, enabling it to verify multiple patches against each other's modification and gain a dense reward for stable training. R4P achieves 72.2% Acc. for verifying patches from SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we design and train a lite scaffold, Mini-SE, with pure reinforcement learning where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2% Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original Qwen3-32B. This can be further improved to 32.8% with R4P for test-time scaling. Furthermore, R4P verifies patches within a second, 50x faster than testing on average. The stable scaling curves of rewards and accuracy along with high efficiency reflect R4P's practicality.</li>
</ul>

<h3>Title: SeeDNorm: Self-Rescaled Dynamic Normalization</h3>
<ul>
<li><strong>Authors: </strong>Wenrui Cai, Defa Zhu, Qingjie Liu, Qiyang Min</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22777">https://arxiv.org/abs/2510.22777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22777">https://arxiv.org/pdf/2510.22777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22777]] SeeDNorm: Self-Rescaled Dynamic Normalization(https://arxiv.org/abs/2510.22777)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Normalization layer constitutes an essential component in neural networks. In transformers, the predominantly used RMSNorm constrains vectors to a unit hypersphere, followed by dimension-wise rescaling through a learnable scaling coefficient $\gamma$ to maintain the representational capacity of the model. However, RMSNorm discards the input norm information in forward pass and a static scaling factor $\gamma$ may be insufficient to accommodate the wide variability of input data and distributional shifts, thereby limiting further performance improvements, particularly in zero-shot scenarios that large language models routinely encounter. To address this limitation, we propose SeeDNorm, which enhances the representational capability of the model by dynamically adjusting the scaling coefficient based on the current input, thereby preserving the input norm information and enabling data-dependent, self-rescaled dynamic normalization. During backpropagation, SeeDNorm retains the ability of RMSNorm to dynamically adjust gradient according to the input norm. We provide a detailed analysis of the training optimization for SeedNorm and proposed corresponding solutions to address potential instability issues that may arise when applying SeeDNorm. We validate the effectiveness of SeeDNorm across models of varying sizes in large language model pre-training as well as supervised and unsupervised computer vision tasks. By introducing a minimal number of parameters and with neglligible impact on model efficiency, SeeDNorm achieves consistently superior performance compared to previously commonly used normalization layers such as RMSNorm and LayerNorm, as well as element-wise activation alternatives to normalization layers like DyT.</li>
</ul>

<h3>Title: Self-Calibrated Consistency can Fight Back for Adversarial Robustness in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Liu, Jiawei Du, Xiao Liu, Prayag Tiwari, Mingkun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22785">https://arxiv.org/abs/2510.22785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22785">https://arxiv.org/pdf/2510.22785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22785]] Self-Calibrated Consistency can Fight Back for Adversarial Robustness in Vision-Language Models(https://arxiv.org/abs/2510.22785)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Pre-trained vision-language models (VLMs) such as CLIP have demonstrated strong zero-shot capabilities across diverse domains, yet remain highly vulnerable to adversarial perturbations that disrupt image-text alignment and compromise reliability. Existing defenses typically rely on adversarial fine-tuning with labeled data, limiting their applicability in zero-shot settings. In this work, we identify two key weaknesses of current CLIP adversarial attacks -- lack of semantic guidance and vulnerability to view variations -- collectively termed semantic and viewpoint fragility. To address these challenges, we propose Self-Calibrated Consistency (SCC), an effective test-time defense. SCC consists of two complementary modules: Semantic consistency, which leverages soft pseudo-labels from counterattack warm-up and multi-view predictions to regularize cross-modal alignment and separate the target embedding from confusable negatives; and Spatial consistency, aligning perturbed visual predictions via augmented views to stabilize inference under adversarial perturbations. Together, these modules form a plug-and-play inference strategy. Extensive experiments on 22 benchmarks under diverse attack settings show that SCC consistently improves the zero-shot robustness of CLIP while maintaining accuracy, and can be seamlessly integrated with other VLMs for further gains. These findings highlight the great potential of establishing an adversarially robust paradigm from CLIP, with implications extending to broader vision-language domains such as BioMedCLIP.</li>
</ul>

<h3>Title: VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions</h3>
<ul>
<li><strong>Authors: </strong>Thu Phuong Nguyen, Duc M. Nguyen, Hyotaek Jeon, Hyunwook Lee, Hyunmin Song, Sungahn Ko, Taehwan Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22798">https://arxiv.org/abs/2510.22798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22798">https://arxiv.org/pdf/2510.22798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22798]] VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions(https://arxiv.org/abs/2510.22798)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Automatically assessing handwritten mathematical solutions is an important problem in educational technology with practical applications, but it remains a significant challenge due to the diverse formats, unstructured layouts, and symbolic complexity of student work. To address this challenge, we introduce VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics Expressions-designed to assess open-form handwritten math responses with high accuracy and interpretable reasoning traces. VEHME integrates a two-phase training pipeline: (i) supervised fine-tuning using structured reasoning data, and (ii) reinforcement learning that aligns model outputs with multi-dimensional grading objectives, including correctness, reasoning depth, and error localization. To enhance spatial understanding, we propose an Expression-Aware Visual Prompting Module, trained on our synthesized multi-line math expressions dataset to robustly guide attention in visually heterogeneous inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art performance among open-source models and approaches the accuracy of proprietary systems, demonstrating its potential as a scalable and accessible tool for automated math assessment. Our training and experiment code is publicly available at our GitHub repository.</li>
</ul>

<h3>Title: MedXplain-VQA: Multi-Component Explainable Medical Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Hai-Dang Nguyen, Minh-Anh Dang, Minh-Tan Le, Minh-Tuan Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22803">https://arxiv.org/abs/2510.22803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22803">https://arxiv.org/pdf/2510.22803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22803]] MedXplain-VQA: Multi-Component Explainable Medical Visual Question Answering(https://arxiv.org/abs/2510.22803)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, explainability</a></li>
<li><strong>Abstract: </strong>Explainability is critical for the clinical adoption of medical visual question answering (VQA) systems, as physicians require transparent reasoning to trust AI-generated diagnoses. We present MedXplain-VQA, a comprehensive framework integrating five explainable AI components to deliver interpretable medical image analysis. The framework leverages a fine-tuned BLIP-2 backbone, medical query reformulation, enhanced Grad-CAM attention, precise region extraction, and structured chain-of-thought reasoning via multi-modal language models. To evaluate the system, we introduce a medical-domain-specific framework replacing traditional NLP metrics with clinically relevant assessments, including terminology coverage, clinical structure quality, and attention region relevance. Experiments on 500 PathVQA histopathology samples demonstrate substantial improvements, with the enhanced system achieving a composite score of 0.683 compared to 0.378 for baseline methods, while maintaining high reasoning confidence (0.890). Our system identifies 3-5 diagnostically relevant regions per sample and generates structured explanations averaging 57 words with appropriate clinical terminology. Ablation studies reveal that query reformulation provides the most significant initial improvement, while chain-of-thought reasoning enables systematic diagnostic processes. These findings underscore the potential of MedXplain-VQA as a robust, explainable medical VQA system. Future work will focus on validation with medical experts and large-scale clinical datasets to ensure clinical readiness.</li>
</ul>

<h3>Title: A Theory of the Mechanics of Information: Generalization Through Measurement of Uncertainty (Learning is Measuring)</h3>
<ul>
<li><strong>Authors: </strong>Christopher J. Hazard, Michael Resnick, Jacob Beel, Jack Xia, Cade Mack, Dominic Glennie, Matthew Fulp, David Maze, Andrew Bassett, Martin Koistinen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22809">https://arxiv.org/abs/2510.22809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22809">https://arxiv.org/pdf/2510.22809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22809]] A Theory of the Mechanics of Information: Generalization Through Measurement of Uncertainty (Learning is Measuring)(https://arxiv.org/abs/2510.22809)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Traditional machine learning relies on explicit models and domain assumptions, limiting flexibility and interpretability. We introduce a model-free framework using surprisal (information theoretic uncertainty) to directly analyze and perform inferences from raw data, eliminating distribution modeling, reducing bias, and enabling efficient updates including direct edits and deletion of training data. By quantifying relevance through uncertainty, the approach enables generalizable inference across tasks including generative inference, causal discovery, anomaly detection, and time series forecasting. It emphasizes traceability, interpretability, and data-driven decision making, offering a unified, human-understandable framework for machine learning, and achieves at or near state-of-the-art performance across most common machine learning tasks. The mathematical foundations create a ``physics'' of information, which enable these techniques to apply effectively to a wide variety of complex data types, including missing data. Empirical results indicate that this may be a viable alternative path to neural networks with regard to scalable machine learning and artificial intelligence that can maintain human understandability of the underlying mechanics.</li>
</ul>

<h3>Title: MAGIC-Talk: Motion-aware Audio-Driven Talking Face Generation with Customizable Identity Control</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Nazarieh, Zhenhua Feng, Diptesh Kanojia, Muhammad Awais, Josef Kittler</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22810">https://arxiv.org/abs/2510.22810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22810">https://arxiv.org/pdf/2510.22810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22810]] MAGIC-Talk: Motion-aware Audio-Driven Talking Face Generation with Customizable Identity Control(https://arxiv.org/abs/2510.22810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Audio-driven talking face generation has gained significant attention for applications in digital media and virtual avatars. While recent methods improve audio-lip synchronization, they often struggle with temporal consistency, identity preservation, and customization, especially in long video generation. To address these issues, we propose MAGIC-Talk, a one-shot diffusion-based framework for customizable and temporally stable talking face generation. MAGIC-Talk consists of ReferenceNet, which preserves identity and enables fine-grained facial editing via text prompts, and AnimateNet, which enhances motion coherence using structured motion priors. Unlike previous methods requiring multiple reference images or fine-tuning, MAGIC-Talk maintains identity from a single image while ensuring smooth transitions across frames. Additionally, a progressive latent fusion strategy is introduced to improve long-form video quality by reducing motion inconsistencies and flickering. Extensive experiments demonstrate that MAGIC-Talk outperforms state-of-the-art methods in visual quality, identity preservation, and synchronization accuracy, offering a robust solution for talking face generation.</li>
</ul>

<h3>Title: Air Quality Prediction Using LOESS-ARIMA and Multi-Scale CNN-BiLSTM with Residual-Gated Attention</h3>
<ul>
<li><strong>Authors: </strong>Soham Pahari, Sandeep Chand Kumain</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22818">https://arxiv.org/abs/2510.22818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22818">https://arxiv.org/pdf/2510.22818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22818]] Air Quality Prediction Using LOESS-ARIMA and Multi-Scale CNN-BiLSTM with Residual-Gated Attention(https://arxiv.org/abs/2510.22818)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Air pollution remains a critical environmental and public health concern in Indian megacities such as Delhi, Kolkata, and Mumbai, where sudden spikes in pollutant levels challenge timely intervention. Accurate Air Quality Index (AQI) forecasting is difficult due to the coexistence of linear trends, seasonal variations, and volatile nonlinear patterns. This paper proposes a hybrid forecasting framework that integrates LOESS decomposition, ARIMA modeling, and a multi-scale CNN-BiLSTM network with a residual-gated attention mechanism. The LOESS step separates the AQI series into trend, seasonal, and residual components, with ARIMA modeling the smooth components and the proposed deep learning module capturing multi-scale volatility in the residuals. Model hyperparameters are tuned via the Unified Adaptive Multi-Stage Metaheuristic Optimizer (UAMMO), combining multiple optimization strategies for efficient convergence. Experiments on 2021-2023 AQI datasets from the Central Pollution Control Board show that the proposed method consistently outperforms statistical, deep learning, and hybrid baselines across PM2.5, O3, CO, and NOx in three major cities, achieving up to 5-8% lower MSE and higher R^2 scores (>0.94) for all pollutants. These results demonstrate the framework's robustness, sensitivity to sudden pollution events, and applicability to urban air quality management.</li>
</ul>

<h3>Title: Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP</h3>
<ul>
<li><strong>Authors: </strong>Poli Nemkova, Amrit Adhikari, Matthew Pearson, Vamsi Krishna Sadu, Mark V. Albert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22823">https://arxiv.org/abs/2510.22823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22823">https://arxiv.org/pdf/2510.22823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22823]] Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP(https://arxiv.org/abs/2510.22823)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Humanitarian organizations face a critical choice: invest in costly commercial APIs or rely on free open-weight models for multilingual human rights monitoring. While commercial systems offer reliability, open-weight alternatives lack empirical validation -- especially for low-resource languages common in conflict zones. This paper presents the first systematic comparison of commercial and open-weight large language models (LLMs) for human-rights-violation detection across seven languages, quantifying the cost-reliability trade-off facing resource-constrained organizations. Across 78,000 multilingual inferences, we evaluate six models -- four instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0, GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both standard classification metrics and new measures of cross-lingual reliability: Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS), and Language Stability Score (LSS). Results show that alignment, not scale, determines stability: aligned models maintain near-invariant accuracy and balanced calibration across typologically distant and low-resource languages (e.g., Lingala, Burmese), while open-weight models exhibit significant prompt-language sensitivity and calibration drift. These findings demonstrate that multilingual alignment enables language-agnostic reasoning and provide practical guidance for humanitarian organizations balancing budget constraints with reliability in multilingual deployment.</li>
</ul>

<h3>Title: FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zahraa Al Sahili, Maryam Fetanat, Maimuna Nowaz, Ioannis Patras, Matthew Purver</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22827">https://arxiv.org/abs/2510.22827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22827">https://arxiv.org/pdf/2510.22827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22827]] FairJudge: MLLM Judging for Social Attributes and Prompt Image Alignment(https://arxiv.org/abs/2510.22827)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) systems lack simple, reproducible ways to evaluate how well images match prompts and how models treat social attributes. Common proxies -- face classifiers and contrastive similarity -- reward surface cues, lack calibrated abstention, and miss attributes only weakly visible (for example, religion, culture, disability). We present FairJudge, a lightweight protocol that treats instruction-following multimodal LLMs as fair judges. It scores alignment with an explanation-oriented rubric mapped to [-1, 1]; constrains judgments to a closed label set; requires evidence grounded in the visible content; and mandates abstention when cues are insufficient. Unlike CLIP-only pipelines, FairJudge yields accountable, evidence-aware decisions; unlike mitigation that alters generators, it targets evaluation fairness. We evaluate gender, race, and age on FairFace, PaTA, and FairCoT; extend to religion, culture, and disability; and assess profession correctness and alignment on IdenProf, FairCoT-Professions, and our new DIVERSIFY-Professions. We also release DIVERSIFY, a 469-image corpus of diverse, non-iconic scenes. Across datasets, judge models outperform contrastive and face-centric baselines on demographic prediction and improve mean alignment while maintaining high profession accuracy, enabling more reliable, reproducible fairness audits.</li>
</ul>

<h3>Title: LLM-based Fusion of Multi-modal Features for Commercial Memorability Prediction</h3>
<ul>
<li><strong>Authors: </strong>Aleksandar Pramov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22829">https://arxiv.org/abs/2510.22829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22829">https://arxiv.org/pdf/2510.22829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22829]] LLM-based Fusion of Multi-modal Features for Commercial Memorability Prediction(https://arxiv.org/abs/2510.22829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper addresses the prediction of commercial (brand) memorability as part of "Subtask 2: Commercial/Ad Memorability" within the "Memorability: Predicting movie and commercial memorability" task at the MediaEval 2025 workshop competition. We propose a multimodal fusion system with a Gemma-3 LLM backbone that integrates pre-computed visual (ViT) and textual (E5) features by multi-modal projections. The model is adapted using Low-Rank Adaptation (LoRA). A heavily-tuned ensemble of gradient boosted trees serves as a baseline. A key contribution is the use of LLM-generated rationale prompts, grounded in expert-derived aspects of memorability, to guide the fusion model. The results demonstrate that the LLM-based system exhibits greater robustness and generalization performance on the final test set, compared to the baseline. The paper's codebase can be found at this https URL</li>
</ul>

<h3>Title: Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays</h3>
<ul>
<li><strong>Authors: </strong>Haowei Hua (1), Hong Jiao (2), Xinyi Wang (3) ((1) Princeton University, (2) University of Maryland, College Park, (3) University of Maryland, College Park &amp; Beijing Normal University)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22830">https://arxiv.org/abs/2510.22830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22830">https://arxiv.org/pdf/2510.22830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22830]] Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays(https://arxiv.org/abs/2510.22830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>BERT and its variants are extensively explored for automated scoring. However, a limit of 512 tokens for these encoder-based models showed the deficiency in automated scoring of long essays. Thus, this research explores generative language models for automated scoring of long essays via summarization and prompting. The results revealed great improvement of scoring accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab Automated Essay Scoring 2.0 dataset.</li>
</ul>

<h3>Title: Clustering by Denoising: Latent plug-and-play diffusion for single-cell data</h3>
<ul>
<li><strong>Authors: </strong>Dominik Meier, Shixing Yu, Sagnik Nandy, Promit Ghosal, Kyra Gan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22835">https://arxiv.org/abs/2510.22835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22835">https://arxiv.org/pdf/2510.22835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22835]] Clustering by Denoising: Latent plug-and-play diffusion for single-cell data(https://arxiv.org/abs/2510.22835)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Single-cell RNA sequencing (scRNA-seq) enables the study of cellular heterogeneity. Yet, clustering accuracy, and with it downstream analyses based on cell labels, remain challenging due to measurement noise and biological variability. In standard latent spaces (e.g., obtained through PCA), data from different cell types can be projected close together, making accurate clustering difficult. We introduce a latent plug-and-play diffusion framework that separates the observation and denoising space. This separation is operationalized through a novel Gibbs sampling procedure: the learned diffusion prior is applied in a low-dimensional latent space to perform denoising, while to steer this process, noise is reintroduced into the original high-dimensional observation space. This unique "input-space steering" ensures the denoising trajectory remains faithful to the original data structure. Our approach offers three key advantages: (1) adaptive noise handling via a tunable balance between prior and observed data; (2) uncertainty quantification through principled uncertainty estimates for downstream analysis; and (3) generalizable denoising by leveraging clean reference data to denoise noisier datasets, and via averaging, improve quality beyond the training set. We evaluate robustness on both synthetic and real single-cell genomics data. Our method improves clustering accuracy on synthetic data across varied noise levels and dataset shifts. On real-world single-cell data, our method demonstrates improved biological coherence in the resulting cell clusters, with cluster boundaries that better align with known cell type markers and developmental trajectories.</li>
</ul>

<h3>Title: Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aya Nakayama, Brian Wong, Yuji Nishimura, Kaito Tanaka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22838">https://arxiv.org/abs/2510.22838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22838">https://arxiv.org/pdf/2510.22838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22838]] Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models(https://arxiv.org/abs/2510.22838)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The "style trap" poses a significant challenge for Large Vision-Language Models (LVLMs), hindering robust semantic understanding across diverse visual styles, especially in in-context learning (ICL). Existing methods often fail to effectively decouple style from content, hindering generalization. To address this, we propose the Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR), a novel framework for stable semantic understanding and adaptive cross-style visual reasoning. SP-CSVR integrates a Cross-Style Feature Encoder (CSFE) for style-content disentanglement, a Semantic-Aligned In-Context Decoder (SAICD) for efficient few-shot style adaptation, and an Adaptive Semantic Consistency Module (ASCM) employing multi-task contrastive learning to enforce cross-style semantic invariance. Extensive experiments on a challenging multi-style dataset demonstrate SP-CSVR's state-of-the-art performance across visual captioning, visual question answering, and in-context style adaptation. Comprehensive evaluations, including ablation studies and generalization analysis, confirm SP-CSVR's efficacy in enhancing robustness, generalization, and efficiency across diverse visual styles.</li>
</ul>

<h3>Title: Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning</h3>
<ul>
<li><strong>Authors: </strong>Prerna Ravi, Dong Won Lee, Beatriz Flamia, Jasmine David, Brandon Hanks, Cynthia Breazeal, Emma Anderson, Grace Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22844">https://arxiv.org/abs/2510.22844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22844">https://arxiv.org/pdf/2510.22844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22844]] Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning(https://arxiv.org/abs/2510.22844)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Understanding how ideas develop and flow in small-group conversations is critical for analyzing collaborative learning. A key structural feature of these interactions is threading, the way discourse talk naturally organizes into interwoven topical strands that evolve over time. While threading has been widely studied in asynchronous text settings, detecting threads in synchronous spoken dialogue remains challenging due to overlapping turns and implicit cues. At the same time, large language models (LLMs) show promise for automating discourse analysis but often struggle with long-context tasks that depend on tracing these conversational links. In this paper, we investigate whether explicit thread linkages can improve LLM-based coding of relational moves in group talk. We contribute a systematic guidebook for identifying threads in synchronous multi-party transcripts and benchmark different LLM prompting strategies for automated threading. We then test how threading influences performance on downstream coding of conversational analysis frameworks, that capture core collaborative actions such as agreeing, building, and eliciting. Our results show that providing clear conversational thread information improves LLM coding performance and underscores the heavy reliance of downstream analysis on well-structured dialogue. We also discuss practical trade-offs in time and cost, emphasizing where human-AI hybrid approaches can yield the best value. Together, this work advances methods for combining LLMs and robust conversational thread structures to make sense of complex, real-time group interactions.</li>
</ul>

<h3>Title: Once Upon an Input: Reasoning via Per-Instance Program Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Adam Stein, Neelay Velingker, Mayur Naik, Eric Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22849">https://arxiv.org/abs/2510.22849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22849">https://arxiv.org/pdf/2510.22849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22849]] Once Upon an Input: Reasoning via Per-Instance Program Synthesis(https://arxiv.org/abs/2510.22849)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at zero-shot inference but continue to struggle with complex, multi-step reasoning. Recent methods that augment LLMs with intermediate reasoning steps such as Chain of Thought (CoT) and Program of Thought (PoT) improve performance but often produce undesirable solutions, especially in algorithmic domains. We introduce Per-Instance Program Synthesis (PIPS), a method that generates and refines programs at the instance-level using structural feedback without relying on task-specific guidance or explicit test cases. To further improve performance, PIPS incorporates a confidence metric that dynamically chooses between direct inference and program synthesis on a per-instance basis. Experiments across three frontier LLMs and 30 benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question answering tasks, relational reasoning tasks, and mathematical reasoning tasks show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and 9.4% compared to PoT and CoT respectively, and reduces undesirable program generations by 65.1% on the algorithmic tasks compared to PoT with Gemini-2.0-Flash.</li>
</ul>

<h3>Title: Semantic Surgery: Zero-Shot Concept Erasure in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Lexiang Xiong, Chengyu Liu, Jingwen Ye, Yan Liu, Yuecong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22851">https://arxiv.org/abs/2510.22851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22851">https://arxiv.org/pdf/2510.22851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22851]] Semantic Surgery: Zero-Shot Concept Erasure in Diffusion Models(https://arxiv.org/abs/2510.22851)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Concept erasure in text-to-image diffusion models is crucial for mitigating harmful content, yet existing methods often compromise generative quality. We introduce Semantic Surgery, a novel training-free, zero-shot framework for concept erasure that operates directly on text embeddings before the diffusion process. It dynamically estimates the presence of target concepts in a prompt and performs a calibrated vector subtraction to neutralize their influence at the source, enhancing both erasure completeness and locality. The framework includes a Co-Occurrence Encoding module for robust multi-concept erasure and a visual feedback loop to address latent concept persistence. As a training-free method, Semantic Surgery adapts dynamically to each prompt, ensuring precise interventions. Extensive experiments on object, explicit content, artistic style, and multi-celebrity erasure tasks show our method significantly outperforms state-of-the-art approaches. We achieve superior completeness and robustness while preserving locality and image quality (e.g., 93.58 H-score in object erasure, reducing explicit content to just 1 instance, and 8.09 H_a in style erasure with no quality degradation). This robustness also allows our framework to function as a built-in threat detection system, offering a practical solution for safer text-to-image generation.</li>
</ul>

<h3>Title: Encoder-Decoder Diffusion Language Models for Efficient Training and Inference</h3>
<ul>
<li><strong>Authors: </strong>Marianne Arriola, Yair Schiff, Hao Phung, Aaron Gokaslan, Volodymyr Kuleshov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22852">https://arxiv.org/abs/2510.22852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22852">https://arxiv.org/pdf/2510.22852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22852]] Encoder-Decoder Diffusion Language Models for Efficient Training and Inference(https://arxiv.org/abs/2510.22852)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models enable parallel token sampling for faster inference than autoregressive approaches. However, prior diffusion models use a decoder-only architecture, which requires sampling algorithms that invoke the full network at every denoising step and incur high computational cost. Our key insight is that discrete diffusion models perform two types of computation: 1) representing clean tokens and 2) denoising corrupted tokens, which enables us to use separate modules for each task. We propose an encoder-decoder architecture to accelerate discrete diffusion inference, which relies on an encoder to represent clean tokens and a lightweight decoder to iteratively refine a noised sequence. We also show that this architecture enables faster training of block diffusion models, which partition sequences into blocks for better quality and are commonly used in diffusion language model inference. We introduce a framework for Efficient Encoder-Decoder Diffusion (E2D2), consisting of an architecture with specialized training and sampling algorithms, and we show that E2D2 achieves superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks. We provide the code, model weights, and blog post on the project page: this https URL</li>
</ul>

<h3>Title: A Review of End-to-End Precipitation Prediction Using Remote Sensing Data: from Divination to Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Yugong Zeng, Jonathan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22855">https://arxiv.org/abs/2510.22855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22855">https://arxiv.org/pdf/2510.22855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22855]] A Review of End-to-End Precipitation Prediction Using Remote Sensing Data: from Divination to Machine Learning(https://arxiv.org/abs/2510.22855)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Precipitation prediction has undergone a profound transformation -- from early symbolic and empirical methods rooted in divination and observation, to modern technologies based on atmospheric physics and artificial intelligence. This review traces the historical and technological evolution of precipitation forecasting, presenting a survey about end-to-end precipitation prediction technologies that spans ancient practices, the foundations of meteorological science, the rise of numerical weather prediction (NWP), and the emergence of machine learning (ML) and deep learning (DL) models. We first explore traditional and indigenous forecasting methods, then describe the development of physical modeling and statistical frameworks that underpin contemporary operational forecasting. Particular emphasis is placed on recent advances in neural network-based approaches, including automated deep learning, interpretability-driven design, and hybrid physical-data models. By compositing research across multiple eras and paradigms, this review not only depicts the history of end-to-end precipitation prediction but also outlines future directions in next generation forecasting systems.</li>
</ul>

<h3>Title: Guardian: Decoupling Exploration from Safety in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Kaitong Cai, Jusheng Zhang, Jing Yang, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22859">https://arxiv.org/abs/2510.22859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22859">https://arxiv.org/pdf/2510.22859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22859]] Guardian: Decoupling Exploration from Safety in Reinforcement Learning(https://arxiv.org/abs/2510.22859)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hybrid offline--online reinforcement learning (O2O RL) promises both sample efficiency and robust exploration, but suffers from instability due to distribution shift between offline and online data. We introduce RLPD-GX, a framework that decouples policy optimization from safety enforcement: a reward-seeking learner explores freely, while a projection-based guardian guarantees rule-consistent execution and safe value backups. This design preserves the exploratory value of online interactions without collapsing to conservative policies. To further stabilize training, we propose dynamic curricula that gradually extend temporal horizons and anneal offline--online data mixing. We prove convergence via a contraction property of the guarded Bellman operator, and empirically show state-of-the-art performance on Atari-100k, achieving a normalized mean score of 3.02 (+45\% over prior hybrid methods) with stronger safety and stability. Beyond Atari, ablations demonstrate consistent gains across safety-critical and long-horizon tasks, underscoring the generality of our design. Extensive and comprehensive results highlight decoupled safety enforcement as a simple yet principled route to robust O2O RL, suggesting a broader paradigm for reconciling exploration and safety in reinforcement learning.</li>
</ul>

<h3>Title: Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Linyang He, Tianjun Zhong, Richard Antonello, Gavin Mischler, Micah Goldblum, Nima Mesgarani</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22860">https://arxiv.org/abs/2510.22860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22860">https://arxiv.org/pdf/2510.22860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22860]] Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement(https://arxiv.org/abs/2510.22860)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly "entangled," mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing.</li>
</ul>

<h3>Title: Long-Term PM2.5 Forecasting Using a DTW-Enhanced CNN-GRU Model</h3>
<ul>
<li><strong>Authors: </strong>Amirali Ataee Naeini, Arshia Ataee Naeini, Fatemeh Karami Mohammadi, Omid Ghaffarpasand</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22863">https://arxiv.org/abs/2510.22863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22863">https://arxiv.org/pdf/2510.22863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22863]] Long-Term PM2.5 Forecasting Using a DTW-Enhanced CNN-GRU Model(https://arxiv.org/abs/2510.22863)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Reliable long-term forecasting of PM2.5 concentrations is critical for public health early-warning systems, yet existing deep learning approaches struggle to maintain prediction stability beyond 48 hours, especially in cities with sparse monitoring networks. This paper presents a deep learning framework that combines Dynamic Time Warping (DTW) for intelligent station similarity selection with a CNN-GRU architecture to enable extended-horizon PM2.5 forecasting in Isfahan, Iran, a city characterized by complex pollution dynamics and limited monitoring coverage. Unlike existing approaches that rely on computationally intensive transformer models or external simulation tools, our method integrates three key innovations: (i) DTW-based historical sampling to identify similar pollution patterns across peer stations, (ii) a lightweight CNN-GRU architecture augmented with meteorological features, and (iii) a scalable design optimized for sparse networks. Experimental validation using multi-year hourly data from eight monitoring stations demonstrates superior performance compared to state-of-the-art deep learning methods, achieving R2 = 0.91 for 24-hour forecasts. Notably, this is the first study to demonstrate stable 10-day PM2.5 forecasting (R2 = 0.73 at 240 hours) without performance degradation, addressing critical early-warning system requirements. The framework's computational efficiency and independence from external tools make it particularly suitable for deployment in resource-constrained urban environments.</li>
</ul>

<h3>Title: Interpreting and Mitigating Unwanted Uncertainty in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tiasa Singha Roy, Ayush Rajesh Jhaveri, Ilias Triantafyllopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22866">https://arxiv.org/abs/2510.22866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22866">https://arxiv.org/pdf/2510.22866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22866]] Interpreting and Mitigating Unwanted Uncertainty in LLMs(https://arxiv.org/abs/2510.22866)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Despite their impressive capabilities, Large Language Models (LLMs) exhibit unwanted uncertainty, a phenomenon where a model changes a previously correct answer into an incorrect one when re-prompted. This behavior undermines trust and poses serious risks in high-stakes domains. In this work, we investigate the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack retrieval framework and integrate a Flip-style re-evaluation prompt to simulate realistic answer-flipping scenarios. We find that retrieval heads are not primarily responsible for avoiding uncertainty. Instead, we identify a small set of non-retrieval attention heads that disproportionately attend to misleading tokens in uncertain contexts. Masking these heads yields significant improvements, reducing flip behavior by up to 15% without introducing incoherence or overcorrection. However, when tested for downstream tasks, we observe trade-offs with flip behavior. Our findings contribute to the growing field of mechanistic interpretability and present a simple yet effective technique for mitigating uncertainty-driven failure modes in LLMs.</li>
</ul>

<h3>Title: Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Qianyu Zhou, Farhad Imani, Jiong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22868">https://arxiv.org/abs/2510.22868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22868">https://arxiv.org/pdf/2510.22868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22868]] Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models(https://arxiv.org/abs/2510.22868)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Wind turbine blades operate in harsh environments, making timely damage detection essential for preventing failures and optimizing maintenance. Drone-based inspection and deep learning are promising, but typically depend on large, labeled datasets, which limit their ability to detect rare or evolving damage types. To address this, we propose a zero-shot-oriented inspection framework that integrates Retrieval-Augmented Generation (RAG) with Vision-Language Models (VLM). A multimodal knowledge base is constructed, comprising technical documentation, representative reference images, and domain-specific guidelines. A hybrid text-image retriever with keyword-aware reranking assembles the most relevant context to condition the VLM at inference, injecting domain knowledge without task-specific training. We evaluate the framework on 30 labeled blade images covering diverse damage categories. Although the dataset is small due to the difficulty of acquiring verified blade imagery, it covers multiple representative defect types. On this test set, the RAG-grounded VLM correctly classified all samples, whereas the same VLM without retrieval performed worse in both accuracy and precision. We further compare against open-vocabulary baselines and incorporate uncertainty Clopper-Pearson confidence intervals to account for the small-sample setting. Ablation studies indicate that the key advantage of the framework lies in explainability and generalizability: retrieved references ground the reasoning process and enable the detection of previously unseen defects by leveraging domain knowledge rather than relying solely on visual cues. This research contributes a data-efficient solution for industrial inspection that reduces dependence on extensive labeled datasets.</li>
</ul>

<h3>Title: A Comprehensive Dataset for Human vs. AI Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Amit Sheth, Vasu Sharma, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, Amitava Das</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22874">https://arxiv.org/abs/2510.22874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22874">https://arxiv.org/pdf/2510.22874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22874]] A Comprehensive Dataset for Human vs. AI Generated Text Detection(https://arxiv.org/abs/2510.22874)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has led to increasingly human-like AI-generated text, raising concerns about content authenticity, misinformation, and trustworthiness. Addressing the challenge of reliably detecting AI-generated text and attributing it to specific models requires large-scale, diverse, and well-annotated datasets. In this work, we present a comprehensive dataset comprising over 58,000 text samples that combine authentic New York Times articles with synthetic versions generated by multiple state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B, Yi-Large, and GPT-4-o. The dataset provides original article abstracts as prompts, full human-authored narratives. We establish baseline results for two key tasks: distinguishing human-written from AI-generated text, achieving an accuracy of 58.35\%, and attributing AI texts to their generating models with an accuracy of 8.92\%. By bridging real-world journalistic content with modern generative models, the dataset aims to catalyze the development of robust detection and attribution methods, fostering trust and transparency in the era of generative AI. Our dataset is available at: this https URL.</li>
</ul>

<h3>Title: Limits of Generative Pre-Training in Structured EMR Trajectories with Irregular Sampling</h3>
<ul>
<li><strong>Authors: </strong>Nicholas I-Hsien Kuo, Blanca Gallego, Louisa Jorm</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22878">https://arxiv.org/abs/2510.22878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22878">https://arxiv.org/pdf/2510.22878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22878]] Limits of Generative Pre-Training in Structured EMR Trajectories with Irregular Sampling(https://arxiv.org/abs/2510.22878)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Foundation models refer to architectures trained on vast datasets using autoregressive pre-training from natural language processing to capture intricate patterns and motifs. They were originally developed to transfer such learned knowledge to downstream predictive tasks. Recently, however, some studies repurpose these learned representations for phenotype discovery without rigorous validation, risking superficially realistic but clinically incoherent embeddings. To test this mismatch, we trained two autoregressive models -- a sequence-to-sequence LSTM and a reduced Transformer -- on longitudinal ART for HIV and Acute Hypotension datasets. Controlled irregularity was added during training via random inter-visit gaps, while test sequences stayed complete. Patient-trajectory synthesis evaluated distributional and correlational fidelity. Both reproduced feature distributions but failed to preserve cross-feature structure -- showing that generative pre-training yields local realism but limited clinical coherence. These results highlight the need for domain-specific evaluation and support trajectory synthesis as a practical probe before fine-tuning or deployment.</li>
</ul>

<h3>Title: Learning Reconfigurable Representations for Multimodal Federated Learning with Missing Data</h3>
<ul>
<li><strong>Authors: </strong>Duong M. Nguyen, Trong Nghia Hoang, Thanh Trung Huynh, Quoc Viet Hung Nguyen, Phi Le Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22880">https://arxiv.org/abs/2510.22880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22880">https://arxiv.org/pdf/2510.22880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22880]] Learning Reconfigurable Representations for Multimodal Federated Learning with Missing Data(https://arxiv.org/abs/2510.22880)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Multimodal federated learning in real-world settings often encounters incomplete and heterogeneous data across clients. This results in misaligned local feature representations that limit the effectiveness of model aggregation. Unlike prior work that assumes either differing modality sets without missing input features or a shared modality set with missing features across clients, we consider a more general and realistic setting where each client observes a different subset of modalities and might also have missing input features within each modality. To address the resulting misalignment in learned representations, we propose a new federated learning framework featuring locally adaptive representations based on learnable client-side embedding controls that encode each client's data-missing patterns. These embeddings serve as reconfiguration signals that align the globally aggregated representation with each client's local context, enabling more effective use of shared information. Furthermore, the embedding controls can be algorithmically aggregated across clients with similar data-missing patterns to enhance the robustness of reconfiguration signals in adapting the global representation. Empirical results on multiple federated multimodal benchmarks with diverse data-missing patterns across clients demonstrate the efficacy of the proposed method, achieving up to 36.45\% performance improvement under severe data incompleteness. The method is also supported by a theoretical analysis with an explicit performance bound that matches our empirical observations. Our source codes are provided at this https URL</li>
</ul>

<h3>Title: Offline Preference Optimization via Maximum Marginal Likelihood Estimation</h3>
<ul>
<li><strong>Authors: </strong>Saeed Najafi, Alona Fyshe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22881">https://arxiv.org/abs/2510.22881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22881">https://arxiv.org/pdf/2510.22881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22881]] Offline Preference Optimization via Maximum Marginal Likelihood Estimation(https://arxiv.org/abs/2510.22881)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning Large Language Models (LLMs) with human preferences is crucial, but standard methods like Reinforcement Learning from Human Feedback (RLHF) are often complex and unstable. In this work, we propose a new, simpler approach that recasts alignment through the lens of Maximum Marginal Likelihood (MML) estimation. Our new MML based Preference Optimization (MMPO) maximizes the marginal log-likelihood of a preferred text output, using the preference pair as samples for approximation, and forgoes the need for both an explicit reward model and entropy maximization. We theoretically demonstrate that MMPO implicitly performs preference optimization, producing a weighted gradient that naturally up-weights chosen responses over rejected ones. Across models ranging from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable with respect to the hyperparameter $\beta$ compared to alternative baselines, and 2) achieves competitive or superior preference alignment while better preserving the base model's general language capabilities. Through a series of ablation experiments, we show that this improved performance is indeed attributable to MMPO's implicit preference optimization within the gradient updates.</li>
</ul>

<h3>Title: Transforming volcanic monitoring: A dataset and benchmark for onboard volcano activity detection</h3>
<ul>
<li><strong>Authors: </strong>Darshana Priyasad, Tharindu Fernando, Maryam Haghighat, Harshala Gammulle, Clinton Fookes</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22889">https://arxiv.org/abs/2510.22889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22889">https://arxiv.org/pdf/2510.22889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22889]] Transforming volcanic monitoring: A dataset and benchmark for onboard volcano activity detection(https://arxiv.org/abs/2510.22889)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Natural disasters, such as volcanic eruptions, pose significant challenges to daily life and incur considerable global economic losses. The emergence of next-generation small-satellites, capable of constellation-based operations, offers unparalleled opportunities for near-real-time monitoring and onboard processing of such events. However, a major bottleneck remains the lack of extensive annotated datasets capturing volcanic activity, which hinders the development of robust detection systems. This paper introduces a novel dataset explicitly designed for volcanic activity and eruption detection, encompassing diverse volcanoes worldwide. The dataset provides binary annotations to identify volcanic anomalies or non-anomalies, covering phenomena such as temperature anomalies, eruptions, and volcanic ash emissions. These annotations offer a foundational resource for developing and evaluating detection models, addressing a critical gap in volcanic monitoring research. Additionally, we present comprehensive benchmarks using state-of-the-art models to establish baselines for future studies. Furthermore, we explore the potential for deploying these models onboard next-generation satellites. Using the Intel Movidius Myriad X VPU as a testbed, we demonstrate the feasibility of volcanic activity detection directly onboard. This capability significantly reduces latency and enhances response times, paving the way for advanced early warning systems. This paves the way for innovative solutions in volcanic disaster management, encouraging further exploration and refinement of onboard monitoring technologies.</li>
</ul>

<h3>Title: On the Anisotropy of Score-Based Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22899">https://arxiv.org/abs/2510.22899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22899">https://arxiv.org/pdf/2510.22899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22899]] On the Anisotropy of Score-Based Generative Models(https://arxiv.org/abs/2510.22899)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate the role of network architecture in shaping the inductive biases of modern score-based generative models. To this end, we introduce the Score Anisotropy Directions (SADs), architecture-dependent directions that reveal how different networks preferentially capture data structure. Our analysis shows that SADs form adaptive bases aligned with the architecture's output geometry, providing a principled way to predict generalization ability in score models prior to training. Through both synthetic data and standard image benchmarks, we demonstrate that SADs reliably capture fine-grained model behavior and correlate with downstream performance, as measured by Wasserstein metrics. Our work offers a new lens for explaining and predicting directional biases of generative models.</li>
</ul>

<h3>Title: Language Server CLI Empowers Language Agents with Process Rewards</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhang, Lanser Contributors</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22907">https://arxiv.org/abs/2510.22907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22907">https://arxiv.org/pdf/2510.22907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22907]] Language Server CLI Empowers Language Agents with Process Rewards(https://arxiv.org/abs/2510.22907)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: this https URL</li>
</ul>

<h3>Title: Simple Denoising Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huaisheng Zhu, Zhengyu Chen, Shijie Zhou, Zhihui Xie, Yige Yuan, Zhimeng Guo, Siyuan Xu, Hangfan Zhang, Vasant Honavar, Teng Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22926">https://arxiv.org/abs/2510.22926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22926">https://arxiv.org/pdf/2510.22926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22926]] Simple Denoising Diffusion Language Models(https://arxiv.org/abs/2510.22926)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently been extended to language generation through Masked Diffusion Language Models (MDLMs), which achieve performance competitive with strong autoregressive models. However, MDLMs tend to degrade in the few-step regime and cannot directly adopt existing few-step distillation methods designed for continuous diffusion models, as they lack the intrinsic property of mapping from noise to data. Recent Uniform-state Diffusion Models (USDMs), initialized from a uniform prior, alleviate some limitations but still suffer from complex loss formulations that hinder scalability. In this work, we propose a simplified denoising-based loss for USDMs that optimizes only noise-replaced tokens, stabilizing training and matching ELBO-level performance. Furthermore, by framing denoising as self-supervised learning, we introduce a simple modification to our denoising loss with contrastive-inspired negative gradients, which is practical and yield additional improvements in generation quality.</li>
</ul>

<h3>Title: Diffuse to Detect: A Generalizable Framework for Anomaly Detection with Diffusion Models Applications to UAVs and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Mingze Gong, Juan Du, Jianbang You</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22928">https://arxiv.org/abs/2510.22928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22928">https://arxiv.org/pdf/2510.22928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22928]] Diffuse to Detect: A Generalizable Framework for Anomaly Detection with Diffusion Models Applications to UAVs and Beyond(https://arxiv.org/abs/2510.22928)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Anomaly detection in complex, high-dimensional data, such as UAV sensor readings, is essential for operational safety but challenging for existing methods due to their limited sensitivity, scalability, and inability to capture intricate dependencies. We propose the Diffuse to Detect (DTD) framework, a novel approach that innovatively adapts diffusion models for anomaly detection, diverging from their conventional use in generative tasks with high inference time. By comparison, DTD employs a single-step diffusion process to predict noise patterns, enabling rapid and precise identification of anomalies without reconstruction errors. This approach is grounded in robust theoretical foundations that link noise prediction to the data distribution's score function, ensuring reliable deviation detection. By integrating Graph Neural Networks to model sensor relationships as dynamic graphs, DTD effectively captures spatial (inter-sensor) and temporal anomalies. Its two-branch architecture, with parametric neural network-based energy scoring for scalability and nonparametric statistical methods for interpretability, provides flexible trade-offs between computational efficiency and transparency. Extensive evaluations on UAV sensor data, multivariate time series, and images demonstrate DTD's superior performance over existing methods, underscoring its generality across diverse data modalities. This versatility, combined with its adaptability, positions DTD as a transformative solution for safety-critical applications, including industrial monitoring and beyond.</li>
</ul>

<h3>Title: Robust Uncertainty Quantification for Self-Evolving Large Language Models via Continual Domain Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Xiaofan Zhou, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22931">https://arxiv.org/abs/2510.22931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22931">https://arxiv.org/pdf/2510.22931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22931]] Robust Uncertainty Quantification for Self-Evolving Large Language Models via Continual Domain Pretraining(https://arxiv.org/abs/2510.22931)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Continual Learning (CL) is essential for enabling self-evolving large language models (LLMs) to adapt and remain effective amid rapid knowledge growth. Yet, despite its importance, little attention has been given to establishing statistical reliability guarantees for LLMs under CL, particularly in the setting of continual domain pretraining (CDP). Conformal Prediction (CP) has shown promise in offering correctness guarantees for LLMs, but it faces major challenges in CDP: testing data often stems from unknown or shifting domain distributions, under which CP may no longer provide valid guarantees. Moreover, when high coverage is required, CP can yield excessively large prediction sets for unanswerable queries, reducing informativeness. To address these challenges, we introduce an adaptive rejection and non-exchangeable CP framework. Our method first estimates the distribution of questions across domains in the test set using transformer-based clustering, then reweights or resamples the calibration data accordingly. Building on this, adaptive rejection CP allows the LLM to selectively abstain from answering when its confidence or competence shifts significantly. Extensive experiments demonstrate that our framework enhances both the effectiveness and reliability of CP under CDP scenarios. Our code is available at: this https URL</li>
</ul>

<h3>Title: Positional Preservation Embedding for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mouxiao Huang, Borui Jiang, Dehua Zheng, Hailin Hu, Kai Han, Xinghao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22936">https://arxiv.org/abs/2510.22936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22936">https://arxiv.org/pdf/2510.22936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22936]] Positional Preservation Embedding for Multimodal Large Language Models(https://arxiv.org/abs/2510.22936)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have achieved strong performance on vision-language tasks, yet often suffer from inefficiencies due to redundant visual tokens. Existing token merging methods reduce sequence length but frequently disrupt spatial layouts and temporal continuity by disregarding positional relationships. In this work, we propose a novel encoding operator dubbed as \textbf{P}ositional \textbf{P}reservation \textbf{E}mbedding (\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal structure during visual token compression. PPE explicitly introduces the disentangled encoding of 3D positions in the token dimension, enabling each compressed token to encapsulate different positions from multiple original tokens. Furthermore, we show that PPE can effectively support cascade clustering -- a progressive token compression strategy that leads to better performance retention. PPE is a parameter-free and generic operator that can be seamlessly integrated into existing token merging methods without any adjustments. Applied to state-of-the-art token merging framework, PPE achieves consistent improvements of $2\%\sim5\%$ across multiple vision-language benchmarks, including MMBench (general vision understanding), TextVQA (layout understanding) and VideoMME (temporal understanding). These results demonstrate that preserving positional cues is critical for efficient and effective MLLM reasoning.</li>
</ul>

<h3>Title: Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics</h3>
<ul>
<li><strong>Authors: </strong>Matthew So, Judah Goldfeder, Mark Lis, Hod Lipson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22937">https://arxiv.org/abs/2510.22937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22937">https://arxiv.org/pdf/2510.22937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22937]] Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics(https://arxiv.org/abs/2510.22937)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric, transformer</a></li>
<li><strong>Abstract: </strong>There has been a historic assumption that the biometrics of an individual are statistically uncorrelated. We test this assumption by training Bi-Encoder networks on three verification tasks, including fingerprint-to-fingerprint matching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching using 274 subjects with $\sim$100k fingerprints and 7k iris images. We trained ResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such that the contrastive loss between images sampled from the same individual is minimized. The iris ResNet architecture reaches 91 ROC AUC score for iris-to-iris matching, providing clear evidence that the left and right irises of an individual are correlated. Fingerprint models reproduce the positive intra-subject suggested by prior work in this space. This is the first work attempting to use Vision Transformers for this matching. Cross-modal matching rises only slightly above chance, which suggests that more data and a more sophisticated pipeline is needed to obtain compelling results. These findings continue challenge independence assumptions of biometrics and we plan to extend this work to other biometrics in the future. Code available: this https URL.</li>
</ul>

<h3>Title: Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies</h3>
<ul>
<li><strong>Authors: </strong>Bin Wang, YiLu Zhong, MiDi Wan, WenJie Yu, YuanBing Ouyang, Yenan Huang, Hui Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22944">https://arxiv.org/abs/2510.22944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22944">https://arxiv.org/pdf/2510.22944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22944]] Is Your Prompt Poisoning Code? Defect Induction Rates and Security Mitigation Strategies(https://arxiv.org/abs/2510.22944)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become indispensable for automated code generation, yet the quality and security of their outputs remain a critical concern. Existing studies predominantly concentrate on adversarial attacks or inherent flaws within the models. However, a more prevalent yet underexplored issue concerns how the quality of a benign but poorly formulated prompt affects the security of the generated code. To investigate this, we first propose an evaluation framework for prompt quality encompassing three key dimensions: goal clarity, information completeness, and logical consistency. Based on this framework, we construct and publicly release CWE-BENCH-PYTHON, a large-scale benchmark dataset containing tasks with prompts categorized into four distinct levels of normativity (L0-L3). Extensive experiments on multiple state-of-the-art LLMs reveal a clear correlation: as prompt normativity decreases, the likelihood of generating insecure code consistently and markedly increases. Furthermore, we demonstrate that advanced prompting techniques, such as Chain-of-Thought and Self-Correction, effectively mitigate the security risks introduced by low-quality prompts, substantially improving code safety. Our findings highlight that enhancing the quality of user prompts constitutes a critical and effective strategy for strengthening the security of AI-generated code.</li>
</ul>

<h3>Title: QuantumShield: Multilayer Fortification for Quantum Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Dev Gurung, Shiva Raj Pokhrel</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22945">https://arxiv.org/abs/2510.22945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22945">https://arxiv.org/pdf/2510.22945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22945]] QuantumShield: Multilayer Fortification for Quantum Federated Learning(https://arxiv.org/abs/2510.22945)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a groundbreaking quantum-secure federated learning (QFL) framework designed to safeguard distributed learning systems against the emerging threat of quantum-enabled adversaries. As classical cryptographic methods become increasingly vulnerable to quantum attacks, our framework establishes a resilient security architecture that remains robust even in the presence of quantum-capable attackers. We integrate and rigorously evaluate advanced quantum and post-quantum protocols including Quantum Key Distribution (QKD), Quantum Teleportation, Key Encapsulation Mechanisms (KEM) and Post-Quantum Cryptography (PQC) to fortify the QFL process against both classical and quantum threats. These mechanisms are systematically analyzed and implemented to demonstrate their seamless interoperability within a secure and scalable QFL ecosystem. Through comprehensive theoretical modeling and experimental validation, this work provides a detailed security and performance assessment of the proposed framework. Our findings lay a strong foundation for next-generation federated learning systems that are inherently secure in the quantum era.</li>
</ul>

<h3>Title: Manifold Approximation leads to Robust Kernel Alignment</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Tariqul Islam, Du Liu, Deblina Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22953">https://arxiv.org/abs/2510.22953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22953">https://arxiv.org/pdf/2510.22953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22953]] Manifold Approximation leads to Robust Kernel Alignment(https://arxiv.org/abs/2510.22953)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Centered kernel alignment (CKA) is a popular metric for comparing representations, determining equivalence of networks, and neuroscience research. However, CKA does not account for the underlying manifold and relies on numerous heuristics that cause it to behave differently at different scales of data. In this work, we propose Manifold approximated Kernel Alignment (MKA), which incorporates manifold geometry into the alignment task. We derive a theoretical framework for MKA. We perform empirical evaluations on synthetic datasets and real-world examples to characterize and compare MKA to its contemporaries. Our findings suggest that manifold-aware kernel alignment provides a more robust foundation for measuring representations, with potential applications in representation learning.</li>
</ul>

<h3>Title: SARNet: A Spike-Aware consecutive validation Framework for Accurate Remaining Useful Life Prediction</h3>
<ul>
<li><strong>Authors: </strong>Junhao Fan, Wenrui Liang, Wei-Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22955">https://arxiv.org/abs/2510.22955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22955">https://arxiv.org/pdf/2510.22955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22955]] SARNet: A Spike-Aware consecutive validation Framework for Accurate Remaining Useful Life Prediction(https://arxiv.org/abs/2510.22955)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Accurate prediction of remaining useful life (RUL) is essential to enhance system reliability and reduce maintenance risk. Yet many strong contemporary models are fragile around fault onset and opaque to engineers: short, high-energy spikes are smoothed away or misread, fixed thresholds blunt sensitivity, and physics-based explanations are scarce. To remedy this, we introduce SARNet (Spike-Aware Consecutive Validation Framework), which builds on a Modern Temporal Convolutional Network (ModernTCN) and adds spike-aware detection to provide physics-informed interpretability. ModernTCN forecasts degradation-sensitive indicators; an adaptive consecutive threshold validates true spikes while suppressing noise. Failure-prone segments then receive targeted feature engineering (spectral slopes, statistical derivatives, energy ratios), and the final RUL is produced by a stacked RF--LGBM regressor. Across benchmark-ported datasets under an event-triggered protocol, SARNet consistently lowers error compared to recent baselines (RMSE 0.0365, MAE 0.0204) while remaining lightweight, robust, and easy to deploy.</li>
</ul>

<h3>Title: Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts</h3>
<ul>
<li><strong>Authors: </strong>Anwesan Pal, Karen Hovsepian, Tinghao Guo, Mengnan Zhao, Somendra Tripathi, Nikos Kanakaris, George Mihaila, Sumit Nigam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22956">https://arxiv.org/abs/2510.22956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22956">https://arxiv.org/pdf/2510.22956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22956]] Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts(https://arxiv.org/abs/2510.22956)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent investigations into effective context lengths of modern flagship large language models (LLMs) have revealed major limitations in effective question answering (QA) and reasoning over long and complex contexts for even the largest and most impressive cadre of models. While approaches like retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to mitigate this issue, they are sensitive to chunking, embedding and retrieval strategies and models, and furthermore, rely on extensive pre-processing, knowledge acquisition and indexing steps. In this paper, we propose Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy that boosts LLM performance in long-context scenarios, without degrading and altering the integrity and composition of retrieved documents. We validate our hypothesis by augmenting two challenging and directly relevant question-answering benchmarks -- NoLima and NovelQA -- and show that tagging the context or even just adding tag definitions into QA prompts leads to consistent performance gains over the baseline -- up to 17% for 32K token contexts, and 2.9% in complex reasoning question-answering for multi-hop queries requiring knowledge across a wide span of text. Additional details are available at this https URL.</li>
</ul>

<h3>Title: FAME: Fairness-aware Attention-modulated Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Zhidong Li, Longbing Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22960">https://arxiv.org/abs/2510.22960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22960">https://arxiv.org/pdf/2510.22960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22960]] FAME: Fairness-aware Attention-modulated Video Editing(https://arxiv.org/abs/2510.22960)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Training-free video editing (VE) models tend to fall back on gender stereotypes when rendering profession-related prompts. We propose \textbf{FAME} for \textit{Fairness-aware Attention-modulated Video Editing} that mitigates profession-related gender biases while preserving prompt alignment and temporal consistency for coherent VE. We derive fairness embeddings from existing minority representations by softly injecting debiasing tokens into the text encoder. Simultaneously, FAME integrates fairness modulation into both temporal self attention and prompt-to-region cross attention to mitigate the motion corruption and temporal inconsistency caused by directly introducing fairness cues. For temporal self attention, FAME introduces a region constrained attention mask combined with time decay weighting, which enhances intra-region coherence while suppressing irrelevant inter-region interactions. For cross attention, it reweights tokens to region matching scores by incorporating fairness sensitive similarity masks derived from debiasing prompt embeddings. Together, these modulations keep fairness-sensitive semantics tied to the right visual regions and prevent temporal drift across frames. Extensive experiments on new VE fairness-oriented benchmark \textit{FairVE} demonstrate that FAME achieves stronger fairness alignment and semantic fidelity, surpassing existing VE baselines.</li>
</ul>

<h3>Title: CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents</h3>
<ul>
<li><strong>Authors: </strong>Zesen Liu, Zhixiang Zhang, Yuchong Xie, Dongdong She</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22963">https://arxiv.org/abs/2510.22963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22963">https://arxiv.org/pdf/2510.22963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22963]] CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents(https://arxiv.org/abs/2510.22963)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>LLM-powered agents often use prompt compression to reduce inference costs, but this introduces a new security risk. Compression modules, which are optimized for efficiency rather than safety, can be manipulated by adversarial inputs, causing semantic drift and altering LLM behavior. This work identifies prompt compression as a novel attack surface and presents CompressionAttack, the first framework to exploit it. CompressionAttack includes two strategies: HardCom, which uses discrete adversarial edits for hard compression, and SoftCom, which performs latent-space perturbations for soft compression. Experiments on multiple LLMs show up to 80% attack success and 98% preference flips, while remaining highly stealthy and transferable. Case studies in VSCode Cline and Ollama confirm real-world impact, and current defenses prove ineffective, highlighting the need for stronger protections.</li>
</ul>

<h3>Title: Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Liling Yang, Ning Chen, Jun Yue, Yidan Liu, Jiayi Ma, Pedram Ghamisi, Antonio Plaza, Leyuan Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22964">https://arxiv.org/abs/2510.22964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22964">https://arxiv.org/pdf/2510.22964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22964]] Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges(https://arxiv.org/abs/2510.22964)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, interpretability</a></li>
<li><strong>Abstract: </strong>Foundation models have transformed natural language processing and computer vision, and their impact is now reshaping remote sensing image analysis. With powerful generalization and transfer learning capabilities, they align naturally with the multimodal, multi-resolution, and multi-temporal characteristics of remote sensing data. To address unique challenges in the field, multimodal geospatial foundation models (GFMs) have emerged as a dedicated research frontier. This survey delivers a comprehensive review of multimodal GFMs from a modality-driven perspective, covering five core visual and vision-language modalities. We examine how differences in imaging physics and data representation shape interaction design, and we analyze key techniques for alignment, integration, and knowledge transfer to tackle modality heterogeneity, distribution shifts, and semantic gaps. Advances in training paradigms, architectures, and task-specific adaptation strategies are systematically assessed alongside a wealth of emerging benchmarks. Representative multimodal visual and vision-language GFMs are evaluated across ten downstream tasks, with insights into their architectures, performance, and application scenarios. Real-world case studies, spanning land cover mapping, agricultural monitoring, disaster response, climate studies, and geospatial intelligence, demonstrate the practical potential of GFMs. Finally, we outline pressing challenges in domain generalization, interpretability, efficiency, and privacy, and chart promising avenues for future research.</li>
</ul>

<h3>Title: MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Ning, Xixun Lin, Fang Fang, Yanan Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22967">https://arxiv.org/abs/2510.22967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22967">https://arxiv.org/pdf/2510.22967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22967]] MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs(https://arxiv.org/abs/2510.22967)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Large Language Models (LLMs) raises critical concerns about the factual accuracy of their outputs, especially in high-risk domains such as biomedicine, law, and education. Existing evaluation methods for short texts often fail on long-form content due to complex reasoning chains, intertwined perspectives, and cumulative information. To address this, we propose a systematic approach integrating large-scale long-form datasets, multi-agent verification mechanisms, and weighted evaluation metrics. We construct LongHalluQA, a Chinese long-form factuality dataset; and develop MAD-Fact, a debate-based multi-agent verification system. We introduce a fact importance hierarchy to capture the varying significance of claims in long-form texts. Experiments on two benchmarks show that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content. Our work provides a structured framework for evaluating and enhancing factual reliability in long-form LLM outputs, guiding their safe deployment in sensitive domains.</li>
</ul>

<h3>Title: Measuring Teaching with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Michael Hardy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22968">https://arxiv.org/abs/2510.22968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22968">https://arxiv.org/pdf/2510.22968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22968]] Measuring Teaching with LLMs(https://arxiv.org/abs/2510.22968)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Objective and scalable measurement of teaching quality is a persistent challenge in education. While Large Language Models (LLMs) offer potential, general-purpose models have struggled to reliably apply complex, authentic classroom observation instruments. This paper uses custom LLMs built on sentence-level embeddings, an architecture better suited for the long-form, interpretive nature of classroom transcripts than conventional subword tokenization. We systematically evaluate five different sentence embeddings under a data-efficient training regime designed to prevent overfitting. Our results demonstrate that these specialized models can achieve human-level and even super-human performance with expert human ratings above 0.65 and surpassing the average human-human rater correlation. Further, through analysis of annotation context windows, we find that more advanced models-those better aligned with human judgments-attribute a larger share of score variation to lesson-level features rather than isolated utterances, challenging the sufficiency of single-turn annotation paradigms. Finally, to assess external validity, we find that aggregate model scores align with teacher value-added measures, indicating they are capturing features relevant to student learning. However, this trend does not hold at the individual item level, suggesting that while the models learn useful signals, they have not yet achieved full generalization. This work establishes a viable and powerful new methodology for AI-driven instructional measurement, offering a path toward providing scalable, reliable, and valid feedback for educator development.</li>
</ul>

<h3>Title: VALA: Learning Latent Anchors for Training-Free and Temporally Consistent</h3>
<ul>
<li><strong>Authors: </strong>Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Longbing Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22970">https://arxiv.org/abs/2510.22970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22970">https://arxiv.org/pdf/2510.22970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22970]] VALA: Learning Latent Anchors for Training-Free and Temporally Consistent(https://arxiv.org/abs/2510.22970)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in training-free video editing have enabled lightweight and precise cross-frame generation by leveraging pre-trained text-to-image diffusion models. However, existing methods often rely on heuristic frame selection to maintain temporal consistency during DDIM inversion, which introduces manual bias and reduces the scalability of end-to-end inference. In this paper, we propose~\textbf{VALA} (\textbf{V}ariational \textbf{A}lignment for \textbf{L}atent \textbf{A}nchors), a variational alignment module that adaptively selects key frames and compresses their latent features into semantic anchors for consistent video editing. To learn meaningful assignments, VALA propose a variational framework with a contrastive learning objective. Therefore, it can transform cross-frame latent representations into compressed latent anchors that preserve both content and temporal coherence. Our method can be fully integrated into training-free text-to-image based video editing models. Extensive experiments on real-world video editing benchmarks show that VALA achieves state-of-the-art performance in inversion fidelity, editing quality, and temporal consistency, while offering improved efficiency over prior methods.</li>
</ul>

<h3>Title: Advancing Honeywords for Real-World Authentication Security</h3>
<ul>
<li><strong>Authors: </strong>Sudiksha Das, Ashish Kundu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22971">https://arxiv.org/abs/2510.22971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22971">https://arxiv.org/pdf/2510.22971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22971]] Advancing Honeywords for Real-World Authentication Security(https://arxiv.org/abs/2510.22971)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Introduced by Juels and Rivest in 2013, Honeywords, which are decoy passwords stored alongside a real password, appear to be a proactive method to help detect password credentials misuse. However, despite over a decade of research, this technique has not been adopted by major authentication platforms. This position paper argues that the core concept of Honeywords has potential but requires more research on issues such as flatness, integration, and reliability, in order to be a practical deployable solution. This paper examines the current work on Honeyword generation, attacker modeling, and honeychecker architecture, analyzing the subproblems that have been addressed and ongoing issues that prevent this system from being more widely used. The paper then suggests a deployable framework that combines the attacker-resilient, context-aware decoy creation that Honeywords provide with easy integration into existing systems. Honeywords will only move from an academic idea to a practical security tool if technical advances are paired with secure and straightforward architectures, along with adaptive response handling and detailed configuration checks.</li>
</ul>

<h3>Title: Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method</h3>
<ul>
<li><strong>Authors: </strong>Bohan Li, Xin Jin, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22973">https://arxiv.org/abs/2510.22973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22973">https://arxiv.org/pdf/2510.22973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22973]] Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method(https://arxiv.org/abs/2510.22973)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Driving scene generation is a critical domain for autonomous driving, enabling downstream applications, including perception and planning evaluation. Occupancy-centric methods have recently achieved state-of-the-art results by offering consistent conditioning across frames and modalities; however, their performance heavily depends on annotated occupancy data, which still remains scarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic occupancy dataset to date, constructed from the widely used Nuplan benchmark. Its scale and diversity facilitate not only large-scale generative modeling but also autonomous driving downstream applications. Based on this dataset, we develop a unified framework that jointly synthesizes high-quality semantic occupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates a spatio-temporal disentangled architecture to support high-fidelity spatial expansion and temporal forecasting of 4D dynamic occupancy. To bridge modal gaps, we further propose two novel techniques: a Gaussian splatting-based sparse point map rendering strategy that enhances multi-view video generation, and a sensor-aware embedding strategy that explicitly models LiDAR sensor properties for realistic multi-LiDAR simulation. Extensive experiments demonstrate that our method achieves superior generation fidelity and scalability compared to existing approaches, and validates its practical value in downstream tasks. Repo: this https URL</li>
</ul>

<h3>Title: VoMP: Predicting Volumetric Mechanical Property Fields</h3>
<ul>
<li><strong>Authors: </strong>Rishit Dagli, Donglai Xiang, Vismay Modi, Charles Loop, Clement Fuji Tsang, Anka He Chen, Anita Hu, Gavriel State, David I.W. Levin, Maria Shugrina</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22975">https://arxiv.org/abs/2510.22975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22975">https://arxiv.org/pdf/2510.22975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22975]] VoMP: Predicting Volumetric Mechanical Property Fields(https://arxiv.org/abs/2510.22975)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Physical simulation relies on spatially-varying mechanical properties, often laboriously hand-crafted. VoMP is a feed-forward method trained to predict Young's modulus ($E$), Poisson's ratio ($\nu$), and density ($\rho$) throughout the volume of 3D objects, in any representation that can be rendered and voxelized. VoMP aggregates per-voxel multi-view features and passes them to our trained Geometry Transformer to predict per-voxel material latent codes. These latents reside on a manifold of physically plausible materials, which we learn from a real-world dataset, guaranteeing the validity of decoded per-voxel materials. To obtain object-level training data, we propose an annotation pipeline combining knowledge from segmented 3D datasets, material databases, and a vision-language model, along with a new benchmark. Experiments show that VoMP estimates accurate volumetric properties, far outperforming prior art in accuracy and speed.</li>
</ul>

<h3>Title: The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Chenlong Yin, Zeyang Sha, Shiwen Cui, Changhua Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22977">https://arxiv.org/abs/2510.22977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22977">https://arxiv.org/pdf/2510.22977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22977]] The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination(https://arxiv.org/abs/2510.22977)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key strategy for building Agents that "think then act." However, recent observations, like OpenAI's o3, suggest a paradox: stronger reasoning often coincides with increased hallucination, yet no prior work has systematically examined whether reasoning enhancement itself causes tool hallucination. To address this gap, we pose the central question: Does strengthening reasoning increase tool hallucination? To answer this, we introduce SimpleToolHalluBench, a diagnostic benchmark measuring tool hallucination in two failure modes: (i) no tool available, and (ii) only distractor tools available. Through controlled experiments, we establish three key findings. First, we demonstrate a causal relationship: progressively enhancing reasoning through RL increases tool hallucination proportionally with task performance gains. Second, this effect transcends overfitting - training on non-tool tasks (e.g., mathematics) still amplifies subsequent tool hallucination. Third, the effect is method-agnostic, appearing when reasoning is instilled via supervised fine-tuning and when it is merely elicited at inference by switching from direct answers to step-by-step thinking. We also evaluate mitigation strategies including Prompt Engineering and Direct Preference Optimization (DPO), revealing a fundamental reliability-capability trade-off: reducing hallucination consistently degrades utility. Mechanistically, Reasoning RL disproportionately collapses tool-reliability-related representations, and hallucinations surface as amplified divergences concentrated in late-layer residual streams. These findings reveal that current reasoning enhancement methods inherently amplify tool hallucination, highlighting the need for new training objectives that jointly optimize for capability and reliability.</li>
</ul>

<h3>Title: QoSGMAA: A Robust Multi-Order Graph Attention and Adversarial Framework for Sparse QoS Prediction</h3>
<ul>
<li><strong>Authors: </strong>Guanchen Du, Jianlong Xu, Mingtong Li, Ruiqi Wang, Qianqing Guo, Caiyi Chen, Qingcao Dai, Yuxiang Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22982">https://arxiv.org/abs/2510.22982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22982">https://arxiv.org/pdf/2510.22982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22982]] QoSGMAA: A Robust Multi-Order Graph Attention and Adversarial Framework for Sparse QoS Prediction(https://arxiv.org/abs/2510.22982)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of internet technologies, network services have become critical for delivering diverse and reliable applications to users. However, the exponential growth in the number of available services has resulted in many similar offerings, posing significant challenges in selecting optimal services. Predicting Quality of Service (QoS) accurately thus becomes a fundamental prerequisite for ensuring reliability and user satisfaction. However, existing QoS prediction methods often fail to capture rich contextual information and exhibit poor performance under extreme data sparsity and structural noise. To bridge this gap, we propose a novel architecture, QoSMGAA, specifically designed to enhance prediction accuracy in complex and noisy network service environments. QoSMGAA integrates a multi-order attention mechanism to aggregate extensive contextual data and predict missing QoS values effectively. Additionally, our method incorporates adversarial neural networks to perform autoregressive supervised learning based on transformed interaction matrices. To capture complex, higher-order interactions among users and services, we employ a discrete sampling technique leveraging the Gumbel-Softmax method to generate informative negative samples. Comprehensive experimental validation conducted on large-scale real-world datasets demonstrates that our proposed model significantly outperforms existing baseline methods, highlighting its strong potential for practical deployment in service selection and recommendation scenarios.</li>
</ul>

<h3>Title: LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Md Mostafijur Rahman, Radu Marculescu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.22995">https://arxiv.org/abs/2510.22995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.22995">https://arxiv.org/pdf/2510.22995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.22995]] LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation(https://arxiv.org/abs/2510.22995)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>U-shaped networks output logits at multiple spatial scales, each capturing a different blend of coarse context and fine detail. Yet, training still treats these logits in isolation - either supervising only the final, highest-resolution logits or applying deep supervision with identical loss weights at every scale - without exploring mixed-scale combinations. Consequently, the decoder output misses the complementary cues that arise only when coarse and fine predictions are fused. To address this issue, we introduce LoMix (Logits Mixing), a NAS-inspired, differentiable plug-and-play module that generates new mixed-scale outputs and learns how exactly each of them should guide the training process. More precisely, LoMix mixes the multi-scale decoder logits with four lightweight fusion operators: addition, multiplication, concatenation, and attention-based weighted fusion, yielding a rich set of synthetic mutant maps. Every original or mutant map is given a softplus loss weight that is co-optimized with network parameters, mimicking a one-step architecture search that automatically discovers the most useful scales, mixtures, and operators. Plugging LoMix into recent U-shaped architectures (i.e., PVT-V2-B2 backbone with EMCAD decoder) on Synapse 8-organ dataset improves DICE by +4.2% over single-output supervision, +2.2% over deep supervision, and +1.5% over equally weighted additive fusion, all with zero inference overhead. When training data are scarce (e.g., one or two labeled scans), the advantage grows to +9.23%, underscoring LoMix's data efficiency. Across four benchmarks and diverse U-shaped networks, LoMiX improves DICE by up to +13.5% over single-output supervision, confirming that learnable weighted mixed-scale fusion generalizes broadly while remaining data efficient, fully interpretable, and overhead-free at inference. Our code is available at this https URL.</li>
</ul>

<h3>Title: Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures</h3>
<ul>
<li><strong>Authors: </strong>Shenran Wang, Timothy Tin-Long Tse, Jian Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23006">https://arxiv.org/abs/2510.23006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23006">https://arxiv.org/pdf/2510.23006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23006]] Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures(https://arxiv.org/abs/2510.23006)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We perform in-depth evaluations of in-context learning (ICL) on state-of-the-art transformer, state-space, and hybrid large language models over two categories of knowledge-based ICL tasks. Using a combination of behavioral probing and intervention-based methods, we have discovered that, while LLMs of different architectures can behave similarly in task performance, their internals could remain different. We discover that function vectors (FVs) responsible for ICL are primarily located in the self-attention and Mamba layers, and speculate that Mamba2 uses a different mechanism from FVs to perform ICL. FVs are more important for ICL involving parametric knowledge retrieval, but not for contextual knowledge understanding. Our work contributes to a more nuanced understanding across architectures and task types. Methodologically, our approach also highlights the importance of combining both behavioural and mechanistic analyses to investigate LLM capabilities.</li>
</ul>

<h3>Title: UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Pan Zhao, Hui Yuan, Chongzhen Tian, Tian Guo, Raouf Hamzaoui, Zhigeng Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23009">https://arxiv.org/abs/2510.23009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23009">https://arxiv.org/pdf/2510.23009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23009]] UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds(https://arxiv.org/abs/2510.23009)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Lossy compression of point clouds reduces storage and transmission costs; however, it inevitably leads to irreversible distortion in geometry structure and attribute information. To address these issues, we propose a unified geometry and attribute enhancement (UGAE) framework, which consists of three core components: post-geometry enhancement (PoGE), pre-attribute enhancement (PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-based sparse convolutional U-Net is used to reconstruct the geometry structure with high precision by predicting voxel occupancy probabilities. Building on the refined geometry structure, PAE introduces an innovative enhanced geometry-guided recoloring strategy, which uses a detail-aware K-Nearest Neighbors (DA-KNN) method to achieve accurate recoloring and effectively preserve high-frequency details before attribute compression. Finally, at the decoder side, PoAE uses an attribute residual prediction network with a weighted mean squared error (W-MSE) loss to enhance the quality of high-frequency regions while maintaining the fidelity of low-frequency regions. UGAE significantly outperformed existing methods on three benchmark datasets: 8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29), UGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savings for geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with 56.88% BD-bitrate savings for attributes on the Y component. Additionally, it improved perceptual quality significantly.</li>
</ul>

<h3>Title: LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sammriddh Gupta, Sonit Singh, Aditya Joshi, Mira Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23011">https://arxiv.org/abs/2510.23011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23011">https://arxiv.org/pdf/2510.23011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23011]] LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models(https://arxiv.org/abs/2510.23011)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Language educators strive to create a rich experience for learners, while they may be restricted in the extend of feedback and practice they can provide. We present the design and development of LangLingual, a conversational agent built using the LangChain framework and powered by Large Language Models. The system is specifically designed to provide real-time, grammar-focused feedback, generate context-aware language exercises and track learner proficiency over time. The paper discusses the architecture, implementation and evaluation of LangLingual in detail. The results indicate strong usability, positive learning outcomes and encouraging learner engagement.</li>
</ul>

<h3>Title: Softmax is $1/2$-Lipschitz: A tight bound across all $\ell_p$ norms</h3>
<ul>
<li><strong>Authors: </strong>Pravin Nair</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23012">https://arxiv.org/abs/2510.23012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23012">https://arxiv.org/pdf/2510.23012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23012]] Softmax is $1/2$-Lipschitz: A tight bound across all $\ell_p$ norms(https://arxiv.org/abs/2510.23012)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The softmax function is a basic operator in machine learning and optimization, used in classification, attention mechanisms, reinforcement learning, game theory, and problems involving log-sum-exp terms. Existing robustness guarantees of learning models and convergence analysis of optimization algorithms typically consider the softmax operator to have a Lipschitz constant of $1$ with respect to the $\ell_2$ norm. In this work, we prove that the softmax function is contractive with the Lipschitz constant $1/2$, uniformly across all $\ell_p$ norms with $p \ge 1$. We also show that the local Lipschitz constant of softmax attains $1/2$ for $p = 1$ and $p = \infty$, and for $p \in (1,\infty)$, the constant remains strictly below $1/2$ and the supremum $1/2$ is achieved only in the limit. To our knowledge, this is the first comprehensive norm-uniform analysis of softmax Lipschitz continuity. We demonstrate how the sharper constant directly improves a range of existing theoretical results on robustness and convergence. We further validate the sharpness of the $1/2$ Lipschitz constant of the softmax operator through empirical studies on attention-based architectures (ViT, GPT-2, Qwen3-8B) and on stochastic policies in reinforcement learning.</li>
</ul>

<h3>Title: Sentinel: Dynamic Knowledge Distillation for Personalized Federated Intrusion Detection in Heterogeneous IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Gurpreet Singh, Keshav Sood, P. Rajalakshmi, Yong Xiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23019">https://arxiv.org/abs/2510.23019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23019">https://arxiv.org/pdf/2510.23019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23019]] Sentinel: Dynamic Knowledge Distillation for Personalized Federated Intrusion Detection in Heterogeneous IoT Networks(https://arxiv.org/abs/2510.23019)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) offers a privacy-preserving paradigm for machine learning, but its application in intrusion detection systems (IDS) within IoT networks is challenged by severe class imbalance, non-IID data, and high communication this http URL challenges severely degrade the performance of conventional FL methods in real-world network traffic classification. To overcome these limitations, we propose Sentinel, a personalized federated IDS (pFed-IDS) framework that incorporates a dual-model architecture on each client, consisting of a personalized teacher and a lightweight shared student model. This design effectively balances deep local adaptation with efficient global model consensus while preserving client privacy by transmitting only the compact student model, thus reducing communication costs. Sentinel integrates three key mechanisms to ensure robust performance: bidirectional knowledge distillation with adaptive temperature scaling, multi-faceted feature alignment, and class-balanced loss functions. Furthermore, the server employs normalized gradient aggregation with equal client weighting to enhance fairness and mitigate client drift. Extensive experiments on the IoTID20 and 5GNIDD benchmark datasets demonstrate that Sentinel significantly outperforms state-of-the-art federated methods, establishing a new performance benchmark, especially under extreme data heterogeneity, while maintaining communication efficiency.</li>
</ul>

<h3>Title: M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Huixuan Zhang, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23020">https://arxiv.org/abs/2510.23020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23020">https://arxiv.org/pdf/2510.23020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23020]] M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark(https://arxiv.org/abs/2510.23020)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image models are known to struggle with generating images that perfectly align with textual prompts. Several previous studies have focused on evaluating image-text alignment in text-to-image generation. However, these evaluations either address overly simple scenarios, especially overlooking the difficulty of prompts with multiple different instances belonging to the same category, or they introduce metrics that do not correlate well with human evaluation. In this study, we introduce M$^3$T2IBench, a large-scale, multi-category, multi-instance, multi-relation along with an object-detection-based evaluation metric, $AlignScore$, which aligns closely with human evaluation. Our findings reveal that current open-source text-to-image models perform poorly on this challenging benchmark. Additionally, we propose the Revise-Then-Enforce approach to enhance image-text alignment. This training-free post-editing method demonstrates improvements in image-text alignment across a broad range of diffusion models. \footnote{Our code and data has been released in supplementary material and will be made publicly available after the paper is accepted.}</li>
</ul>

<h3>Title: UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization</h3>
<ul>
<li><strong>Authors: </strong>Huixuan Zhang, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23023">https://arxiv.org/abs/2510.23023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23023">https://arxiv.org/pdf/2510.23023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23023]] UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization(https://arxiv.org/abs/2510.23023)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>With the rapid proliferation of image generative models, the authenticity of digital images has become a significant concern. While existing studies have proposed various methods for detecting AI-generated content, current benchmarks are limited in their coverage of diverse generative models and image categories, often overlooking end-to-end image editing and artistic images. To address these limitations, we introduce UniAIDet, a unified and comprehensive benchmark that includes both photographic and artistic images. UniAIDet covers a wide range of generative models, including text-to-image, image-to-image, image inpainting, image editing, and deepfake models. Using UniAIDet, we conduct a comprehensive evaluation of various detection methods and answer three key research questions regarding generalization capability and the relation between detection and localization. Our benchmark and analysis provide a robust foundation for future research.</li>
</ul>

<h3>Title: A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem</h3>
<ul>
<li><strong>Authors: </strong>Chuan Yan, Zeng Li, Kunlin Cai, Liuhuo Wan, Ruomai Ren, Yiran Shen, Guangdong Bai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23024">https://arxiv.org/abs/2510.23024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23024">https://arxiv.org/pdf/2510.23024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23024]] A Multi-Store Privacy Measurement of Virtual Reality App Ecosystem(https://arxiv.org/abs/2510.23024)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, biometric</a></li>
<li><strong>Abstract: </strong>Virtual Reality (VR) has gained increasing traction among various domains in recent years, with major companies such as Meta, Pico, and Microsoft launching their application stores to support third-party developers in releasing their applications (or simply apps). These apps offer rich functionality but inherently collect privacy-sensitive data, such as user biometrics, behaviors, and the surrounding environment. Nevertheless, there is still a lack of domain-specific regulations to govern the data handling of VR apps, resulting in significant variations in their privacy practices among app stores. In this work, we present the first comprehensive multi-store study of privacy practices in the current VR app ecosystem, covering a large-scale dataset involving 6,565 apps collected from five major app stores. We assess both declarative and behavioral privacy practices of VR apps, using a multi-faceted approach based on natural language processing, reverse engineering, and static analysis. Our assessment reveals significant privacy compliance issues across all stores, underscoring the premature status of privacy protection in this rapidly growing ecosystem. For instance, one third of apps fail to declare their use of sensitive data, and 21.5\% of apps neglect to provide valid privacy policies. Our work sheds light on the status quo of privacy protection within the VR app ecosystem for the first time. Our findings should raise an alert to VR app developers and users, and encourage store operators to implement stringent regulations on privacy compliance among VR apps.</li>
</ul>

<h3>Title: Nested AutoRegressive Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Wu, Xuhui Fan, Zhangkai Wu, Longbing Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23028">https://arxiv.org/abs/2510.23028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23028">https://arxiv.org/pdf/2510.23028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23028]] Nested AutoRegressive Models(https://arxiv.org/abs/2510.23028)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>AutoRegressive (AR) models have demonstrated competitive performance in image generation, achieving results comparable to those of diffusion models. However, their token-by-token image generation mechanism remains computationally intensive and existing solutions such as VAR often lead to limited sample diversity. In this work, we propose a Nested AutoRegressive~(NestAR) model, which proposes nested AutoRegressive architectures in generating images. NestAR designs multi-scale modules in a hierarchical order. These different scaled modules are constructed in an AR architecture, where one larger-scale module is conditioned on outputs from its previous smaller-scale module. Within each module, NestAR uses another AR structure to generate ``patches'' of tokens. The proposed nested AR architecture reduces the overall complexity from $\mathcal{O}(n)$ to $\mathcal{O}(\log n)$ in generating $n$ image tokens, as well as increases image diversities. NestAR further incorporates flow matching loss to use continuous tokens, and develops objectives to coordinate these multi-scale modules in model training. NestAR achieves competitive image generation performance while significantly lowering computational cost.</li>
</ul>

<h3>Title: Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures</h3>
<ul>
<li><strong>Authors: </strong>Gokulnath Rajendran, Suman Deb, Anupam Chattopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23034">https://arxiv.org/abs/2510.23034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23034">https://arxiv.org/pdf/2510.23034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23034]] Efficient and Encrypted Inference using Binarized Neural Networks within In-Memory Computing Architectures(https://arxiv.org/abs/2510.23034)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Binarized Neural Networks (BNNs) are a class of deep neural networks designed to utilize minimal computational resources, which drives their popularity across various applications. Recent studies highlight the potential of mapping BNN model parameters onto emerging non-volatile memory technologies, specifically using crossbar architectures, resulting in improved inference performance compared to traditional CMOS implementations. However, the common practice of protecting model parameters from theft attacks by storing them in an encrypted format and decrypting them at runtime introduces significant computational overhead, thus undermining the core principles of in-memory computing, which aim to integrate computation and storage. This paper presents a robust strategy for protecting BNN model parameters, particularly within in-memory computing frameworks. Our method utilizes a secret key derived from a physical unclonable function to transform model parameters prior to storage in the crossbar. Subsequently, the inference operations are performed on the encrypted weights, achieving a very special case of Fully Homomorphic Encryption (FHE) with minimal runtime overhead. Our analysis reveals that inference conducted without the secret key results in drastically diminished performance, with accuracy falling below 15%. These results validate the effectiveness of our protection strategy in securing BNNs within in-memory computing architectures while preserving computational efficiency.</li>
</ul>

<h3>Title: A high-capacity linguistic steganography based on entropy-driven rank-token mapping</h3>
<ul>
<li><strong>Authors: </strong>Jun Jiang, Weiming Zhang, Nenghai Yu, Kejiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23035">https://arxiv.org/abs/2510.23035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23035">https://arxiv.org/pdf/2510.23035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23035]] A high-capacity linguistic steganography based on entropy-driven rank-token mapping(https://arxiv.org/abs/2510.23035)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, generative</a></li>
<li><strong>Abstract: </strong>Linguistic steganography enables covert communication through embedding secret messages into innocuous texts; however, current methods face critical limitations in payload capacity and security. Traditional modification-based methods introduce detectable anomalies, while retrieval-based strategies suffer from low embedding capacity. Modern generative steganography leverages language models to generate natural stego text but struggles with limited entropy in token predictions, further constraining capacity. To address these issues, we propose an entropy-driven framework called RTMStega that integrates rank-based adaptive coding and context-aware decompression with normalized entropy. By mapping secret messages to token probability ranks and dynamically adjusting sampling via context-aware entropy-based adjustments, RTMStega achieves a balance between payload capacity and imperceptibility. Experiments across diverse datasets and models demonstrate that RTMStega triples the payload capacity of mainstream generative steganography, reduces processing time by over 50%, and maintains high text quality, offering a trustworthy solution for secure and efficient covert communication.</li>
</ul>

<h3>Title: KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Xudong Yang, Jincheng Li, Kaiwen Xing, Zhenjia Xiao, Mingjian Duan, Weili Han, Hu Xiong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23036">https://arxiv.org/abs/2510.23036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23036">https://arxiv.org/pdf/2510.23036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23036]] KAPG: Adaptive Password Guessing via Knowledge-Augmented Generation(https://arxiv.org/abs/2510.23036)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>As the primary mechanism of digital authentication, user-created passwords exhibit common patterns and regularities that can be learned from leaked datasets. Password choices are profoundly shaped by external factors, including social contexts, cultural trends, and popular vocabulary. Prevailing password guessing models primarily emphasize patterns derived from leaked passwords, while neglecting these external influences -- a limitation that hampers their adaptability to emerging password trends and erodes their effectiveness over time. To address these challenges, we propose KAPG, a knowledge-augmented password guessing framework that adaptively integrates external lexical knowledge into the guessing process. KAPG couples internal statistical knowledge learned from leaked passwords with external information that reflects real-world trends. By using password prefixes as anchors for knowledge lookup, it dynamically injects relevant external cues during generation while preserving the structural regularities of authentic passwords. Experiments on twelve leaked datasets show that KnowGuess achieves average improvements of 36.5\% and 74.7\% over state-of-the-art models in intra-site and cross-site scenarios, respectively. Further analyses of password overlap and model efficiency highlight its robustness and computational efficiency. To counter these attacks, we further develop KAPSM, a trend-aware and site-specific password strength meter. Experiments demonstrate that KAPSM significantly outperforms existing tools in accuracy across diverse evaluation settings.</li>
</ul>

<h3>Title: Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, Hongkun Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23038">https://arxiv.org/abs/2510.23038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23038">https://arxiv.org/pdf/2510.23038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23038]] Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning(https://arxiv.org/abs/2510.23038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are widely used as judges to evaluate response quality, providing a scalable alternative to human evaluation. However, most LLM judges operate solely on intrinsic text-based reasoning, limiting their ability to verify complex constraints or perform accurate computation. Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks, we propose TIR-Judge, an end-to-end RL framework for training LLM judges that integrates a code executor for precise evaluation. TIR-Judge is built on three principles: (i) diverse training across verifiable and non-verifiable domains, (ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii) iterative RL that bootstraps directly from the initial model without distillation. On seven public benchmarks, TIR-Judge surpasses strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and achieves listwise performance comparable to Claude-Opus-4 despite having only 8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled judge trajectories, matches the performance of distilled variants, demonstrating that tool-augmented judges can self-evolve through iterative reinforcement learning.</li>
</ul>

<h3>Title: LLM Meets Diffusion: A Hybrid Framework for Crystal Material Generation</h3>
<ul>
<li><strong>Authors: </strong>Subhojyoti Khastagir, Kishalay Das, Pawan Goyal, Seung-Cheol Lee, Satadeep Bhattacharjee, Niloy Ganguly</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23040">https://arxiv.org/abs/2510.23040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23040">https://arxiv.org/pdf/2510.23040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23040]] LLM Meets Diffusion: A Hybrid Framework for Crystal Material Generation(https://arxiv.org/abs/2510.23040)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling have shown significant promise in designing novel periodic crystal structures. Existing approaches typically rely on either large language models (LLMs) or equivariant denoising models, each with complementary strengths: LLMs excel at handling discrete atomic types but often struggle with continuous features such as atomic positions and lattice parameters, while denoising models are effective at modeling continuous variables but encounter difficulties in generating accurate atomic compositions. To bridge this gap, we propose CrysLLMGen, a hybrid framework that integrates an LLM with a diffusion model to leverage their complementary strengths for crystal material generation. During sampling, CrysLLMGen first employs a fine-tuned LLM to produce an intermediate representation of atom types, atomic coordinates, and lattice structure. While retaining the predicted atom types, it passes the atomic coordinates and lattice structure to a pre-trained equivariant diffusion model for refinement. Our framework outperforms state-of-the-art generative models across several benchmark tasks and datasets. Specifically, CrysLLMGen not only achieves a balanced performance in terms of structural and compositional validity but also generates more stable and novel materials compared to LLM-based and denoisingbased models Furthermore, CrysLLMGen exhibits strong conditional generation capabilities, effectively producing materials that satisfy user-defined constraints. Code is available at this https URL</li>
</ul>

<h3>Title: SwiftTS: A Swift Selection Framework for Time Series Pre-trained Models via Multi-task Meta-Learning</h3>
<ul>
<li><strong>Authors: </strong>Tengxue Zhang, Biao Ouyang, Yang Shu, Xinyang Chen, Chenjuan Guo, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23051">https://arxiv.org/abs/2510.23051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23051">https://arxiv.org/pdf/2510.23051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23051]] SwiftTS: A Swift Selection Framework for Time Series Pre-trained Models via Multi-task Meta-Learning(https://arxiv.org/abs/2510.23051)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pre-trained models exhibit strong generalization to various downstream tasks. However, given the numerous models available in the model hub, identifying the most suitable one by individually fine-tuning is time-consuming. In this paper, we propose \textbf{SwiftTS}, a swift selection framework for time series pre-trained models. To avoid expensive forward propagation through all candidates, SwiftTS adopts a learning-guided approach that leverages historical dataset-model performance pairs across diverse horizons to predict model performance on unseen datasets. It employs a lightweight dual-encoder architecture that embeds time series and candidate models with rich characteristics, computing patchwise compatibility scores between data and model embeddings for efficient selection. To further enhance the generalization across datasets and horizons, we introduce a horizon-adaptive expert composition module that dynamically adjusts expert weights, and the transferable cross-task learning with cross-dataset and cross-horizon task sampling to enhance out-of-distribution (OOD) robustness. Extensive experiments on 14 downstream datasets and 8 pre-trained models demonstrate that SwiftTS achieves state-of-the-art performance in time series pre-trained model selection.</li>
</ul>

<h3>Title: Knocking-Heads Attention</h3>
<ul>
<li><strong>Authors: </strong>Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23052">https://arxiv.org/abs/2510.23052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23052">https://arxiv.org/pdf/2510.23052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23052]] Knocking-Heads Attention(https://arxiv.org/abs/2510.23052)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to "knock" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.</li>
</ul>

<h3>Title: AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Wang, Suman Raj, Rajkumar Buyya</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23053">https://arxiv.org/abs/2510.23053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23053">https://arxiv.org/pdf/2510.23053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23053]] AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing(https://arxiv.org/abs/2510.23053)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed's practical applicability for large-scale UAV-MEC deployments.</li>
</ul>

<h3>Title: zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks</h3>
<ul>
<li><strong>Authors: </strong>Paritosh Ramanan, H.M. Mohaimanul Islam, Abhiram Reddy Alugula</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23060">https://arxiv.org/abs/2510.23060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23060">https://arxiv.org/pdf/2510.23060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23060]] zkSTAR: A zero knowledge system for time series attack detection enforcing regulatory compliance in critical infrastructure networks(https://arxiv.org/abs/2510.23060)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Industrial control systems (ICS) form the operational backbone of critical infrastructure networks (CIN) such as power grids, water supply systems, and gas pipelines. As cyber threats to these systems escalate, regulatory agencies are imposing stricter compliance requirements to ensure system-wide security and reliability. A central challenge, however, is enabling regulators to verify the effectiveness of detection mechanisms without requiring utilities to disclose sensitive operational data. In this paper, we introduce zkSTAR, a cyberattack detection framework that leverages zk-SNARKs to reconcile these requirements and enable provable detection guarantees while preserving data confidentiality. Our approach builds on established residual-based statistical hypothesis testing methods applied to state-space detection models. Specifically, we design a two-pronged zk-SNARK architecture that enforces temporal consistency of the state-space dynamics and statistical consistency of the detection tests, allowing regulators to temporally verify alarm correctness without visibility into utility-level data. We formally analyze the soundness and zero knowledge properties of our framework and validate its practical feasibility through computational experiments on real-world ICS datasets. As a result, our work demonstrates a scalable, privacy-preserving alternative for regulatory compliance for ICS driven critical infrastructure networks.</li>
</ul>

<h3>Title: Quality-Aware Translation Tagging in Multilingual RAG system</h3>
<ul>
<li><strong>Authors: </strong>Hoyeon Moon, Byeolhee Kim, Nikhil Verma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23070">https://arxiv.org/abs/2510.23070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23070">https://arxiv.org/pdf/2510.23070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23070]] Quality-Aware Translation Tagging in Multilingual RAG system(https://arxiv.org/abs/2510.23070)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English documents and translates them into the query language for low-resource settings. However, poor translation quality degrades response generation performance. Existing approaches either assume sufficient translation quality or utilize the rewriting method, which introduces factual distortion and hallucinations. To mitigate these problems, we propose Quality-Aware Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation quality along three dimensions-semantic equivalence, grammatical accuracy, and naturalness&fluency-and attach these scores as metadata without altering the original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs ranging from 2.4B to 14B parameters, covering two low-resource languages (Korean and Finnish) and one high-resource language (Chinese). QTT-RAG outperforms the baselines by preserving factual integrity while enabling generator models to make informed decisions based on translation reliability. This approach allows for effective usage of cross-lingual documents in low-resource settings with limited native language documents, offering a practical and robust solution across multilingual domains.</li>
</ul>

<h3>Title: Fast-MIA: Efficient and Scalable Membership Inference for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hiromu Takahashi, Shotaro Ishihara</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23074">https://arxiv.org/abs/2510.23074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23074">https://arxiv.org/pdf/2510.23074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23074]] Fast-MIA: Efficient and Scalable Membership Inference for LLMs(https://arxiv.org/abs/2510.23074)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>We propose Fast-MIA (this https URL), a Python library for efficiently evaluating membership inference attacks (MIA) against Large Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due to growing concerns over copyright, security, and data privacy, and has attracted increasing research attention. However, the progress of this research is significantly hindered by two main obstacles: (1) the high computational cost of inference in LLMs, and (2) the lack of standardized and maintained implementations of MIA methods, which makes large-scale empirical comparison difficult. To address these challenges, our library provides fast batch inference and includes implementations of representative MIA methods under a unified evaluation framework. This library supports easy implementation of reproducible benchmarks with simple configuration and extensibility. We release Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and transparent research on LLMs.</li>
</ul>

<h3>Title: Strategies for Robust Deep Learning Based Deformable Registration</h3>
<ul>
<li><strong>Authors: </strong>Joel Honkamaa, Pekka Marttinen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23079">https://arxiv.org/abs/2510.23079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23079">https://arxiv.org/pdf/2510.23079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23079]] Strategies for Robust Deep Learning Based Deformable Registration(https://arxiv.org/abs/2510.23079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning based deformable registration methods have become popular in recent years. However, their ability to generalize beyond training data distribution can be poor, significantly hindering their usability. LUMIR brain registration challenge for Learn2Reg 2025 aims to advance the field by evaluating the performance of the registration on contrasts and modalities different from those included in the training set. Here we describe our submission to the challenge, which proposes a very simple idea for significantly improving robustness by transforming the images into MIND feature space before feeding them into the model. In addition, a special ensembling strategy is proposed that shows a small but consistent improvement.</li>
</ul>

<h3>Title: A Survey on LLM Mid-training</h3>
<ul>
<li><strong>Authors: </strong>Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23081">https://arxiv.org/abs/2510.23081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23081">https://arxiv.org/pdf/2510.23081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23081]] A Survey on LLM Mid-training(https://arxiv.org/abs/2510.23081)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in foundation models have highlighted the significant benefits of multi-stage training, with a particular emphasis on the emergence of mid-training as a vital stage that bridges pre-training and post-training. Mid-training is distinguished by its use of intermediate data and computational resources, systematically enhancing specified capabilities such as mathematics, coding, reasoning, and long-context extension, while maintaining foundational competencies. This survey provides a formal definition of mid-training for large language models (LLMs) and investigates optimization frameworks that encompass data curation, training strategies, and model architecture optimization. We analyze mainstream model implementations in the context of objective-driven interventions, illustrating how mid-training serves as a distinct and critical stage in the progressive development of LLM capabilities. By clarifying the unique contributions of mid-training, this survey offers a comprehensive taxonomy and actionable insights, supporting future research and innovation in the advancement of LLMs.</li>
</ul>

<h3>Title: MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Suchan Lee, Jihoon Choi, Sohyeon Lee, Minseok Song, Bong-Gyu Jang, Hwanjo Yu, Soyeon Caren Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23090">https://arxiv.org/abs/2510.23090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23090">https://arxiv.org/pdf/2510.23090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23090]] MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models(https://arxiv.org/abs/2510.23090)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances have investigated the use of pretrained large language models (LLMs) for time-series forecasting by aligning numerical inputs with LLM embedding spaces. However, existing multimodal approaches often overlook the distinct statistical properties and temporal dependencies that are fundamental to time-series data. To bridge this gap, we propose MAP4TS, a novel Multi-Aspect Prompting Framework that explicitly incorporates classical time-series analysis into the prompt design. Our framework introduces four specialized prompt components: a Global Domain Prompt that conveys dataset-level context, a Local Domain Prompt that encodes recent trends and series-specific behaviors, and a pair of Statistical and Temporal Prompts that embed handcrafted insights derived from autocorrelation (ACF), partial autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined with raw time-series embeddings and passed through a cross-modality alignment module to produce unified representations, which are then processed by an LLM and projected for final forecasting. Extensive experiments across eight diverse datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based methods. Our ablation studies further reveal that prompt-aware designs significantly enhance performance stability and that GPT-2 backbones, when paired with structured prompts, outperform larger models like LLaMA in long-term forecasting tasks.</li>
</ul>

<h3>Title: Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhang, Xin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23101">https://arxiv.org/abs/2510.23101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23101">https://arxiv.org/pdf/2510.23101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23101]] Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for Directed Greybox Fuzzing(https://arxiv.org/abs/2510.23101)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Directed greybox fuzzing (DGF) aims to efficiently trigger bugs at specific target locations by prioritizing seeds whose execution paths are more likely to mutate into triggering target bugs. However, existing DGF approaches suffer from imprecise probability calculations due to their reliance on complex distance metrics derived from static analysis. The over-approximations inherent in static analysis cause a large number of irrelevant execution paths to be mistakenly considered to potentially mutate into triggering target bugs, significantly reducing fuzzing efficiency. We propose to replace static analysis-based distance metrics with precise call stack representations. Call stacks represent precise control flows, thereby avoiding false information in static analysis. We leverage large language models (LLMs) to predict vulnerability-triggering call stacks for guiding seed prioritization. Our approach constructs call graphs through static analysis to identify methods that can potentially reach target locations, then utilizes LLMs to predict the most likely call stack sequence that triggers the vulnerability. Seeds whose execution paths have higher overlap with the predicted call stack are prioritized for mutation. This is the first work to integrate LLMs into the core seed prioritization mechanism of DGF. We implement our approach and evaluate it against several state-of-the-art fuzzers. On a suite of real-world programs, our approach triggers vulnerabilities $1.86\times$ to $3.09\times$ faster compared to baselines. In addition, our approach identifies 10 new vulnerabilities and 2 incomplete fixes in the latest versions of programs used in our controlled experiments through directed patch testing, with 10 assigned CVE IDs.</li>
</ul>

<h3>Title: Leveraging Hierarchical Organization for Medical Multi-document Summarization</h3>
<ul>
<li><strong>Authors: </strong>Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23104">https://arxiv.org/abs/2510.23104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23104">https://arxiv.org/pdf/2510.23104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23104]] Leveraging Hierarchical Organization for Medical Multi-document Summarization(https://arxiv.org/abs/2510.23104)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical multi-document summarization (MDS) is a complex task that requires effectively managing cross-document relationships. This paper investigates whether incorporating hierarchical structures in the inputs of MDS can improve a model's ability to organize and contextualize information across documents compared to traditional flat summarization methods. We investigate two ways of incorporating hierarchical organization across three large language models (LLMs), and conduct comprehensive evaluations of the resulting summaries using automated metrics, model-based metrics, and domain expert evaluation of preference, understandability, clarity, complexity, relevance, coverage, factuality, and coherence. Our results show that human experts prefer model-generated summaries over human-written summaries. Hierarchical approaches generally preserve factuality, coverage, and coherence of information, while also increasing human preference for summaries. Additionally, we examine whether simulated judgments from GPT-4 align with human judgments, finding higher agreement along more objective evaluation facets. Our findings demonstrate that hierarchical structures can improve the clarity of medical summaries generated by models while maintaining content coverage, providing a practical way to improve human preference for generated summaries.</li>
</ul>

<h3>Title: Sampling from Energy distributions with Target Concrete Score Identity</h3>
<ul>
<li><strong>Authors: </strong>Sergei Kholkin, Francisco Vargas, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23106">https://arxiv.org/abs/2510.23106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23106">https://arxiv.org/pdf/2510.23106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23106]] Sampling from Energy distributions with Target Concrete Score Identity(https://arxiv.org/abs/2510.23106)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce the Target Concrete Score Identity Sampler (TCSIS), a method for sampling from unnormalized densities on discrete state spaces by learning the reverse dynamics of a Continuous-Time Markov Chain (CTMC). Our approach builds on a forward in time CTMC with a uniform noising kernel and relies on the proposed Target Concrete Score Identity, which relates the concrete score, the ratio of marginal probabilities of two states, to a ratio of expectations of Boltzmann factors under the forward uniform diffusion kernel. This formulation enables Monte Carlo estimation of the concrete score without requiring samples from the target distribution or computation of the partition function. We approximate the concrete score with a neural network and propose two algorithms: Self-Normalized TCSIS and Unbiased TCSIS. Finally, we demonstrate the effectiveness of TCSIS on problems from statistical physics.</li>
</ul>

<h3>Title: Flexing in 73 Languages: A Single Small Model for Multilingual Inflection</h3>
<ul>
<li><strong>Authors: </strong>Tom√°≈° Sourada, Jana Strakov√°</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23114">https://arxiv.org/abs/2510.23114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23114">https://arxiv.org/pdf/2510.23114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23114]] Flexing in 73 Languages: A Single Small Model for Multilingual Inflection(https://arxiv.org/abs/2510.23114)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a compact, single-model approach to multilingual inflection, the task of generating inflected word forms from base lemmas to express grammatical categories. Our model, trained jointly on data from 73 languages, is lightweight, robust to unseen words, and outperforms monolingual baselines in most languages. This demonstrates the effectiveness of multilingual modeling for inflection and highlights its practical benefits: simplifying deployment by eliminating the need to manage and retrain dozens of separate monolingual models. In addition to the standard SIGMORPHON shared task benchmarks, we evaluate our monolingual and multilingual models on 73 Universal Dependencies (UD) treebanks, extracting lemma-tag-form triples and their frequency counts. To ensure realistic data splits, we introduce a novel frequency-weighted, lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack of an open-source, general-purpose, multilingual morphological inflection system capable of handling unseen words across a wide range of languages, including Czech. All code is publicly released at: this https URL.</li>
</ul>

<h3>Title: Residual Diffusion Bridge Model for Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Hebaixu Wang, Jing Zhang, Haoyang Chen, Haonan Guo, Di Wang, Jiayi Ma, Bo Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23116">https://arxiv.org/abs/2510.23116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23116">https://arxiv.org/pdf/2510.23116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23116]] Residual Diffusion Bridge Model for Image Restoration(https://arxiv.org/abs/2510.23116)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion bridge models establish probabilistic paths between arbitrary paired distributions and exhibit great potential for universal image restoration. Most existing methods merely treat them as simple variants of stochastic interpolants, lacking a unified analytical perspective. Besides, they indiscriminately reconstruct images through global noise injection and removal, inevitably distorting undegraded regions due to imperfect reconstruction. To address these challenges, we propose the Residual Diffusion Bridge Model (RDBM). Specifically, we theoretically reformulate the stochastic differential equations of generalized diffusion bridge and derive the analytical formulas of its forward and reverse processes. Crucially, we leverage the residuals from given distributions to modulate the noise injection and removal, enabling adaptive restoration of degraded regions while preserving intact others. Moreover, we unravel the fundamental mathematical essence of existing bridge models, all of which are special cases of RDBM and empirically demonstrate the optimality of our proposed models. Extensive experiments are conducted to demonstrate the state-of-the-art performance of our method both qualitatively and quantitatively across diverse image restoration tasks. Code is publicly available at this https URL.</li>
</ul>

<h3>Title: Task-Agnostic Fusion of Time Series and Imagery for Earth Observation</h3>
<ul>
<li><strong>Authors: </strong>Gianfranco Basile, Johannes Jakubik, Benedikt Blumenstiel, Thomas Brunschwiler, Juan Bernabe Moreno</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23118">https://arxiv.org/abs/2510.23118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23118">https://arxiv.org/pdf/2510.23118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23118]] Task-Agnostic Fusion of Time Series and Imagery for Earth Observation(https://arxiv.org/abs/2510.23118)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance. Our approach explores deterministic and learned strategies for time series quantization and then leverages a masked correlation learning objective, aligning discrete image and time series tokens in a unified representation space. Instantiated in the Earth observation domain, the pretrained model generates consistent global temperature profiles from satellite imagery and is validated through counterfactual experiments. Across downstream tasks, our task-agnostic pretraining outperforms task-specific fusion by 6\% in R$^2$ and 2\% in RMSE on average, and exceeds baseline methods by 50\% in R$^2$ and 12\% in RMSE. Finally, we analyze gradient sensitivity across modalities, providing insights into model robustness. Code, data, and weights will be released under a permissive license.</li>
</ul>

<h3>Title: Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Ziqiang Cui, Dugang Liu, Yuhua Li, Xiuqiang He, Ruixuan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23123">https://arxiv.org/abs/2510.23123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23123">https://arxiv.org/pdf/2510.23123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23123]] Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation(https://arxiv.org/abs/2510.23123)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). LoRA essentially describes the projection of an input space into a low-dimensional output space, with the dimensionality determined by the LoRA rank. In standard LoRA, all input tokens share the same weights and undergo an identical input-output projection. This limits LoRA's ability to capture token-specific information due to the inherent semantic differences among tokens. To address this limitation, we propose Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts LoRA weights according to the input token, thereby learning token-wise input-output projections in an end-to-end manner. Formally, the weights of TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA weights but achieves more granular adaptation by learning token-wise LoRA weights (i.e., token-wise input-output projections). Extensive experiments across multiple models and datasets demonstrate that TopLoRA consistently outperforms LoRA and its variants. The code is available at this https URL.</li>
</ul>

<h3>Title: DeepSalt: Bridging Laboratory and Satellite Spectra through Domain Adaptation and Knowledge Distillation for Large-Scale Soil Salinity Estimation</h3>
<ul>
<li><strong>Authors: </strong>Rupasree Dey, Abdul Matin, Everett Lewark, Tanjim Bin Faruk, Andrei Bachinin, Sam Leuthold, M. Francesca Cotrufo, Shrideep Pallickara, Sangmi Lee Pallickara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23124">https://arxiv.org/abs/2510.23124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23124">https://arxiv.org/pdf/2510.23124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23124]] DeepSalt: Bridging Laboratory and Satellite Spectra through Domain Adaptation and Knowledge Distillation for Large-Scale Soil Salinity Estimation(https://arxiv.org/abs/2510.23124)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Soil salinization poses a significant threat to both ecosystems and agriculture because it limits plants' ability to absorb water and, in doing so, reduces crop productivity. This phenomenon alters the soil's spectral properties, creating a measurable relationship between salinity and light reflectance that enables remote monitoring. While laboratory spectroscopy provides precise measurements, its reliance on in-situ sampling limits scalability to regional or global levels. Conversely, hyperspectral satellite imagery enables wide-area observation but lacks the fine-grained interpretability of laboratory instruments. To bridge this gap, we introduce DeepSalt, a deep-learning-based spectral transfer framework that leverages knowledge distillation and a novel Spectral Adaptation Unit to transfer high-resolution spectral insights from laboratory-based spectroscopy to satellite-based hyperspectral sensing. Our approach eliminates the need for extensive ground sampling while enabling accurate, large-scale salinity estimation, as demonstrated through comprehensive empirical benchmarks. DeepSalt achieves significant performance gains over methods without explicit domain adaptation, underscoring the impact of the proposed Spectral Adaptation Unit and the knowledge distillation strategy. The model also effectively generalized to unseen geographic regions, explaining a substantial portion of the salinity variance.</li>
</ul>

<h3>Title: DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Wang, Wenhao Li, Ji Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23144">https://arxiv.org/abs/2510.23144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23144">https://arxiv.org/pdf/2510.23144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23144]] DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios(https://arxiv.org/abs/2510.23144)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>3D object detection from multi-view images in traffic scenarios has garnered significant attention in recent years. Many existing approaches rely on object queries that are generated from 3D reference points to localize objects. However, a limitation of these methods is that some reference points are often far from the target object, which can lead to false positive detections. In this paper, we propose a depth-guided query generator for 3D object detection (DQ3D) that leverages depth information and 2D detections to ensure that reference points are sampled from the surface or interior of the object. Furthermore, to address partially occluded objects in current frame, we introduce a hybrid attention mechanism that fuses historical detection results with depth-guided queries, thereby forming hybrid queries. Evaluation on the nuScenes dataset demonstrates that our method outperforms the baseline by 6.3\% in terms of mean Average Precision (mAP) and 4.3\% in the NuScenes Detection Score (NDS).</li>
</ul>

<h3>Title: Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI</h3>
<ul>
<li><strong>Authors: </strong>Aryan Mathur, Asaduddin Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23148">https://arxiv.org/abs/2510.23148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23148">https://arxiv.org/pdf/2510.23148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23148]] Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI(https://arxiv.org/abs/2510.23148)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning agents often struggle when tasks require understanding both vision and language. Conventional architectures typically isolate perception (for example, CNN-based visual encoders) from decision-making (policy networks). This separation can be inefficient, since the policy's failures do not directly help the perception module learn what is important. To address this, we implement the Perception-Decision Interleaving Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that alternates between perception and decision layers within a single transformer. This interleaving allows feedback from decision-making to refine perceptual features dynamically. In addition, we integrate a contrastive loss inspired by CLIP to align textual mission embeddings with visual scene features. We evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that the approach achieves more stable rewards and stronger alignment compared to a standard PPO baseline. The results suggest that interleaved transformer encoders are a promising direction for developing more integrated autonomous agents.</li>
</ul>

<h3>Title: AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes</h3>
<ul>
<li><strong>Authors: </strong>Sixian Liu, Chen Xu, Qiang Wang, Donghai Shi, Yiwen Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23151">https://arxiv.org/abs/2510.23151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23151">https://arxiv.org/pdf/2510.23151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23151]] AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes(https://arxiv.org/abs/2510.23151)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal camera-LiDAR fusion technology has found extensive application in 3D object detection, demonstrating encouraging performance. However, existing methods exhibit significant performance degradation in challenging scenarios characterized by sensor degradation or environmental disturbances. We propose a novel Adaptive Gated Fusion (AG-Fusion) approach that selectively integrates cross-modal knowledge by identifying reliable patterns for robust detection in complex scenes. Specifically, we first project features from each modality into a unified BEV space and enhance them using a window-based attention mechanism. Subsequently, an adaptive gated fusion module based on cross-modal attention is designed to integrate these features into reliable BEV representations robust to challenging environments. Furthermore, we construct a new dataset named Excavator3D (E3D) focusing on challenging excavator operation scenarios to benchmark performance in complex conditions. Our method not only achieves competitive performance on the standard KITTI dataset with 93.92% accuracy, but also significantly outperforms the baseline by 24.88% on the challenging E3D dataset, demonstrating superior robustness to unreliable modal information in complex industrial scenes.</li>
</ul>

<h3>Title: ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix</h3>
<ul>
<li><strong>Authors: </strong>Zile Yang, Ling Li, Na Di, Jinlong Pang, Yao Zhou, Hao Cheng, Bo Han, Jiaheng Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23160">https://arxiv.org/abs/2510.23160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23160">https://arxiv.org/pdf/2510.23160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23160]] ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix(https://arxiv.org/abs/2510.23160)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs) to domain-specific instructions by training on a carefully curated subset of high-quality instruction-response pairs, typically drawn from a larger dataset that often contains many low-quality or noisy samples. However, existing quality-first paradigms often overlook valuable signals in discarded low-quality data and rely on imperfect quality filters. We introduce ENTP (Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a framework that revitalizes low-quality corpora through symbolic purification and neural reconstruction. The symbolic module identifies and prunes noisy samples based on statistical priors, while the neural component synthesizes enriched instruction-response pairs by leveraging latent representations and model knowledge. This neural-symbolic synergy enhances data informativeness and diversity. Experiments show that ENTP-augmented datasets, constructed exclusively from low-quality data, outperform 13 established data-selection baselines across five instruction-following benchmarks, and even surpass fine-tuning on the full original dataset (approximately 300K examples). Our results highlight the untapped potential of low-quality data and underscore the importance of intelligent purification and synthesis for efficient instruction alignment.</li>
</ul>

<h3>Title: Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hang Lei, Shengyi Zong, Zhaoyan Li, Ziren Zhou, Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23163">https://arxiv.org/abs/2510.23163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23163">https://arxiv.org/pdf/2510.23163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23163]] Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs(https://arxiv.org/abs/2510.23163)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The screenplay serves as the foundation for television production, defining narrative structure, character development, and dialogue. While Large Language Models (LLMs) show great potential in creative writing, direct end-to-end generation approaches often fail to produce well-crafted screenplays. We argue this failure stems from forcing a single model to simultaneously master two disparate capabilities: creative narrative construction and rigid format adherence. The resulting outputs may mimic superficial style but lack the deep structural integrity and storytelling substance required for professional use. To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage Refinement (DSR), a decomposed framework that decouples creative narrative generation from format conversion. The first stage transforms a brief outline into rich, novel-style prose. The second stage refines this narrative into a professionally formatted screenplay. This separation enables the model to specialize in one distinct capability at each stage. A key challenge in implementing DSR is the scarcity of paired outline-to-novel training data. We address this through hybrid data synthesis: reverse synthesis deconstructs existing screenplays into structured inputs, while forward synthesis leverages these inputs to generate high-quality narrative texts as training targets. Blind evaluations by professional screenwriters show that DSR achieves a 75% win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of human-level performance. Our work demonstrates that decomposed generation architecture with tailored data synthesis effectively specializes LLMs in complex creative domains.</li>
</ul>

<h3>Title: SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations</h3>
<ul>
<li><strong>Authors: </strong>Shuai Huang, Wenxuan Zhao, Jun Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23182">https://arxiv.org/abs/2510.23182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23182">https://arxiv.org/pdf/2510.23182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23182]] SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations(https://arxiv.org/abs/2510.23182)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at this https URL.</li>
</ul>

<h3>Title: DREaM: Drug-Drug Relation Extraction via Transfer Learning Method</h3>
<ul>
<li><strong>Authors: </strong>Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23189">https://arxiv.org/abs/2510.23189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23189">https://arxiv.org/pdf/2510.23189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23189]] DREaM: Drug-Drug Relation Extraction via Transfer Learning Method(https://arxiv.org/abs/2510.23189)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Relation extraction between drugs plays a crucial role in identifying drug drug interactions and predicting side effects. The advancement of machine learning methods in relation extraction, along with the development of large medical text databases, has enabled the low cost extraction of such relations compared to other approaches that typically require expert knowledge. However, to the best of our knowledge, there are limited datasets specifically designed for drug drug relation extraction currently available. Therefore, employing transfer learning becomes necessary to apply machine learning methods in this domain. In this study, we propose DREAM, a method that first employs a trained relation extraction model to discover relations between entities and then applies this model to a corpus of medical texts to construct an ontology of drug relationships. The extracted relations are subsequently validated using a large language model. Quantitative results indicate that the LLM agreed with 71 of the relations extracted from a subset of PubMed abstracts. Furthermore, our qualitative analysis indicates that this approach can uncover ambiguities in the medical domain, highlighting the challenges inherent in relation extraction in this field.</li>
</ul>

<h3>Title: Evaluation of Vision-LLMs in Surveillance Video</h3>
<ul>
<li><strong>Authors: </strong>Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23190">https://arxiv.org/abs/2510.23190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23190">https://arxiv.org/pdf/2510.23190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23190]] Evaluation of Vision-LLMs in Surveillance Video(https://arxiv.org/abs/2510.23190)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The widespread use of cameras in our society has created an overwhelming amount of video data, far exceeding the capacity for human monitoring. This presents a critical challenge for public safety and security, as the timely detection of anomalous or criminal events is crucial for effective response and prevention. The ability for an embodied agent to recognize unexpected events is fundamentally tied to its capacity for spatial reasoning. This paper investigates the spatial reasoning of vision-language models (VLMs) by framing anomalous action recognition as a zero-shot, language-grounded task, addressing the embodied perception challenge of interpreting dynamic 3D scenes from sparse 2D video. Specifically, we investigate whether small, pre-trained vision--LLMs can act as spatially-grounded, zero-shot anomaly detectors by converting video into text descriptions and scoring labels via textual entailment. We evaluate four open models on UCF-Crime and RWF-2000 under prompting and privacy-preserving conditions. Few-shot exemplars can improve accuracy for some models, but may increase false positives, and privacy filters -- especially full-body GAN transforms -- introduce inconsistencies that degrade accuracy. These results chart where current vision--LLMs succeed (simple, spatially salient events) and where they falter (noisy spatial cues, identity obfuscation). Looking forward, we outline concrete paths to strengthen spatial grounding without task-specific training: structure-aware prompts, lightweight spatial memory across clips, scene-graph or 3D-pose priors during description, and privacy methods that preserve action-relevant geometry. This positions zero-shot, language-grounded pipelines as adaptable building blocks for embodied, real-world video understanding. Our implementation for evaluating VLMs is publicly available at: this https URL</li>
</ul>

<h3>Title: VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Hoonhee Cho, Jae-Young Kang, Giwon Lee, Hyemin Yang, Heejun Park, Seokwoo Jung, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23205">https://arxiv.org/abs/2510.23205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23205">https://arxiv.org/pdf/2510.23205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23205]] VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting(https://arxiv.org/abs/2510.23205)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm that unifies perception, prediction, and planning into a holistic, data-driven framework. However, achieving robustness to varying camera viewpoints, a common real-world challenge due to diverse vehicle configurations, remains an open problem. In this work, we propose VR-Drive, a novel E2E-AD framework that addresses viewpoint generalization by jointly learning 3D scene reconstruction as an auxiliary task to enable planning-aware view synthesis. Unlike prior scene-specific synthesis approaches, VR-Drive adopts a feed-forward inference strategy that supports online training-time augmentation from sparse views without additional annotations. To further improve viewpoint consistency, we introduce a viewpoint-mixed memory bank that facilitates temporal interaction across multiple viewpoints and a viewpoint-consistent distillation strategy that transfers knowledge from original to synthesized views. Trained in a fully end-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and improves planning under viewpoint shifts. In addition, we release a new benchmark dataset to evaluate E2E-AD performance under novel camera viewpoints, enabling comprehensive analysis. Our results demonstrate that VR-Drive is a scalable and robust solution for the real-world deployment of end-to-end autonomous driving systems.</li>
</ul>

<h3>Title: Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks</h3>
<ul>
<li><strong>Authors: </strong>Amal Abed, Ivan Lukic, J√∂rg K.H. Franke, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23208">https://arxiv.org/abs/2510.23208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23208">https://arxiv.org/pdf/2510.23208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23208]] Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks(https://arxiv.org/abs/2510.23208)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown impressive promise in code generation, yet their progress remains limited by the shortage of large-scale datasets that are both diverse and well-aligned with human reasoning. Most existing resources pair problems with solutions, but omit the intermediate thought process that guides coding. To close this gap, we present a scalable synthetic data generation pipeline that produces nearly 800k instruction-reasoning-code-test quadruplets. Each sample combines a task, a step-by-step reasoning trace, a working solution, and executable tests, enabling models to learn not just the what but also the how of problem solving. Our pipeline combines four key components: curated contest problems, web-mined content filtered by relevance classifiers, data expansion guided by reasoning patterns, and multi-stage execution-based validation. A genetic mutation algorithm further increases task diversity while maintaining consistency between reasoning traces and code implementations. Our key finding is that fine-tuning LLMs on this dataset yields consistent improvements on coding benchmarks. Beyond raw accuracy, reasoning-aware data can substitute for model scaling, generalize across architectures, and outperform leading open-source alternatives under identical sample budgets. Our work establishes reasoning-centered synthetic data generation as an efficient approach for advancing coding capabilities in LLMs. We publish our dataset and generation pipeline to facilitate further research.</li>
</ul>

<h3>Title: Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports</h3>
<ul>
<li><strong>Authors: </strong>Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23217">https://arxiv.org/abs/2510.23217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23217">https://arxiv.org/pdf/2510.23217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23217]] Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports(https://arxiv.org/abs/2510.23217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations</li>
</ul>

<h3>Title: Through the Lens: Benchmarking Deepfake Detectors Against Moir√©-Induced Distortions</h3>
<ul>
<li><strong>Authors: </strong>Razaib Tariq, Minji Heo, Simon S. Woo, Shahroz Tariq</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23225">https://arxiv.org/abs/2510.23225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23225">https://arxiv.org/pdf/2510.23225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23225]] Through the Lens: Benchmarking Deepfake Detectors Against Moir√©-Induced Distortions(https://arxiv.org/abs/2510.23225)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deepfake detection remains a pressing challenge, particularly in real-world settings where smartphone-captured media from digital screens often introduces Moir√© artifacts that can distort detection outcomes. This study systematically evaluates state-of-the-art (SOTA) deepfake detectors on Moir√©-affected videos, an issue that has received little attention. We collected a dataset of 12,832 videos, spanning 35.64 hours, from the Celeb-DF, DFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world conditions, including varying screens, smartphones, lighting setups, and camera angles. To further examine the influence of Moir√© patterns on deepfake detection, we conducted additional experiments using our DeepMoir√©Fake, referred to as (DMF) dataset and two synthetic Moir√© generation techniques. Across 15 top-performing detectors, our results show that Moir√© artifacts degrade performance by as much as 25.4%, while synthetically generated Moir√© patterns lead to a 21.4% drop in accuracy. Surprisingly, demoir√©ing methods, intended as a mitigation approach, instead worsened the problem, reducing accuracy by up to 17.2%. These findings underscore the urgent need for detection models that can robustly handle Moir√© distortions alongside other realworld challenges, such as compression, sharpening, and blurring. By introducing the DMF dataset, we aim to drive future research toward closing the gap between controlled experiments and practical deepfake detection.</li>
</ul>

<h3>Title: Robust Iterative Learning Hidden Quantum Markov Models</h3>
<ul>
<li><strong>Authors: </strong>Ning Ning</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph, stat.CO, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23237">https://arxiv.org/abs/2510.23237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23237">https://arxiv.org/pdf/2510.23237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23237]] Robust Iterative Learning Hidden Quantum Markov Models(https://arxiv.org/abs/2510.23237)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hidden Quantum Markov Models (HQMMs) extend classical Hidden Markov Models to the quantum domain, offering a powerful probabilistic framework for modeling sequential data with quantum coherence. However, existing HQMM learning algorithms are highly sensitive to data corruption and lack mechanisms to ensure robustness under adversarial perturbations. In this work, we introduce the Adversarially Corrupted HQMM (AC-HQMM), which formalizes robustness analysis by allowing a controlled fraction of observation sequences to be adversarially corrupted. To learn AC-HQMMs, we propose the Robust Iterative Learning Algorithm (RILA), a derivative-free method that integrates a Remove Corrupted Rows by Entropy Filtering (RCR-EF) module with an iterative stochastic resampling procedure for physically valid Kraus operator updates. RILA incorporates L1-penalized likelihood objectives to enhance stability, resist overfitting, and remain effective under non-differentiable conditions. Across multiple HQMM and HMM benchmarks, RILA demonstrates superior convergence stability, corruption resilience, and preservation of physical validity compared to existing algorithms, establishing a principled and efficient approach for robust quantum sequential learning.</li>
</ul>

<h3>Title: Autoregressive Styled Text Image Generation, but Make it Reliable</h3>
<ul>
<li><strong>Authors: </strong>Carmine Zaccagnino, Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23240">https://arxiv.org/abs/2510.23240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23240">https://arxiv.org/pdf/2510.23240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23240]] Autoregressive Styled Text Image Generation, but Make it Reliable(https://arxiv.org/abs/2510.23240)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Generating faithful and readable styled text images (especially for Styled Handwritten Text generation - HTG) is an open problem with several possible applications across graphic design, document understanding, and image editing. A lot of research effort in this task is dedicated to developing strategies that reproduce the stylistic characteristics of a given writer, with promising results in terms of style fidelity and generalization achieved by the recently proposed Autoregressive Transformer paradigm for HTG. However, this method requires additional inputs, lacks a proper stop mechanism, and might end up in repetition loops, generating visual artifacts. In this work, we rethink the autoregressive formulation by framing HTG as a multimodal prompt-conditioned generation task, and tackle the content controllability issues by introducing special textual input tokens for better alignment with the visual ones. Moreover, we devise a Classifier-Free-Guidance-based strategy for our autoregressive model. Through extensive experimental validation, we demonstrate that our approach, dubbed Eruku, compared to previous solutions requires fewer inputs, generalizes better to unseen styles, and follows more faithfully the textual prompt, improving content adherence.</li>
</ul>

<h3>Title: Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Stefan M. Fischer, Johannes Kiechle, Laura Daza, Lina Felsner, Richard Osuala, Daniel M. Lang, Karim Lekadir, Jan C. Peeken, Julia A. Schnabel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23241">https://arxiv.org/abs/2510.23241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23241">https://arxiv.org/pdf/2510.23241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23241]] Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation(https://arxiv.org/abs/2510.23241)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this work, we introduce Progressive Growing of Patch Size, an automatic curriculum learning approach for 3D medical image segmentation. Our approach progressively increases the patch size during model training, resulting in an improved class balance for smaller patch sizes and accelerated convergence of the training process. We evaluate our curriculum approach in two settings: a resource-efficient mode and a performance mode, both regarding Dice score performance and computational costs across 15 diverse and popular 3D medical image segmentation tasks. The resource-efficient mode matches the Dice score performance of the conventional constant patch size sampling baseline with a notable reduction in training time to only 44%. The performance mode improves upon constant patch size segmentation results, achieving a statistically significant relative mean performance gain of 1.28% in Dice Score. Remarkably, across all 15 tasks, our proposed performance mode manages to surpass the constant patch size baseline in Dice Score performance, while simultaneously reducing training time to only 89%. The benefits are particularly pronounced for highly imbalanced tasks such as lesion segmentation tasks. Rigorous experiments demonstrate that our performance mode not only improves mean segmentation performance but also reduces performance variance, yielding more trustworthy model comparison. Furthermore, our findings reveal that the proposed curriculum sampling is not tied to a specific architecture but represents a broadly applicable strategy that consistently boosts performance across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In summary, we show that this simple yet elegant transformation on input data substantially improves both Dice Score performance and training runtime, while being compatible across diverse segmentation backbones.</li>
</ul>

<h3>Title: A Video Is Not Worth a Thousand Words</h3>
<ul>
<li><strong>Authors: </strong>Sam Pollard, Michael Wray</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23253">https://arxiv.org/abs/2510.23253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23253">https://arxiv.org/pdf/2510.23253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23253]] A Video Is Not Worth a Thousand Words(https://arxiv.org/abs/2510.23253)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As we become increasingly dependent on vision language models (VLMs) to answer questions about the world around us, there is a significant amount of research devoted to increasing both the difficulty of video question answering (VQA) datasets, and the context lengths of the models that they evaluate. The reliance on large language models as backbones has lead to concerns about potential text dominance, and the exploration of interactions between modalities is underdeveloped. How do we measure whether we're heading in the right direction, with the complexity that multi-modal models introduce? We propose a joint method of computing both feature attributions and modality scores based on Shapley values, where both the features and modalities are arbitrarily definable. Using these metrics, we compare $6$ VLM models of varying context lengths on $4$ representative datasets, focusing on multiple-choice VQA. In particular, we consider video frames and whole textual elements as equal features in the hierarchy, and the multiple-choice VQA task as an interaction between three modalities: video, question and answer. Our results demonstrate a dependence on text and show that the multiple-choice VQA task devolves into a model's ability to ignore distractors. Code available at this https URL.</li>
</ul>

<h3>Title: GCAO: Group-driven Clustering via Gravitational Attraction and Optimization</h3>
<ul>
<li><strong>Authors: </strong>Qi Li, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23259">https://arxiv.org/abs/2510.23259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23259">https://arxiv.org/pdf/2510.23259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23259]] GCAO: Group-driven Clustering via Gravitational Attraction and Optimization(https://arxiv.org/abs/2510.23259)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional clustering algorithms often struggle with high-dimensional and non-uniformly distributed data, where low-density boundary samples are easily disturbed by neighboring clusters, leading to unstable and distorted clustering results. To address this issue, we propose a Group-driven Clustering via Gravitational Attraction and Optimization (GCAO) algorithm. GCAO introduces a group-level optimization mechanism that aggregates low-density boundary points into collaboratively moving groups, replacing the traditional point-based contraction process. By combining local density estimation with neighborhood topology, GCAO constructs effective gravitational interactions between groups and their surroundings, enhancing boundary clarity and structural consistency. Using groups as basic motion units, a gravitational contraction strategy ensures globally stable and directionally consistent convergence. Experiments on multiple high-dimensional datasets demonstrate that GCAO outperforms 11 representative clustering methods, achieving average improvements of 37.13%, 52.08%, 44.98%, and 38.81% in NMI, ARI, Homogeneity, and ACC, respectively, while maintaining competitive efficiency and scalability. These results highlight GCAO's superiority in preserving cluster integrity, enhancing boundary separability, and ensuring robust performance on complex data distributions.</li>
</ul>

<h3>Title: Toward Interpretable Evaluation Measures for Time Series Segmentation</h3>
<ul>
<li><strong>Authors: </strong>F√©lix Chavelli, Paul Boniol, Micha√´l Thomazo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23261">https://arxiv.org/abs/2510.23261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23261">https://arxiv.org/pdf/2510.23261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23261]] Toward Interpretable Evaluation Measures for Time Series Segmentation(https://arxiv.org/abs/2510.23261)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Time series segmentation is a fundamental task in analyzing temporal data across various domains, from human activity recognition to energy monitoring. While numerous state-of-the-art methods have been developed to tackle this problem, the evaluation of their performance remains critically limited. Existing measures predominantly focus on change point accuracy or rely on point-based measures such as Adjusted Rand Index (ARI), which fail to capture the quality of the detected segments, ignore the nature of errors, and offer limited interpretability. In this paper, we address these shortcomings by introducing two novel evaluation measures: WARI (Weighted Adjusted Rand Index), that accounts for the position of segmentation errors, and SMS (State Matching Score), a fine-grained measure that identifies and scores four fundamental types of segmentation errors while allowing error-specific weighting. We empirically validate WARI and SMS on synthetic and real-world benchmarks, showing that they not only provide a more accurate assessment of segmentation quality but also uncover insights, such as error provenance and type, that are inaccessible with traditional measures.</li>
</ul>

<h3>Title: PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xinhai Wang, Shu Yang, Liangyu Wang, Lin Zhang, Huanyi Xie, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23264">https://arxiv.org/abs/2510.23264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23264">https://arxiv.org/pdf/2510.23264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23264]] PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization(https://arxiv.org/abs/2510.23264)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Circuit discovery, which involves identifying sparse and task-relevant subnetworks in pre-trained language models, is a cornerstone of mechanistic interpretability. Automated Circuit Discovery (ACDC) has emerged as a pivotal methodology in circuit discovery, but its application to large language models is severely limited by computational inefficiency and prohibitively high memory requirements. Although several accelerated approaches have been proposed, they primarily rely on linear approximations to ACDC, which significantly compromises analytical faithfulness. Our proposed method for accelerating automated circuit discovery, Per Attention Head Quantization (PAHQ), takes a fundamentally different approach by optimizing the efficiency of each individual patching operation. PAHQ leverages a fundamental alignment between activation patching and mixed-precision quantization (MPQ): interpretability analysis through patching essentially performs targeted ablation studies. Therefore, we can maintain high precision exclusively for investigated components while safely reducing precision elsewhere in the network. PAHQ-accelerated ACDC reduces runtime by up to 80\% and memory consumption by up to 30\% compared to unaccelerated ACDC while maintaining faithfulness. Importantly, our method readily integrates with existing edge-based circuit discovery techniques by modifying the attention computation mechanism. This training-free approach provides a practical and novel pathway for accelerating mechanistic interpretability methods. Our code is available at this https URL.</li>
</ul>

<h3>Title: Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Aljafari, Ismail Alturki, Ahmed Mori, Yehya Kadumi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23271">https://arxiv.org/abs/2510.23271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23271">https://arxiv.org/pdf/2510.23271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23271]] Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding(https://arxiv.org/abs/2510.23271)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mubeen is a proprietary Arabic language model developed by MASARAT SA, optimized for deep understanding of Arabic linguistics, Islamic studies, and cultural heritage. Trained on an extensive collection of authentic Arabic sources significantly expanded by digitizing historical manuscripts via a proprietary Arabic OCR engine, the model incorporates seminal scholarly works in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside thousands of academic theses and peer-reviewed research papers. Conditioned through a deep linguistic engineering framework, Mubeen masters not just the meaning but the eloquence of Arabic, enabling precise understanding across classical texts, contemporary writing, and regional dialects with focus on comprehending user intent and delivering accurate, contextually relevant responses. Unlike other Arabic models relying on translated English data that often fail in intent detection or retrieval-augmented generation (RAG), Mubeen uses native Arabic sources to ensure cultural authenticity and accuracy. Its core innovation is the Practical Closure Architecture, designed to solve the "Utility Gap Crisis" where factually correct answers fail to resolve users' core needs, forcing them into frustrating cycles of re-prompting. By prioritizing clarity and decisive guidance, Mubeen transforms from an information repository into a decisive guide, aligning with Saudi Vision 2030. The model's architecture combines deep heritage specialization with multi-disciplinary expert modules, enabling robust performance across both cultural preservation and general knowledge domains.</li>
</ul>

<h3>Title: Code Aesthetics with Agentic Reward Feedback</h3>
<ul>
<li><strong>Authors: </strong>Bang Xiao, Lingjie Jiang, Shaohan Huang, Tengchao Lv, Yupan Huang, Xun Wu, Lei Cui, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23272">https://arxiv.org/abs/2510.23272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23272">https://arxiv.org/pdf/2510.23272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23272]] Code Aesthetics with Agentic Reward Feedback(https://arxiv.org/abs/2510.23272)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.</li>
</ul>

<h3>Title: A Novel Framework for Multi-Modal Protein Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Runjie Zheng, Zhen Wang, Anjie Qiao, Jiancong Xie, Jiahua Rao, Yuedong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23273">https://arxiv.org/abs/2510.23273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23273">https://arxiv.org/pdf/2510.23273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23273]] A Novel Framework for Multi-Modal Protein Representation Learning(https://arxiv.org/abs/2510.23273)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate protein function prediction requires integrating heterogeneous intrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts (e.g., protein-protein interactions and GO term annotations). However, two key challenges hinder effective fusion: (i) cross-modal distributional mismatch among embeddings produced by pre-trained intrinsic encoders, and (ii) noisy relational graphs of extrinsic data that degrade GNN-based information aggregation. We propose Diffused and Aligned Multi-modal Protein Embedding (DAMPE), a unified framework that addresses these through two core mechanisms. First, we propose Optimal Transport (OT)-based representation alignment that establishes correspondence between intrinsic embedding spaces of different modalities, effectively mitigating cross-modal heterogeneity. Second, we develop a Conditional Graph Generation (CGG)-based information fusion method, where a condition encoder fuses the aligned intrinsic embeddings to provide informative cues for graph reconstruction. Meanwhile, our theoretical analysis implies that the CGG objective drives this condition encoder to absorb graph-aware knowledge into its produced protein representations. Empirically, DAMPE outperforms or matches state-of-the-art methods such as DPFunc on standard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains 0.004-0.007 pp. Ablation studies further show that OT-based alignment contributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 pp Fmax. Overall, DAMPE offers a scalable and theoretically grounded approach for robust multi-modal protein representation learning, substantially enhancing protein function prediction.</li>
</ul>

<h3>Title: Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Weixuan Chen, Qianqian Yang, Shuo Shao, Shunpu Tang, Zhiguo Shi, Shui Yu</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23274">https://arxiv.org/abs/2510.23274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23274">https://arxiv.org/pdf/2510.23274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23274]] Privacy-Preserving Semantic Communication over Wiretap Channels with Learnable Differential Privacy(https://arxiv.org/abs/2510.23274)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, generative</a></li>
<li><strong>Abstract: </strong>While semantic communication (SemCom) improves transmission efficiency by focusing on task-relevant information, it also raises critical privacy concerns. Many existing secure SemCom approaches rely on restrictive or impractical assumptions, such as favorable channel conditions for the legitimate user or prior knowledge of the eavesdropper's model. To address these limitations, this paper proposes a novel secure SemCom framework for image transmission over wiretap channels, leveraging differential privacy (DP) to provide approximate privacy guarantees. Specifically, our approach first extracts disentangled semantic representations from source images using generative adversarial network (GAN) inversion method, and then selectively perturbs private semantic representations with approximate DP noise. Distinct from conventional DP-based protection methods, we introduce DP noise with learnable pattern, instead of traditional white Gaussian or Laplace noise, achieved through adversarial training of neural networks (NNs). This design mitigates the inherent non-invertibility of DP while effectively protecting private information. Moreover, it enables explicitly controllable security levels by adjusting the privacy budget according to specific security requirements, which is not achieved in most existing secure SemCom approaches. Experimental results demonstrate that, compared with the previous DP-based method and direct transmission, the proposed method significantly degrades the reconstruction quality for the eavesdropper, while introducing only slight degradation in task performance. Under comparable security levels, our approach achieves an LPIPS advantage of 0.06-0.29 and an FPPSR advantage of 0.10-0.86 for the legitimate user compared with the previous DP-based method.</li>
</ul>

<h3>Title: Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Wang, Beier Zhu, Junzhi Li, Liangyu Yuan, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23285">https://arxiv.org/abs/2510.23285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23285">https://arxiv.org/pdf/2510.23285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23285]] Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling(https://arxiv.org/abs/2510.23285)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative processes, formulated as differential equation solving, frequently balance computational speed with sample quality. Our theoretical investigation of ODE- and SDE-based solvers reveals complementary weaknesses: ODE solvers accumulate irreducible gradient error along deterministic trajectories, while SDE methods suffer from amplified discretization errors when the step budget is limited. Building upon this insight, we introduce AdaSDE, a novel single-step SDE solver that aims to unify the efficiency of ODEs with the error resilience of SDEs. Specifically, we introduce a single per-step learnable coefficient, estimated via lightweight distillation, which dynamically regulates the error correction strength to accelerate diffusion sampling. Notably, our framework can be integrated with existing solvers to enhance their capabilities. Extensive experiments demonstrate state-of-the-art performance: at 5 NFE, AdaSDE achieves FID scores of 4.18 on CIFAR-10, 8.05 on FFHQ and 6.96 on LSUN Bedroom. Codes are available in this https URL.</li>
</ul>

<h3>Title: Predicting symbolic ODEs from multiple trajectories</h3>
<ul>
<li><strong>Authors: </strong>Yakup Emre ≈ûahin, Niki Kilbertus, S√∂ren Becker</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23295">https://arxiv.org/abs/2510.23295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23295">https://arxiv.org/pdf/2510.23295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23295]] Predicting symbolic ODEs from multiple trajectories(https://arxiv.org/abs/2510.23295)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce MIO, a transformer-based model for inferring symbolic ordinary differential equations (ODEs) from multiple observed trajectories of a dynamical system. By combining multiple instance learning with transformer-based symbolic regression, the model effectively leverages repeated observations of the same system to learn more generalizable representations of the underlying dynamics. We investigate different instance aggregation strategies and show that even simple mean aggregation can substantially boost performance. MIO is evaluated on systems ranging from one to four dimensions and under varying noise levels, consistently outperforming existing baselines.</li>
</ul>

<h3>Title: MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Yingying Feng, Jie Li, Jie Hu, Yukang Zhang, Lei Tan, Jiayi Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23301">https://arxiv.org/abs/2510.23301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23301">https://arxiv.org/pdf/2510.23301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23301]] MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification(https://arxiv.org/abs/2510.23301)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-world object re-identification (ReID) systems often face modality inconsistencies, where query and gallery images come from different sensors (e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched conditions, which limits their robustness and scalability in practical applications. To address this challenge, we propose MDReID, a flexible any-to-any image-level ReID framework designed to operate under both modality-matched and modality-mismatched scenarios. MDReID builds on the insight that modality information can be decomposed into two components: modality-shared features that are predictable and transferable, and modality-specific features that capture unique, modality-dependent characteristics. To effectively leverage this, MDReID introduces two key components: the Modality Decoupling Learning (MDL) and Modality-aware Metric Learning (MML). Specifically, MDL explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enforces orthogonality and complementarity between the two components to enhance discriminative power across modalities. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDReID. Notably, MDReID achieves significant mAP improvements of 9.8\%, 3.0\%, and 11.5\% in general modality-matched scenarios, and average gains of 3.4\%, 11.8\%, and 10.9\% in modality-mismatched scenarios, respectively. The code is available at: \textcolor{magenta}{this https URL}.</li>
</ul>

<h3>Title: ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Chang, Chongjie Ye, Yushuang Wu, Yuantao Chen, Yidan Zhang, Zhongjin Luo, Chenghong Li, Yihao Zhi, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23306">https://arxiv.org/abs/2510.23306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23306">https://arxiv.org/pdf/2510.23306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23306]] ReconViaGen: Towards Accurate Multi-view 3D Object Reconstruction via Generation(https://arxiv.org/abs/2510.23306)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing multi-view 3D object reconstruction methods heavily rely on sufficient overlap between input views, where occlusions and sparse coverage in practice frequently yield severe reconstruction incompleteness. Recent advancements in diffusion-based 3D generative techniques offer the potential to address these limitations by leveraging learned generative priors to hallucinate invisible parts of objects, thereby generating plausible 3D structures. However, the stochastic nature of the inference process limits the accuracy and reliability of generation results, preventing existing reconstruction frameworks from integrating such 3D generative priors. In this work, we comprehensively analyze the reasons why diffusion-based 3D generative methods fail to achieve high consistency, including (a) the insufficiency in constructing and leveraging cross-view connections when extracting multi-view image features as conditions, and (b) the poor controllability of iterative denoising during local detail generation, which easily leads to plausible but inconsistent fine geometric and texture details with inputs. Accordingly, we propose ReconViaGen to innovatively integrate reconstruction priors into the generative framework and devise several strategies that effectively address these issues. Extensive experiments demonstrate that our ReconViaGen can reconstruct complete and accurate 3D models consistent with input views in both global structure and local this http URL page: this https URL.</li>
</ul>

<h3>Title: Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks</h3>
<ul>
<li><strong>Authors: </strong>Yaokai Feng, Kouichi Sakurai</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23313">https://arxiv.org/abs/2510.23313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23313">https://arxiv.org/pdf/2510.23313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23313]] Network Intrusion Detection: Evolution from Conventional Approaches to LLM Collaboration and Emerging Risks(https://arxiv.org/abs/2510.23313)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>This survey systematizes the evolution of network intrusion detection systems (NIDS), from conventional methods such as signature-based and neural network (NN)-based approaches to recent integrations with large language models (LLMs). It clearly and concisely summarizes the current status, strengths, and limitations of conventional techniques, and explores the practical benefits of integrating LLMs into NIDS. Recent research on the application of LLMs to NIDS in diverse environments is reviewed, including conventional network infrastructures, autonomous vehicle environments and IoT environments. From this survey, readers will learn that: 1) the earliest methods, signature-based IDSs, continue to make significant contributions to modern systems, despite their well-known weaknesses; 2) NN-based detection, although considered promising and under development for more than two decades, and despite numerous related approaches, still faces significant challenges in practical deployment; 3) LLMs are useful for NIDS in many cases, and a number of related approaches have been proposed; however, they still face significant challenges in practical applications. Moreover, they can even be exploited as offensive tools, such as for generating malware, crafting phishing messages, or launching cyberattacks. Recently, several studies have been proposed to address these challenges, which are also reviewed in this survey; and 4) strategies for constructing domain-specific LLMs have been proposed and are outlined in this survey, as it is nearly impossible to train a NIDS-specific LLM from scratch.</li>
</ul>

<h3>Title: Arabic Little STT: Arabic Children Speech Recognition Dataset</h3>
<ul>
<li><strong>Authors: </strong>Mouhand Alkadri, Dania Desouki, Khloud Al Jallad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23319">https://arxiv.org/abs/2510.23319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23319">https://arxiv.org/pdf/2510.23319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23319]] Arabic Little STT: Arabic Children Speech Recognition Dataset(https://arxiv.org/abs/2510.23319)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The performance of Artificial Intelligence (AI) systems fundamentally depends on high-quality training data. However, low-resource languages like Arabic suffer from severe data scarcity. Moreover, the absence of child-specific speech corpora is an essential gap that poses significant challenges. To address this gap, we present our created dataset, Arabic Little STT, a dataset of Levantine Arabic child speech recorded in classrooms, containing 355 utterances from 288 children (ages 6 - 13). We further conduct a systematic assessment of Whisper, a state-of-the-art automatic speech recognition (ASR) model, on this dataset and compare its performance with adult Arabic benchmarks. Our evaluation across eight Whisper variants reveals that even the best-performing model (Large_v3) struggles significantly, achieving a 0.66 word error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on adult datasets. These results align with other research on English speech. Results highlight the critical need for dedicated child speech benchmarks and inclusive training data in ASR development. Emphasizing that such data must be governed by strict ethical and privacy frameworks to protect sensitive child information. We hope that this study provides an initial step for future work on equitable speech technologies for Arabic-speaking children. We hope that our publicly available dataset enrich the children's demographic representation in ASR datasets.</li>
</ul>

<h3>Title: Towards Scaling Deep Neural Networks with Predictive Coding: Theory and Practice</h3>
<ul>
<li><strong>Authors: </strong>Francesco Innocenti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23323">https://arxiv.org/abs/2510.23323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23323">https://arxiv.org/pdf/2510.23323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23323]] Towards Scaling Deep Neural Networks with Predictive Coding: Theory and Practice(https://arxiv.org/abs/2510.23323)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Backpropagation (BP) is the standard algorithm for training the deep neural networks that power modern artificial intelligence including large language models. However, BP is energy inefficient and unlikely to be implemented by the brain. This thesis studies an alternative, potentially more efficient brain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks (PCNs) perform inference by iterative equilibration of neuron activities before learning or weight updates. Recent work has suggested that this iterative inference procedure provides a range of benefits over BP, such as faster training. However, these advantages have not been consistently observed, the inference and learning dynamics of PCNs are still poorly understood, and deep PCNs remain practically untrainable. Here, we make significant progress towards scaling PCNs by taking a theoretical approach grounded in optimisation theory. First, we show that the learning dynamics of PC can be understood as an approximate trust-region method using second-order information, despite explicitly using only first-order local updates. Second, going beyond this approximation, we show that PC can in principle make use of arbitrarily higher-order information, such that for feedforward networks the effective landscape on which PC learns is far more benign and robust to vanishing gradients than the (mean squared error) loss landscape. Third, motivated by a study of the inference dynamics of PCNs, we propose a new parameterisation called ``$\mu$PC'', which for the first time allows stable training of 100+ layer networks with little tuning and competitive performance on simple tasks. Overall, this thesis significantly advances our fundamental understanding of the inference and learning dynamics of PCNs, while highlighting the need for future research to focus on hardware co-design if PC is to compete with BP at scale.</li>
</ul>

<h3>Title: Multitask Multimodal Self-Supervised Learning for Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Cristian Simionescu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23325">https://arxiv.org/abs/2510.23325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23325">https://arxiv.org/pdf/2510.23325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23325]] Multitask Multimodal Self-Supervised Learning for Medical Images(https://arxiv.org/abs/2510.23325)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This thesis works to address a pivotal challenge in medical image analysis: the reliance on extensive labeled datasets, which are often limited due to the need for expert annotation and constrained by privacy and legal issues. By focusing on the development of self-supervised learning techniques and domain adaptation methods, this research aims to circumvent these limitations, presenting a novel approach to enhance the utility and efficacy of deep learning in medical imaging. Central to this thesis is the development of the Medformer, an innovative neural network architecture designed for multitask learning and deep domain adaptation. This model is adept at pre-training on diverse medical image datasets, handling varying sizes and modalities, and is equipped with a dynamic input-output adaptation mechanism. This enables efficient processing and integration of a wide range of medical image types, from 2D X-rays to complex 3D MRIs, thus mitigating the dependency on large labeled datasets. Further, the thesis explores the current state of self-supervised learning in medical imaging. It introduces novel pretext tasks that are capable of extracting meaningful information from unlabeled data, significantly advancing the model's interpretative abilities. This approach is validated through rigorous experimentation, including the use of the MedMNIST dataset, demonstrating the model's proficiency in learning generalized features applicable to various downstream tasks. In summary, this thesis contributes to the advancement of medical image analysis by offering a scalable, adaptable framework that reduces reliance on labeled data. It paves the way for more accurate, efficient diagnostic tools in healthcare, signifying a major step forward in the application of deep learning in medical imaging.</li>
</ul>

<h3>Title: Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Atif Quamar, Mohammad Areeb, Nishant Sharma, Ananth Shreekumar, Jonathan Rosenthal, Muslum Ozgur Ozmen, Mikhail Kuznetsov, Z. Berkay Celik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23334">https://arxiv.org/abs/2510.23334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23334">https://arxiv.org/pdf/2510.23334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23334]] Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models(https://arxiv.org/abs/2510.23334)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>LLM alignment remains a critical challenge. Inference-time methods provide a flexible alternative to fine-tuning, but their uniform computational effort often yields suboptimal alignment. We hypothesize that for many alignment tasks, the initial tokens of a response are disproportionately more critical. To leverage this principle, we introduce AdaSearch, a novel blockwise search strategy. It adaptively allocates a fixed computational budget using a sampling schedule, focusing search effort on these critical tokens. We apply AdaSearch to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our comprehensive evaluation across eight LLMs demonstrates that AdaSearch outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates improve by over 10% for harmlessness generation, controlled sentiment generation, and for mathematical reasoning tasks relative to Best-of-N.</li>
</ul>

<h3>Title: BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Zheng, Pai Liu, Xi Chen, Jizheng Dong, Sihan Jia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23337">https://arxiv.org/abs/2510.23337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23337">https://arxiv.org/pdf/2510.23337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23337]] BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning(https://arxiv.org/abs/2510.23337)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human-like virtual characters are crucial for games, storytelling, and virtual reality, yet current methods rely heavily on annotated data or handcrafted persona prompts, making it difficult to scale up and generate realistic, contextually coherent personas. We create the first QA dataset for BaZi-based persona reasoning, where real human experiences categorized into wealth, health, kinship, career, and relationships are represented as life-event questions and answers. Furthermore, we propose the first BaZi-LLM system that integrates symbolic reasoning with large language models to generate temporally dynamic and fine-grained virtual personas. Compared with mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a 30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information is used, our model's accuracy drops by 20%-45%, showing the potential of culturally grounded symbolic-LLM integration for realistic character simulation.</li>
</ul>

<h3>Title: LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data</h3>
<ul>
<li><strong>Authors: </strong>Teng Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23341">https://arxiv.org/abs/2510.23341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23341">https://arxiv.org/pdf/2510.23341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23341]] LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data(https://arxiv.org/abs/2510.23341)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The scarcity of high-quality knowledge graphs (KGs) remains a critical bottleneck for downstream AI applications, as existing extraction methods rely heavily on error-prone pattern-matching techniques or resource-intensive large language models (LLMs). While recent tools leverage LLMs to generate KGs, their computational demands limit accessibility for low-resource environments. Our paper introduces LightKGG, a novel framework that enables efficient KG extraction from textual data using small-scale language models (SLMs) through two key technical innovations: (1) Context-integrated Graph extraction integrates contextual information with nodes and edges into a unified graph structure, reducing the reliance on complex semantic processing while maintaining more key information; (2) Topology-enhanced relationship inference leverages the inherent topology of the extracted graph to efficiently infer relationships, enabling relationship discovery without relying on complex language understanding capabilities of LLMs. By enabling accurate KG construction with minimal hardware requirements, this work bridges the gap between automated knowledge extraction and practical deployment scenarios while introducing scientifically rigorous methods for optimizing SLM efficiency in structured NLP tasks.</li>
</ul>

<h3>Title: How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes</h3>
<ul>
<li><strong>Authors: </strong>Sheri Osborn, Rohit Valecha, H. Raghav Rao, Dan Sass, Anthony Rios</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23358">https://arxiv.org/abs/2510.23358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23358">https://arxiv.org/pdf/2510.23358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23358]] How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes(https://arxiv.org/abs/2510.23358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence is reshaping labor markets, yet we lack tools to systematically forecast its effects on employment. This paper introduces a benchmark for evaluating how well large language models (LLMs) can anticipate changes in job demand, especially in occupations affected by AI. Existing research has shown that LLMs can extract sentiment, summarize economic reports, and emulate forecaster behavior, but little work has assessed their use for forward-looking labor prediction. Our benchmark combines two complementary datasets: a high-frequency index of sector-level job postings in the United States, and a global dataset of projected occupational changes due to AI adoption. We format these data into forecasting tasks with clear temporal splits, minimizing the risk of information leakage. We then evaluate LLMs using multiple prompting strategies, comparing task-scaffolded, persona-driven, and hybrid approaches across model families. We assess both quantitative accuracy and qualitative consistency over time. Results show that structured task prompts consistently improve forecast stability, while persona prompts offer advantages on short-term trends. However, performance varies significantly across sectors and horizons, highlighting the need for domain-aware prompting and rigorous evaluation protocols. By releasing our benchmark, we aim to support future research on labor forecasting, prompt design, and LLM-based economic reasoning. This work contributes to a growing body of research on how LLMs interact with real-world economic data, and provides a reproducible testbed for studying the limits and opportunities of AI as a forecasting tool in the context of labor markets.</li>
</ul>

<h3>Title: Robust Non-negative Proximal Gradient Algorithm for Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Hanzhang Wang, Zonglin Liu, Jingyi Xu, Chenyang Wang, Zhiwei Zhong, Qiangqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23362">https://arxiv.org/abs/2510.23362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23362">https://arxiv.org/pdf/2510.23362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23362]] Robust Non-negative Proximal Gradient Algorithm for Inverse Problems(https://arxiv.org/abs/2510.23362)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Proximal gradient algorithms (PGA), while foundational for inverse problems like image reconstruction, often yield unstable convergence and suboptimal solutions by violating the critical non-negativity constraint. We identify the gradient descent step as the root cause of this issue, which introduces negative values and induces high sensitivity to hyperparameters. To overcome these limitations, we propose a novel multiplicative update proximal gradient algorithm (SSO-PGA) with convergence guarantees, which is designed for robustness in non-negative inverse problems. Our key innovation lies in superseding the gradient descent step with a learnable sigmoid-based operator, which inherently enforces non-negativity and boundedness by transforming traditional subtractive updates into multiplicative ones. This design, augmented by a sliding parameter for enhanced stability and convergence, not only improves robustness but also boosts expressive capacity and noise immunity. We further formulate a degradation model for multi-modal restoration and derive its SSO-PGA-based optimization algorithm, which is then unfolded into a deep network to marry the interpretability of optimization with the power of deep learning. Extensive numerical and real-world experiments demonstrate that our method significantly surpasses traditional PGA and other state-of-the-art algorithms, ensuring superior performance and stability.</li>
</ul>

<h3>Title: Interpretable Tile-Based Classification of Paclitaxel Exposure</h3>
<ul>
<li><strong>Authors: </strong>Sean Fletcher, Gabby Scott, Douglas Currie, Xin Zhang, Yuqi Song, Bruce MacLeod</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23363">https://arxiv.org/abs/2510.23363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23363">https://arxiv.org/pdf/2510.23363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23363]] Interpretable Tile-Based Classification of Paclitaxel Exposure(https://arxiv.org/abs/2510.23363)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Medical image analysis is central to drug discovery and preclinical evaluation, where scalable, objective readouts can accelerate decision-making. We address classification of paclitaxel (Taxol) exposure from phase-contrast microscopy of C6 glioma cells -- a task with subtle dose differences that challenges full-image models. We propose a simple tiling-and-aggregation pipeline that operates on local patches and combines tile outputs into an image label, achieving state-of-the-art accuracy on the benchmark dataset and improving over the published baseline by around 20 percentage points, with trends confirmed by cross-validation. To understand why tiling is effective, we further apply Grad-CAM and Score-CAM and attention analyses, which enhance model interpretability and point toward robustness-oriented directions for future medical image research. Code is released to facilitate reproduction and extension.</li>
</ul>

<h3>Title: ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping</h3>
<ul>
<li><strong>Authors: </strong>Hyeongkyun Kim, Orestis Oikonomou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23364">https://arxiv.org/abs/2510.23364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23364">https://arxiv.org/pdf/2510.23364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23364]] ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping(https://arxiv.org/abs/2510.23364)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Flood susceptibility mapping (FSM) is vital for disaster prevention but remains challenging in data-scarce regions where hydrodynamic models require dense geophysical inputs. This work introduces ZeroFlood, a geospatial foundation model framework for data-efficient FSM. The approach fine-tunes Geospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning, enabling flood prediction from basic Earth observation data such as Sentinel-1 or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-rich regions, ZeroFlood bridges data availability gaps through cross-modal representation learning. Experiments with TerraMind and Prithvi GFMs show that TiM enhances model robustness, with the TerraMind-Large configuration achieving an F1 score of 67.21. The results demonstrate the feasibility of foundation-model-based FSM as a scalable and data-efficient solution for flood risk management.</li>
</ul>

<h3>Title: Symbolic Neural Generation with Applications to Lead Discovery in Drug Design</h3>
<ul>
<li><strong>Authors: </strong>Ashwin Srinivasan, A Baskar, Tirtharaj Dash, Michael Bain, Sanjay Kumar Dey, Mainak Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23379">https://arxiv.org/abs/2510.23379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23379">https://arxiv.org/pdf/2510.23379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23379]] Symbolic Neural Generation with Applications to Lead Discovery in Drug Design(https://arxiv.org/abs/2510.23379)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigate a relatively underexplored class of hybrid neurosymbolic models integrating symbolic learning with neural reasoning to construct data generators meeting formal correctness criteria. In \textit{Symbolic Neural Generators} (SNGs), symbolic learners examine logical specifications of feasible data from a small set of instances -- sometimes just one. Each specification in turn constrains the conditional information supplied to a neural-based generator, which rejects any instance violating the symbolic specification. Like other neurosymbolic approaches, SNG exploits the complementary strengths of symbolic and neural methods. The outcome of an SNG is a triple $(H, X, W)$, where $H$ is a symbolic description of feasible instances constructed from data, $X$ a set of generated new instances that satisfy the description, and $W$ an associated weight. We introduce a semantics for such systems, based on the construction of appropriate \textit{base} and \textit{fibre} partially-ordered sets combined into an overall partial order, and outline a probabilistic extension relevant to practical applications. In this extension, SNGs result from searching over a weighted partial ordering. We implement an SNG combining a restricted form of Inductive Logic Programming (ILP) with a large language model (LLM) and evaluate it on early-stage drug design. Our main interest is the description and the set of potential inhibitor molecules generated by the SNG. On benchmark problems -- where drug targets are well understood -- SNG performance is statistically comparable to state-of-the-art methods. On exploratory problems with poorly understood targets, generated molecules exhibit binding affinities on par with leading clinical candidates. Experts further find the symbolic specifications useful as preliminary filters, with several generated molecules identified as viable for synthesis and wet-lab testing.</li>
</ul>

<h3>Title: An Efficient Remote Sensing Super Resolution Method Exploring Diffusion Priors and Multi-Modal Constraints for Crop Type Mapping</h3>
<ul>
<li><strong>Authors: </strong>Songxi Yang, Tang Sui, Qunying Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23382">https://arxiv.org/abs/2510.23382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23382">https://arxiv.org/pdf/2510.23382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23382]] An Efficient Remote Sensing Super Resolution Method Exploring Diffusion Priors and Multi-Modal Constraints for Crop Type Mapping(https://arxiv.org/abs/2510.23382)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Super resolution offers a way to harness medium even lowresolution but historically valuable remote sensing image archives. Generative models, especially diffusion models, have recently been applied to remote sensing super resolution (RSSR), yet several challenges exist. First, diffusion models are effective but require expensive training from scratch resources and have slow inference speeds. Second, current methods have limited utilization of auxiliary information as real-world constraints to reconstruct scientifically realistic images. Finally, most current methods lack evaluation on downstream tasks. In this study, we present a efficient LSSR framework for RSSR, supported by a new multimodal dataset of paired 30 m Landsat 8 and 10 m Sentinel 2 imagery. Built on frozen pretrained Stable Diffusion, LSSR integrates crossmodal attention with auxiliary knowledge (Digital Elevation Model, land cover, month) and Synthetic Aperture Radar guidance, enhanced by adapters and a tailored Fourier NDVI loss to balance spatial details and spectral fidelity. Extensive experiments demonstrate that LSSR significantly improves crop boundary delineation and recovery, achieving state-of-the-art performance with Peak Signal-to-Noise Ratio/Structural Similarity Index Measure of 32.63/0.84 (RGB) and 23.99/0.78 (IR), and the lowest NDVI Mean Squared Error (0.042), while maintaining efficient inference (0.39 sec/image). Moreover, LSSR transfers effectively to NASA Harmonized Landsat and Sentinel (HLS) super resolution, yielding more reliable crop classification (F1: 0.86) than Sentinel-2 (F1: 0.85). These results highlight the potential of RSSR to advance precision agriculture.</li>
</ul>

<h3>Title: The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation</h3>
<ul>
<li><strong>Authors: </strong>Farid Bagirov, Mikhail Arkhipov, Ksenia Sycheva, Evgeniy Glukhov, Egor Bogomolov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23393">https://arxiv.org/abs/2510.23393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23393">https://arxiv.org/pdf/2510.23393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23393]] The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation(https://arxiv.org/abs/2510.23393)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models. Despite its success in single generation problem solving, the reinforcement learning fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of generations and a resulting degradation of performance during Best-of-N sampling for large N values. In this work, we focus on optimizing the max@k metric, a continuous generalization of pass@k. We derive an unbiased on-policy gradient estimate for direct optimization of this metric. Furthermore, we extend our derivations to the off-policy updates, a common element in modern RLVR algorithms, that allows better sample efficiency. Empirically, we show that our objective effectively optimizes max@k metric in off-policy scenarios, aligning the model with the Best-of-N inference strategy.</li>
</ul>

<h3>Title: Detecting Religious Language in Climate Discourse</h3>
<ul>
<li><strong>Authors: </strong>Evy Beijen, Pien Pieterse, Yusuf √áelik, Willem Th. van Peursen, Sandjai Bhulai, Meike Morren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23395">https://arxiv.org/abs/2510.23395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23395">https://arxiv.org/pdf/2510.23395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23395]] Detecting Religious Language in Climate Discourse(https://arxiv.org/abs/2510.23395)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Religious language continues to permeate contemporary discourse, even in ostensibly secular domains such as environmental activism and climate change debates. This paper investigates how explicit and implicit forms of religious language appear in climate-related texts produced by secular and religious nongovernmental organizations (NGOs). We introduce a dual methodological approach: a rule-based model using a hierarchical tree of religious terms derived from ecotheology literature, and large language models (LLMs) operating in a zero-shot setting. Using a dataset of more than 880,000 sentences, we compare how these methods detect religious language and analyze points of agreement and divergence. The results show that the rule-based method consistently labels more sentences as religious than LLMs. These findings highlight not only the methodological challenges of computationally detecting religious language but also the broader tension over whether religious language should be defined by vocabulary alone or by contextual meaning. This study contributes to digital methods in religious studies by demonstrating both the potential and the limitations of approaches for analyzing how the sacred persists in climate discourse.</li>
</ul>

<h3>Title: EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Musleh Alharthi, Kaleel Mahmood, Sarosh Patel, Ausif Mahmood</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23396">https://arxiv.org/abs/2510.23396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23396">https://arxiv.org/pdf/2510.23396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23396]] EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting(https://arxiv.org/abs/2510.23396)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The immense success of the Transformer architecture in Natural Language Processing has led to its adoption in Time Se ries Forecasting (TSF), where superior performance has been shown. However, a recent important paper questioned their effectiveness by demonstrating that a simple single layer linear model outperforms Transformer-based models. This was soon shown to be not as valid, by a better transformer-based model termed PatchTST. More re cently, TimeLLM demonstrated even better results by repurposing a Large Language Model (LLM) for the TSF domain. Again, a follow up paper challenged this by demonstrating that removing the LLM component or replacing it with a basic attention layer in fact yields better performance. One of the challenges in forecasting is the fact that TSF data favors the more recent past, and is sometimes subject to unpredictable events. Based upon these recent insights in TSF, we propose a strong Mixture of Experts (MoE) framework. Our method combines the state-of-the-art (SOTA) models including xLSTM, en hanced Linear, PatchTST, and minGRU, among others. This set of complimentary and diverse models for TSF are integrated in a Trans former based MoE gating network. Our proposed model outperforms all existing TSF models on standard benchmarks, surpassing even the latest approaches based on MoE frameworks.</li>
</ul>

<h3>Title: VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations</h3>
<ul>
<li><strong>Authors: </strong>Lu Dong, Haiyu Zhang, Han Lin, Ziang Yan, Xiangyu Zeng, Hongjie Zhang, Yifei Huang, Yi Wang, Zhen-Hua Ling, Limin Wang, Yali Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23397">https://arxiv.org/abs/2510.23397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23397">https://arxiv.org/pdf/2510.23397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23397]] VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations(https://arxiv.org/abs/2510.23397)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video temporal grounding (VTG) aims to locate precise segments in videos based on language queries, which is a fundamental challenge in video understanding. While recent Multimodal Large Language Models (MLLMs) have shown promise in tackling VTG through reinforcement learning (RL), they overlook the challenges arising from both the quality and difficulty of training samples. (1) Partially annotated samples. Many samples contain relevant segments beyond the annotated interval, introducing ambiguous supervision. (2) Hard-to-ground samples. Samples with poor zero-shot performance produce consistently low and indistinguishable rewards during RL training, exhibiting no clear preference among multiple outputs and thus hindering learning efficiency. To address these challenges, we propose VideoTG-R1, a novel curriculum RL framework with reflected boundary annotations, enabling data-efficient training. Specifically, we propose a Boundary Reflection Agent that utilizes MLLMs to predict query-relevant timestamps outside the annotated intervals, allowing us to identify and filter out partially annotated samples, thereby reducing ambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess the training difficulty of each sample and design a curriculum RL strategy that dynamically masks the videos of hard-to-ground samples according to the training steps, easing the training difficulty and providing clearer preference. Experiments on the VTG and grounded VideoQA tasks demonstrate the effectiveness of our method. Remarkably, with only 10% of the training samples and 21% of the computational budget, VideoTG-R1 outperforms full-data counterparts under both group relative policy optimization (GRPO) and supervised fine-tuning (SFT). The code is available at this https URL.</li>
</ul>

<h3>Title: Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based Approach</h3>
<ul>
<li><strong>Authors: </strong>Youngjun Choi, Joonseong Kang, Sungjun Lim, Kyungwoo Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23409">https://arxiv.org/abs/2510.23409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23409">https://arxiv.org/pdf/2510.23409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23409]] Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based Approach(https://arxiv.org/abs/2510.23409)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Data valuation has become central in the era of data-centric AI. It drives efficient training pipelines and enables objective pricing in data markets by assigning a numeric value to each data point. Most existing data valuation methods estimate the effect of removing individual data points by evaluating changes in model validation performance under in-distribution (ID) settings, as opposed to out-of-distribution (OOD) scenarios where data follow different patterns. Since ID and OOD data behave differently, data valuation methods based on ID loss often fail to generalize to OOD settings, particularly when the validation set contains no OOD data. Furthermore, although OOD-aware methods exist, they involve heavy computational costs, which hinder practical deployment. To address these challenges, we introduce \emph{Eigen-Value} (EV), a plug-and-play data valuation framework for OOD robustness that uses only an ID data subset, including during validation. EV provides a new spectral approximation of domain discrepancy, which is the gap of loss between ID and OOD using ratios of eigenvalues of ID data's covariance matrix. EV then estimates the marginal contribution of each data point to this discrepancy via perturbation theory, alleviating the computational burden. Subsequently, EV plugs into ID loss-based methods by adding an EV term without any additional training loop. We demonstrate that EV achieves improved OOD robustness and stable value rankings across real-world datasets, while remaining computationally lightweight. These results indicate that EV is practical for large-scale settings with domain shift, offering an efficient path to OOD-robust data valuation.</li>
</ul>

<h3>Title: Symmetria: A Synthetic Dataset for Learning in Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Ivan Sipiran, Gustavo Santelices, Lucas Oyarz√∫n, Andrea Ranieri, Chiara Romanengo, Silvia Biasotti, Bianca Falcidieno</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23414">https://arxiv.org/abs/2510.23414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23414">https://arxiv.org/pdf/2510.23414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23414]] Symmetria: A Synthetic Dataset for Learning in Point Clouds(https://arxiv.org/abs/2510.23414)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unlike image or text domains that benefit from an abundance of large-scale datasets, point cloud learning techniques frequently encounter limitations due to the scarcity of extensive datasets. To overcome this limitation, we present Symmetria, a formula-driven dataset that can be generated at any arbitrary scale. By construction, it ensures the absolute availability of precise ground truth, promotes data-efficient experimentation by requiring fewer samples, enables broad generalization across diverse geometric settings, and offers easy extensibility to new tasks and modalities. Using the concept of symmetry, we create shapes with known structure and high variability, enabling neural networks to learn point cloud features effectively. Our results demonstrate that this dataset is highly effective for point cloud self-supervised pre-training, yielding models with strong performance in downstream tasks such as classification and segmentation, which also show good few-shot learning capabilities. Additionally, our dataset can support fine-tuning models to classify real-world objects, highlighting our approach's practical utility and application. We also introduce a challenging task for symmetry detection and provide a benchmark for baseline comparisons. A significant advantage of our approach is the public availability of the dataset, the accompanying code, and the ability to generate very large collections, promoting further research and innovation in point cloud learning.</li>
</ul>

<h3>Title: Towards Generalisable Foundation Models for 3D Brain MRI</h3>
<ul>
<li><strong>Authors: </strong>Moona Mazher, Geoff J. M. Parker, Daniel C. Alexander</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23415">https://arxiv.org/abs/2510.23415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23415">https://arxiv.org/pdf/2510.23415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23415]] Towards Generalisable Foundation Models for 3D Brain MRI(https://arxiv.org/abs/2510.23415)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models in artificial intelligence (AI) are transforming medical imaging by enabling general-purpose feature learning from large-scale, unlabeled datasets. In this work, we introduce BrainFound, a self-supervised foundation model for brain MRI, built by extending DINO-v2, a vision transformer originally designed for 2D natural images. BrainFound adapts DINO-v2 to model full 3D brain anatomy by incorporating volumetric information from sequential MRI slices, moving beyond conventional single-slice paradigms. It supports both single- and multimodal inputs, enabling a broad range of downstream tasks, including disease detection and image segmentation, while generalising across varied imaging protocols and clinical scenarios. We show that BrainFound consistently outperforms existing self-supervised pretraining strategies and supervised baselines, particularly in label-scarce and multi-contrast settings. By integrating information from diverse 3D MRI modalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces dependency on extensive expert annotations. This flexibility makes BrainFound a scalable and practical solution for 3D neuroimaging pipelines, with significant potential for clinical deployment and research innovation.</li>
</ul>

<h3>Title: PrivacyGuard: A Modular Framework for Privacy Auditing in Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Luca Melis, Matthew Grange, Iden Kalemaj, Karan Chadha, Shengyuan Hu, Elena Kashtelyan, Will Bullock</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23427">https://arxiv.org/abs/2510.23427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23427">https://arxiv.org/pdf/2510.23427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23427]] PrivacyGuard: A Modular Framework for Privacy Auditing in Machine Learning(https://arxiv.org/abs/2510.23427)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, extraction, membership infer</a></li>
<li><strong>Abstract: </strong>The increasing deployment of Machine Learning (ML) models in sensitive domains motivates the need for robust, practical privacy assessment tools. PrivacyGuard is a comprehensive tool for empirical differential privacy (DP) analysis, designed to evaluate privacy risks in ML models through state-of-the-art inference attacks and advanced privacy measurement techniques. To this end, PrivacyGuard implements a diverse suite of privacy attack-- including membership inference , extraction, and reconstruction attacks -- enabling both off-the-shelf and highly configurable privacy analyses. Its modular architecture allows for the seamless integration of new attacks, and privacy metrics, supporting rapid adaptation to emerging research advances. We make PrivacyGuard available at this https URL.</li>
</ul>

<h3>Title: FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network</h3>
<ul>
<li><strong>Authors: </strong>Fangtong Sun, Congyu Li, Ke Yang, Yuchen Pan, Hanwen Yu, Xichuan Zhang, Yiying Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23444">https://arxiv.org/abs/2510.23444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23444">https://arxiv.org/pdf/2510.23444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23444]] FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network(https://arxiv.org/abs/2510.23444)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Low-light vision remains a fundamental challenge in computer vision due to severe illumination degradation, which significantly affects the performance of downstream tasks such as detection and segmentation. While recent state-of-the-art methods have improved performance through invariant feature learning modules, they still fall short due to incomplete modeling of low-light conditions. Therefore, we revisit low-light image formation and extend the classical Lambertian model to better characterize low-light conditions. By shifting our analysis to the frequency domain, we theoretically prove that the frequency-domain channel ratio can be leveraged to extract illumination-invariant features via a structured filtering process. We then propose a novel and end-to-end trainable module named \textbf{F}requency-domain \textbf{R}adial \textbf{B}asis \textbf{Net}work (\textbf{FRBNet}), which integrates the frequency-domain channel ratio operation with a learnable frequency domain filter for the overall illumination-invariant feature enhancement. As a plug-and-play module, FRBNet can be integrated into existing networks for low-light downstream tasks without modifying loss functions. Extensive experiments across various downstream tasks demonstrate that FRBNet achieves superior performance, including +2.2 mAP for dark object detection and +2.9 mIoU for nighttime segmentation. Code is available at: this https URL.</li>
</ul>

<h3>Title: Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23451">https://arxiv.org/abs/2510.23451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23451">https://arxiv.org/pdf/2510.23451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23451]] Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences(https://arxiv.org/abs/2510.23451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.</li>
</ul>

<h3>Title: SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Khoa Nguyen, Khang Tran, NhatHai Phan, Cristian Borcea, Rouming Jin, Issa Khalil</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23455">https://arxiv.org/abs/2510.23455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23455">https://arxiv.org/pdf/2510.23455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23455]] SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning(https://arxiv.org/abs/2510.23455)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel training algorithm to leverage the geographic information of mobile users in Federated Learning (FL). SGFusion maps the data collected by mobile devices onto geographical zones and trains one FL model per zone, which adapts well to the data and behaviors of users in that zone. SGFusion models the local data-based correlation among geographical zones as a hierarchical random graph (HRG) optimized by Markov Chain Monte Carlo sampling. At each training step, every zone fuses its local gradient with gradients derived from a small set of other zones sampled from the HRG. This approach enables knowledge fusion and sharing among geographical zones in a probabilistic and stochastic gradient fusion process with self-attention weights, such that "more similar" zones have "higher probabilities" of sharing gradients with "larger attention weights." SGFusion remarkably improves model utility without introducing undue computational cost. Extensive theoretical and empirical results using a heart-rate prediction dataset collected across 6 countries show that models trained with SGFusion converge with upper-bounded expected errors and significantly improve utility in all countries compared to existing approaches without notable cost in system scalability.</li>
</ul>

<h3>Title: Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era</h3>
<ul>
<li><strong>Authors: </strong>Saleh Darzi, Mirza Masfiqur Rahman, Imtiaz Karim, Rouzbeh Behnia, Attila A Yavuz, Elisa Bertino</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23457">https://arxiv.org/abs/2510.23457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23457">https://arxiv.org/pdf/2510.23457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23457]] Authentication Against Insecure Bootstrapping for 5G Networks: Feasibility, Resiliency, and Transitional Solutions in Post-Quantum Era(https://arxiv.org/abs/2510.23457)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>The 5G protocol lacks a robust base station authentication mechanism during the initial bootstrapping phase, leaving it susceptible to threats such as fake base station attacks. Conventional solutions, including digital signatures based on Public Key Infrastructures (PKIs) and identity-based signatures, are inadequate against quantum-capable adversaries. While integrating NIST's Post-Quantum Cryptography (PQC) standards is a leading approach for quantum resistance, their suitability for 5G base station authentication remains unexplored. Moreover, current solutions are predominantly centralized and lack security features such as distributed authentication. This work presents, to our knowledge, the first comprehensive network-level performance characterization of integrating NIST-PQC standards and conventional digital signatures (including threshold and identity-based schemes) into 5G base station authentication. Our findings reveal significant feasibility concerns, with direct PQC adoption hindered by protocol constraints and large signature sizes. We also highlight the performance limitations of conventional methods due to the overhead of certificate chains. To mitigate these challenges, we propose BORG, a transitional authentication solution based on a Hierarchical Identity-Based Threshold Signature scheme with a Fail-Stop property. BORG offers post-mortem post-quantum forgery detection and distributed trust via threshold and compact signatures, well-suited for 5G's stringent requirements. Our performance analysis underscores an important warning on the infeasibility of direct PQC integration and positions BORG as an effective transitional solution toward future quantum-resilient 5G authentication.</li>
</ul>

<h3>Title: Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station</h3>
<ul>
<li><strong>Authors: </strong>Hao Liang, Haifeng Wen, Kaishun Wu, Hong Xing</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23463">https://arxiv.org/abs/2510.23463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23463">https://arxiv.org/pdf/2510.23463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23463]] Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station(https://arxiv.org/abs/2510.23463)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed learning paradigm that preserves privacy by eliminating the need to exchange raw data during training. In its prototypical edge instantiation with underlying wireless transmissions enabled by analog over-the-air computing (AirComp), referred to as \emph{over-the-air FL (AirFL)}, the inherent channel noise plays a unique role of \emph{frenemy} in the sense that it degrades training due to noisy global aggregation while providing a natural source of randomness for privacy-preserving mechanisms, formally quantified by \emph{differential privacy (DP)}. It remains, nevertheless, challenging to effectively harness such channel impairments, as prior arts, under assumptions of either simple channel models or restricted types of loss functions, mostly considering (local) DP enhancement with a single-round or non-convergent bound on privacy loss. In this paper, we study AirFL over multiple-access fading channels with a multi-antenna base station (BS) subject to user-level DP requirements. Despite a recent study, which claimed in similar settings that artificial noise (AN) must be injected to ensure DP in general, we demonstrate, on the contrary, that DP can be gained as a \emph{perk} even \emph{without} employing any AN. Specifically, we derive a novel bound on DP that converges under general bounded-domain assumptions on model parameters, along with a convergence bound with general smooth and non-convex loss functions. Next, we optimize over receive beamforming and power allocations to characterize the optimal convergence-privacy trade-offs, which also reveal explicit conditions in which DP is achievable without compromising training. Finally, our theoretical findings are validated by extensive numerical results.</li>
</ul>

<h3>Title: Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts</h3>
<ul>
<li><strong>Authors: </strong>Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23464">https://arxiv.org/abs/2510.23464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23464">https://arxiv.org/pdf/2510.23464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23464]] Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts(https://arxiv.org/abs/2510.23464)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Financial narratives from U.S. Securities and Exchange Commission (SEC) filing reports and quarterly earnings call transcripts (ECTs) are very important for investors, auditors, and regulators. However, their length, financial jargon, and nuanced language make fine-grained analysis difficult. Prior sentiment analysis in the financial domain required a large, expensive labeled dataset, making the sentence-level stance towards specific financial targets challenging. In this work, we introduce a sentence-level corpus for stance detection focused on three core financial metrics: debt, earnings per share (EPS), and sales. The sentences were extracted from Form 10-K annual reports and ECTs, and labeled for stance (positive, negative, neutral) using the advanced ChatGPT-o3-pro model under rigorous human validation. Using this corpus, we conduct a systematic evaluation of modern large language models (LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting strategies. Our results show that few-shot with CoT prompting performs best compared to supervised baselines, and LLMs' performance varies across the SEC and ECT datasets. Our findings highlight the practical viability of leveraging LLMs for target-specific stance in the financial domain without requiring extensive labeled data.</li>
</ul>

<h3>Title: Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Yang, Xingbo Fu, Jundong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23469">https://arxiv.org/abs/2510.23469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23469">https://arxiv.org/pdf/2510.23469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23469]] Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks(https://arxiv.org/abs/2510.23469)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In recent years, pre-training Graph Neural Networks (GNNs) through self-supervised learning on unlabeled graph data has emerged as a widely adopted paradigm in graph learning. Although the paradigm is effective for pre-training powerful GNN models, the objective gap often exists between pre-training and downstream tasks. To bridge this gap, graph prompting adapts pre-trained GNN models to specific downstream tasks with extra learnable prompts while keeping the pre-trained GNN models frozen. As recent graph prompting methods largely focus on enhancing model utility on downstream tasks, they often overlook fairness concerns when designing prompts for adaptation. In fact, pre-trained GNN models will produce discriminative node representations across demographic subgroups, as downstream graph data inherently contains biases in both node attributes and graph structures. To address this issue, we propose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairness for adapting pre-trained GNN models to downstream tasks. To mitigate attribute bias, we design an Adaptive Feature Rectification module that learns customized attribute prompts to suppress sensitive information at the input layer, reducing bias at the source. Afterward, we propose an Adaptive Message Calibration module that generates structure prompts at each layer, which adjust the message from neighboring nodes to enable dynamic and soft calibration of the information flow. Finally, ADPrompt jointly optimizes the two prompting modules to adapt the pre-trained GNN while enhancing fairness. We conduct extensive experiments on four datasets with four pre-training strategies to evaluate the performance of ADPrompt. The results demonstrate that our proposed ADPrompt outperforms seven baseline methods on node classification tasks.</li>
</ul>

<h3>Title: Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shijian Wang, Jiarui Jin, Xingjian Wang, Linxin Song, Runhao Fu, Hecheng Wang, Zongyuan Ge, Yuan Lu, Xuelian Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23473">https://arxiv.org/abs/2510.23473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23473">https://arxiv.org/pdf/2510.23473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23473]] Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning(https://arxiv.org/abs/2510.23473)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in image reasoning methods, particularly "Thinking with Images", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic "grounding" and "captioning" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.</li>
</ul>

<h3>Title: MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring</h3>
<ul>
<li><strong>Authors: </strong>Tengchao Yang, Sichen Guo, Mengzhao Jia, Jiaming Su, Yuanyang Liu, Zhihan Zhang, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23477">https://arxiv.org/abs/2510.23477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23477">https://arxiv.org/pdf/2510.23477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23477]] MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring(https://arxiv.org/abs/2510.23477)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective math tutoring requires not only solving problems but also diagnosing students' difficulties and guiding them step by step. While multimodal large language models (MLLMs) show promise, existing benchmarks largely overlook these tutoring skills. We introduce MMTutorBench, the first benchmark for AI math tutoring, consisting of 685 problems built around pedagogically significant key-steps. Each problem is paired with problem-specific rubrics that enable fine-grained evaluation across six dimensions, and structured into three tasks-Insight Discovery, Operation Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find clear performance gaps between proprietary and open-source systems, substantial room compared to human tutors, and consistent trends across input variants: OCR pipelines degrade tutoring quality, few-shot prompting yields limited gains, and our rubric-based LLM-as-a-Judge proves highly reliable. These results highlight both the difficulty and diagnostic value of MMTutorBench for advancing AI tutoring.</li>
</ul>

<h3>Title: MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xin Jin, Siyuan Li, Siyong Jian, Kai Yu, Huan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23479">https://arxiv.org/abs/2510.23479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23479">https://arxiv.org/pdf/2510.23479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23479]] MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding(https://arxiv.org/abs/2510.23479)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Vision-language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs.</li>
</ul>

<h3>Title: Towards a Functionally Complete and Parameterizable TFHE Processor</h3>
<ul>
<li><strong>Authors: </strong>Valentin Reyes H√§usler, Gabriel Ott, Aruna Jayasena, Andreas Peter</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23483">https://arxiv.org/abs/2510.23483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23483">https://arxiv.org/pdf/2510.23483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23483]] Towards a Functionally Complete and Parameterizable TFHE Processor(https://arxiv.org/abs/2510.23483)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect</a></li>
<li><strong>Abstract: </strong>Fully homomorphic encryption allows the evaluation of arbitrary functions on encrypted data. It can be leveraged to secure outsourced and multiparty computation. TFHE is a fast torus-based fully homomorphic encryption scheme that allows both linear operations, as well as the evaluation of arbitrary non-linear functions. It currently provides the fastest bootstrapping operation performance of any other FHE scheme. Despite its fast performance, TFHE suffers from a considerably higher computational overhead for the evaluation of homomorphic circuits. Computations in the encrypted domain are orders of magnitude slower than their unencrypted equivalents. This bottleneck hinders the widespread adoption of (T)FHE for the protection of sensitive data. While state-of-the-art implementations focused on accelerating and outsourcing single operations, their scalability and practicality are constrained by high memory bandwidth costs. In order to overcome this, we propose an FPGA-based hardware accelerator for the evaluation of homomorphic circuits. Specifically, we design a functionally complete TFHE processor for FPGA hardware capable of processing instructions on the data completely on the FPGA. In order to achieve a higher throughput from our TFHE processor, we implement an improved programmable bootstrapping module which outperforms the current state-of-the-art by 240\% to 480\% more bootstrappings per second. Our efficient, compact, and scalable design lays the foundation for implementing complete FPGA-based TFHE processor architectures.</li>
</ul>

<h3>Title: Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap</h3>
<ul>
<li><strong>Authors: </strong>Elisabeth J√ºttner, Leona Krath, Stefan Korfhage, Hannah Dr√∂ge, Matthias B. Hullin, Markus Plack</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23494">https://arxiv.org/abs/2510.23494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23494">https://arxiv.org/pdf/2510.23494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23494]] Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap(https://arxiv.org/abs/2510.23494)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.</li>
</ul>

<h3>Title: Mixed Precision Training of Neural ODEs</h3>
<ul>
<li><strong>Authors: </strong>Elena Celledoni, Brynjulf Owren, Lars Ruthotto, Tianjiao Nicole Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23498">https://arxiv.org/abs/2510.23498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23498">https://arxiv.org/pdf/2510.23498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23498]] Mixed Precision Training of Neural ODEs(https://arxiv.org/abs/2510.23498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Exploiting low-precision computations has become a standard strategy in deep learning to address the growing computational costs imposed by ever larger models and datasets. However, naively performing all computations in low precision can lead to roundoff errors and instabilities. Therefore, mixed precision training schemes usually store the weights in high precision and use low-precision computations only for whitelisted operations. Despite their success, these principles are currently not reliable for training continuous-time architectures such as neural ordinary differential equations (Neural ODEs). This paper presents a mixed precision training framework for neural ODEs, combining explicit ODE solvers with a custom backpropagation scheme, and demonstrates its effectiveness across a range of learning tasks. Our scheme uses low-precision computations for evaluating the velocity, parameterized by the neural network, and for storing intermediate states, while stability is provided by a custom dynamic adjoint scaling and by accumulating the solution and gradients in higher precision. These contributions address two key challenges in training neural ODE: the computational cost of repeated network evaluations and the growth of memory requirements with the number of time steps or layers. Along with the paper, we publish our extendable, open-source PyTorch package rampde, whose syntax resembles that of leading packages to provide a drop-in replacement in existing codes. We demonstrate the reliability and effectiveness of our scheme using challenging test cases and on neural ODE applications in image classification and generative models, achieving approximately 50% memory reduction and up to 2x speedup while maintaining accuracy comparable to single-precision training.</li>
</ul>

<h3>Title: Towards Deep Physics-Informed Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Spyros Rigas, Fotios Anagnostopoulos, Michalis Papachristou, Georgios Alexandridis</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23501">https://arxiv.org/abs/2510.23501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23501">https://arxiv.org/pdf/2510.23501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23501]] Towards Deep Physics-Informed Kolmogorov-Arnold Networks(https://arxiv.org/abs/2510.23501)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Since their introduction, Kolmogorov-Arnold Networks (KANs) have been successfully applied across several domains, with physics-informed machine learning (PIML) emerging as one of the areas where they have thrived. In the PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the standard due to their computational efficiency. However, like their multilayer perceptron-based counterparts, cPIKANs face significant challenges when scaled to depth, leading to training instabilities that limit their applicability to several PDE problems. To address this, we propose a basis-agnostic, Glorot-like initialization scheme that preserves activation variance and yields substantial improvements in stability and accuracy over the default initialization of cPIKANs. Inspired by the PirateNet architecture, we further introduce Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in deep cPIKANs where initialization alone is not sufficient. Through empirical tests and information bottleneck analysis, we show that RGA KANs successfully traverse all training phases, unlike baseline cPIKANs, which stagnate in the diffusion phase in specific PDE settings. Evaluations on seven standard forward PDE benchmarks under a fixed training pipeline with adaptive components demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and PirateNets - often by several orders of magnitude - while remaining stable in settings where the others diverge.</li>
</ul>

<h3>Title: iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Usama Zidan, Mohamed Gaber, Mohammed M. Abdelsamea</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23504">https://arxiv.org/abs/2510.23504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23504">https://arxiv.org/pdf/2510.23504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23504]] iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification(https://arxiv.org/abs/2510.23504)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Graph neural networks have emerged as a promising paradigm for image processing, yet their performance in image classification tasks is hindered by a limited consideration of the underlying structure and relationships among visual entities. This work presents iPac, a novel approach to introduce a new graph representation of images to enhance graph neural network image classification by recognizing the importance of underlying structure and relationships in medical image classification. iPac integrates various stages, including patch partitioning, feature extraction, clustering, graph construction, and graph-based learning, into a unified network to advance graph neural network image classification. By capturing relevant features and organising them into clusters, we construct a meaningful graph representation that effectively encapsulates the semantics of the image. Experimental evaluation on diverse medical image datasets demonstrates the efficacy of iPac, exhibiting an average accuracy improvement of up to 5% over baseline methods. Our approach offers a versatile and generic solution for image classification, particularly in the realm of medical images, by leveraging the graph representation and accounting for the inherent structure and relationships among visual entities.</li>
</ul>

<h3>Title: A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off Perspective</h3>
<ul>
<li><strong>Authors: </strong>Siamak Ghodsi, Amjad Seyedi, Tai Le Quy, Fariba Karimi, Eirini Ntoutsi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23507">https://arxiv.org/abs/2510.23507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23507">https://arxiv.org/pdf/2510.23507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23507]] A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off Perspective(https://arxiv.org/abs/2510.23507)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Fair graph clustering seeks partitions that respect network structure while maintaining proportional representation across sensitive groups, with applications spanning community detection, team formation, resource allocation, and social network analysis. Many existing approaches enforce rigid constraints or rely on multi-stage pipelines (e.g., spectral embedding followed by $k$-means), limiting trade-off control, interpretability, and scalability. We introduce \emph{DFNMF}, an end-to-end deep nonnegative tri-factorization tailored to graphs that directly optimizes cluster assignments with a soft statistical-parity regularizer. A single parameter $\lambda$ tunes the fairness--utility balance, while nonnegativity yields parts-based factors and transparent soft memberships. The optimization uses sparse-friendly alternating updates and scales near-linearly with the number of edges. Across synthetic and real networks, DFNMF achieves substantially higher group balance at comparable modularity, often dominating state-of-the-art baselines on the Pareto front. The code is available at this https URL.</li>
</ul>

<h3>Title: M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Geng, Jonathan Tonglet, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23508">https://arxiv.org/abs/2510.23508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23508">https://arxiv.org/pdf/2510.23508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23508]] M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset(https://arxiv.org/abs/2510.23508)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Existing real-world datasets for multimodal automated fact-checking have multiple limitations: they contain few instances, focus on only one or two languages and tasks, suffer from evidence leakage, or depend on external sets of news articles for sourcing true claims. To address these shortcomings, we introduce M4FC, a new real-world dataset comprising 4,982 images paired with 6,980 claims. The images, verified by professional fact-checkers from 22 organizations, represent diverse cultural and geographic contexts. Each claim is available in one or two out of ten languages. M4FC spans six multimodal fact-checking tasks: visual claim extraction, claimant intent prediction, fake detection, image contextualization, location verification, and verdict prediction. We provide baseline results for all tasks and analyze how combining intermediate tasks influence downstream verdict prediction performance. We make our dataset and code available.</li>
</ul>

<h3>Title: FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time</h3>
<ul>
<li><strong>Authors: </strong>Yaoli Liu, Yao-Xiang Ding, Kun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23515">https://arxiv.org/abs/2510.23515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23515">https://arxiv.org/pdf/2510.23515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23515]] FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time(https://arxiv.org/abs/2510.23515)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>This paper proposes FreeFuse, a novel training-free approach for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to existing methods that either focus on pre-inference LoRA weight merging or rely on segmentation models and complex techniques like noise blending to isolate LoRA outputs, our key insight is that context-aware dynamic subject masks can be automatically derived from cross-attention layer weights. Mathematical analysis shows that directly applying these masks to LoRA outputs during inference well approximates the case where the subject LoRA is integrated into the diffusion model and used individually for the masked region. FreeFuse demonstrates superior practicality and efficiency as it requires no additional training, no modification to LoRAs, no auxiliary models, and no user-defined prompt templates or region specifications. Alternatively, it only requires users to provide the LoRA activation words for seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both generation quality and usability under the multi-subject generation tasks. The project page is at this https URL</li>
</ul>

<h3>Title: DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Wanmeng Li, Simone Mosco, Daniel Fusaro, Alberto Pretto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23525">https://arxiv.org/abs/2510.23525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23525">https://arxiv.org/pdf/2510.23525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23525]] DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation(https://arxiv.org/abs/2510.23525)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Annotating real-world LiDAR point clouds for use in intelligent autonomous systems is costly. To overcome this limitation, self-training-based Unsupervised Domain Adaptation (UDA) has been widely used to improve point cloud semantic segmentation by leveraging synthetic point cloud data. However, we argue that existing methods do not effectively utilize unlabeled data, as they either rely on predefined or fixed confidence thresholds, resulting in suboptimal performance. In this paper, we propose a Dynamic Pseudo-Label Filtering (DPLF) scheme to enhance real data utilization in point cloud UDA semantic segmentation. Additionally, we design a simple and efficient Prior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift between synthetic and real-world point clouds. Finally, we utilize data mixing consistency loss to push the model to learn context-free representations. We implement and thoroughly evaluate our approach through extensive comparisons with state-of-the-art methods. Experiments on two challenging synthetic-to-real point cloud semantic segmentation tasks demonstrate that our approach achieves superior performance. Ablation studies confirm the effectiveness of the DPLF and PG-DAP modules. We release the code of our method in this paper.</li>
</ul>

<h3>Title: A U-Net and Transformer Pipeline for Multilingual Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Sahay, Radhika Agarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23554">https://arxiv.org/abs/2510.23554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23554">https://arxiv.org/pdf/2510.23554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23554]] A U-Net and Transformer Pipeline for Multilingual Image Translation(https://arxiv.org/abs/2510.23554)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural Machine Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images.</li>
</ul>

<h3>Title: EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT</h3>
<ul>
<li><strong>Authors: </strong>Baoqi Pei, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, Fei Wu, Yu Qiao, Jiangmiao Pang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23569">https://arxiv.org/abs/2510.23569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23569">https://arxiv.org/pdf/2510.23569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23569]] EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT(https://arxiv.org/abs/2510.23569)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models MLLMs, which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense hand-object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning RFT to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks. Full code and data are released at this https URL.</li>
</ul>

<h3>Title: More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hongkai Lin, Dingkang Liang, Mingyang Du, Xin Zhou, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23574">https://arxiv.org/abs/2510.23574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23574">https://arxiv.org/pdf/2510.23574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23574]] More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models(https://arxiv.org/abs/2510.23574)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative depth estimation methods leverage the rich visual priors stored in pre-trained text-to-image diffusion models, demonstrating astonishing zero-shot capability. However, parameter updates during training lead to catastrophic degra- dation in the image generation capability of the pre-trained model. We introduce MERGE, a unified model for image generation and depth estimation, starting from a fixed pre-trained text-to-image model. MERGE demonstrates that the pre-trained text-to-image model can do more than image generation, but also expand to depth estimation effortlessly. Specifically, MERGE introduces a play- and-plug framework that enables seamless switching between image generation and depth estimation modes through simple and pluggable converters. Meanwhile, we propose a Group Reuse Mechanism to encourage parameter reuse and im- prove the utilization of the additional learnable parameters. MERGE unleashes the powerful depth estimation capability of the pre-trained text-to-image model while preserving its original image generation ability. Compared to other unified models for image generation and depth estimation, MERGE achieves state-of- the-art performance across multiple depth estimation benchmarks. The code will be made available at this https URL</li>
</ul>

<h3>Title: Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Luis Ramos, Hiram Calvo, Olga Kolesnikova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23585">https://arxiv.org/abs/2510.23585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23585">https://arxiv.org/pdf/2510.23585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23585]] Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models(https://arxiv.org/abs/2510.23585)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The identification of hope speech has become a promised NLP task, considering the need to detect motivational expressions of agency and goal-directed behaviour on social media platforms. This proposal evaluates traditional machine learning models and fine-tuned transformers for a previously split hope speech dataset as train, development and test set. On development test, a linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM with RBF kernel reached 0.77, and Na√Øve Bayes hit 0.75. Transformer models delivered better results, the best model achieved weighted precision of 0.82, weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80 accuracy. These results suggest that while optimally configured traditional machine learning models remain agile, transformer architectures detect some subtle semantics of hope to achieve higher precision and recall in hope speech detection, suggesting that larges transformers and LLMs could perform better in small datasets.</li>
</ul>

<h3>Title: FARMER: Flow AutoRegressive Transformer over Pixels</h3>
<ul>
<li><strong>Authors: </strong>Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, Rui Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23588">https://arxiv.org/abs/2510.23588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23588">https://arxiv.org/pdf/2510.23588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23588]] FARMER: Flow AutoRegressive Transformer over Pixels(https://arxiv.org/abs/2510.23588)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.</li>
</ul>

<h3>Title: InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video Cameras</h3>
<ul>
<li><strong>Authors: </strong>Erich Liang, Roma Bhattacharjee, Sreemanti Dey, Rafael Moschopoulos, Caitlin Wang, Michel Liao, Grace Tan, Andrew Wang, Karhan Kayan, Stamatis Alexandropoulos, Jia Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23589">https://arxiv.org/abs/2510.23589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23589">https://arxiv.org/pdf/2510.23589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23589]] InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video Cameras(https://arxiv.org/abs/2510.23589)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurately tracking camera intrinsics is crucial for achieving 3D understanding from 2D video. However, most 3D algorithms assume that camera intrinsics stay constant throughout a video, which is often not true for many real-world in-the-wild videos. A major obstacle in this field is a lack of dynamic camera intrinsics benchmarks--existing benchmarks typically offer limited diversity in scene content and intrinsics variation, and none provide per-frame intrinsic changes for consecutive video frames. In this paper, we present Intrinsics in Flux (InFlux), a real-world benchmark that provides per-frame ground truth intrinsics annotations for videos with dynamic intrinsics. Compared to prior benchmarks, InFlux captures a wider range of intrinsic variations and scene diversity, featuring 143K+ annotated frames from 386 high-resolution indoor and outdoor videos with dynamic camera intrinsics. To ensure accurate per-frame intrinsics, we build a comprehensive lookup table of calibration experiments and extend the Kalibr toolbox to improve its accuracy and robustness. Using our benchmark, we evaluate existing baseline methods for predicting camera intrinsics and find that most struggle to achieve accurate predictions on videos with dynamic intrinsics. For the dataset, code, videos, and submission, please visit this https URL.</li>
</ul>

<h3>Title: Lightweight Robust Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Cheol Woo Kim, Shresth Verma, Mauricio Tec, Milind Tambe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23590">https://arxiv.org/abs/2510.23590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23590">https://arxiv.org/pdf/2510.23590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23590]] Lightweight Robust Direct Preference Optimization(https://arxiv.org/abs/2510.23590)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has become a popular method for fine-tuning large language models (LLMs) due to its stability and simplicity. However, it is also known to be sensitive to noise in the data and prone to overfitting. Recent works have proposed using distributionally robust optimization (DRO) to address potential noise and distributional shift in the data. However, these methods often suffer from excessive conservatism and high computational cost. We propose DPO-PRO (DPO with Preference Robustness), a robust fine-tuning algorithm based on DPO which accounts for uncertainty in the preference distribution through a lightweight DRO formulation. Unlike prior DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences, avoiding unnecessary conservatism and incurring negligible computational overhead. We further show that DPO-PRO is equivalent to a regularized DPO objective that penalizes model overconfidence under weak preference signals. We evaluate DPO-PRO on standard alignment benchmarks and a real-world public health task. Experimental results show that our method consistently improves robustness to noisy preference signals compared to existing DPO variants.</li>
</ul>

<h3>Title: Think Twice: Branch-and-Rethink Reasoning Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Yizhu Jiao, Jiaqi Zeng, Julien Veron Vialard, Oleksii Kuchaiev, Jiawei Han, Olivier Delalleau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23596">https://arxiv.org/abs/2510.23596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23596">https://arxiv.org/pdf/2510.23596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23596]] Think Twice: Branch-and-Rethink Reasoning Reward Model(https://arxiv.org/abs/2510.23596)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-oncescoringintofocused, second-lookreasoning, BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet consequential errors while remaining practical and scalable. Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains. The code and the model will be released soon.</li>
</ul>

<h3>Title: PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity</h3>
<ul>
<li><strong>Authors: </strong>Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23603">https://arxiv.org/abs/2510.23603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23603">https://arxiv.org/pdf/2510.23603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23603]] PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity(https://arxiv.org/abs/2510.23603)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.</li>
</ul>

<h3>Title: Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling</h3>
<ul>
<li><strong>Authors: </strong>Shuhong Zheng, Ashkan Mirzaei, Igor Gilitschenski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23605">https://arxiv.org/abs/2510.23605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23605">https://arxiv.org/pdf/2510.23605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23605]] Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling(https://arxiv.org/abs/2510.23605)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at this https URL.</li>
</ul>

<h3>Title: Variational Masked Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yichi Zhang, Alex Schwing, Zhizhen Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.23606">https://arxiv.org/abs/2510.23606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.23606">https://arxiv.org/pdf/2510.23606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.23606]] Variational Masked Diffusion Models(https://arxiv.org/abs/2510.23606)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully learns dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where learning of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
