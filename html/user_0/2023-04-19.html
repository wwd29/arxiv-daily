<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: An Ethereum-compatible blockchain that explicates and ensures design-level safety properties for smart contracts. (arXiv:2304.08655v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08655">http://arxiv.org/abs/2304.08655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08655] An Ethereum-compatible blockchain that explicates and ensures design-level safety properties for smart contracts](http://arxiv.org/abs/2304.08655) #secure</code></li>
<li>Summary: <p>Smart contracts are crucial elements of decentralized technologies, but they
face significant obstacles to trustworthiness due to security bugs and
trapdoors. To address the core issue, we propose a technology that enables
programmers to focus on design-level properties rather than specific low-level
attack patterns. Our proposed technology, called Theorem-Carrying-Transaction
(TCT), combines the benefits of runtime checking and symbolic proof. Under the
TCT protocol, every transaction must carry a theorem that proves its adherence
to the safety properties in the invoked contracts, and the blockchain checks
the proof before executing the transaction. The unique design of TCT ensures
that the theorems are provable and checkable in an efficient manner. We believe
that TCT holds a great promise for enabling provably secure smart contracts in
the future. As such, we call for collaboration toward this vision.
</p></li>
</ul>

<h3>Title: FlexiChain 2.0: NodeChain Assisting Integrated Decentralized Vault for Effective Data Authentication and Device Integrity in Complex Cyber-Physical Systems. (arXiv:2304.08713v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08713">http://arxiv.org/abs/2304.08713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08713] FlexiChain 2](http://arxiv.org/abs/2304.08713) #secure</code></li>
<li>Summary: <p>Distributed Ledger Technology (DLT) has been introduced using the most common
consensus algorithm either for an electronic cash system or a decentralized
programmable assets platform which provides general services. Most established
reliable networks are unsuitable for all applications such as smart cities
applications, and, in particular, Internet of Things (IoT) and Cyber Physical
Systems (CPS) applications. The purpose of this paper is to provide a suitable
DLT for IoT and CPS that could satisfy their requirements. The proposed work
has been designed based on the requirements of Cyber Physical Systems.
FlexiChain is proposed as a layer zero network that could be formed from
independent blockchains. Also, NodeChain has been introduced to be a
distributed (Unique ID) UID aggregation vault to secure all nodes' UIDs.
Moreover, NodeChain is proposed to serve mainly FlexiChain for all node
security requirements. NodeChain targets the security and integrity of each
node. Also, the linked UIDs create a chain of narration that keeps track not
merely for assets but also for who authenticated the assets. The security
results present a higher resistance against four types of attacks. Furthermore,
the strength of the network is presented from the early stages compared to
blockchain and central authority. FlexiChain technology has been introduced to
be a layer zero network for all CPS decentralized applications taking into
accounts their requirements. FlexiChain relies on lightweight processing
mechanisms and creates other methods to increase security.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: A Decentralized Authorization and Security Framework for Distributed Research Workflows. (arXiv:2304.08557v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08557">http://arxiv.org/abs/2304.08557</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08557] A Decentralized Authorization and Security Framework for Distributed Research Workflows](http://arxiv.org/abs/2304.08557) #security</code></li>
<li>Summary: <p>Research challenges such as climate change and the search for habitable
planets increasingly use academic and commercial computing resources
distributed across different institutions and physical sites. Furthermore, such
analyses often require a level of automation that precludes direct human
interaction, and securing these workflows involves adherence to security
policies across institutions. In this paper, we present a decentralized
authorization and security framework that enables researchers to utilize
resources across different sites while allowing service providers to maintain
autonomy over their secrets and authorization policies. We describe this
framework as part of the Tapis platform, a web-based, hosted API used by
researchers from multiple institutions, and we measure the performance of
various authorization and security queries, including cross-site queries. We
conclude with two use case studies -- a project at the University of Hawaii to
study climate change and the NASA NEID telescope project that searches the
galaxy for exoplanets.
</p></li>
</ul>

<h3>Title: In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT. (arXiv:2304.08979v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08979">http://arxiv.org/abs/2304.08979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08979] In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT](http://arxiv.org/abs/2304.08979) #security</code></li>
<li>Summary: <p>The way users acquire information is undergoing a paradigm shift with the
advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves
knowledge from the model itself and generates answers for users. ChatGPT's
impressive question-answering (QA) capability has attracted more than 100
million users within a short period of time but has also raised concerns
regarding its reliability. In this paper, we perform the first large-scale
measurement of ChatGPT's reliability in the generic QA scenario with a
carefully curated set of 5,695 questions across ten datasets and eight domains.
We find that ChatGPT's reliability varies across different domains, especially
underperforming in law and science questions. We also demonstrate that system
roles, originally designed by OpenAI to allow users to steer ChatGPT's
behavior, can impact ChatGPT's reliability. We further show that ChatGPT is
vulnerable to adversarial examples, and even a single character change can
negatively affect its reliability in certain cases. We believe that our study
provides valuable insights into ChatGPT's reliability and underscores the need
for strengthening the reliability and security of large language models (LLMs).
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees. (arXiv:2304.08928v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08928">http://arxiv.org/abs/2304.08928</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08928] ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees](http://arxiv.org/abs/2304.08928) #privacy</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have become a popular tool for learning on
graphs, but their widespread use raises privacy concerns as graph data can
contain personal or sensitive information. Differentially private GNN models
have been recently proposed to preserve privacy while still allowing for
effective learning over graph-structured datasets. However, achieving an ideal
balance between accuracy and privacy in GNNs remains challenging due to the
intrinsic structural connectivity of graphs. In this paper, we propose a new
differentially private GNN called ProGAP that uses a progressive training
scheme to improve such accuracy-privacy trade-offs. Combined with the
aggregation perturbation technique to ensure differential privacy, ProGAP
splits a GNN into a sequence of overlapping submodels that are trained
progressively, expanding from the first submodel to the complete model.
Specifically, each submodel is trained over the privately aggregated node
embeddings learned and cached by the previous submodels, leading to an
increased expressive power compared to previous approaches while limiting the
incurred privacy costs. We formally prove that ProGAP ensures edge-level and
node-level privacy guarantees for both training and inference stages, and
evaluate its performance on benchmark graph datasets. Experimental results
demonstrate that ProGAP can achieve up to 5%-10% higher accuracy than existing
state-of-the-art differentially private GNNs.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: InversOS: Efficient Control-Flow Protection for AArch64 Applications with Privilege Inversion. (arXiv:2304.08717v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08717">http://arxiv.org/abs/2304.08717</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08717] InversOS: Efficient Control-Flow Protection for AArch64 Applications with Privilege Inversion](http://arxiv.org/abs/2304.08717) #protect</code></li>
<li>Summary: <p>With the increasing popularity of AArch64 processors in general-purpose
computing, securing software running on AArch64 systems against control-flow
hijacking attacks has become a critical part toward secure computation. Shadow
stacks keep shadow copies of function return addresses and, when protected from
illegal modifications and coupled with forward-edge control-flow integrity,
form an effective and proven defense against such attacks. However, AArch64
lacks native support for write-protected shadow stacks, while software
alternatives either incur prohibitive performance overhead or provide weak
security guarantees.
</p></li>
</ul>

<p>We present InversOS, the first hardware-assisted write-protected shadow
stacks for AArch64 user-space applications, utilizing commonly available
features of AArch64 to achieve efficient intra-address space isolation (called
Privilege Inversion) required to protect shadow stacks. Privilege Inversion
adopts unconventional design choices that run protected applications in the
kernel mode and mark operating system (OS) kernel memory as user-accessible;
InversOS therefore uses a novel combination of OS kernel modifications,
compiler transformations, and another AArch64 feature to ensure the safety of
doing so and to support legacy applications. We show that InversOS is secure by
design, effective against various control-flow hijacking attacks, and
performant on selected benchmarks and applications (incurring overhead of 7.0%
on LMBench, 7.1% on SPEC CPU 2017, and 3.0% on Nginx web server).
</p>

<h2>defense</h2>
<h3>Title: GlobalMind: Global Multi-head Interactive Self-attention Network for Hyperspectral Change Detection. (arXiv:2304.08687v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08687">http://arxiv.org/abs/2304.08687</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08687] GlobalMind: Global Multi-head Interactive Self-attention Network for Hyperspectral Change Detection](http://arxiv.org/abs/2304.08687) #defense</code></li>
<li>Summary: <p>High spectral resolution imagery of the Earth's surface enables users to
monitor changes over time in fine-grained scale, playing an increasingly
important role in agriculture, defense, and emergency response. However, most
current algorithms are still confined to describing local features and fail to
incorporate a global perspective, which limits their ability to capture
interactions between global features, thus usually resulting in incomplete
change regions. In this paper, we propose a Global Multi-head INteractive
self-attention change Detection network (GlobalMind) to explore the implicit
correlation between different surface objects and variant land cover
transformations, acquiring a comprehensive understanding of the data and
accurate change detection result. Firstly, a simple but effective Global Axial
Segmentation (GAS) strategy is designed to expand the self-attention
computation along the row space or column space of hyperspectral images,
allowing the global connection with high efficiency. Secondly, with GAS, the
global spatial multi-head interactive self-attention (Global-M) module is
crafted to mine the abundant spatial-spectral feature involving potential
correlations between the ground objects from the entire rich and complex
hyperspectral space. Moreover, to acquire the accurate and complete
cross-temporal changes, we devise a global temporal interactive multi-head
self-attention (GlobalD) module which incorporates the relevance and variation
of bi-temporal spatial-spectral features, deriving the integrate potential same
kind of changes in the local and global range with the combination of GAS. We
perform extensive experiments on five mostly used hyperspectral datasets, and
our method outperforms the state-of-the-art algorithms with high accuracy and
efficiency.
</p></li>
</ul>

<h3>Title: GrOVe: Ownership Verification of Graph Neural Networks using Embeddings. (arXiv:2304.08566v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08566">http://arxiv.org/abs/2304.08566</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08566] GrOVe: Ownership Verification of Graph Neural Networks using Embeddings](http://arxiv.org/abs/2304.08566) #defense</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have emerged as a state-of-the-art approach to
model and draw inferences from large scale graph-structured data in various
application settings such as social networking. The primary goal of a GNN is to
learn an embedding for each graph node in a dataset that encodes both the node
features and the local graph structure around the node. Embeddings generated by
a GNN for a graph node are unique to that GNN. Prior work has shown that GNNs
are prone to model extraction attacks. Model extraction attacks and defenses
have been explored extensively in other non-graph settings. While detecting or
preventing model extraction appears to be difficult, deterring them via
effective ownership verification techniques offer a potential defense. In
non-graph settings, fingerprinting models, or the data used to build them, have
shown to be a promising approach toward ownership verification. We present
GrOVe, a state-of-the-art GNN model fingerprinting scheme that, given a target
model and a suspect model, can reliably determine if the suspect model was
trained independently of the target model or if it is a surrogate of the target
model obtained via model extraction. We show that GrOVe can distinguish between
surrogate and independent models even when the independent model uses the same
training dataset and architecture as the original target model. Using six
benchmark datasets and three model architectures, we show that consistently
achieves low false-positive and false-negative rates. We demonstrate that is
robust against known fingerprint evasion techniques while remaining
computationally efficient.
</p></li>
</ul>

<h3>Title: Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08767">http://arxiv.org/abs/2304.08767</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08767] Masked Language Model Based Textual Adversarial Example Detection](http://arxiv.org/abs/2304.08767) #defense</code></li>
<li>Summary: <p>Adversarial attacks are a serious threat to the reliable deployment of
machine learning models in safety-critical applications. They can misguide
current models to predict incorrectly by slightly modifying the inputs.
Recently, substantial work has shown that adversarial examples tend to deviate
from the underlying data manifold of normal examples, whereas pre-trained
masked language models can fit the manifold of normal NLP data. To explore how
to use the masked language model in adversarial detection, we propose a novel
textual adversarial example detection method, namely Masked Language
Model-based Detection (MLMD), which can produce clearly distinguishable signals
between normal examples and adversarial examples by exploring the changes in
manifolds induced by the masked language model. MLMD features a plug and play
usage (i.e., no need to retrain the victim model) for adversarial defense and
it is agnostic to classification tasks, victim model's architectures, and
to-be-defended attack methods. We evaluate MLMD on various benchmark textual
datasets, widely studied machine learning models, and state-of-the-art (SOTA)
adversarial attacks (in total $3<em>4</em>4 = 48$ settings). Experimental results show
that MLMD can achieve strong performance, with detection accuracy up to 0.984,
0.967, and 0.901 on AG-NEWS, IMDB, and SST-2 datasets, respectively.
Additionally, MLMD is superior, or at least comparable to, the SOTA detection
defenses in detection accuracy and F1 score. Among many defenses based on the
off-manifold assumption of adversarial examples, this work offers a new angle
for capturing the manifold change. The code for this work is openly accessible
at \url{https://github.com/mlmddetection/MLMDdetection}.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08968">http://arxiv.org/abs/2304.08968</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08968] Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs](http://arxiv.org/abs/2304.08968) #attack</code></li>
<li>Summary: <p>The self-attention revolution allowed generative language models to scale and
achieve increasingly impressive abilities. Such models - commonly referred to
as Large Language Models (LLMs) - have recently gained prominence with the
general public, thanks to conversational fine-tuning, putting their behavior in
line with public expectations regarding AI. This prominence amplified prior
concerns regarding the misuse of LLMs and led to the emergence of numerous
tools to detect LLMs in the wild.
</p></li>
</ul>

<p>Unfortunately, most such tools are critically flawed. While major
publications in the LLM detectability field suggested that LLMs were easy to
detect with fine-tuned autoencoders, the limitations of their results are easy
to overlook. Specifically, they assumed publicly available generative models
without fine-tunes or non-trivial prompts. While the importance of these
assumptions has been demonstrated, until now, it remained unclear how well such
detection could be countered.
</p>
<p>Here, we show that an attacker with access to such detectors' reference human
texts and output not only evades detection but can fully frustrate the detector
training - with a reasonable budget and all its outputs labeled as such.
Achieving it required combining common "reinforcement from critic" loss
function modification and AdamW optimizer, which led to surprisingly good
fine-tuning generalization. Finally, we warn against the temptation to
transpose the conclusions obtained in RNN-driven text GANs to LLMs due to their
better representative ability.
</p>
<p>These results have critical implications for the detection and prevention of
malicious use of generative language models, and we hope they will aid the
designers of generative models and detectors.
</p>

<h3>Title: Towards the Transferable Audio Adversarial Attack via Ensemble Methods. (arXiv:2304.08811v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08811">http://arxiv.org/abs/2304.08811</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08811] Towards the Transferable Audio Adversarial Attack via Ensemble Methods](http://arxiv.org/abs/2304.08811) #attack</code></li>
<li>Summary: <p>In recent years, deep learning (DL) models have achieved significant progress
in many domains, such as autonomous driving, facial recognition, and speech
recognition. However, the vulnerability of deep learning models to adversarial
attacks has raised serious concerns in the community because of their
insufficient robustness and generalization. Also, transferable attacks have
become a prominent method for black-box attacks. In this work, we explore the
potential factors that impact adversarial examples (AEs) transferability in
DL-based speech recognition. We also discuss the vulnerability of different DL
systems and the irregular nature of decision boundaries. Our results show a
remarkable difference in the transferability of AEs between speech and images,
with the data relevance being low in images but opposite in speech recognition.
Motivated by dropout-based ensemble approaches, we propose random gradient
ensembles and dynamic gradient-weighted ensembles, and we evaluate the impact
of ensembles on the transferability of AEs. The results show that the AEs
created by both approaches are valid for transfer to the black box API.
</p></li>
</ul>

<h3>Title: BadVFL: Backdoor Attacks in Vertical Federated Learning. (arXiv:2304.08847v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08847">http://arxiv.org/abs/2304.08847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08847] BadVFL: Backdoor Attacks in Vertical Federated Learning](http://arxiv.org/abs/2304.08847) #attack</code></li>
<li>Summary: <p>Federated learning (FL) enables multiple parties to collaboratively train a
machine learning model without sharing their data; rather, they train their own
model locally and send updates to a central server for aggregation. Depending
on how the data is distributed among the participants, FL can be classified
into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the
same set of training instances but only host a different and non-overlapping
subset of the whole feature space. Whereas in HFL, each participant shares the
same set of features while the training set is split into locally owned
training data subsets.
</p></li>
</ul>

<p>VFL is increasingly used in applications like financial fraud detection;
nonetheless, very little work has analyzed its security. In this paper, we
focus on robustness in VFL, in particular, on backdoor attacks, whereby an
adversary attempts to manipulate the aggregate model during the training
process to trigger misclassifications. Performing backdoor attacks in VFL is
more challenging than in HFL, as the adversary i) does not have access to the
labels during training and ii) cannot change the labels as she only has access
to the feature embeddings. We present a first-of-its-kind clean-label backdoor
attack in VFL, which consists of two phases: a label inference and a backdoor
phase. We demonstrate the effectiveness of the attack on three different
datasets, investigate the factors involved in its success, and discuss
countermeasures to mitigate its impact.
</p>

<h2>robust</h2>
<h3>Title: RS2G: Data-Driven Scene-Graph Extraction and Embedding for Robust Autonomous Perception and Scenario Understanding. (arXiv:2304.08600v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08600">http://arxiv.org/abs/2304.08600</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08600] RS2G: Data-Driven Scene-Graph Extraction and Embedding for Robust Autonomous Perception and Scenario Understanding](http://arxiv.org/abs/2304.08600) #robust</code></li>
<li>Summary: <p>Human drivers naturally reason about interactions between road users to
understand and safely navigate through traffic. Thus, developing autonomous
vehicles necessitates the ability to mimic such knowledge and model
interactions between road users to understand and navigate unpredictable,
dynamic environments. However, since real-world scenarios often differ from
training datasets, effectively modeling the behavior of various road users in
an environment remains a significant research challenge. This reality
necessitates models that generalize to a broad range of domains and explicitly
model interactions between road users and the environment to improve scenario
understanding. Graph learning methods address this problem by modeling
interactions using graph representations of scenarios. However, existing
methods cannot effectively transfer knowledge gained from the training domain
to real-world scenarios. This constraint is caused by the domain-specific rules
used for graph extraction that can vary in effectiveness across domains,
limiting generalization ability. To address these limitations, we propose
RoadScene2Graph (RS2G): a data-driven graph extraction and modeling approach
that learns to extract the best graph representation of a road scene for
solving autonomous scene understanding tasks. We show that RS2G enables better
performance at subjective risk assessment than rule-based graph extraction
methods and deep-learning-based models. RS2G also improves generalization and
Sim2Real transfer learning, which denotes the ability to transfer knowledge
gained from simulation datasets to unseen real-world scenarios. We also present
ablation studies showing how RS2G produces a more useful graph representation
for downstream classifiers. Finally, we show how RS2G can identify the relative
importance of rule-based graph edges and enables intelligent graph sparsity
tuning.
</p></li>
</ul>

<h3>Title: ProPanDL: A Modular Architecture for Uncertainty-Aware Panoptic Segmentation. (arXiv:2304.08645v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08645">http://arxiv.org/abs/2304.08645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08645] ProPanDL: A Modular Architecture for Uncertainty-Aware Panoptic Segmentation](http://arxiv.org/abs/2304.08645) #robust</code></li>
<li>Summary: <p>We introduce ProPanDL, a family of networks capable of uncertainty-aware
panoptic segmentation. Unlike existing segmentation methods, ProPanDL is
capable of estimating full probability distributions for both the semantic and
spatial aspects of panoptic segmentation. We implement and evaluate ProPanDL
variants capable of estimating both parametric (Variance Network) and
parameter-free (SampleNet) distributions quantifying pixel-wise spatial
uncertainty. We couple these approaches with two methods (Temperature Scaling
and Evidential Deep Learning) for semantic uncertainty estimation. To evaluate
the uncertainty-aware panoptic segmentation task, we address limitations with
existing approaches by proposing new metrics that enable separate evaluation of
spatial and semantic uncertainty. We additionally propose the use of the energy
score, a proper scoring rule, for more robust evaluation of spatial output
distributions. Using these metrics, we conduct an extensive evaluation of
ProPanDL variants. Our results demonstrate that ProPanDL is capable of
estimating well-calibrated and meaningful output distributions while still
retaining strong performance on the base panoptic segmentation task.
</p></li>
</ul>

<h3>Title: Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections. (arXiv:2304.08706v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08706">http://arxiv.org/abs/2304.08706</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08706] Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections](http://arxiv.org/abs/2304.08706) #robust</code></li>
<li>Summary: <p>Neural implicit methods have achieved high-quality 3D object surfaces under
slight specular highlights. However, high specular reflections (HSR) often
appear in front of target objects when we capture them through glasses. The
complex ambiguity in these scenes violates the multi-view consistency, then
makes it challenging for recent methods to reconstruct target objects
correctly. To remedy this issue, we present a novel surface reconstruction
framework, NeuS-HSR, based on implicit neural rendering. In NeuS-HSR, the
object surface is parameterized as an implicit signed distance function (SDF).
To reduce the interference of HSR, we propose decomposing the rendered image
into two appearances: the target object and the auxiliary plane. We design a
novel auxiliary plane module by combining physical assumptions and neural
networks to generate the auxiliary plane appearance. Extensive experiments on
synthetic and real-world datasets demonstrate that NeuS-HSR outperforms
state-of-the-art approaches for accurate and robust target surface
reconstruction against HSR. Code is available at
https://github.com/JiaxiongQ/NeuS-HSR.
</p></li>
</ul>

<h3>Title: You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object Tracking. (arXiv:2304.08709v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08709">http://arxiv.org/abs/2304.08709</a></li>
<li>Code URL: <a href="https://github.com/wangxiyang2022/YONTD-MOT">https://github.com/wangxiyang2022/YONTD-MOT</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08709] You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object Tracking](http://arxiv.org/abs/2304.08709) #robust</code></li>
<li>Summary: <p>Firstly, a new multi-object tracking framework is proposed in this paper
based on multi-modal fusion. By integrating object detection and multi-object
tracking into the same model, this framework avoids the complex data
association process in the classical TBD paradigm, and requires no additional
training. Secondly, confidence of historical trajectory regression is explored,
possible states of a trajectory in the current frame (weak object or strong
object) are analyzed and a confidence fusion module is designed to guide
non-maximum suppression of trajectory and detection for ordered association.
Finally, extensive experiments are conducted on the KITTI and Waymo datasets.
The results show that the proposed method can achieve robust tracking by using
only two modal detectors and it is more accurate than many of the latest TBD
paradigm-based multi-modal tracking methods. The source codes of the proposed
method are available at https://github.com/wangxiyang2022/YONTD-MOT
</p></li>
</ul>

<h3>Title: TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08821">http://arxiv.org/abs/2304.08821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08821] TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models](http://arxiv.org/abs/2304.08821) #robust</code></li>
<li>Summary: <p>Data augmentation has been established as an efficacious approach to
supplement useful information for low-resource datasets. Traditional
augmentation techniques such as noise injection and image transformations have
been widely used. In addition, generative data augmentation (GDA) has been
shown to produce more diverse and flexible data. While generative adversarial
networks (GANs) have been frequently used for GDA, they lack diversity and
controllability compared to text-to-image diffusion models. In this paper, we
propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the
capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image
(T2I) generative models for data augmentation. By conditioning the T2I model on
detailed descriptions produced by T2T models, we are able to generate
photo-realistic labeled images in a flexible and controllable manner.
Experiments on in-domain classification, cross-domain classification, and image
captioning tasks show consistent improvements over other data augmentation
baselines. Analytical studies in varied settings, including few-shot,
long-tail, and adversarial, further reinforce the effectiveness of TTIDA in
enhancing performance and increasing robustness.
</p></li>
</ul>

<h3>Title: SDFReg: Learning Signed Distance Functions for Point Cloud Registration. (arXiv:2304.08929v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08929">http://arxiv.org/abs/2304.08929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08929] SDFReg: Learning Signed Distance Functions for Point Cloud Registration](http://arxiv.org/abs/2304.08929) #robust</code></li>
<li>Summary: <p>Learning-based point cloud registration methods can handle clean point clouds
well, while it is still challenging to generalize to noisy and partial point
clouds. To this end, we propose a novel framework for noisy and partial point
cloud registration. By introducing a neural implicit function representation,
we replace the problem of rigid registration between point clouds with a
registration problem between the point cloud and the neural implicit function.
We then alternately optimize the implicit function representation and the
registration between the implicit function and point cloud. In this way, point
cloud registration can be performed in a coarse-to-fine manner. Since our
method avoids computing point correspondences, it is robust to the noise and
incompleteness of point clouds. Compared with the registration methods based on
global features, our method can deal with surfaces with large density
variations and achieve higher registration accuracy. Experimental results and
comparisons demonstrate the effectiveness of the proposed framework.
</p></li>
</ul>

<h3>Title: PG-VTON: A Novel Image-Based Virtual Try-On Method via Progressive Inference Paradigm. (arXiv:2304.08956v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08956">http://arxiv.org/abs/2304.08956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08956] PG-VTON: A Novel Image-Based Virtual Try-On Method via Progressive Inference Paradigm](http://arxiv.org/abs/2304.08956) #robust</code></li>
<li>Summary: <p>Virtual try-on is a promising computer vision topic with a high commercial
value wherein a new garment is visually worn on a person with a photo-realistic
effect. Previous studies conduct their shape and content inference at one
stage, employing a single-scale warping mechanism and a relatively
unsophisticated content inference mechanism. These approaches have led to
suboptimal results in terms of garment warping and skin reservation under
challenging try-on scenarios. To address these limitations, we propose a novel
virtual try-on method via progressive inference paradigm (PGVTON) that
leverages a top-down inference pipeline and a general garment try-on strategy.
Specifically, we propose a robust try-on parsing inference method by
disentangling semantic categories and introducing consistency. Exploiting the
try-on parsing as the shape guidance, we implement the garment try-on via
warping-mapping-composition. To facilitate adaptation to a wide range of try-on
scenarios, we adopt a covering more and selecting one warping strategy and
explicitly distinguish tasks based on alignment. Additionally, we regulate
StyleGAN2 to implement re-naked skin inpainting, conditioned on the target skin
shape and spatial-agnostic skin features. Experiments demonstrate that our
method has state-of-the-art performance under two challenging scenarios. The
code will be available at https://github.com/NerdFNY/PGVTON.
</p></li>
</ul>

<h3>Title: Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Motion Compensation. (arXiv:2304.08978v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08978">http://arxiv.org/abs/2304.08978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08978] Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Motion Compensation](http://arxiv.org/abs/2304.08978) #robust</code></li>
<li>Summary: <p>This paper presents a novel visual-LiDAR odometry and mapping method with
low-drift characteristics. The proposed method is based on two popular
approaches, ORB-SLAM and A-LOAM, with monocular scale correction and
visual-assisted LiDAR motion compensation modifications. The scale corrector
calculates the proportion between the depth of image keypoints recovered by
triangulation and that provided by LiDAR, using an outlier rejection process
for accuracy improvement. Concerning LiDAR motion compensation, the visual
odometry approach gives the initial guesses of LiDAR motions for better
performance. This methodology is not only applicable to high-resolution LiDAR
but can also adapt to low-resolution LiDAR. To evaluate the proposed SLAM
system's robustness and accuracy, we conducted experiments on the KITTI
Odometry and S3E datasets. Experimental results illustrate that our method
significantly outperforms standalone ORB-SLAM2 and A-LOAM. Furthermore,
regarding the accuracy of visual odometry with scale correction, our method
performs similarly to the stereo-mode ORB-SLAM2.
</p></li>
</ul>

<h3>Title: MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning. (arXiv:2304.08981v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08981">http://arxiv.org/abs/2304.08981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08981] MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning](http://arxiv.org/abs/2304.08981) #robust</code></li>
<li>Summary: <p>Over the past few decades, multimodal emotion recognition has made remarkable
progress with the development of deep learning. However, existing technologies
are difficult to meet the demand for practical applications. To improve the
robustness, we launch a Multimodal Emotion Recognition Challenge (MER 2023) to
motivate global researchers to build innovative technologies that can further
accelerate and foster research. For this year's challenge, we present three
distinct sub-challenges: (1) MER-MULTI, in which participants recognize both
discrete and dimensional emotions; (2) MER-NOISE, in which noise is added to
test videos for modality robustness evaluation; (3) MER-SEMI, which provides
large amounts of unlabeled samples for semi-supervised learning. In this paper,
we test a variety of multimodal features and provide a competitive baseline for
each sub-challenge. Our system achieves 77.57% on the F1 score and 0.82 on the
mean squared error (MSE) for MER-MULTI, 69.82% on the F1 score and 1.12 on MSE
for MER-NOISE, and 86.75% on the F1 score for MER-SEMI, respectively. Baseline
code is available at https://github.com/zeroQiaoba/MER2023-Baseline.
</p></li>
</ul>

<h3>Title: Robustness of Visual Explanations to Common Data Augmentation. (arXiv:2304.08984v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08984">http://arxiv.org/abs/2304.08984</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08984] Robustness of Visual Explanations to Common Data Augmentation](http://arxiv.org/abs/2304.08984) #robust</code></li>
<li>Summary: <p>As the use of deep neural networks continues to grow, understanding their
behaviour has become more crucial than ever. Post-hoc explainability methods
are a potential solution, but their reliability is being called into question.
Our research investigates the response of post-hoc visual explanations to
naturally occurring transformations, often referred to as augmentations. We
anticipate explanations to be invariant under certain transformations, such as
changes to the colour map while responding in an equivariant manner to
transformations like translation, object scaling, and rotation. We have found
remarkable differences in robustness depending on the type of transformation,
with some explainability methods (such as LRP composites and Guided Backprop)
being more stable than others. We also explore the role of training with data
augmentation. We provide evidence that explanations are typically less robust
to augmentation than classification performance, regardless of whether data
augmentation is used in training or not.
</p></li>
</ul>

<h3>Title: CDFI: Cross Domain Feature Interaction for Robust Bronchi Lumen Detection. (arXiv:2304.09115v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.09115">http://arxiv.org/abs/2304.09115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.09115] CDFI: Cross Domain Feature Interaction for Robust Bronchi Lumen Detection](http://arxiv.org/abs/2304.09115) #robust</code></li>
<li>Summary: <p>Endobronchial intervention is increasingly used as a minimally invasive means
for the treatment of pulmonary diseases. In order to reduce the difficulty of
manipulation in complex airway networks, robust lumen detection is essential
for intraoperative guidance. However, these methods are sensitive to visual
artifacts which are inevitable during the surgery. In this work, a cross domain
feature interaction (CDFI) network is proposed to extract the structural
features of lumens, as well as to provide artifact cues to characterize the
visual features. To effectively extract the structural and artifact features,
the Quadruple Feature Constraints (QFC) module is designed to constrain the
intrinsic connections of samples with various imaging-quality. Furthermore, we
design a Guided Feature Fusion (GFF) module to supervise the model for adaptive
feature fusion based on different types of artifacts. Results show that the
features extracted by the proposed method can preserve the structural
information of lumen in the presence of large visual variations, bringing
much-improved lumen detection accuracy.
</p></li>
</ul>

<h3>Title: Variational Relational Point Completion Network for Robust 3D Classification. (arXiv:2304.09131v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.09131">http://arxiv.org/abs/2304.09131</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.09131] Variational Relational Point Completion Network for Robust 3D Classification](http://arxiv.org/abs/2304.09131) #robust</code></li>
<li>Summary: <p>Real-scanned point clouds are often incomplete due to viewpoint, occlusion,
and noise, which hampers 3D geometric modeling and perception. Existing point
cloud completion methods tend to generate global shape skeletons and hence lack
fine local details. Furthermore, they mostly learn a deterministic
partial-to-complete mapping, but overlook structural relations in man-made
objects. To tackle these challenges, this paper proposes a variational
framework, Variational Relational point Completion Network (VRCNet) with two
appealing properties: 1) Probabilistic Modeling. In particular, we propose a
dual-path architecture to enable principled probabilistic modeling across
partial and complete clouds. One path consumes complete point clouds for
reconstruction by learning a point VAE. The other path generates complete
shapes for partial point clouds, whose embedded distribution is guided by
distribution obtained from the reconstruction path during training. 2)
Relational Enhancement. Specifically, we carefully design point self-attention
kernel and point selective kernel module to exploit relational point features,
which refines local shape details conditioned on the coarse completion. In
addition, we contribute multi-view partial point cloud datasets (MVP and MVP-40
dataset) containing over 200,000 high-quality scans, which render partial 3D
shapes from 26 uniformly distributed camera poses for each 3D CAD model.
Extensive experiments demonstrate that VRCNet outperforms state-of-the-art
methods on all standard point cloud completion benchmarks. Notably, VRCNet
shows great generalizability and robustness on real-world point cloud scans.
Moreover, we can achieve robust 3D classification for partial point clouds with
the help of VRCNet, which can highly increase classification accuracy.
</p></li>
</ul>

<h3>Title: Tailoring Domain Adaptation for Machine Translation Quality Estimation. (arXiv:2304.08891v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08891">http://arxiv.org/abs/2304.08891</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08891] Tailoring Domain Adaptation for Machine Translation Quality Estimation](http://arxiv.org/abs/2304.08891) #robust</code></li>
<li>Summary: <p>While quality estimation (QE) can play an important role in the translation
process, its effectiveness relies on the availability and quality of training
data. For QE in particular, high-quality labeled data is often lacking due to
the high-cost and effort associated with labeling such data. Aside from the
data scarcity challenge, QE models should also be generalizable, i.e., they
should be able to handle data from different domains, both generic and
specific. To alleviate these two main issues -- data scarcity and domain
mismatch -- this paper combines domain adaptation and data augmentation within
a robust QE system. Our method is to first train a generic QE model and then
fine-tune it on a specific domain while retaining generic knowledge. Our
results show a significant improvement for all the language pairs investigated,
better cross-lingual inference, and a superior performance in zero-shot
learning scenarios as compared to state-of-the-art baselines.
</p></li>
</ul>

<h3>Title: CyFormer: Accurate State-of-Health Prediction of Lithium-Ion Batteries via Cyclic Attention. (arXiv:2304.08502v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08502">http://arxiv.org/abs/2304.08502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08502] CyFormer: Accurate State-of-Health Prediction of Lithium-Ion Batteries via Cyclic Attention](http://arxiv.org/abs/2304.08502) #robust</code></li>
<li>Summary: <p>Predicting the State-of-Health (SoH) of lithium-ion batteries is a
fundamental task of battery management systems on electric vehicles. It aims at
estimating future SoH based on historical aging data. Most existing deep
learning methods rely on filter-based feature extractors (e.g., CNN or Kalman
filters) and recurrent time sequence models. Though efficient, they generally
ignore cyclic features and the domain gap between training and testing
batteries. To address this problem, we present CyFormer, a transformer-based
cyclic time sequence model for SoH prediction. Instead of the conventional
CNN-RNN structure, we adopt an encoder-decoder architecture. In the encoder,
row-wise and column-wise attention blocks effectively capture intra-cycle and
inter-cycle connections and extract cyclic features. In the decoder, the SoH
queries cross-attend to these features to form the final predictions. We
further utilize a transfer learning strategy to narrow the domain gap between
the training and testing set. To be specific, we use fine-tuning to shift the
model to a target working condition. Finally, we made our model more efficient
by pruning. The experiment shows that our method attains an MAE of 0.75\% with
only 10\% data for fine-tuning on a testing battery, surpassing prior methods
by a large margin. Effective and robust, our method provides a potential
solution for all cyclic time sequence prediction tasks.
</p></li>
</ul>

<h3>Title: W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08754">http://arxiv.org/abs/2304.08754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08754] W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting](http://arxiv.org/abs/2304.08754) #robust</code></li>
<li>Summary: <p>Weather forecasting is a long-standing computational challenge with direct
societal and economic impacts. This task involves a large amount of continuous
data collection and exhibits rich spatiotemporal dependencies over long
periods, making it highly suitable for deep learning models. In this paper, we
apply pre-training techniques to weather forecasting and propose W-MAE, a
Weather model with Masked AutoEncoder pre-training for multi-variable weather
forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct
spatial correlations within meteorological variables. On the temporal scale, we
fine-tune the pre-trained W-MAE to predict the future states of meteorological
variables, thereby modeling the temporal dependencies present in weather data.
We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data,
with samples selected every six hours and using only two years of data. Under
the same training data conditions, we compare W-MAE with FourCastNet, and W-MAE
outperforms FourCastNet in precipitation forecasting. In the setting where the
training data is far less than that of FourCastNet, our model still performs
much better in precipitation prediction (0.80 vs. 0.98). Additionally,
experiments show that our model has a stable and significant advantage in
short-to-medium-range forecasting (i.e., forecasting time ranges from 6 hours
to one week), and the longer the prediction time, the more evident the
performance advantage of W-MAE, further proving its robustness.
</p></li>
</ul>

<h3>Title: A Domain-Region Based Evaluation of ML Performance Robustness to Covariate Shift. (arXiv:2304.08855v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08855">http://arxiv.org/abs/2304.08855</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08855] A Domain-Region Based Evaluation of ML Performance Robustness to Covariate Shift](http://arxiv.org/abs/2304.08855) #robust</code></li>
<li>Summary: <p>Most machine learning methods assume that the input data distribution is the
same in the training and testing phases. However, in practice, this
stationarity is usually not met and the distribution of inputs differs, leading
to unexpected performance of the learned model in deployment. The issue in
which the training and test data inputs follow different probability
distributions while the input-output relationship remains unchanged is referred
to as covariate shift. In this paper, the performance of conventional machine
learning models was experimentally evaluated in the presence of covariate
shift. Furthermore, a region-based evaluation was performed by decomposing the
domain of probability density function of the input data to assess the
classifier's performance per domain region. Distributional changes were
simulated in a two-dimensional classification problem. Subsequently, a higher
four-dimensional experiments were conducted. Based on the experimental
analysis, the Random Forests algorithm is the most robust classifier in the
two-dimensional case, showing the lowest degradation rate for accuracy and
F1-score metrics, with a range between 0.1% and 2.08%. Moreover, the results
reveal that in higher-dimensional experiments, the performance of the models is
predominantly influenced by the complexity of the classification function,
leading to degradation rates exceeding 25% in most cases. It is also concluded
that the models exhibit high bias towards the region with high density in the
input space domain of the training samples.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Deep Unrestricted Document Image Rectification. (arXiv:2304.08796v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08796">http://arxiv.org/abs/2304.08796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08796] Deep Unrestricted Document Image Rectification](http://arxiv.org/abs/2304.08796) #extraction</code></li>
<li>Summary: <p>In recent years, tremendous efforts have been made on document image
rectification, but existing advanced algorithms are limited to processing
restricted document images, i.e., the input images must incorporate a complete
document. Once the captured image merely involves a local text region, its
rectification quality is degraded and unsatisfactory. Our previously proposed
DocTr, a transformer-assisted network for document image rectification, also
suffers from this limitation. In this work, we present DocTr++, a novel unified
framework for document image rectification, without any restrictions on the
input distorted images. Our major technical improvements can be concluded in
three aspects. Firstly, we upgrade the original architecture by adopting a
hierarchical encoder-decoder structure for multi-scale representation
extraction and parsing. Secondly, we reformulate the pixel-wise mapping
relationship between the unrestricted distorted document images and the
distortion-free counterparts. The obtained data is used to train our DocTr++
for unrestricted document image rectification. Thirdly, we contribute a
real-world test set and metrics applicable for evaluating the rectification
quality. To our best knowledge, this is the first learning-based method for the
rectification of unrestricted document images. Extensive experiments are
conducted, and the results demonstrate the effectiveness and superiority of our
method. We hope our DocTr++ will serve as a strong baseline for generic
document image rectification, prompting the further advancement and application
of learning-based algorithms. The source code and the proposed dataset are
publicly available at https://github.com/fh2019ustc/DocTr-Plus.
</p></li>
</ul>

<h3>Title: Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08801">http://arxiv.org/abs/2304.08801</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08801] Speaker Profiling in Multiparty Conversations](http://arxiv.org/abs/2304.08801) #extraction</code></li>
<li>Summary: <p>In conversational settings, individuals exhibit unique behaviors, rendering a
one-size-fits-all approach insufficient for generating responses by dialogue
agents. Although past studies have aimed to create personalized dialogue agents
using speaker persona information, they have relied on the assumption that the
speaker's persona is already provided. However, this assumption is not always
valid, especially when it comes to chatbots utilized in industries like
banking, hotel reservations, and airline bookings. This research paper aims to
fill this gap by exploring the task of Speaker Profiling in Conversations
(SPC). The primary objective of SPC is to produce a summary of persona
characteristics for each individual speaker present in a dialogue. To
accomplish this, we have divided the task into three subtasks: persona
discovery, persona-type identification, and persona-value extraction. Given a
dialogue, the first subtask aims to identify all utterances that contain
persona information. Subsequently, the second task evaluates these utterances
to identify the type of persona information they contain, while the third
subtask identifies the specific persona values for each identified type. To
address the task of SPC, we have curated a new dataset named SPICE, which comes
with specific labels. We have evaluated various baselines on this dataset and
benchmarked it with a new neural model, SPOT, which we introduce in this paper.
Furthermore, we present a comprehensive analysis of SPOT, examining the
limitations of individual modules both quantitatively and qualitatively.
</p></li>
</ul>

<h3>Title: A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese. (arXiv:2304.08999v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08999">http://arxiv.org/abs/2304.08999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08999] A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese](http://arxiv.org/abs/2304.08999) #extraction</code></li>
<li>Summary: <p>Textual health records of cancer patients are usually protracted and highly
unstructured, making it very time-consuming for health professionals to get a
complete overview of the patient's therapeutic course. As such limitations can
lead to suboptimal and/or inefficient treatment procedures, healthcare
providers would greatly benefit from a system that effectively summarizes the
information of those records. With the advent of deep neural models, this
objective has been partially attained for English clinical texts, however, the
research community still lacks an effective solution for languages with limited
resources. In this paper, we present the approach we developed to extract
procedures, drugs, and diseases from oncology health records written in
European Portuguese. This project was conducted in collaboration with the
Portuguese Institute for Oncology which, besides holding over $10$ years of
duly protected medical records, also provided oncologist expertise throughout
the development of the project. Since there is no annotated corpus for
biomedical entity extraction in Portuguese, we also present the strategy we
followed in annotating the corpus for the development of the models. The final
models, which combined a neural architecture with entity linking, achieved
$F_1$ scores of $88.6$, $95.0$, and $55.8$ per cent in the mention extraction
of procedures, drugs, and diseases, respectively.
</p></li>
</ul>

<h3>Title: CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.09048">http://arxiv.org/abs/2304.09048</a></li>
<li>Code URL: <a href="https://github.com/zjunlp/DeepKE/tree/main/example/llm">https://github.com/zjunlp/DeepKE/tree/main/example/llm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.09048] CodeKGC: Code Language Model for Generative Knowledge Graph Construction](http://arxiv.org/abs/2304.09048) #extraction</code></li>
<li>Summary: <p>Current generative knowledge graph construction approaches usually fail to
capture structural knowledge by simply flattening natural language into
serialized texts or a specification language. However, large generative
language model trained on structured data such as code has demonstrated
impressive capability in understanding natural language for structural
prediction and reasoning tasks. Intuitively, we address the task of generative
knowledge graph construction with code language model: given a code-format
natural language input, the target is to generate triples which can be
represented as code completion tasks. Specifically, we develop schema-aware
prompts that effectively utilize the semantic structure within the knowledge
graph. As code inherently possesses structure, such as class and function
definitions, it serves as a useful model for prior semantic structural
knowledge. Furthermore, we employ a rationale-enhanced generation method to
boost the performance. Rationales provide intermediate steps, thereby improving
knowledge extraction abilities. Experimental results indicate that the proposed
approach can obtain better performance on benchmark datasets compared with
baselines. Code and datasets are available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Crossing Roads of Federated Learning and Smart Grids: Overview, Challenges, and Perspectives. (arXiv:2304.08602v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08602">http://arxiv.org/abs/2304.08602</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08602] Crossing Roads of Federated Learning and Smart Grids: Overview, Challenges, and Perspectives](http://arxiv.org/abs/2304.08602) #federate</code></li>
<li>Summary: <p>Consumer's privacy is a main concern in Smart Grids (SGs) due to the
sensitivity of energy data, particularly when used to train machine learning
models for different services. These data-driven models often require huge
amounts of data to achieve acceptable performance leading in most cases to
risks of privacy leakage. By pushing the training to the edge, Federated
Learning (FL) offers a good compromise between privacy preservation and the
predictive performance of these models. The current paper presents an overview
of FL applications in SGs while discussing their advantages and drawbacks,
mainly in load forecasting, electric vehicles, fault diagnoses, load
disaggregation and renewable energies. In addition, an analysis of main design
trends and possible taxonomies is provided considering data partitioning, the
communication topology, and security mechanisms. Towards the end, an overview
of main challenges facing this technology and potential future directions is
presented.
</p></li>
</ul>

<h3>Title: Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks. (arXiv:2304.08996v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08996">http://arxiv.org/abs/2304.08996</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08996] Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks](http://arxiv.org/abs/2304.08996) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is a promising paradigm that enables distributed
clients to collaboratively train a shared global model while keeping the
training data locally. However, the performance of FL is often limited by poor
communication links and slow convergence when FL is deployed over wireless
networks. Besides, due to the limited radio resources, it is crucial to select
clients and control resource allocation accurately for improved FL performance.
Motivated by these challenges, a joint optimization problem of client selection
and resource allocation is formulated in this paper, aiming to minimize the
total time consumption of each round in FL over non-orthogonal multiple access
(NOMA) enabled wireless network. Specifically, based on a metric termed the age
of update (AoU), we first propose a novel client selection scheme by accounting
for the staleness of the received local FL models. After that, the closed-form
solutions of resource allocation are obtained by monotonicity analysis and dual
decomposition method. Moreover, to further improve the performance of FL, the
deployment of artificial neural network (ANN) at the server is proposed to
predict the local FL models of the unselected clients at each round. Finally,
extensive simulation results demonstrate the superior performance of the
proposed schemes.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Quantum Annealing for Single Image Super-Resolution. (arXiv:2304.08924v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08924">http://arxiv.org/abs/2304.08924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08924] Quantum Annealing for Single Image Super-Resolution](http://arxiv.org/abs/2304.08924) #fair</code></li>
<li>Summary: <p>This paper proposes a quantum computing-based algorithm to solve the single
image super-resolution (SISR) problem. One of the well-known classical
approaches for SISR relies on the well-established patch-wise sparse modeling
of the problem. Yet, this field's current state of affairs is that deep neural
networks (DNNs) have demonstrated far superior results than traditional
approaches. Nevertheless, quantum computing is expected to become increasingly
prominent for machine learning problems soon. As a result, in this work, we
take the privilege to perform an early exploration of applying a quantum
computing algorithm to this important image enhancement problem, i.e., SISR.
Among the two paradigms of quantum computing, namely universal gate quantum
computing and adiabatic quantum computing (AQC), the latter has been
successfully applied to practical computer vision problems, in which quantum
parallelism has been exploited to solve combinatorial optimization efficiently.
This work demonstrates formulating quantum SISR as a sparse coding optimization
problem, which is solved using quantum annealers accessed via the D-Wave Leap
platform. The proposed AQC-based algorithm is demonstrated to achieve improved
speed-up over a classical analog while maintaining comparable SISR accuracy.
</p></li>
</ul>

<h3>Title: Audio-Driven Talking Face Generation with Diverse yet Realistic Facial Animations. (arXiv:2304.08945v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08945">http://arxiv.org/abs/2304.08945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08945] Audio-Driven Talking Face Generation with Diverse yet Realistic Facial Animations](http://arxiv.org/abs/2304.08945) #fair</code></li>
<li>Summary: <p>Audio-driven talking face generation, which aims to synthesize talking faces
with realistic facial animations (including accurate lip movements, vivid
facial expression details and natural head poses) corresponding to the audio,
has achieved rapid progress in recent years. However, most existing work
focuses on generating lip movements only without handling the closely
correlated facial expressions, which degrades the realism of the generated
faces greatly. This paper presents DIRFA, a novel method that can generate
talking faces with diverse yet realistic facial animations from the same
driving audio. To accommodate fair variation of plausible facial animations for
the same audio, we design a transformer-based probabilistic mapping network
that can model the variational facial animation distribution conditioned upon
the input audio and autoregressively convert the audio signals into a facial
animation sequence. In addition, we introduce a temporally-biased mask into the
mapping network, which allows to model the temporal dependency of facial
animations and produce temporally smooth facial animation sequence. With the
generated facial animation sequence and a source image, photo-realistic talking
faces can be synthesized with a generic generation network. Extensive
experiments show that DIRFA can generate talking faces with realistic facial
animations effectively.
</p></li>
</ul>

<h3>Title: UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining. (arXiv:2304.09151v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.09151">http://arxiv.org/abs/2304.09151</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.09151] UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining](http://arxiv.org/abs/2304.09151) #fair</code></li>
<li>Summary: <p>Pretrained multilingual large language models have typically used heuristic
temperature-based sampling to balance between different languages. However
previous work has not systematically evaluated the efficacy of different
pretraining language distributions across model scales. In this paper, we
propose a new sampling method, UniMax, that delivers more uniform coverage of
head languages while mitigating overfitting on tail languages by explicitly
capping the number of repeats over each language's corpus. We perform an
extensive series of ablations testing a range of sampling strategies on a suite
of multilingual benchmarks, while varying model scale. We find that UniMax
outperforms standard temperature-based sampling, and the benefits persist as
scale increases. As part of our contribution, we release: (i) an improved and
refreshed mC4 multilingual corpus consisting of 29 trillion characters across
107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained
with UniMax sampling.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model. (arXiv:2304.08577v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08577">http://arxiv.org/abs/2304.08577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08577] Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model](http://arxiv.org/abs/2304.08577) #diffusion</code></li>
<li>Summary: <p>With the recent surge in popularity of AR/VR applications, realistic and
accurate control of 3D full-body avatars has become a highly demanded feature.
A particular challenge is that only a sparse tracking signal is available from
standalone HMDs (Head Mounted Devices), often limited to tracking the user's
head and wrists. While this signal is resourceful for reconstructing the upper
body motion, the lower body is not tracked and must be synthesized from the
limited information provided by the upper body joints. In this paper, we
present AGRoL, a novel conditional diffusion model specifically designed to
track full bodies given sparse upper-body tracking signals. Our model is based
on a simple multi-layer perceptron (MLP) architecture and a novel conditioning
scheme for motion data. It can predict accurate and smooth full-body motion,
particularly the challenging lower body movement. Unlike common diffusion
architectures, our compact architecture can run in real-time, making it
suitable for online body-tracking applications. We train and evaluate our model
on AMASS motion capture dataset, and demonstrate that our approach outperforms
state-of-the-art methods in generated motion accuracy and smoothness. We
further justify our design choices through extensive experiments and ablation
studies.
</p></li>
</ul>

<h3>Title: Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. (arXiv:2304.08818v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08818">http://arxiv.org/abs/2304.08818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08818] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2304.08818) #diffusion</code></li>
<li>Summary: <p>Latent Diffusion Models (LDMs) enable high-quality image synthesis while
avoiding excessive compute demands by training a diffusion model in a
compressed lower-dimensional latent space. Here, we apply the LDM paradigm to
high-resolution video generation, a particularly resource-intensive task. We
first pre-train an LDM on images only; then, we turn the image generator into a
video generator by introducing a temporal dimension to the latent space
diffusion model and fine-tuning on encoded image sequences, i.e., videos.
Similarly, we temporally align diffusion model upsamplers, turning them into
temporally consistent video super resolution models. We focus on two relevant
real-world applications: Simulation of in-the-wild driving data and creative
content creation with text-to-video modeling. In particular, we validate our
Video LDM on real driving videos of resolution 512 x 1024, achieving
state-of-the-art performance. Furthermore, our approach can easily leverage
off-the-shelf pre-trained image LDMs, as we only need to train a temporal
alignment model in that case. Doing so, we turn the publicly available,
state-of-the-art text-to-image LDM Stable Diffusion into an efficient and
expressive text-to-video model with resolution up to 1280 x 2048. We show that
the temporal layers trained in this way generalize to different fine-tuned
text-to-image LDMs. Utilizing this property, we show the first results for
personalized text-to-video generation, opening exciting directions for future
content creation. Project page:
https://research.nvidia.com/labs/toronto-ai/VideoLDM/
</p></li>
</ul>

<h3>Title: UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer. (arXiv:2304.08870v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08870">http://arxiv.org/abs/2304.08870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08870] UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer](http://arxiv.org/abs/2304.08870) #diffusion</code></li>
<li>Summary: <p>Existing person image generative models can do either image generation or
pose transfer but not both. We propose a unified diffusion model, UPGPT to
provide a universal solution to perform all the person image tasks -
generative, pose transfer, and editing. With fine-grained multimodality and
disentanglement capabilities, our approach offers fine-grained control over the
generation and the editing process of images using a combination of pose, text,
and image, all without needing a semantic segmentation mask which can be
challenging to obtain or edit. We also pioneer the parameterized body SMPL
model in pose-guided person image generation to demonstrate new capability -
simultaneous pose and camera view interpolation while maintaining a person's
appearance. Results on the benchmark DeepFashion dataset show that UPGPT is the
new state-of-the-art while simultaneously pioneering new capabilities of edit
and pose transfer in human image generation.
</p></li>
</ul>

<h3>Title: Look ATME: The Discriminator Mean Entropy Needs Attention. (arXiv:2304.09024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.09024">http://arxiv.org/abs/2304.09024</a></li>
<li>Code URL: <a href="https://github.com/dlr-mi/atme">https://github.com/dlr-mi/atme</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.09024] Look ATME: The Discriminator Mean Entropy Needs Attention](http://arxiv.org/abs/2304.09024) #diffusion</code></li>
<li>Summary: <p>Generative adversarial networks (GANs) are successfully used for image
synthesis but are known to face instability during training. In contrast,
probabilistic diffusion models (DMs) are stable and generate high-quality
images, at the cost of an expensive sampling procedure. In this paper, we
introduce a simple method to allow GANs to stably converge to their theoretical
optimum, while bringing in the denoising machinery from DMs. These models are
combined into a simpler model (ATME) that only requires a forward pass during
inference, making predictions cheaper and more accurate than DMs and popular
GANs. ATME breaks an information asymmetry existing in most GAN models in which
the discriminator has spatial knowledge of where the generator is failing. To
restore the information symmetry, the generator is endowed with knowledge of
the entropic state of the discriminator, which is leveraged to allow the
adversarial game to converge towards equilibrium. We demonstrate the power of
our method in several image-to-image translation tasks, showing superior
performance than state-of-the-art methods at a lesser cost. Code is available
at https://github.com/DLR-MI/atme
</p></li>
</ul>

<h3>Title: Two-stage Denoising Diffusion Model for Source Localization in Graph Inverse Problems. (arXiv:2304.08841v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.08841">http://arxiv.org/abs/2304.08841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.08841] Two-stage Denoising Diffusion Model for Source Localization in Graph Inverse Problems](http://arxiv.org/abs/2304.08841) #diffusion</code></li>
<li>Summary: <p>Source localization is the inverse problem of graph information dissemination
and has broad practical applications.
</p></li>
</ul>

<p>However, the inherent intricacy and uncertainty in information dissemination
pose significant challenges, and the ill-posed nature of the source
localization problem further exacerbates these challenges. Recently, deep
generative models, particularly diffusion models inspired by classical
non-equilibrium thermodynamics, have made significant progress. While diffusion
models have proven to be powerful in solving inverse problems and producing
high-quality reconstructions, applying them directly to the source localization
is infeasible for two reasons. Firstly, it is impossible to calculate the
posterior disseminated results on a large-scale network for iterative denoising
sampling, which would incur enormous computational costs. Secondly, in the
existing methods for this field, the training data itself are ill-posed
(many-to-one); thus simply transferring the diffusion model would only lead to
local optima.
</p>
<p>To address these challenges, we propose a two-stage optimization framework,
the source localization denoising diffusion model (SL-Diff). In the coarse
stage, we devise the source proximity degrees as the supervised signals to
generate coarse-grained source predictions. This aims to efficiently initialize
the next stage, significantly reducing its convergence time and calibrating the
convergence process. Furthermore, the introduction of cascade temporal
information in this training method transforms the many-to-one mapping
relationship into a one-to-one relationship, perfectly addressing the ill-posed
problem. In the fine stage, we design a diffusion model for the graph inverse
problem that can quantify the uncertainty in the dissemination. The proposed
SL-Diff yields excellent prediction results within a reasonable sampling time
at extensive experiments.
</p>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
