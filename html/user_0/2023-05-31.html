<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Blockchain Censorship. (arXiv:2305.18545v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18545">http://arxiv.org/abs/2305.18545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18545] Blockchain Censorship](http://arxiv.org/abs/2305.18545) #security</code></li>
<li>Summary: <p>Permissionless blockchains promise to be resilient against censorship by a
single entity. This suggests that deterministic rules, and not third-party
actors, are responsible for deciding if a transaction is appended to the
blockchain or not. In 2022, the U.S. Office of Foreign Assets Control (OFAC)
sanctioned a Bitcoin mixer and an Ethereum application, putting the neutrality
of permissionless blockchains to the test.
</p></li>
</ul>

<p>In this paper, we formalize quantify and analyze the security impact of
blockchain censorship. We start by defining censorship, followed by a
quantitative assessment of current censorship practices. We find that 46% of
Ethereum blocks were made by censoring actors that intend to comply with OFAC
sanctions, indicating the significant impact of OFAC sanctions on the
neutrality of public blockchains.
</p>
<p>We further uncover that censorship not only impacts neutrality, but also
security. We show how after Ethereum's move to Proof-of-Stake (PoS) and
adoption of Proposer-Builder Separation (PBS) the inclusion of censored
transactions was delayed by an average of 85%. Inclusion delays compromise a
transaction's security by, e.g., strengthening a sandwich adversary. Finally we
prove a fundamental limitation of PoS and Proof-of-Work (PoW) protocols against
censorship resilience.
</p>

<h3>Title: Securing Cloud File Systems using Shielded Execution. (arXiv:2305.18639v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18639">http://arxiv.org/abs/2305.18639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18639] Securing Cloud File Systems using Shielded Execution](http://arxiv.org/abs/2305.18639) #security</code></li>
<li>Summary: <p>Cloud file systems offer organizations a scalable and reliable file storage
solution. However, cloud file systems have become prime targets for
adversaries, and traditional designs are not equipped to protect organizations
against the myriad of attacks that may be initiated by a malicious cloud
provider, co-tenant, or end-client. Recently proposed designs leveraging
cryptographic techniques and trusted execution environments (TEEs) still force
organizations to make undesirable trade-offs, consequently leading to either
security, functional, or performance limitations. In this paper, we introduce
TFS, a cloud file system that leverages the security capabilities provided by
TEEs to bootstrap new security protocols that meet real-world security,
functional, and performance requirements. Through extensive security and
performance analyses, we show that TFS can ensure stronger security guarantees
while still providing practical utility and performance w.r.t. state-of-the-art
systems; compared to the widely-used NFS, TFS achieves up to 2.1X speedups
across micro-benchmarks and incurs <1X overhead for most macro-benchmark
workloads. TFS demonstrates that organizations need not sacrifice file system
security to embrace the functional and performance advantages of outsourcing.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: You Don't Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images. (arXiv:2305.18337v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18337">http://arxiv.org/abs/2305.18337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18337] You Don't Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images](http://arxiv.org/abs/2305.18337) #privacy</code></li>
<li>Summary: <p>Synthetic images generated from deep generative models have the potential to
address data scarcity and data privacy issues. The selection of synthesis
models is mostly based on image quality measurements, and most researchers
favor synthetic images that produce realistic images, i.e., images with good
fidelity scores, such as low Fr\'echet Inception Distance (FID) and high Peak
Signal-To-Noise Ratio (PSNR). However, the quality of synthetic images is not
limited to fidelity, and a wide spectrum of metrics should be evaluated to
comprehensively measure the quality of synthetic images. In addition, quality
metrics are not truthful predictors of the utility of synthetic images, and the
relations between these evaluation metrics are not yet clear. In this work, we
have established a comprehensive set of evaluators for synthetic images,
including fidelity, variety, privacy, and utility. By analyzing more than 100k
chest X-ray images and their synthetic copies, we have demonstrated that there
is an inevitable trade-off between synthetic image fidelity, variety, and
privacy. In addition, we have empirically demonstrated that the utility score
does not require images with both high fidelity and high variety. For intra-
and cross-task data augmentation, mode-collapsed images and low-fidelity images
can still demonstrate high utility. Finally, our experiments have also showed
that it is possible to produce images with both high utility and privacy, which
can provide a strong rationale for the use of deep generative models in
privacy-preserving applications. Our study can shore up comprehensive guidance
for the evaluation of synthetic images and elicit further developments for
utility-aware deep generative models in medical image synthesis.
</p></li>
</ul>

<h3>Title: Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18395">http://arxiv.org/abs/2305.18395</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18395] Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks](http://arxiv.org/abs/2305.18395) #privacy</code></li>
<li>Summary: <p>Large Language Models (LLMs) have shown promising performance in
knowledge-intensive reasoning tasks that require a compound understanding of
knowledge. However, deployment of the LLMs in real-world applications can be
challenging due to their high computational requirements and concerns on data
privacy. Previous studies have focused on building task-specific small language
models (LMs) by fine-tuning them with labeled data or distilling LLMs. However,
these approaches are ill-suited for knowledge-intensive reasoning tasks due to
the limited capacity of small LMs in memorizing the knowledge required.
Motivated by our theoretical analysis on memorization, we propose
Knowledge-Augmented Reasoning Distillation (KARD), a novel method that
fine-tunes small LMs to generate rationales with augmented knowledge retrieved
from an external knowledge base. Moreover, we further propose a neural reranker
to obtain documents relevant to rationale generation. We empirically show that
KARD significantly improves the performance of small T5 and Flan-T5 models on
the challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE and
StrategyQA. Notably, our method makes the 250M models achieve superior
performance against the fine-tuned 3B models, having 12 times larger
parameters, on both MedQA-USMLE and StrategyQA benchmarks.
</p></li>
</ul>

<h3>Title: LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18396">http://arxiv.org/abs/2305.18396</a></li>
<li>Code URL: <a href="https://github.com/privatellm001/private-llm-inference">https://github.com/privatellm001/private-llm-inference</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18396] LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers](http://arxiv.org/abs/2305.18396) #privacy</code></li>
<li>Summary: <p>Prior works have attempted to build private inference frameworks for
transformer-based large language models (LLMs) in a server-client setting,
where the server holds the model parameters and the client inputs the private
data for inference. However, these frameworks impose significant overhead when
the private inputs are forward propagated through the original LLMs. In this
paper, we show that substituting the computation- and communication-heavy
operators in the transformer architecture with privacy-computing friendly
approximations can greatly reduce the private inference costs with minor impact
on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our
privacy-computing friendly model inference pipeline achieves a $5\times$
acceleration in computation and an 80\% reduction in communication overhead,
while retaining nearly identical accuracy.
</p></li>
</ul>

<h3>Title: Training Private Models That Know What They Don't Know. (arXiv:2305.18393v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18393">http://arxiv.org/abs/2305.18393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18393] Training Private Models That Know What They Don't Know](http://arxiv.org/abs/2305.18393) #privacy</code></li>
<li>Summary: <p>Training reliable deep learning models which avoid making overconfident but
incorrect predictions is a longstanding challenge. This challenge is further
exacerbated when learning has to be differentially private: protection provided
to sensitive data comes at the price of injecting additional randomness into
the learning process. In this work, we conduct a thorough empirical
investigation of selective classifiers -- that can abstain when they are unsure
-- under a differential privacy constraint. We find that several popular
selective prediction approaches are ineffective in a differentially private
setting as they increase the risk of privacy leakage. At the same time, we
identify that a recent approach that only uses checkpoints produced by an
off-the-shelf private learning algorithm stands out as particularly suitable
under DP. Further, we show that differential privacy does not just harm utility
but also degrades selective classification performance. To analyze this effect
across privacy levels, we propose a novel evaluation mechanism which isolate
selective prediction performance across model utility levels. Our experimental
results show that recovering the performance level attainable by non-private
models is possible but comes at a considerable coverage cost as the privacy
budget decreases.
</p></li>
</ul>

<h3>Title: Unleashing the Power of Randomization in Auditing Differentially Private ML. (arXiv:2305.18447v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18447">http://arxiv.org/abs/2305.18447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18447] Unleashing the Power of Randomization in Auditing Differentially Private ML](http://arxiv.org/abs/2305.18447) #privacy</code></li>
<li>Summary: <p>We present a rigorous methodology for auditing differentially private machine
learning algorithms by adding multiple carefully designed examples called
canaries. We take a first principles approach based on three key components.
First, we introduce Lifted Differential Privacy (LiDP) that expands the
definition of differential privacy to handle randomized datasets. This gives us
the freedom to design randomized canaries. Second, we audit LiDP by trying to
distinguish between the model trained with $K$ canaries versus $K - 1$ canaries
in the dataset, leaving one canary out. By drawing the canaries i.i.d., LiDP
can leverage the symmetry in the design and reuse each privately trained model
to run multiple statistical tests, one for each canary. Third, we introduce
novel confidence intervals that take advantage of the multiple test statistics
by adapting to the empirical higher-order correlations. Together, this new
recipe demonstrates significant improvements in sample complexity, both
theoretically and empirically, using synthetic and real data. Further, recent
advances in designing stronger canaries can be readily incorporated into the
new framework.
</p></li>
</ul>

<h3>Title: Federated Learning of Gboard Language Models with Differential Privacy. (arXiv:2305.18465v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18465">http://arxiv.org/abs/2305.18465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18465] Federated Learning of Gboard Language Models with Differential Privacy](http://arxiv.org/abs/2305.18465) #privacy</code></li>
<li>Summary: <p>We train language models (LMs) with federated learning (FL) and differential
privacy (DP) in the Google Keyboard (Gboard). We apply the
DP-Follow-the-Regularized-Leader (DP-FTRL)~\citep{kairouz21b} algorithm to
achieve meaningfully formal DP guarantees without requiring uniform sampling of
client devices. To provide favorable privacy-utility trade-offs, we introduce a
new client participation criterion and discuss the implication of its
configuration in large scale systems. We show how quantile-based clip
estimation~\citep{andrew2019differentially} can be combined with DP-FTRL to
adaptively choose the clip norm during training or reduce the hyperparameter
tuning in preparation for training. With the help of pretraining on public
data, we train and deploy more than twenty Gboard LMs that achieve high utility
and $\rho-$zCDP privacy guarantees with $\rho \in (0.2, 2)$, with two models
additionally trained with secure aggregation~\citep{bonawitz2017practical}. We
are happy to announce that all the next word prediction neural network LMs in
Gboard now have DP guarantees, and all future launches of Gboard neural network
LMs will require DP guarantees. We summarize our experience and provide
concrete suggestions on DP training for practitioners.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18400">http://arxiv.org/abs/2305.18400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18400] A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning](http://arxiv.org/abs/2305.18400) #protect</code></li>
<li>Summary: <p>Trustworthy Federated Learning (TFL) typically leverages protection
mechanisms to guarantee privacy. However, protection mechanisms inevitably
introduce utility loss or efficiency reduction while protecting data privacy.
Therefore, protection mechanisms and their parameters should be carefully
chosen to strike an optimal tradeoff between \textit{privacy leakage},
\textit{utility loss}, and \textit{efficiency reduction}. To this end,
federated learning practitioners need tools to measure the three factors and
optimize the tradeoff between them to choose the protection mechanism that is
most appropriate to the application at hand. Motivated by this requirement, we
propose a framework that (1) formulates TFL as a problem of finding a
protection mechanism to optimize the tradeoff between privacy leakage, utility
loss, and efficiency reduction and (2) formally defines bounded measurements of
the three factors. We then propose a meta-learning algorithm to approximate
this optimization problem and find optimal protection parameters for
representative protection mechanisms, including Randomization, Homomorphic
Encryption, Secret Sharing, and Compression. We further design estimation
algorithms to quantify these found optimal protection parameters in a practical
horizontal federated learning setting and provide a theoretical analysis of the
estimation error.
</p></li>
</ul>

<h3>Title: Identification of stormwater control strategies and their associated uncertainties using Bayesian Optimization. (arXiv:2305.18630v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18630">http://arxiv.org/abs/2305.18630</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18630] Identification of stormwater control strategies and their associated uncertainties using Bayesian Optimization](http://arxiv.org/abs/2305.18630) #protect</code></li>
<li>Summary: <p>Dynamic control is emerging as an effective methodology for operating
stormwater systems under stress from rapidly evolving weather patterns.
Informed by rainfall predictions and real-time sensor measurements, control
assets in the stormwater network can be dynamically configured to tune the
behavior of the stormwater network to reduce the risk of urban flooding,
equalize flows to the water reclamation facilities, and protect the receiving
water bodies. However, developing such control strategies requires significant
human and computational resources, and a methodology does not yet exist for
quantifying the risks associated with implementing these control strategies. To
address these challenges, in this paper, we introduce a Bayesian
Optimization-based approach for identifying stormwater control strategies and
estimating the associated uncertainties. We evaluate the efficacy of this
approach in identifying viable control strategies in a simulated environment on
real-world inspired combined and separated stormwater networks. We demonstrate
the computational efficiency of the proposed approach by comparing it against a
Genetic algorithm. Furthermore, we extend the Bayesian Optimization-based
approach to quantify the uncertainty associated with the identified control
strategies and evaluate it on a synthetic stormwater network. To our knowledge,
this is the first-ever stormwater control methodology that quantifies
uncertainty associated with the identified control actions. This Bayesian
optimization-based stormwater control methodology is an off-the-shelf control
approach that can be applied to control any stormwater network as long we have
access to the rainfall predictions, and there exists a model for simulating the
behavior of the stormwater network.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18462">http://arxiv.org/abs/2305.18462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18462] Membership Inference Attacks against Language Models via Neighbourhood Comparison](http://arxiv.org/abs/2305.18462) #attack</code></li>
<li>Summary: <p>Membership Inference attacks (MIAs) aim to predict whether a data sample was
present in the training data of a machine learning model or not, and are widely
used for assessing the privacy risks of language models. Most existing attacks
rely on the observation that models tend to assign higher probabilities to
their training samples than non-training points. However, simple thresholding
of the model score in isolation tends to lead to high false-positive rates as
it does not account for the intrinsic complexity of a sample. Recent work has
demonstrated that reference-based attacks which compare model scores to those
obtained from a reference model trained on similar data can substantially
improve the performance of MIAs. However, in order to train reference models,
attacks of this kind make the strong and arguably unrealistic assumption that
an adversary has access to samples closely resembling the original training
data. Therefore, we investigate their performance in more realistic scenarios
and find that they are highly fragile in relation to the data distribution used
to train reference models. To investigate whether this fragility provides a
layer of safety, we propose and evaluate neighbourhood attacks, which compare
model scores for a given sample to scores of synthetically generated neighbour
texts and therefore eliminate the need for access to the training data
distribution. We show that, in addition to being competitive with
reference-based attacks that have perfect knowledge about the training data
distribution, our attack clearly outperforms existing reference-free attacks as
well as reference-based attacks with imperfect knowledge, which demonstrates
the need for a reevaluation of the threat model of adversarial attacks.
</p></li>
</ul>

<h3>Title: Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models. (arXiv:2305.18585v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18585">http://arxiv.org/abs/2305.18585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18585] Exploiting Explainability to Design Adversarial Attacks and Evaluate Attack Resilience in Hate-Speech Detection Models](http://arxiv.org/abs/2305.18585) #attack</code></li>
<li>Summary: <p>The advent of social media has given rise to numerous ethical challenges,
with hate speech among the most significant concerns. Researchers are
attempting to tackle this problem by leveraging hate-speech detection and
employing language models to automatically moderate content and promote civil
discourse. Unfortunately, recent studies have revealed that hate-speech
detection systems can be misled by adversarial attacks, raising concerns about
their resilience. While previous research has separately addressed the
robustness of these models under adversarial attacks and their
interpretability, there has been no comprehensive study exploring their
intersection. The novelty of our work lies in combining these two critical
aspects, leveraging interpretability to identify potential vulnerabilities and
enabling the design of targeted adversarial attacks. We present a comprehensive
and comparative analysis of adversarial robustness exhibited by various
hate-speech detection models. Our study evaluates the resilience of these
models against adversarial attacks using explainability techniques. To gain
insights into the models' decision-making processes, we employ the Local
Interpretable Model-agnostic Explanations (LIME) framework. Based on the
explainability results obtained by LIME, we devise and execute targeted attacks
on the text by leveraging the TextAttack tool. Our findings enhance the
understanding of the vulnerabilities and strengths exhibited by
state-of-the-art hate-speech detection models. This work underscores the
importance of incorporating explainability in the development and evaluation of
such models to enhance their resilience against adversarial attacks.
Ultimately, this work paves the way for creating more robust and reliable
hate-speech detection systems, fostering safer online environments and
promoting ethical discourse on social media platforms.
</p></li>
</ul>

<h3>Title: Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. (arXiv:2305.18384v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18384">http://arxiv.org/abs/2305.18384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18384] Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study](http://arxiv.org/abs/2305.18384) #attack</code></li>
<li>Summary: <p>Large amounts of incremental learning algorithms have been proposed to
alleviate the catastrophic forgetting issue arises while dealing with
sequential data on a time series. However, the adversarial robustness of
incremental learners has not been widely verified, leaving potential security
risks. Specifically, for poisoning-based backdoor attacks, we argue that the
nature of streaming data in IL provides great convenience to the adversary by
creating the possibility of distributed and cross-task attacks -- an adversary
can affect \textbf{any unknown} previous or subsequent task by data poisoning
\textbf{at any time or time series} with extremely small amount of backdoor
samples injected (e.g., $0.1\%$ based on our observations). To attract the
attention of the research community, in this paper, we empirically reveal the
high vulnerability of 11 typical incremental learners against poisoning-based
backdoor attack on 3 learning scenarios, especially the cross-task
generalization effect of backdoor knowledge, while the poison ratios range from
$5\%$ to as low as $0.1\%$. Finally, the defense mechanism based on activation
clustering is found to be effective in detecting our trigger pattern to
mitigate potential security risks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Motion-Scenario Decoupling for Rat-Aware Video Position Prediction: Strategy and Benchmark. (arXiv:2305.18310v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18310">http://arxiv.org/abs/2305.18310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18310] Motion-Scenario Decoupling for Rat-Aware Video Position Prediction: Strategy and Benchmark](http://arxiv.org/abs/2305.18310) #robust</code></li>
<li>Summary: <p>Recently significant progress has been made in human action recognition and
behavior prediction using deep learning techniques, leading to improved
vision-based semantic understanding. However, there is still a lack of
high-quality motion datasets for small bio-robotics, which presents more
challenging scenarios for long-term movement prediction and behavior control
based on third-person observation. In this study, we introduce RatPose, a
bio-robot motion prediction dataset constructed by considering the influence
factors of individuals and environments based on predefined annotation rules.
To enhance the robustness of motion prediction against these factors, we
propose a Dual-stream Motion-Scenario Decoupling (\textit{DMSD}) framework that
effectively separates scenario-oriented and motion-oriented features and
designs a scenario contrast loss and motion clustering loss for overall
training. With such distinctive architecture, the dual-branch feature flow
information is interacted and compensated in a decomposition-then-fusion
manner. Moreover, we demonstrate significant performance improvements of the
proposed \textit{DMSD} framework on different difficulty-level tasks. We also
implement long-term discretized trajectory prediction tasks to verify the
generalization ability of the proposed dataset.
</p></li>
</ul>

<h3>Title: ColibriUAV: An Ultra-Fast, Energy-Efficient Neuromorphic Edge Processing UAV-Platform with Event-Based and Frame-Based Cameras. (arXiv:2305.18371v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18371">http://arxiv.org/abs/2305.18371</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18371] ColibriUAV: An Ultra-Fast, Energy-Efficient Neuromorphic Edge Processing UAV-Platform with Event-Based and Frame-Based Cameras](http://arxiv.org/abs/2305.18371) #robust</code></li>
<li>Summary: <p>The interest in dynamic vision sensor (DVS)-powered unmanned aerial vehicles
(UAV) is raising, especially due to the microsecond-level reaction time of the
bio-inspired event sensor, which increases robustness and reduces latency of
the perception tasks compared to a RGB camera. This work presents ColibriUAV, a
UAV platform with both frame-based and event-based cameras interfaces for
efficient perception and near-sensor processing. The proposed platform is
designed around Kraken, a novel low-power RISC-V System on Chip with two
hardware accelerators targeting spiking neural networks and deep ternary neural
networks.Kraken is capable of efficiently processing both event data from a DVS
camera and frame data from an RGB camera. A key feature of Kraken is its
integrated, dedicated interface with a DVS camera. This paper benchmarks the
end-to-end latency and power efficiency of the neuromorphic and event-based UAV
subsystem, demonstrating state-of-the-art event data with a throughput of 7200
frames of events per second and a power consumption of 10.7 \si{\milli\watt},
which is over 6.6 times faster and a hundred times less power-consuming than
the widely-used data reading approach through the USB interface. The overall
sensing and processing power consumption is below 50 mW, achieving latency in
the milliseconds range, making the platform suitable for low-latency autonomous
nano-drones as well.
</p></li>
</ul>

<h3>Title: BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning. (arXiv:2305.18377v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18377">http://arxiv.org/abs/2305.18377</a></li>
<li>Code URL: <a href="https://github.com/zjfheart/badlabels">https://github.com/zjfheart/badlabels</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18377] BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning](http://arxiv.org/abs/2305.18377) #robust</code></li>
<li>Summary: <p>Label-noise learning (LNL) aims to increase the model's generalization given
training data with noisy labels. To facilitate practical LNL algorithms,
researchers have proposed different label noise types, ranging from
class-conditional to instance-dependent noises. In this paper, we introduce a
novel label noise type called BadLabel, which can significantly degrade the
performance of existing LNL algorithms by a large margin. BadLabel is crafted
based on the label-flipping attack against standard classification, where
specific samples are selected and their labels are flipped to other labels so
that the loss values of clean and noisy labels become indistinguishable. To
address the challenge posed by BadLabel, we further propose a robust LNL method
that perturbs the labels in an adversarial manner at each epoch to make the
loss values of clean and noisy labels again distinguishable. Once we select a
small set of (mostly) clean labeled data, we can apply the techniques of
semi-supervised learning to train the model accurately. Empirically, our
experimental results demonstrate that existing LNL algorithms are vulnerable to
the newly introduced BadLabel noise type, while our proposed robust LNL method
can effectively improve the generalization performance of the model under
various types of label noise. The new dataset of noisy labels and the source
codes of robust LNL algorithms are available at
https://github.com/zjfheart/BadLabels.
</p></li>
</ul>

<h3>Title: Human Body Shape Classification Based on a Single Image. (arXiv:2305.18480v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18480">http://arxiv.org/abs/2305.18480</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18480] Human Body Shape Classification Based on a Single Image](http://arxiv.org/abs/2305.18480) #robust</code></li>
<li>Summary: <p>There is high demand for online fashion recommender systems that incorporate
the needs of the consumer's body shape. As such, we present a methodology to
classify human body shape from a single image. This is achieved through the use
of instance segmentation and keypoint estimation models, trained only on
open-source benchmarking datasets. The system is capable of performing in noisy
environments owing to to robust background subtraction. The proposed
methodology does not require 3D body recreation as a result of classification
based on estimated keypoints, nor requires historical information about a user
to operate - calculating all required measurements at the point of use. We
evaluate our methodology both qualitatively against existing body shape
classifiers and quantitatively against a novel dataset of images, which we
provide for use to the community. The resultant body shape classification can
be utilised in a variety of downstream tasks, such as input to size and fit
recommendation or virtual try-on systems.
</p></li>
</ul>

<h3>Title: Fashion Object Detection for Tops &amp; Bottoms. (arXiv:2305.18482v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18482">http://arxiv.org/abs/2305.18482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18482] Fashion Object Detection for Tops &amp; Bottoms](http://arxiv.org/abs/2305.18482) #robust</code></li>
<li>Summary: <p>Fashion is one of the largest world's industries and computer vision
techniques have been becoming more popular in recent years, in particular, for
tasks such as object detection and apparel segmentation. Even with the rapid
growth in computer vision solutions, specifically for the fashion industry,
many problems are far for being resolved. Therefore, not at all times,
adjusting out-of-the-box pre-trained computer vision models will provide the
desired solution. In the present paper is proposed a pipeline that takes a
noisy image with a person and specifically detects the regions with garments
that are bottoms or tops. Our solution implements models that are capable of
finding human parts in an image e.g. full-body vs half-body, or no human is
found. Then, other models knowing that there's a human and its composition
(e.g. not always we have a full-body) finds the bounding boxes/regions of the
image that very likely correspond to a bottom or a top. For the creation of
bounding boxes/regions task, a benchmark dataset was specifically prepared. The
results show that the Mask RCNN solution is robust, and generalized enough to
be used and scalable in unseen apparel/fashion data.
</p></li>
</ul>

<h3>Title: Evaluating 3D Shape Analysis Methods for Robustness to Rotation Invariance. (arXiv:2305.18557v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18557">http://arxiv.org/abs/2305.18557</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18557] Evaluating 3D Shape Analysis Methods for Robustness to Rotation Invariance](http://arxiv.org/abs/2305.18557) #robust</code></li>
<li>Summary: <p>This paper analyzes the robustness of recent 3D shape descriptors to SO(3)
rotations, something that is fundamental to shape modeling. Specifically, we
formulate the task of rotated 3D object instance detection. To do so, we
consider a database of 3D indoor scenes, where objects occur in different
orientations. We benchmark different methods for feature extraction and
classification in the context of this task. We systematically contrast
different choices in a variety of experimental settings investigating the
impact on the performance of different rotation distributions, different
degrees of partial observations on the object, and the different levels of
difficulty of negative pairs. Our study, on a synthetic dataset of 3D scenes
where objects instances occur in different orientations, reveals that deep
learning-based rotation invariant methods are effective for relatively easy
settings with easy-to-distinguish pairs. However, their performance decreases
significantly when the difference in rotations on the input pair is large, or
when the degree of observation of input objects is reduced, or the difficulty
level of input pair is increased. Finally, we connect feature encodings
designed for rotation-invariant methods to 3D geometry that enable them to
acquire the property of rotation invariance.
</p></li>
</ul>

<h3>Title: CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18315">http://arxiv.org/abs/2305.18315</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18315] CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities](http://arxiv.org/abs/2305.18315) #robust</code></li>
<li>Summary: <p>A basic task for most Legal Artificial Intelligence (Legal AI) applications
is Named Entity Recognition (NER). However, texts produced in the context of
legal practice make references to entities that are not trivially recognized by
the currently available NERs. There is a lack of categorization of legislation,
jurisprudence, evidence, penalties, the roles of people in a legal process
(judge, lawyer, victim, defendant, witness), types of locations (crime
location, defendant's address), etc. In this sense, there is still a need for a
robust golden collection, annotated with fine-grained entities of the legal
domain, and which covers various documents of a legal process, such as
petitions, inquiries, complaints, decisions and sentences. In this article, we
describe the development of the Golden Collection of the Brazilian Judiciary
(CDJUR-BR) contemplating a set of fine-grained named entities that have been
annotated by experts in legal documents. The creation of CDJUR-BR followed its
own methodology that aimed to attribute a character of comprehensiveness and
robustness. Together with the CDJUR-BR repository we provided a NER based on
the BERT model and trained with the CDJUR-BR, whose results indicated the
prevalence of the CDJUR-BR.
</p></li>
</ul>

<h3>Title: ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models. (arXiv:2305.18323v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18323">http://arxiv.org/abs/2305.18323</a></li>
<li>Code URL: <a href="https://github.com/billxbf/rewoo">https://github.com/billxbf/rewoo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18323] ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models](http://arxiv.org/abs/2305.18323) #robust</code></li>
<li>Summary: <p>Augmented Language Models (ALMs) blend the reasoning capabilities of Large
Language Models (LLMs) with tools that allow for knowledge retrieval and action
execution. Existing ALM systems trigger LLM thought processes while pulling
observations from these tools in an interleaved fashion. Specifically, an LLM
reasons to call an external tool, gets halted to fetch the tool's response, and
then decides the next action based on all preceding response tokens. Such a
paradigm, though straightforward and easy to implement, often leads to huge
computation complexity from redundant prompts and repeated execution. This
study addresses such challenges for the first time, proposing a modular
paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning
process from external observations, thus significantly reducing token
consumption. Comprehensive evaluations across six public NLP benchmarks and a
curated dataset reveal consistent performance enhancements with our proposed
methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy
improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO
demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency,
decoupling parametric modules from non-parametric tool calls enables
instruction fine-tuning to offload LLMs into smaller language models, thus
substantially reducing model parameters. Our illustrative work offloads
reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant
potential for truly efficient and scalable ALM systems.
</p></li>
</ul>

<h3>Title: From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework. (arXiv:2305.18503v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18503">http://arxiv.org/abs/2305.18503</a></li>
<li>Code URL: <a href="https://github.com/thunlp/robtest">https://github.com/thunlp/robtest</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18503] From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework](http://arxiv.org/abs/2305.18503) #robust</code></li>
<li>Summary: <p>Textual adversarial attacks can discover models' weaknesses by adding
semantic-preserved but misleading perturbations to the inputs. The long-lasting
adversarial attack-and-defense arms race in Natural Language Processing (NLP)
is algorithm-centric, providing valuable techniques for automatic robustness
evaluation. However, the existing practice of robustness evaluation may exhibit
issues of incomprehensive evaluation, impractical evaluation protocol, and
invalid adversarial samples. In this paper, we aim to set up a unified
automatic robustness evaluation framework, shifting towards model-centric
evaluation to further exploit the advantages of adversarial attacks. To address
the above challenges, we first determine robustness evaluation dimensions based
on model capabilities and specify the reasonable algorithm to generate
adversarial samples for each dimension. Then we establish the evaluation
protocol, including evaluation settings and metrics, under realistic demands.
Finally, we use the perturbation degree of adversarial samples to control the
sample validity. We implement a toolkit RobTest that realizes our automatic
robustness evaluation framework. In our experiments, we conduct a robustness
evaluation of RoBERTa models to demonstrate the effectiveness of our evaluation
framework, and further show the rationality of each component in the framework.
The code will be made public at \url{https://github.com/thunlp/RobTest}.
</p></li>
</ul>

<h3>Title: Sample Complexity of Variance-reduced Distributionally Robust Q-learning. (arXiv:2305.18420v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18420">http://arxiv.org/abs/2305.18420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18420] Sample Complexity of Variance-reduced Distributionally Robust Q-learning](http://arxiv.org/abs/2305.18420) #robust</code></li>
<li>Summary: <p>Dynamic decision making under distributional shifts is of fundamental
interest in theory and applications of reinforcement learning: The distribution
of the environment on which the data is collected can differ from that of the
environment on which the model is deployed. This paper presents two novel
model-free algorithms, namely the distributionally robust Q-learning and its
variance-reduced counterpart, that can effectively learn a robust policy
despite distributional shifts. These algorithms are designed to efficiently
approximate the $q$-function of an infinite-horizon $\gamma$-discounted robust
Markov decision process with Kullback-Leibler uncertainty set to an entry-wise
$\epsilon$-degree of precision. Further, the variance-reduced distributionally
robust Q-learning combines the synchronous Q-learning with variance-reduction
techniques to enhance its performance. Consequently, we establish that it
attains a minmax sample complexity upper bound of $\tilde
O(|S||A|(1-\gamma)^{-4}\epsilon^{-2})$, where $S$ and $A$ denote the state and
action spaces. This is the first complexity result that is independent of the
uncertainty size $\delta$, thereby providing new complexity theoretic insights.
Additionally, a series of numerical experiments confirm the theoretical
findings and the efficiency of the algorithms in handling distributional
shifts.
</p></li>
</ul>

<h3>Title: HyperTime: Hyperparameter Optimization for Combating Temporal Distribution Shifts. (arXiv:2305.18421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18421">http://arxiv.org/abs/2305.18421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18421] HyperTime: Hyperparameter Optimization for Combating Temporal Distribution Shifts](http://arxiv.org/abs/2305.18421) #robust</code></li>
<li>Summary: <p>In this work, we propose a hyperparameter optimization method named
\emph{HyperTime} to find hyperparameters robust to potential temporal
distribution shifts in the unseen test data. Our work is motivated by an
important observation that it is, in many cases, possible to achieve temporally
robust predictive performance via hyperparameter optimization. Based on this
observation, we leverage the `worst-case-oriented' philosophy from the robust
optimization literature to help find such robust hyperparameter configurations.
HyperTime imposes a lexicographic priority order on average validation loss and
worst-case validation loss over chronological validation sets. We perform a
theoretical analysis on the upper bound of the expected test loss, which
reveals the unique advantages of our approach. We also demonstrate the strong
empirical performance of the proposed method on multiple machine learning tasks
with temporal distribution shifts.
</p></li>
</ul>

<h3>Title: Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals. (arXiv:2305.18425v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18425">http://arxiv.org/abs/2305.18425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18425] Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals](http://arxiv.org/abs/2305.18425) #robust</code></li>
<li>Summary: <p>In this paper, we present an efficient method for storing fine-tuned models
by leveraging the low-rank properties of weight residuals. Our key observation
is that weight residuals in large overparameterized models exhibit even
stronger low-rank characteristics. Based on this insight, we propose Efficient
Residual Encoding (ERE), a novel approach that achieves efficient storage of
fine-tuned model weights by approximating the low-rank weight residuals.
Furthermore, we analyze the robustness of weight residuals and push the limit
of storage efficiency by utilizing additional quantization and layer-wise rank
allocation. Our experimental results demonstrate that our method significantly
reduces memory footprint while preserving performance in various tasks and
modalities. We release our code.
</p></li>
</ul>

<h3>Title: GBG++: A Fast and Stable Granular Ball Generation Method for Classification. (arXiv:2305.18450v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18450">http://arxiv.org/abs/2305.18450</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18450] GBG++: A Fast and Stable Granular Ball Generation Method for Classification](http://arxiv.org/abs/2305.18450) #robust</code></li>
<li>Summary: <p>Granular ball computing (GBC), as an efficient, robust, and scalable learning
method, has become a popular research topic of granular computing. GBC includes
two stages: granular ball generation (GBG) and multi-granularity learning based
on the granular ball (GB). However, the stability and efficiency of existing
GBG methods need to be further improved due to their strong dependence on
$k$-means or $k$-division. In addition, GB-based classifiers only unilaterally
consider the GB's geometric characteristics to construct classification rules,
but the GB's quality is ignored. Therefore, in this paper, based on the
attention mechanism, a fast and stable GBG (GBG++) method is proposed first.
Specifically, the proposed GBG++ method only needs to calculate the distances
from the data-driven center to the undivided samples when splitting each GB,
instead of randomly selecting the center and calculating the distances between
it to all samples. Moreover, an outlier detection method is introduced to
identify local outliers. Consequently, the GBG++ method can significantly
improve effectiveness, robustness, and efficiency while being absolutely
stable. Second, considering the influence of the sample size within the GB on
the GB's quality, based on the GBG++ method, a $k$-nearest neighbors algorithm
(GB$k$NN++) which can reduce misclassification at class boundary to some extent
is presented. Finally, the experimental results indicate that the proposed
method outperforms several existing GB-based classifiers and classical machine
learning classifiers on $20$ public benchmark data sets.
</p></li>
</ul>

<h3>Title: Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18451">http://arxiv.org/abs/2305.18451</a></li>
<li>Code URL: <a href="https://github.com/namkyeong/cmrl">https://github.com/namkyeong/cmrl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18451] Shift-Robust Molecular Relational Learning with Causal Substructure](http://arxiv.org/abs/2305.18451) #robust</code></li>
<li>Summary: <p>Recently, molecular relational learning, whose goal is to predict the
interaction behavior between molecular pairs, got a surge of interest in
molecular sciences due to its wide range of applications. In this work, we
propose CMRL that is robust to the distributional shift in molecular relational
learning by detecting the core substructure that is causally related to
chemical reactions. To do so, we first assume a causal relationship based on
the domain knowledge of molecular sciences and construct a structural causal
model (SCM) that reveals the relationship between variables. Based on the SCM,
we introduce a novel conditional intervention framework whose intervention is
conditioned on the paired molecule. With the conditional intervention
framework, our model successfully learns from the causal substructure and
alleviates the confounding effect of shortcut substructures that are spuriously
correlated to chemical reactions. Extensive experiments on various tasks with
real-world and synthetic datasets demonstrate the superiority of CMRL over
state-of-the-art baseline models. Our code is available at
https://github.com/Namkyeong/CMRL.
</p></li>
</ul>

<h3>Title: SANE: The phases of gradient descent through Sharpness Adjusted Number of Effective parameters. (arXiv:2305.18490v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18490">http://arxiv.org/abs/2305.18490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18490] SANE: The phases of gradient descent through Sharpness Adjusted Number of Effective parameters](http://arxiv.org/abs/2305.18490) #robust</code></li>
<li>Summary: <p>Modern neural networks are undeniably successful. Numerous studies have
investigated how the curvature of loss landscapes can affect the quality of
solutions. In this work we consider the Hessian matrix during network training.
We reiterate the connection between the number of "well-determined" or
"effective" parameters and the generalisation performance of neural nets, and
we demonstrate its use as a tool for model comparison. By considering the local
curvature, we propose Sharpness Adjusted Number of Effective parameters (SANE),
a measure of effective dimensionality for the quality of solutions. We show
that SANE is robust to large learning rates, which represent learning regimes
that are attractive but (in)famously unstable. We provide evidence and
characterise the Hessian shifts across "loss basins" at large learning rates.
Finally, extending our analysis to deeper neural networks, we provide an
approximation to the full-network Hessian, exploiting the natural ordering of
neural weights, and use this approximation to provide extensive empirical
evidence for our claims.
</p></li>
</ul>

<h3>Title: Robust Lipschitz Bandits to Adversarial Corruptions. (arXiv:2305.18543v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18543">http://arxiv.org/abs/2305.18543</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18543] Robust Lipschitz Bandits to Adversarial Corruptions](http://arxiv.org/abs/2305.18543) #robust</code></li>
<li>Summary: <p>Lipschitz bandit is a variant of stochastic bandits that deals with a
continuous arm set defined on a metric space, where the reward function is
subject to a Lipschitz constraint. In this paper, we introduce a new problem of
Lipschitz bandits in the presence of adversarial corruptions where an adaptive
adversary corrupts the stochastic rewards up to a total budget $C$. The budget
is measured by the sum of corruption levels across the time horizon $T$. We
consider both weak and strong adversaries, where the weak adversary is unaware
of the current action before the attack, while the strong one can observe it.
Our work presents the first line of robust Lipschitz bandit algorithms that can
achieve sub-linear regret under both types of adversary, even when the total
budget of corruption $C$ is unrevealed to the agent. We provide a lower bound
under each type of adversary, and show that our algorithm is optimal under the
strong case. Finally, we conduct experiments to illustrate the effectiveness of
our algorithms against two classic kinds of attacks.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Learning from Multi-Perception Features for Real-Word Image Super-resolution. (arXiv:2305.18547v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18547">http://arxiv.org/abs/2305.18547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18547] Learning from Multi-Perception Features for Real-Word Image Super-resolution](http://arxiv.org/abs/2305.18547) #extraction</code></li>
<li>Summary: <p>Currently, there are two popular approaches for addressing real-world image
super-resolution problems: degradation-estimation-based and blind-based
methods. However, degradation-estimation-based methods may be inaccurate in
estimating the degradation, making them less applicable to real-world LR
images. On the other hand, blind-based methods are often limited by their fixed
single perception information, which hinders their ability to handle diverse
perceptual characteristics. To overcome this limitation, we propose a novel SR
method called MPF-Net that leverages multiple perceptual features of input
images. Our method incorporates a Multi-Perception Feature Extraction (MPFE)
module to extract diverse perceptual information and a series of newly-designed
Cross-Perception Blocks (CPB) to combine this information for effective
super-resolution reconstruction. Additionally, we introduce a contrastive
regularization term (CR) that improves the model's learning capability by using
newly generated HR and LR images as positive and negative samples for ground
truth HR. Experimental results on challenging real-world SR datasets
demonstrate that our approach significantly outperforms existing
state-of-the-art methods in both qualitative and quantitative measures.
</p></li>
</ul>

<h3>Title: REFinD: Relation Extraction Financial Dataset. (arXiv:2305.18322v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18322">http://arxiv.org/abs/2305.18322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18322] REFinD: Relation Extraction Financial Dataset](http://arxiv.org/abs/2305.18322) #extraction</code></li>
<li>Summary: <p>A number of datasets for Relation Extraction (RE) have been created to aide
downstream tasks such as information retrieval, semantic search, question
answering and textual entailment. However, these datasets fail to capture
financial-domain specific challenges since most of these datasets are compiled
using general knowledge sources such as Wikipedia, web-based text and news
articles, hindering real-life progress and adoption within the financial world.
To address this limitation, we propose REFinD, the first large-scale annotated
dataset of relations, with $\sim$29K instances and 22 relations amongst 8 types
of entity pairs, generated entirely over financial documents. We also provide
an empirical evaluation with various state-of-the-art models as benchmarks for
the RE task and highlight the challenges posed by our dataset. We observed that
various state-of-the-art deep learning models struggle with numeric inference,
relational and directional ambiguity.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Reducing Communication for Split Learning by Randomized Top-k Sparsification. (arXiv:2305.18469v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18469">http://arxiv.org/abs/2305.18469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18469] Reducing Communication for Split Learning by Randomized Top-k Sparsification](http://arxiv.org/abs/2305.18469) #federate</code></li>
<li>Summary: <p>Split learning is a simple solution for Vertical Federated Learning (VFL),
which has drawn substantial attention in both research and application due to
its simplicity and efficiency. However, communication efficiency is still a
crucial issue for split learning. In this paper, we investigate multiple
communication reduction methods for split learning, including cut layer size
reduction, top-k sparsification, quantization, and L1 regularization. Through
analysis of the cut layer size reduction and top-k sparsification, we further
propose randomized top-k sparsification, to make the model generalize and
converge better. This is done by selecting top-k elements with a large
probability while also having a small probability to select non-top-k elements.
Empirical results show that compared with other communication-reduction
methods, our proposed randomized top-k sparsification achieves a better model
performance under the same compression level.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18569">http://arxiv.org/abs/2305.18569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18569] Fairness of ChatGPT](http://arxiv.org/abs/2305.18569) #fair</code></li>
<li>Summary: <p>Understanding and addressing unfairness in LLMs are crucial for responsible
AI deployment. However, there is a limited availability of quantitative
analyses and in-depth studies regarding fairness evaluations in LLMs,
especially when applying LLMs to high-stakes fields. This work aims to fill
this gap by providing a systematic evaluation of the effectiveness and fairness
of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's
performance in high-takes fields including education, criminology, finance and
healthcare. To make thorough evaluation, we consider both group fairness and
individual fairness and we also observe the disparities in ChatGPT's outputs
under a set of biased or unbiased prompts. This work contributes to a deeper
understanding of LLMs' fairness performance, facilitates bias mitigation and
fosters the development of responsible artificial intelligence systems.
</p></li>
</ul>

<h3>Title: Generalized Disparate Impact for Configurable Fairness Solutions in ML. (arXiv:2305.18504v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18504">http://arxiv.org/abs/2305.18504</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18504] Generalized Disparate Impact for Configurable Fairness Solutions in ML](http://arxiv.org/abs/2305.18504) #fair</code></li>
<li>Summary: <p>We make two contributions in the field of AI fairness over continuous
protected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)
indicator (the only one currently available for such a case) is valuable but
subject to a few crucial limitations regarding semantics, interpretability, and
robustness. Second, we introduce a family of indicators that are: 1)
complementary to HGR in terms of semantics; 2) fully interpretable and
transparent; 3) robust over finite samples; 4) configurable to suit specific
applications. Our approach also allows us to define fine-grained constraints to
permit certain types of dependence and forbid others selectively. By expanding
the available options for continuous protected attributes, our approach
represents a significant contribution to the area of fair artificial
intelligence.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model. (arXiv:2305.18638v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18638">http://arxiv.org/abs/2305.18638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18638] Short Answer Grading Using One-shot Prompting and Text Similarity Scoring Model](http://arxiv.org/abs/2305.18638) #interpretability</code></li>
<li>Summary: <p>In this study, we developed an automated short answer grading (ASAG) model
that provided both analytic scores and final holistic scores. Short answer
items typically consist of multiple sub-questions, and providing an analytic
score and the text span relevant to each sub-question can increase the
interpretability of the automated scores. Furthermore, they can be used to
generate actionable feedback for students. Despite these advantages, most
studies have focused on predicting only holistic scores due to the difficulty
in constructing dataset with manual annotations. To address this difficulty, we
used large language model (LLM)-based one-shot prompting and a text similarity
scoring model with domain adaptation using small manually annotated dataset.
The accuracy and quadratic weighted kappa of our model were 0.67 and 0.71 on a
subset of the publicly available ASAG dataset. The model achieved a substantial
improvement over the majority baseline.
</p></li>
</ul>

<h3>Title: Visual Knowledge Discovery with General Line Coordinates. (arXiv:2305.18429v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18429">http://arxiv.org/abs/2305.18429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18429] Visual Knowledge Discovery with General Line Coordinates](http://arxiv.org/abs/2305.18429) #interpretability</code></li>
<li>Summary: <p>Understanding black-box Machine Learning methods on multidimensional data is
a key challenge in Machine Learning. While many powerful Machine Learning
methods already exist, these methods are often unexplainable or perform poorly
on complex data. This paper proposes visual knowledge discovery approaches
based on several forms of lossless General Line Coordinates. These are an
expansion of the previously introduced General Line Coordinates Linear and
Dynamic Scaffolding Coordinates to produce, explain, and visualize non-linear
classifiers with explanation rules. To ensure these non-linear models and rules
are accurate, General Line Coordinates Linear also developed new interactive
visual knowledge discovery algorithms for finding worst-case validation splits.
These expansions are General Line Coordinates non-linear, interactive rules
linear, hyperblock rules linear, and worst-case linear. Experiments across
multiple benchmark datasets show that this visual knowledge discovery method
can compete with other visual and computational Machine Learning algorithms
while improving both interpretability and accuracy in linear and non-linear
classifications. Major benefits from these expansions consist of the ability to
build accurate and highly interpretable models and rules from hyperblocks, the
ability to analyze interpretability weaknesses in a model, and the input of
expert knowledge through interactive and human-guided visual knowledge
discovery methods.
</p></li>
</ul>

<h3>Title: Interactive Decision Tree Creation and Enhancement with Complete Visualization for Explainable Modeling. (arXiv:2305.18432v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18432">http://arxiv.org/abs/2305.18432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18432] Interactive Decision Tree Creation and Enhancement with Complete Visualization for Explainable Modeling](http://arxiv.org/abs/2305.18432) #interpretability</code></li>
<li>Summary: <p>To increase the interpretability and prediction accuracy of the Machine
Learning (ML) models, visualization of ML models is a key part of the ML
process. Decision Trees (DTs) are essential in machine learning (ML) because
they are used to understand many black box ML models including Deep Learning
models. In this research, two new methods for creation and enhancement with
complete visualizing Decision Trees as understandable models are suggested.
These methods use two versions of General Line Coordinates (GLC): Bended
Coordinates (BC) and Shifted Paired Coordinates (SPC). The Bended Coordinates
are a set of line coordinates, where each coordinate is bended in a threshold
point of the respective DT node. In SPC, each n-D point is visualized in a set
of shifted pairs of 2-D Cartesian coordinates as a directed graph. These new
methods expand and complement the capabilities of existing methods to visualize
DT models more completely. These capabilities allow us to observe and analyze:
(1) relations between attributes, (2) individual cases relative to the DT
structure, (3) data flow in the DT, (4) sensitivity of each split threshold in
the DT nodes, and (5) density of cases in parts of the n-D space. These
features are critical for DT models' performance evaluation and improvement by
domain experts and end users as they help to prevent overgeneralization and
overfitting of the models. The advantages of this methodology are illustrated
in the case studies on benchmark real-world datasets. The paper also
demonstrates how to generalize them for decision tree visualizations in
different General Line Coordinates.
</p></li>
</ul>

<h3>Title: Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization. (arXiv:2305.18437v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18437">http://arxiv.org/abs/2305.18437</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18437] Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization](http://arxiv.org/abs/2305.18437) #interpretability</code></li>
<li>Summary: <p>Building accurate and interpretable Machine Learning (ML) models for
heterogeneous/mixed data is a long-standing challenge for algorithms designed
for numeric data. This work focuses on developing numeric coding schemes for
non-numeric attributes for ML algorithms to support accurate and explainable ML
models, methods for lossless visualization of n-D non-numeric categorical data
with visual rule discovery in these visualizations, and accurate and
explainable ML models for categorical data. This study proposes a
classification of mixed data types and analyzes their important role in Machine
Learning. It presents a toolkit for enforcing interpretability of all internal
operations of ML algorithms on mixed data with a visual data exploration on
mixed data. A new Sequential Rule Generation (SRG) algorithm for explainable
rule generation with categorical data is proposed and successfully evaluated in
multiple computational experiments. This work is one of the steps to the full
scope ML algorithms for mixed data supported by lossless visualization of n-D
data in General Line Coordinates beyond Parallel Coordinates.
</p></li>
</ul>

<h3>Title: Learning Linear Groups in Neural Networks. (arXiv:2305.18552v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18552">http://arxiv.org/abs/2305.18552</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18552] Learning Linear Groups in Neural Networks](http://arxiv.org/abs/2305.18552) #interpretability</code></li>
<li>Summary: <p>Employing equivariance in neural networks leads to greater parameter
efficiency and improved generalization performance through the encoding of
domain knowledge in the architecture; however, the majority of existing
approaches require an a priori specification of the desired symmetries. We
present a neural network architecture, Linear Group Networks (LGNs), for
learning linear groups acting on the weight space of neural networks. Linear
groups are desirable due to their inherent interpretability, as they can be
represented as finite matrices. LGNs learn groups without any supervision or
knowledge of the hidden symmetries in the data and the groups can be mapped to
well known operations in machine learning. We use LGNs to learn groups on
multiple datasets while considering different downstream tasks; we demonstrate
that the linear group structure depends on both the data distribution and the
considered task.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: Baselines for Identifying Watermarked Large Language Models. (arXiv:2305.18456v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18456">http://arxiv.org/abs/2305.18456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18456] Baselines for Identifying Watermarked Large Language Models](http://arxiv.org/abs/2305.18456) #watermark</code></li>
<li>Summary: <p>We consider the emerging problem of identifying the presence and use of
watermarking schemes in widely used, publicly hosted, closed source large
language models (LLMs). We introduce a suite of baseline algorithms for
identifying watermarks in LLMs that rely on analyzing distributions of output
tokens and logits generated by watermarked and unmarked LLMs. Notably,
watermarked LLMs tend to produce distributions that diverge qualitatively and
identifiably from standard models. Furthermore, we investigate the
identifiability of watermarks at varying strengths and consider the tradeoffs
of each of our identification mechanisms with respect to watermarking scenario.
Along the way, we formalize the specific problem of identifying watermarks in
LLMs, as well as LLM watermarks and watermark detection in general, providing a
framework and foundations for studying them.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models. (arXiv:2305.18433v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18433">http://arxiv.org/abs/2305.18433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18433] Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models](http://arxiv.org/abs/2305.18433) #diffusion</code></li>
<li>Summary: <p>Most existing cross-modal generative methods based on diffusion models use
guidance to provide control over the latent space to enable conditional
generation across different modalities. Such methods focus on providing
guidance through separately-trained models, each for one modality. As a result,
these methods suffer from cross-modal information loss and are limited to
unidirectional conditional generation. Inspired by how humans synchronously
acquire multi-modal information and learn the correlation between modalities,
we explore a multi-modal diffusion model training and sampling scheme that uses
channel-wise image conditioning to learn cross-modality correlation during the
training phase to better mimic the learning process in the brain. Our empirical
results demonstrate that our approach can achieve data generation conditioned
on all correlated modalities.
</p></li>
</ul>

<h3>Title: Generating Driving Scenes with Diffusion. (arXiv:2305.18452v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18452">http://arxiv.org/abs/2305.18452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18452] Generating Driving Scenes with Diffusion](http://arxiv.org/abs/2305.18452) #diffusion</code></li>
<li>Summary: <p>In this paper we describe a learned method of traffic scene generation
designed to simulate the output of the perception system of a self-driving car.
In our "Scene Diffusion" system, inspired by latent diffusion, we use a novel
combination of diffusion and object detection to directly create realistic and
physically plausible arrangements of discrete bounding boxes for agents. We
show that our scene generation model is able to adapt to different regions in
the US, producing scenarios that capture the intricacies of each region.
</p></li>
</ul>

<h3>Title: Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18455">http://arxiv.org/abs/2305.18455</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18455] Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models](http://arxiv.org/abs/2305.18455) #diffusion</code></li>
<li>Summary: <p>Due to the ease of training, ability to scale, and high sample quality,
diffusion models (DMs) have become the preferred option for generative
modeling, with numerous pre-trained models available for a wide variety of
datasets. Containing intricate information about data distributions,
pre-trained DMs are valuable assets for downstream applications. In this work,
we consider learning from pre-trained DMs and transferring their knowledge to
other generative models in a data-free fashion. Specifically, we propose a
general framework called Diff-Instruct to instruct the training of arbitrary
generative models as long as the generated samples are differentiable with
respect to the model parameters. Our proposed Diff-Instruct is built on a
rigorous mathematical foundation where the instruction process directly
corresponds to minimizing a novel divergence we call Integral Kullback-Leibler
(IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL
divergence along a diffusion process, which we show to be more robust in
comparing distributions with misaligned supports. We also reveal non-trivial
connections of our method to existing works such as DreamFusion, and generative
adversarial training. To demonstrate the effectiveness and universality of
Diff-Instruct, we consider two scenarios: distilling pre-trained diffusion
models and refining existing GAN models. The experiments on distilling
pre-trained diffusion models show that Diff-Instruct results in
state-of-the-art single-step diffusion-based models. The experiments on
refining GAN models show that the Diff-Instruct can consistently improve the
pre-trained generators of GAN models across various settings.
</p></li>
</ul>

<h3>Title: Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation. (arXiv:2305.18470v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18470">http://arxiv.org/abs/2305.18470</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18470] Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation](http://arxiv.org/abs/2305.18470) #diffusion</code></li>
<li>Summary: <p>Generative models have had a profound impact on vision and language, paving
the way for a new era of multimodal generative applications. While these
successes have inspired researchers to explore using generative models in
science and engineering to accelerate the design process and reduce the
reliance on iterative optimization, challenges remain. Specifically,
engineering optimization methods based on physics still outperform generative
models when dealing with constrained environments where data is scarce and
precision is paramount. To address these challenges, we introduce Diffusion
Optimization Models (DOM) and Trajectory Alignment (TA), a learning framework
that demonstrates the efficacy of aligning the sampling trajectory of diffusion
models with the optimization trajectory derived from traditional physics-based
methods. This alignment ensures that the sampling process remains grounded in
the underlying physical principles. Our method allows for generating feasible
and high-performance designs in as few as two steps without the need for
expensive preprocessing, external surrogate models, or additional labeled data.
We apply our framework to structural topology optimization, a fundamental
problem in mechanical design, evaluating its performance on in- and
out-of-distribution configurations. Our results demonstrate that TA outperforms
state-of-the-art deep generative models on in-distribution configurations and
halves the inference computational cost. When coupled with a few steps of
optimization, it also improves manufacturability for out-of-distribution
conditions. By significantly improving performance and inference efficiency,
DOM enables us to generate high-quality designs in just a few steps and guide
them toward regions of high performance and manufacturability, paving the way
for the widespread application of generative models in large-scale data-driven
design.
</p></li>
</ul>

<h3>Title: Controllable Text-to-Image Generation with GPT-4. (arXiv:2305.18583v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18583">http://arxiv.org/abs/2305.18583</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18583] Controllable Text-to-Image Generation with GPT-4](http://arxiv.org/abs/2305.18583) #diffusion</code></li>
<li>Summary: <p>Current text-to-image generation models often struggle to follow textual
instructions, especially the ones requiring spatial reasoning. On the other
hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable
precision in generating code snippets for sketching out text inputs
graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide
the diffusion-based text-to-image pipelines with programmatic sketches
generated by GPT-4, enhancing their abilities for instruction following.
Control-GPT works by querying GPT-4 to write TikZ code, and the generated
sketches are used as references alongside the text instructions for diffusion
models (e.g., ControlNet) to generate photo-realistic images. One major
challenge to training our pipeline is the lack of a dataset containing aligned
text, images, and sketches. We address the issue by converting instance masks
in existing datasets into polygons to mimic the sketches used at test time. As
a result, Control-GPT greatly boosts the controllability of image generation.
It establishes a new state-of-art on the spatial arrangement and object
positioning generation and enhances users' control of object positions, sizes,
etc., nearly doubling the accuracy of prior models. Our work, as a first
attempt, shows the potential for employing LLMs to enhance the performance in
computer vision tasks.
</p></li>
</ul>

<h3>Title: BRIGHT: Bi-level Feature Representation of Image Collections using Groups of Hash Tables. (arXiv:2305.18601v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18601">http://arxiv.org/abs/2305.18601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18601] BRIGHT: Bi-level Feature Representation of Image Collections using Groups of Hash Tables](http://arxiv.org/abs/2305.18601) #diffusion</code></li>
<li>Summary: <p>We present BRIGHT, a bi-levelfeature representation for an imagecollection,
consisting of a per-image latent space on top of a multi-scale feature grid
space. Our representation is learned by an autoencoder to encode images
intocontinuouskey codes, which are used to retrieve features fromgroups of
multi-resolution hashtables. Our key codes and hash tables are trained together
continuously with well-defined gradient flows, leading to high usage of the
hash table entries and improved generative modeling compared to discrete Vector
Quantization (VQ). Differently from existing continuous representations such as
KL-regularized latent codes, our key codes are strictly bounded in scale and
variance. Overall, feature encoding by BRIGHT is compact, efficient to train,
and enables generative modeling over the image codes using state-of-the-art
generators such as latent diffusion models(LDMs). Experimental results show
that our method achieves comparable recon-struction results to VQ methods while
having a smaller and more efficient decoder network. By applying LDM over our
key code space, we achieve state-of-the-art performance on image synthesis on
the LSUN-Church and human-face datasets.
</p></li>
</ul>

<h3>Title: Likelihood-Based Diffusion Language Models. (arXiv:2305.18619v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18619">http://arxiv.org/abs/2305.18619</a></li>
<li>Code URL: <a href="https://github.com/igul222/plaid">https://github.com/igul222/plaid</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18619] Likelihood-Based Diffusion Language Models](http://arxiv.org/abs/2305.18619) #diffusion</code></li>
<li>Summary: <p>Despite a growing interest in diffusion-based language models, existing work
has not shown that these models can attain nontrivial likelihoods on standard
language modeling benchmarks. In this work, we take the first steps towards
closing the likelihood gap between autoregressive and diffusion-based language
models, with the goal of building and releasing a diffusion model which
outperforms a small but widely-known autoregressive model. We pursue this goal
through algorithmic improvements, scaling laws, and increased compute. On the
algorithmic front, we introduce several methodological improvements for the
maximum-likelihood training of diffusion language models. We then study scaling
laws for our diffusion models and find compute-optimal training regimes which
differ substantially from autoregressive models. Using our methods and scaling
analysis, we train and release Plaid 1B, a large diffusion language model which
outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent
samples in unconditional and zero-shot control settings.
</p></li>
</ul>

<h3>Title: Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18459">http://arxiv.org/abs/2305.18459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18459] Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning](http://arxiv.org/abs/2305.18459) #diffusion</code></li>
<li>Summary: <p>Diffusion models have demonstrated highly-expressive generative capabilities
in vision and NLP. Recent studies in reinforcement learning (RL) have shown
that diffusion models are also powerful in modeling complex policies or
trajectories in offline datasets. However, these works have been limited to
single-task settings where a generalist agent capable of addressing multi-task
predicaments is absent. In this paper, we aim to investigate the effectiveness
of a single diffusion model in modeling large-scale multi-task offline data,
which can be challenging due to diverse and multimodal data distribution.
Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a
diffusion-based method that incorporates Transformer backbones and prompt
learning for generative planning and data synthesis in multi-task offline
settings. \textsc{MTDiff} leverages vast amounts of knowledge available in
multi-task data and performs implicit knowledge sharing among tasks. For
generative planning, we find \textsc{MTDiff} outperforms state-of-the-art
algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data
synthesis, \textsc{MTDiff} generates high-quality data for testing tasks given
a single demonstration as a prompt, which enhances the low-quality datasets for
even unseen tasks.
</p></li>
</ul>

<h3>Title: On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18593">http://arxiv.org/abs/2305.18593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18593] On Diffusion Modeling for Anomaly Detection](http://arxiv.org/abs/2305.18593) #diffusion</code></li>
<li>Summary: <p>Known for their impressive performance in generative modeling, diffusion
models are attractive candidates for density-based anomaly detection. This
paper investigates different variations of diffusion modeling for unsupervised
and semi-supervised anomaly detection. In particular, we find that Denoising
Diffusion Probability Models (DDPM) are performant on anomaly detection
benchmarks yet computationally expensive. By simplifying DDPM in application to
anomaly detection, we are naturally led to an alternative approach called
Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior
distribution over diffusion time for a given input, enabling the identification
of anomalies due to their higher posterior density at larger timesteps. We
derive an analytical form for this posterior density and leverage a deep neural
network to improve inference efficiency. Through empirical evaluations on the
ADBench benchmark, we demonstrate that all diffusion-based anomaly detection
methods perform competitively. Notably, DTPM achieves orders of magnitude
faster inference time than DDPM, while outperforming it on this benchmark.
These results establish diffusion-based anomaly detection as an interpretable
and scalable alternative to traditional methods and recent deep-learning
techniques.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h3>Title: Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18413">http://arxiv.org/abs/2305.18413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18413] Learning to Learn from APIs: Black-Box Data-Free Meta-Learning](http://arxiv.org/abs/2305.18413) #data-free</code></li>
<li>Summary: <p>Data-free meta-learning (DFML) aims to enable efficient learning of new tasks
by meta-learning from a collection of pre-trained models without access to the
training data. Existing DFML work can only meta-learn from (i) white-box and
(ii) small-scale pre-trained models (iii) with the same architecture,
neglecting the more practical setting where the users only have inference
access to the APIs with arbitrary model architectures and model scale inside.
To solve this issue, we propose a Bi-level Data-free Meta Knowledge
Distillation (BiDf-MKD) framework to transfer more general meta knowledge from
a collection of black-box APIs to one single meta model. Specifically, by just
querying APIs, we inverse each API to recover its training data via a
zero-order gradient estimator and then perform meta-learning via a novel
bi-level meta knowledge distillation structure, in which we design a boundary
query set recovery technique to recover a more informative query set near the
decision boundary. In addition, to encourage better generalization within the
setting of limited API budgets, we propose task memory replay to diversify the
underlying task distribution by covering more interpolated tasks. Extensive
experiments in various real-world scenarios show the superior performance of
our BiDf-MKD framework.
</p></li>
</ul>

<h2>transformer</h2>
<h3>Title: MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18391">http://arxiv.org/abs/2305.18391</a></li>
<li>Code URL: <a href="https://github.com/vasilikikou/memegraphs">https://github.com/vasilikikou/memegraphs</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18391] MemeGraphs: Linking Memes to Knowledge Graphs](http://arxiv.org/abs/2305.18391) #transformer</code></li>
<li>Summary: <p>Memes are a popular form of communicating trends and ideas in social media
and on the internet in general, combining the modalities of images and text.
They can express humor and sarcasm but can also have offensive content.
Analyzing and classifying memes automatically is challenging since their
interpretation relies on the understanding of visual elements, language, and
background knowledge. Thus, it is important to meaningfully represent these
sources and the interaction between them in order to classify a meme as a
whole. In this work, we propose to use scene graphs, that express images in
terms of objects and their visual relations, and knowledge graphs as structured
representations for meme classification with a Transformer-based architecture.
We compare our approach with ImgBERT, a multimodal model that uses only learned
(instead of structured) representations of the meme, and observe consistent
improvements. We further provide a dataset with human graph annotations that we
compare to automatically generated graphs and entity linking. Analysis shows
that automatic methods link more entities than human annotators and that
automatically generated graphs are better suited for hatefulness classification
in memes.
</p></li>
</ul>

<h3>Title: Solar Irradiance Anticipative Transformer. (arXiv:2305.18487v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18487">http://arxiv.org/abs/2305.18487</a></li>
<li>Code URL: <a href="https://github.com/gittingthehubbing/siat">https://github.com/gittingthehubbing/siat</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18487] Solar Irradiance Anticipative Transformer](http://arxiv.org/abs/2305.18487) #transformer</code></li>
<li>Summary: <p>This paper proposes an anticipative transformer-based model for short-term
solar irradiance forecasting. Given a sequence of sky images, our proposed
vision transformer encodes features of consecutive images, feeding into a
transformer decoder to predict irradiance values associated with future unseen
sky images. We show that our model effectively learns to attend only to
relevant features in images in order to forecast irradiance. Moreover, the
proposed anticipative transformer captures long-range dependencies between sky
images to achieve a forecasting skill of 21.45 % on a 15 minute ahead
prediction for a newly introduced dataset of all-sky images when compared to a
smart persistence model.
</p></li>
</ul>

<h3>Title: Automated Feedback Generation for a Chemistry Database and Abstracting Exercise. (arXiv:2305.18319v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18319">http://arxiv.org/abs/2305.18319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18319] Automated Feedback Generation for a Chemistry Database and Abstracting Exercise](http://arxiv.org/abs/2305.18319) #transformer</code></li>
<li>Summary: <p>Timely feedback is an important part of teaching and learning. Here we
describe how a readily available neural network transformer (machine-learning)
model (BERT) can be used to give feedback on the structure of the response to
an abstracting exercise where students are asked to summarise the contents of a
published article after finding it from a publication database. The dataset
contained 207 submissions from two consecutive years of the course, summarising
a total of 21 different papers from the primary literature. The model was
pre-trained using an available dataset (approx. 15,000 samples) and then
fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be
important. The sentences in the student submissions are characterised into
three classes - background, technique and observation - which allows a
comparison of how each submission is structured. Comparing the structure of the
students' abstract a large collection of those from the PubMed database shows
that students in this exercise concentrate more on the background to the paper
and less on the techniques and results than the abstracts to papers themselves.
The results allowed feedback for each submitted assignment to be automatically
generated.
</p></li>
</ul>

<h3>Title: Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18390">http://arxiv.org/abs/2305.18390</a></li>
<li>Code URL: <a href="https://github.com/thunlp/modularity-analysis">https://github.com/thunlp/modularity-analysis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18390] Emergent Modularity in Pre-trained Transformers](http://arxiv.org/abs/2305.18390) #transformer</code></li>
<li>Summary: <p>This work examines the presence of modularity in pre-trained Transformers, a
feature commonly found in human brains and thought to be vital for general
intelligence. In analogy to human brains, we consider two main characteristics
of modularity: (1) functional specialization of neurons: we evaluate whether
each neuron is mainly specialized in a certain function, and find that the
answer is yes. (2) function-based neuron grouping: we explore finding a
structure that groups neurons into modules by function, and each module works
for its corresponding function. Given the enormous amount of possible
structures, we focus on Mixture-of-Experts as a promising candidate, which
partitions neurons into experts and usually activates different experts for
different inputs. Experimental results show that there are functional experts,
where clustered are the neurons specialized in a certain function. Moreover,
perturbing the activations of functional experts significantly affects the
corresponding function. Finally, we study how modularity emerges during
pre-training, and find that the modular structure is stabilized at the early
stage, which is faster than neuron stabilization. It suggests that Transformers
first construct the modular structure and then learn fine-grained neuron
functions. Our code and data are available at
https://github.com/THUNLP/modularity-analysis.
</p></li>
</ul>

<h3>Title: SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics. (arXiv:2305.18513v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18513">http://arxiv.org/abs/2305.18513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18513] SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics](http://arxiv.org/abs/2305.18513) #transformer</code></li>
<li>Summary: <p>Transformer-based models, such as BERT and ViT, have achieved
state-of-the-art results across different natural language processing (NLP) and
computer vision (CV) tasks. However, these models are extremely memory
intensive during their fine-tuning process, making them difficult to deploy on
GPUs with limited memory resources. To address this issue, we introduce a new
tool called SlimFit that reduces the memory requirements of these models by
dynamically analyzing their training dynamics and freezing less-contributory
layers during fine-tuning. The layers to freeze are chosen using a runtime
inter-layer scheduling algorithm. SlimFit adopts quantization and pruning for
particular layers to balance the load of dynamic activations and to minimize
the memory footprint of static activations, where static activations refer to
those that cannot be discarded regardless of freezing. This allows SlimFit to
freeze up to 95% of layers and reduce the overall on-device GPU memory usage of
transformer-based models such as ViT and BERT by an average of 2.2x, across
different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10,
CIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy. For
such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory
usage with an accuracy degradation of only up to 0.4%. As a result, while
fine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128
requires 3 and 2 32GB GPUs respectively, SlimFit enables their fine-tuning on a
single 32GB GPU without any significant accuracy degradation.
</p></li>
</ul>

<h3>Title: Improving Generalization for Multimodal Fake News Detection. (arXiv:2305.18599v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18599">http://arxiv.org/abs/2305.18599</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18599] Improving Generalization for Multimodal Fake News Detection](http://arxiv.org/abs/2305.18599) #transformer</code></li>
<li>Summary: <p>The increasing proliferation of misinformation and its alarming impact have
motivated both industry and academia to develop approaches for fake news
detection. However, state-of-the-art approaches are usually trained on datasets
of smaller size or with a limited set of specific topics. As a consequence,
these models lack generalization capabilities and are not applicable to
real-world data. In this paper, we propose three models that adopt and
fine-tune state-of-the-art multimodal transformers for multimodal fake news
detection. We conduct an in-depth analysis by manipulating the input data aimed
to explore models performance in realistic use cases on social media. Our study
across multiple models demonstrates that these systems suffer significant
performance drops against manipulated data. To reduce the bias and improve
model generalization, we suggest training data augmentation to conduct more
meaningful experiments for fake news detection on social media. The proposed
data augmentation techniques enable models to generalize better and yield
improved state-of-the-art results.
</p></li>
</ul>

<h3>Title: Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers. (arXiv:2305.18382v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18382">http://arxiv.org/abs/2305.18382</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18382] Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers](http://arxiv.org/abs/2305.18382) #transformer</code></li>
<li>Summary: <p>Efficient time series forecasting has become critical for real-world
applications, particularly with deep neural networks (DNNs). Efficiency in DNNs
can be achieved through sparse connectivity and reducing the model size.
However, finding the sparsity level automatically during training remains a
challenging task due to the heterogeneity in the loss-sparsity tradeoffs across
the datasets. In this paper, we propose \enquote{\textbf{P}runing with
\textbf{A}daptive \textbf{S}parsity \textbf{L}evel} (\textbf{PALS}), to
automatically seek an optimal balance between loss and sparsity, all without
the need for a predefined sparsity level. PALS draws inspiration from both
sparse training and during-training methods. It introduces the novel "expand"
mechanism in training sparse neural networks, allowing the model to dynamically
shrink, expand, or remain stable to find a proper sparsity level. In this
paper, we focus on achieving efficiency in transformers known for their
excellent time series forecasting performance but high computational cost.
Nevertheless, PALS can be applied directly to any DNN. In the scope of these
arguments, we demonstrate its effectiveness also on the DLinear model.
Experimental results on six benchmark datasets and five state-of-the-art
transformer variants show that PALS substantially reduces model size while
maintaining comparable performance to the dense model. More interestingly, PALS
even outperforms the dense model, in 12 and 14 cases out of 30 cases in terms
of MSE and MAE loss, respectively, while reducing 65% parameter count and 63%
FLOPs on average. Our code will be publicly available upon acceptance of the
paper.
</p></li>
</ul>

<h3>Title: On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18399">http://arxiv.org/abs/2305.18399</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18399] On the impact of activation and normalization in obtaining isometric embeddings at initialization](http://arxiv.org/abs/2305.18399) #transformer</code></li>
<li>Summary: <p>In this paper, we explore the structure of the penultimate Gram matrix in
deep neural networks, which contains the pairwise inner products of outputs
corresponding to a batch of inputs. In several architectures it has been
observed that this Gram matrix becomes degenerate with depth at initialization,
which dramatically slows training. Normalization layers, such as batch or layer
normalization, play a pivotal role in preventing the rank collapse issue.
Despite promising advances, the existing theoretical results (i) do not extend
to layer normalization, which is widely used in transformers, (ii) can not
characterize the bias of normalization quantitatively at finite depth.
</p></li>
</ul>

<p>To bridge this gap, we provide a proof that layer normalization, in
conjunction with activation layers, biases the Gram matrix of a multilayer
perceptron towards isometry at an exponential rate with depth at
initialization. We quantify this rate using the Hermite expansion of the
activation function, highlighting the importance of higher order ($\ge 2$)
Hermite coefficients in the bias towards isometry.
</p>

<h3>Title: Geometric Algebra Transformers. (arXiv:2305.18415v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18415">http://arxiv.org/abs/2305.18415</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18415] Geometric Algebra Transformers](http://arxiv.org/abs/2305.18415) #transformer</code></li>
<li>Summary: <p>Problems involving geometric data arise in a variety of fields, including
computer vision, robotics, chemistry, and physics. Such data can take numerous
forms, such as points, direction vectors, planes, or transformations, but to
date there is no single architecture that can be applied to such a wide variety
of geometric types while respecting their symmetries. In this paper we
introduce the Geometric Algebra Transformer (GATr), a general-purpose
architecture for geometric data. GATr represents inputs, outputs, and hidden
states in the projective geometric algebra, which offers an efficient
16-dimensional vector space representation of common geometric objects as well
as operators acting on them. GATr is equivariant with respect to E(3), the
symmetry group of 3D Euclidean space. As a transformer, GATr is scalable,
expressive, and versatile. In experiments with n-body modeling and robotic
planning, GATr shows strong improvements over non-geometric baselines.
</p></li>
</ul>

<h3>Title: Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18475">http://arxiv.org/abs/2305.18475</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18475] Approximation theory of transformer networks for sequence modeling](http://arxiv.org/abs/2305.18475) #transformer</code></li>
<li>Summary: <p>The transformer is a widely applied architecture in sequence modeling
applications, but the theoretical understanding of its working principles is
limited. In this work, we investigate the ability of transformers to
approximate sequential relationships. We first prove a universal approximation
theorem for the transformer hypothesis space. From its derivation, we identify
a novel notion of regularity under which we can prove an explicit approximation
rate estimate. This estimate reveals key structural properties of the
transformer and suggests the types of sequence relationships that the
transformer is adapted to approximating. In particular, it allows us to
concretely discuss the structural bias between the transformer and classical
sequence modeling methods, such as recurrent neural networks. Our findings are
supported by numerical experiments.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?. (arXiv:2305.18398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18398">http://arxiv.org/abs/2305.18398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18398] Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?](http://arxiv.org/abs/2305.18398) #generative</code></li>
<li>Summary: <p>Text-conditioned image generation models have recently achieved astonishing
results in image quality and text alignment and are consequently employed in a
fast-growing number of applications. Since they are highly data-driven, relying
on billion-sized datasets randomly scraped from the web, they also reproduce
inappropriate human behavior. Specifically, we demonstrate inappropriate
degeneration on a large-scale for various generative text-to-image models, thus
motivating the need for monitoring and moderating them at deployment. To this
end, we evaluate mitigation strategies at inference to suppress the generation
of inappropriate content. Our findings show that we can use models'
representations of the world's ugliness to align them with human preferences.
</p></li>
</ul>

<h3>Title: Alteration-free and Model-agnostic Origin Attribution of Generated Images. (arXiv:2305.18439v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18439">http://arxiv.org/abs/2305.18439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18439] Alteration-free and Model-agnostic Origin Attribution of Generated Images](http://arxiv.org/abs/2305.18439) #generative</code></li>
<li>Summary: <p>Recently, there has been a growing attention in image generation models.
However, concerns have emerged regarding potential misuse and intellectual
property (IP) infringement associated with these models. Therefore, it is
necessary to analyze the origin of images by inferring if a specific image was
generated by a particular model, i.e., origin attribution. Existing methods are
limited in their applicability to specific types of generative models and
require additional steps during training or generation. This restricts their
use with pre-trained models that lack these specific operations and may
compromise the quality of image generation. To overcome this problem, we first
develop an alteration-free and model-agnostic origin attribution method via
input reverse-engineering on image generation models, i.e., inverting the input
of a particular model for a specific image. Given a particular model, we first
analyze the differences in the hardness of reverse-engineering tasks for the
generated images of the given model and other images. Based on our analysis, we
propose a method that utilizes the reconstruction loss of reverse-engineering
to infer the origin. Our proposed method effectively distinguishes between
generated images from a specific generative model and other images, including
those generated by different models and real images.
</p></li>
</ul>

<h3>Title: Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18342">http://arxiv.org/abs/2305.18342</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18342] Neural Task Synthesis for Visual Programming](http://arxiv.org/abs/2305.18342) #generative</code></li>
<li>Summary: <p>Generative neural models hold great promise in enhancing programming
education by synthesizing new content for students. We seek to design neural
models that can automatically generate programming tasks for a given
specification in the context of visual programming domains. Despite the recent
successes of large generative models like GPT-4, our initial results show that
these models are ineffective in synthesizing visual programming tasks and
struggle with logical and spatial reasoning. We propose a novel neuro-symbolic
technique, NeurTaskSyn, that can synthesize programming tasks for a
specification given in the form of desired programming concepts exercised by
its solution code and constraints on the visual task. NeurTaskSyn has two
components: the first component is trained via imitation learning procedure to
generate possible solution codes, and the second component is trained via
reinforcement learning procedure to guide an underlying symbolic execution
engine that generates visual tasks for these codes. We demonstrate the
effectiveness of NeurTaskSyn through an extensive empirical evaluation and a
qualitative study on reference tasks taken from the Hour of Code: Classic Maze
challenge by Code-dot-org and the Intro to Programming with Karel course by
CodeHS-dot-com.
</p></li>
</ul>

<h3>Title: A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18486">http://arxiv.org/abs/2305.18486</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18486] A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets](http://arxiv.org/abs/2305.18486) #generative</code></li>
<li>Summary: <p>The development of large language models (LLMs) such as ChatGPT has brought a
lot of attention recently. However, their evaluation in the benchmark academic
datasets remains under-explored due to the difficulty of evaluating the
generative outputs produced by this model against the ground truth. In this
paper, we aim to present a thorough evaluation of ChatGPT's performance on
diverse academic datasets, covering tasks like question-answering, text
summarization, code generation, commonsense reasoning, mathematical
problem-solving, machine translation, bias detection, and ethical
considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze
255K responses it generates in these datasets. This makes our work the largest
evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate
the strengths and weaknesses of ChatGPT in various tasks and provide insights
for future research using LLMs. We also report a new emergent ability to follow
multi-query instructions that we mostly found in ChatGPT and other
instruction-tuned models. Our extensive evaluation shows that even though
ChatGPT is capable of performing a wide variety of tasks, and may obtain
impressive performance in several benchmark datasets, it is still far from
achieving the ability to reliably solve many challenging tasks. By providing a
thorough assessment of ChatGPT's performance across diverse NLP tasks, this
paper sets the stage for a targeted deployment of ChatGPT-like LLMs in
real-world applications.
</p></li>
</ul>

<h3>Title: Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling. (arXiv:2305.18375v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18375">http://arxiv.org/abs/2305.18375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18375] Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling](http://arxiv.org/abs/2305.18375) #generative</code></li>
<li>Summary: <p>Learning to denoise has emerged as a prominent paradigm to design
state-of-the-art deep generative models for natural images. How to use it to
model the distributions of both continuous real-valued data and categorical
data has been well studied in recently proposed diffusion models. However, it
is found in this paper to have limited ability in modeling some other types of
data, such as count and non-negative continuous data, that are often highly
sparse, skewed, heavy-tailed, and/or overdispersed. To this end, we propose
learning to jump as a general recipe for generative modeling of various types
of data. Using a forward count thinning process to construct learning
objectives to train a deep neural network, it employs a reverse count
thickening process to iteratively refine its generation through that network.
We demonstrate when learning to jump is expected to perform comparably to
learning to denoise, and when it is expected to perform better. For example,
learning to jump is recommended when the training data is non-negative and
exhibits strong sparsity, skewness, heavy-tailedness, and/or heterogeneity.
</p></li>
</ul>

<h3>Title: Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18378">http://arxiv.org/abs/2305.18378</a></li>
<li>Code URL: <a href="https://github.com/kylehkhsu/disentangle">https://github.com/kylehkhsu/disentangle</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18378] Disentanglement via Latent Quantization](http://arxiv.org/abs/2305.18378) #generative</code></li>
<li>Summary: <p>In disentangled representation learning, a model is asked to tease apart a
dataset's underlying sources of variation and represent them independently of
one another. Since the model is provided with no ground truth information about
these sources, inductive biases take a paramount role in enabling
disentanglement. In this work, we construct an inductive bias towards
compositionally encoding and decoding data by enforcing a harsh communication
bottleneck. Concretely, we do this by (i) quantizing the latent space into
learnable discrete codes with a separate scalar codebook per dimension and (ii)
applying strong model regularization via an unusually high weight decay.
Intuitively, the quantization forces the encoder to use a small number of
latent values across many datapoints, which in turn enables the decoder to
assign a consistent meaning to each value. Regularization then serves to drive
the model towards this parsimonious strategy. We demonstrate the broad
applicability of this approach by adding it to both basic data-reconstructing
(vanilla autoencoder) and latent-reconstructing (InfoGAN) generative models. In
order to reliably assess these models, we also propose InfoMEC, new metrics for
disentanglement that are cohesively grounded in information theory and fix
well-established shortcomings in previous metrics. Together with
regularization, latent quantization dramatically improves the modularity and
explicitness of learned representations on a representative suite of benchmark
datasets. In particular, our quantized-latent autoencoder (QLAE) consistently
outperforms strong methods from prior work in these key disentanglement
properties without compromising data reconstruction.
</p></li>
</ul>

<h3>Title: GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18427">http://arxiv.org/abs/2305.18427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18427] GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning](http://arxiv.org/abs/2305.18427) #generative</code></li>
<li>Summary: <p>A major challenge in reinforcement learning is to determine which
state-action pairs are responsible for future rewards that are delayed. Return
Decomposition offers a solution by redistributing rewards from observed
sequences while preserving policy invariance. While the majority of current
approaches construct the reward redistribution in an uninterpretable manner, we
propose to explicitly model the contributions of state and action from a causal
perspective, resulting in an interpretable return decomposition. In this paper,
we start by studying the role of causal generative models in return
decomposition by characterizing the generation of Markovian rewards and
trajectory-wise long-term return and further propose a framework, called
Generative Return Decomposition (GRD), for policy optimization in delayed
reward scenarios. Specifically, GRD first identifies the unobservable Markovian
rewards and causal relations in the generative process. Then, GRD makes use of
the identified causal generative model to form a compact representation to
train policy over the most favorable subspace of the state space of the agent.
Theoretically, we show that the unobservable Markovian reward function is
identifiable, as well as the underlying causal structure and causal models.
Experimental results show that our method outperforms state-of-the-art methods
and the provided visualization further demonstrates the interpretability of our
method.
</p></li>
</ul>

<h3>Title: Bringing regularized optimal transport to lightspeed: a splitting method adapted for GPUs. (arXiv:2305.18483v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18483">http://arxiv.org/abs/2305.18483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18483] Bringing regularized optimal transport to lightspeed: a splitting method adapted for GPUs](http://arxiv.org/abs/2305.18483) #generative</code></li>
<li>Summary: <p>We present an efficient algorithm for regularized optimal transport. In
contrast to previous methods, we use the Douglas-Rachford splitting technique
to develop an efficient solver that can handle a broad class of regularizers.
The algorithm has strong global convergence guarantees, low per-iteration cost,
and can exploit GPU parallelization, making it considerably faster than the
state-of-the-art for many problems. We illustrate its competitiveness in
several applications, including domain adaptation and learning of generative
models.
</p></li>
</ul>

<h3>Title: Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18612">http://arxiv.org/abs/2305.18612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18612] Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders](http://arxiv.org/abs/2305.18612) #generative</code></li>
<li>Summary: <p>Multivariate time series (MTS) imputation is a widely studied problem in
recent years. Existing methods can be divided into two main groups, including
(1) deep recurrent or generative models that primarily focus on time series
features, and (2) graph neural networks (GNNs) based models that utilize the
topological information from the inherent graph structure of MTS as relational
inductive bias for imputation. Nevertheless, these methods either neglect
topological information or assume the graph structure is fixed and accurately
known. Thus, they fail to fully utilize the graph dynamics for precise
imputation in more challenging MTS data such as networked time series (NTS),
where the underlying graph is constantly changing and might have missing edges.
In this paper, we propose a novel approach to overcome these limitations.
First, we define the problem of imputation over NTS which contains missing
values in both node time series features and graph structures. Then, we design
a new model named PoGeVon which leverages variational autoencoder (VAE) to
predict missing values over both node time series features and graph
structures. In particular, we propose a new node position embedding based on
random walk with restart (RWR) in the encoder with provable higher expressive
power compared with message-passing based graph neural networks (GNNs). We
further design a decoder with 3-stage predictions from the perspective of
multi-task learning to impute missing values in both time series and graph
structures reciprocally. Experiment results demonstrate the effectiveness of
our model over baselines.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18500">http://arxiv.org/abs/2305.18500</a></li>
<li>Code URL: <a href="https://github.com/txh-mercury/vast">https://github.com/txh-mercury/vast</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18500] VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset](http://arxiv.org/abs/2305.18500) #large language model</code></li>
<li>Summary: <p>Vision and text have been fully explored in contemporary video-text
foundational models, while other modalities such as audio and subtitles in
videos have not received sufficient attention. In this paper, we resort to
establish connections between multi-modality video tracks, including Vision,
Audio, and Subtitle, and Text by exploring an automatically generated
large-scale omni-modality video caption dataset called VAST-27M. Specifically,
we first collect 27 million open-domain video clips and separately train a
vision and an audio captioner to generate vision and audio captions. Then, we
employ an off-the-shelf Large Language Model (LLM) to integrate the generated
captions, together with subtitles and instructional prompts into omni-modality
captions. Based on the proposed VAST-27M dataset, we train an omni-modality
video-text foundational model named VAST, which can perceive and process
vision, audio, and subtitle modalities from video, and better support various
tasks including vision-text, audio-text, and multi-modal video-text tasks
(retrieval, captioning and QA). Extensive experiments have been conducted to
demonstrate the effectiveness of our proposed VAST-27M corpus and VAST
foundation model. VAST achieves 22 new state-of-the-art results on various
cross-modality benchmarks. Code, model and dataset will be released at
https://github.com/TXH-mercury/VAST.
</p></li>
</ul>

<h3>Title: Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain. (arXiv:2305.18324v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18324">http://arxiv.org/abs/2305.18324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18324] Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain](http://arxiv.org/abs/2305.18324) #large language model</code></li>
<li>Summary: <p>A common way to use large pre-trained language models for downstream tasks is
to fine tune them using additional layers. This may not work well if downstream
domain is a specialized domain whereas the large language model has been
pre-trained on a generic corpus. In this paper, we discuss the use of regular
expression patterns employed as features for domain knowledge during the
process of fine tuning, in addition to domain specific text. Our experiments on
real scenario production data show that this method of fine tuning improves the
downstream text classification tasks as compared to fine tuning only on domain
specific text. We also show that the use of attention network for fine tuning
improves results compared to simple linear layers.
</p></li>
</ul>

<h3>Title: LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. (arXiv:2305.18354v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18354">http://arxiv.org/abs/2305.18354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18354] LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations](http://arxiv.org/abs/2305.18354) #large language model</code></li>
<li>Summary: <p>Can a Large Language Model (LLM) solve simple abstract reasoning problems? We
explore this broad question through a systematic analysis of GPT on the
Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract
reasoning ability from limited examples in which solutions require some "core
knowledge" of concepts such as objects, goal states, counting, and basic
geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when
using textual encodings for their two-dimensional input-output grids. Our
failure analysis reveals that GPT-4's capacity to identify objects and reason
about them is significantly influenced by the sequential nature of the text
that represents an object within a text encoding of a task. To test this
hypothesis, we design a new benchmark, the 1D-ARC, which consists of
one-dimensional (array-like) tasks that are more conducive to GPT-based
reasoning, and where it indeed performs better than on the (2D) ARC. To
alleviate this issue, we propose an object-based representation that is
obtained through an external tool, resulting in nearly doubling the performance
on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the
state-of-the-art GPT-4 is unable to "reason" perfectly within non-language
domains such as the 1D-ARC or a simple ARC subset, our study reveals that the
use of object-based representations can significantly improve its reasoning
ability. Visualizations, GPT logs, and data are available at
https://khalil-research.github.io/LLM4ARC.
</p></li>
</ul>

<h3>Title: What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18365">http://arxiv.org/abs/2305.18365</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18365] What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks](http://arxiv.org/abs/2305.18365) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) with strong abilities in natural language
processing tasks have emerged and have been rapidly applied in various kinds of
areas such as science, finance and software engineering. However, the
capability of LLMs to advance the field of chemistry remains unclear. In this
paper,we establish a comprehensive benchmark containing 8 practical chemistry
tasks, including 1) name prediction, 2) property prediction, 3) yield
prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants
from products), 6)text-based molecule design, 7) molecule captioning, and 8)
reagent selection. Our analysis draws on widely recognized datasets including
BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the
capacities of LLMs within the context of practical chemistry. Three GPT models
(GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in
zero-shot and few-shot in-context learning settings with carefully selected
demonstration examples and specially crafted prompts. The key results of our
investigation are 1) GPT-4 outperforms the other two models among the three
evaluated; 2) GPT models exhibit less competitive performance in tasks
demanding precise understanding of molecular SMILES representation, such as
reaction prediction and retrosynthesis;3) GPT models demonstrate strong
capabilities in text-related explanation tasks such as molecule captioning; and
4) GPT models exhibit comparable or better performance to classical machine
learning models when applied to chemical problems that can be transformed into
classification or ranking tasks, such as property prediction, and yield
prediction.
</p></li>
</ul>

<h3>Title: Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18404">http://arxiv.org/abs/2305.18404</a></li>
<li>Code URL: <a href="https://github.com/bhaweshiitk/conformalllm">https://github.com/bhaweshiitk/conformalllm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18404] Conformal Prediction with Large Language Models for Multi-Choice Question Answering](http://arxiv.org/abs/2305.18404) #large language model</code></li>
<li>Summary: <p>As large language models continue to be widely developed, robust uncertainty
quantification techniques will become crucial for their safe deployment in
high-stakes scenarios. In this work, we explore how conformal prediction can be
used to provide uncertainty quantification in language models for the specific
task of multiple-choice question-answering. We find that the uncertainty
estimates from conformal prediction are tightly correlated with prediction
accuracy. This observation can be useful for downstream applications such as
selective classification and filtering out low-quality predictions. We also
investigate the exchangeability assumption required by conformal prediction to
out-of-subject questions, which may be a more realistic scenario for many
practical applications. Our work contributes towards more trustworthy and
reliable usage of large language models in safety-critical situations, where
robust guarantees of error rate are required.
</p></li>
</ul>

<h3>Title: Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data. (arXiv:2305.18410v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18410">http://arxiv.org/abs/2305.18410</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18410] Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data](http://arxiv.org/abs/2305.18410) #large language model</code></li>
<li>Summary: <p>The need for more usable and explainable machine learning models in
healthcare increases the importance of developing and utilizing causal
discovery algorithms, which aim to discover causal relations by analyzing
observational data. Explainable approaches aid clinicians and biologists in
predicting the prognosis of diseases and suggesting proper treatments. However,
very little research has been conducted at the crossroads between causal
discovery, genomics, and breast cancer, and we aim to bridge this gap.
Moreover, evaluation of causal discovery methods on real data is in general
notoriously difficult because ground-truth causal relations are usually
unknown, and accordingly, in this paper, we also propose to address the
evaluation problem with large language models. In particular, we exploit
suitable causal discovery algorithms to investigate how various perturbations
in the genome can affect the survival of patients diagnosed with breast cancer.
We used three main causal discovery algorithms: PC, Greedy Equivalence Search
(GES), and a Generalized Precision Matrix-based one. We experiment with a
subset of The Cancer Genome Atlas, which contains information about mutations,
copy number variations, protein levels, and gene expressions for 705 breast
cancer patients. Our findings reveal important factors related to the vital
status of patients using causal discovery algorithms. However, the reliability
of these results remains a concern in the medical domain. Accordingly, as
another contribution of the work, the results are validated through language
models trained on biomedical literature, such as BlueBERT and other large
language models trained on medical corpora. Our results profess proper
utilization of causal discovery algorithms and language models for revealing
reliable causal relations for clinical applications.
</p></li>
</ul>

<h3>Title: Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18466">http://arxiv.org/abs/2305.18466</a></li>
<li>Code URL: <a href="https://github.com/socialfoundations/tttlm">https://github.com/socialfoundations/tttlm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18466] Test-Time Training on Nearest Neighbors for Large Language Models](http://arxiv.org/abs/2305.18466) #large language model</code></li>
<li>Summary: <p>Many recent efforts aim to augment language models with relevant information
retrieved from a database at test time. We avoid the need for prompt
engineering by directly fine-tuning the model on data retrieved at test time
using its standard training setup. For this purpose, we build a large-scale
distributed nearest neighbor index based on text embeddings of the Pile
dataset. Given a query to a language model, our system retrieves the neighbors
of the query and fine-tunes the model on the text data corresponding to those
neighbors. Surprisingly, retrieving and training on as few as 20 neighbors,
each for only one gradient iteration, drastically improves performance across
more than twenty language modeling tasks in the Pile benchmark. For example,
test-time training significantly narrows the performance gap between a small
GPT2 model and a GPTNeo model, more than ten times larger, that was
specifically trained to convergence on the Pile. Sufficient index quality and
size, however, are important. Our work establishes a valuable first baseline
for implementing test-time training in the context of large language models,
opening the door to numerous promising research avenues.
</p></li>
</ul>

<h3>Title: Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models. (arXiv:2305.18507v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18507">http://arxiv.org/abs/2305.18507</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18507] Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models](http://arxiv.org/abs/2305.18507) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have scaled up to unlock a wide range of complex
reasoning tasks with the aid of various prompting methods. However, current
prompting methods generate natural language intermediate steps to help
reasoning, which can cause imperfect task reduction and confusion. To mitigate
such limitations, we explore code prompting, a neural symbolic prompting method
with both zero-shot and few-shot versions which triggers code as intermediate
steps. We conduct experiments on 7 widely-used benchmarks involving symbolic
reasoning and arithmetic reasoning. Code prompting generally outperforms
chain-of-thought (CoT) prompting. To further understand the performance and
limitations of code prompting, we perform extensive ablation studies and error
analyses, and identify several exclusive advantages of using symbolic
promptings compared to natural language. We also consider the ensemble of code
prompting and CoT prompting to combine the strengths of both. Finally, we show
through experiments how code annotations and their locations affect code
prompting.
</p></li>
</ul>

<h3>Title: Self Information Update for Large Language Models through Mitigating Exposure Bias. (arXiv:2305.18582v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18582">http://arxiv.org/abs/2305.18582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18582] Self Information Update for Large Language Models through Mitigating Exposure Bias](http://arxiv.org/abs/2305.18582) #large language model</code></li>
<li>Summary: <p>Current LLMs have demonstrated remarkable capabilities in addressing users'
requests for various types of information. However, these models are limited by
the most recent data available in their pretraining corpora, rendering them
incapable of providing up-to-date information. Retraining LLMs from scratch is
cost-prohibitive, and the effectiveness of continual fine-tuning on new corpora
has not been thoroughly examined. Additionally, current update procedures
typically demand significant human input to prepare the information into more
structured format, such as knowledge triples, conversational data or responses
with human feedback. In this study, we conduct a comprehensive examination of a
novel self information update task in LLMs, which only requires the provision
of informative text corpora. For instance, we can use the latest news articles
to update the LLMs' existing knowledge. We define the self information update
task and assess the continual fine-tuning approach for this purpose. We observe
that the naive method of continual fine-tuning can be problematic due to LLMs'
exposure bias, which prioritizes existing information over new information we
aim to integrate and leads to incorrect reasoning chains that ultimately
diminish the efficacy of information updates. Based on our analysis, we propose
an effective method to mitigate exposure bias by incorporating the selection of
relevant facts into training losses. Furthermore, we develop a dataset to
evaluate information updates, derived from news articles published after March</li>
<li>Experimental results demonstrate that our proposed approach significantly
increases the factual consistency score (0 to 1) by 0.16 while having minimal
impact on performance for instructions not directly related to the new
information.
</p></li>
</ul>

<h3>Title: Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard. (arXiv:2305.18618v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18618">http://arxiv.org/abs/2305.18618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18618] Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3](http://arxiv.org/abs/2305.18618) #large language model</code></li>
<li>Summary: <p>A comparison between three chatbots which are based on large language models,
namely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their
ability to give correct answers to mathematics and logic problems. In
particular, we check their ability to Understand the problem at hand; Apply
appropriate algorithms or methods for its solution; and Generate a coherent
response and a correct answer. We use 30 questions that are clear, without any
ambiguities, fully described with plain text only, and have a unique, well
defined correct answer. The questions are divided into two sets of 15 each. The
questions of Set A are 15 "Original" problems that cannot be found online,
while Set B contains 15 "Published" problems that one can find online, usually
with their solution. Each question is posed three times to each chatbot. The
answers are recorded and discussed, highlighting their strengths and
weaknesses. It has been found that for straightforward arithmetic, algebraic
expressions, or basic logic puzzles, chatbots may provide accurate solutions,
although not in every attempt. However, for more complex mathematical problems
or advanced logic tasks, their answers, although written in a usually
"convincing" way, may not be reliable. Consistency is also an issue, as many
times a chatbot will provide conflicting answers when given the same question
more than once. A comparative quantitative evaluation of the three chatbots is
made through scoring their final answers based on correctness. It was found
that ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes
third in the original questions of Set A, behind the other two chatbots, while
it has the best performance (first place) in the published questions of Set B.
This is probably because Bard has direct access to the internet, in contrast to
ChatGPT chatbots which do not have any communication with the outside world.
</p></li>
</ul>

<h3>Title: CONA: A novel CONtext-Aware instruction paradigm for communication using large language model. (arXiv:2305.18620v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18620">http://arxiv.org/abs/2305.18620</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18620] CONA: A novel CONtext-Aware instruction paradigm for communication using large language model](http://arxiv.org/abs/2305.18620) #large language model</code></li>
<li>Summary: <p>We introduce CONA, a novel context-aware instruction paradigm for effective
knowledge dissemination using generative pre-trained transformer (GPT) models.
CONA is a flexible framework designed to leverage the capabilities of Large
Language Models (LLMs) and incorporate DIKW (Data, Information, Knowledge,
Wisdom) hierarchy to automatically instruct and optimise presentation content,
anticipate potential audience inquiries, and provide context-aware answers that
adaptive to the knowledge level of the audience group. The unique aspect of the
CONA paradigm lies in its combination of an independent advisory mechanism and
a recursive feedback loop rooted on the DIKW hierarchy. This synergy
significantly enhances context-aware contents, ensuring they are accessible and
easily comprehended by the audience. This paradigm is an early pioneer to
explore new methods for knowledge dissemination and communication in the LLM
era, offering effective support for everyday knowledge sharing scenarios. We
conduct experiments on a range of audience roles, along with materials from
various disciplines using GPT4. Both quantitative and qualitative results
demonstrated that the proposed CONA paradigm achieved remarkable performance
compared to the outputs guided by conventional prompt engineering.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Explicit Visual Prompting for Universal Foreground Segmentations. (arXiv:2305.18476v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18476">http://arxiv.org/abs/2305.18476</a></li>
<li>Code URL: <a href="https://github.com/nifangbaage/explicit-visual-prompt">https://github.com/nifangbaage/explicit-visual-prompt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18476] Explicit Visual Prompting for Universal Foreground Segmentations](http://arxiv.org/abs/2305.18476) #segmentation</code></li>
<li>Summary: <p>Foreground segmentation is a fundamental problem in computer vision, which
includes salient object detection, forgery detection, defocus blur detection,
shadow detection, and camouflage object detection. Previous works have
typically relied on domain-specific solutions to address accuracy and
robustness issues in those applications. In this paper, we present a unified
framework for a number of foreground segmentation tasks without any
task-specific designs. We take inspiration from the widely-used pre-training
and then prompt tuning protocols in NLP and propose a new visual prompting
model, named Explicit Visual Prompting (EVP). Different from the previous
visual prompting which is typically a dataset-level implicit embedding, our key
insight is to enforce the tunable parameters focusing on the explicit visual
content from each individual image, i.e., the features from frozen patch
embeddings and high-frequency components. Our method freezes a pre-trained
model and then learns task-specific knowledge using a few extra parameters.
Despite introducing only a small number of tunable parameters, EVP achieves
superior performance than full fine-tuning and other parameter-efficient
fine-tuning methods. Experiments in fourteen datasets across five tasks show
the proposed method outperforms other task-specific methods while being
considerably simple. The proposed method demonstrates the scalability in
different architectures, pre-trained weights, and tasks. The code is available
at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.
</p></li>
</ul>

<h3>Title: Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR. (arXiv:2305.18419v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.18419">http://arxiv.org/abs/2305.18419</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.18419] Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR](http://arxiv.org/abs/2305.18419) #segmentation</code></li>
<li>Summary: <p>We propose a method of segmenting long-form speech by separating semantically
complete sentences within the utterance. This prevents the ASR decoder from
needlessly processing faraway context while also preventing it from missing
relevant context within the current sentence. Semantically complete sentence
boundaries are typically demarcated by punctuation in written text; but
unfortunately, spoken real-world utterances rarely contain punctuation. We
address this limitation by distilling punctuation knowledge from a
bidirectional teacher language model (LM) trained on written, punctuated text.
We compare our segmenter, which is distilled from the LM teacher, against a
segmenter distilled from a acoustic-pause-based teacher used in other works, on
a streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2%
relative WER gain along with a 60 ms median end-of-segment latency reduction on
a YouTube captioning task.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
