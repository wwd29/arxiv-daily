<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: ForensiBlock: A Provenance-Driven Blockchain Framework for Data Forensics and Auditability. (arXiv:2308.03927v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03927">http://arxiv.org/abs/2308.03927</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03927]] ForensiBlock: A Provenance-Driven Blockchain Framework for Data Forensics and Auditability(http://arxiv.org/abs/2308.03927)</code></li>
<li>Summary: <p>Maintaining accurate provenance records is paramount in digital forensics, as
they underpin evidence credibility and integrity, addressing essential aspects
like accountability and reproducibility. Blockchains have several properties
that can address these requirements. Previous systems utilized public
blockchains, i.e., treated blockchain as a black box, and benefiting from the
immutability property. However, the blockchain was accessible to everyone,
giving rise to security concerns and moreover, efficient extraction of
provenance faces challenges due to the enormous scale and complexity of digital
data. This necessitates a tailored blockchain design for digital forensics. Our
solution, Forensiblock has a novel design that automates investigation steps,
ensures secure data access, traces data origins, preserves records, and
expedites provenance extraction. Forensiblock incorporates Role-Based Access
Control with Staged Authorization (RBAC-SA) and a distributed Merkle root for
case tracking. These features support authorized resource access with an
efficient retrieval of provenance records. Particularly, comparing two methods
for extracting provenance records off chain storage retrieval with Merkle root
verification and a brute-force search the offchain method is significantly
better, especially as the blockchain size and number of cases increase. We also
found that our distributed Merkle root creation slightly increases smart
contract processing time but significantly improves history access. Overall, we
show that Forensiblock offers secure, efficient, and reliable handling of
digital forensic data
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Enabling Data Confidentiality with Public Blockchains. (arXiv:2308.03791v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03791">http://arxiv.org/abs/2308.03791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03791]] Enabling Data Confidentiality with Public Blockchains(http://arxiv.org/abs/2308.03791)</code></li>
<li>Summary: <p>Blockchain technology is apt to facilitate the automation of multi-party
cooperations among various players in a decentralized setting, especially in
cases where trust among participants is limited. Transactions are stored in a
ledger, a replica of which is retained by every node of the blockchain network.
The operations saved thereby are thus publicly accessible. While this aspect
enhances transparency, reliability, and persistence, it hinders the utilization
of public blockchains for process automation as it violates typical
confidentiality requirements in corporate settings. To overcome this issue, we
propose our approach named Multi-Authority Approach to Transaction Systems for
Interoperating Applications (MARTSIA). Based on Multi-Authority Attribute-Based
Encryption (MA-ABE), MARTSIA enables read-access control over shared data at
the level of message parts. User-defined policies determine whether an actor
can interpret the publicly stored information or not, depending on the actor's
attributes declared by a consortium of certifiers. Still, all nodes in the
blockchain network can attest to the publication of the (encrypted) data. We
provide a formal analysis of the security guarantees of MARTSIA, and illustrate
the proof-of-concept implementation over multiple blockchain platforms. To
demonstrate its interoperability, we showcase its usage in ensemble with a
state-of-the-art blockchain-based engine for multi-party process execution, and
three real-world decentralized applications in the context of NFT markets,
supply chain, and retail.
</p></li>
</ul>

<h3>Title: SoK: Acoustic Side Channels. (arXiv:2308.03806v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03806">http://arxiv.org/abs/2308.03806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03806]] SoK: Acoustic Side Channels(http://arxiv.org/abs/2308.03806)</code></li>
<li>Summary: <p>We provide a state-of-the-art analysis of acoustic side channels, cover all
the significant academic research in the area, discuss their security
implications and countermeasures, and identify areas for future research. We
also make an attempt to bridge side channels and inverse problems, two fields
that appear to be completely isolated from each other but have deep
connections.
</p></li>
</ul>

<h3>Title: Exploring Security Practices in Infrastructure as Code: An Empirical Study. (arXiv:2308.03952v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03952">http://arxiv.org/abs/2308.03952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03952]] Exploring Security Practices in Infrastructure as Code: An Empirical Study(http://arxiv.org/abs/2308.03952)</code></li>
<li>Summary: <p>Cloud computing has become popular thanks to the widespread use of
Infrastructure as Code (IaC) tools, allowing the community to conveniently
manage and configure cloud infrastructure using scripts. However, the scripting
process itself does not automatically prevent practitioners from introducing
misconfigurations, vulnerabilities, or privacy risks. As a result, ensuring
security relies on practitioners understanding and the adoption of explicit
policies, guidelines, or best practices. In order to understand how
practitioners deal with this problem, in this work, we perform an empirical
study analyzing the adoption of IaC scripted security best practices. First, we
select and categorize widely recognized Terraform security practices
promulgated in the industry for popular cloud providers such as AWS, Azure, and
Google Cloud. Next, we assess the adoption of these practices by each cloud
provider, analyzing a sample of 812 open-source projects hosted on GitHub. For
that, we scan each project configuration files, looking for policy
implementation through static analysis (checkov). Additionally, we investigate
GitHub measures that might be correlated with adopting these best practices.
The category Access policy emerges as the most widely adopted in all providers,
while Encryption in rest are the most neglected policies. Regarding GitHub
measures correlated with best practice adoption, we observe a positive, strong
correlation between a repository number of stars and adopting practices in its
cloud infrastructure. Based on our findings, we provide guidelines for cloud
practitioners to limit infrastructure vulnerability and discuss further aspects
associated with policies that have yet to be extensively embraced within the
industry.
</p></li>
</ul>

<h3>Title: Evil Operation: Breaking Speaker Recognition with PaddingBack. (arXiv:2308.04179v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04179">http://arxiv.org/abs/2308.04179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04179]] Evil Operation: Breaking Speaker Recognition with PaddingBack(http://arxiv.org/abs/2308.04179)</code></li>
<li>Summary: <p>Machine Learning as a Service (MLaaS) has gained popularity due to
advancements in machine learning. However, untrusted third-party platforms have
raised concerns about AI security, particularly in backdoor attacks. Recent
research has shown that speech backdoors can utilize transformations as
triggers, similar to image backdoors. However, human ears easily detect these
transformations, leading to suspicion. In this paper, we introduce PaddingBack,
an inaudible backdoor attack that utilizes malicious operations to make
poisoned samples indistinguishable from clean ones. Instead of using external
perturbations as triggers, we exploit the widely used speech signal operation,
padding, to break speaker recognition systems. Our experimental results
demonstrate the effectiveness of the proposed approach, achieving a
significantly high attack success rate while maintaining a high rate of benign
accuracy. Furthermore, PaddingBack demonstrates the ability to resist defense
methods while maintaining its stealthiness against human perception. The
results of the stealthiness experiment have been made available at
https://nbufabio25.github.io/paddingback/.
</p></li>
</ul>

<h3>Title: Novel Area-Efficient and Flexible Architectures for Optimal Ate Pairing on FPGA. (arXiv:2308.04261v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04261">http://arxiv.org/abs/2308.04261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04261]] Novel Area-Efficient and Flexible Architectures for Optimal Ate Pairing on FPGA(http://arxiv.org/abs/2308.04261)</code></li>
<li>Summary: <p>While FPGA is a suitable platform for implementing cryptographic algorithms,
there are several challenges associated with implementing Optimal Ate pairing
on FPGA, such as security, limited computing resources, and high power
consumption. To overcome these issues, this study introduces three approaches
that can execute the optimal Ate pairing on Barreto-Naehrig curves using
Jacobean coordinates with the goal of reaching 128-bit security on the Genesys
board. The first approach is a pure software implementation utilizing the
MicroBlaze processor. The second involves a combination of software and
hardware, with key operations in $F_{p}$ and $F_{p^{2}}$ being transformed into
IP cores for the MicroBlaze. The third approach builds on the second by
incorporating parallelism to improve the pairing process. The utilization of
multiple MicroBlaze processors within a single system offers both versatility
and parallelism to speed up pairing calculations. A variety of methods and
parameters are used to optimize the pairing computation, including Montgomery
modular multiplication, the Karatsuba method, Jacobean coordinates, the Complex
squaring method, sparse multiplication, squaring in $G_{\phi 6}F_{p^{12}}$, and
the addition chain method. The proposed systems are designed to efficiently
utilize limited resources in restricted environments, while still completing
tasks in a timely manner.
</p></li>
</ul>

<h3>Title: The Vulnerable Nature of Decentralized Governance in DeFi. (arXiv:2308.04267v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04267">http://arxiv.org/abs/2308.04267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04267]] The Vulnerable Nature of Decentralized Governance in DeFi(http://arxiv.org/abs/2308.04267)</code></li>
<li>Summary: <p>Decentralized Finance (DeFi) platforms are often governed by Decentralized
Autonomous Organizations (DAOs) which are implemented via governance protocols.
Governance tokens are distributed to users of the platform, granting them
voting rights in the platform's governance protocol. Many DeFi platforms have
already been subject to attacks resulting in the loss of millions of dollars in
user funds.
</p>
<p>In this paper we show that governance tokens are often not used as intended
and may be harmful to the security of DeFi platforms. We show that (1) users
often do not use governance tokens to vote, (2) that voting rates are
negatively correlated to gas prices, (3) voting is very centralized.
</p>
<p>We explore vulnerabilities in the design of DeFi platform's governance
protocols and analyze different governance attacks, focusing on the
transferable nature of voting rights via governance tokens. Following the
movement and holdings of governance tokens, we show they are often used to
perform a single action and then sold off. We present evidence of DeFi
platforms using other platforms' governance protocols to promote their own
agenda at the expense of the host platform.
</p></li>
</ul>

<h3>Title: XGBD: Explanation-Guided Graph Backdoor Detection. (arXiv:2308.04406v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04406">http://arxiv.org/abs/2308.04406</a></li>
<li>Code URL: https://github.com/guanzihan/gnn_backdoor_detection</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04406]] XGBD: Explanation-Guided Graph Backdoor Detection(http://arxiv.org/abs/2308.04406)</code></li>
<li>Summary: <p>Backdoor attacks pose a significant security risk to graph learning models.
Backdoors can be embedded into the target model by inserting backdoor triggers
into the training dataset, causing the model to make incorrect predictions when
the trigger is present. To counter backdoor attacks, backdoor detection has
been proposed. An emerging detection strategy in the vision and NLP domains is
based on an intriguing phenomenon: when training models on a mixture of
backdoor and clean samples, the loss on backdoor samples drops significantly
faster than on clean samples, allowing backdoor samples to be easily detected
by selecting samples with the lowest loss values. However, the ignorance of
topological feature information on graph data limits its detection
effectiveness when applied directly to the graph domain. To this end, we
propose an explanation-guided backdoor detection method to take advantage of
the topological information. Specifically, we train a helper model on the graph
dataset, feed graph samples into the model, and then adopt explanation methods
to attribute model prediction to an important subgraph. We observe that
backdoor samples have distinct attribution distribution than clean samples, so
the explanatory subgraph could serve as more discriminative features for
detecting backdoor samples. Comprehensive experiments on multiple popular
datasets and attack methods demonstrate the effectiveness and explainability of
our method. Our code is available:
https://github.com/GuanZihan/GNN_backdoor_detection.
</p></li>
</ul>

<h3>Title: Chrisimos: A useful Proof-of-Work for finding Minimal Dominating Set of a graph. (arXiv:2308.04407v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04407">http://arxiv.org/abs/2308.04407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04407]] Chrisimos: A useful Proof-of-Work for finding Minimal Dominating Set of a graph(http://arxiv.org/abs/2308.04407)</code></li>
<li>Summary: <p>Hash-based Proof-of-Work (PoW) used in the Bitcoin Blockchain leads to high
energy consumption and resource wastage. In this paper, we aim to re-purpose
the energy by replacing the hash function with real-life problems having
commercial utility. We propose Chrisimos, a useful Proof-of-Work where miners
are required to find a minimal dominating set for real-life graph instances. A
miner who is able to output the smallest dominating set for the given graph
within the block interval time wins the mining game. We also propose a new
chain selection rule that ensures the security of the scheme. Thus our protocol
also realizes a decentralized minimal dominating set solver for any graph
instance. We provide formal proof of correctness and show via experimental
results that the block interval time is within feasible bounds of hash-based
PoW.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning. (arXiv:2308.03977v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03977">http://arxiv.org/abs/2308.03977</a></li>
<li>Code URL: https://github.com/facebookresearch/pug</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03977]] PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning(http://arxiv.org/abs/2308.03977)</code></li>
<li>Summary: <p>Synthetic image datasets offer unmatched advantages for designing and
evaluating deep neural networks: they make it possible to (i) render as many
data samples as needed, (ii) precisely control each scene and yield granular
ground truth labels (and captions), (iii) precisely control distribution shifts
between training and testing to isolate variables of interest for sound
experimentation. Despite such promise, the use of synthetic image data is still
limited -- and often played down -- mainly due to their lack of realism. Most
works therefore rely on datasets of real images, which have often been scraped
from public images on the internet, and may have issues with regards to
privacy, bias, and copyright, while offering little control over how objects
precisely appear. In this work, we present a path to democratize the use of
photorealistic synthetic data: we develop a new generation of interactive
environments for representation learning research, that offer both
controllability and realism. We use the Unreal Engine, a powerful game engine
well known in the entertainment industry, to produce PUG (Photorealistic Unreal
Graphics) environments and datasets for representation learning. In this paper,
we demonstrate the potential of PUG to enable more rigorous evaluations of
vision models.
</p></li>
</ul>

<h3>Title: Person Re-Identification without Identification via Event Anonymization. (arXiv:2308.04402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04402">http://arxiv.org/abs/2308.04402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04402]] Person Re-Identification without Identification via Event Anonymization(http://arxiv.org/abs/2308.04402)</code></li>
<li>Summary: <p>Wide-scale use of visual surveillance in public spaces puts individual
privacy at stake while increasing resource consumption (energy, bandwidth, and
computation). Neuromorphic vision sensors (event-cameras) have been recently
considered a valid solution to the privacy issue because they do not capture
detailed RGB visual information of the subjects in the scene. However, recent
deep learning architectures have been able to reconstruct images from event
cameras with high fidelity, reintroducing a potential threat to privacy for
event-based vision applications. In this paper, we aim to anonymize
event-streams to protect the identity of human subjects against such image
reconstruction attacks. To achieve this, we propose an end-to-end network
architecture jointly optimized for the twofold objective of preserving privacy
and performing a downstream task such as person ReId. Our network learns to
scramble events, enforcing the degradation of images recovered from the privacy
attacker. In this work, we also bring to the community the first ever
event-based person ReId dataset gathered to evaluate the performance of our
approach. We validate our approach with extensive experiments and report
results on the synthetic event data simulated from the publicly available
SoftBio dataset and our proposed Event-ReId dataset.
</p></li>
</ul>

<h3>Title: The Still Secret Ballot: The Limited Privacy Cost of Transparent Election Results. (arXiv:2308.04100v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04100">http://arxiv.org/abs/2308.04100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04100]] The Still Secret Ballot: The Limited Privacy Cost of Transparent Election Results(http://arxiv.org/abs/2308.04100)</code></li>
<li>Summary: <p>After an election, should election officials release an electronic record of
each ballot? The release of such cast vote records could bolster the legitimacy
of the certified result. But it may also facilitate vote revelation, where an
analyst unravels the secret ballot by uniquely linking vote choices on the
anonymous ballot to the voter's name and address in the public voter file. We
provide the first empirical study of the extent of vote revelation under
several possible election-reporting regimes, ranging from precinct-level
results to the individual ballot records known as cast vote records. Using
Maricopa County, Arizona, as a case study, we find that cast vote records could
reveal less than 0.2% of any voters' choices in the 2020 general election.
Perhaps counterintuitively, releasing cast vote records coded by precinct and
vote method are no more revelatory than releasing aggregate vote tallies for
each precinct and vote method. We conclude that cast vote records are
sufficiently privacy-protecting, and suggest how the privacy violations that do
remain could be reduced.
</p></li>
</ul>

<h3>Title: Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage. (arXiv:2308.04341v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04341">http://arxiv.org/abs/2308.04341</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04341]] Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage(http://arxiv.org/abs/2308.04341)</code></li>
<li>Summary: <p>Machine learning models are increasingly utilized across impactful domains to
predict individual outcomes. As such, many models provide algorithmic recourse
to individuals who receive negative outcomes. However, recourse can be
leveraged by adversaries to disclose private information. This work presents
the first attempt at mitigating such attacks. We present two novel methods to
generate differentially private recourse: Differentially Private Model (DPM)
and Laplace Recourse (LR). Using logistic regression classifiers and real world
and synthetic datasets, we find that DPM and LR perform well in reducing what
an adversary can infer, especially at low FPR. When training dataset size is
large enough, we find particular success in preventing privacy leakage while
maintaining model and recourse accuracy with our novel LR method.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Eye-Shield: Real-Time Protection of Mobile Device Screen Information from Shoulder Surfing. (arXiv:2308.03868v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03868">http://arxiv.org/abs/2308.03868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03868]] Eye-Shield: Real-Time Protection of Mobile Device Screen Information from Shoulder Surfing(http://arxiv.org/abs/2308.03868)</code></li>
<li>Summary: <p>People use mobile devices ubiquitously for computing, communication, storage,
web browsing, and more. As a result, the information accessed and stored within
mobile devices, such as financial and health information, text messages, and
emails, can often be sensitive. Despite this, people frequently use their
mobile devices in public areas, becoming susceptible to a simple yet effective
attack, shoulder surfing. Shoulder surfing occurs when a person near a mobile
user peeks at the user's mobile device, potentially acquiring passcodes, PINs,
browsing behavior, or other personal information. We propose Eye-Shield, a
solution to prevent shoulder surfers from accessing or stealing sensitive
on-screen information. Eye-Shield is designed to protect all types of on-screen
information in real time, without any serious impediment to users' interactions
with their mobile devices. Eye-Shield generates images that appear readable at
close distances, but appear blurry or pixelated at farther distances and wider
angles. It is capable of protecting on-screen information from shoulder
surfers, operating in real time, and being minimally intrusive to the intended
users. Eye-Shield protects images and text from shoulder surfers by reducing
recognition rates to 24.24% and 15.91%. Our implementations of Eye-Shield, with
frame rates of 24 FPS for Android and 43 FPS for iOS, effectively work on
screen resolutions as high as 1440x3088. Eye-Shield also incurs acceptable
memory usage, CPU utilization, and energy overhead. Finally, our MTurk and
in-person user studies indicate that Eye-Shield protects on-screen information
without a large usability cost for privacy-conscious users.
</p></li>
</ul>

<h3>Title: Caching-based Multicast Message Authentication in Time-critical Industrial Control Systems. (arXiv:2308.04034v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04034">http://arxiv.org/abs/2308.04034</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04034]] Caching-based Multicast Message Authentication in Time-critical Industrial Control Systems(http://arxiv.org/abs/2308.04034)</code></li>
<li>Summary: <p>Attacks against industrial control systems (ICSs) often exploit the
insufficiency of authentication mechanisms. Verifying whether the received
messages are intact and issued by legitimate sources can prevent malicious
data/command injection by illegitimate or compromised devices. However, the key
challenge is to introduce message authentication for various ICS communication
models, including multicast or broadcast, with a messaging rate that can be as
high as thousands of messages per second, within very stringent latency
constraints. For example, certain commands for protection in smart grids must
be delivered within 2 milliseconds, ruling out public-key cryptography. This
paper proposes two lightweight message authentication schemes, named CMA and
its multicast variant CMMA, that perform precomputation and caching to
authenticate future messages. With minimal precomputation and communication
overhead, C(M)MA eliminates all cryptographic operations for the source after
the message is given, and all expensive cryptographic operations for the
destinations after the message is received. C(M)MA considers the urgency
profile (or likelihood) of a set of future messages for even faster
verification of the most time-critical (or likely) messages. We demonstrate the
feasibility of C(M)MA in an ICS setting based on a substation automation system
in smart grids.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models. (arXiv:2308.03906v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03906">http://arxiv.org/abs/2308.03906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03906]] TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models(http://arxiv.org/abs/2308.03906)</code></li>
<li>Summary: <p>We present a Multimodal Backdoor Defense technique TIJO (Trigger Inversion
using Joint Optimization). Recent work <a href="http://export.arxiv.org/abs/2112.07668">arXiv:2112.07668</a> has demonstrated
successful backdoor attacks on multimodal models for the Visual Question
Answering task. Their dual-key backdoor trigger is split across two modalities
(image and text), such that the backdoor is activated if and only if the
trigger is present in both modalities. We propose TIJO that defends against
dual-key attacks through a joint optimization that reverse-engineers the
trigger in both the image and text modalities. This joint optimization is
challenging in multimodal models due to the disconnected nature of the visual
pipeline which consists of an offline feature extractor, whose output is then
fused with the text using a fusion module. The key insight enabling the joint
optimization in TIJO is that the trigger inversion needs to be carried out in
the object detection box feature space as opposed to the pixel space. We
demonstrate the effectiveness of our method on the TrojVQA benchmark, where
TIJO improves upon the state-of-the-art unimodal methods from an AUC of 0.6 to
0.92 on multimodal dual-key backdoors. Furthermore, our method also improves
upon the unimodal baselines on unimodal backdoors. We present ablation studies
and qualitative results to provide insights into our algorithm such as the
critical importance of overlaying the inverted feature triggers on all visual
features during trigger inversion. The prototype implementation of TIJO is
available at https://github.com/SRI-CSL/TIJO.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels. (arXiv:2308.03792v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03792">http://arxiv.org/abs/2308.03792</a></li>
<li>Code URL: https://github.com/stanislavfort/multi-attacks</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03792]] Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels(http://arxiv.org/abs/2308.03792)</code></li>
<li>Summary: <p>We show that we can easily design a single adversarial perturbation $P$ that
changes the class of $n$ images $X_1,X_2,\dots,X_n$ from their original,
unperturbed classes $c_1, c_2,\dots,c_n$ to desired (not necessarily all the
same) classes $c^*_1,c^*_2,\dots,c^*_n$ for up to hundreds of images and target
classes at once. We call these \textit{multi-attacks}. Characterizing the
maximum $n$ we can achieve under different conditions such as image resolution,
we estimate the number of regions of high class confidence around a particular
image in the space of pixels to be around $10^{\mathcal{O}(100)}$, posing a
significant problem for exhaustive defense strategies. We show several
immediate consequences of this: adversarial attacks that change the resulting
class based on their intensity, and scale-independent adversarial examples. To
demonstrate the redundancy and richness of class decision boundaries in the
pixel space, we look for its two-dimensional sections that trace images and
spell words using particular classes. We also show that ensembling reduces
susceptibility to multi-attacks, and that classifiers trained on random labels
are more susceptible. Our code is available on GitHub.
</p></li>
</ul>

<h3>Title: PAIF: Perception-Aware Infrared-Visible Image Fusion for Attack-Tolerant Semantic Segmentation. (arXiv:2308.03979v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03979">http://arxiv.org/abs/2308.03979</a></li>
<li>Code URL: https://github.com/liuzhu-cv/paif</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03979]] PAIF: Perception-Aware Infrared-Visible Image Fusion for Attack-Tolerant Semantic Segmentation(http://arxiv.org/abs/2308.03979)</code></li>
<li>Summary: <p>Infrared and visible image fusion is a powerful technique that combines
complementary information from different modalities for downstream semantic
perception tasks. Existing learning-based methods show remarkable performance,
but are suffering from the inherent vulnerability of adversarial attacks,
causing a significant decrease in accuracy. In this work, a perception-aware
fusion framework is proposed to promote segmentation robustness in adversarial
scenes. We first conduct systematic analyses about the components of image
fusion, investigating the correlation with segmentation robustness under
adversarial perturbations. Based on these analyses, we propose a harmonized
architecture search with a decomposition-based structure to balance standard
accuracy and robustness. We also propose an adaptive learning strategy to
improve the parameter robustness of image fusion, which can learn effective
feature extraction under diverse adversarial perturbations. Thus, the goals of
image fusion (\textit{i.e.,} extracting complementary features from source
modalities and defending attack) can be realized from the perspectives of
architectural and learning strategies. Extensive experimental results
demonstrate that our scheme substantially enhances the robustness, with gains
of 15.3% mIOU of segmentation in the adversarial scene, compared with advanced
competitors. The source codes are available at
https://github.com/LiuZhu-CV/PAIF.
</p></li>
</ul>

<h3>Title: How Generalizable are Deepfake Detectors? An Empirical Study. (arXiv:2308.04177v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04177">http://arxiv.org/abs/2308.04177</a></li>
<li>Code URL: https://github.com/boutiquelee/deepfakeempiricalstudy</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04177]] How Generalizable are Deepfake Detectors? An Empirical Study(http://arxiv.org/abs/2308.04177)</code></li>
<li>Summary: <p>Deepfake videos and images are becoming increasingly credible, posing a
significant threat given their potential to facilitate fraud or bypass access
control systems. This has motivated the development of deepfake detection
methods, in which deep learning models are trained to distinguish between real
and synthesized footage. Unfortunately, existing detection models struggle to
generalize to deepfakes from datasets they were not trained on, but little work
has been done to examine why or how this limitation can be addressed. In this
paper, we present the first empirical study on the generalizability of deepfake
detectors, an essential goal for detectors to stay one step ahead of attackers.
Our study utilizes six deepfake datasets, five deepfake detection methods, and
two model augmentation approaches, confirming that detectors do not generalize
in zero-shot settings. Additionally, we find that detectors are learning
unwanted properties specific to synthesis methods and struggling to extract
discriminative features, limiting their ability to generalize. Finally, we find
that there are neurons universally contributing to detection across seen and
unseen datasets, illuminating a possible path forward to zero-shot
generalizability.
</p></li>
</ul>

<h3>Title: Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning. (arXiv:2308.04373v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04373">http://arxiv.org/abs/2308.04373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04373]] Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning(http://arxiv.org/abs/2308.04373)</code></li>
<li>Summary: <p>The main premise of federated learning is that machine learning model updates
are computed locally, in particular to preserve user data privacy, as those
never leave the perimeter of their device. This mechanism supposes the general
model, once aggregated, to be broadcast to collaborating and non malicious
nodes. However, without proper defenses, compromised clients can easily probe
the model inside their local memory in search of adversarial examples. For
instance, considering image-based applications, adversarial examples consist of
imperceptibly perturbed images (to the human eye) misclassified by the local
model, which can be later presented to a victim node's counterpart model to
replicate the attack. To mitigate such malicious probing, we introduce Pelta, a
novel shielding mechanism leveraging trusted hardware. By harnessing the
capabilities of Trusted Execution Environments (TEEs), Pelta masks part of the
back-propagation chain rule, otherwise typically exploited by attackers for the
design of malicious samples. We evaluate Pelta on a state of the art ensemble
model and demonstrate its effectiveness against the Self Attention Gradient
adversarial Attack.
</p></li>
</ul>

<h3>Title: Improving Performance of Semi-Supervised Learning by Adversarial Attacks. (arXiv:2308.04018v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04018">http://arxiv.org/abs/2308.04018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04018]] Improving Performance of Semi-Supervised Learning by Adversarial Attacks(http://arxiv.org/abs/2308.04018)</code></li>
<li>Summary: <p>Semi-supervised learning (SSL) algorithm is a setup built upon a realistic
assumption that access to a large amount of labeled data is tough. In this
study, we present a generalized framework, named SCAR, standing for Selecting
Clean samples with Adversarial Robustness, for improving the performance of
recent SSL algorithms. By adversarially attacking pre-trained models with
semi-supervision, our framework shows substantial advances in classifying
images. We introduce how adversarial attacks successfully select high-confident
unlabeled data to be labeled with current predictions. On CIFAR10, three recent
SSL algorithms with SCAR result in significantly improved image classification.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: GeoTransformer: Fast and Robust Point Cloud Registration with Geometric Transformer. (arXiv:2308.03768v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03768">http://arxiv.org/abs/2308.03768</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03768]] GeoTransformer: Fast and Robust Point Cloud Registration with Geometric Transformer(http://arxiv.org/abs/2308.03768)</code></li>
<li>Summary: <p>We study the problem of extracting accurate correspondences for point cloud
registration. Recent keypoint-free methods have shown great potential through
bypassing the detection of repeatable keypoints which is difficult to do
especially in low-overlap scenarios. They seek correspondences over downsampled
superpoints, which are then propagated to dense points. Superpoints are matched
based on whether their neighboring patches overlap. Such sparse and loose
matching requires contextual features capturing the geometric structure of the
point clouds. We propose Geometric Transformer, or GeoTransformer for short, to
learn geometric feature for robust superpoint matching. It encodes pair-wise
distances and triplet-wise angles, making it invariant to rigid transformation
and robust in low-overlap cases. The simplistic design attains surprisingly
high matching accuracy such that no RANSAC is required in the estimation of
alignment transformation, leading to $100$ times acceleration. Extensive
experiments on rich benchmarks encompassing indoor, outdoor, synthetic,
multiway and non-rigid demonstrate the efficacy of GeoTransformer. Notably, our
method improves the inlier ratio by $18{\sim}31$ percentage points and the
registration recall by over $7$ points on the challenging 3DLoMatch benchmark.
Our code and models are available at
\url{https://github.com/qinzheng93/GeoTransformer}.
</p></li>
</ul>

<h3>Title: Distributionally Robust Classification on a Data Budget. (arXiv:2308.03821v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03821">http://arxiv.org/abs/2308.03821</a></li>
<li>Code URL: https://github.com/penfever/vlhub</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03821]] Distributionally Robust Classification on a Data Budget(http://arxiv.org/abs/2308.03821)</code></li>
<li>Summary: <p>Real world uses of deep learning require predictable model behavior under
distribution shifts. Models such as CLIP show emergent natural distributional
robustness comparable to humans, but may require hundreds of millions of
training samples. Can we train robust learners in a domain where data is
limited? To rigorously address this question, we introduce JANuS (Joint
Annotations and Names Set), a collection of four new training datasets with
images, labels, and corresponding captions, and perform a series of carefully
controlled investigations of factors contributing to robustness in image
classification, then compare those results to findings derived from a
large-scale meta-analysis. Using this approach, we show that standard ResNet-50
trained with the cross-entropy loss on 2.4 million image samples can attain
comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To
our knowledge, this is the first result showing (near) state-of-the-art
distributional robustness on limited data budgets. Our dataset is available at
\url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used
to reproduce our experiments can be found at
\url{https://github.com/penfever/vlhub/}.
</p></li>
</ul>

<h3>Title: From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal. (arXiv:2308.03867v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03867">http://arxiv.org/abs/2308.03867</a></li>
<li>Code URL: https://github.com/yunguo224/lhp-rain</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03867]] From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal(http://arxiv.org/abs/2308.03867)</code></li>
<li>Summary: <p>Learning-based image deraining methods have made great progress. However, the
lack of large-scale high-quality paired training samples is the main bottleneck
to hamper the real image deraining (RID). To address this dilemma and advance
RID, we construct a Large-scale High-quality Paired real rain benchmark
(LHP-Rain), including 3000 video sequences with 1 million high-resolution
(1920*1080) frame pairs. The advantages of the proposed dataset over the
existing ones are three-fold: rain with higher-diversity and larger-scale,
image with higher-resolution and higher-quality ground-truth. Specifically, the
real rains in LHP-Rain not only contain the classical rain
streak/veiling/occlusion in the sky, but also the \textbf{splashing on the
ground} overlooked by deraining community. Moreover, we propose a novel robust
low-rank tensor recovery model to generate the GT with better separating the
static background from the dynamic rain. In addition, we design a simple
transformer-based single image deraining baseline, which simultaneously utilize
the self-attention and cross-layer attention within the image and rain layer
with discriminative feature representation. Extensive experiments verify the
superiority of the proposed dataset and deraining method over state-of-the-art.
</p></li>
</ul>

<h3>Title: ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals. (arXiv:2308.03936v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03936">http://arxiv.org/abs/2308.03936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03936]] ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals(http://arxiv.org/abs/2308.03936)</code></li>
<li>Summary: <p>We propose an exhaustive methodology that leverages all levels of feature
abstraction, targeting an enhancement in the generalizability of image
classification to unobserved hospitals. Our approach incorporates
augmentation-based self-supervision with common distribution shifts in
histopathology scenarios serving as the pretext task. This enables us to derive
invariant features from training images without relying on training labels,
thereby covering different abstraction levels. Moving onto the subsequent
abstraction level, we employ a domain alignment module to facilitate further
extraction of invariant features across varying training hospitals. To
represent the highly specific features of participating hospitals, an encoder
is trained to classify hospital labels, independent of their diagnostic labels.
The features from each of these encoders are subsequently disentangled to
minimize redundancy and segregate the features. This representation, which
spans a broad spectrum of semantic information, enables the development of a
model demonstrating increased robustness to unseen images from disparate
distributions. Experimental results from the PACS dataset (a domain
generalization benchmark), a synthetic dataset created by applying
histopathology-specific jitters to the MHIST dataset (defining different
domains with varied distribution shifts), and a Renal Cell Carcinoma dataset
derived from four image repositories from TCGA, collectively indicate that our
proposed model is adept at managing varying levels of image granularity. Thus,
it shows improved generalizability when faced with new, out-of-distribution
hospital images.
</p></li>
</ul>

<h3>Title: PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection. (arXiv:2308.03982v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03982">http://arxiv.org/abs/2308.03982</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03982]] PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection(http://arxiv.org/abs/2308.03982)</code></li>
<li>Summary: <p>Recently, polar-based representation has shown promising properties in
perceptual tasks. In addition to Cartesian-based approaches, which separate
point clouds unevenly, representing point clouds as polar grids has been
recognized as an alternative due to (1) its advantage in robust performance
under different resolutions and (2) its superiority in streaming-based
approaches. However, state-of-the-art polar-based detection methods inevitably
suffer from the feature distortion problem because of the non-uniform division
of polar representation, resulting in a non-negligible performance gap compared
to Cartesian-based approaches. To tackle this issue, we present PARTNER, a
novel 3D object detector in the polar coordinate. PARTNER alleviates the
dilemma of feature distortion with global representation re-alignment and
facilitates the regression by introducing instance-level geometric information
into the detection head. Extensive experiments show overwhelming advantages in
streaming-based detection and different resolutions. Furthermore, our method
outperforms the previous polar-based works with remarkable margins of 3.68% and
9.15% on Waymo and ONCE validation set, thus achieving competitive results over
the state-of-the-art methods.
</p></li>
</ul>

<h3>Title: OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation. (arXiv:2308.04126v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04126">http://arxiv.org/abs/2308.04126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04126]] OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation(http://arxiv.org/abs/2308.04126)</code></li>
<li>Summary: <p>This paper presents OmniDataComposer, an innovative approach for multimodal
data fusion and unlimited data generation with an intent to refine and
uncomplicate interplay among diverse data modalities. Coming to the core
breakthrough, it introduces a cohesive data structure proficient in processing
and merging multimodal data inputs, which include video, audio, and text. Our
crafted algorithm leverages advancements across multiple operations such as
video/image caption extraction, dense caption extraction, Automatic Speech
Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything
Model(RAM), and object tracking. OmniDataComposer is capable of identifying
over 6400 categories of objects, substantially broadening the spectrum of
visual information. It amalgamates these diverse modalities, promoting
reciprocal enhancement among modalities and facilitating cross-modal data
correction. \textbf{The final output metamorphoses each video input into an
elaborate sequential document}, virtually transmuting videos into thorough
narratives, making them easier to be processed by large language models. Future
prospects include optimizing datasets for each modality to encourage unlimited
data generation. This robust base will offer priceless insights to models like
ChatGPT, enabling them to create higher quality datasets for video captioning
and easing question-answering tasks based on video content. OmniDataComposer
inaugurates a new stage in multimodal learning, imparting enormous potential
for augmenting AI's understanding and generation of complex, real-world data.
</p></li>
</ul>

<h3>Title: Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness. (arXiv:2308.04137v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04137">http://arxiv.org/abs/2308.04137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04137]] Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness(http://arxiv.org/abs/2308.04137)</code></li>
<li>Summary: <p>Reliable and robust evaluation methods are a necessary first step towards
developing machine learning models that are themselves robust and reliable.
Unfortunately, current evaluation protocols typically used to assess
classifiers fail to comprehensively evaluate performance as they tend to rely
on limited types of test data, and ignore others. For example, using the
standard test data fails to evaluate the predictions made by the classifier to
samples from classes it was not trained on. On the other hand, testing with
data containing samples from unknown classes fails to evaluate how well the
classifier can predict the labels for known classes. This article advocates
bench-marking performance using a wide range of different types of data and
using a single metric that can be applied to all such data types to produce a
consistent evaluation of performance. Using such a benchmark it is found that
current deep neural networks, including those trained with methods that are
believed to produce state-of-the-art robustness, are extremely vulnerable to
making mistakes on certain types of data. This means that such models will be
unreliable in real-world scenarios where they may encounter data from many
different domains, and that they are insecure as they can easily be fooled into
making the wrong decisions. It is hoped that these results will motivate the
wider adoption of more comprehensive testing methods that will, in turn, lead
to the development of more robust machine learning methods in the future.
</p>
<p>Code is available at:
\url{https://codeberg.org/mwspratling/RobustnessEvaluation}
</p></li>
</ul>

<h3>Title: Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention. (arXiv:2308.04156v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04156">http://arxiv.org/abs/2308.04156</a></li>
<li>Code URL: https://github.com/fanning-zhang/satnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04156]] Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention(http://arxiv.org/abs/2308.04156)</code></li>
<li>Summary: <p>Stereoscopic image quality assessment (SIQA) plays a crucial role in
evaluating and improving the visual experience of 3D content. Existing
binocular properties and attention-based methods for SIQA have achieved
promising performance. However, these bottom-up approaches are inadequate in
exploiting the inherent characteristics of the human visual system (HVS). This
paper presents a novel network for SIQA via stereo attention, employing a
top-down perspective to guide the quality assessment process. Our proposed
method realizes the guidance from high-level binocular signals down to
low-level monocular signals, while the binocular and monocular information can
be calibrated progressively throughout the processing pipeline. We design a
generalized Stereo AttenTion (SAT) block to implement the top-down philosophy
in stereo perception. This block utilizes the fusion-generated attention map as
a high-level binocular modulator, influencing the representation of two
low-level monocular features. Additionally, we introduce an Energy Coefficient
(EC) to account for recent findings indicating that binocular responses in the
primate primary visual cortex are less than the sum of monocular responses. The
adaptive EC can tune the magnitude of binocular response flexibly, thus
enhancing the formation of robust binocular features within our framework. To
extract the most discriminative quality information from the summation and
subtraction of the two branches of monocular features, we utilize a
dual-pooling strategy that applies min-pooling and max-pooling operations to
the respective branches. Experimental results highlight the superiority of our
top-down method in simulating the property of visual perception and advancing
the state-of-the-art in the SIQA field. The code of this work is available at
https://github.com/Fanning-Zhang/SATNet.
</p></li>
</ul>

<h3>Title: Robust retrieval of material chemical states in X-ray microspectroscopy. (arXiv:2308.04207v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04207">http://arxiv.org/abs/2308.04207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04207]] Robust retrieval of material chemical states in X-ray microspectroscopy(http://arxiv.org/abs/2308.04207)</code></li>
<li>Summary: <p>X-ray microspectroscopic techniques are essential for studying morphological
and chemical changes in materials, providing high-resolution structural and
spectroscopic information. However, its practical data analysis for reliably
retrieving the chemical states remains a major obstacle to accelerating the
fundamental understanding of materials in many research fields. In this work,
we propose a novel data formulation model for X-ray microspectroscopy and
develop a dedicated unmixing framework to solve this problem, which is robust
to noise and spectral variability. Moreover, this framework is not limited to
the analysis of two-state material chemistry, making it an effective
alternative to conventional and widely-used methods. In addition, an
alternative directional multiplier method with provable convergence is applied
to obtain the solution efficiently. Our framework can accurately identify and
characterize chemical states in complex and heterogeneous samples, even under
challenging conditions such as low signal-to-noise ratios and overlapping
spectral features. Extensive experimental results on simulated and real
datasets demonstrate its effectiveness and reliability.
</p></li>
</ul>

<h3>Title: Will your Doorbell Camera still recognize you as you grow old. (arXiv:2308.04224v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04224">http://arxiv.org/abs/2308.04224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04224]] Will your Doorbell Camera still recognize you as you grow old(http://arxiv.org/abs/2308.04224)</code></li>
<li>Summary: <p>Robust authentication for low-power consumer devices such as doorbell cameras
poses a valuable and unique challenge. This work explores the effect of age and
aging on the performance of facial authentication methods. Two public age
datasets, AgeDB and Morph-II have been used as baselines in this work. A
photo-realistic age transformation method has been employed to augment a set of
high-quality facial images with various age effects. Then the effect of these
synthetic aging data on the high-performance deep-learning-based face
recognition model is quantified by using various metrics including Receiver
Operating Characteristic (ROC) curves and match score distributions.
Experimental results demonstrate that long-term age effects are still a
significant challenge for the state-of-the-art facial authentication method.
</p></li>
</ul>

<h3>Title: When Super-Resolution Meets Camouflaged Object Detection: A Comparison Study. (arXiv:2308.04370v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04370">http://arxiv.org/abs/2308.04370</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04370]] When Super-Resolution Meets Camouflaged Object Detection: A Comparison Study(http://arxiv.org/abs/2308.04370)</code></li>
<li>Summary: <p>Super Resolution (SR) and Camouflaged Object Detection (COD) are two hot
topics in computer vision with various joint applications. For instance,
low-resolution surveillance images can be successively processed by
super-resolution techniques and camouflaged object detection. However, in
previous work, these two areas are always studied in isolation. In this paper,
we, for the first time, conduct an integrated comparative evaluation for both.
Specifically, we benchmark different super-resolution methods on commonly used
COD datasets, and meanwhile, we evaluate the robustness of different COD models
by using COD data processed by SR methods. Our goal is to bridge these two
domains, discover novel experimental phenomena, summarize new experim.
</p></li>
</ul>

<h3>Title: Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach. (arXiv:2308.03800v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03800">http://arxiv.org/abs/2308.03800</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03800]] Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach(http://arxiv.org/abs/2308.03800)</code></li>
<li>Summary: <p>In this report, I present a deep learning approach to conduct a natural
language processing (hereafter NLP) binary classification task for analyzing
financial-fraud texts. First, I searched for regulatory announcements and
enforcement bulletins from HKEX news to define fraudulent companies and to
extract their MD&amp;A reports before I organized the sentences from the reports
with labels and reporting time. My methodology involved different kinds of
neural network models, including Multilayer Perceptrons with Embedding layers,
vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and
Gated Recurrent Unit (GRU) for the text classification task. By utilizing this
diverse set of models, I aim to perform a comprehensive comparison of their
accuracy in detecting financial fraud. My results bring significant
implications for financial fraud detection as this work contributes to the
growing body of research at the intersection of deep learning, NLP, and
finance, providing valuable insights for industry practitioners, regulators,
and researchers in the pursuit of more robust and effective fraud detection
methodologies.
</p></li>
</ul>

<h3>Title: On Monotonic Aggregation for Open-domain QA. (arXiv:2308.04176v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04176">http://arxiv.org/abs/2308.04176</a></li>
<li>Code URL: https://github.com/yeonseokjeong/judge-specialist</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04176]] On Monotonic Aggregation for Open-domain QA(http://arxiv.org/abs/2308.04176)</code></li>
<li>Summary: <p>Question answering (QA) is a critical task for speech-based retrieval from
knowledge sources, by sifting only the answers without requiring to read
supporting documents. Specifically, open-domain QA aims to answer user
questions on unrestricted knowledge sources. Ideally, adding a source should
not decrease the accuracy, but we find this property (denoted as
"monotonicity") does not hold for current state-of-the-art methods. We identify
the cause, and based on that we propose Judge-Specialist framework. Our
framework consists of (1) specialist retrievers/readers to cover individual
sources, and (2) judge, a dedicated language model to select the final answer.
Our experiments show that our framework not only ensures monotonicity, but also
outperforms state-of-the-art multi-source QA methods on Natural Questions.
Additionally, we show that our models robustly preserve the monotonicity
against noise from speech recognition. We publicly release our code and
setting.
</p></li>
</ul>

<h3>Title: Applications of Machine Learning to Modelling and Analysing Dynamical Systems. (arXiv:2308.03763v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03763">http://arxiv.org/abs/2308.03763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03763]] Applications of Machine Learning to Modelling and Analysing Dynamical Systems(http://arxiv.org/abs/2308.03763)</code></li>
<li>Summary: <p>We explore the use of Physics Informed Neural Networks to analyse nonlinear
Hamiltonian Dynamical Systems with a first integral of motion. In this work, we
propose an architecture which combines existing Hamiltonian Neural Network
structures into Adaptable Symplectic Recurrent Neural Networks which preserve
Hamilton's equations as well as the symplectic structure of phase space while
predicting dynamics for the entire parameter space. This architecture is found
to significantly outperform previously proposed neural networks when predicting
Hamiltonian dynamics especially in potentials which contain multiple
parameters. We demonstrate its robustness using the nonlinear Henon-Heiles
potential under chaotic, quasiperiodic and periodic conditions.
</p>
<p>The second problem we tackle is whether we can use the high dimensional
nonlinear capabilities of neural networks to predict the dynamics of a
Hamiltonian system given only partial information of the same. Hence we attempt
to take advantage of Long Short Term Memory networks to implement Takens'
embedding theorem and construct a delay embedding of the system followed by
mapping the topologically invariant attractor to the true form. This
architecture is then layered with Adaptable Symplectic nets to allow for
predictions which preserve the structure of Hamilton's equations. We show that
this method works efficiently for single parameter potentials and provides
accurate predictions even over long periods of time.
</p></li>
</ul>

<h3>Title: Fixed Inter-Neuron Covariability Induces Adversarial Robustness. (arXiv:2308.03956v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03956">http://arxiv.org/abs/2308.03956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03956]] Fixed Inter-Neuron Covariability Induces Adversarial Robustness(http://arxiv.org/abs/2308.03956)</code></li>
<li>Summary: <p>The vulnerability to adversarial perturbations is a major flaw of Deep Neural
Networks (DNNs) that raises question about their reliability when in real-world
scenarios. On the other hand, human perception, which DNNs are supposed to
emulate, is highly robust to such perturbations, indicating that there may be
certain features of the human perception that make it robust but are not
represented in the current class of DNNs. One such feature is that the activity
of biological neurons is correlated and the structure of this correlation tends
to be rather rigid over long spans of times, even if it hampers performance and
learning. We hypothesize that integrating such constraints on the activations
of a DNN would improve its adversarial robustness, and, to test this
hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which
comprises of neurons whose activations are consistent with each other, as they
conform to a fixed, but learned, covariability pattern. When evaluated on image
and sound recognition tasks, the models with a SCA layer achieved high
accuracy, and exhibited significantly greater robustness than multi-layer
perceptron models to state-of-the-art Auto-PGD adversarial attacks
\textit{without being trained on adversarially perturbed data
</p></li>
</ul>

<h3>Title: Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation. (arXiv:2308.04061v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04061">http://arxiv.org/abs/2308.04061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04061]] Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation(http://arxiv.org/abs/2308.04061)</code></li>
<li>Summary: <p>Adversarial robustness is a research area that has recently received a lot of
attention in the quest for trustworthy artificial intelligence. However, recent
works on adversarial robustness have focused on supervised learning where it is
assumed that labeled data is plentiful. In this paper, we investigate
semi-supervised adversarial training where labeled data is scarce. We derive
two upper bounds for the robust risk and propose a regularization term for
unlabeled data motivated by these two upper bounds. Then, we develop a
semi-supervised adversarial training algorithm that combines the proposed
regularization term with knowledge distillation using a semi-supervised teacher
(i.e., a teacher model trained using a semi-supervised learning algorithm). Our
experiments show that our proposed algorithm achieves state-of-the-art
performance with significant margins compared to existing algorithms. In
particular, compared to supervised learning algorithms, performance of our
proposed algorithm is not much worse even when the amount of labeled data is
very small. For example, our algorithm with only 8\% labeled data is comparable
to supervised adversarial training algorithms that use all labeled data, both
in terms of standard and robust accuracies on CIFAR-10.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: EFaR 2023: Efficient Face Recognition Competition. (arXiv:2308.04168v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04168">http://arxiv.org/abs/2308.04168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04168]] EFaR 2023: Efficient Face Recognition Competition(http://arxiv.org/abs/2308.04168)</code></li>
<li>Summary: <p>This paper presents the summary of the Efficient Face Recognition Competition
(EFaR) held at the 2023 International Joint Conference on Biometrics (IJCB
2023). The competition received 17 submissions from 6 different teams. To drive
further development of efficient face recognition models, the submitted
solutions are ranked based on a weighted score of the achieved verification
accuracies on a diverse set of benchmarks, as well as the deployability given
by the number of floating-point operations and model size. The evaluation of
submissions is extended to bias, cross-quality, and large-scale recognition
benchmarks. Overall, the paper gives an overview of the achieved performance
values of the submitted solutions as well as a diverse set of baselines. The
submitted solutions use small, efficient network architectures to reduce the
computational cost, some solutions apply model quantization. An outlook on
possible techniques that are underrepresented in current solutions is given as
well.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake Extraction from Remote Sensing Imagery. (arXiv:2308.04397v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04397">http://arxiv.org/abs/2308.04397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04397]] LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake Extraction from Remote Sensing Imagery(http://arxiv.org/abs/2308.04397)</code></li>
<li>Summary: <p>Lake extraction from remote sensing imagery is challenging due to the complex
shapes of lakes and the presence of noise. Existing methods suffer from blurred
segmentation boundaries and poor foreground modeling. In this paper, we propose
a hybrid CNN-Transformer architecture, called LEFormer, for accurate lake
extraction. LEFormer contains four main modules: CNN encoder, Transformer
encoder, cross-encoder fusion, and lightweight decoder. The CNN encoder
recovers local spatial information and improves fine-scale details.
Simultaneously, the Transformer encoder captures long-range dependencies
between sequences of any length, allowing them to obtain global features and
context information better. Finally, a lightweight decoder is employed for mask
prediction. We evaluate the performance and efficiency of LEFormer on two
datasets, the Surface Water (SW) and the Qinghai-Tibet Plateau Lake (QTPL).
Experimental results show that LEFormer consistently achieves state-of-the-art
(SOTA) performance and efficiency on these two datasets, outperforming existing
methods. Specifically, LEFormer achieves 90.86% and 97.42% mIoU on the SW and
QTPL datasets with a parameter count of 3.61M, respectively, while being 20x
minor than the previous SOTA method.
</p></li>
</ul>

<h3>Title: A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. (arXiv:2308.03891v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03891">http://arxiv.org/abs/2308.03891</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03891]] A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction(http://arxiv.org/abs/2308.03891)</code></li>
<li>Summary: <p>Causal knowledge extraction is the task of extracting relevant causes and
effects from text by detecting the causal relation. Although this task is
important for language understanding and knowledge discovery, recent works in
this domain have largely focused on binary classification of a text segment as
causal or non-causal. In this regard, we perform a thorough analysis of three
sequence tagging models for causal knowledge extraction and compare it with a
span based approach to causality extraction. Our experiments show that
embeddings from pre-trained language models (e.g. BERT) provide a significant
performance boost on this task compared to previous state-of-the-art models
with complex architectures. We observe that span based models perform better
than simple sequence tagging models based on BERT across all 4 data sets from
diverse domains with different types of cause-effect phrases.
</p></li>
</ul>

<h3>Title: A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset. (arXiv:2308.04037v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04037">http://arxiv.org/abs/2308.04037</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04037]] A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset(http://arxiv.org/abs/2308.04037)</code></li>
<li>Summary: <p>Text Classification is the process of categorizing text into the relevant
categories and its algorithms are at the core of many Natural Language
Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP
are the most highly used information retrieval methods in text classification.
We have investigated and analyzed the feature weighting method for text
classification on unstructured data. The proposed model considered two features
N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset
for sentiment analysis. Then we have used the state-of-the-art classifier to
validate the method i.e., Support Vector Machine (SVM), Logistic Regression,
Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and
k-nearest neighbors (KNN). From those two feature extractions, a significant
increase in feature extraction with TF-IDF features rather than based on
N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall
(93.81%), and F1-score (91.99%) value in Random Forest classifier.
</p></li>
</ul>

<h3>Title: Legal Summarisation through LLMs: The PRODIGIT Project. (arXiv:2308.04416v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04416">http://arxiv.org/abs/2308.04416</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04416]] Legal Summarisation through LLMs: The PRODIGIT Project(http://arxiv.org/abs/2308.04416)</code></li>
<li>Summary: <p>We present some initial results of a large-scale Italian project called
PRODIGIT which aims to support tax judges and lawyers through digital
technology, focusing on AI. We have focused on generation of summaries of
judicial decisions and on the extraction of related information, such as the
identification of legal issues and decision-making criteria, and the
specification of keywords. To this end, we have deployed and evaluated
different tools and approaches to extractive and abstractive summarisation. We
have applied LLMs, and particularly on GPT4, which has enabled us to obtain
results that proved satisfactory, according to an evaluation by expert tax
judges and lawyers. On this basis, a prototype application is being built which
will be made publicly available.
</p></li>
</ul>

<h3>Title: DroidDissector: A Static and Dynamic Analysis Tool for Android Malware Detection. (arXiv:2308.04170v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04170">http://arxiv.org/abs/2308.04170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04170]] DroidDissector: A Static and Dynamic Analysis Tool for Android Malware Detection(http://arxiv.org/abs/2308.04170)</code></li>
<li>Summary: <p>DroidDissector is an extraction tool for both static and dynamic features.
The aim is to provide Android malware researchers and analysts with an
integrated tool that can extract all of the most widely used features in
Android malware detection from one location. The static analysis module
extracts features from both the manifest file and the source code of the
application to obtain a broad array of features that include permissions, API
call graphs and opcodes. The dynamic analysis module runs on the latest version
of Android and analyses the complete behaviour of an application by tracking
the system calls used, network traffic generated, API calls used and log files
produced by the application.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data. (arXiv:2308.04070v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04070">http://arxiv.org/abs/2308.04070</a></li>
<li>Code URL: https://github.com/nvidia/nvflare</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04070]] ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data(http://arxiv.org/abs/2308.04070)</code></li>
<li>Summary: <p>Developing a generalized segmentation model capable of simultaneously
delineating multiple organs and diseases is highly desirable. Federated
learning (FL) is a key technology enabling the collaborative development of a
model without exchanging training data. However, the limited access to fully
annotated training data poses a major challenge to training generalizable
models. We propose "ConDistFL", a framework to solve this problem by combining
FL with knowledge distillation. Local models can extract the knowledge of
unlabeled organs and tumors from partially annotated data from the global model
with an adequately designed conditional probability representation. We validate
our framework on four distinct partially annotated abdominal CT datasets from
the MSD and KiTS19 challenges. The experimental results show that the proposed
framework significantly outperforms FedAvg and FedOpt baselines. Moreover, the
performance on an external test dataset demonstrates superior generalizability
compared to models trained on each dataset separately. Our ablation study
suggests that ConDistFL can perform well without frequent aggregation, reducing
the communication cost of FL. Our implementation will be available at
https://github.com/NVIDIA/NVFlare/tree/dev/research/condist-fl.
</p></li>
</ul>

<h3>Title: FLIPS: Federated Learning using Intelligent Participant Selection. (arXiv:2308.03901v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03901">http://arxiv.org/abs/2308.03901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03901]] FLIPS: Federated Learning using Intelligent Participant Selection(http://arxiv.org/abs/2308.03901)</code></li>
<li>Summary: <p>This paper presents the design and implementation of FLIPS, a middleware
system to manage data and participant heterogeneity in federated learning (FL)
training workloads. In particular, we examine the benefits of label
distribution clustering on participant selection in federated learning. FLIPS
clusters parties involved in an FL training job based on the label distribution
of their data apriori, and during FL training, ensures that each cluster is
equitably represented in the participants selected. FLIPS can support the most
common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To
manage platform heterogeneity and dynamic resource availability, FLIPS
incorporates a straggler management mechanism to handle changing capacities in
distributed, smart community applications. Privacy of label distributions,
clustering and participant selection is ensured through a trusted execution
environment (TEE). Our comprehensive empirical evaluation compares FLIPS with
random participant selection, as well as two other "smart" selection mechanisms
- Oort and gradient clustering using two real-world datasets, two different
non-IID distributions and three common FL algorithms (FedYogi, FedProx and
FedAvg). We demonstrate that FLIPS significantly improves convergence,
achieving higher accuracy by 17 - 20 % with 20 - 60 % lower communication
costs, and these benefits endure in the presence of straggler participants.
</p></li>
</ul>

<h3>Title: The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers. (arXiv:2308.03945v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03945">http://arxiv.org/abs/2308.03945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03945]] The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers(http://arxiv.org/abs/2308.03945)</code></li>
<li>Summary: <p>Federated learning (FL) addresses data privacy concerns by enabling
collaborative training of AI models across distributed data owners. Wide
adoption of FL faces the fundamental challenges of data heterogeneity and the
large scale of data owners involved. In this paper, we investigate the prospect
of Transformer-based FL models for achieving generalization and personalization
in this setting. We conduct extensive comparative experiments involving FL with
Transformers, ResNet, and personalized ResNet-based FL approaches under various
scenarios. These experiments consider varying numbers of data owners to
demonstrate Transformers' advantages over deep neural networks in large-scale
heterogeneous FL tasks. In addition, we analyze the superior performance of
Transformers by comparing the Centered Kernel Alignment (CKA) representation
similarity across different layers and FL models to gain insight into the
reasons behind their promising capabilities.
</p></li>
</ul>

<h3>Title: Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients. (arXiv:2308.04077v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04077">http://arxiv.org/abs/2308.04077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04077]] Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients(http://arxiv.org/abs/2308.04077)</code></li>
<li>Summary: <p>Federated optimization, an emerging paradigm which finds wide real-world
applications such as federated learning, enables multiple clients (e.g., edge
devices) to collaboratively optimize a global function. The clients do not
share their local datasets and typically only share their local gradients.
However, the gradient information is not available in many applications of
federated optimization, which hence gives rise to the paradigm of federated
zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from
the limitations of query and communication inefficiency, which can be
attributed to (a) their reliance on a substantial number of function queries
for gradient estimation and (b) the significant disparity between their
realized local updates and the intended global updates. To this end, we (a)
introduce trajectory-informed gradient surrogates which is able to use the
history of function queries during optimization for accurate and
query-efficient gradient estimation, and (b) develop the technique of adaptive
gradient correction using these gradient surrogates to mitigate the
aforementioned disparity. Based on these, we propose the federated zeroth-order
optimization using trajectory-informed surrogate gradients (FZooS) algorithm
for query- and communication-efficient federated ZOO. Our FZooS achieves
theoretical improvements over the existing approaches, which is supported by
our real-world experiments such as federated black-box adversarial attack and
federated non-differentiable metric optimization.
</p></li>
</ul>

<h3>Title: Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review. (arXiv:2308.04404v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04404">http://arxiv.org/abs/2308.04404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04404]] Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review(http://arxiv.org/abs/2308.04404)</code></li>
<li>Summary: <p>These days with the rising computational capabilities of wireless user
equipment such as smart phones, tablets, and vehicles, along with growing
concerns about sharing private data, a novel machine learning model called
federated learning (FL) has emerged. FL enables the separation of data
acquisition and computation at the central unit, which is different from
centralized learning that occurs in a data center. FL is typically used in a
wireless edge network where communication resources are limited and unreliable.
Bandwidth constraints necessitate scheduling only a subset of UEs for updates
in each iteration, and because the wireless medium is shared, transmissions are
susceptible to interference and are not assured. The article discusses the
significance of Machine Learning in wireless communication and highlights
Federated Learning (FL) as a novel approach that could play a vital role in
future mobile networks, particularly 6G and beyond.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles. (arXiv:2308.04346v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04346">http://arxiv.org/abs/2308.04346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04346]] Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles(http://arxiv.org/abs/2308.04346)</code></li>
<li>Summary: <p>We investigate the potential for nationality biases in natural language
processing (NLP) models using human evaluation methods. Biased NLP models can
perpetuate stereotypes and lead to algorithmic discrimination, posing a
significant challenge to the fairness and justice of AI systems. Our study
employs a two-step mixed-methods approach that includes both quantitative and
qualitative analysis to identify and understand the impact of nationality bias
in a text generation model. Through our human-centered quantitative analysis,
we measure the extent of nationality bias in articles generated by AI sources.
We then conduct open-ended interviews with participants, performing qualitative
coding and thematic analysis to understand the implications of these biases on
human readers. Our findings reveal that biased NLP models tend to replicate and
amplify existing societal biases, which can translate to harm if used in a
sociotechnical setting. The qualitative analysis from our interviews offers
insights into the experience readers have when encountering such articles,
highlighting the potential to shift a reader's perception of a country. These
findings emphasize the critical role of public perception in shaping AI's
impact on society and the need to correct biases in AI systems.
</p></li>
</ul>

<h3>Title: SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04430">http://arxiv.org/abs/2308.04430</a></li>
<li>Code URL: https://github.com/kernelmachine/silo-lm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04430]] SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore(http://arxiv.org/abs/2308.04430)</code></li>
<li>Summary: <p>The legality of training language models (LMs) on copyrighted or otherwise
restricted data is under intense debate. However, as we show, model performance
significantly degrades if trained only on low-risk text (e.g., out-of-copyright
books or government documents), due to its limited size and domain coverage. We
present SILO, a new language model that manages this risk-performance tradeoff
during inference. SILO is built by (1) training a parametric LM on Open License
Corpus (OLC), a new corpus we curate with 228B tokens of public domain and
permissively licensed text and (2) augmenting it with a more general and easily
modifiable nonparametric datastore (e.g., containing copyrighted books or news)
that is only queried during inference. The datastore allows use of high-risk
data without training on it, supports sentence-level data attribution, and
enables data producers to opt out from the model by removing content from the
store. These capabilities can foster compliance with data-use regulations such
as the fair use doctrine in the United States and the GDPR in the European
Union. Our experiments show that the parametric LM struggles on domains not
covered by OLC. However, access to the datastore greatly improves out of domain
performance, closing 90% of the performance gap with an LM trained on the Pile,
a more diverse corpus with mostly high-risk text. We also analyze which
nonparametric approach works best, where the remaining errors lie, and how
performance scales with datastore size. Our results suggest that it is possible
to build high quality language models while mitigating their legal risk.
</p></li>
</ul>

<h3>Title: Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data. (arXiv:2308.03892v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03892">http://arxiv.org/abs/2308.03892</a></li>
<li>Code URL: https://github.com/anupshakya07/attn-scaling</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03892]] Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data(http://arxiv.org/abs/2308.03892)</code></li>
<li>Summary: <p>Understanding a student's problem-solving strategy can have a significant
impact on effective math learning using Intelligent Tutoring Systems (ITSs) and
Adaptive Instructional Systems (AISs). For instance, the ITS/AIS can better
personalize itself to correct specific misconceptions that are indicated by
incorrect strategies, specific problems can be designed to improve strategies
and frustration can be minimized by adapting to a student's natural way of
thinking rather than trying to fit a standard strategy for all. While it may be
possible for human experts to identify strategies manually in classroom
settings with sufficient student interaction, it is not possible to scale this
up to big data. Therefore, we leverage advances in Machine Learning and AI
methods to perform scalable strategy prediction that is also fair to students
at all skill levels. Specifically, we develop an embedding called MVec where we
learn a representation based on the mastery of students. We then cluster these
embeddings with a non-parametric clustering method where we progressively learn
clusters such that we group together instances that have approximately
symmetrical strategies. The strategy prediction model is trained on instances
sampled from these clusters. This ensures that we train the model over diverse
strategies and also that strategies from a particular group do not bias the DNN
model, thus allowing it to optimize its parameters over all groups. Using real
world large-scale student interaction datasets from MATHia, we implement our
approach using transformers and Node2Vec for learning the mastery embeddings
and LSTMs for predicting strategies. We show that our approach can scale up to
achieve high accuracy by training on a small sample of a large dataset and also
has predictive equality, i.e., it can predict strategies equally well for
learners at diverse skill levels.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2308.04424v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04424">http://arxiv.org/abs/2308.04424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04424]] A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition(http://arxiv.org/abs/2308.04424)</code></li>
<li>Summary: <p>The joint task of Dialog Sentiment Classification (DSC) and Act Recognition
(DAR) aims to predict the sentiment label and act label for each utterance in a
dialog simultaneously. However, current methods encode the dialog context in
only one direction, which limits their ability to thoroughly comprehend the
context. Moreover, these methods overlook the explicit correlations between
sentiment and act labels, which leads to an insufficient ability to capture
rich sentiment and act clues and hinders effective and accurate reasoning. To
address these issues, we propose a Bi-directional Multi-hop Inference Model
(BMIM) that leverages a feature selection network and a bi-directional
multi-hop inference network to iteratively extract and integrate rich sentiment
and act clues in a bi-directional manner. We also employ contrastive learning
and dual learning to explicitly model the correlations of sentiment and act
labels. Our experiments on two widely-used datasets show that BMIM outperforms
state-of-the-art baselines by at least 2.6% on F1 score in DAR and 1.4% on F1
score in DSC. Additionally, Our proposed model not only improves the
performance but also enhances the interpretability of the joint sentiment and
act prediction task.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models. (arXiv:2308.04220v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04220">http://arxiv.org/abs/2308.04220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04220]] Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models(http://arxiv.org/abs/2308.04220)</code></li>
<li>Summary: <p>In this work, we propose a methodology for investigating the application of
semantic attention to enhance the explainability of Graph Neural Network
(GNN)-based models, introducing semantically-informed perturbations and
establishing a correlation between predicted feature-importance weights and
model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for
tasks like scene interpretation, leveraging flexible graph structures to
concisely describe complex features and relationships. As traditional
explainability methods used in eXplainable AI (XAI) cannot be directly applied
to such structures, graph-specific approaches are introduced. Attention
mechanisms have demonstrated their efficacy in estimating the importance of
input features in deep learning models and thus have been previously employed
to provide feature-based explanations for GNN predictions. Building upon these
insights, we extend existing attention-based graph-explainability methods
investigating the use of attention weights as importance indicators of
semantically sorted feature sets. Through analysing the behaviour of predicted
attention-weights distribution in correlation with model accuracy, we gain
valuable insights into feature importance with respect to the behaviour of the
GNN model. We apply our methodology to a lidar pointcloud estimation model
successfully identifying key semantic classes that contribute to enhanced
performance effectively generating reliable post-hoc semantic explanations.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Synthetic Augmentation with Large-scale Unconditional Pre-training. (arXiv:2308.04020v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04020">http://arxiv.org/abs/2308.04020</a></li>
<li>Code URL: https://github.com/karenyyy/histodiffaug</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04020]] Synthetic Augmentation with Large-scale Unconditional Pre-training(http://arxiv.org/abs/2308.04020)</code></li>
<li>Summary: <p>Deep learning based medical image recognition systems often require a
substantial amount of training data with expert annotations, which can be
expensive and time-consuming to obtain. Recently, synthetic augmentation
techniques have been proposed to mitigate the issue by generating realistic
images conditioned on class labels. However, the effectiveness of these methods
heavily depends on the representation capability of the trained generative
model, which cannot be guaranteed without sufficient labeled training data. To
further reduce the dependency on annotated data, we propose a synthetic
augmentation method called HistoDiffusion, which can be pre-trained on
large-scale unlabeled datasets and later applied to a small-scale labeled
dataset for augmented training. In particular, we train a latent diffusion
model (LDM) on diverse unlabeled datasets to learn common features and generate
realistic images without conditional inputs. Then, we fine-tune the model with
classifier guidance in latent space on an unseen labeled dataset so that the
model can synthesize images of specific categories. Additionally, we adopt a
selective mechanism to only add synthetic samples with high confidence of
matching to target labels. We evaluate our proposed method by pre-training on
three histopathology datasets and testing on a histopathology dataset of
colorectal cancer (CRC) excluded from the pre-training datasets. With
HistoDiffusion augmentation, the classification accuracy of a backbone
classifier is remarkably improved by 6.4% using a small set of the original
labels. Our code is available at https://github.com/karenyyy/HistoDiffAug.
</p></li>
</ul>

<h3>Title: MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2308.04249v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04249">http://arxiv.org/abs/2308.04249</a></li>
<li>Code URL: https://github.com/reedonepeck/minddiffuser</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04249]] MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion(http://arxiv.org/abs/2308.04249)</code></li>
<li>Summary: <p>Reconstructing visual stimuli from brain recordings has been a meaningful and
challenging task. Especially, the achievement of precise and controllable image
reconstruction bears great significance in propelling the progress and
utilization of brain-computer interfaces. Despite the advancements in complex
image reconstruction techniques, the challenge persists in achieving a cohesive
alignment of both semantic (concepts and objects) and structure (position,
orientation, and size) with the image stimuli. To address the aforementioned
issue, we propose a two-stage image reconstruction model called MindDiffuser.
In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings
decoded from fMRI are put into Stable Diffusion, which yields a preliminary
image that contains semantic information. In Stage 2, we utilize the CLIP
visual feature decoded from fMRI as supervisory information, and continually
adjust the two feature vectors decoded in Stage 1 through backpropagation to
align the structural information. The results of both qualitative and
quantitative analyses demonstrate that our model has surpassed the current
state-of-the-art models on Natural Scenes Dataset (NSD). The subsequent
experimental findings corroborate the neurobiological plausibility of the
model, as evidenced by the interpretability of the multimodal feature employed,
which align with the corresponding brain responses.
</p></li>
</ul>

<h3>Title: Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On. (arXiv:2308.04288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04288">http://arxiv.org/abs/2308.04288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04288]] Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On(http://arxiv.org/abs/2308.04288)</code></li>
<li>Summary: <p>Fabricating and designing 3D garments has become extremely demanding with the
increasing need for synthesizing realistic dressed persons for a variety of
applications, e.g. 3D virtual try-on, digitalization of 2D clothes into 3D
apparel, and cloth animation. It thus necessitates a simple and straightforward
pipeline to obtain high-quality texture from simple input, such as 2D reference
images. Since traditional warping-based texture generation methods require a
significant number of control points to be manually selected for each type of
garment, which can be a time-consuming and tedious process. We propose a novel
method, called Cloth2Tex, which eliminates the human burden in this process.
Cloth2Tex is a self-supervised method that generates texture maps with
reasonable layout and structural consistency. Another key feature of Cloth2Tex
is that it can be used to support high-fidelity texture inpainting. This is
done by combining Cloth2Tex with a prevailing latent diffusion model. We
evaluate our approach both qualitatively and quantitatively and demonstrate
that Cloth2Tex can generate high-quality texture maps and achieve the best
visual effects in comparison to other methods. Project page:
tomguluson92.github.io/projects/cloth2tex/
</p></li>
</ul>

<h3>Title: DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images. (arXiv:2308.04417v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04417">http://arxiv.org/abs/2308.04417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04417]] DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images(http://arxiv.org/abs/2308.04417)</code></li>
<li>Summary: <p>Optical satellite images are a critical data source; however, cloud cover
often compromises their quality, hindering image applications and analysis.
Consequently, effectively removing clouds from optical satellite images has
emerged as a prominent research direction. While recent advancements in cloud
removal primarily rely on generative adversarial networks, which may yield
suboptimal image quality, diffusion models have demonstrated remarkable success
in diverse image-generation tasks, showcasing their potential in addressing
this challenge. This paper presents a novel framework called DiffCR, which
leverages conditional guided diffusion with deep convolutional networks for
high-performance cloud removal for optical satellite imagery. Specifically, we
introduce a decoupled encoder for conditional image feature extraction,
providing a robust color representation to ensure the close similarity of
appearance information between the conditional input and the synthesized
output. Moreover, we propose a novel and efficient time and condition fusion
block within the cloud removal model to accurately simulate the correspondence
between the appearance in the conditional image and the target image at a low
computational cost. Extensive experimental evaluations on two commonly used
benchmark datasets demonstrate that DiffCR consistently achieves
state-of-the-art performance on all metrics, with parameter and computational
complexities amounting to only 5.1% and 5.4%, respectively, of those previous
best methods. The source code, pre-trained models, and all the experimental
results will be publicly available at https://github.com/XavierJiezou/DiffCR
upon the paper's acceptance of this work.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03767">http://arxiv.org/abs/2308.03767</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03767]] Enhancing image captioning with depth information using a Transformer-based framework(http://arxiv.org/abs/2308.03767)</code></li>
<li>Summary: <p>Captioning images is a challenging scene-understanding task that connects
computer vision and natural language processing. While image captioning models
have been successful in producing excellent descriptions, the field has
primarily focused on generating a single sentence for 2D images. This paper
investigates whether integrating depth information with RGB images can enhance
the captioning task and generate better descriptions. For this purpose, we
propose a Transformer-based encoder-decoder framework for generating a
multi-sentence description of a 3D scene. The RGB image and its corresponding
depth map are provided as inputs to our framework, which combines them to
produce a better understanding of the input scene. Depth maps could be ground
truth or estimated, which makes our framework widely applicable to any RGB
captioning dataset. We explored different fusion approaches to fuse RGB and
depth images. The experiments are performed on the NYU-v2 dataset and the
Stanford image paragraph captioning dataset. During our work with the NYU-v2
dataset, we found inconsistent labeling that prevents the benefit of using
depth information to enhance the captioning task. The results were even worse
than using RGB images only. As a result, we propose a cleaned version of the
NYU-v2 dataset that is more consistent and informative. Our results on both
datasets demonstrate that the proposed framework effectively benefits from
depth information, whether it is ground truth or estimated, and generates
better captions. Code, pre-trained models, and the cleaned version of the
NYU-v2 dataset will be made publically available.
</p></li>
</ul>

<h3>Title: Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection. (arXiv:2308.03826v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03826">http://arxiv.org/abs/2308.03826</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03826]] Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection(http://arxiv.org/abs/2308.03826)</code></li>
<li>Summary: <p>Salient Object Detection (SOD) aims to identify and segment the most
conspicuous objects in an image or video. As an important pre-processing step,
it has many potential applications in multimedia and vision tasks. With the
advance of imaging devices, SOD with high-resolution images is of great demand,
recently. However, traditional SOD methods are largely limited to
low-resolution images, making them difficult to adapt to the development of
High-Resolution SOD (HRSOD). Although some HRSOD methods emerge, there are no
large enough datasets for training and evaluating. Besides, current HRSOD
methods generally produce incomplete object regions and irregular object
boundaries. To address above issues, in this work, we first propose a new
HRS10K dataset, which contains 10,500 high-quality annotated images at 2K-8K
resolution. As far as we know, it is the largest dataset for the HRSOD task,
which will significantly help future works in training and evaluating models.
Furthermore, to improve the HRSOD performance, we propose a novel Recurrent
Multi-scale Transformer (RMFormer), which recurrently utilizes shared
Transformers and multi-scale refinement architectures. Thus, high-resolution
saliency maps can be generated with the guidance of lower-resolution
predictions. Extensive experiments on both high-resolution and low-resolution
benchmarks show the effectiveness and superiority of the proposed framework.
The source code and dataset are released at:
https://github.com/DrowsyMon/RMFormer.
</p></li>
</ul>

<h3>Title: CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification. (arXiv:2308.03968v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03968">http://arxiv.org/abs/2308.03968</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03968]] CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification(http://arxiv.org/abs/2308.03968)</code></li>
<li>Summary: <p>Medical image classification poses unique challenges due to the long-tailed
distribution of diseases, the co-occurrence of diagnostic findings, and the
multiple views available for each study or patient. This paper introduces our
solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed
Classification on Chest X-Rays. Our approach introduces CheXFusion, a
transformer-based fusion module incorporating multi-view images. The fusion
module, guided by self-attention and cross-attention mechanisms, efficiently
aggregates multi-view features while considering label co-occurrence.
Furthermore, we explore data balancing and self-training methods to optimize
the model's performance. Our solution achieves state-of-the-art results with
0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our
success in the task underscores the significance of considering multi-view
settings, class imbalance, and label co-occurrence in medical image
classification. Public code is available at
https://github.com/dongkyuk/CXR-LT-public-solution
</p></li>
</ul>

<h3>Title: Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning. (arXiv:2308.04016v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04016">http://arxiv.org/abs/2308.04016</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04016]] Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning(http://arxiv.org/abs/2308.04016)</code></li>
<li>Summary: <p>Compositional zero-shot learning (CZSL) aims to recognize unseen compositions
with prior knowledge of known primitives (attribute and object). Previous works
for CZSL often suffer from grasping the contextuality between attribute and
object, as well as the discriminability of visual features, and the long-tailed
distribution of real-world compositional data. We propose a simple and scalable
framework called Composition Transformer (CoT) to address these issues. CoT
employs object and attribute experts in distinctive manners to generate
representative embeddings, using the visual network hierarchically. The object
expert extracts representative object embeddings from the final layer in a
bottom-up manner, while the attribute expert makes attribute embeddings in a
top-down manner with a proposed object-guided attention module that models
contextuality explicitly. To remedy biased prediction caused by imbalanced data
distribution, we develop a simple minority attribute augmentation (MAA) that
synthesizes virtual samples by mixing two images and oversampling minority
attribute classes. Our method achieves SoTA performance on several benchmarks,
including MIT-States, C-GQA, and VAW-CZSL. We also demonstrate the
effectiveness of CoT in improving visual discrimination and addressing the
model bias from the imbalanced data distribution. The code is available at
https://github.com/HanjaeKim98/CoT.
</p></li>
</ul>

<h3>Title: SODFormer: Streaming Object Detection with Transformer Using Events and Frames. (arXiv:2308.04047v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04047">http://arxiv.org/abs/2308.04047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04047]] SODFormer: Streaming Object Detection with Transformer Using Events and Frames(http://arxiv.org/abs/2308.04047)</code></li>
<li>Summary: <p>DAVIS camera, streaming two complementary sensing modalities of asynchronous
events and frames, has gradually been used to address major object detection
challenges (e.g., fast motion blur and low-light). However, how to effectively
leverage rich temporal cues and fuse two heterogeneous visual streams remains a
challenging endeavor. To address this challenge, we propose a novel streaming
object detector with Transformer, namely SODFormer, which first integrates
events and frames to continuously detect objects in an asynchronous manner.
Technically, we first build a large-scale multimodal neuromorphic object
detection dataset (i.e., PKU-DAVIS-SOD) over 1080.1k manual labels. Then, we
design a spatiotemporal Transformer architecture to detect objects via an
end-to-end sequence prediction problem, where the novel temporal Transformer
module leverages rich temporal cues from two visual streams to improve the
detection performance. Finally, an asynchronous attention-based fusion module
is proposed to integrate two heterogeneous sensing modalities and take
complementary advantages from each end, which can be queried at any time to
locate objects and break through the limited output frequency from synchronized
frame-based fusion strategies. The results show that the proposed SODFormer
outperforms four state-of-the-art methods and our eight baselines by a
significant margin. We also show that our unifying framework works well even in
cases where the conventional frame-based camera fails, e.g., high-speed motion
and low-light conditions. Our dataset and code can be available at
https://github.com/dianzl/SODFormer.
</p></li>
</ul>

<h3>Title: Class-level Structural Relation Modelling and Smoothing for Visual Representation Learning. (arXiv:2308.04142v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04142">http://arxiv.org/abs/2308.04142</a></li>
<li>Code URL: https://github.com/czt117/csrms</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04142]] Class-level Structural Relation Modelling and Smoothing for Visual Representation Learning(http://arxiv.org/abs/2308.04142)</code></li>
<li>Summary: <p>Representation learning for images has been advanced by recent progress in
more complex neural models such as the Vision Transformers and new learning
theories such as the structural causal models. However, these models mainly
rely on the classification loss to implicitly regularize the class-level data
distributions, and they may face difficulties when handling classes with
diverse visual patterns. We argue that the incorporation of the structural
information between data samples may improve this situation. To achieve this
goal, this paper presents a framework termed \textbf{C}lass-level Structural
Relation Modeling and Smoothing for Visual Representation Learning (CSRMS),
which includes the Class-level Relation Modelling, Class-aware Graph Sampling,
and Relational Graph-Guided Representation Learning modules to model a
relational graph of the entire dataset and perform class-aware smoothing and
regularization operations to alleviate the issue of intra-class visual
diversity and inter-class similarity. Specifically, the Class-level Relation
Modelling module uses a clustering algorithm to learn the data distributions in
the feature space and identify three types of class-level sample relations for
the training set; Class-aware Graph Sampling module extends typical training
batch construction process with three strategies to sample dataset-level
sub-graphs; and Relational Graph-Guided Representation Learning module employs
a graph convolution network with knowledge-guided smoothing operations to ease
the projection from different visual patterns to the same class. Experiments
demonstrate the effectiveness of structured knowledge modelling for enhanced
representation learning and show that CSRMS can be incorporated with any
state-of-the-art visual representation learning models for performance gains.
The source codes and demos have been released at
https://github.com/czt117/CSRMS.
</p></li>
</ul>

<h3>Title: EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation. (arXiv:2308.04162v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04162">http://arxiv.org/abs/2308.04162</a></li>
<li>Code URL: https://github.com/lab206/epcformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04162]] EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation(http://arxiv.org/abs/2308.04162)</code></li>
<li>Summary: <p>Audio-guided Video Object Segmentation (A-VOS) and Referring Video Object
Segmentation (R-VOS) are two highly-related tasks, which both aim to segment
specific objects from video sequences according to user-provided expression
prompts. However, due to the challenges in modeling representations for
different modalities, contemporary methods struggle to strike a balance between
interaction flexibility and high-precision localization and segmentation. In
this paper, we address this problem from two perspectives: the alignment
representation of audio and text and the deep interaction among audio, text,
and visual features. First, we propose a universal architecture, the Expression
Prompt Collaboration Transformer, herein EPCFormer. Next, we propose an
Expression Alignment (EA) mechanism for audio and text expressions. By
introducing contrastive learning for audio and text expressions, the proposed
EPCFormer realizes comprehension of the semantic equivalence between audio and
text expressions denoting the same objects. Then, to facilitate deep
interactions among audio, text, and video features, we introduce an
Expression-Visual Attention (EVA) mechanism. The knowledge of video object
segmentation in terms of the expression prompts can seamlessly transfer between
the two tasks by deeply exploring complementary cues between text and audio.
Experiments on well-recognized benchmarks demonstrate that our universal
EPCFormer attains state-of-the-art results on both tasks. The source code of
EPCFormer will be made publicly available at
https://github.com/lab206/EPCFormer.
</p></li>
</ul>

<h3>Title: Exploring Transformers for Open-world Instance Segmentation. (arXiv:2308.04206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04206">http://arxiv.org/abs/2308.04206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04206]] Exploring Transformers for Open-world Instance Segmentation(http://arxiv.org/abs/2308.04206)</code></li>
<li>Summary: <p>Open-world instance segmentation is a rising task, which aims to segment all
objects in the image by learning from a limited number of base-category
objects. This task is challenging, as the number of unseen categories could be
hundreds of times larger than that of seen categories. Recently, the DETR-like
models have been extensively studied in the closed world while stay unexplored
in the open world. In this paper, we utilize the Transformer for open-world
instance segmentation and present SWORD. Firstly, we introduce to attach the
stop-gradient operation before classification head and further add IoU heads
for discovering novel objects. We demonstrate that a simple stop-gradient
operation not only prevents the novel objects from being suppressed as
background, but also allows the network to enjoy the merit of heuristic label
assignment. Secondly, we propose a novel contrastive learning framework to
enlarge the representations between objects and background. Specifically, we
maintain a universal object queue to obtain the object center, and dynamically
select positive and negative samples from the object queries for contrastive
learning. While the previous works only focus on pursuing average recall and
neglect average precision, we show the prominence of SWORD by giving
consideration to both criteria. Our models achieve state-of-the-art performance
in various open-world cross-category and cross-dataset generalizations.
Particularly, in VOC to non-VOC setup, our method sets new state-of-the-art
results of 40.0% on ARb100 and 34.9% on ARm100. For COCO to UVO generalization,
SWORD significantly outperforms the previous best open-world model by 5.9% on
APm and 8.1% on ARm100.
</p></li>
</ul>

<h3>Title: Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval. (arXiv:2308.04343v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04343">http://arxiv.org/abs/2308.04343</a></li>
<li>Code URL: https://github.com/luminosityx/hat</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04343]] Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval(http://arxiv.org/abs/2308.04343)</code></li>
<li>Summary: <p>Most existing cross-modal retrieval methods employ two-stream encoders with
different architectures for images and texts, \textit{e.g.}, CNN for images and
RNN/Transformer for texts. Such discrepancy in architectures may induce
different semantic distribution spaces and limit the interactions between
images and texts, and further result in inferior alignment between images and
texts. To fill this research gap, inspired by recent advances of Transformers
in vision tasks, we propose to unify the encoder architectures with
Transformers for both modalities. Specifically, we design a cross-modal
retrieval framework purely based on two-stream Transformers, dubbed
\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an image
Transformer, a text Transformer, and a hierarchical alignment module. With such
identical architectures, the encoders could produce representations with more
similar characteristics for images and texts, and make the interactions and
alignments between them much easier. Besides, to leverage the rich semantics,
we devise a hierarchical alignment scheme to explore multi-level
correspondences of different layers between images and texts. To evaluate the
effectiveness of the proposed HAT, we conduct extensive experiments on two
benchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate that
HAT outperforms SOTA baselines by a large margin. Specifically, on two key
tasks, \textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves
7.6\% and 16.7\% relative score improvement of Recall@1 on MSCOCO, and 4.4\%
and 11.6\% on Flickr30k respectively. The code is available at
\url{https://github.com/LuminosityX/HAT}.
</p></li>
</ul>

<h3>Title: 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment. (arXiv:2308.04352v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04352">http://arxiv.org/abs/2308.04352</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04352]] 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment(http://arxiv.org/abs/2308.04352)</code></li>
<li>Summary: <p>3D vision-language grounding (3D-VL) is an emerging field that aims to
connect the 3D physical world with natural language, which is crucial for
achieving embodied intelligence. Current 3D-VL models rely heavily on
sophisticated modules, auxiliary losses, and optimization tricks, which calls
for a simple and unified model. In this paper, we propose 3D-VisTA, a
pre-trained Transformer for 3D Vision and Text Alignment that can be easily
adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention
layers for both single-modal modeling and multi-modal fusion without any
sophisticated task-specific design. To further enhance its performance on 3D-VL
tasks, we construct ScanScribe, the first large-scale 3D scene-text pairs
dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185
unique indoor scenes originating from ScanNet and 3R-Scan datasets, along with
paired 278K scene descriptions generated from existing 3D-VL tasks, templates,
and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object
modeling and scene-text matching. It achieves state-of-the-art results on
various 3D-VL tasks, ranging from visual grounding and dense captioning to
question answering and situated reasoning. Moreover, 3D-VisTA demonstrates
superior data efficiency, obtaining strong performance even with limited
annotations during downstream task fine-tuning.
</p></li>
</ul>

<h3>Title: SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition. (arXiv:2308.04369v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04369">http://arxiv.org/abs/2308.04369</a></li>
<li>Code URL: https://github.com/event-ahu/sstformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04369]] SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition(http://arxiv.org/abs/2308.04369)</code></li>
<li>Summary: <p>Event camera-based pattern recognition is a newly arising research topic in
recent years. Current researchers usually transform the event streams into
images, graphs, or voxels, and adopt deep neural networks for event-based
classification. Although good performance can be achieved on simple event
recognition datasets, however, their results may be still limited due to the
following two issues. Firstly, they adopt spatial sparse event streams for
recognition only, which may fail to capture the color and detailed texture
information well. Secondly, they adopt either Spiking Neural Networks (SNN) for
energy-efficient recognition with suboptimal results, or Artificial Neural
Networks (ANN) for energy-intensive, high-performance recognition. However,
seldom of them consider achieving a balance between these two aspects. In this
paper, we formally propose to recognize patterns by fusing RGB frames and event
streams simultaneously and propose a new RGB frame-event recognition framework
to address the aforementioned issues. The proposed method contains four main
modules, i.e., memory support Transformer network for RGB frame encoding,
spiking neural network for raw event stream encoding, multi-modal bottleneck
fusion module for RGB-Event feature aggregation, and prediction head. Due to
the scarce of RGB-Event based classification dataset, we also propose a
large-scale PokerEvent dataset which contains 114 classes, and 27102
frame-event pairs recorded using a DVS346 event camera. Extensive experiments
on two RGB-Event based classification datasets fully validated the
effectiveness of our proposed framework. We hope this work will boost the
development of pattern recognition by fusing RGB frames and event streams. Both
our dataset and source code of this work will be released at
https://github.com/Event-AHU/SSTFormer.
</p></li>
</ul>

<h3>Title: Character-level NMT and language similarity. (arXiv:2308.04398v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04398">http://arxiv.org/abs/2308.04398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04398]] Character-level NMT and language similarity(http://arxiv.org/abs/2308.04398)</code></li>
<li>Summary: <p>We explore the effectiveness of character-level neural machine translation
using Transformer architecture for various levels of language similarity and
size of the training dataset on translation between Czech and Croatian, German,
Hungarian, Slovak, and Spanish. We evaluate the models using automatic MT
metrics and show that translation between similar languages benefits from
character-level input segmentation, while for less related languages,
character-level vanilla Transformer-base often lags behind subword-level
segmentation. We confirm previous findings that it is possible to close the gap
by finetuning the already trained subword-level models to character-level.
</p></li>
</ul>

<h3>Title: PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning. (arXiv:2308.03953v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03953">http://arxiv.org/abs/2308.03953</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03953]] PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning(http://arxiv.org/abs/2308.03953)</code></li>
<li>Summary: <p>Deep learning has emerged as an effective solution for addressing the
challenges of short-term voltage stability assessment (STVSA) in power systems.
However, existing deep learning-based STVSA approaches face limitations in
adapting to topological changes, sample labeling, and handling small datasets.
To overcome these challenges, this paper proposes a novel phasor measurement
unit (PMU) measurements-based STVSA method by using deep transfer learning. The
method leverages the real-time dynamic information captured by PMUs to create
an initial dataset. It employs temporal ensembling for sample labeling and
utilizes least squares generative adversarial networks (LSGAN) for data
augmentation, enabling effective deep learning on small-scale datasets.
Additionally, the method enhances adaptability to topological changes by
exploring connections between different faults. Experimental results on the
IEEE 39-bus test system demonstrate that the proposed method improves model
evaluation accuracy by approximately 20% through transfer learning, exhibiting
strong adaptability to topological changes. Leveraging the self-attention
mechanism of the Transformer model, this approach offers significant advantages
over shallow learning methods and other deep learning-based approaches.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings. (arXiv:2308.04052v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04052">http://arxiv.org/abs/2308.04052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04052]] The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings(http://arxiv.org/abs/2308.04052)</code></li>
<li>Summary: <p>The five-dollar model is a lightweight text-to-image generative architecture
that generates low dimensional images from an encoded text prompt. This model
can successfully generate accurate and aesthetically pleasing content in low
dimensional domains, with limited amounts of training data. Despite the small
size of both the model and datasets, the generated images are still able to
maintain the encoded semantic meaning of the textual prompt. We apply this
model to three small datasets: pixel art video game maps, video game sprite
images, and down-scaled emoji images and apply novel augmentation strategies to
improve the performance of our model on these limited datasets. We evaluate our
models performance using cosine similarity score between text-image pairs
generated by the CLIP VIT-B/32 model.
</p></li>
</ul>

<h3>Title: From Unimodal to Multimodal: improving the sEMG-Based Pattern Recognition via deep generative models. (arXiv:2308.04091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04091">http://arxiv.org/abs/2308.04091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04091]] From Unimodal to Multimodal: improving the sEMG-Based Pattern Recognition via deep generative models(http://arxiv.org/abs/2308.04091)</code></li>
<li>Summary: <p>Multimodal hand gesture recognition (HGR) systems can achieve higher
recognition accuracy. However, acquiring multimodal gesture recognition data
typically requires users to wear additional sensors, thereby increasing
hardware costs. This paper proposes a novel generative approach to improve
Surface Electromyography (sEMG)-based HGR accuracy via virtual Inertial
Measurement Unit (IMU) signals. Specifically, we trained a deep generative
model based on the intrinsic correlation between forearm sEMG signals and
forearm IMU signals to generate virtual forearm IMU signals from the input
forearm sEMG signals at first. Subsequently, the sEMG signals and virtual IMU
signals were fed into a multimodal Convolutional Neural Network (CNN) model for
gesture recognition. To evaluate the performance of the proposed approach, we
conducted experiments on 6 databases, including 5 publicly available databases
and our collected database comprising 28 subjects performing 38 gestures,
containing both sEMG and IMU data. The results show that our proposed approach
outperforms the sEMG-based unimodal HGR method (with increases of
2.15%-13.10%). It demonstrates that incorporating virtual IMU signals,
generated by deep generative models, can significantly enhance the accuracy of
sEMG-based HGR. The proposed approach represents a successful attempt to
transition from unimodal HGR to multimodal HGR without additional sensor
hardware.
</p></li>
</ul>

<h3>Title: Vision-Based Autonomous Navigation for Unmanned Surface Vessel in Extreme Marine Conditions. (arXiv:2308.04283v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04283">http://arxiv.org/abs/2308.04283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04283]] Vision-Based Autonomous Navigation for Unmanned Surface Vessel in Extreme Marine Conditions(http://arxiv.org/abs/2308.04283)</code></li>
<li>Summary: <p>Visual perception is an important component for autonomous navigation of
unmanned surface vessels (USV), particularly for the tasks related to
autonomous inspection and tracking. These tasks involve vision-based navigation
techniques to identify the target for navigation. Reduced visibility under
extreme weather conditions in marine environments makes it difficult for
vision-based approaches to work properly. To overcome these issues, this paper
presents an autonomous vision-based navigation framework for tracking target
objects in extreme marine conditions. The proposed framework consists of an
integrated perception pipeline that uses a generative adversarial network (GAN)
to remove noise and highlight the object features before passing them to the
object detector (i.e., YOLOv5). The detected visual features are then used by
the USV to track the target. The proposed framework has been thoroughly tested
in simulation under extremely reduced visibility due to sandstorms and fog. The
results are compared with state-of-the-art de-hazing methods across the
benchmarked MBZIRC simulation dataset, on which the proposed scheme has
outperformed the existing methods across various metrics.
</p></li>
</ul>

<h3>Title: Domain Adaptive Person Search via GAN-based Scene Synthesis for Cross-scene Videos. (arXiv:2308.04322v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04322">http://arxiv.org/abs/2308.04322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04322]] Domain Adaptive Person Search via GAN-based Scene Synthesis for Cross-scene Videos(http://arxiv.org/abs/2308.04322)</code></li>
<li>Summary: <p>Person search has recently been a challenging task in the computer vision
domain, which aims to search specific pedestrians from real
cameras.Nevertheless, most surveillance videos comprise only a handful of
images of each pedestrian, which often feature identical backgrounds and
clothing. Hence, it is difficult to learn more discriminative features for
person search in real scenes. To tackle this challenge, we draw on Generative
Adversarial Networks (GAN) to synthesize data from surveillance videos. GAN has
thrived in computer vision problems because it produces high-quality images
efficiently. We merely alter the popular Fast R-CNN model, which is capable of
processing videos and yielding accurate detection outcomes. In order to
appropriately relieve the pressure brought by the two-stage model, we design an
Assisted-Identity Query Module (AIDQ) to provide positive images for the behind
part. Besides, the proposed novel GAN-based Scene Synthesis model that can
synthesize high-quality cross-id person images for person search tasks. In
order to facilitate the feature learning of the GAN-based Scene Synthesis
model, we adopt an online learning strategy that collaboratively learns the
synthesized images and original images. Extensive experiments on two widely
used person search benchmarks, CUHK-SYSU and PRW, have shown that our method
has achieved great performance, and the extensive ablation study further
justifies our GAN-synthetic data can effectively increase the variability of
the datasets and be more realistic.
</p></li>
</ul>

<h3>Title: A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces. (arXiv:2308.04426v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04426">http://arxiv.org/abs/2308.04426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04426]] A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces(http://arxiv.org/abs/2308.04426)</code></li>
<li>Summary: <p>Accurate detection of natural deterioration and man-made damage on the
surfaces of ancient stele in the first instance is essential for their
preventive conservation. Existing methods for cultural heritage preservation
are not able to achieve this goal perfectly due to the difficulty of balancing
accuracy, efficiency, timeliness, and cost. This paper presents a deep-learning
method to automatically detect above mentioned emergencies on ancient stone
stele in real time, employing autoencoder (AE) and generative adversarial
network (GAN). The proposed method overcomes the limitations of existing
methods by requiring no extensive anomaly samples while enabling comprehensive
detection of unpredictable anomalies. the method includes stages of monitoring,
data acquisition, pre-processing, model structuring, and post-processing.
Taking the Longmen Grottoes' stone steles as a case study, an unsupervised
learning model based on AE and GAN architectures is proposed and validated with
a reconstruction accuracy of 99.74\%. The method's evaluation revealed the
proficient detection of seven artificially designed anomalies and demonstrated
precision and reliability without false alarms. This research provides novel
ideas and possibilities for the application of deep learning in the field of
cultural heritage.
</p></li>
</ul>

<h3>Title: SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool. (arXiv:2308.03983v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03983">http://arxiv.org/abs/2308.03983</a></li>
<li>Code URL: https://github.com/rcgai/simplyretrieve</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03983]] SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool(http://arxiv.org/abs/2308.03983)</code></li>
<li>Summary: <p>Large Language Model (LLM) based Generative AI systems have seen significant
progress in recent years. Integrating a knowledge retrieval architecture allows
for seamless integration of private data into publicly available Generative AI
systems using pre-trained LLM without requiring additional model fine-tuning.
Moreover, Retrieval-Centric Generation (RCG) approach, a promising future
research direction that explicitly separates roles of LLMs and retrievers in
context interpretation and knowledge memorization, potentially leads to more
efficient implementation. SimplyRetrieve is an open-source tool with the goal
of providing a localized, lightweight, and user-friendly interface to these
sophisticated advancements to the machine learning community. SimplyRetrieve
features a GUI and API based RCG platform, assisted by a Private Knowledge Base
Constructor and a Retrieval Tuning Module. By leveraging these capabilities,
users can explore the potential of RCG for improving generative AI performance
while maintaining privacy standards. The tool is available at
https://github.com/RCGAI/SimplyRetrieve with an MIT license.
</p></li>
</ul>

<h3>Title: Goodness-of-Fit of Attributed Probabilistic Graph Generative Models. (arXiv:2308.03773v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03773">http://arxiv.org/abs/2308.03773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03773]] Goodness-of-Fit of Attributed Probabilistic Graph Generative Models(http://arxiv.org/abs/2308.03773)</code></li>
<li>Summary: <p>Probabilistic generative models of graphs are important tools that enable
representation and sampling. Many recent works have created probabilistic
models of graphs that are capable of representing not only entity interactions
but also their attributes. However, given a generative model of random
attributed graph(s), the general conditions that establish goodness of fit are
not clear a-priori. In this paper, we define goodness of fit in terms of the
mean square contingency coefficient for random binary networks. For this
statistic, we outline a procedure for assessing the quality of the structure of
a learned attributed graph by ensuring that the discrepancy of the mean square
contingency coefficient (constant, or random) is minimal with high probability.
We apply these criteria to verify the representation capability of a
probabilistic generative model for various popular types of graph models.
</p></li>
</ul>

<h3>Title: Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models. (arXiv:2308.03960v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03960">http://arxiv.org/abs/2308.03960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03960]] Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models(http://arxiv.org/abs/2308.03960)</code></li>
<li>Summary: <p>Preliminary trajectory design is a global search problem that seeks multiple
qualitatively different solutions to a trajectory optimization problem. Due to
its high dimensionality and non-convexity, and the frequent adjustment of
problem parameters, the global search becomes computationally demanding. In
this paper, we exploit the clustering structure in the solutions and propose an
amortized global search (AmorGS) framework. We use deep generative models to
predict trajectory solutions that share similar structures with previously
solved problems, which accelerates the global search for unseen parameter
values. Our method is evaluated using De Jong's 5th function and a low-thrust
circular restricted three-body problem.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Few-shot medical image classification with simple shape and texture text descriptors using vision-language models. (arXiv:2308.04005v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04005">http://arxiv.org/abs/2308.04005</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04005]] Few-shot medical image classification with simple shape and texture text descriptors using vision-language models(http://arxiv.org/abs/2308.04005)</code></li>
<li>Summary: <p>In this work, we investigate the usefulness of vision-language models (VLMs)
and large language models for binary few-shot classification of medical images.
We utilize the GPT-4 model to generate text descriptors that encapsulate the
shape and texture characteristics of objects in medical images. Subsequently,
these GPT-4 generated descriptors, alongside VLMs pre-trained on natural
images, are employed to classify chest X-rays and breast ultrasound images. Our
results indicate that few-shot classification of medical images using VLMs and
GPT-4 generated descriptors is a viable approach. However, accurate
classification requires to exclude certain descriptors from the calculations of
the classification scores. Moreover, we assess the ability of VLMs to evaluate
shape features in breast mass ultrasound images. We further investigate the
degree of variability among the sets of text descriptors produced by GPT-4. Our
work provides several important insights about the application of VLMs for
medical image analysis.
</p></li>
</ul>

<h3>Title: Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions. (arXiv:2308.04152v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04152">http://arxiv.org/abs/2308.04152</a></li>
<li>Code URL: https://github.com/dcdmllm/cheetah</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04152]] Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions(http://arxiv.org/abs/2308.04152)</code></li>
<li>Summary: <p>Multimodal Large Language Models (MLLMs) have recently sparked significant
interest, which demonstrates emergent capabilities to serve as a
general-purpose model for various vision-language tasks. However, existing
methods mainly focus on limited types of instructions with a single image as
visual context, which hinders the widespread availability of MLLMs. In this
paper, we introduce the I4 benchmark to comprehensively evaluate the
instruction following ability on complicated interleaved vision-language
instructions, which involve intricate image-text sequential context, covering a
diverse range of scenarios (e.g., visually-rich webpages/textbooks, lecture
slides, embodied dialogue). Systematic evaluation on our I4 benchmark reveals a
common defect of existing methods: the Visual Prompt Generator (VPG) trained on
image-captioning alignment objective tends to attend to common foreground
information for captioning but struggles to extract specific information
required by particular tasks. To address this issue, we propose a generic and
lightweight controllable knowledge re-injection module, which utilizes the
sophisticated reasoning ability of LLMs to control the VPG to conditionally
extract instruction-specific visual information and re-inject it into the LLM.
Further, we introduce an annotation-free cross-attention guided counterfactual
image training strategy to methodically learn the proposed module by
collaborating a cascade of foundation models. Enhanced by the proposed module
and training strategy, we present Cheetah, a MLLM that can effectively handle a
wide variety of interleaved vision-language instructions and achieves
state-of-the-art zero-shot performance across all tasks of I4, without
high-quality multimodal instruction tuning data. Moreover, Cheetah also
exhibits competitive performance compared with state-of-the-art instruction
tuned models on concurrent MME benchmark.
</p></li>
</ul>

<h3>Title: Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models. (arXiv:2308.03853v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03853">http://arxiv.org/abs/2308.03853</a></li>
<li>Code URL: https://github.com/madhumitasushil/oncllmextraction</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03853]] Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models(http://arxiv.org/abs/2308.03853)</code></li>
<li>Summary: <p>Both medical care and observational studies in oncology require a thorough
understanding of a patient's disease progression and treatment history, often
elaborately documented in clinical notes. Despite their vital role, no current
oncology information representation and annotation schema fully encapsulates
the diversity of information recorded within these notes. Although large
language models (LLMs) have recently exhibited impressive performance on
various medical natural language processing tasks, due to the current lack of
comprehensively annotated oncology datasets, an extensive evaluation of LLMs in
extracting and reasoning with the complex rhetoric in oncology notes remains
understudied. We developed a detailed schema for annotating textual oncology
information, encompassing patient characteristics, tumor characteristics,
tests, treatments, and temporality. Using a corpus of 10 de-identified breast
cancer progress notes at University of California, San Francisco, we applied
this schema to assess the abilities of three recently-released LLMs (GPT-4,
GPT-3.5-turbo, and FLAN-UL2) to perform zero-shot extraction of detailed
oncological history from two narrative sections of clinical progress notes. Our
team annotated 2750 entities, 2874 modifiers, and 1623 relationships. The GPT-4
model exhibited overall best performance, with an average BLEU score of 0.69,
an average ROUGE score of 0.72, and an average accuracy of 67% on complex tasks
(expert manual evaluation). Notably, it was proficient in tumor characteristic
and medication extraction, and demonstrated superior performance in inferring
symptoms due to cancer and considerations of future medications. The analysis
demonstrates that GPT-4 is potentially already usable to extract important
facts from cancer progress notes needed for clinical research, complex
population management, and documenting quality patient care.
</p></li>
</ul>

<h3>Title: Simple synthetic data reduces sycophancy in large language models. (arXiv:2308.03958v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03958">http://arxiv.org/abs/2308.03958</a></li>
<li>Code URL: https://github.com/google/sycophancy-intervention</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03958]] Simple synthetic data reduces sycophancy in large language models(http://arxiv.org/abs/2308.03958)</code></li>
<li>Summary: <p>Sycophancy is an undesirable behavior where models tailor their responses to
follow a human user's view even when that view is not objectively correct
(e.g., adapting liberal views once a user reveals that they are liberal). In
this paper, we study the prevalence of sycophancy in language models and
propose a simple synthetic-data intervention to reduce this behavior.
</p>
<p>First, on a set of three sycophancy tasks (Perez et al., 2022) where models
are asked for an opinion on statements with no correct answers (e.g.,
politics), we observe that both model scaling and instruction tuning
significantly increase sycophancy for PaLM models up to 540B parameters.
Second, we extend sycophancy evaluations to simple addition statements that are
objectively incorrect, finding that despite knowing that these statements are
wrong, language models will still agree with them if the user does as well.
</p>
<p>To reduce sycophancy, we present a straightforward synthetic-data
intervention that takes public NLP tasks and encourages models to be robust to
user opinions on these tasks. Adding these data in a lightweight finetuning
step can significantly reduce sycophantic behavior on held-out prompts. Code
for generating synthetic data for intervention can be found at
https://github.com/google/sycophancy-intervention.
</p></li>
</ul>

<h3>Title: Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04014">http://arxiv.org/abs/2308.04014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04014]] Continual Pre-Training of Large Language Models: How to (re)warm your model?(http://arxiv.org/abs/2308.04014)</code></li>
<li>Summary: <p>Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to restart the process over again once new data becomes available. A much
cheaper and more efficient solution would be to enable the continual
pre-training of these models, i.e. updating pre-trained models with new data
instead of re-training them from scratch. However, the distribution shift
induced by novel data typically results in degraded performance on past data.
Taking a step towards efficient continual pre-training, in this work, we
examine the effect of different warm-up strategies. Our hypothesis is that the
learning rate must be re-increased to improve compute efficiency when training
on a new dataset. We study the warmup phase of models pre-trained on the Pile
(upstream data, 300B tokens) as we continue to pre-train on SlimPajama
(downstream data, 297B tokens), following a linear warmup and cosine decay
schedule. We conduct all experiments on the Pythia 410M language model
architecture and evaluate performance through validation perplexity. We
experiment with different pre-training checkpoints, various maximum learning
rates, and various warmup lengths. Our results show that while rewarming models
first increases the loss on upstream and downstream data, in the longer run it
improves the downstream performance, outperforming models trained from
scratch$\unicode{x2013}$even for a large downstream dataset.
</p></li>
</ul>

<h3>Title: Large Language Model Prompt Chaining for Long Legal Document Classification. (arXiv:2308.04138v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04138">http://arxiv.org/abs/2308.04138</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04138]] Large Language Model Prompt Chaining for Long Legal Document Classification(http://arxiv.org/abs/2308.04138)</code></li>
<li>Summary: <p>Prompting is used to guide or steer a language model in generating an
appropriate response that is consistent with the desired outcome. Chaining is a
strategy used to decompose complex tasks into smaller, manageable components.
In this study, we utilize prompt chaining for extensive legal document
classification tasks, which present difficulties due to their intricate
domain-specific language and considerable length. Our approach begins with the
creation of a concise summary of the original document, followed by a semantic
search for related exemplar texts and their corresponding annotations from a
training corpus. Finally, we prompt for a label - based on the task - to
assign, by leveraging the in-context learning from the few-shot prompt. We
demonstrate that through prompt chaining, we can not only enhance the
performance over zero-shot, but also surpass the micro-F1 score achieved by
larger models, such as ChatGPT zero-shot, using smaller models.
</p></li>
</ul>

<h3>Title: Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04215">http://arxiv.org/abs/2308.04215</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04215]] Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance(http://arxiv.org/abs/2308.04215)</code></li>
<li>Summary: <p>Retrieval augmented models show promise in enhancing traditional language
models by improving their contextual understanding, integrating private data,
and reducing hallucination. However, the processing time required for retrieval
augmented large language models poses a challenge when applying them to tasks
that require real-time responses, such as composition assistance.
</p>
<p>To overcome this limitation, we propose the Hybrid Retrieval-Augmented
Generation (HybridRAG) framework that leverages a hybrid setting that combines
both client and cloud models. HybridRAG incorporates retrieval-augmented memory
generated asynchronously by a Large Language Model (LLM) in the cloud. By
integrating this retrieval augmented memory, the client model acquires the
capability to generate highly effective responses, benefiting from the LLM's
capabilities. Furthermore, through asynchronous memory integration, the client
model is capable of delivering real-time responses to user requests without the
need to wait for memory synchronization from the cloud. Our experiments on
Wikitext and Pile subsets show that HybridRAG achieves lower latency than a
cloud-based retrieval-augmented LLM, while outperforming client-only models in
utility.
</p></li>
</ul>

<h3>Title: Learning Evaluation Models from Large Language Models for Sequence Generation. (arXiv:2308.04386v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04386">http://arxiv.org/abs/2308.04386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04386]] Learning Evaluation Models from Large Language Models for Sequence Generation(http://arxiv.org/abs/2308.04386)</code></li>
<li>Summary: <p>Large language models achieve state-of-the-art performance on sequence
generation evaluation, but typically have a large number of parameters. This is
a computational challenge as presented by applying their evaluation capability
at scale. To overcome the challenge, in this paper, we propose \textbf{ECT}, an
\textbf{e}valuation \textbf{c}apability \textbf{t}ransfer method, to transfer
the evaluation capability from LLMs to relatively lightweight language models.
Based on the proposed ECT, we learn various evaluation models from ChatGPT, and
employ them as reward models to improve sequence generation models via
reinforcement learning and reranking approaches. Experimental results on
machine translation, text style transfer, and summarization tasks demonstrate
the effectiveness of our ECT. Notably, applying the learned evaluation models
to sequence generation models results in better generated sequences as
evaluated by commonly used metrics and ChatGPT.
</p></li>
</ul>

<h3>Title: "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. (arXiv:2308.03825v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03825">http://arxiv.org/abs/2308.03825</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03825]] "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models(http://arxiv.org/abs/2308.03825)</code></li>
<li>Summary: <p>The misuse of large language models (LLMs) has garnered significant attention
from the general public and LLM vendors. In response, efforts have been made to
align LLMs with human values and intent use. However, a particular type of
adversarial prompts, known as jailbreak prompt, has emerged and continuously
evolved to bypass the safeguards and elicit harmful content from LLMs. In this
paper, we conduct the first measurement study on jailbreak prompts in the wild,
with 6,387 prompts collected from four platforms over six months. Leveraging
natural language processing technologies and graph-based community detection
methods, we discover unique characteristics of jailbreak prompts and their
major attack strategies, such as prompt injection and privilege escalation. We
also observe that jailbreak prompts increasingly shift from public platforms to
private ones, posing new challenges for LLM vendors in proactive detection. To
assess the potential harm caused by jailbreak prompts, we create a question set
comprising 46,800 samples across 13 forbidden scenarios. Our experiments show
that current LLMs and safeguards cannot adequately defend jailbreak prompts in
all scenarios. Particularly, we identify two highly effective jailbreak prompts
which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and
they have persisted online for over 100 days. Our work sheds light on the
severe and evolving threat landscape of jailbreak prompts. We hope our study
can facilitate the research community and LLM vendors in promoting safer and
regulated LLMs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection. (arXiv:2308.03766v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03766">http://arxiv.org/abs/2308.03766</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03766]] AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection(http://arxiv.org/abs/2308.03766)</code></li>
<li>Summary: <p>This research paper presents AMaizeD: An End to End Pipeline for Automatic
Maize Disease Detection, an automated framework for early detection of diseases
in maize crops using multispectral imagery obtained from drones. A custom
hand-collected dataset focusing specifically on maize crops was meticulously
gathered by expert researchers and agronomists. The dataset encompasses a
diverse range of maize varieties, cultivation practices, and environmental
conditions, capturing various stages of maize growth and disease progression.
By leveraging multispectral imagery, the framework benefits from improved
spectral resolution and increased sensitivity to subtle changes in plant
health. The proposed framework employs a combination of convolutional neural
networks (CNNs) as feature extractors and segmentation techniques to identify
both the maize plants and their associated diseases. Experimental results
demonstrate the effectiveness of the framework in detecting a range of maize
diseases, including powdery mildew, anthracnose, and leaf blight. The framework
achieves state-of-the-art performance on the custom hand-collected dataset and
contributes to the field of automated disease detection in agriculture,
offering a practical solution for early identification of diseases in maize
crops advanced machine learning techniques and deep learning architectures.
</p></li>
</ul>

<h3>Title: Visual Saliency Detection in Advanced Driver Assistance Systems. (arXiv:2308.03770v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03770">http://arxiv.org/abs/2308.03770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03770]] Visual Saliency Detection in Advanced Driver Assistance Systems(http://arxiv.org/abs/2308.03770)</code></li>
<li>Summary: <p>Visual Saliency refers to the innate human mechanism of focusing on and
extracting important features from the observed environment. Recently, there
has been a notable surge of interest in the field of automotive research
regarding the estimation of visual saliency. While operating a vehicle, drivers
naturally direct their attention towards specific objects, employing
brain-driven saliency mechanisms that prioritize certain elements over others.
In this investigation, we present an intelligent system that combines a
drowsiness detection system for drivers with a scene comprehension pipeline
based on saliency. To achieve this, we have implemented a specialized 3D deep
network for semantic segmentation, which has been pretrained and tailored for
processing the frames captured by an automotive-grade external camera. The
proposed pipeline was hosted on an embedded platform utilizing the STA1295
core, featuring ARM A7 dual-cores, and embeds an hardware accelerator.
Additionally, we employ an innovative biosensor embedded on the car steering
wheel to monitor the driver drowsiness, gathering the PhotoPlethysmoGraphy
(PPG) signal of the driver. A dedicated 1D temporal deep convolutional network
has been devised to classify the collected PPG time-series, enabling us to
assess the driver level of attentiveness. Ultimately, we compare the determined
attention level of the driver with the corresponding saliency-based scene
classification to evaluate the overall safety level. The efficacy of the
proposed pipeline has been validated through extensive experimental results.
</p></li>
</ul>

<h3>Title: AquaSAM: Underwater Image Foreground Segmentation. (arXiv:2308.04218v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04218">http://arxiv.org/abs/2308.04218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04218]] AquaSAM: Underwater Image Foreground Segmentation(http://arxiv.org/abs/2308.04218)</code></li>
<li>Summary: <p>The Segment Anything Model (SAM) has revolutionized natural image
segmentation, nevertheless, its performance on underwater images is still
restricted. This work presents AquaSAM, the first attempt to extend the success
of SAM on underwater images with the purpose of creating a versatile method for
the segmentation of various underwater targets. To achieve this, we begin by
classifying and extracting various labels automatically in SUIM dataset.
Subsequently, we develop a straightforward fine-tuning method to adapt SAM to
general foreground underwater image segmentation. Through extensive experiments
involving eight segmentation tasks like human divers, we demonstrate that
AquaSAM outperforms the default SAM model especially at hard tasks like coral
reefs. AquaSAM achieves an average Dice Similarity Coefficient (DSC) of 7.13
(%) improvement and an average of 8.27 (%) on mIoU improvement in underwater
segmentation tasks.
</p></li>
</ul>

<h3>Title: AICSD: Adaptive Inter-Class Similarity Distillation for Semantic Segmentation. (arXiv:2308.04243v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04243">http://arxiv.org/abs/2308.04243</a></li>
<li>Code URL: https://github.com/amirmansurian/aicsd</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04243]] AICSD: Adaptive Inter-Class Similarity Distillation for Semantic Segmentation(http://arxiv.org/abs/2308.04243)</code></li>
<li>Summary: <p>In recent years, deep neural networks have achieved remarkable accuracy in
computer vision tasks. With inference time being a crucial factor, particularly
in dense prediction tasks such as semantic segmentation, knowledge distillation
has emerged as a successful technique for improving the accuracy of lightweight
student networks. The existing methods often neglect the information in
channels and among different classes. To overcome these limitations, this paper
proposes a novel method called Inter-Class Similarity Distillation (ICSD) for
the purpose of knowledge distillation. The proposed method transfers high-order
relations from the teacher network to the student network by independently
computing intra-class distributions for each class from network outputs. This
is followed by calculating inter-class similarity matrices for distillation
using KL divergence between distributions of each pair of classes. To further
improve the effectiveness of the proposed method, an Adaptive Loss Weighting
(ALW) training strategy is proposed. Unlike existing methods, the ALW strategy
gradually reduces the influence of the teacher network towards the end of
training process to account for errors in teacher's predictions. Extensive
experiments conducted on two well-known datasets for semantic segmentation,
Cityscapes and Pascal VOC 2012, validate the effectiveness of the proposed
method in terms of mIoU and pixel accuracy. The proposed method outperforms
most of existing knowledge distillation methods as demonstrated by both
quantitative and qualitative evaluations. Code is available at:
https://github.com/AmirMansurian/AICSD
</p></li>
</ul>

<h3>Title: All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation. (arXiv:2308.04321v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04321">http://arxiv.org/abs/2308.04321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04321]] All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation(http://arxiv.org/abs/2308.04321)</code></li>
<li>Summary: <p>In this work, we propose a new transformer-based regularization to better
localize objects for Weakly supervised semantic segmentation (WSSS). In
image-level WSSS, Class Activation Map (CAM) is adopted to generate object
localization as pseudo segmentation labels. To address the partial activation
issue of the CAMs, consistency regularization is employed to maintain
activation intensity invariance across various image augmentations. However,
such methods ignore pair-wise relations among regions within each CAM, which
capture context and should also be invariant across image views. To this end,
we propose a new all-pairs consistency regularization (ACR). Given a pair of
augmented views, our approach regularizes the activation intensities between a
pair of augmented views, while also ensuring that the affinity across regions
within each view remains consistent. We adopt vision transformers as the
self-attention mechanism naturally embeds pair-wise affinity. This enables us
to simply regularize the distance between the attention matrices of augmented
image pairs. Additionally, we introduce a novel class-wise localization method
that leverages the gradients of the class token. Our method can be seamlessly
integrated into existing WSSS methods using transformers without modifying the
architectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Our
method produces noticeably better class localization maps (67.3% mIoU on PASCAL
VOC train), resulting in superior WSSS performances.
</p></li>
</ul>

<h3>Title: Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs. (arXiv:2308.04356v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04356">http://arxiv.org/abs/2308.04356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04356]] Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs(http://arxiv.org/abs/2308.04356)</code></li>
<li>Summary: <p>Automatic segmentation of knee bony anatomy is essential in orthopedics, and
it has been around for several years in both pre-operative and post-operative
settings. While deep learning algorithms have demonstrated exceptional
performance in medical image analysis, the assessment of fairness and potential
biases within these models remains limited. This study aims to revisit deep
learning-powered knee-bony anatomy segmentation using plain radiographs to
uncover visible gender and racial biases. The current contribution offers the
potential to advance our understanding of biases, and it provides practical
insights for researchers and practitioners in medical imaging. The proposed
mitigation strategies mitigate gender and racial biases, ensuring fair and
unbiased segmentation results. Furthermore, this work promotes equal access to
accurate diagnoses and treatment outcomes for diverse patient populations,
fostering equitable and inclusive healthcare provision.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
