<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-05</h1>
<h3>Title: Team PA-VCG's Solution for Competition on Understanding Chinese College Entrance Exam Papers in ICDAR'25</h3>
<ul>
<li><strong>Authors: </strong>Wei Wu, Wenjie Wang, Yang Tan, Ying Liu, Liang Diao, Lin Huang, Kaihe Xu, Wenfeng Xie, Ziling Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00834">https://arxiv.org/abs/2508.00834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00834">https://arxiv.org/pdf/2508.00834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00834]] Team PA-VCG's Solution for Competition on Understanding Chinese College Entrance Exam Papers in ICDAR'25(https://arxiv.org/abs/2508.00834)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This report presents Team PA-VGG's solution for the ICDAR'25 Competition on Understanding Chinese College Entrance Exam Papers. In addition to leveraging high-resolution image processing and a multi-image end-to-end input strategy to address the challenges of dense OCR extraction and complex document layouts in Gaokao papers, our approach introduces domain-specific post-training strategies. Experimental results demonstrate that our post-training approach achieves the most outstanding performance, securing first place with an accuracy rate of 89.6%.</li>
</ul>

<h3>Title: PCS Workflow for Veridical Data Science in the Age of AI</h3>
<ul>
<li><strong>Authors: </strong>Zachary T. Rewolinski, Bin Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00835">https://arxiv.org/abs/2508.00835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00835">https://arxiv.org/pdf/2508.00835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00835]] PCS Workflow for Veridical Data Science in the Age of AI(https://arxiv.org/abs/2508.00835)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data science is a pillar of artificial intelligence (AI), which is transforming nearly every domain of human activity, from the social and physical sciences to engineering and medicine. While data-driven findings in AI offer unprecedented power to extract insights and guide decision-making, many are difficult or impossible to replicate. A key reason for this challenge is the uncertainty introduced by the many choices made throughout the data science life cycle (DSLC). Traditional statistical frameworks often fail to account for this uncertainty. The Predictability-Computability-Stability (PCS) framework for veridical (truthful) data science offers a principled approach to addressing this challenge throughout the DSLC. This paper presents an updated and streamlined PCS workflow, tailored for practitioners and enhanced with guided use of generative AI. We include a running example to display the PCS framework in action, and conduct a related case study which showcases the uncertainty in downstream predictions caused by judgment calls in the data cleaning stage.</li>
</ul>

<h3>Title: Quantum-Resistant RSA Modulus Decomposition via Adaptive Rényi Entropy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ruopengyu Xu, Chenglian Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, math.NT, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00840">https://arxiv.org/abs/2508.00840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00840">https://arxiv.org/pdf/2508.00840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00840]] Quantum-Resistant RSA Modulus Decomposition via Adaptive Rényi Entropy Optimization(https://arxiv.org/abs/2508.00840)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This paper establishes a rigorous theoretical foundation for enhancing RSA's quantum resistance through adaptive Rényi entropy optimization in modulus decomposition. We introduce a novel number-theoretic framework that fundamentally alters RSA's vulnerability landscape against Shor's algorithm by strategically constraining prime selection to minimize Rényi entropy $\mathscr{H}_2$. Our approach features three fundamental innovations: (1) a quantum-number theoretic security model establishing an exponential relationship between prime distribution asymmetry and quantum attack complexity, (2) an adaptive prime generation algorithm producing $\mathscr{H}_2$-optimized moduli with provable security guarantees, and (3) a security reduction proof demonstrating computational equivalence to lattice-based schemes under quantum random oracle model. Theoretical analysis proves our construction achieves $\Omega(2^{k/3})$ quantum attack complexity for $k$-bit moduli while maintaining classical security assumptions equivalent to standard RSA. \textbf{Key Enhancements in Revision:} (1) Prime existence proof for critical parameter $\gamma < 2^{-k/6}$ via Bombieri-Vinogradov theorem (Theorem 3.1), (2) Explicit lattice embedding construction for Ring-LWE reduction (Theorem 5.3), (3) Quantum Fano bound for information-theoretic security (Theorem 6.3).</li>
</ul>

<h3>Title: Inclusive Review on Advances in Masked Human Face Recognition Technologies</h3>
<ul>
<li><strong>Authors: </strong>Ali Haitham Abdul Amir, Zainab N. Nemer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00841">https://arxiv.org/abs/2508.00841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00841">https://arxiv.org/pdf/2508.00841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00841]] Inclusive Review on Advances in Masked Human Face Recognition Technologies(https://arxiv.org/abs/2508.00841)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, biometric, extraction</a></li>
<li><strong>Abstract: </strong>Masked Face Recognition (MFR) is an increasingly important area in biometric recognition technologies, especially with the widespread use of masks as a result of the COVID-19 pandemic. This development has created new challenges for facial recognition systems due to the partial concealment of basic facial features. This paper aims to provide a comprehensive review of the latest developments in the field, with a focus on deep learning techniques, especially convolutional neural networks (CNNs) and twin networks (Siamese networks), which have played a pivotal role in improving the accuracy of covering face recognition. The paper discusses the most prominent challenges, which include changes in lighting, different facial positions, partial concealment, and the impact of mask types on the performance of systems. It also reviews advanced technologies developed to overcome these challenges, including data enhancement using artificial databases and multimedia methods to improve the ability of systems to generalize. In addition, the paper highlights advance in deep network design, feature extraction techniques, evaluation criteria, and data sets used in this area. Moreover, it reviews the various applications of masked face recognition in the fields of security and medicine, highlighting the growing importance of these systems in light of recurrent health crises and increasing security threats. Finally, the paper focuses on future research trends such as developing more efficient algorithms and integrating multimedia technologies to improve the performance of recognition systems in real-world environments and expand their applications.</li>
</ul>

<h3>Title: eBPF-Based Real-Time DDoS Mitigation for IoT Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Abdurrahman Tolay</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00851">https://arxiv.org/abs/2508.00851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00851">https://arxiv.org/pdf/2508.00851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00851]] eBPF-Based Real-Time DDoS Mitigation for IoT Edge Devices(https://arxiv.org/abs/2508.00851)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>The rapid expansion of the Internet of Things (IoT) has intensified security challenges, notably from Distributed Denial of Service (DDoS) attacks launched by compromised, resource-constrained devices. Traditional defenses are often ill-suited for the IoT paradigm, creating a need for lightweight, high-performance, edge-based solutions. This paper presents the design, implementation, and evaluation of an IoT security framework that leverages the extended Berkeley Packet Filter (eBPF) and the eXpress Data Path (XDP) for in-kernel mitigation of DDoS attacks. The system uses a rate-based detection algorithm to identify and block malicious traffic at the earliest stage of the network stack. The framework is evaluated using both Docker-based simulations and real-world deployment on a Raspberry Pi 4, showing over 97% mitigation effectiveness under a 100 Mbps flood. Legitimate traffic remains unaffected, and system stability is preserved even under attack. These results confirm that eBPF/XDP provides a viable and highly efficient solution for hardening IoT edge devices against volumetric network attacks.</li>
</ul>

<h3>Title: A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Zhang, Feifan Zhang, Weidong Tang, Lei Shi, Tailai Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00855">https://arxiv.org/abs/2508.00855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00855">https://arxiv.org/pdf/2508.00855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00855]] A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks(https://arxiv.org/abs/2508.00855)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Nonlinear partial differential equations (PDEs) are pivotal in modeling complex physical systems, yet traditional Physics-Informed Neural Networks (PINNs) often struggle with unresolved residuals in critical spatiotemporal regions and violations of temporal causality. To address these limitations, we propose a novel Residual Guided Training strategy for Physics-Informed Transformer via Generative Adversarial Networks (GAN). Our framework integrates a decoder-only Transformer to inherently capture temporal correlations through autoregressive processing, coupled with a residual-aware GAN that dynamically identifies and prioritizes high-residual regions. By introducing a causal penalty term and an adaptive sampling mechanism, the method enforces temporal causality while refining accuracy in problematic domains. Extensive numerical experiments on the Allen-Cahn, Klein-Gordon, and Navier-Stokes equations demonstrate significant improvements, achieving relative MSE reductions of up to three orders of magnitude compared to baseline methods. This work bridges the gap between deep learning and physics-driven modeling, offering a robust solution for multiscale and time-dependent PDE systems.</li>
</ul>

<h3>Title: Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches</h3>
<ul>
<li><strong>Authors: </strong>Margarita Bugueño, Gerard de Melo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00864">https://arxiv.org/abs/2508.00864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00864">https://arxiv.org/pdf/2508.00864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00864]] Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches(https://arxiv.org/abs/2508.00864)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In document classification, graph-based models effectively capture document structure, overcoming sequence length limitations and enhancing contextual understanding. However, most existing graph document representations rely on heuristics, domain-specific rules, or expert knowledge. Unlike previous approaches, we propose a method to learn data-driven graph structures, eliminating the need for manual design and reducing domain dependence. Our approach constructs homogeneous weighted graphs with sentences as nodes, while edges are learned via a self-attention model that identifies dependencies between sentence pairs. A statistical filtering strategy aims to retain only strongly correlated sentences, improving graph quality while reducing the graph size. Experiments on three document classification datasets demonstrate that learned graphs consistently outperform heuristic-based graphs, achieving higher accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness of the statistical filtering in improving classification robustness. These results highlight the potential of automatic graph generation over traditional heuristic approaches and open new directions for broader applications in NLP.</li>
</ul>

<h3>Title: A Data-Driven Machine Learning Approach for Predicting Axial Load Capacity in Steel Storage Rack Columns</h3>
<ul>
<li><strong>Authors: </strong>Bakhtiyar Mammadli, Casim Yazici, Muhammed Gürbüz, İrfan Kocaman, F. Javier Dominguez-Gutierrez, Fatih Mehmet Özkal</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00876">https://arxiv.org/abs/2508.00876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00876">https://arxiv.org/pdf/2508.00876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00876]] A Data-Driven Machine Learning Approach for Predicting Axial Load Capacity in Steel Storage Rack Columns(https://arxiv.org/abs/2508.00876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>In this study, we present a machine learning (ML) framework to predict the axial load-bearing capacity, (kN), of cold-formed steel structural members. The methodology emphasizes robust model selection and interpretability, addressing the limitations of traditional analytical approaches in capturing the nonlinearities and geometrical complexities inherent to buckling behavior. The dataset, comprising key geometric and mechanical parameters of steel columns, was curated with appropriate pre-processing steps including removal of non-informative identifiers and imputation of missing values. A comprehensive suite of regression algorithms, ranging from linear models to kernel-based regressors and ensemble tree methods was evaluated. Among these, Gradient Boosting Regression exhibited superior predictive performance across multiple metrics, including the coefficient of determination (R2), root mean squared error (RMSE), and mean absolute error (MAE), and was consequently selected as the final model. Model interpretability was addressed using SHapley Additive exPlanations (SHAP), enabling insight into the relative importance and interaction of input features influencing the predicted axial capacity. To facilitate practical deployment, the model was integrated into an interactive, Python-based web interface via Streamlit. This tool allows end-users-such as structural engineers and designers, to input design parameters manually or through CSV upload, and to obtain real-time predictions of axial load capacity without the need for programming expertise. Applied to the context of steel storage rack columns, the framework demonstrates how data-driven tools can enhance design safety, streamline validation workflows, and inform decision-making in structural applications where buckling is a critical failure mode</li>
</ul>

<h3>Title: GNN-ASE: Graph-Based Anomaly Detection and Severity Estimation in Three-Phase Induction Machines</h3>
<ul>
<li><strong>Authors: </strong>Moutaz Bellah Bentrad, Adel Ghoggal, Tahar Bahi, Abderaouf Bahi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00879">https://arxiv.org/abs/2508.00879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00879">https://arxiv.org/pdf/2508.00879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00879]] GNN-ASE: Graph-Based Anomaly Detection and Severity Estimation in Three-Phase Induction Machines(https://arxiv.org/abs/2508.00879)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>The diagnosis of induction machines has traditionally relied on model-based methods that require the development of complex dynamic models, making them difficult to implement and computationally expensive. To overcome these limitations, this paper proposes a model-free approach using Graph Neural Networks (GNNs) for fault diagnosis in induction machines. The focus is on detecting multiple fault types -- including eccentricity, bearing defects, and broken rotor bars -- under varying severity levels and load conditions. Unlike traditional approaches, raw current and vibration signals are used as direct inputs, eliminating the need for signal preprocessing or manual feature extraction. The proposed GNN-ASE model automatically learns and extracts relevant features from raw inputs, leveraging the graph structure to capture complex relationships between signal types and fault patterns. It is evaluated for both individual fault detection and multi-class classification of combined fault conditions. Experimental results demonstrate the effectiveness of the proposed model, achieving 92.5\% accuracy for eccentricity defects, 91.2\% for bearing faults, and 93.1\% for broken rotor bar detection. These findings highlight the model's robustness and generalization capability across different operational scenarios. The proposed GNN-based framework offers a lightweight yet powerful solution that simplifies implementation while maintaining high diagnostic performance. It stands as a promising alternative to conventional model-based diagnostic techniques for real-world induction machine monitoring and predictive maintenance.</li>
</ul>

<h3>Title: Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Vijja Wichitwechkarn, Charles Fox, Ruchi Choudhary</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00881">https://arxiv.org/abs/2508.00881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00881">https://arxiv.org/pdf/2508.00881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00881]] Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models(https://arxiv.org/abs/2508.00881)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Foundation models for natural language processing have many coherent definitions of hallucination and methods for its detection and mitigation. However, analogous definitions and methods do not exist for multi-variate time-series (MVTS) foundation models. We propose new definitions for MVTS hallucination, along with new detection and mitigation methods using a diffusion model to estimate hallucination levels. We derive relational datasets from popular time-series datasets to benchmark these relational hallucination levels. Using these definitions and models, we find that open-source pre-trained MVTS imputation foundation models relationally hallucinate on average up to 59.5% as much as a weak baseline. The proposed mitigation method reduces this by up to 47.7% for these models. The definition and methods may improve adoption and safe usage of MVTS foundation models.</li>
</ul>

<h3>Title: Multi-Grained Temporal-Spatial Graph Learning for Stable Traffic Flow Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhenan Lin, Yuni Lai, Wai Lun Lo, Richard Tai-Chiu Hsung, Harris Sik-Ho Tsang, Xiaoyu Xue, Kai Zhou, Yulin Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00884">https://arxiv.org/abs/2508.00884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00884">https://arxiv.org/pdf/2508.00884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00884]] Multi-Grained Temporal-Spatial Graph Learning for Stable Traffic Flow Forecasting(https://arxiv.org/abs/2508.00884)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Time-evolving traffic flow forecasting are playing a vital role in intelligent transportation systems and smart cities. However, the dynamic traffic flow forecasting is a highly nonlinear problem with complex temporal-spatial dependencies. Although the existing methods has provided great contributions to mine the temporal-spatial patterns in the complex traffic networks, they fail to encode the globally temporal-spatial patterns and are prone to overfit on the pre-defined geographical correlations, and thus hinder the model's robustness on the complex traffic environment. To tackle this issue, in this work, we proposed a multi-grained temporal-spatial graph learning framework to adaptively augment the globally temporal-spatial patterns obtained from a crafted graph transformer encoder with the local patterns from the graph convolution by a crafted gated fusion unit with residual connection techniques. Under these circumstances, our proposed model can mine the hidden global temporal-spatial relations between each monitor stations and balance the relative importance of local and global temporal-spatial patterns. Experiment results demonstrate the strong representation capability of our proposed method and our model consistently outperforms other strong baselines on various real-world traffic networks.</li>
</ul>

<h3>Title: Stochastic Optimal Control via Measure Relaxations</h3>
<ul>
<li><strong>Authors: </strong>Etienne Buehrle, Christoph Stiller</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00886">https://arxiv.org/abs/2508.00886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00886">https://arxiv.org/pdf/2508.00886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00886]] Stochastic Optimal Control via Measure Relaxations(https://arxiv.org/abs/2508.00886)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The optimal control problem of stochastic systems is commonly solved via robust or scenario-based optimization methods, which are both challenging to scale to long optimization horizons. We cast the optimal control problem of a stochastic system as a convex optimization problem over occupation measures. We demonstrate our method on a set of synthetic and real-world scenarios, learning cost functions from data via Christoffel polynomials. The code for our experiments is available at this https URL.</li>
</ul>

<h3>Title: A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data</h3>
<ul>
<li><strong>Authors: </strong>Amir Hossein Kalantari, Eleonora Papadimitriou, Amir Pooyan Afghari</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.CO, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00888">https://arxiv.org/abs/2508.00888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00888">https://arxiv.org/pdf/2508.00888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00888]] A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data(https://arxiv.org/abs/2508.00888)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Naturalistic driving studies offer a powerful means for observing and quantifying real-world driving behaviour. One of their prominent applications in traffic safety is the continuous monitoring and classification of risky driving behaviour. However, many existing frameworks rely on fixed time windows and static thresholds for distinguishing between safe and risky behaviour - limiting their ability to respond to the stochastic nature of real-world driving. This study proposes a dynamic and individualised framework for identifying risky driving behaviour using Belgian naturalistic driving data. The approach leverages a rolling time window and bi-level optimisation to dynamically calibrate both risk thresholds and model hyperparameters, capturing subtle behavioural shifts. Two safety indicators, speed-weighted headway and harsh driving events, were evaluated using three data-driven models: Random Forest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong capability in capturing subtle changes in driving behaviour, particularly excelling in high-recall tasks, making it promising for early-stage risk detection. XGBoost provided the most balanced and stable performance across different thresholds and evaluation metrics. While random forest showed more variability, it responded sensitively to dynamic threshold adjustments, which may be advantageous during model adaptation or tuning. Speed-weighted headway emerged as a more stable and context-sensitive risk indicator than harsh driving events, likely due to its robustness to label sparsity and contextual variation. Overall, the findings support the value of adaptive, personalised risk detection approaches for enhancing real-time safety feedback and tailoring driver support in intelligent transport systems.</li>
</ul>

<h3>Title: FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts</h3>
<ul>
<li><strong>Authors: </strong>Hagyeong Shin, Binoy Robin Dalal, Iwona Bialynicka-Birula, Navjot Matharu, Ryan Muir, Xingwei Yang, Samuel W. K. Wong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00889">https://arxiv.org/abs/2508.00889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00889">https://arxiv.org/pdf/2508.00889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00889]] FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts(https://arxiv.org/abs/2508.00889)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known to hallucinate, producing natural language outputs that are not grounded in the input, reference materials, or real-world knowledge. In enterprise applications where AI features support business decisions, such hallucinations can be particularly detrimental. LLMs that analyze and summarize contact center conversations introduce a unique set of challenges for factuality evaluation, because ground-truth labels often do not exist for analytical interpretations about sentiments captured in the conversation and root causes of the business problems. To remedy this, we first introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in the human annotation guideline and the LLM-judges' prompt to ground the factuality labels in linguistically-informed evaluation criteria. We then introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality \textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm. Lastly, we report our findings from aligning LLM-judges on the 3D paradigm. Overall, our findings contribute a new approach for automatically evaluating the factuality of outputs generated by an AI system for analyzing contact center conversations.</li>
</ul>

<h3>Title: HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Zhu, Jiale Han, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00892">https://arxiv.org/abs/2508.00892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00892">https://arxiv.org/pdf/2508.00892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00892]] HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models(https://arxiv.org/abs/2508.00892)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, steal, membership infer, watermark</a></li>
<li><strong>Abstract: </strong>Image-based AI models are increasingly deployed across a wide range of domains, including healthcare, security, and consumer applications. However, many image datasets carry sensitive or proprietary content, raising critical concerns about unauthorized data usage. Data owners therefore need reliable mechanisms to verify whether their proprietary data has been misused to train third-party models. Existing solutions, such as backdoor watermarking and membership inference, face inherent trade-offs between verification effectiveness and preservation of data integrity. In this work, we propose HoneyImage, a novel method for dataset ownership verification in image recognition models. HoneyImage selectively modifies a small number of hard samples to embed imperceptible yet verifiable traces, enabling reliable ownership verification while maintaining dataset integrity. Extensive experiments across four benchmark datasets and multiple model architectures show that HoneyImage consistently achieves strong verification accuracy with minimal impact on downstream performance while maintaining imperceptible. The proposed HoneyImage method could provide data owners with a practical mechanism to protect ownership over valuable image datasets, encouraging safe sharing and unlocking the full transformative potential of data-driven AI.</li>
</ul>

<h3>Title: Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis</h3>
<ul>
<li><strong>Authors: </strong>Hoang Hai Nam Nguyen, Minh Tien Tran, Hoheok Kim, Ho Won Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00896">https://arxiv.org/abs/2508.00896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00896">https://arxiv.org/pdf/2508.00896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00896]] Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis(https://arxiv.org/abs/2508.00896)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>The effectiveness of machine learning in metallographic microstructure segmentation is often constrained by the lack of human-annotated phase masks, particularly for rare or compositionally complex morphologies within the metal alloy. We introduce PF-DiffSeg, a phase-fraction controlled, one-stage denoising diffusion framework that jointly synthesizes microstructure images and their corresponding segmentation masks in a single generative trajectory to further improve segmentation accuracy. By conditioning on global phase-fraction vectors, augmented to represent real data distribution and emphasize minority classes, our model generates compositionally valid and structurally coherent microstructure image and mask samples that improve both data diversity and training efficiency. Evaluated on the MetalDAM benchmark for additively manufactured multiphase steel, our synthetic augmentation method yields notable improvements in segmentation accuracy compared to standard augmentation strategies especially in minority classes and further outperforms a two-stage mask-guided diffusion and generative adversarial network (GAN) baselines, while also reducing inference time compared to conventional approach. The method integrates generation and conditioning into a unified framework, offering a scalable solution for data augmentation in metallographic applications.</li>
</ul>

<h3>Title: Maximize margins for robust splicing detection</h3>
<ul>
<li><strong>Authors: </strong>Julien Simon de Kergunic (CRIStAL), Rony Abecidan (CRIStAL), Patrick Bas (CRIStAL), Vincent Itier (IMT Nord Europe, CRIStAL)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00897">https://arxiv.org/abs/2508.00897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00897">https://arxiv.org/pdf/2508.00897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00897]] Maximize margins for robust splicing detection(https://arxiv.org/abs/2508.00897)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite recent progress in splicing detection, deep learning-based forensic tools remain difficult to deploy in practice due to their high sensitivity to training conditions. Even mild post-processing applied to evaluation images can significantly degrade detector performance, raising concerns about their reliability in operational contexts. In this work, we show that the same deep architecture can react very differently to unseen post-processing depending on the learned weights, despite achieving similar accuracy on in-distribution test data. This variability stems from differences in the latent spaces induced by training, which affect how samples are separated internally. Our experiments reveal a strong correlation between the distribution of latent margins and a detector's ability to generalize to post-processed images. Based on this observation, we propose a practical strategy for building more robust detectors: train several variants of the same model under different conditions, and select the one that maximizes latent margins.</li>
</ul>

<h3>Title: Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Jose M. Sánchez Velázquez, Mingbo Cai, Andrew Coney, Álvaro J. García- Tejedor, Alberto Nogales</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00898">https://arxiv.org/abs/2508.00898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00898">https://arxiv.org/pdf/2508.00898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00898]] Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models(https://arxiv.org/abs/2508.00898)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In recent years, advances in Artificial Intelligence have significantly impacted computer science, particularly in the field of computer vision, enabling solutions to complex problems such as video frame prediction. Video frame prediction has critical applications in weather forecasting or autonomous systems and can provide technical improvements, such as video compression and streaming. Among Artificial Intelligence methods, Deep Learning has emerged as highly effective for solving vision-related tasks, although current frame prediction models still have room for enhancement. This paper evaluates several hybrid deep learning approaches that combine the feature extraction capabilities of autoencoders with temporal sequence modelling using Recurrent Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related architectures. The proposed solutions were rigorously evaluated on three datasets that differ in terms of synthetic versus real-world scenarios and grayscale versus color imagery. Results demonstrate that the approaches perform well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale videos with real data are the easiest to predict.</li>
</ul>

<h3>Title: Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Ruichen Xu, Kexin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00901">https://arxiv.org/abs/2508.00901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00901">https://arxiv.org/pdf/2508.00901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00901]] Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge(https://arxiv.org/abs/2508.00901)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Modern large language models excel in knowledge-intensive tasks, yet how transformers acquire (store) knowledge during pre-training and extract (retrieve) it during post-fine-tuning inference remains theoretically opaque. While prior theoretical work has begun to investigate these questions through the analysis of training dynamics, such studies are limited to single-layer, attention-only architectures. However, most existing studies suggest that MLPs are the most contributing components for storing knowledge in transformer-based language models. Meanwhile, our empirical investigations reveal that such simplified models, when trained using standard next-token prediction objectives, may be incapable of acquiring or extracting factual knowledge. To overcome this limitation, we introduce a tractable one-layer transformer framework that crucially incorporates both self-attention and MLP modules. By tracking its gradient dynamics, we establish convergence and generalization guarantees that illuminate the ability of knowledge acquisition and extraction. We prove that 1) Transformers can achieve near-optimal training loss during pre-training, signifying effective knowledge acquisition; 2) With a large fine-tuning dataset and specific data multiplicity conditions met, transformers can achieve low generalization error when tested on factual knowledge learned during pre-training but not reinforced during the fine-tuning, indicating successful knowledge extraction; 3) When the conditions are not satisfied, transformers exhibit high generalization loss, resulting in hallucinations. Our analysis includes both full fine-tuning and low-rank fine-tuning. Furthermore, our analysis offers theoretical insights into several pertinent empirical phenomena, such as the role of learning rate schedules. Experiments on synthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate our results.</li>
</ul>

<h3>Title: Cyber-Zero: Training Cybersecurity Agents without Runtime</h3>
<ul>
<li><strong>Authors: </strong>Terry Yue Zhuo, Dingmin Wang, Hantian Ding, Varun Kumar, Zijian Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00910">https://arxiv.org/abs/2508.00910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00910">https://arxiv.org/pdf/2508.00910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00910]] Cyber-Zero: Training Cybersecurity Agents without Runtime(https://arxiv.org/abs/2508.00910)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.</li>
</ul>

<h3>Title: TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Mohammadi, Ziyi Wu, Igor Gilitschenski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00913">https://arxiv.org/abs/2508.00913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00913">https://arxiv.org/pdf/2508.00913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00913]] TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras(https://arxiv.org/abs/2508.00913)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Long-term temporal information is crucial for event-based perception tasks, as raw events only encode pixel brightness changes. Recent works show that when trained from scratch, recurrent models achieve better results than feedforward models in these tasks. However, when leveraging self-supervised pre-trained weights, feedforward models can outperform their recurrent counterparts. Current self-supervised learning (SSL) methods for event-based pre-training largely mimic RGB image-based approaches. They pre-train feedforward models on raw events within a short time interval, ignoring the temporal information of events. In this work, we introduce TESPEC, a self-supervised pre-training framework tailored for learning spatio-temporal information. TESPEC is well-suited for recurrent models, as it is the first framework to leverage long event sequences during pre-training. TESPEC employs the masked image modeling paradigm with a new reconstruction target. We design a novel method to accumulate events into pseudo grayscale videos containing high-level semantic information about the underlying scene, which is robust to sensor noise and reduces motion blur. Reconstructing this target thus requires the model to reason about long-term history of events. Extensive experiments demonstrate our state-of-the-art results in downstream tasks, including object detection, semantic segmentation, and monocular depth estimation. Project webpage: this https URL.</li>
</ul>

<h3>Title: Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiazhen Pan, Bailiang Jian, Paul Hager, Yundi Zhang, Che Liu, Friedrike Jungmann, Hongwei Bran Li, Chenyu You, Junde Wu, Jiayuan Zhu, Fenglin Liu, Yuyuan Liu, Niklas Bubeck, Christian Wachinger, Chen (Cherise)Chen, Zhenyu Gong, Cheng Ouyang, Georgios Kaissis, Benedikt Wiestler, Daniel Rueckert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00923">https://arxiv.org/abs/2508.00923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00923">https://arxiv.org/pdf/2508.00923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00923]] Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models(https://arxiv.org/abs/2508.00923)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the safety and reliability of large language models (LLMs) in clinical practice is critical to prevent patient harm and promote trustworthy healthcare applications of AI. However, LLMs are advancing so rapidly that static safety benchmarks often become obsolete upon publication, yielding only an incomplete and sometimes misleading picture of model trustworthiness. We demonstrate that a Dynamic, Automatic, and Systematic (DAS) red-teaming framework that continuously stress-tests LLMs can reveal significant weaknesses of current LLMs across four safety-critical domains: robustness, privacy, bias/fairness, and hallucination. A suite of adversarial agents is applied to autonomously mutate test cases, identify/evolve unsafe-triggering strategies, and evaluate responses, uncovering vulnerabilities in real time without human intervention. Applying DAS to 15 proprietary and open-source LLMs revealed a stark contrast between static benchmark performance and vulnerability under adversarial pressure. Despite a median MedQA accuracy exceeding 80\%, 94\% of previously correct answers failed our dynamic robustness tests. We observed similarly high failure rates across other domains: privacy leaks were elicited in 86\% of scenarios, cognitive-bias priming altered clinical recommendations in 81\% of fairness tests, and we identified hallucination rates exceeding 66\% in widely used models. Such profound residual risks are incompatible with routine clinical practice. By converting red-teaming from a static checklist into a dynamic stress-test audit, DAS red-teaming offers the surveillance that hospitals/regulators/technology vendors require as LLMs become embedded in patient chatbots, decision-support dashboards, and broader healthcare workflows. Our framework delivers an evolvable, scalable, and reliable safeguard for the next generation of medical AI.</li>
</ul>

<h3>Title: XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML</h3>
<ul>
<li><strong>Authors: </strong>Ernesto L. Estevanell-Valladares, Suilan Estevez-Velarde, Yoan Gutiérrez, Andrés Montoyo, Ruslan Mitkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00924">https://arxiv.org/abs/2508.00924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00924">https://arxiv.org/pdf/2508.00924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00924]] XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML(https://arxiv.org/abs/2508.00924)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Experts in machine learning leverage domain knowledge to navigate decisions in model selection, hyperparameter optimisation, and resource allocation. This is particularly critical for fine-tuning language models (LMs), where repeated trials incur substantial computational overhead and environmental impact. However, no existing automated framework simultaneously tackles the entire model selection and HPO task for resource-efficient LM fine-tuning. We introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past experiences to optimise discriminative and generative LM fine-tuning pipelines efficiently. XAutoLM learns from stored successes and failures by extracting task- and system-level meta-features to bias its sampling toward fruitful configurations and away from costly dead ends. On four text classification and two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front. In contrast, simpler memory-based baselines suffer negative transfer. We release XAutoLM and our experience store to catalyse resource-efficient, Green AI fine-tuning in the NLP community.</li>
</ul>

<h3>Title: Hybrid Hypergraph Networks for Multimodal Sequence Data Classification</h3>
<ul>
<li><strong>Authors: </strong>Feng Xu, Hui Wang, Yuting Huang, Danwei Zhang, Zizhu Fan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00926">https://arxiv.org/abs/2508.00926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00926">https://arxiv.org/pdf/2508.00926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00926]] Hybrid Hypergraph Networks for Multimodal Sequence Data Classification(https://arxiv.org/abs/2508.00926)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Modeling temporal multimodal data poses significant challenges in classification tasks, particularly in capturing long-range temporal dependencies and intricate cross-modal interactions. Audiovisual data, as a representative example, is inherently characterized by strict temporal order and diverse modalities. Effectively leveraging the temporal structure is essential for understanding both intra-modal dynamics and inter-modal correlations. However, most existing approaches treat each modality independently and rely on shallow fusion strategies, which overlook temporal dependencies and hinder the model's ability to represent complex structural relationships. To address the limitation, we propose the hybrid hypergraph network (HHN), a novel framework that models temporal multimodal data via a segmentation-first, graph-later strategy. HHN splits sequences into timestamped segments as nodes in a heterogeneous graph. Intra-modal structures are captured via hyperedges guided by a maximum entropy difference criterion, enhancing node heterogeneity and structural discrimination, followed by hypergraph convolution to extract high-order dependencies. Inter-modal links are established through temporal alignment and graph attention for semantic fusion. HHN achieves state-of-the-art (SOTA) results on four multimodal datasets, demonstrating its effectiveness in complex classification tasks.</li>
</ul>

<h3>Title: OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hanchen Yang, Jiaqi Wang, Jiannong Cao, Wengen Li, Jialun Zheng, Yangning Li, Chunyu Miao, Jihong Guan, Shuigeng Zhou, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00933">https://arxiv.org/abs/2508.00933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00933">https://arxiv.org/pdf/2508.00933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00933]] OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction(https://arxiv.org/abs/2508.00933)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Sea surface temperature (SST) prediction is a critical task in ocean science, supporting various applications, such as weather forecasting, fisheries management, and storm tracking. While existing data-driven methods have demonstrated significant success, they often neglect to leverage the rich domain knowledge accumulated over the past decades, limiting further advancements in prediction accuracy. The recent emergence of large language models (LLMs) has highlighted the potential of integrating domain knowledge for downstream tasks. However, the application of LLMs to SST prediction remains underexplored, primarily due to the challenge of integrating ocean domain knowledge and numerical data. To address this issue, we propose Ocean Knowledge Graph-enhanced LLM (OKG-LLM), a novel framework for global SST prediction. To the best of our knowledge, this work presents the first systematic effort to construct an Ocean Knowledge Graph (OKG) specifically designed to represent diverse ocean knowledge for SST prediction. We then develop a graph embedding network to learn the comprehensive semantic and structural knowledge within the OKG, capturing both the unique characteristics of individual sea regions and the complex correlations between them. Finally, we align and fuse the learned knowledge with fine-grained numerical SST data and leverage a pre-trained LLM to model SST patterns for accurate prediction. Extensive experiments on the real-world dataset demonstrate that OKG-LLM consistently outperforms state-of-the-art methods, showcasing its effectiveness, robustness, and potential to advance SST prediction. The codes are available in the online repository.</li>
</ul>

<h3>Title: How Cybersecurity Behaviors affect the Success of Darknet Drug Vendors: A Quantitative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Syon Balakrishnan, Aaron Grinberg</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00934">https://arxiv.org/abs/2508.00934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00934">https://arxiv.org/pdf/2508.00934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00934]] How Cybersecurity Behaviors affect the Success of Darknet Drug Vendors: A Quantitative Analysis(https://arxiv.org/abs/2508.00934)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Understanding behavioral drivers of success in illicit digital marketplaces is critical for developing effective enforcement strategies and understanding digital commerce evolution, as darknet drug markets represent a growing share of the total drug economy. This study employs quantitative regression analysis of 50,000+ listings from 2,653 vendors in the Agora marketplace (2014-2015), examining relationships between cybersecurity signaling (PGP encryption mentions), product diversification, and commercial success through nested regression specifications controlling for reputation, pricing, and category-specific factors. Product diversification emerges as the dominant predictor of vendor scale, increasing the odds of large vendor status by 169% per additional category, while PGP encryption signaling functions primarily as a professional marker rather than an independent success factor. Vendor success depends on portfolio breadth rather than specialization, with category-specific enforcement creating differential market constraints. Successful vendors operate as diversified enterprises capable of rapid pivoting between product categories, requiring targeted enforcement towards diversified vendors based on coordinated multi-category enforcement approaches rather than traditional substance-specific targeting strategies.</li>
</ul>

<h3>Title: Measuring Harmfulness of Computer-Using Agents</h3>
<ul>
<li><strong>Authors: </strong>Aaron Xuxiang Tian, Ruofan Zhang, Janet Tang, Jiaxin Wen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00935">https://arxiv.org/abs/2508.00935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00935">https://arxiv.org/pdf/2508.00935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00935]] Measuring Harmfulness of Computer-Using Agents(https://arxiv.org/abs/2508.00935)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Computer-using agents (CUAs), which autonomously control computers to perform multi-step actions, might pose significant safety risks if misused. Existing benchmarks mostly evaluate language models' (LMs) safety risks in chatbots or simple tool-usage scenarios, without granting full computer access. To better evaluate CUAs' misuse risks, we introduce a new benchmark: CUAHarm. CUAHarm consists of 104 expert-written realistic misuse risks, such as disabling firewalls, leaking confidential information, launching denial-of-service attacks, or installing backdoors. We provide a sandbox environment and rule-based verifiable rewards to measure CUAs' success rates in executing these tasks (e.g., whether the firewall is indeed disabled), not just refusal. We evaluate multiple frontier open-source and proprietary LMs, such as Claude Sonnet, GPT-4o, Gemini Pro 1.5, Llama-3.3-70B, and Mistral Large 2. Surprisingly, even without carefully designed jailbreaking prompts, these frontier LMs comply with executing these malicious tasks at a high success rate (e.g., 59% for Claude 3.7 Sonnet). Newer models show higher misuse rates: Claude 3.7 Sonnet succeeds on 15% more tasks than Claude 3.5. While these models are robust to common malicious prompts (e.g., creating a bomb) in chatbot settings, they behave unsafely as CUAs. We further evaluate a leading agentic framework (UI-TARS-1.5) and find that while it improves performance, it also amplifies misuse risks. Benign variants reveal refusals stem from alignment, not capability limits. To mitigate risks, we explore using LMs to monitor CUAs' actions and chain-of-thoughts (CoTs). Monitoring CUAs is significantly harder than chatbot outputs. Monitoring CoTs yields modest gains, with average detection accuracy at only 72%. Even with hierarchical summarization, improvement is limited to 4%. CUAHarm will be released at this https URL.</li>
</ul>

<h3>Title: Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hassan Ugail, Hamad Mansour Alawar, AbdulNasser Abbas Zehi, Ahmed Mohammad Alkendi, Ismail Lujain Jaleel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00941">https://arxiv.org/abs/2508.00941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00941">https://arxiv.org/pdf/2508.00941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00941]] Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition(https://arxiv.org/abs/2508.00941)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Face recognition systems experience severe performance degradation when processing low-quality forensic evidence imagery. This paper presents an evaluation of latent diffusion-based enhancement for improving face recognition under forensically relevant degradations. Using a dataset of 3,000 individuals from LFW with 24,000 recognition attempts, we implement the Flux.1 Kontext Dev pipeline with Facezoom LoRA adaptation to test against seven degradation categories, including compression artefacts, blur effects, and noise contamination. Our approach demonstrates substantial improvements, increasing overall recognition accuracy from 29.1% to 84.5% (55.4 percentage point improvement, 95% CI: [54.1, 56.7]). Statistical analysis reveals significant performance gains across all degradation types, with effect sizes exceeding conventional thresholds for practical significance. These findings establish the potential of sophisticated diffusion based enhancement in forensic face recognition applications.</li>
</ul>

<h3>Title: LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Chloe Li, Mary Phuong, Noah Y. Siegel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00943">https://arxiv.org/abs/2508.00943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00943">https://arxiv.org/pdf/2508.00943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00943]] LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring(https://arxiv.org/abs/2508.00943)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat to this is sandbagging - the strategic underperformance on evaluations by AI models or their developers. One promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36\% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand why the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.</li>
</ul>

<h3>Title: Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Hongfeng Ai, Quangao Liu, Maowei Jiang, Ruiyuan Kang, Ruiqi Li, Jiahua Dong, Mengting Xiao, Cheng Jiang, Chenzhong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00945">https://arxiv.org/abs/2508.00945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00945">https://arxiv.org/pdf/2508.00945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00945]] Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment(https://arxiv.org/abs/2508.00945)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) face challenges in effectively coordinating diverse attention mechanisms for cross-modal embedding learning, leading to mismatched attention and suboptimal performance. We propose Consistent Cross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross Attention (LPWCA) to capture fine-grained regional-semantic correlations by jointly weighting patch and layer-wise embedding, and Progressive Attention Integration (PAI) that systematically coordinates LPWCA, layer-wise, and patch-wise attention mechanisms in sequence. This progressive design ensures consistency from semantic to regional levels while preventing attention drift and maximizing individual attention benefits. Experimental results on ten diverse vision-language benchmarks demonstrate that our CCRA-enhanced LLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all baseline methods with only 3.55M additional parameters, while providing enhanced interpretability through more regionally focused and semantically aligned attention patterns.</li>
</ul>

<h3>Title: From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model</h3>
<ul>
<li><strong>Authors: </strong>Yeong-Joon Ju, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00955">https://arxiv.org/abs/2508.00955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00955">https://arxiv.org/pdf/2508.00955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00955]] From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model(https://arxiv.org/abs/2508.00955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have emerged as a promising solution for universal embedding tasks, yet adapting their generative nature for discriminative representation learning remains a significant challenge. The dominant paradigm of large-scale contrastive pre-training suffers from critical inefficiencies, including prohibitive computational costs and a failure to leverage the intrinsic, instruction-following capabilities of MLLMs. To overcome these limitations, we propose an efficient framework for universal multimodal embeddings, which bridges this gap by centering on two synergistic components. First, our hierarchical embedding prompt template employs a two-level instruction architecture that forces the model to produce discriminative representations. Building on this strong foundation, our second component, self-aware hard negative sampling, redefines the fine-tuning process by leveraging the model's own understanding to efficiently mine challenging negatives while actively filtering out potential false negatives. Our comprehensive experiments show that our hierarchical prompt achieves zero-shot performance competitive with contrastively trained baselines and enhances the fine-tuning process by lifting a simple in-batch negative baseline by 4.8 points on the MMEB benchmark. We further boost the performance via our self-aware hard negative sampling, achieving the state-of-the-art performance without the contrative pre-training. Our work presents an effective and efficient pathway to adapt MLLMs for universal embedding tasks, significantly reducing training time.</li>
</ul>

<h3>Title: Small sample-based adaptive text classification through iterative and contrastive description refinement</h3>
<ul>
<li><strong>Authors: </strong>Amrit Rajeev, Udayaadithya Avadhanam, Harshula Tulapurkar, SaiBarath Sundar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00957">https://arxiv.org/abs/2508.00957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00957">https://arxiv.org/pdf/2508.00957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00957]] Small sample-based adaptive text classification through iterative and contrastive description refinement(https://arxiv.org/abs/2508.00957)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Zero-shot text classification remains a difficult task in domains with evolving knowledge and ambiguous category boundaries, such as ticketing systems. Large language models (LLMs) often struggle to generalize in these scenarios due to limited topic separability, while few-shot methods are constrained by insufficient data diversity. We propose a classification framework that combines iterative topic refinement, contrastive prompting, and active learning. Starting with a small set of labeled samples, the model generates initial topic labels. Misclassified or ambiguous samples are then used in an iterative contrastive prompting process to refine category distinctions by explicitly teaching the model to differentiate between closely related classes. The framework features a human-in-the-loop component, allowing users to introduce or revise category definitions in natural language. This enables seamless integration of new, unseen categories without retraining, making the system well-suited for real-world, dynamic environments. The evaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with minimal accuracy shift after introducing unseen classes (82% and 87%, respectively). The results highlight the effectiveness of prompt-based semantic reasoning for fine-grained classification with limited supervision.</li>
</ul>

<h3>Title: Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables</h3>
<ul>
<li><strong>Authors: </strong>Rubén Muñoz-Sierra, Manuel Doblaré, Jacobo Ayensa-Jiménez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00959">https://arxiv.org/abs/2508.00959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00959">https://arxiv.org/pdf/2508.00959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00959]] Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables(https://arxiv.org/abs/2508.00959)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Physically Guided Neural Networks with Internal Variables are SciML tools that use only observable data for training and and have the capacity to unravel internal state relations. They incorporate physical knowledge both by prescribing the model architecture and using loss regularization, thus endowing certain specific neurons with a physical meaning as internal state variables. Despite their potential, these models face challenges in scalability when applied to high-dimensional data such as fine-grid spatial fields or time-evolving systems. In this work, we propose some enhancements to the PGNNIV framework that address these scalability limitations through reduced-order modeling techniques. Specifically, we introduce alternatives to the original decoder structure using spectral decomposition, POD, and pretrained autoencoder-based mappings. These surrogate decoders offer varying trade-offs between computational efficiency, accuracy, noise tolerance, and generalization, while improving drastically the scalability. Additionally, we integrate model reuse via transfer learning and fine-tuning strategies to exploit previously acquired knowledge, supporting efficient adaptation to novel materials or configurations, and significantly reducing training time while maintaining or improving model performance. To illustrate these various techniques, we use a representative case governed by the nonlinear diffusion equation, using only observable data. Results demonstrate that the enhanced PGNNIV framework successfully identifies the underlying constitutive state equations while maintaining high predictive accuracy. It also improves robustness to noise, mitigates overfitting, and reduces computational demands. The proposed techniques can be tailored to various scenarios depending on data availability, resources, and specific modeling objectives, overcoming scalability challenges in all the scenarios.</li>
</ul>

<h3>Title: FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Penglei Sun, Wanyun Zhou, Zikai Wei, Yongqi Zhang, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00961">https://arxiv.org/abs/2508.00961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00961">https://arxiv.org/pdf/2508.00961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00961]] FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph(https://arxiv.org/abs/2508.00961)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Individual investors are significantly outnumbered and disadvantaged in financial markets, overwhelmed by abundant information and lacking professional analysis. Equity research reports stand out as crucial resources, offering valuable insights. By leveraging these reports, large language models (LLMs) can enhance investors' decision-making capabilities and strengthen financial analysis. However, two key challenges limit their effectiveness: (1) the rapid evolution of market events often outpaces the slow update cycles of existing knowledge bases, (2) the long-form and unstructured nature of financial reports further hinders timely and context-aware integration by LLMs. To address these challenges, we tackle both data and methodological aspects. First, we introduce the Event-Enhanced Automated Construction of Financial Knowledge Graph (FinKario), a dataset comprising over 305,360 entities, 9,625 relational triples, and 19 distinct relation types. FinKario automatically integrates real-time company fundamentals and market events through prompt-driven extraction guided by professional institutional templates, providing structured and accessible financial insights for LLMs. Additionally, we propose a Two-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the retrieval of evolving, large-scale financial knowledge to ensure efficient and precise data access. Extensive experiments show that FinKario with FinKario-RAG achieves superior stock trend prediction accuracy, outperforming financial LLMs by 18.81% and institutional strategies by 17.85% on average in backtesting.</li>
</ul>

<h3>Title: Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification</h3>
<ul>
<li><strong>Authors: </strong>Timothy Oladunni, Alex Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00963">https://arxiv.org/abs/2508.00963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00963">https://arxiv.org/pdf/2508.00963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00963]] Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification(https://arxiv.org/abs/2508.00963)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This study proposes a novel perspective on multimodal deep learning for biomedical signal classification, systematically analyzing how complementary feature domains impact model performance. While fusing multiple domains often presumes enhanced accuracy, this work demonstrates that adding modalities can yield diminishing returns, as not all fusions are inherently advantageous. To validate this, five deep learning models were designed, developed, and rigorously evaluated: three unimodal (1D-CNN for time, 2D-CNN for time-frequency, and 1D-CNN-Transformer for frequency) and two multimodal (Hybrid 1, which fuses 1D-CNN and 2D-CNN; Hybrid 2, which combines 1D-CNN, 2D-CNN, and a Transformer). For ECG classification, bootstrapping and Bayesian inference revealed that Hybrid 1 consistently outperformed the 2D-CNN baseline across all metrics (p-values < 0.05, Bayesian probabilities > 0.90), confirming the synergistic complementarity of the time and time-frequency domains. Conversely, Hybrid 2's inclusion of the frequency domain offered no further improvement and sometimes a marginal decline, indicating representational redundancy; a phenomenon further substantiated by a targeted ablation study. This research redefines a fundamental principle of multimodal design in biomedical signal analysis. We demonstrate that optimal domain fusion isn't about the number of modalities, but the quality of their inherent complementarity. This paradigm-shifting concept moves beyond purely heuristic feature selection. Our novel theoretical contribution, "Complementary Feature Domains in Multimodal ECG Deep Learning," presents a mathematically quantifiable framework for identifying ideal domain combinations, demonstrating that optimal multimodal performance arises from the intrinsic information-theoretic complementarity among fused domains.</li>
</ul>

<h3>Title: VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI</h3>
<ul>
<li><strong>Authors: </strong>Roie Kazoom, Ofir Cohen, Rami Puzis, Asaf Shabtai, Ofer Hadar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00965">https://arxiv.org/abs/2508.00965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00965">https://arxiv.org/pdf/2508.00965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00965]] VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI(https://arxiv.org/abs/2508.00965)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce VAULT, a fully automated adversarial RAG pipeline that systematically uncovers and remedies weaknesses in NLI models through three stages: retrieval, adversarial generation, and iterative retraining. First, we perform balanced few-shot retrieval by embedding premises with both semantic (BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM prompts to generate adversarial hypotheses, which are then validated by an LLM ensemble for label fidelity. Finally, the validated adversarial examples are injected back into the training set at increasing mixing ratios, progressively fortifying a zero-shot RoBERTa-base this http URL standard benchmarks, VAULT elevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from 75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%. It also consistently outperforms prior in-context adversarial methods by up to 2.0% across datasets. By automating high-quality adversarial data curation at scale, VAULT enables rapid, human-independent robustness improvements in NLI inference tasks.</li>
</ul>

<h3>Title: Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles</h3>
<ul>
<li><strong>Authors: </strong>Lucas Robinet, Ahmad Berjaoui, Elizabeth Cohen-Jonathan Moyal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00969">https://arxiv.org/abs/2508.00969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00969">https://arxiv.org/pdf/2508.00969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00969]] Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles(https://arxiv.org/abs/2508.00969)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has driven major advances in computational pathology by enabling models to learn rich representations from hematoxylin and eosin (H&E)-stained cancer tissue. However, histopathology alone often falls short for molecular characterization and understanding clinical outcomes, as important information is contained in high-dimensional omics profiles like transcriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS, a unified transformer-based pre-training framework that encodes both histopathology and multi-omics data into a shared latent space. At its core, MORPHEUS relies on a masked modeling objective applied to randomly selected omics portions, encouraging the model to learn biologically meaningful cross-modal relationships. The same pre-trained network can be applied to histopathology alone or in combination with any subset of omics modalities, seamlessly adapting to the available inputs. Additionally, MORPHEUS enables any-to-any omics generation, enabling one or more omics profiles to be inferred from any subset of modalities, including H&E alone. Pre-trained on a large pan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methods across diverse modality combinations and tasks, positioning itself as a promising framework for developing multimodal foundation models in oncology. The code is available at: this https URL</li>
</ul>

<h3>Title: ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling</h3>
<ul>
<li><strong>Authors: </strong>Daniel Andrés López, Vincent Weber, Severin Zentgraf, Barlo Hillen, Perikles Simon, Elmar Schömer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.00974">https://arxiv.org/abs/2508.00974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.00974">https://arxiv.org/pdf/2508.00974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.00974]] ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling(https://arxiv.org/abs/2508.00974)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Infrared thermography is emerging as a powerful tool in sports medicine, allowing assessment of thermal radiation during exercise and analysis of anatomical regions of interest, such as the well-exposed calves. Building on our previous advanced automatic annotation method, we aimed to transfer the stereo- and multimodal-based labeling approach from treadmill running to ergometer cycling. Therefore, the training of the semantic segmentation network with automatic labels and fine-tuning on high-quality manually annotated images has been examined and compared in different data set combinations. The results indicate that fine-tuning with a small fraction of manual data is sufficient to improve the overall performance of the deep neural network. Finally, combining automatically generated labels with small manually annotated data sets accelerates the adaptation of deep neural networks to new use cases, such as the transition from treadmill to bicycle.</li>
</ul>

<h3>Title: Optimal Scheduling Algorithms for LLM Inference: Theory and Practice</h3>
<ul>
<li><strong>Authors: </strong>Agrim Bari, Parikshit Hegde, Gustavo de Veciana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01002">https://arxiv.org/abs/2508.01002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01002">https://arxiv.org/pdf/2508.01002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01002]] Optimal Scheduling Algorithms for LLM Inference: Theory and Practice(https://arxiv.org/abs/2508.01002)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the growing use of Large Language Model (LLM)-based tools like ChatGPT, Perplexity, and Gemini across industries, there is a rising need for efficient LLM inference systems. These systems handle requests with a unique two-phase computation structure: a prefill-phase that processes the full input prompt and a decode-phase that autoregressively generates tokens one at a time. This structure calls for new strategies for routing and scheduling requests. In this paper, we take a comprehensive approach to this challenge by developing a theoretical framework that models routing and scheduling in LLM inference systems. We identify two key design principles-optimal tiling and dynamic resource allocation-that are essential for achieving high throughput. Guided by these principles, we propose the Resource-Aware Dynamic (RAD) scheduler and prove that it achieves throughput optimality under mild conditions. To address practical Service Level Objectives (SLOs) such as serving requests with different Time Between Token (TBT) constraints, we design the SLO-Aware LLM Inference (SLAI) scheduler. SLAI uses real-time measurements to prioritize decode requests that are close to missing their TBT deadlines and reorders prefill requests based on known prompt lengths to further reduce the Time To First Token (TTFT) delays. We evaluate SLAI on the Openchat ShareGPT4 dataset using the Mistral-7B model on an NVIDIA RTX ADA 6000 GPU. Compared to Sarathi-Serve, SLAI reduces the median TTFT by 53% and increases the maximum serving capacity by 26% such that median TTFT is below 0.5 seconds, while meeting tail TBT latency constraints.</li>
</ul>

<h3>Title: UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu</h3>
<ul>
<li><strong>Authors: </strong>Farah Adeeba, Brian Dillon, Hassan Sajjad, Rajesh Bhatt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01006">https://arxiv.org/abs/2508.01006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01006">https://arxiv.org/pdf/2508.01006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01006]] UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu(https://arxiv.org/abs/2508.01006)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual Large Language Models (LLMs) have shown remarkable performance across various languages; however, they often include significantly less data for low-resource languages such as Urdu compared to high-resource languages like English. To assess the linguistic knowledge of LLMs in Urdu, we present the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of minimally different sentences that contrast in grammatical acceptability. UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena, carefully curated using the Urdu Treebank and diverse Urdu text corpora. A human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator agreement, confirming the reliability of the dataset. We evaluate twenty multilingual LLMs on UrBLiMP, revealing significant variation in performance across linguistic phenomena. While LLaMA-3-70B achieves the highest average accuracy (94.73%), its performance is statistically comparable to other top models such as Gemma-3-27B-PT. These findings highlight both the potential and the limitations of current multilingual LLMs in capturing fine-grained syntactic knowledge in low-resource languages.</li>
</ul>

<h3>Title: ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Cihang Peng, Qiming Hou, Zhong Ren, Kun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01008">https://arxiv.org/abs/2508.01008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01008">https://arxiv.org/pdf/2508.01008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01008]] ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation(https://arxiv.org/abs/2508.01008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present ROVI, a high-quality synthetic dataset for instance-grounded text-to-image generation, created by labeling 1M curated web images. Our key innovation is a strategy called re-captioning, focusing on the pre-detection stage, where a VLM (Vision-Language Model) generates comprehensive visual descriptions that are then processed by an LLM (Large Language Model) to extract a flat list of potential categories for OVDs (Open-Vocabulary Detectors) to detect. This approach yields a global prompt inherently linked to instance annotations while capturing secondary visual elements humans typically overlook. Evaluations show that ROVI exceeds existing detection datasets in image quality and resolution while containing two orders of magnitude more categories with an open-vocabulary nature. For demonstrative purposes, a text-to-image model GLIGEN trained on ROVI significantly outperforms state-of-the-art alternatives in instance grounding accuracy, prompt fidelity, and aesthetic quality. Our dataset and reproducible pipeline are available at this https URL.</li>
</ul>

<h3>Title: v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Gnankan Landry Regis N'guessan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01010">https://arxiv.org/abs/2508.01010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01010">https://arxiv.org/pdf/2508.01010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01010]] v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning(https://arxiv.org/abs/2508.01010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conventional deep learning models embed data in Euclidean space $\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, word senses, or file systems. We introduce van der Put Neural Networks (v-PuNNs), the first architecture whose neurons are characteristic functions of p-adic balls in $\mathbb{Z}_p$. Under our Transparent Ultrametric Representation Learning (TURL) principle every weight is itself a p-adic number, giving exact subtree semantics. A new Finite Hierarchical Approximation Theorem shows that a depth-K v-PuNN with $\sum_{j=0}^{K-1}p^{\,j}$ neurons universally represents any K-level tree. Because gradients vanish in this discrete space, we propose Valuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministic variant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On three canonical benchmarks our CPU-only implementation sets new state-of-the-art: WordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GO molecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $\rho = -0.96$ with true taxonomic distance. The learned metric is perfectly ultrametric (zero triangle violations), and its fractal and information-theoretic properties are analyzed. Beyond classification we derive structural invariants for quantum systems (HiPaQ) and controllable generative codes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory and deep learning, offering exact, interpretable, and efficient models for hierarchical data.</li>
</ul>

<h3>Title: AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise</h3>
<ul>
<li><strong>Authors: </strong>Byron Dowling, Jozef Probcin, Adam Czajka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01015">https://arxiv.org/abs/2508.01015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01015">https://arxiv.org/pdf/2508.01015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01015]] AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise(https://arxiv.org/abs/2508.01015)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Can we teach machines to assess the expertise of humans solving visual tasks automatically based on eye tracking features? This paper proposes AutoSIGHT, Automatic System for Immediate Grading of Human experTise, that classifies expert and non-expert performers, and builds upon an ensemble of features extracted from eye tracking data while the performers were solving a visual task. Results on the task of iris Presentation Attack Detection (PAD) used for this study show that with a small evaluation window of just 5 seconds, AutoSIGHT achieves an average average Area Under the ROC curve performance of 0.751 in subject-disjoint train-test regime, indicating that such detection is viable. Furthermore, when a larger evaluation window of up to 30 seconds is available, the Area Under the ROC curve (AUROC) increases to 0.8306, indicating the model is effectively leveraging more information at a cost of slightly delayed decisions. This work opens new areas of research on how to incorporate the automatic weighing of human and machine expertise into human-AI pairing setups, which need to react dynamically to nonstationary expertise distribution between the human and AI players (e.g. when the experts need to be replaced, or the task at hand changes rapidly). Along with this paper, we offer the eye tracking data used in this study collected from 6 experts and 53 non-experts solving iris PAD visual task.</li>
</ul>

<h3>Title: Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans</h3>
<ul>
<li><strong>Authors: </strong>Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01045">https://arxiv.org/abs/2508.01045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01045">https://arxiv.org/pdf/2508.01045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01045]] Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans(https://arxiv.org/abs/2508.01045)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>With the increasing number of CT scan examinations, there is a need for automated methods such as organ segmentation, anomaly detection and report generation to assist radiologists in managing their increasing workload. Multi-label classification of 3D CT scans remains a critical yet challenging task due to the complex spatial relationships within volumetric data and the variety of observed anomalies. Existing approaches based on 3D convolutional networks have limited abilities to model long-range dependencies while Vision Transformers suffer from high computational costs and often require extensive pre-training on large-scale datasets from the same domain to achieve competitive performance. In this work, we propose an alternative by introducing a new graph-based approach that models CT scans as structured graphs, leveraging axial slice triplets nodes processed through spectral domain convolution to enhance multi-label anomaly classification performance. Our method exhibits strong cross-dataset generalization, and competitive performance while achieving robustness to z-axis translation. An ablation study evaluates the contribution of each proposed component.</li>
</ul>

<h3>Title: Explaining GNN Explanations with Edge Gradients</h3>
<ul>
<li><strong>Authors: </strong>Jesse He, Akbar Rafiey, Gal Mishne, Yusu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01048">https://arxiv.org/abs/2508.01048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01048">https://arxiv.org/pdf/2508.01048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01048]] Explaining GNN Explanations with Edge Gradients(https://arxiv.org/abs/2508.01048)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In recent years, the remarkable success of graph neural networks (GNNs) on graph-structured data has prompted a surge of methods for explaining GNN predictions. However, the state-of-the-art for GNN explainability remains in flux. Different comparisons find mixed results for different methods, with many explainers struggling on more complex GNN architectures and tasks. This presents an urgent need for a more careful theoretical analysis of competing GNN explanation methods. In this work we take a closer look at GNN explanations in two different settings: input-level explanations, which produce explanatory subgraphs of the input graph, and layerwise explanations, which produce explanatory subgraphs of the computation graph. We establish the first theoretical connections between the popular perturbation-based and classical gradient-based methods, as well as point out connections between other recently proposed methods. At the input level, we demonstrate conditions under which GNNExplainer can be approximated by a simple heuristic based on the sign of the edge gradients. In the layerwise setting, we point out that edge gradients are equivalent to occlusion search for linear GNNs. Finally, we demonstrate how our theoretical results manifest in practice with experiments on both synthetic and real datasets.</li>
</ul>

<h3>Title: Centralized Adaptive Sampling for Reliable Co-Training of Independent Multi-Agent Policies</h3>
<ul>
<li><strong>Authors: </strong>Nicholas E. Corrado, Josiah P. Hanna</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01049">https://arxiv.org/abs/2508.01049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01049">https://arxiv.org/pdf/2508.01049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01049]] Centralized Adaptive Sampling for Reliable Co-Training of Independent Multi-Agent Policies(https://arxiv.org/abs/2508.01049)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Independent on-policy policy gradient algorithms are widely used for multi-agent reinforcement learning (MARL) in cooperative and no-conflict games, but they are known to converge suboptimally when each agent's policy gradient points toward a suboptimal equilibrium. In this work, we identify a subtler failure mode that arises \textit{even when the expected policy gradients of all agents point toward an optimal solution.} After collecting a finite set of trajectories, stochasticity in independent action sampling can cause the joint data distribution to deviate from the expected joint on-policy distribution. This \textit{sampling error} w.r.t. the joint on-policy distribution produces inaccurate gradient estimates that can lead agents to converge suboptimally. In this paper, we investigate if joint sampling error can be reduced through coordinated action selection and whether doing so improves the reliability of policy gradient learning in MARL. Toward this end, we introduce an adaptive action sampling approach to reduce joint sampling error. Our method, Multi-Agent Proximal Robust On-Policy Sampling (MA-PROPS), uses a centralized behavior policy that we continually adapt to place larger probability on joint actions that are currently under-sampled w.r.t. the current joint policy. We empirically evaluate MA-PROPS in a diverse range of multi-agent games and demonstrate that (1) MA-PROPS reduces joint sampling error more efficiently than standard on-policy sampling and (2) improves the reliability of independent policy gradient algorithms, increasing the fraction of training runs that converge to an optimal joint policy.</li>
</ul>

<h3>Title: Autonomous Penetration Testing: Solving Capture-the-Flag Challenges with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Isabelle Bakker, John Hastings</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01054">https://arxiv.org/abs/2508.01054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01054">https://arxiv.org/pdf/2508.01054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01054]] Autonomous Penetration Testing: Solving Capture-the-Flag Challenges with LLMs(https://arxiv.org/abs/2508.01054)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates the ability of GPT-4o to autonomously solve beginner-level offensive security tasks by connecting the model to OverTheWire's Bandit capture-the-flag game. Of the 25 levels that were technically compatible with a single-command SSH framework, GPT-4o solved 18 unaided and another two after minimal prompt hints for an overall 80% success rate. The model excelled at single-step challenges that involved Linux filesystem navigation, data extraction or decoding, and straightforward networking. The approach often produced the correct command in one shot and at a human-surpassing speed. Failures involved multi-command scenarios that required persistent working directories, complex network reconnaissance, daemon creation, or interaction with non-standard shells. These limitations highlight current architectural deficiencies rather than a lack of general exploit knowledge. The results demonstrate that large language models (LLMs) can automate a substantial portion of novice penetration-testing workflow, potentially lowering the expertise barrier for attackers and offering productivity gains for defenders who use LLMs as rapid reconnaissance aides. Further, the unsolved tasks reveal specific areas where secure-by-design environments might frustrate simple LLM-driven attacks, informing future hardening strategies. Beyond offensive cybersecurity applications, results suggest the potential to integrate LLMs into cybersecurity education as practice aids.</li>
</ul>

<h3>Title: FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuan Liu, Siru Ouyang, Xianrui Zhong, Jiawei Han, Huimin Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01055">https://arxiv.org/abs/2508.01055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01055">https://arxiv.org/pdf/2508.01055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01055]] FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models(https://arxiv.org/abs/2508.01055)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have gained significant attention in chemistry. However, most existing datasets center on molecular-level property prediction and overlook the role of fine-grained functional group (FG) information. Incorporating FG-level data can provide valuable prior knowledge that links molecular structures with textual descriptions, which can be used to build more interpretable, structure-aware LLMs for reasoning on molecule-related tasks. Moreover, LLMs can learn from such fine-grained information to uncover hidden relationships between specific functional groups and molecular properties, thereby advancing molecular design and drug discovery. Here, we introduce FGBench, a dataset comprising 625K molecular property reasoning problems with functional group information. Functional groups are precisely annotated and localized within the molecule, which ensures the dataset's interoperability thereby facilitating further multimodal applications. FGBench includes both regression and classification tasks on 245 different functional groups across three categories for molecular property reasoning: (1) single functional group impacts, (2) multiple functional group interactions, and (3) direct molecular comparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the results indicate that current LLMs struggle with FG-level property reasoning, highlighting the need to enhance reasoning capabilities in LLMs for chemistry tasks. We anticipate that the methodology employed in FGBench to construct datasets with functional group-level information will serve as a foundational framework for generating new question-answer pairs, enabling LLMs to better understand fine-grained molecular structure-property relationships. The dataset and evaluation code are available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sara Yavari, Rahul Nitin Pandya, Jacob Furst</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01058">https://arxiv.org/abs/2508.01058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01058">https://arxiv.org/pdf/2508.01058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01058]] ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation(https://arxiv.org/abs/2508.01058)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of brain tumors in MRI scans is critical for clinical diagnosis and treatment planning. We propose a semi-supervised, two-stage framework that extends the ReCoSeg approach to the larger and more heterogeneous BraTS 2021 dataset, while eliminating the need for ground-truth masks for the segmentation objective. In the first stage, a residual-guided denoising diffusion probabilistic model (DDPM) performs cross-modal synthesis by reconstructing the T1ce modality from FLAIR, T1, and T2 scans. The residual maps, capturing differences between predicted and actual T1ce images, serve as spatial priors to enhance downstream segmentation. In the second stage, a lightweight U-Net takes as input the concatenation of residual maps, computed as the difference between real T1ce and synthesized T1ce, with T1, T2, and FLAIR modalities to improve whole tumor segmentation. To address the increased scale and variability of BraTS 2021, we apply slice-level filtering to exclude non-informative samples and optimize thresholding strategies to balance precision and recall. Our method achieves a Dice score of $93.02\%$ and an IoU of $86.7\%$ for whole tumor segmentation on the BraTS 2021 dataset, outperforming the ReCoSeg baseline on BraTS 2020 (Dice: $91.7\%$, IoU: $85.3\%$), and demonstrating improved accuracy and scalability for real-world, multi-center MRI datasets.</li>
</ul>

<h3>Title: Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Sajana Weerawardhena, Paul Kassianik, Blaine Nelson, Baturay Saglam, Anu Vellore, Aman Priyanshu, Supriti Vijay, Massimo Aufiero, Arthur Goldblatt, Fraser Burch, Ed Li, Jianliang He, Dhruv Kedia, Kojin Oshiba, Zhouran Yang, Yaron Singer, Amin Karbasi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01059">https://arxiv.org/abs/2508.01059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01059">https://arxiv.org/pdf/2508.01059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01059]] Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report(https://arxiv.org/abs/2508.01059)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at this https URL.</li>
</ul>

<h3>Title: CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception</h3>
<ul>
<li><strong>Authors: </strong>Chenyi Wang, Ruoyu Song, Raymond Muller, Jean-Philippe Monteuuis, Z. Berkay Celik, Jonathan Petit, Ryan Gerdes, Ming Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01062">https://arxiv.org/abs/2508.01062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01062">https://arxiv.org/pdf/2508.01062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01062]] CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception(https://arxiv.org/abs/2508.01062)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Cooperative perception (CP) enhances situational awareness of connected and autonomous vehicles by exchanging and combining messages from multiple agents. While prior work has explored adversarial integrity attacks that degrade perceptual accuracy, little is known about CP's robustness against attacks on timeliness (or availability), a safety-critical requirement for autonomous driving. In this paper, we present CP-FREEZER, the first latency attack that maximizes the computation delay of CP algorithms by injecting adversarial perturbation via V2V messages. Our attack resolves several unique challenges, including the non-differentiability of point cloud preprocessing, asynchronous knowledge of the victim's input due to transmission delays, and uses a novel loss function that effectively maximizes the execution time of the CP pipeline. Extensive experiments show that CP-FREEZER increases end-to-end CP latency by over $90\times$, pushing per-frame processing time beyond 3 seconds with a 100% success rate on our real-world vehicle testbed. Our findings reveal a critical threat to the availability of CP systems, highlighting the urgent need for robust defenses.</li>
</ul>

<h3>Title: Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fenghe Tang, Bingkun Nian, Jianrui Ding, Wenxin Ma, Quan Quan, Chengqi Dong, Jie Yang, Wei Liu, S. Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01064">https://arxiv.org/abs/2508.01064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01064">https://arxiv.org/pdf/2508.01064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01064]] Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation(https://arxiv.org/abs/2508.01064)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In clinical practice, medical image analysis often requires efficient execution on resource-constrained mobile devices. However, existing mobile models-primarily optimized for natural images-tend to perform poorly on medical tasks due to the significant information density gap between natural and medical domains. Combining computational efficiency with medical imaging-specific architectural advantages remains a challenge when developing lightweight, universal, and high-performing networks. To address this, we propose a mobile model called Mobile U-shaped Vision Transformer (Mobile U-ViT) tailored for medical image segmentation. Specifically, we employ the newly purposed ConvUtr as a hierarchical patch embedding, featuring a parameter-efficient large-kernel CNN with inverted bottleneck fusion. This design exhibits transformer-like representation learning capacity while being lighter and faster. To enable efficient local-global information exchange, we introduce a novel Large-kernel Local-Global-Local (LGL) block that effectively balances the low information density and high-level semantic discrepancy of medical images. Finally, we incorporate a shallow and lightweight transformer bottleneck for long-range modeling and employ a cascaded decoder with downsample skip connections for dense prediction. Despite its reduced computational demands, our medical-optimized architecture achieves state-of-the-art performance across eight public 2D and 3D datasets covering diverse imaging modalities, including zero-shot testing on four unseen datasets. These results establish it as an efficient yet powerful and generalization solution for mobile medical image analysis. Code is available at this https URL.</li>
</ul>

<h3>Title: Evading Data Provenance in Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Zhu, Sichu Liang, Wenwen Wang, Zhuomeng Zhang, Fangqi Li, Shi-Lin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01074">https://arxiv.org/abs/2508.01074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01074">https://arxiv.org/pdf/2508.01074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01074]] Evading Data Provenance in Deep Neural Networks(https://arxiv.org/abs/2508.01074)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Modern over-parameterized deep models are highly data-dependent, with large scale general-purpose and domain-specific datasets serving as the bedrock for rapid advancements. However, many datasets are proprietary or contain sensitive information, making unrestricted model training problematic. In the open world where data thefts cannot be fully prevented, Dataset Ownership Verification (DOV) has emerged as a promising method to protect copyright by detecting unauthorized model training and tracing illicit activities. Due to its diversity and superior stealth, evading DOV is considered extremely challenging. However, this paper identifies that previous studies have relied on oversimplistic evasion attacks for evaluation, leading to a false sense of security. We introduce a unified evasion framework, in which a teacher model first learns from the copyright dataset and then transfers task-relevant yet identifier-independent domain knowledge to a surrogate student using an out-of-distribution (OOD) dataset as the intermediary. Leveraging Vision-Language Models and Large Language Models, we curate the most informative and reliable subsets from the OOD gallery set as the final transfer set, and propose selectively transferring task-oriented knowledge to achieve a better trade-off between generalization and evasion effectiveness. Experiments across diverse datasets covering eleven DOV methods demonstrate our approach simultaneously eliminates all copyright identifiers and significantly outperforms nine state-of-the-art evasion attacks in both generalization and effectiveness, with moderate computational overhead. As a proof of concept, we reveal key vulnerabilities in current DOV methods, highlighting the need for long-term development to enhance practicality.</li>
</ul>

<h3>Title: Provably Secure Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Zhou, Yinglun Feng, Zhongliang Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01084">https://arxiv.org/abs/2508.01084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01084">https://arxiv.org/pdf/2508.01084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01084]] Provably Secure Retrieval-Augmented Generation(https://arxiv.org/abs/2508.01084)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, defense, attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Although Retrieval-Augmented Generation (RAG) systems have been widely applied, the privacy and security risks they face, such as data leakage and data poisoning, have not been systematically addressed yet. Existing defense strategies primarily rely on heuristic filtering or enhancing retriever robustness, which suffer from limited interpretability, lack of formal security guarantees, and vulnerability to adaptive attacks. To address these challenges, this paper proposes the first provably secure framework for RAG systems(SAG). Our framework employs a pre-storage full-encryption scheme to ensure dual protection of both retrieved content and vector embeddings, guaranteeing that only authorized entities can access the data. Through formal security proofs, we rigorously verify the scheme's confidentiality and integrity under a computational security model. Extensive experiments across multiple benchmark datasets demonstrate that our framework effectively resists a range of state-of-the-art attacks. This work establishes a theoretical foundation and practical paradigm for verifiably secure RAG systems, advancing AI-powered services toward formally guaranteed security.</li>
</ul>

<h3>Title: An Unconditionally Secure Encryption Scheme for IoBT Networks</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Moltafet, Hamid R. Sadjadpour, Zouheir Rezki</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01085">https://arxiv.org/abs/2508.01085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01085">https://arxiv.org/pdf/2508.01085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01085]] An Unconditionally Secure Encryption Scheme for IoBT Networks(https://arxiv.org/abs/2508.01085)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>We consider an Internet of Battlefield Things (IoBT) system consisting of multiple devices that want to securely communicate with each other during a mission in the presence of an adversary with unbounded computational power. The adversary has complete access to listen/read the ciphertext without tampering with the communication line. We provide an unconditionally secure encryption scheme to exchange messages among devices in the system. The main idea behind the scheme is to provide secret keys to exchange messages using a random binary matrix that is securely shared among all the devices, and pair-wise random secret keys established between each pair of devices attempting to communicate before the mission. The scheme is implemented by using finite group modular addition. We show that the scheme is absolutely semantically secure, i.e., the scheme guarantees that an adversary with unbounded computational power cannot get even one bit of information about a message, except for an exponentially small probability in a security parameter. Besides that, we show that even if the random binary matrix is revealed to the adversary, the provided scheme is computationally secure against the key recovery attack.</li>
</ul>

<h3>Title: COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ryan Rabinowitz, Steve Cruz, Walter Scheirer, Terrance E. Boult</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01087">https://arxiv.org/abs/2508.01087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01087">https://arxiv.org/pdf/2508.01087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01087]] COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition(https://arxiv.org/abs/2508.01087)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Handling novelty remains a key challenge in visual recognition systems. Existing open-set recognition (OSR) methods rely on the familiarity hypothesis, detecting novelty by the absence of familiar features. We propose a novel attenuation hypothesis: small weights learned during training attenuate features and serve a dual role-differentiating known classes while discarding information useful for distinguishing known from unknown classes. To leverage this overlooked information, we present COSTARR, a novel approach that combines both the requirement of familiar features and the lack of unfamiliar ones. We provide a probabilistic interpretation of the COSTARR score, linking it to the likelihood of correct classification and belonging in a known class. To determine the individual contributions of the pre- and post-attenuated features to COSTARR's performance, we conduct ablation studies that show both pre-attenuated deep features and the underutilized post-attenuated Hadamard product features are essential for improving OSR. Also, we evaluate COSTARR in a large-scale setting using ImageNet2012-1K as known data and NINCO, iNaturalist, OpenImage-O, and other datasets as unknowns, across multiple modern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments demonstrate that COSTARR generalizes effectively across various architectures and significantly outperforms prior state-of-the-art methods by incorporating previously discarded attenuation information, advancing open-set recognition capabilities.</li>
</ul>

<h3>Title: AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions</h3>
<ul>
<li><strong>Authors: </strong>Mikhail Bychkov, Matey Yordanov, Andrei Kuchma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01095">https://arxiv.org/abs/2508.01095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01095">https://arxiv.org/pdf/2508.01095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01095]] AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions(https://arxiv.org/abs/2508.01095)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework designed for robust, real-time detection and classification of industrial smoke emissions. The framework addresses critical limitations of current monitoring systems, which often lack the specificity to distinguish smoke types and struggle with environmental variability. AURA leverages both the dynamic movement patterns and the distinct color characteristics of industrial smoke to provide enhanced accuracy and reduced false positives. This framework aims to significantly improve environmental compliance, operational safety, and public health outcomes by enabling precise, automated monitoring of industrial emissions.</li>
</ul>

<h3>Title: Cross-Domain Web Information Extraction at Pinterest</h3>
<ul>
<li><strong>Authors: </strong>Michael Farag, Patrick Halina, Andrey Zaytsev, Alekhya Munagala, Imtihan Ahmed, Junhao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01096">https://arxiv.org/abs/2508.01096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01096">https://arxiv.org/pdf/2508.01096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01096]] Cross-Domain Web Information Extraction at Pinterest(https://arxiv.org/abs/2508.01096)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>The internet offers a massive repository of unstructured information, but it's a significant challenge to convert this into a structured format. At Pinterest, the ability to accurately extract structured product data from e-commerce websites is essential to enhance user experiences and improve content distribution. In this paper, we present Pinterest's system for attribute extraction, which achieves remarkable accuracy and scalability at a manageable cost. Our approach leverages a novel webpage representation that combines structural, visual, and text modalities into a compact form, optimizing it for small model learning. This representation captures each visible HTML node with its text, style and layout information. We show how this allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract attributes more accurately than much more complex Large Language Models (LLMs) such as Generative Pre-trained Transformer (GPT). Our results demonstrate a system that is highly scalable, processing over 1,000 URLs per second, while being 1000 times more cost-effective than the cheapest GPT alternatives.</li>
</ul>

<h3>Title: Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Yuekun Dai, Haitian Li, Shangchen Zhou, Chen Change Loy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01098">https://arxiv.org/abs/2508.01098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01098">https://arxiv.org/pdf/2508.01098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01098]] Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting(https://arxiv.org/abs/2508.01098)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>RGBA images, with the additional alpha channel, are crucial for any application that needs blending, masking, or transparency effects, making them more versatile than standard RGB images. Nevertheless, existing image inpainting methods are designed exclusively for RGB images. Conventional approaches to transparent image inpainting typically involve placing a background underneath RGBA images and employing a two-stage process: image inpainting followed by image matting. This pipeline, however, struggles to preserve transparency consistency in edited regions, and matting can introduce jagged edges along transparency boundaries. To address these challenges, we propose Trans-Adapter, a plug-and-play adapter that enables diffusion-based inpainting models to process transparent images directly. Trans-Adapter also supports controllable editing via ControlNet and can be seamlessly integrated into various community models. To evaluate our method, we introduce LayerBench, along with a novel non-reference alpha edge quality evaluation metric for assessing transparency edge quality. We conduct extensive experiments on LayerBench to demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Flow Matching for Probabilistic Learning of Dynamical Systems from Missing or Noisy Data</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Rout, Eldad Haber, Stephane Gaudreault</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01101">https://arxiv.org/abs/2508.01101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01101">https://arxiv.org/pdf/2508.01101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01101]] Flow Matching for Probabilistic Learning of Dynamical Systems from Missing or Noisy Data(https://arxiv.org/abs/2508.01101)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning dynamical systems is crucial across many fields, yet applying machine learning techniques remains challenging due to missing variables and noisy data. Classical mathematical models often struggle in these scenarios due to the arose ill-posedness of the physical systems. Stochastic machine learning techniques address this challenge by enabling the modeling of such ill-posed problems. Thus, a single known input to the trained machine learning model may yield multiple plausible outputs, and all of the outputs are correct. In such scenarios, probabilistic forecasting is inherently meaningful. In this study, we introduce a variant of flow matching for probabilistic forecasting which estimates possible future states as a distribution over possible outcomes rather than a single-point prediction. Perturbation of complex dynamical states is not trivial. Community uses typical Gaussian or uniform perturbations to crucial variables to model uncertainty. However, not all variables behave in a Gaussian fashion. So, we also propose a generative machine learning approach to physically and logically perturb the states of complex high-dimensional dynamical systems. Finally, we establish the mathematical foundations of our method and demonstrate its effectiveness on several challenging dynamical systems, including a variant of the high-dimensional WeatherBench dataset, which models the global weather at a 5.625° meridional resolution.</li>
</ul>

<h3>Title: Protecting Student Mental Health with a Context-Aware Machine Learning Framework for Stress Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Md Sultanul Islam Ovi, Jamal Hossain, Md Raihan Alam Rahi, Fatema Akter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01105">https://arxiv.org/abs/2508.01105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01105">https://arxiv.org/pdf/2508.01105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01105]] Protecting Student Mental Health with a Context-Aware Machine Learning Framework for Stress Monitoring(https://arxiv.org/abs/2508.01105)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Student mental health is an increasing concern in academic institutions, where stress can severely impact well-being and academic performance. Traditional assessment methods rely on subjective surveys and periodic evaluations, offering limited value for timely intervention. This paper introduces a context-aware machine learning framework for classifying student stress using two complementary survey-based datasets covering psychological, academic, environmental, and social factors. The framework follows a six-stage pipeline involving preprocessing, feature selection (SelectKBest, RFECV), dimensionality reduction (PCA), and training with six base classifiers: SVM, Random Forest, Gradient Boosting, XGBoost, AdaBoost, and Bagging. To enhance performance, we implement ensemble strategies, including hard voting, soft voting, weighted voting, and stacking. Our best models achieve 93.09% accuracy with weighted hard voting on the Student Stress Factors dataset and 99.53% with stacking on the Stress and Well-being dataset, surpassing previous benchmarks. These results highlight the potential of context-integrated, data-driven systems for early stress detection and underscore their applicability in real-world academic settings to support student well-being.</li>
</ul>

<h3>Title: AdVAR-DNN: Adversarial Misclassification Attack on Collaborative DNN Inference</h3>
<ul>
<li><strong>Authors: </strong>Shima Yousefi, Motahare Mounesan, Saptarshi Debroy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01107">https://arxiv.org/abs/2508.01107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01107">https://arxiv.org/pdf/2508.01107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01107]] AdVAR-DNN: Adversarial Misclassification Attack on Collaborative DNN Inference(https://arxiv.org/abs/2508.01107)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack</a></li>
<li><strong>Abstract: </strong>In recent years, Deep Neural Networks (DNNs) have become increasingly integral to IoT-based environments, enabling realtime visual computing. However, the limited computational capacity of these devices has motivated the adoption of collaborative DNN inference, where the IoT device offloads part of the inference-related computation to a remote server. Such offloading often requires dynamic DNN partitioning information to be exchanged among the participants over an unsecured network or via relays/hops, leading to novel privacy vulnerabilities. In this paper, we propose AdVAR-DNN, an adversarial variational autoencoder (VAE)-based misclassification attack, leveraging classifiers to detect model information and a VAE to generate untraceable manipulated samples, specifically designed to compromise the collaborative inference process. AdVAR-DNN attack uses the sensitive information exchange vulnerability of collaborative DNN inference and is black-box in nature in terms of having no prior knowledge about the DNN model and how it is partitioned. Our evaluation using the most popular object classification DNNs on the CIFAR-100 dataset demonstrates the effectiveness of AdVAR-DNN in terms of high attack success rate with little to no probability of detection.</li>
</ul>

<h3>Title: A hierarchy tree data structure for behavior-based user segment representation</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Xuejiao Kang, Sathya Iyer, Idris Malik, Ruixuan Li, Juan Wang, Xinchen Lu, Xiangxue Zhao, Dayong Wang, Menghan Liu, Isaac Liu, Feng Liang, Yinzhe Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01115">https://arxiv.org/abs/2508.01115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01115">https://arxiv.org/pdf/2508.01115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01115]] A hierarchy tree data structure for behavior-based user segment representation(https://arxiv.org/abs/2508.01115)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>User attributes are essential in multiple stages of modern recommendation systems and are particularly important for mitigating the cold-start problem and improving the experience of new or infrequent users. We propose Behavior-based User Segmentation (BUS), a novel tree-based data structure that hierarchically segments the user universe with various users' categorical attributes based on the users' product-specific engagement behaviors. During the BUS tree construction, we use Normalized Discounted Cumulative Gain (NDCG) as the objective function to maximize the behavioral representativeness of marginal users relative to active users in the same segment. The constructed BUS tree undergoes further processing and aggregation across the leaf nodes and internal nodes, allowing the generation of popular social content and behavioral patterns for each node in the tree. To further mitigate bias and improve fairness, we use the social graph to derive the user's connection-based BUS segments, enabling the combination of behavioral patterns extracted from both the user's own segment and connection-based segments as the connection aware BUS-based recommendation. Our offline analysis shows that the BUS-based retrieval significantly outperforms traditional user cohort-based aggregation on ranking quality. We have successfully deployed our data structure and machine learning algorithm and tested it with various production traffic serving billions of users daily, achieving statistically significant improvements in the online product metrics, including music ranking and email notifications. To the best of our knowledge, our study represents the first list-wise learning-to-rank framework for tree-based recommendation that effectively integrates diverse user categorical attributes while preserving real-world semantic interpretability at a large industrial scale.</li>
</ul>

<h3>Title: UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation</h3>
<ul>
<li><strong>Authors: </strong>Chaitanya Patel, Hiroki Nakamura, Yuta Kyuragi, Kazuki Kozuka, Juan Carlos Niebles, Ehsan Adeli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01126">https://arxiv.org/abs/2508.01126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01126">https://arxiv.org/pdf/2508.01126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01126]] UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation(https://arxiv.org/abs/2508.01126)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.</li>
</ul>

<h3>Title: Transformers in Pseudo-Random Number Generation: A Dual Perspective on Theory and Practice</h3>
<ul>
<li><strong>Authors: </strong>Ran Li, Lingshu Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01134">https://arxiv.org/abs/2508.01134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01134">https://arxiv.org/pdf/2508.01134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01134]] Transformers in Pseudo-Random Number Generation: A Dual Perspective on Theory and Practice(https://arxiv.org/abs/2508.01134)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Pseudo-random number generators (PRNGs) are high-nonlinear processes, and they are key blocks in optimization of Large language models. Transformers excel at processing complex nonlinear relationships. Thus it is reasonable to generate high-quality pseudo-random numbers based on transformers. In this paper, we explore this question from both theoretical and practical perspectives, highlighting the potential benefits and implications of Transformer in PRNGs. We theoretically demonstrate that decoder-only Transformer models with Chain-of-Thought can simulate both the Linear Congruential Generator (LCG) and Mersenne Twister (MT) PRNGs. Based on this, we conclude that the log-precision decoder-only Transformer can represent non-uniform $\text{AC}^0$. Our simulative theoretical findings are validated through experiments. The random numbers generated by Transformer-based PRNGs successfully pass the majority of NIST tests, whose heat maps exhibit clear statistical randomness. Finally, we assess their capability in prediction attacks.</li>
</ul>

<h3>Title: Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Zeduo Zhang, Yalda Mohsenzadeh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01137">https://arxiv.org/abs/2508.01137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01137">https://arxiv.org/pdf/2508.01137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01137]] Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach(https://arxiv.org/abs/2508.01137)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>To develop a domain-agnostic, semi-supervised anomaly detection framework that integrates deep reinforcement learning (DRL) to address challenges such as large-scale data, overfitting, and class imbalance, focusing on brain MRI volumes. This retrospective study used publicly available brain MRI datasets collected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and 578 T2-weighted MRI volumes (from healthy subjects) for training, while the BraTS 2021 dataset provided 251 volumes for validation and 1000 for testing (unhealthy subjects with Glioblastomas). Preprocessing included normalization, skull-stripping, and co-registering to a uniform voxel size. Experiments were conducted on both T1- and T2-weighted modalities. Additional experiments and ablation analyses were also carried out on the industrial datasets. The proposed method integrates DRL with feature representations to handle label scarcity, large-scale data and overfitting. Statistical analysis was based on several detection and segmentation metrics including AUROC and Dice score. The proposed method achieved an AUROC of 88.7% (pixel-level) and 96.7% (image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA) methods. On industrial surface datasets, the model also showed competitive performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset, indicating strong cross-domain generalization. Studies on anomaly sample size showed a monotonic increase in AUROC as more anomalies were seen, without evidence of overfitting or additional computational cost. The domain-agnostic semi-supervised approach using DRL shows significant promise for MRI anomaly detection, achieving strong performance on both medical and industrial datasets. Its robustness, generalizability and efficiency highlight its potential for real-world clinical applications.</li>
</ul>

<h3>Title: Dataset Condensation with Color Compensation</h3>
<ul>
<li><strong>Authors: </strong>Huyu Wu, Duo Su, Junjie Hou, Guang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01139">https://arxiv.org/abs/2508.01139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01139">https://arxiv.org/pdf/2508.01139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01139]] Dataset Condensation with Color Compensation(https://arxiv.org/abs/2508.01139)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Dataset condensation always faces a constitutive trade-off: balancing performance and fidelity under extreme compression. Existing methods struggle with two bottlenecks: image-level selection methods (Coreset Selection, Dataset Quantization) suffer from inefficiency condensation, while pixel-level optimization (Dataset Distillation) introduces semantic distortion due to over-parameterization. With empirical observations, we find that a critical problem in dataset condensation is the oversight of color's dual role as an information carrier and a basic semantic representation unit. We argue that improving the colorfulness of condensed images is beneficial for representation learning. Motivated by this, we propose DC3: a Dataset Condensation framework with Color Compensation. After a calibrated selection strategy, DC3 utilizes the latent diffusion model to enhance the color diversity of an image rather than creating a brand-new one. Extensive experiments demonstrate the superior performance and generalization of DC3 that outperforms SOTA methods across multiple benchmarks. To the best of our knowledge, besides focusing on downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion models with condensed datasets. The FID results prove that training networks with our high-quality datasets is feasible without model collapse or other degradation issues. Code and generated data will be released soon.</li>
</ul>

<h3>Title: Beyond Algorithmic Proofs: Towards Implementation-Level Provable Security</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Shang, Luning Zhang, Zhongxiang Zheng</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01144">https://arxiv.org/abs/2508.01144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01144">https://arxiv.org/pdf/2508.01144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01144]] Beyond Algorithmic Proofs: Towards Implementation-Level Provable Security(https://arxiv.org/abs/2508.01144)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>While traditional cryptographic research focuses on algorithm-level provable security, many real-world attacks exploit weaknesses in system implementations, such as memory mismanagement, poor entropy sources, and insecure key lifecycles. Existing approaches address these risks in isolation but lack a unified, verifiable framework for modeling implementation-layer security. In this work, we propose Implementation-Level Provable Security, a new paradigm that defines security in terms of structurally verifiable resilience against real-world attack surfaces during deployment. To demonstrate its feasibility, we present SEER (Secure and Efficient Encryption-based Erasure via Ransomware), a file destruction system that repurposes and reinforces the encryption core of Babuk ransomware. SEER incorporates key erasure, entropy validation, and execution consistency checks to ensure a well-constrained, auditable attack surface. Our evaluation shows that SEER achieves strong irrecoverability guarantees while maintaining practical performance. This work demonstrates a shift from abstract theoretical models toward practically verifiable implementation-layer security.</li>
</ul>

<h3>Title: DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Kotaro Yoshida, Yuji Naraki, Takafumi Horie, Ryotaro Shimizu, Hiroki Naganuma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01148">https://arxiv.org/abs/2508.01148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01148">https://arxiv.org/pdf/2508.01148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01148]] DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging(https://arxiv.org/abs/2508.01148)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Model merging has emerged as an efficient and flexible paradigm for multi-task learning, with numerous methods being proposed in recent years. However, these state-of-the-art techniques are typically evaluated on benchmark suites that are highly favorable to model merging, and their robustness in more realistic settings remains largely unexplored. In this work, we first investigate the vulnerabilities of model-merging methods and pinpoint the source-model characteristics that critically underlie them. Specifically, we identify two factors that are particularly harmful to the merging process: (1) disparities in task vector norms, and (2) the low confidence of the source models. To address this issue, we propose DisTaC (Distillation for Task vector Conditioning), a novel method that pre-conditions these problematic task vectors before the merge. DisTaC leverages knowledge distillation to adjust a task vector's norm and increase source-model confidence while preserving its essential task-specific knowledge. Our extensive experiments demonstrate that by pre-conditioning task vectors with DisTaC, state-of-the-art merging techniques can successfully integrate models exhibiting the harmful traits -- where they would otherwise fail -- achieving significant performance gains.</li>
</ul>

<h3>Title: OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding</h3>
<ul>
<li><strong>Authors: </strong>Dianyi Yang, Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, Yufeng Yue, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01150">https://arxiv.org/abs/2508.01150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01150">https://arxiv.org/pdf/2508.01150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01150]] OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding(https://arxiv.org/abs/2508.01150)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements in 3D scene understanding have made significant strides in enabling interaction with scenes using open-vocabulary queries, particularly for VR/AR and robotic applications. Nevertheless, existing methods are hindered by rigid offline pipelines and the inability to provide precise 3D object-level understanding given open-ended queries. In this paper, we present OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that improves semantic modeling and refines object-level understanding. OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed Distance Field to facilitate lossless fusion of semantic features on-the-fly. Furthermore, we introduce a novel multimodal language-guided approach named MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D objects by adaptively adjusting similarity thresholds, achieving an improvement 17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments demonstrate that our method outperforms existing methods in 3D object understanding and scene reconstruction quality, as well as showcasing its effectiveness in language-guided scene interaction. The code is available at this https URL .</li>
</ul>

<h3>Title: Personalized Safety Alignment for Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Lei, Jinbin Bai, Qingyu Shi, Aosong Feng, Kaidong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01151">https://arxiv.org/abs/2508.01151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01151">https://arxiv.org/pdf/2508.01151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01151]] Personalized Safety Alignment for Text-to-Image Diffusion Models(https://arxiv.org/abs/2508.01151)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have revolutionized visual content generation, but current safety mechanisms apply uniform standards that often fail to account for individual user preferences. These models overlook the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. We introduce a new dataset, Sage, which captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism. Experiments show that PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores. Our code, data, and models are publicly available at this https URL.</li>
</ul>

<h3>Title: LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Yan, Meijun Sun, Ge-Peng Ji, Fahad Shahbaz Khan, Salman Khan, Deng-Ping Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01152">https://arxiv.org/abs/2508.01152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01152">https://arxiv.org/pdf/2508.01152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01152]] LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation(https://arxiv.org/abs/2508.01152)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>We present LawDIS, a language-window-based controllable dichotomous image segmentation (DIS) framework that produces high-quality object masks. Our framework recasts DIS as an image-conditioned mask generation task within a latent diffusion model, enabling seamless integration of user controls. LawDIS is enhanced with macro-to-micro control modes. Specifically, in macro mode, we introduce a language-controlled segmentation strategy (LS) to generate an initial mask based on user-provided language prompts. In micro mode, a window-controlled refinement strategy (WR) allows flexible refinement of user-defined regions (i.e., size-adjustable windows) within the initial mask. Coordinated by a mode switcher, these modes can operate independently or jointly, making the framework well-suited for high-accuracy, personalised applications. Extensive experiments on the DIS5K benchmark reveal that our LawDIS significantly outperforms 11 cutting-edge methods across all metrics. Notably, compared to the second-best model MVANet, we achieve $F_\beta^\omega$ gains of 4.6\% with both the LS and WR strategies and 3.6\% gains with only the LS strategy on DIS-TE. Codes will be made available at this https URL.</li>
</ul>

<h3>Title: TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xiahan Yang, Hui Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01153">https://arxiv.org/abs/2508.01153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01153">https://arxiv.org/pdf/2508.01153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01153]] TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition(https://arxiv.org/abs/2508.01153)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Scene Text Recognition (STR) remains a challenging task due to complex visual appearances and limited semantic priors. We propose TEACH, a novel training paradigm that injects ground-truth text into the model as auxiliary input and progressively reduces its influence during training. By encoding target labels into the embedding space and applying loss-aware masking, TEACH simulates a curriculum learning process that guides the model from label-dependent learning to fully visual recognition. Unlike language model-based approaches, TEACH requires no external pretraining and introduces no inference overhead. It is model-agnostic and can be seamlessly integrated into existing encoder-decoder frameworks. Extensive experiments across multiple public benchmarks show that models trained with TEACH achieve consistently improved accuracy, especially under challenging conditions, validating its robustness and general applicability.</li>
</ul>

<h3>Title: Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates</h3>
<ul>
<li><strong>Authors: </strong>Liam G. McCoy, Fateme Nateghi Haredasht, Kanav Chopra, David Wu, David JH Wu, Abass Conteh, Sarita Khemani, Saloni Kumar Maharaj, Vishnu Ravi, Arth Pahwa, Yingjie Weng, Leah Rosengaus, Lena Giang, Kelvin Zhenghao Li, Olivia Jee, Daniel Shirvani, Ethan Goh, Jonathan H. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01159">https://arxiv.org/abs/2508.01159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01159">https://arxiv.org/pdf/2508.01159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01159]] Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates(https://arxiv.org/abs/2508.01159)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates the capacity of large language models (LLMs) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician communication.</li>
</ul>

<h3>Title: T2S: Tokenized Skill Scaling for Lifelong Imitation Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongquan Zhang, Jingyu Gong, Zhizhong Zhang, Xin Tan, Yanyun Qu, Yuan Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01167">https://arxiv.org/abs/2508.01167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01167">https://arxiv.org/pdf/2508.01167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01167]] T2S: Tokenized Skill Scaling for Lifelong Imitation Learning(https://arxiv.org/abs/2508.01167)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The main challenge in lifelong imitation learning lies in the balance between mitigating catastrophic forgetting of previous skills while maintaining sufficient capacity for acquiring new ones. However, current approaches typically address these aspects in isolation, overlooking their internal correlation in lifelong skill acquisition. We address this limitation with a unified framework named Tokenized Skill Scaling (T2S). Specifically, by tokenizing the model parameters, the linear parameter mapping of the traditional transformer is transformed into cross-attention between input and learnable tokens, thereby enhancing model scalability through the easy extension of new tokens. Additionally, we introduce language-guided skill scaling to transfer knowledge across tasks efficiently and avoid linearly growing parameters. Extensive experiments across diverse tasks demonstrate that T2S: 1) effectively prevents catastrophic forgetting (achieving an average NBT of 1.0% across the three LIBERO task suites), 2) excels in new skill scaling with minimal increases in trainable parameters (needing only 8.0% trainable tokens in an average of lifelong tasks), and 3) enables efficient knowledge transfer between tasks (achieving an average FWT of 77.7% across the three LIBERO task suites), offering a promising solution for lifelong imitation learning.</li>
</ul>

<h3>Title: DELTAv2: Accelerating Dense 3D Tracking</h3>
<ul>
<li><strong>Authors: </strong>Tuan Duc Ngo, Ashkan Mirzaei, Guocheng Qian, Hanwen Liang, Chuang Gan, Evangelos Kalogerakis, Peter Wonka, Chaoyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01170">https://arxiv.org/abs/2508.01170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01170">https://arxiv.org/pdf/2508.01170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01170]] DELTAv2: Accelerating Dense 3D Tracking(https://arxiv.org/abs/2508.01170)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose a novel algorithm for accelerating dense long-term 3D point tracking in videos. Through analysis of existing state-of-the-art methods, we identify two major computational bottlenecks. First, transformer-based iterative tracking becomes expensive when handling a large number of trajectories. To address this, we introduce a coarse-to-fine strategy that begins tracking with a small subset of points and progressively expands the set of tracked trajectories. The newly added trajectories are initialized using a learnable interpolation module, which is trained end-to-end alongside the tracking network. Second, we propose an optimization that significantly reduces the cost of correlation feature computation, another key bottleneck in prior methods. Together, these improvements lead to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy.</li>
</ul>

<h3>Title: No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views</h3>
<ul>
<li><strong>Authors: </strong>Ranran Huang, Krystian Mikolajczyk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01171">https://arxiv.org/abs/2508.01171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01171">https://arxiv.org/pdf/2508.01171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01171]] No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views(https://arxiv.org/abs/2508.01171)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: this https URL.</li>
</ul>

<h3>Title: MARS: A Meta-Adaptive Reinforcement Learning Framework for Risk-Aware Multi-Agent Portfolio Management</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Chen, Jing Li, Guiling Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01173">https://arxiv.org/abs/2508.01173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01173">https://arxiv.org/pdf/2508.01173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01173]] MARS: A Meta-Adaptive Reinforcement Learning Framework for Risk-Aware Multi-Agent Portfolio Management(https://arxiv.org/abs/2508.01173)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has shown significant promise in automated portfolio management; however, effectively balancing risk and return remains a central challenge, as many models fail to adapt to dynamically changing market conditions. In this paper, we propose Meta-controlled Agents for a Risk-aware System (MARS), a novel RL framework designed to explicitly address this limitation through a multi-agent, risk-aware approach. Instead of a single monolithic model, MARS employs a Heterogeneous Agent Ensemble where each agent possesses a unique, intrinsic risk profile. This profile is enforced by a dedicated Safety-Critic network and a specific risk-tolerance threshold, allowing agents to specialize in behaviors ranging from capital preservation to aggressive growth. To navigate different market regimes, a high-level Meta-Adaptive Controller (MAC) learns to dynamically orchestrate the ensemble. By adjusting its reliance on conservative versus aggressive agents, the MAC effectively lowers portfolio volatility during downturns and seeks higher returns in bull markets, thus minimizing maximum drawdown and enhancing overall stability. This two-tiered structure allows MARS to generate a disciplined and adaptive portfolio that is robust to market fluctuations. The framework achieves a superior balance between risk and return by leveraging behavioral diversity rather than explicit market-feature engineering. Experiments on major international stock indexes, including periods of significant financial crisis, demonstrate the efficacy of our framework on risk-adjusted criteria, significantly reducing maximum drawdown and volatility while maintaining competitive returns.</li>
</ul>

<h3>Title: RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaichen Zhang, Shenghao Gao, Yuzhong Hong, Haipeng Sun, Junwei Bao, Hongfei Jiang, Yang Song, Hong Dingqian, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01174">https://arxiv.org/abs/2508.01174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01174">https://arxiv.org/pdf/2508.01174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01174]] RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large Language Models(https://arxiv.org/abs/2508.01174)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current large language model post-training optimizes a risk-neutral objective that maximizes expected reward, yet evaluation relies heavily on risk-seeking metrics like Pass@k (at least one success in k trials) and Max@k (maximum reward across k responses). This mismatch in risk preferences can inevitably lead to suboptimal performance. To bridge this gap, we propose Risk-Seeking Policy Optimization (RSPO), a novel method that directly targets Pass@k and Max@k during training. A key challenge in optimizing these metrics is the "hitchhiking" problem: low-reward responses are inadvertently reinforced if they co-occur with a high-reward response within a sample of k generations, resulting in inefficient optimization. RSPO addresses this problem by leveraging the closed-form probability that a given response is the maximum among k samplings. Despite the complexity of nested gradients over multiple responses, RSPO produces efficient, unbiased gradient estimators for both metrics. We validate our approach with both rigorous theoretical analysis and comprehensive experimental results.</li>
</ul>

<h3>Title: BSL: A Unified and Generalizable Multitask Learning Platform for Virtual Drug Discovery from Design to Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Kun Li, Zhennan Wu, Yida Xiong, Hongzhi Zhang, Longtao Hu, Zhonglie Liu, Junqi Zeng, Wenjie Wu, Mukun Chen, Jiameng Chen, Wenbin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01195">https://arxiv.org/abs/2508.01195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01195">https://arxiv.org/pdf/2508.01195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01195]] BSL: A Unified and Generalizable Multitask Learning Platform for Virtual Drug Discovery from Design to Synthesis(https://arxiv.org/abs/2508.01195)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Drug discovery is of great social significance in safeguarding human health, prolonging life, and addressing the challenges of major diseases. In recent years, artificial intelligence has demonstrated remarkable advantages in key tasks across bioinformatics and pharmacology, owing to its efficient data processing and data representation capabilities. However, most existing computational platforms cover only a subset of core tasks, leading to fragmented workflows and low efficiency. In addition, they often lack algorithmic innovation and show poor generalization to out-of-distribution (OOD) data, which greatly hinders the progress of drug discovery. To address these limitations, we propose Baishenglai (BSL), a deep learning-enhanced, open-access platform designed for virtual drug discovery. BSL integrates seven core tasks within a unified and modular framework, incorporating advanced technologies such as generative models and graph neural networks. In addition to achieving state-of-the-art (SOTA) performance on multiple benchmark datasets, the platform emphasizes evaluation mechanisms that focus on generalization to OOD molecular structures. Comparative experiments with existing platforms and baseline methods demonstrate that BSL provides a comprehensive, scalable, and effective solution for virtual drug discovery, offering both algorithmic innovation and high-precision prediction for real-world pharmaceutical research. In addition, BSL demonstrated its practical utility by discovering novel modulators of the GluN1/GluN3A NMDA receptor, successfully identifying three compounds with clear bioactivity in in-vitro electrophysiological assays. These results highlight BSL as a promising and comprehensive platform for accelerating biomedical research and drug discovery. The platform is accessible at this https URL.</li>
</ul>

<h3>Title: A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding</h3>
<ul>
<li><strong>Authors: </strong>Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01197">https://arxiv.org/abs/2508.01197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01197">https://arxiv.org/pdf/2508.01197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01197]] A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding(https://arxiv.org/abs/2508.01197)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at this https URL.</li>
</ul>

<h3>Title: Adaptive Content Restriction for Large Language Models via Suffix Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yige Li, Peihai Jiang, Jun Sun, Peng Shu, Tianming Liu, Zhen Xiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01198">https://arxiv.org/abs/2508.01198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01198">https://arxiv.org/pdf/2508.01198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01198]] Adaptive Content Restriction for Large Language Models via Suffix Optimization(https://arxiv.org/abs/2508.01198)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant success across diverse applications. However, enforcing content restrictions remains a significant challenge due to their expansive output space. One aspect of content restriction is preventing LLMs from generating harmful content via model alignment approaches such as supervised fine-tuning (SFT). Yet, the need for content restriction may vary significantly across user groups, change rapidly over time, and not always align with general definitions of harmfulness. Applying SFT to each of these specific use cases is impractical due to the high computational, data, and storage demands. Motivated by this need, we propose a new task called \textit{Adaptive Content Restriction} (AdaCoRe), which focuses on lightweight strategies -- methods without model fine-tuning -- to prevent deployed LLMs from generating restricted terms for specific use cases. We propose the first method for AdaCoRe, named \textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to any prompt to a) prevent a target LLM from generating a set of restricted terms, while b) preserving the output quality. To evaluate AdaCoRe approaches, including our SOP, we create a new \textit{Content Restriction Benchmark} (CoReBench), which contains 400 prompts for 80 restricted terms across 8 carefully selected categories. We demonstrate the effectiveness of SOP on CoReBench, which outperforms the system-level baselines such as system suffix by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B, Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also demonstrate that SOP is effective on POE, an online platform hosting various commercial LLMs, highlighting its practicality in real-world scenarios.</li>
</ul>

<h3>Title: Showcasing standards and approaches for cybersecurity, safety, and privacy issues in connected and autonomous vehicles</h3>
<ul>
<li><strong>Authors: </strong>Ricardo M. Czekster</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01207">https://arxiv.org/abs/2508.01207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01207">https://arxiv.org/pdf/2508.01207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01207]] Showcasing standards and approaches for cybersecurity, safety, and privacy issues in connected and autonomous vehicles(https://arxiv.org/abs/2508.01207)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>In the automotive industry there is a need to handle broad quality deficiencies, eg, performance, maintainability, cybersecurity, safety, and privacy, to mention a few. The idea is to prevent these issues from reaching end-users, ie, road users and inadvertently, pedestrians, aiming to potentially reduce accidents, and allow safe operation in dynamic attack surfaces, for the benefit of a host of stakeholders. This paper aims to bridge cybersecurity, safety, and privacy concerns in Connected and Autonomous Vehicles (CAV) with respect to Risk Assessment (RA) and Threat Modelling (TM) altogether. Practitioners know the vast literature on this topic given the sheer number of recommendations, standards, best practices, and existing approaches, at times impairing projects and fostering valuable and actionable threat analysis. In this paper we collate key outcomes by highlighting latest standards and approaches in RA and TM research to tackle complex attack surfaces as the ones posed by automotive settings. We aim to provide the community with a list of approaches to align expectations with stakeholders when deciding where and when to focus threat related analysis in automotive solutions.</li>
</ul>

<h3>Title: RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification</h3>
<ul>
<li><strong>Authors: </strong>Tianze Wang, Zhang Zhang, Chao Yue, Nuoran Li, Chao Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01210">https://arxiv.org/abs/2508.01210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01210">https://arxiv.org/pdf/2508.01210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01210]] RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification(https://arxiv.org/abs/2508.01210)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Acquiring the road surface conditions in advance based on visual technologies provides effective information for the planning and control system of autonomous vehicles, thus improving the safety and driving comfort of the vehicles. Recently, the Mamba architecture based on state-space models has shown remarkable performance in visual processing tasks, benefiting from the efficient global receptive field. However, existing Mamba architectures struggle to achieve state-of-the-art visual road surface classification due to their lack of effective extraction of the local texture of the road surface. In this paper, we explore for the first time the potential of visual Mamba architectures for road surface classification task and propose a method that effectively combines local and global perception, called RoadMamba. Specifically, we utilize the Dual State Space Model (DualSSM) to effectively extract the global semantics and local texture of the road surface and decode and fuse the dual features through the Dual Attention Fusion (DAF). In addition, we propose a dual auxiliary loss to explicitly constrain dual branches, preventing the network from relying only on global semantic information from the deep large receptive field and ignoring the local texture. The proposed RoadMamba achieves the state-of-the-art performance in experiments on a large-scale road surface classification dataset containing 1 million samples.</li>
</ul>

<h3>Title: StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Yuanlin Yang, Quanjian Song, Zhexian Gao, Ge Wang, Shanshan Li, Xiaoyan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01215">https://arxiv.org/abs/2508.01215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01215">https://arxiv.org/pdf/2508.01215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01215]] StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling(https://arxiv.org/abs/2508.01215)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as the dominant paradigm for style transfer, but their text-driven mechanism is hindered by a core limitation: it treats textual descriptions as uniform, monolithic guidance. This limitation overlooks the semantic gap between the non-spatial nature of textual descriptions and the spatially-aware attributes of visual style, often leading to the loss of semantic structure and fine-grained details during stylization. In this paper, we propose StyDeco, an unsupervised framework that resolves this limitation by learning text representations specifically tailored for the style transfer task. Our framework first employs Prior-Guided Data Distillation (PGD), a strategy designed to distill stylistic knowledge without human supervision. It leverages a powerful frozen generative model to automatically synthesize pseudo-paired data. Subsequently, we introduce Contrastive Semantic Decoupling (CSD), a task-specific objective that adapts a text encoder using domain-specific weights. CSD performs a two-class clustering in the semantic space, encouraging source and target representations to form distinct clusters. Extensive experiments on three classic benchmarks demonstrate that our framework outperforms several existing approaches in both stylistic fidelity and structural preservation, highlighting its effectiveness in style transfer with semantic preservation. In addition, our framework supports a unique de-stylization process, further demonstrating its extensibility. Our code is vailable at this https URL.</li>
</ul>

<h3>Title: Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?</h3>
<ul>
<li><strong>Authors: </strong>Bolei Chen, Shengsheng Yan, Yongzheng Cui, Jiaxu Kang, Ping Zhong, Jianxin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01216">https://arxiv.org/abs/2508.01216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01216">https://arxiv.org/pdf/2508.01216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01216]] Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?(https://arxiv.org/abs/2508.01216)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Since a building's floorplan remains consistent over time and is inherently robust to changes in visual appearance, visual Floorplan Localization (FLoc) has received increasing attention from researchers. However, as a compact and minimalist representation of the building's layout, floorplans contain many repetitive structures (e.g., hallways and corners), thus easily result in ambiguous localization. Existing methods either pin their hopes on matching 2D structural cues in floorplans or rely on 3D geometry-constrained visual pre-trainings, ignoring the richer contextual information provided by visual images. In this paper, we suggest using broader visual scene context to empower FLoc algorithms with scene layout priors to eliminate localization uncertainty. In particular, we propose an unsupervised learning technique with clustering constraints to pre-train a room discriminator on self-collected unlabeled room images. Such a discriminator can empirically extract the hidden room type of the observed image and distinguish it from other room types. By injecting the scene context information summarized by the discriminator into an FLoc algorithm, the room style knowledge is effectively exploited to guide definite visual FLoc. We conducted sufficient comparative studies on two standard visual Floc benchmarks. Our experiments show that our approach outperforms state-of-the-art methods and achieves significant improvements in robustness and accuracy.</li>
</ul>

<h3>Title: WebDS: An End-to-End Benchmark for Web-based Data Science</h3>
<ul>
<li><strong>Authors: </strong>Ethan Hsu, Hong Meng Yam, Ines Bouissou, Aaron Murali John, Raj Thota, Josh Koe, Vivek Sarath Putta, G K Dharesan, Alexander Spangher, Shikhar Murty, Tenghao Huang, Christopher D. Manning</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01222">https://arxiv.org/abs/2508.01222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01222">https://arxiv.org/pdf/2508.01222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01222]] WebDS: An End-to-End Benchmark for Web-based Data Science(https://arxiv.org/abs/2508.01222)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A large portion of real-world data science tasks are complex and require multi-hop web-based interactions: finding appropriate data available on the internet, synthesizing real-time data of various modalities from different locations, and producing summarized analyses. Existing web benchmarks often focus on simplistic interactions, such as form submissions or e-commerce transactions, and often do not require diverse tool-using capabilities required for web based data science. Conversely, traditional data science benchmarks typically concentrate on static, often textually bound datasets and do not assess end-to-end workflows that encompass data acquisition, cleaning, analysis, and insight generation. In response, we introduce WebDS, the first end-to-end web-based data science benchmark. It comprises 870 web-based data science tasks across 29 diverse websites from structured government data portals to unstructured news media, challenging agents to perform complex, multi-step operations requiring the use of tools and heterogeneous data formats that better reflect the realities of modern data analytics. Evaluations of current SOTA LLM agents indicate significant performance gaps in accomplishing these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web Voyager, successfully completes only 15% of tasks in WebDS, which our analysis suggests is due to new failure modes like poor information grounding, repetitive behavior and shortcut-taking that agents performing WebDS' tasks display. By providing a more robust and realistic testing ground, WebDS sets the stage for significant advances in the development of practically useful LLM-based data science.</li>
</ul>

<h3>Title: WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework</h3>
<ul>
<li><strong>Authors: </strong>Yue Chen, Minghua He, Fangkai Yang, Pu Zhao, Lu Wang, Yu Kang, Yifei Dong, Yuefeng Zhan, Hao Sun, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01245">https://arxiv.org/abs/2508.01245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01245">https://arxiv.org/pdf/2508.01245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01245]] WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework(https://arxiv.org/abs/2508.01245)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in solving mathematical problems, yet their performance is often limited by the availability of high-quality, diverse training data. Existing methods focus on augmenting datasets through rephrasing or difficulty progression but overlook the specific failure modes of LLMs. This results in synthetic questions that the model can already solve, providing minimal performance gains. To address this, we propose WarriorMath, a defect-aware framework for mathematical problem solving that integrates both targeted data synthesis and progressive training. In the synthesis stage, we employ multiple expert LLMs in a collaborative process to generate, critique, and refine problems. Questions that base LLMs fail to solve are identified and iteratively improved through expert-level feedback, producing high-quality, defect-aware training data. In the training stage, we introduce a progressive learning framework that iteratively fine-tunes the model using increasingly challenging data tailored to its weaknesses. Experiments on six mathematical benchmarks show that WarriorMath outperforms strong baselines by 12.57% on average, setting a new state-of-the-art. Our results demonstrate the effectiveness of a defect-aware, multi-expert framework for improving mathematical ability.</li>
</ul>

<h3>Title: NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiazhen Yan, Fan Wang, Weiwei Jiang, Ziqiang Li, Zhangjie Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01248">https://arxiv.org/abs/2508.01248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01248">https://arxiv.org/pdf/2508.01248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01248]] NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection(https://arxiv.org/abs/2508.01248)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of generative models, such as GANs and diffusion models, has facilitated the creation of highly realistic images, raising growing concerns over their misuse in security-sensitive domains. While existing detectors perform well under known generative settings, they often fail to generalize to unknown generative models, especially when semantic content between real and fake images is closely aligned. In this paper, we revisit the use of CLIP features for AI-generated image detection and uncover a critical limitation: the high-level semantic information embedded in CLIP's visual features hinders effective discrimination. To address this, we propose NS-Net, a novel detection framework that leverages NULL-Space projection to decouple semantic information from CLIP's visual features, followed by contrastive learning to capture intrinsic distributional differences between real and generated images. Furthermore, we design a Patch Selection strategy to preserve fine-grained artifacts by mitigating semantic bias caused by global image structures. Extensive experiments on an open-world benchmark comprising images generated by 40 diverse generative models show that NS-Net outperforms existing state-of-the-art methods, achieving a 7.4\% improvement in detection accuracy, thereby demonstrating strong generalization across both GAN- and diffusion-based image generation techniques.</li>
</ul>

<h3>Title: AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</h3>
<ul>
<li><strong>Authors: </strong>Peiran Wang, Yang Liu, Yunfei Lu, Yifeng Cai, Hongbo Chen, Qingyou Yang, Jie Zhang, Jue Hong, Ye Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01249">https://arxiv.org/abs/2508.01249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01249">https://arxiv.org/pdf/2508.01249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01249]] AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection(https://arxiv.org/abs/2508.01249)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's working traces as graph-based intermediate representations with control flow and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools & data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis over sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can achieve 95.75% of TPR, with only 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect prompt injection vulnerabilities and enforce fine-grained security constraints.</li>
</ul>

<h3>Title: DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqin Wang, Xianxu Hou, Meidan Ding, Junliang Chen, Kaijun Deng, Jinheng Xie, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01250">https://arxiv.org/abs/2508.01250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01250">https://arxiv.org/pdf/2508.01250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01250]] DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing(https://arxiv.org/abs/2508.01250)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Face parsing aims to segment facial images into key components such as eyes, lips, and eyebrows. While existing methods rely on dense pixel-level annotations, such annotations are expensive and labor-intensive to obtain. To reduce annotation cost, we introduce Weakly Supervised Face Parsing (WSFP), a new task setting that performs dense facial component segmentation using only weak supervision, such as image-level labels and natural language descriptions. WSFP introduces unique challenges due to the high co-occurrence and visual similarity of facial components, which lead to ambiguous activations and degraded parsing performance. To address this, we propose DisFaceRep, a representation disentanglement framework designed to separate co-occurring facial components through both explicit and implicit mechanisms. Specifically, we introduce a co-occurring component disentanglement strategy to explicitly reduce dataset-level bias, and a text-guided component disentanglement loss to guide component separation using language supervision implicitly. Extensive experiments on CelebAMask-HQ, LaPa, and Helen demonstrate the difficulty of WSFP and the effectiveness of DisFaceRep, which significantly outperforms existing weakly supervised semantic segmentation methods. The code will be released at \href{this https URL}{\textcolor{cyan}{this https URL}}.</li>
</ul>

<h3>Title: Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Hung-Chieh Fang, Hsuan-Tien Lin, Irwin King, Yifei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01251">https://arxiv.org/abs/2508.01251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01251">https://arxiv.org/pdf/2508.01251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01251]] Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning(https://arxiv.org/abs/2508.01251)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Unsupervised Learning (FUL) aims to learn expressive representations in federated and self-supervised settings. The quality of representations learned in FUL is usually determined by uniformity, a measure of how uniformly representations are distributed in the embedding space. However, existing solutions perform well in achieving intra-client (local) uniformity for local models while failing to achieve inter-client (global) uniformity after aggregation due to non-IID data distributions and the decentralized nature of FUL. To address this issue, we propose Soft Separation and Distillation (SSD), a novel approach that preserves inter-client uniformity by encouraging client representations to spread toward different directions. This design reduces interference during client model aggregation, thereby improving global uniformity while preserving local representation expressiveness. We further enhance this effect by introducing a projector distillation module to address the discrepancy between loss optimization and representation quality. We evaluate SSD in both cross-silo and cross-device federated settings, demonstrating consistent improvements in representation quality and task performance across various training scenarios. Our results highlight the importance of inter-client uniformity in FUL and establish SSD as an effective solution to this challenge. Project page: this https URL</li>
</ul>

<h3>Title: ODOV: Towards Open-Domain Open-Vocabulary Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Zhang, Ruize Han, Fangnan Zhou, Song Wang, Wei Feng, Liang Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01253">https://arxiv.org/abs/2508.01253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01253">https://arxiv.org/pdf/2508.01253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01253]] ODOV: Towards Open-Domain Open-Vocabulary Object Detection(https://arxiv.org/abs/2508.01253)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we handle a new problem of Open-Domain Open-Vocabulary (ODOV) object detection, which considers the detection model's adaptability to the real world including both domain and category shifts. For this problem, we first construct a new benchmark OD-LVIS, which includes 46,949 images, covers 18 complex real-world domains and 1,203 categories, and provides a comprehensive dataset for evaluating real-world object detection. Besides, we develop a novel baseline method for ODOV this http URL proposed method first leverages large language models to generate the domain-agnostic text prompts for category embedding. It further learns the domain embedding from the given image, which, during testing, can be integrated into the category embedding to form the customized domain-specific category embedding for each test image. We provide sufficient benchmark evaluations for the proposed ODOV detection task and report the results, which verify the rationale of ODOV detection, the usefulness of our benchmark, and the superiority of the proposed method.</li>
</ul>

<h3>Title: Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025</h3>
<ul>
<li><strong>Authors: </strong>Long S. T. Nguyen, Khang H. N. Vo, Thu H. A. Nguyen, Tuan C. Bui, Duc Q. Nguyen, Thanh-Tung Tran, Anh D. Nguyen, Minh L. Nguyen, Fabien Baldacci, Thang H. Bui, Emanuel Di Nardo, Angelo Ciaramella, Son H. Le, Ihsan Ullah, Lorenzo Di Rocco, Tho T. Quan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01263">https://arxiv.org/abs/2508.01263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01263">https://arxiv.org/pdf/2508.01263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01263]] Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025(https://arxiv.org/abs/2508.01263)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>The growing integration of Artificial Intelligence (AI) into education has intensified the need for transparency and interpretability. While hackathons have long served as agile environments for rapid AI prototyping, few have directly addressed eXplainable AI (XAI) in real-world educational contexts. This paper presents a comprehensive analysis of the XAI Challenge 2025, a hackathon-style competition jointly organized by Ho Chi Minh City University of Technology (HCMUT) and the International Workshop on Trustworthiness and Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked participants with building Question-Answering (QA) systems capable of answering student queries about university policies while generating clear, logic-based natural language explanations. To promote transparency and trustworthiness, solutions were required to use lightweight Large Language Models (LLMs) or hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed via logic-based templates with Z3 validation and refined through expert student review to ensure alignment with real-world academic scenarios. We describe the challenge's motivation, structure, dataset construction, and evaluation protocol. Situating the competition within the broader evolution of AI hackathons, we argue that it represents a novel effort to bridge LLMs and symbolic reasoning in service of explainability. Our findings offer actionable insights for future XAI-centered educational systems and competitive research initiatives.</li>
</ul>

<h3>Title: Enhancing Diffusion-based Dataset Distillation via Adversary-Guided Curriculum Sampling</h3>
<ul>
<li><strong>Authors: </strong>Lexiao Zou, Gongwei Chen, Yanda Chen, Miao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01264">https://arxiv.org/abs/2508.01264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01264">https://arxiv.org/pdf/2508.01264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01264]] Enhancing Diffusion-based Dataset Distillation via Adversary-Guided Curriculum Sampling(https://arxiv.org/abs/2508.01264)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to encapsulate the rich information contained in dataset into a compact distilled dataset but it faces performance degradation as the image-per-class (IPC) setting or image resolution grows larger. Recent advancements demonstrate that integrating diffusion generative models can effectively facilitate the compression of large-scale datasets while maintaining efficiency due to their superiority in matching data distribution and summarizing representative patterns. However, images sampled from diffusion models are always blamed for lack of diversity which may lead to information redundancy when multiple independent sampled images are aggregated as a distilled dataset. To address this issue, we propose Adversary-guided Curriculum Sampling (ACS), which partitions the distilled dataset into multiple curricula. For generating each curriculum, ACS guides diffusion sampling process by an adversarial loss to challenge a discriminator trained on sampled images, thus mitigating information overlap between curricula and fostering a more diverse distilled dataset. Additionally, as the discriminator evolves with the progression of curricula, ACS generates images from simpler to more complex, ensuring efficient and systematic coverage of target data informational spectrum. Extensive experiments demonstrate the effectiveness of ACS, which achieves substantial improvements of 4.1\% on Imagewoof and 2.1\% on ImageNet-1k over the state-of-the-art.</li>
</ul>

<h3>Title: ModelNet40-E: An Uncertainty-Aware Benchmark for Point Cloud Classification</h3>
<ul>
<li><strong>Authors: </strong>Pedro Alonso, Tianrui Li, Chongshou Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01269">https://arxiv.org/abs/2508.01269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01269">https://arxiv.org/pdf/2508.01269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01269]] ModelNet40-E: An Uncertainty-Aware Benchmark for Point Cloud Classification(https://arxiv.org/abs/2508.01269)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We introduce ModelNet40-E, a new benchmark designed to assess the robustness and calibration of point cloud classification models under synthetic LiDAR-like noise. Unlike existing benchmarks, ModelNet40-E provides both noise-corrupted point clouds and point-wise uncertainty annotations via Gaussian noise parameters ({\sigma}, {\mu}), enabling fine-grained evaluation of uncertainty modeling. We evaluate three popular models-PointNet, DGCNN, and Point Transformer v3-across multiple noise levels using classification accuracy, calibration metrics, and uncertainty-awareness. While all models degrade under increasing noise, Point Transformer v3 demonstrates superior calibration, with predicted uncertainties more closely aligned with the underlying measurement uncertainty.</li>
</ul>

<h3>Title: PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zonglei Jing, Xiao Yang, Xiaoqian Li, Siyuan Liang, Aishan Liu, Mingchuan Zhang, Xianglong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01272">https://arxiv.org/abs/2508.01272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01272">https://arxiv.org/pdf/2508.01272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01272]] PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation(https://arxiv.org/abs/2508.01272)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models have demonstrated remarkable generative capabilities but remain vulnerable to producing not-safe-for-work (NSFW) content, such as violent or explicit imagery. While recent moderation efforts have introduced soft prompt-guided tuning by appending defensive tokens to the input, these approaches often rely on large-scale curated image-text datasets and apply static, one-size-fits-all defenses at inference time. However, this results not only in high computational cost and degraded benign image quality, but also in limited adaptability to the diverse and nuanced safety requirements of real-world prompts. To address these challenges, we propose PromptSafe, a gated prompt tuning framework that combines a lightweight, text-only supervised soft embedding with an inference-time gated control network. Instead of training on expensive image-text datasets, we first rewrite unsafe prompts into semantically aligned but safe alternatives using an LLM, constructing an efficient text-only training corpus. Based on this, we optimize a universal soft prompt that repels unsafe and attracts safe embeddings during the diffusion denoising process. To avoid over-suppressing benign prompts, we introduce a gated mechanism that adaptively adjusts the defensive strength based on estimated prompt toxicity, thereby aligning defense intensity with prompt risk and ensuring strong protection for harmful inputs while preserving benign generation quality. Extensive experiments across multiple benchmarks and T2I models show that PromptSafe achieves a SOTA unsafe generation rate (2.36%), while preserving high benign fidelity. Furthermore, PromptSafe demonstrates strong generalization to unseen harmful categories, robust transferability across diffusion model architectures, and resilience under adaptive adversarial attacks, highlighting its practical value for safe and scalable deployment.</li>
</ul>

<h3>Title: Defending Against Beta Poisoning Attacks in Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Nilufer Gulciftci, M. Emre Gursoy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01276">https://arxiv.org/abs/2508.01276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01276">https://arxiv.org/pdf/2508.01276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01276]] Defending Against Beta Poisoning Attacks in Machine Learning Models(https://arxiv.org/abs/2508.01276)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Poisoning attacks, in which an attacker adversarially manipulates the training dataset of a machine learning (ML) model, pose a significant threat to ML security. Beta Poisoning is a recently proposed poisoning attack that disrupts model accuracy by making the training dataset linearly nonseparable. In this paper, we propose four defense strategies against Beta Poisoning attacks: kNN Proximity-Based Defense (KPB), Neighborhood Class Comparison (NCC), Clustering-Based Defense (CBD), and Mean Distance Threshold (MDT). The defenses are based on our observations regarding the characteristics of poisoning samples generated by Beta Poisoning, e.g., poisoning samples have close proximity to one another, and they are centered near the mean of the target class. Experimental evaluations using MNIST and CIFAR-10 datasets demonstrate that KPB and MDT can achieve perfect accuracy and F1 scores, while CBD and NCC also provide strong defensive capabilities. Furthermore, by analyzing performance across varying parameters, we offer practical insights regarding defenses' behaviors under varying conditions.</li>
</ul>

<h3>Title: Blockchain security based on cryptography: a review</h3>
<ul>
<li><strong>Authors: </strong>Wenwen Zhou, Dongyang Lyu, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01280">https://arxiv.org/abs/2508.01280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01280">https://arxiv.org/pdf/2508.01280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01280]] Blockchain security based on cryptography: a review(https://arxiv.org/abs/2508.01280)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>As an emerging service framework built by combining cryptography, P2P network, consensus mechanism and innovative contract technology, blockchain has been widely used in digital finance, data sharing, message traceability and electronic evidence preservation because of its decentralised, non-tamperable and transaction traceability. However, with the complex and changeable application scenarios of blockchain technology and the continuous enhancement of blockchain attack technology, the security of the blockchain system has been seriously threatened, dramatically affecting the development and application of blockchain technology. This paper aims to analyse the attacks on blockchain from the perspective of cryptography. Firstly, from the cryptography technology in the blockchain, the principle of hash functions, digital signatures, and other technologies, as well as their role in the blockchain, are introduced. Then, based on the six-layer architecture of the blockchain, the attacks on the data layer, the network layer, the consensus layer, the contract layer, the incentive layer and the application layer are analysed, and the methods to mitigate or resist the attacks are proposed. Secondly, the attack principles of 51% attack, Double-Spending attack, Reentrancy attack, Replay attack, Sybil attack and Timestamp Tampering attack were analysed, and the mitigation or defence solutions for these six attacks were designed. Finally, the core problems to be solved in blockchain technology are summarised, and the future development of blockchain security technology is projected.</li>
</ul>

<h3>Title: Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Yanyan Wang, Hongye Tan, Jiye Liang, Xiaoli Li, Ru Li, Jeff Z.Pan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01290">https://arxiv.org/abs/2508.01290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01290">https://arxiv.org/pdf/2508.01290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01290]] Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities(https://arxiv.org/abs/2508.01290)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) shows impressive performance by supplementing and substituting parametric knowledge in Large Language Models (LLMs). Retrieved knowledge can be divided into three types: explicit answer evidence, implicit answer clue, and insufficient answer context which can be further categorized into totally irrelevant and partially relevant information. Effectively utilizing partially relevant knowledge remains a key challenge for RAG systems, especially in incomplete knowledge base retrieval. Contrary to the conventional view, we propose a new perspective: LLMs can be awakened via partially relevant knowledge already embedded in LLMs. To comprehensively investigate this phenomenon, the triplets located in the gold reasoning path and their variants are used to construct partially relevant knowledge by removing the path that contains the answer. We provide theoretical analysis of the awakening effect in LLMs and support our hypothesis with experiments on two Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we present a new task, Unseen Entity KGQA, simulating real-world challenges where entity linking fails due to KG incompleteness. Our awakening-based approach demonstrates greater efficacy in practical applications, outperforms traditional methods that rely on embedding-based similarity which are prone to returning noisy information.</li>
</ul>

<h3>Title: GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Ngoc Bui Lam Quang, Nam Le Nguyen Binh, Thanh-Huy Nguyen, Le Thien Phuc Nguyen, Quan Nguyen, Ulas Bagci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01293">https://arxiv.org/abs/2508.01293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01293">https://arxiv.org/pdf/2508.01293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01293]] GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification(https://arxiv.org/abs/2508.01293)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multiple Instance Learning (MIL) is the leading approach for whole slide image (WSI) classification, enabling efficient analysis of gigapixel pathology slides. Recent work has introduced vision-language models (VLMs) into MIL pipelines to incorporate medical knowledge through text-based class descriptions rather than simple class names. However, when these methods rely on large language models (LLMs) to generate clinical descriptions or use fixed-length prompts to represent complex pathology concepts, the limited token capacity of VLMs often constrains the expressiveness and richness of the encoded class information. Additionally, descriptions generated solely by LLMs may lack domain grounding and fine-grained medical specificity, leading to suboptimal alignment with visual features. To address these challenges, we propose a vision-language MIL framework with two key contributions: (1) A grounded multi-agent description generation system that leverages curated pathology textbooks and agent specialization (e.g., morphology, spatial context) to produce accurate and diverse clinical descriptions; (2) A text encoding strategy using a list of descriptions rather than a single prompt, capturing fine-grained and complementary clinical signals for better alignment with visual features. Integrated into a VLM-MIL pipeline, our approach shows improved performance over single-prompt class baselines and achieves results comparable to state-of-the-art models, as demonstrated on renal and lung cancer datasets.</li>
</ul>

<h3>Title: FedCD: A Fairness-aware Federated Cognitive Diagnosis Framework</h3>
<ul>
<li><strong>Authors: </strong>Shangshang Yang, Jialin Han, Xiaoshan Yu, Ziwen Wang, Hao Jiang, Haiping Ma, Xingyi Zhang, Geyong Min</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01296">https://arxiv.org/abs/2508.01296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01296">https://arxiv.org/pdf/2508.01296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01296]] FedCD: A Fairness-aware Federated Cognitive Diagnosis Framework(https://arxiv.org/abs/2508.01296)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate, fair</a></li>
<li><strong>Abstract: </strong>Online intelligent education platforms have generated a vast amount of distributed student learning data. This influx of data presents opportunities for cognitive diagnosis (CD) to assess students' mastery of knowledge concepts while also raising significant data privacy and security challenges. To cope with this issue, federated learning (FL) becomes a promising solution by jointly training models across multiple local clients without sharing their original data. However, the data quality problem, caused by the ability differences and educational context differences between different groups/schools of students, further poses a challenge to the fairness of models. To address this challenge, this paper proposes a fairness-aware federated cognitive diagnosis framework (FedCD) to jointly train CD models built upon a novel parameter decoupling-based personalization strategy, preserving privacy of data and achieving precise and fair diagnosis of students on each client. As an FL paradigm, FedCD trains a local CD model for the students in each client based on its local student learning data, and each client uploads its partial model parameters to the central server for parameter aggregation according to the devised innovative personalization strategy. The main idea of this strategy is to decouple model parameters into two parts: the first is used as locally personalized parameters, containing diagnostic function-related model parameters, to diagnose each client's students fairly; the second is the globally shared parameters across clients and the server, containing exercise embedding parameters, which are updated via fairness-aware aggregation, to alleviate inter-school unfairness. Experiments on three real-world datasets demonstrate the effectiveness of the proposed FedCD framework and the personalization strategy compared to five FL approaches under three CD models.</li>
</ul>

<h3>Title: KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference</h3>
<ul>
<li><strong>Authors: </strong>Chenming Tang, Yutong Yang, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01302">https://arxiv.org/abs/2508.01302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01302">https://arxiv.org/pdf/2508.01302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01302]] KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference(https://arxiv.org/abs/2508.01302)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing aims to modify outdated knowledge in large language models (LLMs) efficiently while retaining their powerful capabilities. Most existing methods rely on either parameter-level editing or retrieval-based approaches. In this work, we propose Knowledge Editing alignment with Diverse Augmentation and Self-adaptive inference (KEDAS) to better align LLMs with knowledge editing. In the alignment phase, LLMs learn to apply in-context edited knowledge via low-rank adaptation. During editing, we design a diverse edit augmentation technique to improve the recall of edits. After that, a self-adaptive post-alignment inference mechanism is proposed, in which a filter-based smart retriever is employed to perform a dynamic selection of inference routing. Specifically, irrelevant queries will go through the original pre-alignment model directly, while relevant ones, together with their related edits, go through the model with aligned adapters activated. In experiments, KEDAS secures the highest overall performance scores in 35 out of 36 cases across four datasets with three LLMs on three settings, surpassing its strong knowledge editing alignment counterpart by about 19.8 harmonic mean scores of edit success, locality and portability and outperforming both parameter editing and retrieval-based baselines significantly. Analysis of computational cost and performance on general tasks further validates the robustness and efficiency of KEDAS, indicating that it presents an ideal paradigm of knowledge editing alignment.</li>
</ul>

<h3>Title: Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Shuangli Du, Jing Wang, Minghua Zhao, Zhenyu Xu, Jie Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01303">https://arxiv.org/abs/2508.01303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01303">https://arxiv.org/pdf/2508.01303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01303]] Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation(https://arxiv.org/abs/2508.01303)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>State-of-the-art stereo matching (SM) models trained on synthetic data often fail to generalize to real data domains due to domain differences, such as color, illumination, contrast, and texture. To address this challenge, we leverage data augmentation to expand the training domain, encouraging the model to acquire robust cross-domain feature representations instead of domain-dependent shortcuts. This paper proposes an uncertainty-guided data augmentation (UgDA) method, which argues that the image statistics in RGB space (mean and standard deviation) carry the domain characteristics. Thus, samples in unseen domains can be generated by properly perturbing these statistics. Furthermore, to simulate more potential domains, Gaussian distributions founded on batch-level statistics are poposed to model the unceratinty of perturbation direction and intensity. Additionally, we further enforce feature consistency between original and augmented data for the same scene, encouraging the model to learn structure aware, shortcuts-invariant feature representations. Our approach is simple, architecture-agnostic, and can be integrated into any SM networks. Extensive experiments on several challenging benchmarks have demonstrated that our method can significantly improve the generalization performance of existing SM networks.</li>
</ul>

<h3>Title: D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Weibo Zhou, Lingbo Li, Shangsong Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01309">https://arxiv.org/abs/2508.01309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01309">https://arxiv.org/pdf/2508.01309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01309]] D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation(https://arxiv.org/abs/2508.01309)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>The scarcity and high cost of high-quality question-answering (QA) datasets hinder supervised fine-tuning (SFT) for domain-specific large language models (LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that utilizes LLMs and prompt engineering to produce diverse, high-quality QA datasets from arbitrary textual sources. D-SCoRE integrates $\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T $\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms, such as semantic role transformation, question type balancing, and counterfactual materials, enhance diversity and relevance, overcoming limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade hardware. Its simplicity and scalability enable efficient QA generation and high-performance fine-tuning across domains.</li>
</ul>

<h3>Title: C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor</h3>
<ul>
<li><strong>Authors: </strong>Haoquan Lu, Hanzhe Liang, Jie Zhang, Chenxi Hu, Jinbao Wang, Can Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01311">https://arxiv.org/abs/2508.01311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01311">https://arxiv.org/pdf/2508.01311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01311]] C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor(https://arxiv.org/abs/2508.01311)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>3D Anomaly Detection (AD) has shown great potential in detecting anomalies or defects of high-precision industrial products. However, existing methods are typically trained in a class-specific manner and also lack the capability of learning from emerging classes. In this study, we proposed a continual learning framework named Continual 3D Anomaly Detection (C3D-AD), which can not only learn generalized representations for multi-class point clouds but also handle new classes emerging over this http URL, in the feature extraction module, to extract generalized local features from diverse product types of different tasks efficiently, Kernel Attention with random feature Layer (KAL) is introduced, which normalizes the feature space. Then, to reconstruct data correctly and continually, an efficient Kernel Attention with learnable Advisor (KAA) mechanism is proposed, which learns the information from new categories while discarding redundant old information within both the encoder and decoder. Finally, to keep the representation consistency over tasks, a Reconstruction with Parameter Perturbation (RPP) module is proposed by designing a representation rehearsal loss function, which ensures that the model remembers previous category information and returns category-adaptive this http URL experiments on three public datasets demonstrate the effectiveness of the proposed method, achieving an average performance of 66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD, respectively.</li>
</ul>

<h3>Title: P3P Made Easy</h3>
<ul>
<li><strong>Authors: </strong>Seong Hun Lee, Patrick Vandewalle, Javier Civera</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01312">https://arxiv.org/abs/2508.01312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01312">https://arxiv.org/pdf/2508.01312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01312]] P3P Made Easy(https://arxiv.org/abs/2508.01312)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>We present a novel algebraic solution to the Perspective-Three-Point (P3P) problem, which aims to recover the absolute pose of a calibrated camera from three 2D-3D correspondences. Our method reformulates the problem into a quartic polynomial with coefficients that are analytically simple and computationally efficient. Despite its simplicity, the proposed solver achieves accuracy and runtime performance comparable to state-of-the-art methods. Extensive experiments on synthetic datasets validate its robustness and efficiency. This combination of simplicity and performance makes our solver appealing for both real-time systems and educational contexts, where interpretability and reliability are critical.</li>
</ul>

<h3>Title: Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Abbaspour Onari, Lucie Charlotte Magister, Yaoxin Wu, Amalia Lupi, Dario Creazzo, Mattia Tordin, Luigi Di Donatantonio, Emilio Quaia, Chao Zhang, Isel Grau, Marco S. Nobile, Yingqian Zhang, Pietro Liò</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01316">https://arxiv.org/abs/2508.01316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01316">https://arxiv.org/pdf/2508.01316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01316]] Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust(https://arxiv.org/abs/2508.01316)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Distal myopathy represents a genetically heterogeneous group of skeletal muscle disorders with broad clinical manifestations, posing diagnostic challenges in radiology. To address this, we propose a novel multimodal attention-aware fusion architecture that combines features extracted from two distinct deep learning models, one capturing global contextual information and the other focusing on local details, representing complementary aspects of the input data. Uniquely, our approach integrates these features through an attention gate mechanism, enhancing both predictive performance and interpretability. Our method achieves a high classification accuracy on the BUSI benchmark and a proprietary distal myopathy dataset, while also generating clinically relevant saliency maps that support transparent decision-making in medical diagnosis. We rigorously evaluated interpretability through (1) functionally grounded metrics, coherence scoring against reference masks and incremental deletion analysis, and (2) application-grounded validation with seven expert radiologists. While our fusion strategy boosts predictive performance relative to single-stream and alternative fusion strategies, both quantitative and qualitative evaluations reveal persistent gaps in anatomical specificity and clinical usefulness of the interpretability. These findings highlight the need for richer, context-aware interpretability methods and human-in-the-loop feedback to meet clinicians' expectations in real-world diagnostic settings.</li>
</ul>

<h3>Title: LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points</h3>
<ul>
<li><strong>Authors: </strong>Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Hongfei Yan, Jingang Wang, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01317">https://arxiv.org/abs/2508.01317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01317">https://arxiv.org/pdf/2508.01317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01317]] LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points(https://arxiv.org/abs/2508.01317)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>The advancement of large language models (LLMs) struggles with the scarcity of high-quality, diverse training data. To address this limitation, we propose LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that enables flexible control over discipline and difficulty distributions while balancing KP coverage and popularity. LinkSyn extracts KPs from question-answering (QA) seed data and constructs a KP graph to synthesize diverse QA data from multiple seeds strongly linked by KPs and sampled from graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution value function to guide the adjustment of path sampling probability and balance KP coverage and popularity during graph walks; (2) diffusion-based synthesis via DeepSeek-R1 by leveraging multiple seeds with dense logical associations along each path; and (3) high-difficulty QA enhancement within given disciplines by flexible difficulty adjustments. By executing LinkSyn, we synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens. Extensive experiments on Llama-3 8B demonstrate that continual pre-training with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and CMMLU, establishing new SOTA results. LinkQA consistently enhances performance across model size and initial FLOPs scales.</li>
</ul>

<h3>Title: Fusion Sampling Validation in Data Partitioning for Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Christopher Godwin Udomboso, Caston Sigauke, Ini Adinya</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01325">https://arxiv.org/abs/2508.01325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01325">https://arxiv.org/pdf/2508.01325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01325]] Fusion Sampling Validation in Data Partitioning for Machine Learning(https://arxiv.org/abs/2508.01325)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Effective data partitioning is known to be crucial in machine learning. Traditional cross-validation methods like K-Fold Cross-Validation (KFCV) enhance model robustness but often compromise generalisation assessment due to high computational demands and extensive data shuffling. To address these issues, the integration of the Simple Random Sampling (SRS), which, despite providing representative samples, can result in non-representative sets with imbalanced data. The study introduces a hybrid model, Fusion Sampling Validation (FSV), combining SRS and KFCV to optimise data partitioning. FSV aims to minimise biases and merge the simplicity of SRS with the accuracy of KFCV. The study used three datasets of 10,000, 50,000, and 100,000 samples, generated with a normal distribution (mean 0, variance 1) and initialised with seed 42. KFCV was performed with five folds and ten repetitions, incorporating a scaling factor to ensure robust performance estimation and generalisation capability. FSV integrated a weighted factor to enhance performance and generalisation further. Evaluations focused on mean estimates (ME), variance estimates (VE), mean squared error (MSE), bias, the rate of convergence for mean estimates (ROC\_ME), and the rate of convergence for variance estimates (ROC\_VE). Results indicated that FSV consistently outperformed SRS and KFCV, with ME values of 0.000863, VE of 0.949644, MSE of 0.952127, bias of 0.016288, ROC\_ME of 0.005199, and ROC\_VE of 0.007137. FSV demonstrated superior accuracy and reliability in data partitioning, particularly in resource-constrained environments and extensive datasets, providing practical solutions for effective machine learning implementations.</li>
</ul>

<h3>Title: Large-Scale Diverse Synthesis for Mid-Training</h3>
<ul>
<li><strong>Authors: </strong>Xuemiao Zhang, Chengying Tu, Can Ren, Rongxiang Weng, Hongfei Yan, Jingang Wang, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01326">https://arxiv.org/abs/2508.01326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01326">https://arxiv.org/pdf/2508.01326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01326]] Large-Scale Diverse Synthesis for Mid-Training(https://arxiv.org/abs/2508.01326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The scarcity of high-quality, knowledge-intensive training data hinders the development of large language models (LLMs), as traditional corpora provide limited information. Previous studies have synthesized and integrated corpora-dependent question-answering (QA) data to improve model performance but face challenges in QA data scalability and knowledge diversity, particularly in cross-domain contexts. Furthermore, leveraging our designed discipline and difficulty annotation system, we probe model deficiencies in STEM disciplines and high-difficulty data. To overcome these limitations, we propose a novel diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA dataset. Our synthesis framework: (1) curates seed data from heterogeneous sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade synthesis to boost data diversity and high-difficulty synthesis to mitigate difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output quality. We utilize BoostQA in mid-training, a mid-stage between pre-training and post-training, to optimize domain-specific knowledge acquisition and enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also demonstrates robust scalability, with performance consistently improving as model size, data volume, and initial FLOPs scale.</li>
</ul>

<h3>Title: Referring Remote Sensing Image Segmentation with Cross-view Semantics Interaction Network</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Yang, Lihe Zhang, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01331">https://arxiv.org/abs/2508.01331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01331">https://arxiv.org/pdf/2508.01331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01331]] Referring Remote Sensing Image Segmentation with Cross-view Semantics Interaction Network(https://arxiv.org/abs/2508.01331)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recently, Referring Remote Sensing Image Segmentation (RRSIS) has aroused wide attention. To handle drastic scale variation of remote targets, existing methods only use the full image as input and nest the saliency-preferring techniques of cross-scale information interaction into traditional single-view structure. Although effective for visually salient targets, they still struggle in handling tiny, ambiguous ones in lots of real scenarios. In this work, we instead propose a paralleled yet unified segmentation framework Cross-view Semantics Interaction Network (CSINet) to solve the limitations. Motivated by human behavior in observing targets of interest, the network orchestrates visual cues from remote and close distances to conduct synergistic prediction. In its every encoding stage, a Cross-View Window-attention module (CVWin) is utilized to supplement global and local semantics into close-view and remote-view branch features, finally promoting the unified representation of feature in every encoding stage. In addition, we develop a Collaboratively Dilated Attention enhanced Decoder (CDAD) to mine the orientation property of target and meanwhile integrate cross-view multiscale features. The proposed network seamlessly enhances the exploitation of global and local semantics, achieving significant improvements over others while maintaining satisfactory speed.</li>
</ul>

<h3>Title: BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability</h3>
<ul>
<li><strong>Authors: </strong>Zhenhua Zou, Zhuotao Liu, Lepeng Zhao, Qiuyang Zhan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01332">https://arxiv.org/abs/2508.01332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01332">https://arxiv.org/pdf/2508.01332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01332]] BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability(https://arxiv.org/abs/2508.01332)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>The rapid adoption of agentic AI, powered by large language models (LLMs), is transforming enterprise ecosystems with autonomous agents that execute complex workflows. Yet we observe several key security vulnerabilities in LLM-driven multi-agent systems (MASes): fragmented identity frameworks, insecure communication channels, and inadequate defenses against Byzantine agents or adversarial prompts. In this paper, we present the first systematic analysis of these emerging multi-agent risks and explain why the legacy security strategies cannot effectively address these risks. Afterwards, we propose BlockA2A, the first unified multi-agent trust framework that enables secure and verifiable and agent-to-agent interoperability. At a high level, BlockA2A adopts decentralized identifiers (DIDs) to enable fine-grained cross-domain agent authentication, blockchain-anchored ledgers to enable immutable auditability, and smart contracts to dynamically enforce context-aware access control policies. BlockA2A eliminates centralized trust bottlenecks, ensures message authenticity and execution integrity, and guarantees accountability across agent interactions. Furthermore, we propose a Defense Orchestration Engine (DOE) that actively neutralizes attacks through real-time mechanisms, including Byzantine agent flagging, reactive execution halting, and instant permission revocation. Empirical evaluations demonstrate BlockA2A's effectiveness in neutralizing prompt-based, communication-based, behavioral and systemic MAS attacks. We formalize its integration into existing MAS and showcase a practical implementation for Google's A2A protocol. Experiments confirm that BlockA2A and DOE operate with sub-second overhead, enabling scalable deployment in production LLM-based MAS environments.</li>
</ul>

<h3>Title: Zero-shot Segmentation of Skin Conditions: Erythema with Edit-Friendly Inversion</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Moutselos, Ilias Maglogiannis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01334">https://arxiv.org/abs/2508.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01334">https://arxiv.org/pdf/2508.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01334]] Zero-shot Segmentation of Skin Conditions: Erythema with Edit-Friendly Inversion(https://arxiv.org/abs/2508.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>This study proposes a zero-shot image segmentation framework for detecting erythema (redness of the skin) using edit-friendly inversion in diffusion models. The method synthesizes reference images of the same patient that are free from erythema via generative editing and then accurately aligns these references with the original images. Color-space analysis is performed with minimal user intervention to identify erythematous regions. This approach significantly reduces the reliance on labeled dermatological datasets while providing a scalable and flexible diagnostic support tool by avoiding the need for any annotated training masks. In our initial qualitative experiments, the pipeline successfully isolated facial erythema in diverse cases, demonstrating performance improvements over baseline threshold-based techniques. These results highlight the potential of combining generative diffusion models and statistical color segmentation for computer-aided dermatology, enabling efficient erythema detection without prior training data.</li>
</ul>

<h3>Title: StyleSentinel: Reliable Artistic Copyright Verification via Stylistic Fingerprints</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Chen, Liqin Wang, Wei Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01335">https://arxiv.org/abs/2508.01335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01335">https://arxiv.org/pdf/2508.01335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01335]] StyleSentinel: Reliable Artistic Copyright Verification via Stylistic Fingerprints(https://arxiv.org/abs/2508.01335)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>The versatility of diffusion models in generating customized images has led to unauthorized usage of personal artwork, which poses a significant threat to the intellectual property of artists. Existing approaches relying on embedding additional information, such as perturbations, watermarks, and backdoors, suffer from limited defensive capabilities and fail to protect artwork published online. In this paper, we propose StyleSentinel, an approach for copyright protection of artwork by verifying an inherent stylistic fingerprint in the artist's artwork. Specifically, we employ a semantic self-reconstruction process to enhance stylistic expressiveness within the artwork, which establishes a dense and style-consistent manifold foundation for feature learning. Subsequently, we adaptively fuse multi-layer image features to encode abstract artistic style into a compact stylistic fingerprint. Finally, we model the target artist's style as a minimal enclosing hypersphere boundary in the feature space, transforming complex copyright verification into a robust one-class learning task. Extensive experiments demonstrate that compared with the state-of-the-art, StyleSentinel achieves superior performance on the one-sample verification task. We also demonstrate the effectiveness through online platforms.</li>
</ul>

<h3>Title: SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and Potholes</h3>
<ul>
<li><strong>Authors: </strong>Chuanqi Liang, Jie Fu, Lei Luo, Miao Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01339">https://arxiv.org/abs/2508.01339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01339">https://arxiv.org/pdf/2508.01339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01339]] SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and Potholes(https://arxiv.org/abs/2508.01339)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With increasing demand for ride comfort in new energy vehicles, accurate real-time detection of speed bumps and potholes is critical for predictive suspension control. This paper proposes SBP-YOLO, a lightweight detection framework based on YOLOv11, optimized for embedded deployment. The model integrates GhostConv for efficient computation, VoVGSCSPC for multi-scale feature enhancement, and a Lightweight Efficiency Detection Head (LEDH) to reduce early-stage feature processing costs. A hybrid training strategy combining NWD loss, knowledge distillation, and Albumentations-based weather augmentation improves detection robustness, especially for small and distant targets. Experiments show SBP-YOLO achieves 87.0% mAP (outperforming YOLOv11n by 5.8%) and runs at 139.5 FPS on a Jetson AGX Xavier with TensorRT FP16 quantization. The results validate its effectiveness for real-time road condition perception in intelligent suspension systems.</li>
</ul>

<h3>Title: UEChecker: Detecting Unchecked External Call Vulnerabilities in DApps via Graph Analysis</h3>
<ul>
<li><strong>Authors: </strong>Dechao Kong, Xiaoqi Li, Wenkai Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01343">https://arxiv.org/abs/2508.01343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01343">https://arxiv.org/pdf/2508.01343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01343]] UEChecker: Detecting Unchecked External Call Vulnerabilities in DApps via Graph Analysis(https://arxiv.org/abs/2508.01343)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The increasing number of attacks on the contract layer of DApps has resulted in economic losses amounting to $66 billion. Vulnerabilities arise when contracts interact with external protocols without verifying the results of the calls, leading to exploit entry points such as flash loan attacks and reentrancy attacks. In this paper, we propose UEChecker, a deep learning-based tool that utilizes a call graph and a Graph Convolutional Network to detect unchecked external call vulnerabilities. We design the following components: An edge prediction module that reconstructs the feature representation of nodes and edges in the call graph; A node aggregation module that captures structural information from both the node itself and its neighbors, thereby enhancing feature representation between nodes and improving the model's understanding of the global graph structure; A Conformer Block module that integrates multi-head attention, convolutional modules, and feedforward neural networks to more effectively capture dependencies of different scales within the call graph, extending beyond immediate neighbors and enhancing the performance of vulnerability detection. Finally, we combine these modules with Graph Convolutional Network to detect unchecked external call vulnerabilities. By auditing the smart contracts of 608 DApps, our results show that our tool achieves an accuracy of 87.59% in detecting unchecked external call vulnerabilities. Furthermore, we compare our tool with GAT, LSTM, and GCN baselines, and in the comparison experiments, UEChecker consistently outperforms these models in terms of accuracy.</li>
</ul>

<h3>Title: Convergence Analysis of Aggregation-Broadcast in LoRA-enabled Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Xin Chen, Shuaijun Chen, Omid Tavallaie, Nguyen Tran, Shuhuang Xiang, Albert Zomaya</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01348">https://arxiv.org/abs/2508.01348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01348">https://arxiv.org/pdf/2508.01348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01348]] Convergence Analysis of Aggregation-Broadcast in LoRA-enabled Federated Learning(https://arxiv.org/abs/2508.01348)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training across decentralized data sources while preserving data privacy. However, the growing size of Machine Learning (ML) models poses communication and computation challenges in FL. Low-Rank Adaptation (LoRA) has recently been introduced into FL as an efficient fine-tuning method, reducing communication overhead by updating only a small number of trainable parameters. Despite its effectiveness, how to aggregate LoRA-updated local models on the server remains a critical and understudied problem. In this paper, we provide a unified convergence analysis for LoRA-based FL. We first categories the current aggregation method into two major type: Sum-Product (SP) and Product-Sum (PS). Then we formally define the Aggregation-Broadcast Operator (ABO) and derive a general convergence condition under mild assumptions. Furthermore, we present several sufficient conditions that guarantee convergence of the global model. These theoretical analyze offer a principled understanding of various aggregation strategies. Notably, we prove that the SP and PS aggregation methods both satisfy our convergence condition, but differ in their ability to achieve the optimal convergence rate. Extensive experiments on standard benchmarks validate our theoretical findings.</li>
</ul>

<h3>Title: NATLM: Detecting Defects in NFT Smart Contracts Leveraging LLM</h3>
<ul>
<li><strong>Authors: </strong>Yuanzheng Niu, Xiaoqi Li, Wenkai Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01351">https://arxiv.org/abs/2508.01351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01351">https://arxiv.org/pdf/2508.01351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01351]] NATLM: Detecting Defects in NFT Smart Contracts Leveraging LLM(https://arxiv.org/abs/2508.01351)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Security issues are becoming increasingly significant with the rapid evolution of Non-fungible Tokens (NFTs). As NFTs are traded as digital assets, they have emerged as prime targets for cyber attackers. In the development of NFT smart contracts, there may exist undiscovered defects that could lead to substantial financial losses if exploited. To tackle this issue, this paper presents a framework called NATLM(NFT Assistant LLM), designed to detect potential defects in NFT smart contracts. The framework effectively identifies four common types of vulnerabilities in NFT smart contracts: ERC-721 Reentrancy, Public Burn, Risky Mutable Proxy, and Unlimited Minting. Relying exclusively on large language models (LLMs) for defect detection can lead to a high false-positive rate. To enhance detection performance, NATLM integrates static analysis with LLMs, specifically Gemini Pro 1.5. Initially, NATLM employs static analysis to extract structural, syntactic, and execution flow information from the code, represented through Abstract Syntax Trees (AST) and Control Flow Graphs (CFG). These extracted features are then combined with vectors of known defect examples to create a matrix for input into the knowledge base. Subsequently, the feature vectors and code vectors of the analyzed contract are compared with the contents of the knowledge base. Finally, the LLM performs deep semantic analysis to enhance detection capabilities, providing a more comprehensive and accurate identification of potential security issues. Experimental results indicate that NATLM analyzed 8,672 collected NFT smart contracts, achieving an overall precision of 87.72%, a recall of 89.58%, and an F1 score of 88.94%. The results outperform other baseline experiments, successfully identifying four common types of defects.</li>
</ul>

<h3>Title: ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihan Wang, Rui Zhang, Hongwei Li, Wenshu Fan, Wenbo Jiang, Qingchuan Zhao, Guowen Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01365">https://arxiv.org/abs/2508.01365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01365">https://arxiv.org/pdf/2508.01365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01365]] ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models(https://arxiv.org/abs/2508.01365)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Backdoor attacks pose a significant threat to Large Language Models (LLMs), where adversaries can embed hidden triggers to manipulate LLM's outputs. Most existing defense methods, primarily designed for classification tasks, are ineffective against the autoregressive nature and vast output space of LLMs, thereby suffering from poor performance and high latency. To address these limitations, we investigate the behavioral discrepancies between benign and backdoored LLMs in output space. We identify a critical phenomenon which we term sequence lock: a backdoored model generates the target sequence with abnormally high and consistent confidence compared to benign generation. Building on this insight, we propose ConfGuard, a lightweight and effective detection method that monitors a sliding window of token confidences to identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a near 100\% true positive rate (TPR) and a negligible false positive rate (FPR) in the vast majority of cases. Crucially, the ConfGuard enables real-time detection almost without additional latency, making it a practical backdoor defense for real-world LLM deployments.</li>
</ul>

<h3>Title: MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Roman Koshkin, Pengyu Dai, Nozomi Fujikawa, Masahito Togami, Marco Visentini-Scarzanella</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01370">https://arxiv.org/abs/2508.01370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01370">https://arxiv.org/pdf/2508.01370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01370]] MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis(https://arxiv.org/abs/2508.01370)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present an autonomous framework that leverages Large Language Models (LLMs) to automate end-to-end business analysis and market report generation. At its core, the system employs specialized agents - Researcher, Reviewer, Writer, and Retriever - that collaborate to analyze data and produce comprehensive reports. These agents learn from real professional consultants' presentation materials at Amazon through in-context learning to replicate professional analytical methodologies. The framework executes a multi-step process: querying databases, analyzing data, generating insights, creating visualizations, and composing market reports. We also introduce a novel LLM-based evaluation system for assessing report quality, which shows alignment with expert human evaluations. Building on these evaluations, we implement an iterative improvement mechanism that optimizes report quality through automated review cycles. Experimental results show that report quality can be improved by both automated review cycles and consultants' unstructured knowledge. In experimental validation, our framework generates detailed 6-page reports in 7 minutes at a cost of approximately \$1. Our work could be an important step to automatically create affordable market insights.</li>
</ul>

<h3>Title: Lightweight Backbone Networks Only Require Adaptive Lightweight Self-Attention Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Fengyun Li, Chao Zheng, Yangyang Fang, Jialiang Lan, Jianhua Liang, Luhao Zhang, Fa Si</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01385">https://arxiv.org/abs/2508.01385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01385">https://arxiv.org/pdf/2508.01385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01385]] Lightweight Backbone Networks Only Require Adaptive Lightweight Self-Attention Mechanisms(https://arxiv.org/abs/2508.01385)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Currently, lightweight hybrid backbone networks have partially alleviated the issue of computational saturation, but the imbalance in computational efficiencys between convolutional neural networks (CNNs) and attention mechanisms is becoming increasingly apparent. Specifically, although linear attention mechanisms and their variants have made progress in lightweight design, they still fail to meet the demands of hybrid models for long-sequence modeling. On the other hand, existing lightweight SoftMax attention computations typically reduce the feature map to a fixed size to decrease the number of sequences, thereby compressing the computational scale. However, the process of determining the feature map reduction ratio is cumbersome, and computational saturation issues still persist. To address this issue, this paper proposes a lightweight SoftMax attention mechanism with adaptive feature map sizes, named Fast Window Attention (FWA), which generates a small number of key sequences (Key and Value) through window aggregation for attention computation. Additionally, it explains the rationality of using ReLU to simulate SoftMax operations in lightweight global attention mechanisms. Finally, the paper designs a global-local feature fusion mechanism and combines it with GhostNet to propose a lightweight hybrid backbone network, LOLViT. Through visual tasks such as classification (ImageNet 1K), detection (COCO 2017), and segmentation (BDD100K), along with extensive ablation studies, it is demonstrated that LOLViT outperforms CNN models of the same level in both inference speed and model accuracy. Notably, the inference speed of LOLViT-X is 5x that of MobileViT-X.</li>
</ul>

<h3>Title: Effects of Feature Correlations on Associative Memory Capacity</h3>
<ul>
<li><strong>Authors: </strong>Stefan Bielmeier, Gerald Friedland</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01395">https://arxiv.org/abs/2508.01395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01395">https://arxiv.org/pdf/2508.01395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01395]] Effects of Feature Correlations on Associative Memory Capacity(https://arxiv.org/abs/2508.01395)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We investigate how feature correlations influence the capacity of Dense Associative Memory (DAM), a Transformer attention-like model. Practical machine learning scenarios involve feature-correlated data and learn representations in the input space, but current capacity analyses do not account for this. We develop an empirical framework to analyze the effects of data structure on capacity dynamics. Specifically, we systematically construct datasets that vary in feature correlation and pattern separation using Hamming distance from information theory, and compute the model's corresponding storage capacity using a simple binary search algorithm. Our experiments confirm that memory capacity scales exponentially with increasing separation in the input space. Feature correlations do not alter this relationship fundamentally, but reduce capacity slightly at constant separation. This effect is amplified at higher polynomial degrees in the energy function, suggesting that Associative Memory is more limited in depicting higher-order interactions between features than patterns. Our findings bridge theoretical work and practical settings for DAM, and might inspire more data-centric methods.</li>
</ul>

<h3>Title: MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Rezaie Mianroodi, Amirali Rezaie, Niko Grisel Todorov, Cyril Rakovski, Frank Rudzicz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01401">https://arxiv.org/abs/2508.01401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01401">https://arxiv.org/pdf/2508.01401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01401]] MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs(https://arxiv.org/abs/2508.01401)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Physicians spend significant time documenting clinical encounters, a burden that contributes to professional burnout. To address this, robust automation tools for medical documentation are crucial. We introduce MedSynth -- a novel dataset of synthetic medical dialogues and notes designed to advance the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks. Informed by an extensive analysis of disease distributions, this dataset includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We demonstrate that our dataset markedly enhances the performance of models in generating medical notes from dialogues, and dialogues from medical notes. The dataset provides a valuable resource in a field where open-access, privacy-compliant, and diverse training data are scarce. Code is available at this https URL and the dataset is available at this https URL.</li>
</ul>

<h3>Title: ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chuangchuang Tan, Jinglu Wang, Xiang Ming, Renshuai Tao, Yunchao Wei, Yao Zhao, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01402">https://arxiv.org/abs/2508.01402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01402">https://arxiv.org/pdf/2508.01402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01402]] ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models(https://arxiv.org/abs/2508.01402)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Advances in generative models have led to AI-generated images visually indistinguishable from authentic ones. Despite numerous studies on detecting AI-generated images with classifiers, a gap persists between such methods and human cognitive forensic analysis. We present ForenX, a novel method that not only identifies the authenticity of images but also provides explanations that resonate with human thoughts. ForenX employs the powerful multimodal large language models (MLLMs) to analyze and interpret forensic cues. Furthermore, we overcome the limitations of standard MLLMs in detecting forgeries by incorporating a specialized forensic prompt that directs the MLLMs attention to forgery-indicative attributes. This approach not only enhance the generalization of forgery detection but also empowers the MLLMs to provide explanations that are accurate, relevant, and comprehensive. Additionally, we introduce ForgReason, a dataset dedicated to descriptions of forgery evidences in AI-generated images. Curated through collaboration between an LLM-based agent and a team of human annotators, this process provides refined data that further enhances our model's performance. We demonstrate that even limited manual annotations significantly improve explanation quality. We evaluate the effectiveness of ForenX on two major benchmarks. The model's explainability is verified by comprehensive subjective evaluations.</li>
</ul>

<h3>Title: CPformer -- Concept and Physics enhanced Transformer for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Hongwei Ma, Junbin Gao, Minh-Ngoc Tran</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01407">https://arxiv.org/abs/2508.01407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01407">https://arxiv.org/pdf/2508.01407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01407]] CPformer -- Concept and Physics enhanced Transformer for Time Series Forecasting(https://arxiv.org/abs/2508.01407)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate, explainable and physically-credible forecasting remains a persistent challenge for multivariate time-series whose statistical properties vary across domains. We present CPformer, a Concept- and Physics-enhanced Transformer that channels every prediction through five self-supervised, domain-agnostic concepts while enforcing differentiable residuals drawn from first-principle constraints. Unlike prior efficiency-oriented Transformers that rely purely on sparsity or frequency priors , CPformer combines latent transparency with hard scientific guidance while retaining attention for long contexts. We tested CPformer on six publicly-available datasets: sub-hourly Electricity and Traffic, hourly ETT, high-dimensional Weather, weekly Influenza-like Illness, and minute-level Exchange Rate, and CPformer achieves the lowest error in eight of twelve MSE/MAE cells. Relative to the strongest Transformer baseline (FEDformer), CPformer reduces mean-squared-error by 23% on Electricity, 44% on Traffic and 61% on Illness, while matching performance on strictly periodic Weather and ETT series.</li>
</ul>

<h3>Title: ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations</h3>
<ul>
<li><strong>Authors: </strong>Rania Al-Sabbagh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01411">https://arxiv.org/abs/2508.01411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01411">https://arxiv.org/pdf/2508.01411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01411]] ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations(https://arxiv.org/abs/2508.01411)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics, novels, and TV show subtitles that are manually translated and aligned with their English counterparts. The dataset contains 25,557 segment pairs that can be used to benchmark new machine translation models, fine-tune large language models in few-shot settings, and adapt commercial machine translation applications such as Google Translate. Additionally, the dataset is a valuable resource for research in various disciplines, including translation studies, cross-linguistic analysis, and lexical semantics. The dataset can also serve pedagogical purposes by training translation students and aid professional translators as a translation memory. The contributions are twofold: first, the dataset features textual genres not found in existing parallel Egyptian Arabic and English datasets, and second, it is a gold-standard dataset that has been translated and aligned by human experts.</li>
</ul>

<h3>Title: Discovering Bias Associations through Open-Ended LLM Generations</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Pan, Chahat Raj, Ziwei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01412">https://arxiv.org/abs/2508.01412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01412">https://arxiv.org/pdf/2508.01412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01412]] Discovering Bias Associations through Open-Ended LLM Generations(https://arxiv.org/abs/2508.01412)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Social biases embedded in Large Language Models (LLMs) raise critical concerns, resulting in representational harms -- unfair or distorted portrayals of demographic groups -- that may be expressed in subtle ways through generated language. Existing evaluation methods often depend on predefined identity-concept associations, limiting their ability to surface new or unexpected forms of bias. In this work, we present the Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through comprehensive experiments spanning multiple models and diverse real-world contexts, BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities. Our findings advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs. Data, code, and results are available at this https URL</li>
</ul>

<h3>Title: AI-Driven Cybersecurity Threat Detection: Building Resilient Defense Systems Using Predictive Analytics</h3>
<ul>
<li><strong>Authors: </strong>Biswajit Chandra Das, M Saif Sartaz, Syed Ali Reza, Arat Hossain, Md Nasiruddin, Kanchon Kumar Bishnu, Kazi Sharmin Sultana, Sadia Sharmeen Shatyi, MD Azam Khan, Joynal Abed</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01422">https://arxiv.org/abs/2508.01422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01422">https://arxiv.org/pdf/2508.01422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01422]] AI-Driven Cybersecurity Threat Detection: Building Resilient Defense Systems Using Predictive Analytics(https://arxiv.org/abs/2508.01422)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>This study examines how Artificial Intelligence can aid in identifying and mitigating cyber threats in the U.S. across four key areas: intrusion detection, malware classification, phishing detection, and insider threat analysis. Each of these problems has its quirks, meaning there needs to be different approaches to each, so we matched the models to the shape of the problem. For intrusion detection, catching things like unauthorized access, we tested unsupervised anomaly detection methods. Isolation forests and deep autoencoders both gave us useful signals by picking up odd patterns in network traffic. When it came to malware detection, we leaned on ensemble models like Random Forest and XGBoost, trained on features pulled from files and traffic logs. Phishing was more straightforward. We fed standard classifiers (logistic regression, Random Forest, XGBoost) a mix of email and web-based features. These models handled the task surprisingly well. Phishing turned out to be the easiest problem to crack, at least with the data we had. There was a different story. We utilized an LSTM autoencoder to identify behavioral anomalies in user activity logs. It caught every suspicious behavior but flagged a lot of harmless ones too. That kind of model makes sense when the cost of missing a threat is high and you are willing to sift through some noise. What we saw across the board is that performance was not about stacking the most complex model. What mattered was how well the models structure matched the way the data behaved. When signals were strong and obvious, simple models worked fine. But for messier, more subtle threats, we needed something more adaptive, sequence models and anomaly detectors, though they brought their trade offs. The takeaway here is clear in cybersecurity, context drives the solution.</li>
</ul>

<h3>Title: From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Haonan Bian, Yutao Qi, Rui Yang, Yuanxi Che, Jiaqian Wang, Heming Xia, Ranran Zhen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01424">https://arxiv.org/abs/2508.01424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01424">https://arxiv.org/pdf/2508.01424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01424]] From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs(https://arxiv.org/abs/2508.01424)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), despite their success in question answering, exhibit limitations in complex multi-hop question answering (MQA) tasks that necessitate non-linear, structured reasoning. This limitation stems from their inability to adequately capture deep conceptual relationships between entities. To overcome this challenge, we present **ORACLE** (**O**ntology-driven **R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a training-free framework that combines LLMs' generative capabilities with the structural benefits of knowledge graphs. Our approach operates through three stages: (1) dynamic construction of question-specific knowledge ontologies using LLMs, (2) transformation of these ontologies into First-Order Logic reasoning chains, and (3) systematic decomposition of the original query into logically coherent sub-questions. Experimental results on several standard MQA benchmarks show that our framework achieves highly competitive performance, rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses further confirm the effectiveness of each component, while demonstrating that our method generates more logical and interpretable reasoning chains than existing approaches.</li>
</ul>

<h3>Title: Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification</h3>
<ul>
<li><strong>Authors: </strong>Peirong Zhang, Kai Ding, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01427">https://arxiv.org/abs/2508.01427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01427">https://arxiv.org/pdf/2508.01427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01427]] Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification(https://arxiv.org/abs/2508.01427)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric</a></li>
<li><strong>Abstract: </strong>In this paper, we propose SPECTRUM, a temporal-frequency synergistic model that unlocks the untapped potential of multi-domain representation learning for online handwriting verification (OHV). SPECTRUM comprises three core components: (1) a multi-scale interactor that finely combines temporal and frequency features through dual-modal sequence interaction and multi-scale aggregation, (2) a self-gated fusion module that dynamically integrates global temporal and frequency features via self-driven balancing. These two components work synergistically to achieve micro-to-macro spectral-temporal integration. (3) A multi-domain distance-based verifier then utilizes both temporal and frequency representations to improve discrimination between genuine and forged handwriting, surpassing conventional temporal-only approaches. Extensive experiments demonstrate SPECTRUM's superior performance over existing OHV methods, underscoring the effectiveness of temporal-frequency multi-domain learning. Furthermore, we reveal that incorporating multiple handwritten biometrics fundamentally enhances the discriminative power of handwriting representations and facilitates verification. These findings not only validate the efficacy of multi-domain learning in OHV but also pave the way for future research in multi-domain approaches across both feature and biometric domains. Code is publicly available at this https URL.</li>
</ul>

<h3>Title: Hyperspectral Image Recovery Constrained by Multi-Granularity Non-Local Self-Similarity Priors</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Peng, Yiqing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01435">https://arxiv.org/abs/2508.01435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01435">https://arxiv.org/pdf/2508.01435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01435]] Hyperspectral Image Recovery Constrained by Multi-Granularity Non-Local Self-Similarity Priors(https://arxiv.org/abs/2508.01435)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Hyperspectral image (HSI) recovery, as an upstream image processing task, holds significant importance for downstream tasks such as classification, segmentation, and detection. In recent years, HSI recovery methods based on non-local prior representations have demonstrated outstanding performance. However, these methods employ a fixed-format factor to represent the non-local self-similarity tensor groups, making them unable to adapt to diverse missing scenarios. To address this issue, we introduce the concept of granularity in tensor decomposition for the first time and propose an HSI recovery model constrained by multi-granularity non-local self-similarity priors. Specifically, the proposed model alternately performs coarse-grained decomposition and fine-grained decomposition on the non-local self-similarity tensor groups. Among them, the coarse-grained decomposition builds upon Tucker tensor decomposition, which extracts global structural information of the image by performing singular value shrinkage on the mode-unfolded matrices. The fine-grained decomposition employs the FCTN decomposition, capturing local detail information through modeling pairwise correlations among factor tensors. This architectural approach achieves a unified representation of global, local, and non-local priors for HSIs. Experimental results demonstrate that the model has strong applicability and exhibits outstanding recovery effects in various types of missing scenes such as pixels and stripes.</li>
</ul>

<h3>Title: Nakamoto Consensus from Multiple Resources</h3>
<ul>
<li><strong>Authors: </strong>Mirza Ahad Baig, Christoph U. Günther, Krzysztof Pietrzak</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01448">https://arxiv.org/abs/2508.01448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01448">https://arxiv.org/pdf/2508.01448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01448]] Nakamoto Consensus from Multiple Resources(https://arxiv.org/abs/2508.01448)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>The blocks in the Bitcoin blockchain record the amount of work W that went into creating them through proofs of work. When honest parties control a majority of the work, consensus is achieved by picking the chain with the highest recorded weight. Resources other than work have been considered to secure such longest-chain blockchains. In Chia, blocks record the amount of space S (via a proof of space) and sequential computational steps V (via a VDF). In this paper, we ask what weight functions {\Gamma}(S,V,W) (that assign a weight to a block as a function of the recorded space, speed, and work) are secure in the sense that whenever the weight of the resources controlled by honest parties is larger than the weight of adversarial parties, the blockchain is secure against private double-spending attacks. We completely classify such functions in an idealized "continuous" model: {\Gamma}(S,V,W) is secure against private double-spending attacks if and only if it is homogeneous of degree one in the timed resources V and W, i.e., {\alpha}{\Gamma}(S,V,W)={\Gamma}(S,{\alpha}V, {\alpha}W). This includes Bitcoin rule {\Gamma}(S,V,W)=W and Chia rule {\Gamma}(S,V,W) = SV. In a more realistic model where blocks are created at discrete time-points, one additionally needs some mild assumptions on the dependency on S (basically, the weight should not grow too much if S is slightly increased, say linear as in Chia). Our classification is more general and allows various instantiations of the same resource. It provides a powerful tool for designing new longest-chain blockchains. E.g., consider combining different PoWs to counter centralization, say the Bitcoin PoW W_1 and a memory-hard PoW W_2. Previous work suggested to use W_1+W_2 as weight. Our results show that using {\sqrt}(W_1){\cdot}{\sqrt}(W_2), {\min}{W_1,W_2} are also secure, and we argue that in practice these are much better choices.</li>
</ul>

<h3>Title: Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data</h3>
<ul>
<li><strong>Authors: </strong>Xinlin Zhuang, Feilong Tang, Haolin Yang, Ming Hu, Huifa Li, Haochen Xue, Yichen Li, Junjun He, Zongyuan Ge, Ying Qian, Imran Razzak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01450">https://arxiv.org/abs/2508.01450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01450">https://arxiv.org/pdf/2508.01450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01450]] Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data(https://arxiv.org/abs/2508.01450)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language Models (LLMs) to specialized domains such as medical reasoning. However, existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, defined by knowledge and reasoning complexity, they overlook each sample's optimization utility reflected in its gradient. Interestingly, we find that gradient-based influence alone favors easy-to-optimize samples that cause large parameter shifts but lack deep reasoning chains, while difficulty alone selects noisy or overly complex cases that fail to guide stable optimization. Based on this observation, we propose a data selection strategy, Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence, enabling efficient medical reasoning with minimal fine-tuning data. Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation, as DIQ emphasizes samples that foster expert-like reasoning patterns. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms the baseline, highlighting the superiority of principled data selection over brute-force scaling. The code and data are available at this https URL.</li>
</ul>

<h3>Title: Think Broad, Act Narrow: CWE Identification with Multi-Agent Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Sayagh, Mohammad Ghafari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01451">https://arxiv.org/abs/2508.01451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01451">https://arxiv.org/pdf/2508.01451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01451]] Think Broad, Act Narrow: CWE Identification with Multi-Agent Large Language Models(https://arxiv.org/abs/2508.01451)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Machine learning and Large language models (LLMs) for vulnerability detection has received significant attention in recent years. Unfortunately, state-of-the-art techniques show that LLMs are unsuccessful in even distinguishing the vulnerable function from its benign counterpart, due to three main problems: Vulnerability detection requires deep analysis, which LLMs often struggle with when making a one-shot prediction. Existing techniques typically perform function-level analysis, whereas effective vulnerability detection requires contextual information beyond the function scope. The focus on binary classification can result in identifying a vulnerability but associating it with the wrong security weaknesses (CWE), which may mislead developers. We propose a novel multi-agent LLM approach to address the challenges of identifying CWEs. This approach consists of three steps: (1) a team of LLM agents performs an exhaustive search for potential CWEs in the function under review, (2) another team of agents identifies relevant external context to support or refute each candidate CWE, and (3) a final agent makes informed acceptance or rejection decisions for each CWE based on the gathered context. A preliminary evaluation of our approach shows promising results. In the PrimeVul dataset, Step 1 correctly identifies the appropriate CWE in 40.9\% of the studied vulnerable functions. We further evaluated the full pipeline on ten synthetic programs and found that incorporating context information significantly reduced false positives from 6 to 9 CWEs to just 1 to 2, while still correctly identifying the true CWE in 9 out of 10 cases.</li>
</ul>

<h3>Title: Regression Augmentation With Data-Driven Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shayan Alahyari, Shiva Mehdipour Ghobadlou, Mike Domaratzki</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01455">https://arxiv.org/abs/2508.01455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01455">https://arxiv.org/pdf/2508.01455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01455]] Regression Augmentation With Data-Driven Segmentation(https://arxiv.org/abs/2508.01455)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Imbalanced regression arises when the target distribution is skewed, causing models to focus on dense regions and struggle with underrepresented (minority) samples. Despite its relevance across many applications, few methods have been designed specifically for this challenge. Existing approaches often rely on fixed, ad hoc thresholds to label samples as rare or common, overlooking the continuous complexity of the joint feature-target space and fail to represent the true underlying rare regions. To address these limitations, we propose a fully data-driven GAN-based augmentation framework that uses Mahalanobis-Gaussian Mixture Modeling (GMM) to automatically identify minority samples and employs deterministic nearest-neighbour matching to enrich sparse regions. Rather than preset thresholds, our method lets the data determine which observations are truly rare. Evaluation on 32 benchmark imbalanced regression datasets demonstrates that our approach consistently outperforms state-of-the-art data augmentation methods.</li>
</ul>

<h3>Title: Fast and scalable retrosynthetic planning with a transformer neural network and speculative beam search</h3>
<ul>
<li><strong>Authors: </strong>Mikhail Andronov, Natalia Andronova, Michael Wand, Jürgen Schmidhuber, Djork-Arné Clevert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01459">https://arxiv.org/abs/2508.01459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01459">https://arxiv.org/pdf/2508.01459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01459]] Fast and scalable retrosynthetic planning with a transformer neural network and speculative beam search(https://arxiv.org/abs/2508.01459)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>AI-based computer-aided synthesis planning (CASP) systems are in demand as components of AI-driven drug discovery workflows. However, the high latency of such CASP systems limits their utility for high-throughput synthesizability screening in de novo drug design. We propose a method for accelerating multi-step synthesis planning systems that rely on SMILES-to-SMILES transformers as single-step retrosynthesis models. Our approach reduces the latency of SMILES-to-SMILES transformers powering multi-step synthesis planning in AiZynthFinder through speculative beam search combined with a scalable drafting strategy called Medusa. Replacing standard beam search with our approach allows the CASP system to solve 26\% to 86\% more molecules under the same time constraints of several seconds. Our method brings AI-based CASP systems closer to meeting the strict latency requirements of high-throughput synthesizability screening and improving general user experience.</li>
</ul>

<h3>Title: Uncertainty-Aware Segmentation Quality Prediction via Deep Learning Bayesian Modeling: Comprehensive Evaluation and Interpretation on Skin Cancer and Liver Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sikha O K, Meritxell Riera-Marín, Adrian Galdran, Javier García Lopez, Julia Rodríguez-Comas, Gemma Piella, Miguel A. González Ballester</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01460">https://arxiv.org/abs/2508.01460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01460">https://arxiv.org/pdf/2508.01460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01460]] Uncertainty-Aware Segmentation Quality Prediction via Deep Learning Bayesian Modeling: Comprehensive Evaluation and Interpretation on Skin Cancer and Liver Segmentation(https://arxiv.org/abs/2508.01460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Image segmentation is a critical step in computational biomedical image analysis, typically evaluated using metrics like the Dice coefficient during training and validation. However, in clinical settings without manual annotations, assessing segmentation quality becomes challenging, and models lacking reliability indicators face adoption barriers. To address this gap, we propose a novel framework for predicting segmentation quality without requiring ground truth annotations during test time. Our approach introduces two complementary frameworks: one leveraging predicted segmentation and uncertainty maps, and another integrating the original input image, uncertainty maps, and predicted segmentation maps. We present Bayesian adaptations of two benchmark segmentation models-SwinUNet and Feature Pyramid Network with ResNet50-using Monte Carlo Dropout, Ensemble, and Test Time Augmentation to quantify uncertainty. We evaluate four uncertainty estimates: confidence map, entropy, mutual information, and expected pairwise Kullback-Leibler divergence on 2D skin lesion and 3D liver segmentation datasets, analyzing their correlation with segmentation quality metrics. Our framework achieves an R2 score of 93.25 and Pearson correlation of 96.58 on the HAM10000 dataset, outperforming previous segmentation quality assessment methods. For 3D liver segmentation, Test Time Augmentation with entropy achieves an R2 score of 85.03 and a Pearson correlation of 65.02, demonstrating cross-modality robustness. Additionally, we propose an aggregation strategy that combines multiple uncertainty estimates into a single score per image, offering a more robust and comprehensive assessment of segmentation quality. Finally, we use Grad-CAM and UMAP-based embedding analysis to interpret the model's behavior and reliability, highlighting the impact of uncertainty integration.</li>
</ul>

<h3>Title: Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians</h3>
<ul>
<li><strong>Authors: </strong>Quankai Gao, Iliyan Georgiev, Tuanfeng Y. Wang, Krishna Kumar Singh, Ulrich Neumann, Jae Shin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01464">https://arxiv.org/abs/2508.01464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01464">https://arxiv.org/pdf/2508.01464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01464]] Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians(https://arxiv.org/abs/2508.01464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate its ability to facilitate downstream generation tasks.</li>
</ul>

<h3>Title: EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned Graph-Augmented Transformer</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Ziaeetabar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01465">https://arxiv.org/abs/2508.01465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01465">https://arxiv.org/pdf/2508.01465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01465]] EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned Graph-Augmented Transformer(https://arxiv.org/abs/2508.01465)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate and efficient brain tumor segmentation remains a critical challenge in neuroimaging due to the heterogeneous nature of tumor subregions and the high computational cost of volumetric inference. In this paper, we propose EfficientGFormer, a novel architecture that integrates pretrained foundation models with graph-based reasoning and lightweight efficiency mechanisms for robust 3D brain tumor segmentation. Our framework leverages nnFormer as a modality-aware encoder, transforming multi-modal MRI volumes into patch-level embeddings. These features are structured into a dual-edge graph that captures both spatial adjacency and semantic similarity. A pruned, edge-type-aware Graph Attention Network (GAT) enables efficient relational reasoning across tumor subregions, while a distillation module transfers knowledge from a full-capacity teacher to a compact student model for real-time deployment. Experiments on the MSD Task01 and BraTS 2021 datasets demonstrate that EfficientGFormer achieves state-of-the-art accuracy with significantly reduced memory and inference time, outperforming recent transformer-based and graph-based baselines. This work offers a clinically viable solution for fast and accurate volumetric tumor delineation, combining scalability, interpretability, and generalization.</li>
</ul>

<h3>Title: VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments</h3>
<ul>
<li><strong>Authors: </strong>Imtiaz Karim, Hyunwoo Lee, Hassan Asghar, Kazi Samin Mubasshir, Seulgi Han, Mashroor Hasan Bhuiyan, Elisa Bertino</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01469">https://arxiv.org/abs/2508.01469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01469">https://arxiv.org/pdf/2508.01469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01469]] VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments(https://arxiv.org/abs/2508.01469)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction</a></li>
<li><strong>Abstract: </strong>We present VWAttacker, the first systematic testing framework for analyzing the security of Voice over WiFi (VoWiFi) User Equipment (UE) implementations. VWAttacker includes a complete VoWiFi network testbed that communicates with Commercial-Off-The-Shelf (COTS) UEs based on a simple interface to test the behavior of diverse VoWiFi UE implementations; uses property-guided adversarial testing to uncover security issues in different UEs systematically. To reduce manual effort in extracting and testing properties, we introduce an LLM-based, semi-automatic, and scalable approach for property extraction and testcase (TC) generation. These TCs are systematically mutated by two domain-specific transformations. Furthermore, we introduce two deterministic oracles to detect property violations automatically. Coupled with these techniques, VWAttacker extracts 63 properties from 11 specifications, evaluates 1,116 testcases, and detects 13 issues in 21 UEs. The issues range from enforcing a DH shared secret to 0 to supporting weak algorithms. These issues result in attacks that expose the victim UE's identity or establish weak channels, thus severely hampering the security of cellular networks. We responsibly disclose the findings to all the related vendors. At the time of writing, one of the vulnerabilities has been acknowledged by MediaTek with high severity.</li>
</ul>

<h3>Title: TreeDiff: AST-Guided Code Generation with Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zeng, Jinghan Cao, Zexin Li, Yiming Chen, Tao Ren, Dawei Xiang, Xidong Wu, Shangqian Gao, Tingting Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01473">https://arxiv.org/abs/2508.01473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01473">https://arxiv.org/pdf/2508.01473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01473]] TreeDiff: AST-Guided Code Generation with Diffusion LLMs(https://arxiv.org/abs/2508.01473)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based language models have opened new possibilities for controllable and bidirectional sequence generation. These models provide an alternative to traditional autoregressive approaches by framing text generation as an iterative denoising process. However, applying diffusion models to structured domains such as source code remains a significant challenge. Programming languages differ from natural language in that they follow strict syntactic and semantic rules, with hierarchical organization that must be preserved for correctness. Standard token-level corruption techniques used during training often ignore this structure, which may hinder the model's ability to learn meaningful representations of code. To address this limitation, we propose a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process. Instead of masking individual tokens at random, we selectively corrupt syntactically meaningful code spans derived from AST subtrees. This enables the model to reconstruct programs in a way that respects grammatical boundaries and captures long-range dependencies. Experimental results demonstrate that syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns. These findings highlight the potential of incorporating structural information into diffusion-based training and suggest that syntax-guided denoising is a promising direction for advancing diffusion-based language models in code generation tasks.</li>
</ul>

<h3>Title: HT-Transformer: Event Sequences Classification by Accumulating Prefix Information with History Tokens</h3>
<ul>
<li><strong>Authors: </strong>Ivan Karpukhin, Andrey Savchenko</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01474">https://arxiv.org/abs/2508.01474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01474">https://arxiv.org/pdf/2508.01474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01474]] HT-Transformer: Event Sequences Classification by Accumulating Prefix Information with History Tokens(https://arxiv.org/abs/2508.01474)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep learning has achieved remarkable success in modeling sequential data, including event sequences, temporal point processes, and irregular time series. Recently, transformers have largely replaced recurrent networks in these tasks. However, transformers often underperform RNNs in classification tasks where the objective is to predict future targets. The reason behind this performance gap remains largely unexplored. In this paper, we identify a key limitation of transformers: the absence of a single state vector that provides a compact and effective representation of the entire sequence. Additionally, we show that contrastive pretraining of embedding vectors fails to capture local context, which is crucial for accurate prediction. To address these challenges, we introduce history tokens, a novel concept that facilitates the accumulation of historical information during next-token prediction pretraining. Our approach significantly improves transformer-based models, achieving impressive results in finance, e-commerce, and healthcare tasks. The code is publicly available on GitHub.</li>
</ul>

<h3>Title: Hyperparameter-Free Neurochaos Learning Algorithm for Classification</h3>
<ul>
<li><strong>Authors: </strong>Akhila Henry, Nithin Nagaraj</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01478">https://arxiv.org/abs/2508.01478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01478">https://arxiv.org/pdf/2508.01478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01478]] Hyperparameter-Free Neurochaos Learning Algorithm for Classification(https://arxiv.org/abs/2508.01478)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Neurochaos Learning (NL) is a brain-inspired classification framework that employs chaotic dynamics to extract features from input data and yields state of the art performance on classification tasks. However, NL requires the tuning of multiple hyperparameters and computing of four chaotic features per input sample. In this paper, we propose AutochaosNet - a novel, hyperparameter-free variant of the NL algorithm that eliminates the need for both training and parameter optimization. AutochaosNet leverages a universal chaotic sequence derived from the Champernowne constant and uses the input stimulus to define firing time bounds for feature extraction. Two simplified variants - TM AutochaosNet and TM-FR AutochaosNet - are evaluated against the existing NL architecture - ChaosNet. Our results demonstrate that AutochaosNet achieves competitive or superior classification performance while significantly reducing training time due to reduced computational effort. In addition to eliminating training and hyperparameter tuning, AutochaosNet exhibits excellent generalisation capabilities, making it a scalable and efficient choice for real-world classification tasks. Future work will focus on identifying universal orbits under various chaotic maps and incorporating them into the NL framework to further enhance performance.</li>
</ul>

<h3>Title: Reconstructing Trust Embeddings from Siamese Trust Scores: A Direct-Sum Approach with Fixed-Point Semantics</h3>
<ul>
<li><strong>Authors: </strong>Faruk Alpay, Taylan Alpay, Bugra Kilictas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01479">https://arxiv.org/abs/2508.01479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01479">https://arxiv.org/pdf/2508.01479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01479]] Reconstructing Trust Embeddings from Siamese Trust Scores: A Direct-Sum Approach with Fixed-Point Semantics(https://arxiv.org/abs/2508.01479)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>We study the inverse problem of reconstructing high-dimensional trust embeddings from the one-dimensional Siamese trust scores that many distributed-security frameworks expose. Starting from two independent agents that publish time-stamped similarity scores for the same set of devices, we formalise the estimation task, derive an explicit direct-sum estimator that concatenates paired score series with four moment features, and prove that the resulting reconstruction map admits a unique fixed point under a contraction argument rooted in Banach theory. A suite of synthetic benchmarks (20 devices x 10 time steps) confirms that, even in the presence of Gaussian noise, the recovered embeddings preserve inter-device geometry as measured by Euclidean and cosine metrics; we complement these experiments with non-asymptotic error bounds that link reconstruction accuracy to score-sequence length. Beyond methodology, the paper demonstrates a practical privacy risk: publishing granular trust scores can leak latent behavioural information about both devices and evaluation models. We therefore discuss counter-measures -- score quantisation, calibrated noise, obfuscated embedding spaces -- and situate them within wider debates on transparency versus confidentiality in networked AI systems. All datasets, reproduction scripts and extended proofs accompany the submission so that results can be verified without proprietary code.</li>
</ul>

<h3>Title: Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach</h3>
<ul>
<li><strong>Authors: </strong>Dimitra Panou, Alexandros C. Dimopoulos, Manolis Koubarakis, Martin Reczko</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01480">https://arxiv.org/abs/2508.01480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01480">https://arxiv.org/pdf/2508.01480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01480]] Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach(https://arxiv.org/abs/2508.01480)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, large language model</a></li>
<li><strong>Abstract: </strong>Biomedical text mining and question-answering are essential yet highly demanding tasks, particularly in the face of the exponential growth of biomedical literature. In this work, we present our participation in the 13th edition of the BioASQ challenge, which involves biomedical semantic question-answering for Task 13b and biomedical question-answering for developing topics for the Synergy task. We deploy a selection of open-source large language models (LLMs) as retrieval-augmented generators to answer biomedical questions. Various models are used to process the questions. A majority voting system combines their output to determine the final answer for Yes/No questions, while for list and factoid type questions, the union of their answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring all possible model combinations to contribute to the final answer, resulting in tailored LLM pipelines for each question type. Our findings provide valuable insight into which combinations of LLMs consistently produce superior results for specific question types. In the four rounds of the 2025 BioASQ challenge, our system achieved notable results: in the Synergy task, we secured 1st place for ideal answers and 2nd place for exact answers in round 2, as well as two shared 1st places for exact answers in round 3 and 4.</li>
</ul>

<h3>Title: Training Dynamics of the Cooldown Stage in Warmup-Stable-Decay Learning Rate Scheduler</h3>
<ul>
<li><strong>Authors: </strong>Aleksandr Dremov, Alexander Hägele, Atli Kosson, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01483">https://arxiv.org/abs/2508.01483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01483">https://arxiv.org/pdf/2508.01483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01483]] Training Dynamics of the Cooldown Stage in Warmup-Stable-Decay Learning Rate Scheduler(https://arxiv.org/abs/2508.01483)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Learning rate scheduling is essential in transformer training, where the final annealing plays a crucial role in getting the best performance. However, the mechanisms behind this cooldown phase, with its characteristic drop in loss, remain poorly understood. To address this, we provide a comprehensive analysis focusing solely on the cooldown phase in the Warmup-Stable-Decay (WSD) learning rate scheduler. Our analysis reveals that different cooldown shapes reveal a fundamental bias-variance trade-off in the resulting models, with shapes that balance exploration and exploitation consistently outperforming alternatives. Similarly, we find substantial performance variations $\unicode{x2013}$ comparable to those from cooldown shape selection $\unicode{x2013}$ when tuning AdamW hyperparameters. Notably, we observe consistent improvements with higher values of $\beta_2$ during cooldown. From a loss landscape perspective, we provide visualizations of the landscape during cooldown, supporting the river valley loss perspective empirically. These findings offer practical recommendations for configuring the WSD scheduler in transformer training, emphasizing the importance of optimizing the cooldown phase alongside traditional hyperparameter tuning.</li>
</ul>

<h3>Title: TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu</h3>
<ul>
<li><strong>Authors: </strong>Vallabhaneni Raj Kumar, Ashwin S, Supriya Manna, Niladri Sett, Cheedella V S N M S Hema Harshitha, Kurakula Harshitha, Anand Kumar Sharma, Basina Deepakraj, Tanuj Sarkar, Bondada Navaneeth Krishna, Samanthapudi Shakeer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01486">https://arxiv.org/abs/2508.01486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01486">https://arxiv.org/pdf/2508.01486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01486]] TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu(https://arxiv.org/abs/2508.01486)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, explainability</a></li>
<li><strong>Abstract: </strong>In the Indian subcontinent, Telugu, one of India's six classical languages, is the most widely spoken Dravidian Language. Despite its 96 million speaker base worldwide, Telugu remains underrepresented in the global NLP and Machine Learning landscape, mainly due to lack of high-quality annotated resources. This work introduces TeSent, a comprehensive benchmark dataset for sentiment classification, a key text classification problem, in Telugu. TeSent not only provides ground truth labels for the sentences, but also supplements with provisions for evaluating explainability and fairness, two critical requirements in modern-day machine learning tasks. We scraped Telugu texts covering multiple domains from various social media platforms, news websites and web-blogs to preprocess and generate 26,150 sentences, and developed a custom-built annotation platform and a carefully crafted annotation protocol for collecting the ground truth labels along with their human-annotated rationales. We then fine-tuned several SOTA pre-trained models in two ways: with rationales, and without rationales. Further, we provide a detailed plausibility and faithfulness evaluation suite, which exploits the rationales, for six widely used post-hoc explainers applied on the trained models. Lastly, we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate fairness of Telugu sentiment and emotion related NLP tasks, and provide a fairness evaluation suite for the trained classifier models. Our experimental results suggest that training with rationales may improve model accuracy, reduce bias in models, and make the explainers' output more aligned to human reasoning.</li>
</ul>

<h3>Title: The Homogenizing Effect of Large Language Models on Human Expression and Thought</h3>
<ul>
<li><strong>Authors: </strong>Zhivar Sourati, Alireza S. Ziabari, Morteza Dehghani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01491">https://arxiv.org/abs/2508.01491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01491">https://arxiv.org/pdf/2508.01491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01491]] The Homogenizing Effect of Large Language Models on Human Expression and Thought(https://arxiv.org/abs/2508.01491)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cognitive diversity, reflected in variations of language, perspective, and reasoning, is essential to creativity and collective intelligence. This diversity is rich and grounded in culture, history, and individual experience. Yet as large language models (LLMs) become deeply embedded in people's lives, they risk standardizing language and reasoning. This Review synthesizes evidence across linguistics, cognitive, and computer science to show how LLMs reflect and reinforce dominant styles while marginalizing alternative voices and reasoning strategies. We examine how their design and widespread use contribute to this effect by mirroring patterns in their training data and amplifying convergence as all people increasingly rely on the same models across contexts. Unchecked, this homogenization risks flattening the cognitive landscapes that drive collective intelligence and adaptability.</li>
</ul>

<h3>Title: A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents</h3>
<ul>
<li><strong>Authors: </strong>Clayton Cohn, Surya Rayala, Namrata Srivastava, Joyce Horn Fonteles, Shruti Jain, Xinying Luo, Divya Mereddy, Naveeduddin Mohammed, Gautam Biswas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01503">https://arxiv.org/abs/2508.01503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01503">https://arxiv.org/pdf/2508.01503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01503]] A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents(https://arxiv.org/abs/2508.01503)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) present new opportunities for creating pedagogical agents that engage in meaningful dialogue to support student learning. However, the current use of LLM systems like ChatGPT in classrooms often lacks the solid theoretical foundation found in earlier intelligent tutoring systems. To bridge this gap, we propose a framework that combines Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding in LLM-based agents focused on STEM+C learning. We illustrate this framework with Inquizzitor, an LLM-based formative assessment agent that integrates human-AI hybrid intelligence and provides feedback grounded in cognitive science principles. Our findings show that Inquizzitor delivers high-quality assessment and interaction aligned with core learning theories, offering teachers effective guidance that students value. This research underscores the potential for theory-driven LLM integration in education, highlighting the ability of these systems to provide adaptive and principled instruction.</li>
</ul>

<h3>Title: Instruction-based Time Series Editing</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Qiu, Dongliang Guo, Brynne Sullivan, Teague R. Henry, Tom Hartvigsen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01504">https://arxiv.org/abs/2508.01504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01504">https://arxiv.org/pdf/2508.01504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01504]] Instruction-based Time Series Editing(https://arxiv.org/abs/2508.01504)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In time series editing, we aim to modify some properties of a given time series without altering others. For example, when analyzing a hospital patient's blood pressure, we may add a sudden early drop and observe how it impacts their future while preserving other conditions. Existing diffusion-based editors rely on rigid, predefined attribute vectors as conditions and produce all-or-nothing edits through sampling. This attribute- and sampling-based approach limits flexibility in condition format and lacks customizable control over editing strength. To overcome these limitations, we introduce Instruction-based Time Series Editing, where users specify intended edits using natural language. This allows users to express a wider range of edits in a more accessible format. We then introduce InstructTime, the first instruction-based time series editor. InstructTime takes in time series and instructions, embeds them into a shared multi-modal representation space, then decodes their embeddings to generate edited time series. By learning a structured multi-modal representation space, we can easily interpolate between embeddings to achieve varying degrees of edit. To handle local and global edits together, we propose multi-resolution encoders. In our experiments, we use synthetic and real datasets and find that InstructTime is a state-of-the-art time series editor: InstructTime achieves high-quality edits with controllable strength, can generalize to unseen instructions, and can be easily adapted to unseen conditions through few-shot learning.</li>
</ul>

<h3>Title: FlashSVD: Memory-Efficient Inference with Streaming for Low-Rank Models</h3>
<ul>
<li><strong>Authors: </strong>Zishan Shao, Yixiao Wang, Qinsi Wang, Ting Jiang, Zhixu Du, Hancheng Ye, Danyang Zhuo, Yiran Chen, Hai Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01506">https://arxiv.org/abs/2508.01506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01506">https://arxiv.org/pdf/2508.01506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01506]] FlashSVD: Memory-Efficient Inference with Streaming for Low-Rank Models(https://arxiv.org/abs/2508.01506)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Singular Value Decomposition (SVD) has recently seen a surge of interest as a simple yet powerful tool for large language models (LLMs) compression, with a growing number of works demonstrating 20-80% parameter reductions at minimal accuracy loss. Previous SVD-based approaches have focused primarily on reducing the memory footprint of model weights, largely overlooking the additional activation memory overhead incurred during inference when applying truncated factors via standard dense CUDA kernels. Our experiments demonstrate that this activation overhead, scaling with sequence length and hidden dimension, prevents current SVD compression techniques from achieving any reduction in peak inference memory, thereby limiting their viability for real-world, on-device deployments. We introduce FlashSVD, a novel, end-to-end rank-aware streaming inference framework specifically designed for SVD-compressed large language models. FlashSVD can be seamlessly integrated with any model that employs SVD-based methods for parameter reduction. By fusing low-rank projection kernels directly into both the self-attention and feed-forward network (FFN) pipelines, FlashSVD avoid materializing full-size activation buffers. Instead, small tiles of the truncated factors are loaded into on-chip SRAM, multiplied and reduced on the fly, and immediately evicted, preserving high GPU occupancy and adding no extra latency. On standard encoder benchmarks (e.g., BERT-Base), FlashSVD cuts peak activation memory by up to 70.2% and intermediate transient memory by 75%, all while incur no accuracy loss with upstreaming compression methods, offering a practical path toward memory-constrained deployment of low-rank LLMs.</li>
</ul>

<h3>Title: Frequency-Constrained Learning for Long-Term Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Menglin Kong, Vincent Zhihao Zheng, Lijun Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01508">https://arxiv.org/abs/2508.01508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01508">https://arxiv.org/pdf/2508.01508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01508]] Frequency-Constrained Learning for Long-Term Forecasting(https://arxiv.org/abs/2508.01508)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Many real-world time series exhibit strong periodic structures arising from physical laws, human routines, or seasonal cycles. However, modern deep forecasting models often fail to capture these recurring patterns due to spectral bias and a lack of frequency-aware inductive priors. Motivated by this gap, we propose a simple yet effective method that enhances long-term forecasting by explicitly modeling periodicity through spectral initialization and frequency-constrained optimization. Specifically, we extract dominant low-frequency components via Fast Fourier Transform (FFT)-guided coordinate descent, initialize sinusoidal embeddings with these components, and employ a two-speed learning schedule to preserve meaningful frequency structure during training. Our approach is model-agnostic and integrates seamlessly into existing Transformer-based architectures. Extensive experiments across diverse real-world benchmarks demonstrate consistent performance gains--particularly at long horizons--highlighting the benefits of injecting spectral priors into deep temporal models for robust and interpretable long-range forecasting. Moreover, on synthetic data, our method accurately recovers ground-truth frequencies, further validating its interpretability and effectiveness in capturing latent periodic patterns.</li>
</ul>

<h3>Title: A Reward-Directed Diffusion Framework for Generative Design Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hadi Keramati, Patrick Kirchen, Mohammed Hannan, Rajeev K. Jaiman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01509">https://arxiv.org/abs/2508.01509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01509">https://arxiv.org/pdf/2508.01509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01509]] A Reward-Directed Diffusion Framework for Generative Design Optimization(https://arxiv.org/abs/2508.01509)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This study presents a generative optimization framework that builds on a fine-tuned diffusion model and reward-directed sampling to generate high-performance engineering designs. The framework adopts a parametric representation of the design geometry and produces new parameter sets corresponding to designs with enhanced performance metrics. A key advantage of the reward-directed approach is its suitability for scenarios in which performance metrics rely on costly engineering simulations or surrogate models (e.g. graph-based, ensemble models, or tree-based) are non-differentiable or prohibitively expensive to differentiate. This work introduces the iterative use of a soft value function within a Markov decision process framework to achieve reward-guided decoding in the diffusion model. By incorporating soft-value guidance during both the training and inference phases, the proposed approach reduces computational and memory costs to achieve high-reward designs, even beyond the training data. Empirical results indicate that this iterative reward-directed method substantially improves the ability of the diffusion models to generate samples with reduced resistance in 3D ship hull design and enhanced hydrodynamic performance in 2D airfoil design tasks. The proposed framework generates samples that extend beyond the training data distribution, resulting in a greater 25 percent reduction in resistance for ship design and over 10 percent improvement in the lift-to-drag ratio for the 2D airfoil design. Successful integration of this model into the engineering design life cycle can enhance both designer productivity and overall design performance.</li>
</ul>

<h3>Title: Canoe Paddling Quality Assessment Using Smart Devices: Preliminary Machine Learning Study</h3>
<ul>
<li><strong>Authors: </strong>S. Parab, A. Lamelas, A. Hassan, P. Bhote</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01511">https://arxiv.org/abs/2508.01511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01511">https://arxiv.org/pdf/2508.01511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01511]] Canoe Paddling Quality Assessment Using Smart Devices: Preliminary Machine Learning Study(https://arxiv.org/abs/2508.01511)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, extraction, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Over 22 million Americans participate in paddling-related activities annually, contributing to a global paddlesports market valued at 2.4 billion US dollars in 2020. Despite its popularity, the sport has seen limited integration of machine learning (ML) and remains hindered by the cost of coaching and specialized equipment. This study presents a novel AI-based coaching system that uses ML models trained on motion data and delivers stroke feedback via a large language model (LLM). Participants were recruited through a collaboration with the NYU Concrete Canoe Team. Motion data were collected across two sessions, one with suboptimal form and one with corrected technique, using Apple Watches and smartphones secured in sport straps. The data underwent stroke segmentation and feature extraction. ML models, including Support Vector Classifier, Random Forest, Gradient Boosting, and Extremely Randomized Trees, were trained on both raw and engineered features. A web based interface was developed to visualize stroke quality and deliver LLM-based feedback. Across four participants, eight trials yielded 66 stroke samples. The Extremely Randomized Tree model achieved the highest performance with an F score of 0.9496 under five fold cross validation. The web interface successfully provided both quantitative metrics and qualitative feedback. Sensor placement near the wrists improved data quality. Preliminary results indicate that smartwatches and smartphones can enable low cost, accessible alternatives to traditional paddling instruction. While limited by sample size, the study demonstrates the feasibility of using consumer devices and ML to support stroke refinement and technique improvement.</li>
</ul>

<h3>Title: SimDeep: Federated 3D Indoor Localization via Similarity-Aware Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Jaheen, Sarah Elsamanody, Hamada Rizk, Moustafa Youssef</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01515">https://arxiv.org/abs/2508.01515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01515">https://arxiv.org/pdf/2508.01515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01515]] SimDeep: Federated 3D Indoor Localization via Similarity-Aware Aggregation(https://arxiv.org/abs/2508.01515)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, federate</a></li>
<li><strong>Abstract: </strong>Indoor localization plays a pivotal role in supporting a wide array of location-based services, including navigation, security, and context-aware computing within intricate indoor environments. Despite considerable advancements, deploying indoor localization systems in real-world scenarios remains challenging, largely because of non-independent and identically distributed (non-IID) data and device heterogeneity. In response, we propose SimDeep, a novel Federated Learning (FL) framework explicitly crafted to overcome these obstacles and effectively manage device heterogeneity. SimDeep incorporates a Similarity Aggregation Strategy, which aggregates client model updates based on data similarity, significantly alleviating the issues posed by non-IID data. Our experimental evaluations indicate that SimDeep achieves an impressive accuracy of 92.89%, surpassing traditional federated and centralized techniques, thus underscoring its viability for real-world deployment.</li>
</ul>

<h3>Title: MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Kuo Shi, Jie Lu, Shanshan Ye, Guangquan Zhang, Zhen Fang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01525">https://arxiv.org/abs/2508.01525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01525">https://arxiv.org/pdf/2508.01525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01525]] MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection(https://arxiv.org/abs/2508.01525)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have highlighted the need for robust detectors capable of distinguishing real images from AI-generated images. While existing methods perform well on known generators, their performance often declines when tested with newly emerging or unseen generative models due to overlapping feature embeddings that hinder accurate cross-generator classification. In this paper, we propose Multimodal Discriminative Representation Learning for Generalizable AI-generated Image Detection (MiraGe), a method designed to learn generator-invariant features. Motivated by theoretical insights on intra-class variation minimization and inter-class separation, MiraGe tightly aligns features within the same class while maximizing separation between classes, enhancing feature discriminability. Moreover, we apply multimodal prompt learning to further refine these principles into CLIP, leveraging text embeddings as semantic anchors for effective discriminative representation learning, thereby improving generalizability. Comprehensive experiments across multiple benchmarks show that MiraGe achieves state-of-the-art performance, maintaining robustness even against unseen generators like Sora.</li>
</ul>

<h3>Title: DALEQ -- Explainable Equivalence for Java Bytecode</h3>
<ul>
<li><strong>Authors: </strong>Jens Dietrich, Behnaz Hassanshahi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01530">https://arxiv.org/abs/2508.01530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01530">https://arxiv.org/pdf/2508.01530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01530]] DALEQ -- Explainable Equivalence for Java Bytecode(https://arxiv.org/abs/2508.01530)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The security of software builds has attracted increased attention in recent years in response to incidents like solarwinds and xz. Now, several companies including Oracle and Google rebuild open source projects in a secure environment and publish the resulting binaries through dedicated repositories. This practice enables direct comparison between these rebuilt binaries and the original ones produced by developers and published in repositories such as Maven Central. These binaries are often not bitwise identical; however, in most cases, the differences can be attributed to variations in the build environment, and the binaries can still be considered equivalent. Establishing such equivalence, however, is a labor-intensive and error-prone process. While there are some tools that can be used for this purpose, they all fall short of providing provenance, i.e. readable explanation of why two binaries are equivalent, or not. To address this issue, we present daleq, a tool that disassembles Java byte code into a relational database, and can normalise this database by applying datalog rules. Those databases can then be used to infer equivalence between two classes. Notably, equivalence statements are accompanied with datalog proofs recording the normalisation process. We demonstrate the impact of daleq in an industrial context through a large-scale evaluation involving 2,714 pairs of jars, comprising 265,690 class pairs. In this evaluation, daleq is compared to two existing bytecode transformation tools. Our findings reveal a significant reduction in the manual effort required to assess non-bitwise equivalent artifacts, which would otherwise demand intensive human inspection. Furthermore, the results show that daleq outperforms existing tools by identifying more artifacts rebuilt from the same code as equivalent, even when no behavioral differences are present.</li>
</ul>

<h3>Title: MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning</h3>
<ul>
<li><strong>Authors: </strong>Yi Liu, Xiao Xu, Zeyu Xu, Meng Zhang, Yibo Li, Haoyu Chen, Junkang Zhang, Qiang Wang, Jifa Sun, Siling Lin, Shengxun Cheng, Lingshu Zhang, Kang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01540">https://arxiv.org/abs/2508.01540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01540">https://arxiv.org/pdf/2508.01540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01540]] MagicVL-2B: Empowering Vision-Language Models on Mobile Devices with Lightweight Visual Encoders via Curriculum Learning(https://arxiv.org/abs/2508.01540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have achieved remarkable breakthroughs in recent years, enabling a diverse array of applications in everyday life. However, the substantial computational and storage demands of VLMs pose significant challenges for their efficient deployment on mobile devices, which represent the most ubiquitous and accessible computing platforms today. In this work, we introduce MagicVL-2B, a novel VLM meticulously optimized for flagship smartphones. MagicVL-2B leverages a lightweight visual encoder with fewer than 100M parameters and features a redesigned dynamic resolution scheme that adaptively generates image tokens without excessive modification of image dimensions. To further enhance the performance of this compact encoder within VLMs, we propose a multimodal curriculum learning strategy that incrementally increases task difficulty and data information density throughout training. This approach substantially improves the model's performance across a variety of sub-tasks. Extensive evaluations on standard VLM benchmarks demonstrate that MagicVL-2B matches the accuracy of current state-of-the-art models while reducing on-device power consumption by 41.1%. These results establish MagicVL-2B as a practical and robust solution for real-world mobile vision-language applications, enabling advanced multimodal intelligence to run directly on smartphones.</li>
</ul>

<h3>Title: MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Sara Câmara, Eduardo Luz, Valéria Carvalho, Ivan Meneghini, Gladston Moreira</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01541">https://arxiv.org/abs/2508.01541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01541">https://arxiv.org/pdf/2508.01541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01541]] MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization(https://arxiv.org/abs/2508.01541)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt engineering is crucial for unlocking the potential of Large Language Models (LLMs). Still, since manual prompt design is often complex, non-intuitive, and time-consuming, automatic prompt optimization has emerged as a research area. However, a significant challenge in prompt optimization is managing the inherent trade-off between task performance, such as accuracy, and context size. Most existing automated methods focus on a single objective, typically performance, thereby failing to explore the critical spectrum of efficiency and effectiveness. This paper introduces the MOPrompt, a novel Multi-objective Evolutionary Optimization (EMO) framework designed to optimize prompts for both accuracy and context size (measured in tokens) simultaneously. Our framework maps the Pareto front of prompt solutions, presenting practitioners with a set of trade-offs between context size and performance, a crucial tool for deploying Large Language Models (LLMs) in real-world applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese, using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that MOPrompt substantially outperforms the baseline framework. For the Sabiazinho model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97) as the best baseline solution, but with a 31% reduction in token length.</li>
</ul>

<h3>Title: Leveraging Machine Learning for Botnet Attack Detection in Edge-Computing Assisted IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Dulana Rupanetti, Naima Kaabouch</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01542">https://arxiv.org/abs/2508.01542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01542">https://arxiv.org/pdf/2508.01542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01542]] Leveraging Machine Learning for Botnet Attack Detection in Edge-Computing Assisted IoT Networks(https://arxiv.org/abs/2508.01542)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The increase of IoT devices, driven by advancements in hardware technologies, has led to widespread deployment in large-scale networks that process massive amounts of data daily. However, the reliance on Edge Computing to manage these devices has introduced significant security vulnerabilities, as attackers can compromise entire networks by targeting a single IoT device. In light of escalating cybersecurity threats, particularly botnet attacks, this paper investigates the application of machine learning techniques to enhance security in Edge-Computing-Assisted IoT environments. Specifically, it presents a comparative analysis of Random Forest, XGBoost, and LightGBM -- three advanced ensemble learning algorithms -- to address the dynamic and complex nature of botnet threats. Utilizing a widely recognized IoT network traffic dataset comprising benign and malicious instances, the models were trained, tested, and evaluated for their accuracy in detecting and classifying botnet activities. Furthermore, the study explores the feasibility of deploying these models in resource-constrained edge and IoT devices, demonstrating their practical applicability in real-world scenarios. The results highlight the potential of machine learning to fortify IoT networks against emerging cybersecurity challenges.</li>
</ul>

<h3>Title: Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, Mingyang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01554">https://arxiv.org/abs/2508.01554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01554">https://arxiv.org/pdf/2508.01554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01554]] Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models(https://arxiv.org/abs/2508.01554)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at this https URL.</li>
</ul>

<h3>Title: EvoVLMA: Evolutionary Vision-Language Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Kun Ding, Ying Wang, Shiming Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01558">https://arxiv.org/abs/2508.01558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01558">https://arxiv.org/pdf/2508.01558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01558]] EvoVLMA: Evolutionary Vision-Language Model Adaptation(https://arxiv.org/abs/2508.01558)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pre-trained Vision-Language Models (VLMs) have been exploited in various Computer Vision tasks (e.g., few-shot recognition) via model adaptation, such as prompt tuning and adapters. However, existing adaptation methods are designed by human experts, requiring significant time cost and experience. Inspired by recent advances in Large Language Models (LLMs) based code generation, we propose an Evolutionary Vision-Language Model Adaptation (EvoVLMA) method to automatically search training-free efficient adaptation algorithms for VLMs. We recognize feature selection and logits computation as the key functions in training-free VLM adaptation, and propose a two-stage LLM-assisted evolutionary algorithm for optimizing these parts in a sequential manner, effectively addressing the challenge posed by the expansive search space through a divide-and-conquer strategy. Besides, to enhance the stability and efficiency of searching process, we propose low-precision code conversion, web based code execution and process monitoring, leading to a highly effective automatic algorithm design system. Extensive experiments demonstrate that the algorithms found by EvoVLMA can obtain promising results compared to previous manually-designed ones. More specifically, in the 8-shot image classification setting, the classical APE algorithm can be improved by 1.91 points in recognition accuracy. This research opens new possibilities for automating the optimization of adaptation algorithms of pre-trained multimodal models. Code is available at: this https URL</li>
</ul>

<h3>Title: LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Yujia Tong, Tian Zhang, Jingling Yuan, Yuze Wang, Chuang Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01569">https://arxiv.org/abs/2508.01569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01569">https://arxiv.org/pdf/2508.01569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01569]] LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided Contrastive Learning(https://arxiv.org/abs/2508.01569)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have revolutionized computer vision tasks with their exceptional performance. However, the introduction of privacy regulations such as GDPR and CCPA has brought new challenges to them. These laws grant users the right to withdraw their data, necessitating not only the deletion of data but also the complete removal of its influence from trained models. Machine unlearning emerges as a critical solution, with exact unlearning being computationally prohibitive and approximate methods offering a more practical approach. This work addresses the particularly challenging scenario of random data forgetting in ViTs, where the model must forget specific samples while retaining others, even within the same class. We first reveal the core characteristics of ViTs through selective masking experiments: when high-attention areas are masked, the model retains its recognition capability but significantly weakens its memorization ability. Based on the above insights, we propose LetheViT, a contrastive unlearning method tailored for ViTs. LetheViT uses masked image inputs to generate positive logits and original image inputs to generate negative logits, guiding the model to forget specific details while retaining the general cl category outlines. Experimental results demonstrate that LetheViT achieves state-of-the-art performance, effectively balancing privacy compliance with model efficacy.</li>
</ul>

<h3>Title: Dynamic Clustering for Personalized Federated Learning on Heterogeneous Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Heting Liu, Junzhe Huang, Fang He, Guohong Cao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01580">https://arxiv.org/abs/2508.01580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01580">https://arxiv.org/pdf/2508.01580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01580]] Dynamic Clustering for Personalized Federated Learning on Heterogeneous Edge Devices(https://arxiv.org/abs/2508.01580)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables edge devices to collaboratively learn a global model, but it may not perform well when clients have high data heterogeneity. In this paper, we propose a dynamic clustering algorithm for personalized federated learning system (DC-PFL) to address the problem of data heterogeneity. DC-PFL starts with all clients training a global model and gradually groups the clients into smaller clusters for model personalization based on their data similarities. To address the challenge of estimating data heterogeneity without exposing raw data, we introduce a discrepancy metric called model discrepancy, which approximates data heterogeneity solely based on the model weights received by the server. We demonstrate that model discrepancy is strongly and positively correlated with data heterogeneity and can serve as a reliable indicator of data heterogeneity. To determine when and how to change grouping structures, we propose an algorithm based on the rapid decrease period of the training loss curve. Moreover, we propose a layer-wise aggregation mechanism that aggregates the low-discrepancy layers at a lower frequency to reduce the amount of transmitted data and communication costs. We conduct extensive experiments on various datasets to evaluate our proposed algorithm, and our results show that DC-PFL significantly reduces total training time and improves model accuracy compared to baselines.</li>
</ul>

<h3>Title: Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xinhui Li, Xinyu He, Qiming Hu, Xiaojie Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01582">https://arxiv.org/abs/2508.01582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01582">https://arxiv.org/pdf/2508.01582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01582]] Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models(https://arxiv.org/abs/2508.01582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce, for the first time, the concept of Set Pivot Learning, a paradigm shift that redefines domain generalization (DG) based on Vision Foundation Models (VFMs). Traditional DG assumes that the target domain is inaccessible during training, but the emergence of VFMs, trained on vast and diverse data, renders this assumption unclear and obsolete. Traditional DG assumes that the target domain is inaccessible during training, but the emergence of VFMs, which are trained on vast and diverse datasets, renders this assumption unclear and obsolete. To address this challenge, we propose Set Pivot Learning (SPL), a new definition of domain migration task based on VFMs, which is more suitable for current research and application requirements. Unlike conventional DG methods, SPL prioritizes adaptive refinement over rigid domain transfer, ensuring continuous alignment with evolving real-world conditions. Specifically, SPL features two key attributes: (i) Dynamic adaptation, transitioning from static domain alignment to flexible, task-driven feature optimization, enabling models to evolve with downstream scenarios; (ii) VFM-centric tuning, leveraging pretrained knowledge as a pivot to hone task-specific representations while preserving cross-domain robustness. Building on SPL, we propose a Dynamic Prompt Fine-Tuning method, which combines a Dynamic Class-aware Prompter with a Prompt-guided Feature Focuser, to elevate VFM performance in targeted scenarios. Extensive experiments on benchmark datasets show the effectiveness of our method, highlighting its superiority over state-of-the-art methods, particularly in generalized segmentation.</li>
</ul>

<h3>Title: A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hua Yu, Yaqing Hou, Xu Gui, Shanshan Feng, Dongsheng Zhou, Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01585">https://arxiv.org/abs/2508.01585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01585">https://arxiv.org/pdf/2508.01585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01585]] A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction(https://arxiv.org/abs/2508.01585)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Stochastic Human Motion Prediction (HMP) has received increasing attention due to its wide applications. Despite the rapid progress in generative fields, existing methods often face challenges in learning continuous temporal dynamics and predicting stochastic motion sequences. They tend to overlook the flexibility inherent in complex human motions and are prone to mode collapse. To alleviate these issues, we propose a novel method called STCN, for stochastic and continuous human motion prediction, which consists of two stages. Specifically, in the first stage, we propose a spatio-temporal continuous network to generate smoother human motion sequences. In addition, the anchor set is innovatively introduced into the stochastic HMP task to prevent mode collapse, which refers to the potential human motion patterns. In the second stage, STCN endeavors to acquire the Gaussian mixture distribution (GMM) of observed motion sequences with the aid of the anchor set. It also focuses on the probability associated with each anchor, and employs the strategy of sampling multiple sequences from each anchor to alleviate intra-class differences in human motions. Experimental results on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model obtains competitive performance on both diversity and accuracy.</li>
</ul>

<h3>Title: Diffusion Models for Future Networks and Communications: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Cong Luong, Nguyen Duc Hai, Duc Van Le, Huy T. Nguyen, Thai-Hoc Vu, Thien Huynh-The, Ruichen Zhang, Nguyen Duc Duy Anh, Dusit Niyato, Marco Di Renzo, Dong In Kim, Quoc-Viet Pham</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, cs.IT, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01586">https://arxiv.org/abs/2508.01586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01586">https://arxiv.org/pdf/2508.01586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01586]] Diffusion Models for Future Networks and Communications: A Comprehensive Survey(https://arxiv.org/abs/2508.01586)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rise of Generative AI (GenAI) in recent years has catalyzed transformative advances in wireless communications and networks. Among the members of the GenAI family, Diffusion Models (DMs) have risen to prominence as a powerful option, capable of handling complex, high-dimensional data distribution, as well as consistent, noise-robust performance. In this survey, we aim to provide a comprehensive overview of the theoretical foundations and practical applications of DMs across future communication systems. We first provide an extensive tutorial of DMs and demonstrate how they can be applied to enhance optimizers, reinforcement learning and incentive mechanisms, which are popular approaches for problems in wireless networks. Then, we review and discuss the DM-based methods proposed for emerging issues in future networks and communications, including channel modeling and estimation, signal detection and data reconstruction, integrated sensing and communication, resource management in edge computing networks, semantic communications and other notable issues. We conclude the survey with highlighting technical limitations of DMs and their applications, as well as discussing future research directions.</li>
</ul>

<h3>Title: Lifelong Person Re-identification via Privacy-Preserving Data Replay</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Wang, Haojie Liu, Zhiyong Li, Wei Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01587">https://arxiv.org/abs/2508.01587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01587">https://arxiv.org/pdf/2508.01587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01587]] Lifelong Person Re-identification via Privacy-Preserving Data Replay(https://arxiv.org/abs/2508.01587)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Lifelong person re-identification (LReID) aims to incrementally accumulate knowledge across a sequence of tasks under domain shifts. Recently, replay-based methods have demonstrated strong effectiveness in LReID by rehearsing past samples stored in an auxiliary memory. However, storing historical exemplars raises concerns over data privacy. To avoid this, exemplar-free approaches attempt to match the distribution of past data without storing raw samples. Despite being privacy-friendly, these methods often suffer from performance degradation due to the forgetting of specific past knowledge representations. To this end, we propose to condense information from sequential data into the pixel space in the replay memory, enabling Privacy-Preserving Replay (Pr^2R). More specifically, by distilling the training characteristics of multiple real images into a single image, the condensed samples undergo pixel-level changes. This not only protects the privacy of the original data but also makes the replay samples more representative for sequential tasks. During the style replay phase, we align the current domain to the previous one while simultaneously adapting the replay samples to match the style of the current domain. This dual-alignment strategy effectively mitigates both class-incremental challenges and forgetting caused by domain shifts. Extensive experiments on multiple benchmarks show that the proposed method significantly improves replay effectiveness while preserving data privacy. Specifically, Pr^2R achieves 4% and 6% higher accuracy on sequential tasks compared to the current state-of-the-art and other replay-based methods, respectively.</li>
</ul>

<h3>Title: Censored Sampling for Topology Design: Guiding Diffusion with Human Preferences</h3>
<ul>
<li><strong>Authors: </strong>Euihyun Kim, Keun Park, Yeoneung Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01589">https://arxiv.org/abs/2508.01589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01589">https://arxiv.org/pdf/2508.01589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01589]] Censored Sampling for Topology Design: Guiding Diffusion with Human Preferences(https://arxiv.org/abs/2508.01589)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in denoising diffusion models have enabled rapid generation of optimized structures for topology optimization. However, these models often rely on surrogate predictors to enforce physical constraints, which may fail to capture subtle yet critical design flaws such as floating components or boundary discontinuities that are obvious to human experts. In this work, we propose a novel human-in-the-loop diffusion framework that steers the generative process using a lightweight reward model trained on minimal human feedback. Inspired by preference alignment techniques in generative modeling, our method learns to suppress unrealistic outputs by modulating the reverse diffusion trajectory using gradients of human-aligned rewards. Specifically, we collect binary human evaluations of generated topologies and train classifiers to detect floating material and boundary violations. These reward models are then integrated into the sampling loop of a pre-trained diffusion generator, guiding it to produce designs that are not only structurally performant but also physically plausible and manufacturable. Our approach is modular and requires no retraining of the diffusion model. Preliminary results show substantial reductions in failure modes and improved design realism across diverse test conditions. This work bridges the gap between automated design generation and expert judgment, offering a scalable solution to trustworthy generative design.</li>
</ul>

<h3>Title: BeDKD: Backdoor Defense based on Dynamic Knowledge Distillation and Directional Mapping Modulator</h3>
<ul>
<li><strong>Authors: </strong>Zhengxian Wu, Juan Wen, Wanli Peng, Yinghan Zhou, Changtong dou, Yiming Xue</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01595">https://arxiv.org/abs/2508.01595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01595">https://arxiv.org/pdf/2508.01595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01595]] BeDKD: Backdoor Defense based on Dynamic Knowledge Distillation and Directional Mapping Modulator(https://arxiv.org/abs/2508.01595)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Although existing backdoor defenses have gained success in mitigating backdoor attacks, they still face substantial challenges. In particular, most of them rely on large amounts of clean data to weaken the backdoor mapping but generally struggle with residual trigger effects, resulting in persistently high attack success rates (ASR). Therefore, in this paper, we propose a novel Backdoor defense method based on Directional mapping module and adversarial Knowledge Distillation (BeDKD), which balances the trade-off between defense effectiveness and model performance using a small amount of clean and poisoned data. We first introduce a directional mapping module to identify poisoned data, which destroys clean mapping while keeping backdoor mapping on a small set of flipped clean data. Then, the adversarial knowledge distillation is designed to reinforce clean mapping and suppress backdoor mapping through a cycle iteration mechanism between trust and punish distillations using clean and identified poisoned data. We conduct experiments to mitigate mainstream attacks on three datasets, and experimental results demonstrate that BeDKD surpasses the state-of-the-art defenses and reduces the ASR by 98% without significantly reducing the CACC. Our code are available in this https URL.</li>
</ul>

<h3>Title: Why Heuristic Weighting Works: A Theoretical Analysis of Denoising Score Matching</h3>
<ul>
<li><strong>Authors: </strong>Juyan Zhang, Rhys Newbury, Xinyang Zhang, Tin Tran, Dana Kulic, Michael Burke</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01597">https://arxiv.org/abs/2508.01597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01597">https://arxiv.org/pdf/2508.01597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01597]] Why Heuristic Weighting Works: A Theoretical Analysis of Denoising Score Matching(https://arxiv.org/abs/2508.01597)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Score matching enables the estimation of the gradient of a data distribution, a key component in denoising diffusion models used to recover clean data from corrupted inputs. In prior work, a heuristic weighting function has been used for the denoising score matching loss without formal justification. In this work, we demonstrate that heteroskedasticity is an inherent property of the denoising score matching objective. This insight leads to a principled derivation of optimal weighting functions for generalized, arbitrary-order denoising score matching losses, without requiring assumptions about the noise distribution. Among these, the first-order formulation is especially relevant to diffusion models. We show that the widely used heuristical weighting function arises as a first-order Taylor approximation to the trace of the expected optimal weighting. We further provide theoretical and empirical comparisons, revealing that the heuristical weighting, despite its simplicity, can achieve lower variance than the optimal weighting with respect to parameter gradients, which can facilitate more stable and efficient training.</li>
</ul>

<h3>Title: Drift-aware Collaborative Assistance Mixture of Experts for Heterogeneous Multistream Learning</h3>
<ul>
<li><strong>Authors: </strong>En Yu, Jie Lu, Kun Wang, Xiaoyu Yang, Guangquan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01598">https://arxiv.org/abs/2508.01598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01598">https://arxiv.org/pdf/2508.01598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01598]] Drift-aware Collaborative Assistance Mixture of Experts for Heterogeneous Multistream Learning(https://arxiv.org/abs/2508.01598)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning from multiple data streams in real-world scenarios is fundamentally challenging due to intrinsic heterogeneity and unpredictable concept drifts. Existing methods typically assume homogeneous streams and employ static architectures with indiscriminate knowledge fusion, limiting generalizability in complex dynamic environments. To tackle this gap, we propose CAMEL, a dynamic \textbf{C}ollaborative \textbf{A}ssistance \textbf{M}ixture of \textbf{E}xperts \textbf{L}earning framework. It addresses heterogeneity by assigning each stream an independent system with a dedicated feature extractor and task-specific head. Meanwhile, a dynamic pool of specialized private experts captures stream-specific idiosyncratic patterns. Crucially, collaboration across these heterogeneous streams is enabled by a dedicated assistance expert. This expert employs a multi-head attention mechanism to distill and integrate relevant context autonomously from all other concurrent streams. It facilitates targeted knowledge transfer while inherently mitigating negative transfer from irrelevant sources. Furthermore, we propose an Autonomous Expert Tuner (AET) strategy, which dynamically manages expert lifecycles in response to drift. It instantiates new experts for emerging concepts (freezing prior ones to prevent catastrophic forgetting) and prunes obsolete ones. This expert-level plasticity provides a robust and efficient mechanism for online model capacity adaptation. Extensive experiments demonstrate CAMEL's superior generalizability across diverse multistreams and exceptional resilience against complex concept drifts.</li>
</ul>

<h3>Title: Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment</h3>
<ul>
<li><strong>Authors: </strong>Lubin Gan, Jing Zhang, Linhao Qu, Yijun Wang, Siying Wu, Xiaoyan Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01602">https://arxiv.org/abs/2508.01602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01602">https://arxiv.org/pdf/2508.01602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01602]] Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment(https://arxiv.org/abs/2508.01602)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The fine-grained classification of brain tumor subtypes from histopathological whole slide images is highly challenging due to subtle morphological variations and the scarcity of annotated data. Although vision-language models have enabled promising zero-shot classification, their ability to capture fine-grained pathological features remains limited, resulting in suboptimal subtype discrimination. To address these challenges, we propose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot framework tailored for digital pathology. FG-PAN consists of two key modules: (1) a local feature refinement module that enhances patch-level visual features by modeling spatial relationships among representative patches, and (2) a fine-grained text description generation module that leverages large language models to produce pathology-aware, class-specific semantic prototypes. By aligning refined visual features with LLM-generated fine-grained descriptions, FG-PAN effectively increases class separability in both visual and semantic spaces. Extensive experiments on multiple public pathology datasets, including EBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance and robust generalization in zero-shot brain tumor subtype classification.</li>
</ul>

<h3>Title: Enhancing Math Reasoning in Small-sized LLMs via Preview Difficulty-Aware Intervention</h3>
<ul>
<li><strong>Authors: </strong>Xinhan Di, JoyJiaoW</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01604">https://arxiv.org/abs/2508.01604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01604">https://arxiv.org/pdf/2508.01604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01604]] Enhancing Math Reasoning in Small-sized LLMs via Preview Difficulty-Aware Intervention(https://arxiv.org/abs/2508.01604)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning scaling enhances the reasoning capabilities of large language models, with reinforcement learning serving as the key technique to draw out complex reasoning. However, key technical details of state-of-the-art reasoning LLMs, such as those in the OpenAI O series, Claude 3 series, DeepMind's Gemini 2.5 series, and Grok 3 series, remain undisclosed, making it difficult for the research community to replicate their reinforcement learning training results. Therefore, we start our study from an Early Preview Reinforcement Learning (EPRLI) algorithm built on the open-source GRPO framework, incorporating difficulty-aware intervention for math problems. Applied to a 1.5B-parameter LLM, our method achieves 50.0% on AIME24, 89.2% on Math500, 77.1% on AMC, 35.3% on Minerva, and 51.9% on OBench, superpass O1-Preview and is comparable to O1-mini within standard school-lab settings.</li>
</ul>

<h3>Title: Practical, Generalizable and Robust Backdoor Attacks on Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haoran Dai, Jiawen Wang, Ruo Yang, Manali Sharma, Zhonghao Liao, Yuan Hong, Binghui Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01605">https://arxiv.org/abs/2508.01605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01605">https://arxiv.org/pdf/2508.01605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01605]] Practical, Generalizable and Robust Backdoor Attacks on Text-to-Image Diffusion Models(https://arxiv.org/abs/2508.01605)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models (T2I DMs) have achieved remarkable success in generating high-quality and diverse images from text prompts, yet recent studies have revealed their vulnerability to backdoor attacks. Existing attack methods suffer from critical limitations: 1) they rely on unnatural adversarial prompts that lack human readability and require massive poisoned data; 2) their effectiveness is typically restricted to specific models, lacking generalizability; and 3) they can be mitigated by recent backdoor defenses. To overcome these challenges, we propose a novel backdoor attack framework that achieves three key properties: 1) \emph{Practicality}: Our attack requires only a few stealthy backdoor samples to generate arbitrary attacker-chosen target images, as well as ensuring high-quality image generation in benign scenarios. 2) \emph{Generalizability:} The attack is applicable across multiple T2I DMs without requiring model-specific redesign. 3) \emph{Robustness:} The attack remains effective against existing backdoor defenses and adaptive defenses. Our extensive experimental results on multiple T2I DMs demonstrate that with only 10 carefully crafted backdoored samples, our attack method achieves $>$90\% attack success rate with negligible degradation in benign image generation quality. We also conduct human evaluation to validate our attack effectiveness. Furthermore, recent backdoor detection and mitigation methods, as well as adaptive defense tailored to our attack are not sufficiently effective, highlighting the pressing need for more robust defense mechanisms against the proposed attack.</li>
</ul>

<h3>Title: From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lingyao Li, Runlong Yu, Qikai Hu, Bowei Li, Min Deng, Yang Zhou, Xiaowei Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01608">https://arxiv.org/abs/2508.01608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01608">https://arxiv.org/pdf/2508.01608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01608]] From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models(https://arxiv.org/abs/2508.01608)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Image geolocalization, the task of identifying the geographic location depicted in an image, is important for applications in crisis response, digital forensics, and location-based intelligence. While recent advances in large language models (LLMs) offer new opportunities for visual reasoning, their ability to perform image geolocalization remains underexplored. In this study, we introduce a benchmark called IMAGEO-Bench that systematically evaluates accuracy, distance error, geospatial bias, and reasoning process. Our benchmark includes three diverse datasets covering global street scenes, points of interest (POIs) in the United States, and a private collection of unseen images. Through experiments on 10 state-of-the-art LLMs, including both open- and closed-source models, we reveal clear performance disparities, with closed-source models generally showing stronger reasoning. Importantly, we uncover geospatial biases as LLMs tend to perform better in high-resource regions (e.g., North America, Western Europe, and California) while exhibiting degraded performance in underrepresented areas. Regression diagnostics demonstrate that successful geolocalization is primarily dependent on recognizing urban settings, outdoor environments, street-level imagery, and identifiable landmarks. Overall, IMAGEO-Bench provides a rigorous lens into the spatial reasoning capabilities of LLMs and offers implications for building geolocation-aware AI systems.</li>
</ul>

<h3>Title: Augmented Reinforcement Learning Framework For Enhancing Decision-Making In Machine Learning Models Using External Agents</h3>
<ul>
<li><strong>Authors: </strong>Sandesh Kumar Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01612">https://arxiv.org/abs/2508.01612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01612">https://arxiv.org/pdf/2508.01612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01612]] Augmented Reinforcement Learning Framework For Enhancing Decision-Making In Machine Learning Models Using External Agents(https://arxiv.org/abs/2508.01612)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>This work proposes a novel technique Augmented Reinforcement Learning framework for the improvement of decision-making capabilities of machine learning models. The introduction of agents as external overseers checks on model decisions. The external agent can be anyone, like humans or automated scripts, that helps in decision path correction. It seeks to ascertain the priority of the "Garbage-In, Garbage-Out" problem that caused poor data inputs or incorrect actions in reinforcement learning. The ARL framework incorporates two external agents that aid in course correction and the guarantee of quality data at all points of the training cycle. The External Agent 1 is a real-time evaluator, which will provide feedback light of decisions taken by the model, identify suboptimal actions forming the Rejected Data Pipeline. The External Agent 2 helps in selective curation of the provided feedback with relevance and accuracy in business scenarios creates an approved dataset for future training cycles. The validation of the framework is also applied to a real-world scenario, which is "Document Identification and Information Extraction". This problem originates mainly from banking systems, but can be extended anywhere. The method of classification and extraction of information has to be done correctly here. Experimental results show that including human feedback significantly enhances the ability of the model in order to increase robustness and accuracy in making decisions. The augmented approach, with a combination of machine efficiency and human insight, attains a higher learning standard-mainly in complex or ambiguous environments. The findings of this study show that human-in-the-loop reinforcement learning frameworks such as ARL can provide a scalable approach to improving model performance in data-driven applications.</li>
</ul>

<h3>Title: TCDiff: Triplex Cascaded Diffusion for High-fidelity Multimodal EHRs Generation with Incomplete Clinical Data</h3>
<ul>
<li><strong>Authors: </strong>Yandong Yan, Chenxi Li, Yu Huang, Dexuan Xu, Jiaqi Zhu, Zhongyan Chai, Huamin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01615">https://arxiv.org/abs/2508.01615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01615">https://arxiv.org/pdf/2508.01615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01615]] TCDiff: Triplex Cascaded Diffusion for High-fidelity Multimodal EHRs Generation with Incomplete Clinical Data(https://arxiv.org/abs/2508.01615)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The scarcity of large-scale and high-quality electronic health records (EHRs) remains a major bottleneck in biomedical research, especially as large foundation models become increasingly data-hungry. Synthesizing substantial volumes of de-identified and high-fidelity data from existing datasets has emerged as a promising solution. However, existing methods suffer from a series of limitations: they struggle to model the intrinsic properties of heterogeneous multimodal EHR data (e.g., continuous, discrete, and textual modalities), capture the complex dependencies among them, and robustly handle pervasive data incompleteness. These challenges are particularly acute in Traditional Chinese Medicine (TCM). To this end, we propose TCDiff (Triplex Cascaded Diffusion Network), a novel EHR generation framework that cascades three diffusion networks to learn the features of real-world EHR data, formatting a multi-stage generative process: Reference Modalities Diffusion, Cross-Modal Bridging, and Target Modality Diffusion. Furthermore, to validate our proposed framework, besides two public datasets, we also construct and introduce TCM-SZ1, a novel multimodal EHR dataset for benchmarking. Experimental results show that TCDiff consistently outperforms state-of-the-art baselines by an average of 10% in data fidelity under various missing rate, while maintaining competitive privacy guarantees. This highlights the effectiveness, robustness, and generalizability of our approach in real-world healthcare scenarios.</li>
</ul>

<h3>Title: LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01617">https://arxiv.org/abs/2508.01617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01617">https://arxiv.org/pdf/2508.01617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01617]] LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding(https://arxiv.org/abs/2508.01617)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855\% over LLaVA-Med and 1.867\% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93\% on VQA-RAD, 92.31\% on SLAKE, and 95.15\% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at this https URL.</li>
</ul>

<h3>Title: IMU: Influence-guided Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Xindi Fan, Jing Wu, Mingyi Zhou, Pengwei Liang, Dinh Phung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01620">https://arxiv.org/abs/2508.01620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01620">https://arxiv.org/pdf/2508.01620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01620]] IMU: Influence-guided Machine Unlearning(https://arxiv.org/abs/2508.01620)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, data-free</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that deep learning models are vulnerable to attacks and tend to memorize training data points, raising significant concerns about privacy leakage. This motivates the development of machine unlearning (MU), i.e., a paradigm that enables models to selectively forget specific data points upon request. However, most existing MU algorithms require partial or full fine-tuning on the retain set. This necessitates continued access to the original training data, which is often impractical due to privacy concerns and storage constraints. A few retain-data-free MU methods have been proposed, but some rely on access to auxiliary data and precomputed statistics of the retain set, while others scale poorly when forgetting larger portions of data. In this paper, we propose Influence-guided Machine Unlearning (IMU), a simple yet effective method that conducts MU using only the forget set. Specifically, IMU employs gradient ascent and innovatively introduces dynamic allocation of unlearning intensities across different data points based on their influences. This adaptive strategy significantly enhances unlearning effectiveness while maintaining model utility. Results across vision and language tasks demonstrate that IMU consistently outperforms existing retain-data-free MU methods.</li>
</ul>

<h3>Title: EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuanteng Chen, Yuantian Shao, Peisong Wang, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01625">https://arxiv.org/abs/2508.01625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01625">https://arxiv.org/pdf/2508.01625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01625]] EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models(https://arxiv.org/abs/2508.01625)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) has demonstrated promising potential in scaling LLMs. However, it is hindered by two critical challenges: (1) substantial GPU memory consumption to load all experts; (2) low activated parameters cannot be equivalently translated into inference acceleration effects. In this work, we propose EAC-MoE, an Expert-Selection Aware Compressor for MoE-LLMs, which deeply aligns with the characteristics of MoE from the perspectives of quantization and pruning, and introduces two modules to address these two challenges respectively: (1) The expert selection bias caused by low-bit quantization is a major factor contributing to the performance degradation in MoE-LLMs. Based on this, we propose Quantization with Expert-Selection Calibration (QESC), which mitigates the expert selection bias by calibrating the routers within the MoE; (2) There are always certain experts that are not crucial for the corresponding tasks, yet causing inference latency. Therefore, we propose Pruning based on Expert-Selection Frequency (PESF), which significantly improves inference speed by pruning less frequently used experts for current task. Extensive experiments demonstrate that our approach significantly reduces memory usage and improves inference speed with minimal performance degradation.</li>
</ul>

<h3>Title: OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets</h3>
<ul>
<li><strong>Authors: </strong>Maziyar Panahi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01630">https://arxiv.org/abs/2508.01630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01630">https://arxiv.org/pdf/2508.01630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01630]] OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets(https://arxiv.org/abs/2508.01630)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.</li>
</ul>

<h3>Title: Privacy-Preserving Inference for Quantized BERT Models</h3>
<ul>
<li><strong>Authors: </strong>Tianpei Lu, Bingsheng Zhang, Lekun Peng, Bowen Zheng, Lichun Li, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01636">https://arxiv.org/abs/2508.01636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01636">https://arxiv.org/pdf/2508.01636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01636]] Privacy-Preserving Inference for Quantized BERT Models(https://arxiv.org/abs/2508.01636)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, generative</a></li>
<li><strong>Abstract: </strong>With the increasing deployment of generative machine learning models in privacy-sensitive domains such as healthcare and personalized services, ensuring secure inference has become a critical challenge. Secure multi-party computation (MPC) enables privacy-preserving model inference but suffers from high communication and computation overhead. The main bottleneck lies in the expensive secure evaluation of floating-point operations. Quantization offers a promising solution by converting floating-point operations into lower-precision integer computations, significantly reducing overhead. However, existing MPC-based quantized inference methods either rely on public quantization parameters-posing privacy risks-or suffer from inefficiencies, particularly in handling nonlinear functions such as activations and softmax. In this work, we propose a fine-grained, layer-wise quantization scheme and support 1-bit weight fully connected layers in a secure setting. We design a multi-input lookup table protocol to evaluate softmax efficiently and securely. Furthermore, we use dual secret sharing schemes and perform precision conversions via lookup tables, eliminating truncation overhead entirely. Experimental evaluation on BERT-base models demonstrates that our approach achieves up to $8\times$ speedup compared to Lu \emph{et al}. (NDSS 25), $9\times$ speedup compared to Gupta \emph{et al}. (PETS 24) and $22 \times$ speedup compared to Knott \emph{et al}. (NeurIPS 21).</li>
</ul>

<h3>Title: Semantic Encryption: Secure and Effective Interaction with Cloud-based Large Language Models via Semantic Transformation</h3>
<ul>
<li><strong>Authors: </strong>Dong Chen, Tong Yang, Feipeng Zhai, Pengpeng Ouyang, Qidong Liu, Yafei Li, Chong Fu, Mingliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01638">https://arxiv.org/abs/2508.01638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01638">https://arxiv.org/pdf/2508.01638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01638]] Semantic Encryption: Secure and Effective Interaction with Cloud-based Large Language Models via Semantic Transformation(https://arxiv.org/abs/2508.01638)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>The increasing adoption of Cloud-based Large Language Models (CLLMs) has raised significant concerns regarding data privacy during user interactions. While existing approaches primarily focus on encrypting sensitive information, they often overlook the logical structure of user inputs. This oversight can lead to reduced data utility and degraded performance of CLLMs. To address these limitations and enable secure yet effective interactions, we propose Semantic Encryption (SE)-a plug-and-play framework designed to preserve both privacy and utility. SE consists of two key components: Semantic Encoding and Semantic Decoding. In the encoding phase, a lightweight local model transforms the original user input into an alternative semantic context that maintains the original intent and logical structure while obfuscating sensitive information. This transformed input is then processed by the CLLM, which generates a response based on the transformed semantic context. To maintain a seamless user experience, the decoding phase will reconstruct the CLLM's response back into the original semantic context by referencing the locally stored user input. Extensive experimental evaluations demonstrate that SE effectively protects data privacy without compromising data utility or user experience, offering a practical solution for secure interaction with CLLMs. Particularly, the proposed SE demonstrates a significant improvement over the state-of-the-art InferDPT, surpassing it across various evaluated metrics and datasets.</li>
</ul>

<h3>Title: Glass Surface Segmentation with an RGB-D Camera via Weighted Feature Fusion for Service Robots</h3>
<ul>
<li><strong>Authors: </strong>Henghong Lin, Zihan Zhu, Tao Wang, Anastasia Ioannou, Yuanshui Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01639">https://arxiv.org/abs/2508.01639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01639">https://arxiv.org/pdf/2508.01639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01639]] Glass Surface Segmentation with an RGB-D Camera via Weighted Feature Fusion for Service Robots(https://arxiv.org/abs/2508.01639)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We address the problem of glass surface segmentation with an RGB-D camera, with a focus on effectively fusing RGB and depth information. To this end, we propose a Weighted Feature Fusion (WFF) module that dynamically and adaptively combines RGB and depth features to tackle issues such as transparency, reflections, and occlusions. This module can be seamlessly integrated with various deep neural network backbones as a plug-and-play solution. Additionally, we introduce the MJU-Glass dataset, a comprehensive RGB-D dataset collected by a service robot navigating real-world environments, providing a valuable benchmark for evaluating segmentation models. Experimental results show significant improvements in segmentation accuracy and robustness, with the WFF module enhancing performance in both mean Intersection over Union (mIoU) and boundary IoU (bIoU), achieving a 7.49% improvement in bIoU when integrated with PSPNet. The proposed module and dataset provide a robust framework for advancing glass surface segmentation in robotics and reducing the risk of collisions with glass objects.</li>
</ul>

<h3>Title: Minimal High-Resolution Patches Are Sufficient for Whole Slide Image Representation via Cascaded Dual-Scale Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yujian Liu, Yuechuan Lin, Dongxu Shen, Haoran Li, Yutong Wang, Xiaoli Liu, Shidang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01641">https://arxiv.org/abs/2508.01641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01641">https://arxiv.org/pdf/2508.01641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01641]] Minimal High-Resolution Patches Are Sufficient for Whole Slide Image Representation via Cascaded Dual-Scale Reconstruction(https://arxiv.org/abs/2508.01641)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Whole-slide image (WSI) analysis remains challenging due to the gigapixel scale and sparsely distributed diagnostic regions. Multiple Instance Learning (MIL) mitigates this by modeling the WSI as bags of patches for slide-level prediction. However, most MIL approaches emphasize aggregator design while overlooking the impact of the feature extractor of the feature extraction stage, which is often pretrained on natural images. This leads to domain gap and suboptimal representations. Self-supervised learning (SSL) has shown promise in bridging domain gap via pretext tasks, but it still primarily builds upon generic backbones, thus requiring WSIs to be split into small patches. This inevitably splits histological structures and generates both redundant and interdependent patches, which in turn degrades aggregator performance and drastically increases training costs. To address this challenge, we propose a Cascaded Dual-Scale Reconstruction (CDSR) framework, demonstrating that only an average of 9 high-resolution patches per WSI are sufficient for robust slide-level representation. CDSR employs a two-stage selective sampling strategy that identifies the most informative representative regions from both model-based and semantic perspectives. These patches are then fed into a Local-to-Global Network, which reconstructs spatially coherent high-resolution WSI representations by integrating fine-grained local detail with global contextual information. Unlike existing dense-sampling or SSL pipelines, CDSR is optimized for efficiency and morphological fidelity. Experiments on Camelyon16, TCGA-NSCLC, and TCGA-RCC demonstrate that CDSR achieves improvements of 6.3% in accuracy and 5.5% in area under ROC curve on downstream classification tasks with only 7,070 (4.5% of total) high-resolution patches per dataset on average, outperforming state-of-the-art methods trained on over 10,000,000 patches.</li>
</ul>

<h3>Title: DUP: Detection-guided Unlearning for Backdoor Purification in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Man Hu, Yahui Ding, Yatao Yang, Liangyu Chen, Yanhao Jia, Shuai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01647">https://arxiv.org/abs/2508.01647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01647">https://arxiv.org/pdf/2508.01647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01647]] DUP: Detection-guided Unlearning for Backdoor Purification in Language Models(https://arxiv.org/abs/2508.01647)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>As backdoor attacks become more stealthy and robust, they reveal critical weaknesses in current defense strategies: detection methods often rely on coarse-grained feature statistics, and purification methods typically require full retraining or additional clean models. To address these challenges, we propose DUP (Detection-guided Unlearning for Purification), a unified framework that integrates backdoor detection with unlearning-based purification. The detector captures feature-level anomalies by jointly leveraging class-agnostic distances and inter-layer transitions. These deviations are integrated through a weighted scheme to identify poisoned inputs, enabling more fine-grained analysis. Based on the detection results, we purify the model through a parameter-efficient unlearning mechanism that avoids full retraining and does not require any external clean model. Specifically, we innovatively repurpose knowledge distillation to guide the student model toward increasing its output divergence from the teacher on detected poisoned samples, effectively forcing it to unlearn the backdoor behavior. Extensive experiments across diverse attack methods and language model architectures demonstrate that DUP achieves superior defense performance in detection accuracy and purification efficacy. Our code is available at this https URL.</li>
</ul>

<h3>Title: StrandDesigner: Towards Practical Strand Generation with Sketch Guidance</h3>
<ul>
<li><strong>Authors: </strong>Na Zhang, Moran Li, Chengming Xu, Han Feng, Xiaobin Hu, Jiangning Zhang, Weijian Cao, Chengjie Wang, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01650">https://arxiv.org/abs/2508.01650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01650">https://arxiv.org/pdf/2508.01650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01650]] StrandDesigner: Towards Practical Strand Generation with Sketch Guidance(https://arxiv.org/abs/2508.01650)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](this https URL).</li>
</ul>

<h3>Title: DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding</h3>
<ul>
<li><strong>Authors: </strong>Hanqing Wang, Zhenhao Zhang, Kaiyang Ji, Mingyu Liu, Wenti Yin, Yuchao Chen, Zhirui Liu, Xiangyu Zeng, Tianxiang Gui, Hangxing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01651">https://arxiv.org/abs/2508.01651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01651">https://arxiv.org/pdf/2508.01651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01651]] DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding(https://arxiv.org/abs/2508.01651)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D object affordance grounding aims to predict the touchable regions on a 3d object, which is crucial for human-object interaction, human-robot interaction, embodied perception, and robot learning. Recent advances tackle this problem via learning from demonstration images. However, these methods fail to capture the general affordance knowledge within the image, leading to poor generalization. To address this issue, we propose to use text-to-image diffusion models to extract the general affordance knowledge because we find that such models can generate semantically valid HOI images, which demonstrate that their internal representation space is highly correlated with real-world affordance concepts. Specifically, we introduce the DAG, a diffusion-based 3d affordance grounding framework, which leverages the frozen internal representations of the text-to-image diffusion model and unlocks affordance knowledge within the diffusion model to perform 3D affordance grounding. We further introduce an affordance block and a multi-source affordance decoder to endow 3D dense affordance prediction. Extensive experimental evaluations show that our model excels over well-established methods and exhibits open-world generalization.</li>
</ul>

<h3>Title: Authorship Attribution in Multilingual Machine-Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>Lucio La Cava, Dominik Macko, Róbert Móro, Ivan Srba, Andrea Tagarelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01656">https://arxiv.org/abs/2508.01656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01656">https://arxiv.org/pdf/2508.01656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01656]] Authorship Attribution in Multilingual Machine-Generated Texts(https://arxiv.org/abs/2508.01656)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) have reached human-like fluency and coherence, distinguishing machine-generated text (MGT) from human-written content becomes increasingly difficult. While early efforts in MGT detection have focused on binary classification, the growing landscape and diversity of LLMs require a more fine-grained yet challenging authorship attribution (AA), i.e., being able to identify the precise generator (LLM or human) behind a text. However, AA remains nowadays confined to a monolingual setting, with English being the most investigated one, overlooking the multilingual nature and usage of modern LLMs. In this work, we introduce the problem of Multilingual Authorship Attribution, which involves attributing texts to human or multiple LLM generators across diverse languages. Focusing on 18 languages -- covering multiple families and writing scripts -- and 8 generators (7 LLMs and the human-authored class), we investigate the multilingual suitability of monolingual AA methods, their cross-lingual transferability, and the impact of generators on attribution performance. Our results reveal that while certain monolingual AA methods can be adapted to multilingual settings, significant limitations and challenges remain, particularly in transferring across diverse language families, underscoring the complexity of multilingual AA and the need for more robust approaches to better match real-world scenarios.</li>
</ul>

<h3>Title: Single Point, Full Mask: Velocity-Guided Level Set Evolution for End-to-End Amodal Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhixuan Li, Yujia Liu, Chen Hui, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01661">https://arxiv.org/abs/2508.01661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01661">https://arxiv.org/pdf/2508.01661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01661]] Single Point, Full Mask: Velocity-Guided Level Set Evolution for End-to-End Amodal Segmentation(https://arxiv.org/abs/2508.01661)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Amodal segmentation aims to recover complete object shapes, including occluded regions with no visual appearance, whereas conventional segmentation focuses solely on visible areas. Existing methods typically rely on strong prompts, such as visible masks or bounding boxes, which are costly or impractical to obtain in real-world settings. While recent approaches such as the Segment Anything Model (SAM) support point-based prompts for guidance, they often perform direct mask regression without explicitly modeling shape evolution, limiting generalization in complex occlusion scenarios. Moreover, most existing methods suffer from a black-box nature, lacking geometric interpretability and offering limited insight into how occluded shapes are inferred. To deal with these limitations, we propose VELA, an end-to-end VElocity-driven Level-set Amodal segmentation method that performs explicit contour evolution from point-based prompts. VELA first constructs an initial level set function from image features and the point input, which then progressively evolves into the final amodal mask under the guidance of a shape-specific motion field predicted by a fully differentiable network. This network learns to generate evolution dynamics at each step, enabling geometrically grounded and topologically flexible contour modeling. Extensive experiments on COCOA-cls, D2SA, and KINS benchmarks demonstrate that VELA outperforms existing strongly prompted methods while requiring only a single-point prompt, validating the effectiveness of interpretable geometric modeling under weak guidance. The code will be publicly released.</li>
</ul>

<h3>Title: Shape Distribution Matters: Shape-specific Mixture-of-Experts for Amodal Segmentation under Diverse Occlusions</h3>
<ul>
<li><strong>Authors: </strong>Zhixuan Li, Yujia Liu, Chen Hui, Jeonghaeng Lee, Sanghoon Lee, Weisi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01664">https://arxiv.org/abs/2508.01664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01664">https://arxiv.org/pdf/2508.01664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01664]] Shape Distribution Matters: Shape-specific Mixture-of-Experts for Amodal Segmentation under Diverse Occlusions(https://arxiv.org/abs/2508.01664)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Amodal segmentation targets to predict complete object masks, covering both visible and occluded regions. This task poses significant challenges due to complex occlusions and extreme shape variation, from rigid furniture to highly deformable clothing. Existing one-size-fits-all approaches rely on a single model to handle all shape types, struggling to capture and reason about diverse amodal shapes due to limited representation capacity. A natural solution is to adopt a Mixture-of-Experts (MoE) framework, assigning experts to different shape patterns. However, naively applying MoE without considering the object's underlying shape distribution can lead to mismatched expert routing and insufficient expert specialization, resulting in redundant or underutilized experts. To deal with these issues, we introduce ShapeMoE, a shape-specific sparse Mixture-of-Experts framework for amodal segmentation. The key idea is to learn a latent shape distribution space and dynamically route each object to a lightweight expert tailored to its shape characteristics. Specifically, ShapeMoE encodes each object into a compact Gaussian embedding that captures key shape characteristics. A Shape-Aware Sparse Router then maps the object to the most suitable expert, enabling precise and efficient shape-aware expert routing. Each expert is designed as lightweight and specialized in predicting occluded regions for specific shape patterns. ShapeMoE offers well interpretability via clear shape-to-expert correspondence, while maintaining high capacity and efficiency. Experiments on COCOA-cls, KINS, and D2SA show that ShapeMoE consistently outperforms state-of-the-art methods, especially in occluded region segmentation. The code will be released.</li>
</ul>

<h3>Title: Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Zhixiang Wei, Xiaoxiao Ma, Ruishen Yan, Tao Tu, Huaian Chen, Jinjin Zheng, Yi Jin, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01667">https://arxiv.org/abs/2508.01667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01667">https://arxiv.org/pdf/2508.01667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01667]] Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models(https://arxiv.org/abs/2508.01667)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Foundation Models(VFMs) have achieved remarkable success in various computer vision tasks. However, their application to semantic segmentation is hindered by two significant challenges: (1) the disparity in data scale, as segmentation datasets are typically much smaller than those used for VFM pre-training, and (2) domain distribution shifts, where real-world segmentation scenarios are diverse and often underrepresented during pre-training. To overcome these limitations, we present Rein++, an efficient VFM-based segmentation framework that demonstrates superior generalization from limited data and enables effective adaptation to diverse unlabeled scenarios. Specifically, Rein++ comprises a domain generalization solution Rein-G and a domain adaptation solution Rein-A. Rein-G introduces a set of trainable, instance-aware tokens that effectively refine the VFM's features for the segmentation task. This parameter-efficient approach fine-tunes less than 1% of the backbone's parameters, enabling robust generalization. Building on the Rein-G, Rein-A performs unsupervised domain adaptation at both the instance and logit levels to mitigate domain shifts. In addition, it incorporates a semantic transfer module that leverages the class-agnostic capabilities of the segment anything model to enhance boundary details in the target domain. The integrated Rein++ pipeline first learns a generalizable model on a source domain (e.g., daytime scenes) and subsequently adapts it to diverse target domains (e.g., nighttime scenes) without any target labels. Comprehensive experiments demonstrate that Rein++ significantly outperforms state-of-the-art methods with efficient training, underscoring its roles an efficient, generalizable, and adaptive segmentation solution for VFMs, even for large models with billions of parameters. The code is available at this https URL.</li>
</ul>

<h3>Title: Boosting Generalization Performance in Model-Heterogeneous Federated Learning Using Variational Transposed Convolution</h3>
<ul>
<li><strong>Authors: </strong>Ziru Niu, Hai Dong, A.K. Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01669">https://arxiv.org/abs/2508.01669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01669">https://arxiv.org/pdf/2508.01669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01669]] Boosting Generalization Performance in Model-Heterogeneous Federated Learning Using Variational Transposed Convolution(https://arxiv.org/abs/2508.01669)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a pioneering machine learning paradigm that enables distributed clients to process local data effectively while ensuring data privacy. However, the efficacy of FL is usually impeded by the data heterogeneity among clients, resulting in local models with low generalization performance. To address this problem, traditional model-homogeneous approaches mainly involve debiasing the local training procedures with regularization or dynamically adjusting client weights in aggregation. Nonetheless, these approaches become incompatible for scenarios where clients exhibit heterogeneous model architectures. In this paper, we propose a model-heterogeneous FL framework that can improve clients' generalization performance over unseen data without model aggregation. Instead of model parameters, clients exchange the feature distributions with the server, including the mean and the covariance. Accordingly, clients train a variational transposed convolutional (VTC) neural network with Gaussian latent variables sampled from the feature distributions, and use the VTC model to generate synthetic data. By fine-tuning local models with the synthetic data, clients significantly increase their generalization performance. Experimental results show that our approach obtains higher generalization accuracy than existing model-heterogeneous FL frameworks, as well as lower communication costs and memory consumption</li>
</ul>

<h3>Title: CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions</h3>
<ul>
<li><strong>Authors: </strong>Tae Soo Kim, Yoonjoo Lee, Yoonah Park, Jiho Kim, Young-Ho Kim, Juho Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01674">https://arxiv.org/abs/2508.01674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01674">https://arxiv.org/pdf/2508.01674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01674]] CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions(https://arxiv.org/abs/2508.01674)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request -- under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.</li>
</ul>

<h3>Title: Asynchronous Federated Learning with non-convex client objective functions and heterogeneous dataset</h3>
<ul>
<li><strong>Authors: </strong>Ali Forootani, Raffaele Iervolino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01675">https://arxiv.org/abs/2508.01675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01675">https://arxiv.org/pdf/2508.01675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01675]] Asynchronous Federated Learning with non-convex client objective functions and heterogeneous dataset(https://arxiv.org/abs/2508.01675)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, traditional FL suffers from communication overhead, system heterogeneity, and straggler effects. Asynchronous Federated Learning (AFL) addresses these by allowing clients to update independently, improving scalability and reducing synchronization delays. This paper extends AFL to handle non-convex objective functions and heterogeneous datasets, common in modern deep learning. We present a rigorous convergence analysis, deriving bounds on the expected gradient norm and studying the effects of staleness, variance, and heterogeneity. To mitigate stale updates, we introduce a staleness aware aggregation that prioritizes fresher updates and a dynamic learning rate schedule that adapts to client staleness and heterogeneity, improving stability and convergence. Our framework accommodates variations in computational power, data distribution, and communication delays, making it practical for real world applications. We also analyze the impact of client selection strategies-sampling with or without replacement-on variance and convergence. Implemented in PyTorch with Python's asyncio, our approach is validated through experiments demonstrating improved performance and scalability for asynchronous, heterogeneous, and non-convex FL scenarios.</li>
</ul>

<h3>Title: Benchmarking Adversarial Patch Selection and Location</h3>
<ul>
<li><strong>Authors: </strong>Shai Kimhi, Avi Mendlson, Moshe Kimhi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01676">https://arxiv.org/abs/2508.01676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01676">https://arxiv.org/pdf/2508.01676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01676]] Benchmarking Adversarial Patch Selection and Location(https://arxiv.org/abs/2508.01676)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, segmentation</a></li>
<li><strong>Abstract: </strong>Adversarial patch attacks threaten the reliability of modern vision models. We present PatchMap, the first spatially exhaustive benchmark of patch placement, built by evaluating over 1.5e8 forward passes on ImageNet validation images. PatchMap reveals systematic hot-spots where small patches (as little as 2% of the image) induce confident misclassifications and large drops in model confidence. To demonstrate its utility, we propose a simple segmentation guided placement heuristic that leverages off the shelf masks to identify vulnerable regions without any gradient queries. Across five architectures-including adversarially trained ResNet50, our method boosts attack success rates by 8 to 13 percentage points compared to random or fixed placements. We publicly release PatchMap and the code implementation. The full PatchMap bench (6.5B predictions, multiple backbones) will be released soon to further accelerate research on location-aware defenses and adaptive attacks.</li>
</ul>

<h3>Title: Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhaochen Wang, Yiwei Wang, Yujun Cai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01678">https://arxiv.org/abs/2508.01678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01678">https://arxiv.org/pdf/2508.01678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01678]] Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models(https://arxiv.org/abs/2508.01678)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) often suffer from hallucination, partly due to challenges in aligning multimodal information. We propose Prompt-in-Image, a simple method that embeds textual instructions directly into images. This removes the need for separate text inputs and forces the model to process all content through the visual channel. We evaluate this method on three popular open-source VLMs: Qwen2.5-VL, LLaVA-1.5, and InstructBLIP. The results reveal sharp differences. Prompt-in-Image improves Qwen2.5-VL's performance, increasing POPE accuracy by 4.1 percent (from 80.2 percent to 84.3 percent) and also reducing hallucination rates on MS-COCO. In contrast, LLaVA-1.5 and InstructBLIP experience a severe performance drop, with accuracy falling from around 84 percent to near-random levels. Through detailed analysis, we found that CLIP-based encoders in LLaVA and InstructBLIP exhibit excessive attention bias toward embedded text regions, disrupting visual understanding. In contrast, Qwen's vision encoder handles text-embedded images robustly. Crucially, Prompt-in-Image reduces Qwen's modality gap, enhancing cross-modal alignment by unifying information processing through a single modality.</li>
</ul>

<h3>Title: The Bidirectional Process Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Lingyin Zhang, Jun Gao, Xiaoxue Ren, Ziqiang Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01682">https://arxiv.org/abs/2508.01682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01682">https://arxiv.org/pdf/2508.01682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01682]] The Bidirectional Process Reward Model(https://arxiv.org/abs/2508.01682)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning quality of Large Language Models (LLMs) by assigning fine-grained scores to intermediate reasoning steps within a solution trajectory. However, existing PRMs predominantly adopt a unidirectional left-to-right (L2R) evaluation paradigm, which limits their ability to leverage global context, making it challenging to verify the consistency of earlier steps based on later ones. In light of these challenges, we propose a novel bidirectional evaluation paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly incorporates a parallel right-to-left (R2L) evaluation stream alongside the conventional L2R flow, enabling later reasoning steps to help assess earlier ones in real time. Notably, the built-in R2L evaluation is implemented solely through prompt modifications that reverse the original reasoning trajectory, without any additional parameters or inference latency introduced. This ensures BiPRM remains both efficient and broadly compatible with existing PRM studies. We conduct extensive experiments on two mathematical reasoning benchmarks using samples generated by three different policy models. Our method, BiPRM, is evaluated across three backbones and three distinct PRM objectives. Across all settings, BiPRM consistently outperforms unidirectional baselines, achieving up to a 31.9% improvement in stepwise reward evaluation. Generally, our results highlight BiPRM's effectiveness, robustness, and general applicability, offering a promising new direction for process-based reward modeling.</li>
</ul>

<h3>Title: DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Chi, Huimin Ma, Kafeng Wang, Jianmin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01684">https://arxiv.org/abs/2508.01684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01684">https://arxiv.org/pdf/2508.01684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01684]] DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing(https://arxiv.org/abs/2508.01684)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality.</li>
</ul>

<h3>Title: Innovative tokenisation of structured data for LLM training</h3>
<ul>
<li><strong>Authors: </strong>Kayvan Karim, Hani Ragab Hassen. Hadj Batatia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01685">https://arxiv.org/abs/2508.01685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01685">https://arxiv.org/pdf/2508.01685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01685]] Innovative tokenisation of structured data for LLM training(https://arxiv.org/abs/2508.01685)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Data representation remains a fundamental challenge in machine learning, particularly when adapting sequence-based architectures like Transformers and Large Language Models (LLMs) for structured tabular data. Existing methods often fail to cohesively encode the mix of numerical and categorical features or preserve the inherent structure of tables. This paper introduces a novel, hybrid tokenisation methodology designed to convert tabular data into a unified, sequential format suitable for LLM training. Our approach combines predefined fixed tokens to represent structural elements and low-cardinality categorical features, with a learned subword vocabulary using Byte-Pair Encoding (BPE) for high-cardinality and continuous values. We demonstrate the efficacy of this technique by applying it to a large-scale NetFlow dataset (CIDDS-001), preparing a corpus for a Network Intrusion Detection System (NIDS) foundation model. The evaluation shows that our method is highly efficient, processing over 31 million network flows in under five hours and achieving a significant data compression ratio of 6.18:1. This process resulted in a computationally manageable corpus of over one billion tokens, establishing a viable and generalisable pathway for training foundation models on structured data.</li>
</ul>

<h3>Title: From SHAP to Rules: Distilling Expert Knowledge from Post-hoc Model Explanations in Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Maciej Mozolewski, Szymon Bobek, Grzegorz J. Nalepa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01687">https://arxiv.org/abs/2508.01687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01687">https://arxiv.org/pdf/2508.01687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01687]] From SHAP to Rules: Distilling Expert Knowledge from Post-hoc Model Explanations in Time Series Classification(https://arxiv.org/abs/2508.01687)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Explaining machine learning (ML) models for time series (TS) classification is challenging due to inherent difficulty in raw time series interpretation and doubled down by the high dimensionality. We propose a framework that converts numeric feature attributions from post-hoc, instance-wise explainers (e.g., LIME, SHAP) into structured, human-readable rules. These rules define intervals indicating when and where they apply, improving transparency. Our approach performs comparably to native rule-based methods like Anchor while scaling better to long TS and covering more instances. Rule fusion integrates rule sets through methods such as weighted selection and lasso-based refinement to balance coverage, confidence, and simplicity, ensuring all instances receive an unambiguous, metric-optimized rule. It enhances explanations even for a single explainer. We introduce visualization techniques to manage specificity-generalization trade-offs. By aligning with expert-system principles, our framework consolidates conflicting or overlapping explanations - often resulting from the Rashomon effect - into coherent and domain-adaptable insights. Experiments on UCI datasets confirm that the resulting rule-based representations improve interpretability, decision transparency, and practical applicability for TS classification.</li>
</ul>

<h3>Title: Performance and Storage Analysis of CRYSTALS Kyber as a Post Quantum Replacement for RSA and ECC</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Rodriguez Alvarez, Fernando Rodriguez Merino</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01694">https://arxiv.org/abs/2508.01694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01694">https://arxiv.org/pdf/2508.01694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01694]] Performance and Storage Analysis of CRYSTALS Kyber as a Post Quantum Replacement for RSA and ECC(https://arxiv.org/abs/2508.01694)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>The steady advancement in quantum computer error correction technology has pushed the current record to 48 stable logical qubits, bringing us closer to machines capable of running Shor's algorithm at scales that threaten RSA and ECC cryptography. While the timeline for developing such quantum computers remains uncertain, the cryptographic community must prepare for the transition to quantum-resistant algorithms. CRYSTALS-Kyber, standardized by NIST in 2022, represents a leading post-quantum cryptographic solution, but widespread adoption faces significant challenges. If this migration follows patterns similar to the SHA-1 to SHA-2 transition, organizations may experience prolonged periods of vulnerability, with substantial security and economic consequences. This study evaluates Kyber's practical viability through performance testing across various implementation schemes, utilizing only standard built-in processor acceleration features, some of which include AES-NI and ASIMD, without any specialized hardware additions. Our findings demonstrate that Kyber provides robust security guarantees against quantum attacks while maintaining acceptable performance profiles for most contemporary applications, utilizing only commodity hardware with manufacturer-provided acceleration capabilities.</li>
</ul>

<h3>Title: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</h3>
<ul>
<li><strong>Authors: </strong>Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bin Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01696">https://arxiv.org/abs/2508.01696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01696">https://arxiv.org/pdf/2508.01696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01696]] Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy(https://arxiv.org/abs/2508.01696)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has emerged as a promising framework for enhancing the capabilities of Large Language Models (LLMs), especially in knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to *fully exploit knowledge during generation*. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.</li>
</ul>

<h3>Title: Register Anything: Estimating "Corresponding Prompts" for Segment Anything Model</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Huang, Tingfa Xu, Wen Yan, Dean Barratt, Yipeng Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01697">https://arxiv.org/abs/2508.01697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01697">https://arxiv.org/pdf/2508.01697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01697]] Register Anything: Estimating "Corresponding Prompts" for Segment Anything Model(https://arxiv.org/abs/2508.01697)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Establishing pixel/voxel-level or region-level correspondences is the core challenge in image registration. The latter, also known as region-based correspondence representation, leverages paired regions of interest (ROIs) to enable regional matching while preserving fine-grained capability at pixel/voxel level. Traditionally, this representation is implemented via two steps: segmenting ROIs in each image then matching them between the two images. In this paper, we simplify this into one step by directly "searching for corresponding prompts", using extensively pre-trained segmentation models (e.g., SAM) for a training-free registration approach, PromptReg. Firstly, we introduce the "corresponding prompt problem", which aims to identify a corresponding Prompt Y in Image Y for any given visual Prompt X in Image X, such that the two respectively prompt-conditioned segmentations are a pair of corresponding ROIs from the two images. Secondly, we present an "inverse prompt" solution that generates primary and optionally auxiliary prompts, inverting Prompt X into the prompt space of Image Y. Thirdly, we propose a novel registration algorithm that identifies multiple paired corresponding ROIs by marginalizing the inverted Prompt X across both prompt and spatial dimensions. Comprehensive experiments are conducted on five applications of registering 3D prostate MR, 3D abdomen MR, 3D lung CT, 2D histopathology and, as a non-medical example, 2D aerial images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and learning-based DDF-predicting networks, even yielding competitive performance with weakly-supervised approaches that require fully-segmented training data.</li>
</ul>

<h3>Title: Versatile Transition Generation with Image-to-Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zuhao Yang, Jiahui Zhang, Yingchen Yu, Shijian Lu, Song Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01698">https://arxiv.org/abs/2508.01698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01698">https://arxiv.org/pdf/2508.01698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01698]] Versatile Transition Generation with Image-to-Video Diffusion(https://arxiv.org/abs/2508.01698)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Leveraging text, images, structure maps, or motion trajectories as conditional guidance, diffusion models have achieved great success in automated and high-quality video generation. However, generating smooth and rational transition videos given the first and last video frames as well as descriptive text prompts is far underexplored. We present VTG, a Versatile Transition video Generation framework that can generate smooth, high-fidelity, and semantically coherent video transitions. VTG introduces interpolation-based initialization that helps preserve object identity and handle abrupt content changes effectively. In addition, it incorporates dual-directional motion fine-tuning and representation alignment regularization to mitigate the limitations of pre-trained image-to-video diffusion models in motion smoothness and generation fidelity, respectively. To evaluate VTG and facilitate future studies on unified transition generation, we collected TransitBench, a comprehensive benchmark for transition generation covering two representative transition tasks: concept blending and scene transition. Extensive experiments show that VTG achieves superior transition performance consistently across all four tasks.</li>
</ul>

<h3>Title: TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding</h3>
<ul>
<li><strong>Authors: </strong>Zuhao Yang, Yingchen Yu, Yunqing Zhao, Shijian Lu, Song Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01699">https://arxiv.org/abs/2508.01699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01699">https://arxiv.org/pdf/2508.01699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01699]] TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding(https://arxiv.org/abs/2508.01699)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video Temporal Grounding (VTG) aims to precisely identify video event segments in response to textual queries. The outputs of VTG tasks manifest as sequences of events, each defined by precise timestamps, saliency scores, and textual descriptions. Despite recent advances, a fundamental limitation persists in existing Video Large Language Models (Video-LLMs): they process all task tokens through identical and static pathways, failing to recognize that temporal localization, saliency assessment, and textual generation represent fundamentally distinct tasks requiring specialized processing. To address this, we introduce TimeExpert, a Mixture-of-Experts (MoE)-based Video-LLM that effectively decomposes VTG tasks by dynamically routing task-specific tokens (e.g., timestamps, saliency scores) to specialized experts, with increased computational efficiency. Our design choices enable precise handling of each subtask, leading to improved event modeling across diverse VTG applications. Extensive experiments demonstrate that TimeExpert consistently achieves state-of-the-art performance on various VTG tasks such as Dense Video Captioning, Moment Retrieval, and Video Highlight Detection.</li>
</ul>

<h3>Title: MHARFedLLM: Multimodal Human Activity Recognition Using Federated Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Asmit Bandyopadhyay, Rohit Basu, Tanmay Sen, Swagatam Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01701">https://arxiv.org/abs/2508.01701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01701">https://arxiv.org/pdf/2508.01701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01701]] MHARFedLLM: Multimodal Human Activity Recognition Using Federated Large Language Model(https://arxiv.org/abs/2508.01701)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) plays a vital role in applications such as fitness tracking, smart homes, and healthcare monitoring. Traditional HAR systems often rely on single modalities, such as motion sensors or cameras, limiting robustness and accuracy in real-world environments. This work presents FedTime-MAGNET, a novel multimodal federated learning framework that advances HAR by combining heterogeneous data sources: depth cameras, pressure mats, and accelerometers. At its core is the Multimodal Adaptive Graph Neural Expert Transformer (MAGNET), a fusion architecture that uses graph attention and a Mixture of Experts to generate unified, discriminative embeddings across modalities. To capture complex temporal dependencies, a lightweight T5 encoder only architecture is customized and adapted within this framework. Extensive experiments show that FedTime-MAGNET significantly improves HAR performance, achieving a centralized F1 Score of 0.934 and a strong federated F1 Score of 0.881. These results demonstrate the effectiveness of combining multimodal fusion, time series LLMs, and federated learning for building accurate and robust HAR systems.</li>
</ul>

<h3>Title: Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption</h3>
<ul>
<li><strong>Authors: </strong>Berkay Köprü, Mehrzad Mashal, Yigit Gurses, Akos Kadar, Maximilian Schmitt, Ditty Mathew, Felix Burkhardt, Florian Eyben, Björn W. Schuller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01708">https://arxiv.org/abs/2508.01708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01708">https://arxiv.org/pdf/2508.01708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01708]] Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption(https://arxiv.org/abs/2508.01708)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have advanced natural language processing (NLP) skills such as through next-token prediction and self-attention, but their ability to integrate broad context also makes them prone to incorporating irrelevant information. Prior work has focused on semantic leakage, bias introduced by semantically irrelevant context. In this paper, we introduce expression leakage, a novel phenomenon where LLMs systematically generate sentimentally charged expressions that are semantically unrelated to the input context. To analyse the expression leakage, we collect a benchmark dataset along with a scheme to automatically generate a dataset from free-form text from common-crawl. In addition, we propose an automatic evaluation pipeline that correlates well with human judgment, which accelerates the benchmarking by decoupling from the need of annotation for each analysed model. Our experiments show that, as the model scales in the parameter space, the expression leakage reduces within the same LLM family. On the other hand, we demonstrate that expression leakage mitigation requires specific care during the model building process, and cannot be mitigated by prompting. In addition, our experiments indicate that, when negative sentiment is injected in the prompt, it disrupts the generation process more than the positive sentiment, causing a higher expression leakage rate.</li>
</ul>

<h3>Title: CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</h3>
<ul>
<li><strong>Authors: </strong>Raviraj Joshi, Rakesh Paul, Kanishk Singla, Anusha Kamath, Michael Evans, Katherine Luna, Shaona Ghosh, Utkarsh Vaidya, Eileen Long, Sanjay Singh Chauhan, Niranjan Wartikar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01710">https://arxiv.org/abs/2508.01710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01710">https://arxiv.org/pdf/2508.01710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01710]] CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications(https://arxiv.org/abs/2508.01710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work represents a significant step toward closing the safety gap in multilingual LLMs by enabling the development of culturally aware safety guard models.</li>
</ul>

<h3>Title: GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Bowen Yang, Yun Cao, Chen He, Xiaosu Su</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01711">https://arxiv.org/abs/2508.01711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01711">https://arxiv.org/pdf/2508.01711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01711]] GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval(https://arxiv.org/abs/2508.01711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Text-to-video retrieval requires precise alignment between language and temporally rich video signals. Existing methods predominantly exploit visual cues and often overlook complementary audio semantics or adopt coarse fusion strategies, leading to suboptimal multimodal representations. We present GAID, a framework that jointly address this gap via two key components: (i) a Frame-level Gated Fusion (FGF) that adaptively integrates audio and visual features under textual guidance, enabling fine-grained temporal alignment; and (ii) a Directional Adaptive Semantic Perturbation (DASP) that injects structure-aware perturbations into text embeddings, enhancing robustness and discrimination without incurring multi-pass inference. These modules complement each other -- fusion reduces modality gaps while perturbation regularizes cross-modal matching -- yielding more stable and expressive representations. Extensive experiments on MSR-VTT, DiDeMo, LSMDC, and VATEX show consistent state-of-the-art results across all retrieval metrics with notable efficiency gains. Our code is available at this https URL.</li>
</ul>

<h3>Title: Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Julia Hindel, Ema Mekic, Enamundram Naga Karthik, Rohit Mohan, Daniele Cattaneo, Maria Kalweit, Abhinav Valada</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01713">https://arxiv.org/abs/2508.01713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01713">https://arxiv.org/pdf/2508.01713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01713]] Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation(https://arxiv.org/abs/2508.01713)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Robot-assisted surgeries rely on accurate and real-time scene understanding to safely guide surgical instruments. However, segmentation models trained on static datasets face key limitations when deployed in these dynamic and evolving surgical environments. Class-incremental semantic segmentation (CISS) allows models to continually adapt to new classes while avoiding catastrophic forgetting of prior knowledge, without training on previous data. In this work, we build upon the recently introduced Taxonomy-Oriented Poincaré-regularized Incremental Class Segmentation (TOPICS) approach and propose an enhanced variant, termed TOPICS+, specifically tailored for robust segmentation of surgical scenes. Concretely, we incorporate the Dice loss into the hierarchical loss formulation to handle strong class imbalances, introduce hierarchical pseudo-labeling, and design tailored label taxonomies for robotic surgery environments. We also propose six novel CISS benchmarks designed for robotic surgery environments including multiple incremental steps and several semantic categories to emulate realistic class-incremental settings in surgical environments. In addition, we introduce a refined set of labels with more than 144 classes on the Syn-Mediverse synthetic dataset, hosted online as an evaluation benchmark. We make the code and trained models publicly available at this http URL.</li>
</ul>

<h3>Title: A Provably Secure Network Protocol for Private Communication with Analysis and Tracing Resistance</h3>
<ul>
<li><strong>Authors: </strong>Chao Ge, Wei Yuan, Ge Chen, Yanbin Pan, Yuan Shen</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01714">https://arxiv.org/abs/2508.01714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01714">https://arxiv.org/pdf/2508.01714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01714]] A Provably Secure Network Protocol for Private Communication with Analysis and Tracing Resistance(https://arxiv.org/abs/2508.01714)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Anonymous communication networks have emerged as crucial tools for obfuscating communication pathways and concealing user identities. However, their practical deployments face significant challenges, including susceptibility to artificial intelligence (AI)-powered metadata analysis, difficulties in decentralized architectures, and the absence of provable security guarantees. To address these issues, this paper proposes a novel decentralized anonymous routing protocol with resistance to tracing and traffic analysis. The protocol eliminates dependencies on the threshold model and trusted third-party setups, ensuring indistinguishable identity privacy even in highly adversarial environments. Different from traditional empirical security analysis of anonymous networks, this paper rigorously proves indistinguishable identity privacy for users even in extremely adversarial environments. Furthermore, simulations confirm its practical feasibility, demonstrating both security and efficiency. By achieving information sharing with privacy preservation, the proposed protocol offers a provably secure solution for privacy-preserving communication in digital environments.</li>
</ul>

<h3>Title: Neural Policy Iteration for Stochastic Optimal Control: A Physics-Informed Approach</h3>
<ul>
<li><strong>Authors: </strong>Yeongjong Kim, Yeoneung Kim, Minseok Kim, Namkyeong Cho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01718">https://arxiv.org/abs/2508.01718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01718">https://arxiv.org/pdf/2508.01718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01718]] Neural Policy Iteration for Stochastic Optimal Control: A Physics-Informed Approach(https://arxiv.org/abs/2508.01718)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We propose a physics-informed neural network policy iteration (PINN-PI) framework for solving stochastic optimal control problems governed by second-order Hamilton--Jacobi--Bellman (HJB) equations. At each iteration, a neural network is trained to approximate the value function by minimizing the residual of a linear PDE induced by a fixed policy. This linear structure enables systematic $L^2$ error control at each policy evaluation step, and allows us to derive explicit Lipschitz-type bounds that quantify how value gradient errors propagate to the policy updates. This interpretability provides a theoretical basis for evaluating policy quality during training. Our method extends recent deterministic PINN-based approaches to stochastic settings, inheriting the global exponential convergence guarantees of classical policy iteration under mild conditions. We demonstrate the effectiveness of our method on several benchmark problems, including stochastic cartpole, pendulum problems and high-dimensional linear quadratic regulation (LQR) problems in up to 10D.</li>
</ul>

<h3>Title: Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via Adaptive Vicinity and Auxiliary Regularization</h3>
<ul>
<li><strong>Authors: </strong>Xin Ding, Yun Chen, Yongwei Wang, Kao Zhang, Sen Zhang, Peibei Cao, Xiangxue Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01725">https://arxiv.org/abs/2508.01725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01725">https://arxiv.org/pdf/2508.01725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01725]] Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via Adaptive Vicinity and Auxiliary Regularization(https://arxiv.org/abs/2508.01725)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in conditional generative modeling have introduced Continuous conditional Generative Adversarial Network (CcGAN) and Continuous Conditional Diffusion Model (CCDM) for estimating high-dimensional data distributions conditioned on scalar, continuous regression labels (e.g., angles, ages, or temperatures). However, these approaches face fundamental limitations: CcGAN suffers from data imbalance due to fixed-size vicinity constraints, while CCDM requires computationally expensive iterative sampling. We present CcGAN-AVAR, an enhanced CcGAN framework that addresses both challenges: (1) leveraging the GAN framework's native one-step generation to overcome CCDMs' sampling bottleneck (achieving 300x-2000x faster inference), while (2) two novel components specifically target data imbalance - an adaptive vicinity mechanism that dynamically adjusts vicinity's size, and a multi-task discriminator that constructs two regularization terms (through auxiliary regression and density ratio estimation) to significantly improve generator training. Extensive experiments on four benchmark datasets (64x64 to 192x192 resolution) across eight challenging imbalanced settings demonstrate that CcGAN-AVAR achieves state-of-the-art generation quality while maintaining sampling efficiency.</li>
</ul>

<h3>Title: Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos</h3>
<ul>
<li><strong>Authors: </strong>Jianbo Ma, Hui Luo, Qi Chen, Yuankai Qi, Yumei Sun, Amin Beheshti, Jianlin Zhang, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01730">https://arxiv.org/abs/2508.01730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01730">https://arxiv.org/pdf/2508.01730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01730]] Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos(https://arxiv.org/abs/2508.01730)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.</li>
</ul>

<h3>Title: SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Zhang, Wei Li, Mengmeng Zhang, Jiawei Han, Ran Tao, Shunlin Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01731">https://arxiv.org/abs/2508.01731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01731">https://arxiv.org/pdf/2508.01731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01731]] SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models(https://arxiv.org/abs/2508.01731)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in Remote Sensing Foundation Models (RSFMs) have led to significant breakthroughs in the field. While many RSFMs have been pretrained with massive optical imagery, more multispectral/hyperspectral data remain lack of the corresponding foundation models. To leverage the advantages of spectral imagery in earth observation, we explore whether existing RSFMs can be effectively adapted to process diverse spectral modalities without requiring extensive spectral pretraining. In response to this challenge, we proposed SpectralX, an innovative parameter-efficient fine-tuning framework that adapt existing RSFMs as backbone while introducing a two-stage training approach to handle various spectral inputs, thereby significantly improving domain generalization performance. In the first stage, we employ a masked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) to extract attribute tokens from both spatial and spectral dimensions. Simultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA) that dynamically aggregates multi-attribute expert knowledge while performing layer-wise fine-tuning. With semantic segmentation as downstream task in the second stage, we insert an Attribute-refined Adapter (Are-adapter) into the first stage framework. By iteratively querying low-level semantic features with high-level representations, the model learns to focus on task-beneficial attributes, enabling customized adjustment of RSFMs. Following this two-phase adaptation process, SpectralX is capable of interpreting spectral imagery from new regions or seasons. The codes will be available from the website: this https URL.</li>
</ul>

<h3>Title: Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction</h3>
<ul>
<li><strong>Authors: </strong>Cheng Wang, ziru Liu, Pengcheng Tang, Mingyu Zhang, Quanyu Dai, Yue Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01739">https://arxiv.org/abs/2508.01739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01739">https://arxiv.org/pdf/2508.01739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01739]] Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction(https://arxiv.org/abs/2508.01739)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Identifying user preferences in dialogue systems is a pivotal aspect of providing satisfying services. Current research shows that using large language models (LLMs) to fine-tune a task-specific preference extractor yields excellent results in terms of accuracy and generalization. However, the primary challenge stems from the inherent difficulty in obtaining high-quality labeled multi-turn dialogue data. Accurately tracking user preference transitions across turns not only demands intensive domain expertise and contextual consistency maintenance for annotators (termed \textbf{``Annotating Disaster''}) but also complicates model training due to error propagation in sequential dependency learning. Inspired by the observation that multi-turn preference extraction can be decomposed into iterative executions of one-turn extraction processes. We propose a novel dialogue data generation framework named \textbf{IterChat}. First, we construct a new data format that categorizes the dialogue data into attributed historical preferences and one-turn dialogues. This reduces the probability of annotation errors and improves annotation efficiency. Then, to generate a high-quality and diverse dialogue dataset, we adopt GPT4 to pre-define the preference slots in the target preference extractor task and then randomly sample the subset of the slots and their corresponding schema values to create the dialogue datasets. Experimental results indicate that fine-tuning or only few-shot prompting with the new dialogue format yields superior performance compared to the original multi-turn dialogues. Additionally, the new data format improves annotator efficiency with a win rate of 28.4\% higher than the original multi-turn dialogues.</li>
</ul>

<h3>Title: AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing</h3>
<ul>
<li><strong>Authors: </strong>Zhaonan Wang, Manyi Li, Changhe Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01740">https://arxiv.org/abs/2508.01740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01740">https://arxiv.org/pdf/2508.01740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01740]] AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing(https://arxiv.org/abs/2508.01740)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has witnessed exponential adoption across diverse applications, driving a critical need for semantic-aware 3D Gaussian representations to enable scene understanding and editing tasks. Existing approaches typically attach semantic features to a collection of free Gaussians and distill the features via differentiable rendering, leading to noisy segmentation and a messy selection of Gaussians. In this paper, we introduce AG$^2$aussian, a novel framework that leverages an anchor-graph structure to organize semantic features and regulate Gaussian primitives. Our anchor-graph structure not only promotes compact and instance-aware Gaussian distributions, but also facilitates graph-based propagation, achieving a clean and accurate instance-level Gaussian selection. Extensive validation across four applications, i.e. interactive click-based query, open-vocabulary text-driven query, object removal editing, and physics simulation, demonstrates the advantages of our approach and its benefits to various applications. The experiments and ablation studies further evaluate the effectiveness of the key designs of our approach.</li>
</ul>

<h3>Title: Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruofan Wang, Xin Wang, Yang Yao, Xuan Tong, Xingjun Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01741">https://arxiv.org/abs/2508.01741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01741">https://arxiv.org/pdf/2508.01741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01741]] Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models(https://arxiv.org/abs/2508.01741)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Fine-tuning open-source Vision-Language Models (VLMs) creates a critical yet underexplored attack surface: vulnerabilities in the base VLM could be retained in fine-tuned variants, rendering them susceptible to transferable jailbreak attacks. To demonstrate this risk, we introduce the Simulated Ensemble Attack (SEA), a novel grey-box jailbreak method in which the adversary has full access to the base VLM but no knowledge of the fine-tuned target's weights or training configuration. To improve jailbreak transferability across fine-tuned VLMs, SEA combines two key techniques: Fine-tuning Trajectory Simulation (FTS) and Targeted Prompt Guidance (TPG). FTS generates transferable adversarial images by simulating the vision encoder's parameter shifts, while TPG is a textual strategy that steers the language decoder toward adversarially optimized outputs. Experiments on the Qwen2-VL family (2B and 7B) demonstrate that SEA achieves high transfer attack success rates exceeding 86.5% and toxicity rates near 49.5% across diverse fine-tuned variants, even those specifically fine-tuned to improve safety behaviors. Notably, while direct PGD-based image jailbreaks rarely transfer across fine-tuned VLMs, SEA reliably exploits inherited vulnerabilities from the base model, significantly enhancing transferability. These findings highlight an urgent need to safeguard fine-tuned proprietary VLMs against transferable vulnerabilities inherited from open-source foundations, motivating the development of holistic defenses across the entire model lifecycle.</li>
</ul>

<h3>Title: AGFT: An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zicong Ye, Kunming Zhang, Guoming Tang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01744">https://arxiv.org/abs/2508.01744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01744">https://arxiv.org/pdf/2508.01744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01744]] AGFT: An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization(https://arxiv.org/abs/2508.01744)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The explosive growth of interactive Large Language Models (LLMs) has placed unprecedented demands for low latency on cloud GPUs, forcing them into high-power modes and causing escalating energy costs. Real-time inference workloads exhibit significant dynamic volatility, presenting substantial energy-saving opportunities. However, traditional static or rule-based power management strategies struggle to exploit these opportunities without compromising peak performance. To address this challenge, we propose AGFT (An Adaptive GPU Frequency Tuner), a framework that employs online reinforcement learning to autonomously learn an optimal frequency tuning policy. By monitoring real-time features like request load and latency, AGFT utilizes fine-grained frequency control for precise adjustments and intelligent action space pruning for stable, efficient decision-making. This creates a robust, automated energy management solution. We comprehensively evaluated AGFT in an environment simulating realistic, fluctuating inference requests. The experimental results demonstrate that AGFT successfully saves 44.3% of GPU energy consumption while introducing a minimal performance latency overhead of under 10%. This achievement translates into a comprehensive Energy-Delay Product (EDP) optimization of up to 40.3%, clearly showing that our framework can significantly enhance the energy efficiency and economic benefits of existing LLM inference clusters without compromising service quality.</li>
</ul>

<h3>Title: Energy-Efficient Federated Learning for Edge Real-Time Vision via Joint Data, Computation, and Communication Design</h3>
<ul>
<li><strong>Authors: </strong>Xiangwang Hou, Jingjing Wang, Fangming Guan, Jun Du, Chunxiao Jiang, Yong Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01745">https://arxiv.org/abs/2508.01745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01745">https://arxiv.org/pdf/2508.01745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01745]] Energy-Efficient Federated Learning for Edge Real-Time Vision via Joint Data, Computation, and Communication Design(https://arxiv.org/abs/2508.01745)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, diffusion</a></li>
<li><strong>Abstract: </strong>Emerging real-time computer vision (CV) applications on wireless edge devices demand energy-efficient and privacy-preserving learning. Federated learning (FL) enables on-device training without raw data sharing, yet remains challenging in resource-constrained environments due to energy-intensive computation and communication, as well as limited and non-i.i.d. local data. We propose FedDPQ, an ultra energy-efficient FL framework for real-time CV over unreliable wireless networks. FedDPQ integrates diffusion-based data augmentation, model pruning, communication quantization, and transmission power control to enhance training efficiency. It expands local datasets using synthetic data, reduces computation through pruning, compresses updates via quantization, and mitigates transmission outages with adaptive power control. We further derive a closed-form energy-convergence model capturing the coupled impact of these components, and develop a Bayesian optimization(BO)-based algorithm to jointly tune data augmentation strategy, pruning ratio, quantization level, and power control. To the best of our knowledge, this is the first work to jointly optimize FL performance from the perspectives of data, computation, and communication under unreliable wireless conditions. Experiments on representative CV tasks show that FedDPQ achieves superior convergence speed and energy efficiency.</li>
</ul>

<h3>Title: Improving Noise Efficiency in Privacy-preserving Dataset Distillation</h3>
<ul>
<li><strong>Authors: </strong>Runkai Zheng, Vishnu Asutosh Dasu, Yinong Oliver Wang, Haohan Wang, Fernando De la Torre</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01749">https://arxiv.org/abs/2508.01749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01749">https://arxiv.org/pdf/2508.01749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01749]] Improving Noise Efficiency in Privacy-preserving Dataset Distillation(https://arxiv.org/abs/2508.01749)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Modern machine learning models heavily rely on large datasets that often include sensitive and private information, raising serious privacy concerns. Differentially private (DP) data generation offers a solution by creating synthetic datasets that limit the leakage of private information within a predefined privacy budget; however, it requires a substantial amount of data to achieve performance comparable to models trained on the original data. To mitigate the significant expense incurred with synthetic data generation, Dataset Distillation (DD) stands out for its remarkable training and storage efficiency. This efficiency is particularly advantageous when integrated with DP mechanisms, curating compact yet informative synthetic datasets without compromising privacy. However, current state-of-the-art private DD methods suffer from a synchronized sampling-optimization process and the dependency on noisy training signals from randomly initialized networks. This results in the inefficient utilization of private information due to the addition of excessive noise. To address these issues, we introduce a novel framework that decouples sampling from optimization for better convergence and improves signal quality by mitigating the impact of DP noise through matching in an informative subspace. On CIFAR-10, our method achieves a \textbf{10.0\%} improvement with 50 images per class and \textbf{8.3\%} increase with just \textbf{one-fifth} the distilled set size of previous state-of-the-art methods, demonstrating significant potential to advance privacy-preserving DD.</li>
</ul>

<h3>Title: LLM-Assisted Model-Based Fuzzing of Protocol Implementations</h3>
<ul>
<li><strong>Authors: </strong>Changze Huang, Di Wang, Zhi Quan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01750">https://arxiv.org/abs/2508.01750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01750">https://arxiv.org/pdf/2508.01750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01750]] LLM-Assisted Model-Based Fuzzing of Protocol Implementations(https://arxiv.org/abs/2508.01750)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Testing network protocol implementations is critical for ensuring the reliability, security, and interoperability of distributed systems. Faults in protocol behavior can lead to vulnerabilities and system failures, especially in real-time and mission-critical applications. A common approach to protocol testing involves constructing Markovian models that capture the state transitions and expected behaviors of the protocol. However, building such models typically requires significant domain expertise and manual effort, making the process time-consuming and difficult to scale across diverse protocols and implementations. We propose a novel method that leverages large language models (LLMs) to automatically generate sequences for testing network protocol implementations. Our approach begins by defining the full set of possible protocol states, from which the LLM selects a subset to model the target implementation. Using this state-based model, we prompt the LLM to generate code that produces sequences of states. This program serves as a protocol-specific sequences generator. The sequences generator then generates test inputs to call the protocol implementation under various conditions. We evaluated our approach on three widely used network protocol implementations and successfully identified 12 previously unknown vulnerabilities. We have reported them to the respective developers for confirmation. This demonstrates the practical effectiveness of our LLM-assisted fuzzing framework in uncovering real-world security issues.</li>
</ul>

<h3>Title: Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring</h3>
<ul>
<li><strong>Authors: </strong>Kumail Abbas, Zeeshan Afzal, Aqeel Raza, Taha Mansouri, Andrew W. Dowsey, Chaidate Inchaisri, Ali Alameer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01752">https://arxiv.org/abs/2508.01752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01752">https://arxiv.org/pdf/2508.01752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01752]] Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring(https://arxiv.org/abs/2508.01752)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Activity and behaviour correlate with dairy cow health and welfare, making continual and accurate monitoring crucial for disease identification and farm productivity. Manual observation and frequent assessments are laborious and inconsistent for activity monitoring. In this study, we developed a unique multi-camera, real-time tracking system for indoor-housed Holstein Friesian dairy cows. This technology uses cutting-edge computer vision techniques, including instance segmentation and tracking algorithms to monitor cow activity seamlessly and accurately. An integrated top-down barn panorama was created by geometrically aligning six camera feeds using homographic transformations. The detection phase used a refined YOLO11-m model trained on an overhead cow dataset, obtaining high accuracy (mAP\@0.50 = 0.97, F1 = 0.95). SAMURAI, an upgraded Segment Anything Model 2.1, generated pixel-precise cow masks for instance segmentation utilizing zero-shot learning and motion-aware memory. Even with occlusion and fluctuating posture, a motion-aware Linear Kalman filter and IoU-based data association reliably identified cows over time for object tracking. The proposed system significantly outperformed Deep SORT Realtime. Multi-Object Tracking Accuracy (MOTA) was 98.7% and 99.3% in two benchmark video sequences, with IDF1 scores above 99% and near-zero identity switches. This unified multi-camera system can track dairy cows in complex interior surroundings in real time, according to our data. The system reduces redundant detections across overlapping cameras, maintains continuity as cows move between viewpoints, with the aim of improving early sickness prediction through activity quantification and behavioural classification.</li>
</ul>

<h3>Title: AI-Generated Text is Non-Stationary: Detection via Temporal Tomography</h3>
<ul>
<li><strong>Authors: </strong>Alva West, Yixuan Weng, Minjun Zhu, Luodan Zhang, Zhen Lin, Guangsheng Bao, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01754">https://arxiv.org/abs/2508.01754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01754">https://arxiv.org/pdf/2508.01754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01754]] AI-Generated Text is Non-Stationary: Detection via Temporal Tomography(https://arxiv.org/abs/2508.01754)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The field of AI-generated text detection has evolved from supervised classification to zero-shot statistical analysis. However, current approaches share a fundamental limitation: they aggregate token-level measurements into scalar scores, discarding positional information about where anomalies occur. Our empirical analysis reveals that AI-generated text exhibits significant non-stationarity, statistical properties vary by 73.8\% more between text segments compared to human writing. This discovery explains why existing detectors fail against localized adversarial perturbations that exploit this overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT), a novel detection paradigm that preserves positional information by reformulating detection as a signal processing task. TDT treats token-level discrepancies as a time-series signal and applies Continuous Wavelet Transform to generate a two-dimensional time-scale representation, capturing both the location and linguistic scale of statistical anomalies. On the RAID benchmark, TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More importantly, TDT demonstrates robust performance on adversarial tasks, with 14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its sophisticated analysis, TDT maintains practical efficiency with only 13\% computational overhead. Our work establishes non-stationarity as a fundamental characteristic of AI-generated text and demonstrates that preserving temporal dynamics is essential for robust detection.</li>
</ul>

<h3>Title: Semantically-Guided Inference for Conditional Diffusion Models: Enhancing Covariate Consistency in Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Rui Ding, Hanyang Meng, Zeyang Zhang, Jielong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01761">https://arxiv.org/abs/2508.01761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01761">https://arxiv.org/pdf/2508.01761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01761]] Semantically-Guided Inference for Conditional Diffusion Models: Enhancing Covariate Consistency in Time Series Forecasting(https://arxiv.org/abs/2508.01761)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong performance in time series forecasting, yet often suffer from semantic misalignment between generated trajectories and conditioning covariates, especially under complex or multimodal conditions. To address this issue, we propose SemGuide, a plug-and-play, inference-time method that enhances covariate consistency in conditional diffusion models. Our approach introduces a scoring network to assess the semantic alignment between intermediate diffusion states and future covariates. These scores serve as proxy likelihoods in a stepwise importance reweighting procedure, which progressively adjusts the sampling path without altering the original training process. The method is model-agnostic and compatible with any conditional diffusion framework. Experiments on real-world forecasting tasks show consistent gains in both predictive accuracy and covariate alignment, with especially strong performance under complex conditioning scenarios.</li>
</ul>

<h3>Title: "Energon": Unveiling Transformers from GPU Power and Thermal Side-Channels</h3>
<ul>
<li><strong>Authors: </strong>Arunava Chaudhuri, Shubhi Shukla, Sarani Bhattacharya, Debdeep Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01768">https://arxiv.org/abs/2508.01768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01768">https://arxiv.org/pdf/2508.01768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01768]] "Energon": Unveiling Transformers from GPU Power and Thermal Side-Channels(https://arxiv.org/abs/2508.01768)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, transformer</a></li>
<li><strong>Abstract: </strong>Transformers have become the backbone of many Machine Learning (ML) applications, including language translation, summarization, and computer vision. As these models are increasingly deployed in shared Graphics Processing Unit (GPU) environments via Machine Learning as a Service (MLaaS), concerns around their security grow. In particular, the risk of side-channel attacks that reveal architectural details without physical access remains underexplored, despite the high value of the proprietary models they target. This work to the best of our knowledge is the first to investigate GPU power and thermal fluctuations as side-channels and further exploit them to extract information from pre-trained transformer models. The proposed analysis shows how these side channels can be exploited at user-privilege to reveal critical architectural details such as encoder/decoder layer and attention head for both language and vision transformers. We demonstrate the practical impact by evaluating multiple language and vision pre-trained transformers which are publicly available. Through extensive experimental evaluations, we demonstrate that the attack model achieves a high accuracy of over 89% on average for model family identification and 100% for hyperparameter classification, in both single-process as well as noisy multi-process scenarios. Moreover, by leveraging the extracted architectural information, we demonstrate highly effective black-box transfer adversarial attacks with an average success rate exceeding 93%, underscoring the security risks posed by GPU side-channel leakage in deployed transformer models.</li>
</ul>

<h3>Title: VAGPO: Vision-augmented Asymmetric Group Preference Optimization for the Routing Problems</h3>
<ul>
<li><strong>Authors: </strong>Shiyan Liu, Bohan Tan, Yan Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01774">https://arxiv.org/abs/2508.01774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01774">https://arxiv.org/pdf/2508.01774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01774]] VAGPO: Vision-augmented Asymmetric Group Preference Optimization for the Routing Problems(https://arxiv.org/abs/2508.01774)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The routing problems such as the Traveling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) are well-known combinatorial optimization challenges with broad practical relevance. Recent data-driven optimization methods have made significant progress, yet they often face limitations in training efficiency and generalization to large-scale instances. In this paper, we propose a novel Vision-Augmented Asymmetric Group Preference Optimization (VAGPO) approach for solving the routing problems. By leveraging ResNet-based visual encoding and Transformer-based sequential modeling, VAGPO captures both spatial structure and temporal dependencies. Furthermore, we introduce an asymmetric group preference optimization strategy that significantly accelerates convergence compared to commonly used policy gradient methods. Experimental results on TSP and CVRP benchmarks show that the proposed VAGPO not only achieves highly competitive solution quality but also exhibits strong generalization to larger instances (up to 1000 nodes) without re-training, highlighting its effectiveness in both learning efficiency and scalability.</li>
</ul>

<h3>Title: DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhigang Sun, Yiru Wang, Anqing Jiang, Shuo Wang, Yu Gao, Yuwen Heng, Shouyi Zhang, An He, Hao Jiang, Jinhao Chai, Zichong Gu, Wang Jijun, Shichen Tang, Lavdim Halilaj, Juergen Luettin, Hao Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01778">https://arxiv.org/abs/2508.01778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01778">https://arxiv.org/pdf/2508.01778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01778]] DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion(https://arxiv.org/abs/2508.01778)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at this https URL.</li>
</ul>

<h3>Title: A comprehensive taxonomy of hallucinations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Manuel Cossio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01781">https://arxiv.org/abs/2508.01781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01781">https://arxiv.org/pdf/2508.01781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01781]] A comprehensive taxonomy of hallucinations in Large Language Models(https://arxiv.org/abs/2508.01781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing, yet their propensity for hallucination, generating plausible but factually incorrect or fabricated content, remains a critical challenge. This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal definition and a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training. It explores core distinctions, differentiating between intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality), as well as factuality (absolute correctness) and faithfulness (adherence to input). The report then details specific manifestations, including factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains like code generation and multimodal applications. It analyzes the underlying causes, categorizing them into data-related issues, model-related factors, and prompt-related influences. Furthermore, the report examines cognitive and human factors influencing hallucination perception, surveys evaluation benchmarks and metrics for detection, and outlines architectural and systemic mitigation strategies. Finally, it introduces web-based resources for monitoring LLM releases and performance. This report underscores the complex, multifaceted nature of LLM hallucinations and emphasizes that, given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.</li>
</ul>

<h3>Title: RouteMark: A Fingerprint for Intellectual Property Attribution in Routing-based Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Xin He, Junxi Shen, Zhenheng Tang, Xiaowen Chu, Bo Li, Ivor W. Tsang, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01784">https://arxiv.org/abs/2508.01784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01784">https://arxiv.org/pdf/2508.01784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01784]] RouteMark: A Fingerprint for Intellectual Property Attribution in Routing-based Model Merging(https://arxiv.org/abs/2508.01784)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Model merging via Mixture-of-Experts (MoE) has emerged as a scalable solution for consolidating multiple task-specific models into a unified sparse architecture, where each expert is derived from a model fine-tuned on a distinct task. While effective for multi-task integration, this paradigm introduces a critical yet underexplored challenge: how to attribute and protect the intellectual property (IP) of individual experts after merging. We propose RouteMark, a framework for IP protection in merged MoE models through the design of expert routing fingerprints. Our key insight is that task-specific experts exhibit stable and distinctive routing behaviors under probing inputs. To capture these patterns, we construct expert-level fingerprints using two complementary statistics: the Routing Score Fingerprint (RSF), quantifying the intensity of expert activation, and the Routing Preference Fingerprint (RPF), characterizing the input distribution that preferentially activates each expert. These fingerprints are reproducible, task-discriminative, and lightweight to construct. For attribution and tampering detection, we introduce a similarity-based matching algorithm that compares expert fingerprints between a suspect and a reference (victim) model. Extensive experiments across diverse tasks and CLIP-based MoE architectures show that RouteMark consistently yields high similarity for reused experts and clear separation from unrelated ones. Moreover, it remains robust against both structural tampering (expert replacement, addition, deletion) and parametric tampering (fine-tuning, pruning, permutation), outperforming weight- and activation-based baseliness. Our work lays the foundation for RouteMark as a practical and broadly applicable framework for IP verification in MoE-based model merging.</li>
</ul>

<h3>Title: Skip priors and add graph-based anatomical information, for point-based Couinaud segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiaotong Zhang, Alexander Broersen, Gonnie CM van Erp, Silvia L. Pintea, Jouke Dijkstra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01785">https://arxiv.org/abs/2508.01785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01785">https://arxiv.org/pdf/2508.01785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01785]] Skip priors and add graph-based anatomical information, for point-based Couinaud segmentation(https://arxiv.org/abs/2508.01785)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The preoperative planning of liver surgery relies on Couinaud segmentation from computed tomography (CT) images, to reduce the risk of bleeding and guide the resection procedure. Using 3D point-based representations, rather than voxelizing the CT volume, has the benefit of preserving the physical resolution of the CT. However, point-based representations need prior knowledge of the liver vessel structure, which is time consuming to acquire. Here, we propose a point-based method for Couinaud segmentation, without explicitly providing the prior liver vessel structure. To allow the model to learn this anatomical liver vessel structure, we add a graph reasoning module on top of the point features. This adds implicit anatomical information to the model, by learning affinities across point neighborhoods. Our method is competitive on the MSD and LiTS public datasets in Dice coefficient and average surface distance scores compared to four pioneering point-based methods. Our code is available at this https URL.</li>
</ul>

<h3>Title: CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase</h3>
<ul>
<li><strong>Authors: </strong>Fatimah Mohamed Emad Elden</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01791">https://arxiv.org/abs/2508.01791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01791">https://arxiv.org/pdf/2508.01791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01791]] CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase(https://arxiv.org/abs/2508.01791)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, transformer</a></li>
<li><strong>Abstract: </strong>The field of Continuous Sign Language Recognition (CSLR) poses substantial technical challenges, including fluid inter-sign transitions, the absence of temporal boundaries, and co-articulation effects. This paper, developed for the MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of signer-independent recognition to advance the generalization capabilities of CSLR systems across diverse signers. A data-centric methodology is proposed, centered on systematic feature engineering, a robust preprocessing pipeline, and an optimized model architecture. Key contributions include a principled feature selection process guided by Exploratory Data Analysis (EDA) to isolate communicative keypoints, a rigorous preprocessing pipeline incorporating DBSCAN-based outlier filtering and spatial normalization, and the novel CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer design of the Conformer model, leveraging its capacity to model local temporal dependencies and global sequence context; a characteristic uniquely suited for the spatio-temporal dynamics of sign language. The proposed methodology achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on the development set and 12.01% on the test set, a result that secured a 3rd place ranking on the official competition platform. This research validates the efficacy of cross-domain architectural adaptation, demonstrating that the Conformer model, originally conceived for speech recognition, can be successfully repurposed to establish a new state-of-the-art performance in keypoint-based CSLR.</li>
</ul>

<h3>Title: A Survey on Privacy-Preserving Computing in the Automotive Domain</h3>
<ul>
<li><strong>Authors: </strong>Nergiz Yuca, Nikolay Matyunin, Ektor Arzoglou, Nikolaos Athanasios Anagnostopoulos, Stefan Katzenbeisser</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01798">https://arxiv.org/abs/2508.01798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01798">https://arxiv.org/pdf/2508.01798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01798]] A Survey on Privacy-Preserving Computing in the Automotive Domain(https://arxiv.org/abs/2508.01798)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>As vehicles become increasingly connected and autonomous, they accumulate and manage various personal data, thereby presenting a key challenge in preserving privacy during data sharing and processing. This survey reviews applications of Secure Multi-Party Computation (MPC) and Homomorphic Encryption (HE) that address these privacy concerns in the automotive domain. First, we identify the scope of privacy-sensitive use cases for these technologies, by surveying existing works that address privacy issues in different automotive contexts, such as location-based services, mobility infrastructures, traffic management, etc. Then, we review recent works that employ MPC and HE as solutions for these use cases in detail. Our survey highlights the applicability of these privacy-preserving technologies in the automotive context, while also identifying challenges and gaps in the current research landscape. This work aims to provide a clear and comprehensive overview of this emerging field and to encourage further research in this domain.</li>
</ul>

<h3>Title: Mitigating Persistent Client Dropout in Asynchronous Decentralized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Ignacy Stępka, Nicholas Gisolfi, Kacper Trębacz, Artur Dubrawski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01807">https://arxiv.org/abs/2508.01807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01807">https://arxiv.org/pdf/2508.01807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01807]] Mitigating Persistent Client Dropout in Asynchronous Decentralized Federated Learning(https://arxiv.org/abs/2508.01807)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>We consider the problem of persistent client dropout in asynchronous Decentralized Federated Learning (DFL). Asynchronicity and decentralization obfuscate information about model updates among federation peers, making recovery from a client dropout difficult. Access to the number of learning epochs, data distributions, and all the information necessary to precisely reconstruct the missing neighbor's loss functions is limited. We show that obvious mitigations do not adequately address the problem and introduce adaptive strategies based on client reconstruction. We show that these strategies can effectively recover some performance loss caused by dropout. Our work focuses on asynchronous DFL with local regularization and differs substantially from that in the existing literature. We evaluate the proposed methods on tabular and image datasets, involve three DFL algorithms, and three data heterogeneity scenarios (iid, non-iid, class-focused non-iid). Our experiments show that the proposed adaptive strategies can be effective in maintaining robustness of federated learning, even if they do not reconstruct the missing client's data precisely. We also discuss the limitations and identify future avenues for tackling the problem of client dropout.</li>
</ul>

<h3>Title: AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhao, Chengxiao Dai, Wei Zhuo, Tan Chuan Fu, Yue Xiu, Dusit Niyato, Jonathan Z. Low, Eugene Ho Hong Zhuang, Daren Zong Loong Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01815">https://arxiv.org/abs/2508.01815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01815">https://arxiv.org/pdf/2508.01815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01815]] AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy(https://arxiv.org/abs/2508.01815)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Question answering over heterogeneous knowledge graphs (KGQA) involves reasoning across diverse schemas, incomplete alignments, and distributed data sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific fine-tuning or operate within single-graph settings, limiting their generalizability in low-resource domains and their ability to handle queries spanning multiple graphs. These challenges are particularly relevant in domains such as the circular economy, where information about classifications, processes, and emissions is distributed across independently curated knowledge graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes KGQA into subtasks managed by specialized agents responsible for retrieval, query generation, and verification. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects structurally invalid and semantically underspecified queries through symbolic validation and counterfactual consistency checks. Experiments on real-world circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing the average prompt length by 46.4%. These results demonstrate the benefits of agent-based schema-aware reasoning for scalable KGQA and support decision-making in sustainability domains through robust cross-graph reasoning.</li>
</ul>

<h3>Title: MLP Memory: Language Modeling with Retriever-pretrained External Memory</h3>
<ul>
<li><strong>Authors: </strong>Rubin Wei, Jiaqi Cao, Jiarui Wang, Jushi Kai, Qipeng Guo, Bowen Zhou, Zhouhan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01832">https://arxiv.org/abs/2508.01832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01832">https://arxiv.org/pdf/2508.01832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01832]] MLP Memory: Language Modeling with Retriever-pretrained External Memory(https://arxiv.org/abs/2508.01832)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While modern decoder-only LLMs achieve superior performance across various domains, hallucinations have risen to be a common problem in their generated text, hindering their application in knowledge-intensive tasks. Retriever-augmented generation (RAG) offers a solution, but the non-parametric nature of the retriever hinders its deep interaction with LLM. In this work, we propose to decouple memorization from the LLM decoder using a pretrained, differentiable external memory. The external memory is an MLP pretrained by imitating the behavior of a retriever on the entire pretraining dataset. Our resulting architecture, which comprises a transformer decoder and an external MLP memory pretrained on language modeling and retriever imitation respectively, demonstrates strong perplexity and performance on downstream tasks. Experiments show our architecture exhibits steeper power-law scaling with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web datasets compared to decoder-only models while benefiting from added training without overfitting. We demonstrate superior performance on three hallucination benchmarks and nine memory-intensive tasks. Additionally, our approach delivers $80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP memory improves StrategyQA performance. We will open-source our code and models in the future.</li>
</ul>

<h3>Title: Neural Predictive Control to Coordinate Discrete- and Continuous-Time Models for Time-Series Analysis with Control-Theoretical Improvements</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Muhao Guo, Yang Weng, Hanghang Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01833">https://arxiv.org/abs/2508.01833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01833">https://arxiv.org/pdf/2508.01833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01833]] Neural Predictive Control to Coordinate Discrete- and Continuous-Time Models for Time-Series Analysis with Control-Theoretical Improvements(https://arxiv.org/abs/2508.01833)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep sequence models have achieved notable success in time-series analysis, such as interpolation and forecasting. Recent advances move beyond discrete-time architectures like Recurrent Neural Networks (RNNs) toward continuous-time formulations such as the family of Neural Ordinary Differential Equations (Neural ODEs). Generally, they have shown that capturing the underlying dynamics is beneficial for generic tasks like interpolation, extrapolation, and classification. However, existing methods approximate the dynamics using unconstrained neural networks, which struggle to adapt reliably under distributional shifts. In this paper, we recast time-series problems as the continuous ODE-based optimal control problem. Rather than learning dynamics solely from data, we optimize control actions that steer ODE trajectories toward task objectives, bringing control-theoretical performance guarantees. To achieve this goal, we need to (1) design the appropriate control actions and (2) apply effective optimal control algorithms. As the actions should contain rich context information, we propose to employ the discrete-time model to process past sequences and generate actions, leading to a coordinate model to extract long-term temporal features to modulate short-term continuous dynamics. During training, we apply model predictive control to plan multi-step future trajectories, minimize a task-specific cost, and greedily select the optimal current action. We show that, under mild assumptions, this multi-horizon optimization leads to exponential convergence to infinite-horizon solutions, indicating that the coordinate model can gain robust and generalizable performance. Extensive experiments on diverse time-series datasets validate our method's superior generalization and adaptability compared to state-of-the-art baselines.</li>
</ul>

<h3>Title: Diffusion-based 3D Hand Motion Recovery with Intuitive Physics</h3>
<ul>
<li><strong>Authors: </strong>Yufei Zhang, Zijun Cui, Jeffrey O. Kephart, Qiang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01835">https://arxiv.org/abs/2508.01835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01835">https://arxiv.org/pdf/2508.01835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01835]] Diffusion-based 3D Hand Motion Recovery with Intuitive Physics(https://arxiv.org/abs/2508.01835)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While 3D hand reconstruction from monocular images has made significant progress, generating accurate and temporally coherent motion estimates from videos remains challenging, particularly during hand-object interactions. In this paper, we present a novel 3D hand motion recovery framework that enhances image-based reconstructions through a diffusion-based and physics-augmented motion refinement model. Our model captures the distribution of refined motion estimates conditioned on initial ones, generating improved sequences through an iterative denoising process. Instead of relying on scarce annotated video data, we train our model only using motion capture data without images. We identify valuable intuitive physics knowledge during hand-object interactions, including key motion states and their associated motion constraints. We effectively integrate these physical insights into our diffusion model to improve its performance. Extensive experiments demonstrate that our approach significantly improves various frame-wise reconstruction methods, achieving state-of-the-art (SOTA) performance on existing benchmarks.</li>
</ul>

<h3>Title: A Simple Algebraic Solution for Estimating the Pose of a Camera from Planar Point Features</h3>
<ul>
<li><strong>Authors: </strong>Tarek Bouazza, Tarek Hamel, Claude Samson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01836">https://arxiv.org/abs/2508.01836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01836">https://arxiv.org/pdf/2508.01836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01836]] A Simple Algebraic Solution for Estimating the Pose of a Camera from Planar Point Features(https://arxiv.org/abs/2508.01836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a simple algebraic method to estimate the pose of a camera relative to a planar target from $n \geq 4$ reference points with known coordinates in the target frame and their corresponding bearing measurements in the camera frame. The proposed approach follows a hierarchical structure; first, the unit vector normal to the target plane is determined, followed by the camera's position vector, its distance to the target plane, and finally, the full orientation. To improve the method's robustness to measurement noise, an averaging methodology is introduced to refine the estimation of the target's normal direction. The accuracy and robustness of the approach are validated through extensive experiments.</li>
</ul>

<h3>Title: Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems</h3>
<ul>
<li><strong>Authors: </strong>Zhongliang Guo, Yifei Qian, Yanli Li, Weiye Li, Chun Tong Lei, Shuai Zhao, Lei Fang, Ognjen Arandjelović, Chun Pong Lau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01845">https://arxiv.org/abs/2508.01845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01845">https://arxiv.org/pdf/2508.01845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01845]] Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems(https://arxiv.org/abs/2508.01845)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, biometric, generative</a></li>
<li><strong>Abstract: </strong>Adversarial attacks against computer vision systems have emerged as a critical research area that challenges the fundamental assumptions about neural network robustness and security. This comprehensive survey examines the evolving landscape of adversarial techniques, revealing their dual nature as both sophisticated security threats and valuable defensive tools. We provide a systematic analysis of adversarial attack methodologies across three primary domains: pixel-space attacks, physically realizable attacks, and latent-space attacks. Our investigation traces the technical evolution from early gradient-based methods such as FGSM and PGD to sophisticated optimization techniques incorporating momentum, adaptive step sizes, and advanced transferability mechanisms. We examine how physically realizable attacks have successfully bridged the gap between digital vulnerabilities and real-world threats through adversarial patches, 3D textures, and dynamic optical perturbations. Additionally, we explore the emergence of latent-space attacks that leverage semantic structure in internal representations to create more transferable and meaningful adversarial examples. Beyond traditional offensive applications, we investigate the constructive use of adversarial techniques for vulnerability assessment in biometric authentication systems and protection against malicious generative models. Our analysis reveals critical research gaps, particularly in neural style transfer protection and computational efficiency requirements. This survey contributes a comprehensive taxonomy, evolution analysis, and identification of future research directions, aiming to advance understanding of adversarial vulnerabilities and inform the development of more robust and trustworthy computer vision systems.</li>
</ul>

<h3>Title: Causal Discovery in Multivariate Time Series through Mutual Information Featurization</h3>
<ul>
<li><strong>Authors: </strong>Gian Marco Paldino, Gianluca Bontempi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01848">https://arxiv.org/abs/2508.01848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01848">https://arxiv.org/pdf/2508.01848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01848]] Causal Discovery in Multivariate Time Series through Mutual Information Featurization(https://arxiv.org/abs/2508.01848)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Discovering causal relationships in complex multivariate time series is a fundamental scientific challenge. Traditional methods often falter, either by relying on restrictive linear assumptions or on conditional independence tests that become uninformative in the presence of intricate, non-linear dynamics. This paper proposes a new paradigm, shifting from statistical testing to pattern recognition. We hypothesize that a causal link creates a persistent and learnable asymmetry in the flow of information through a system's temporal graph, even when clear conditional independencies are obscured. We introduce Temporal Dependency to Causality (TD2C), a supervised learning framework that operationalizes this hypothesis. TD2C learns to recognize these complex causal signatures from a rich set of information-theoretic and statistical descriptors. Trained exclusively on a diverse collection of synthetic time series, TD2C demonstrates remarkable zero-shot generalization to unseen dynamics and established, realistic benchmarks. Our results show that TD2C achieves state-of-the-art performance, consistently outperforming established methods, particularly in high-dimensional and non-linear settings. By reframing the discovery problem, our work provides a robust and scalable new tool for uncovering causal structures in complex systems.</li>
</ul>

<h3>Title: Context Guided Transformer Entropy Modeling for Video Compression</h3>
<ul>
<li><strong>Authors: </strong>Junlong Tong, Wei Zhang, Yaohui Jin, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01852">https://arxiv.org/abs/2508.01852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01852">https://arxiv.org/pdf/2508.01852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01852]] Context Guided Transformer Entropy Modeling for Video Compression(https://arxiv.org/abs/2508.01852)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Conditional entropy models effectively leverage spatio-temporal contexts to reduce video redundancy. However, incorporating temporal context often introduces additional model complexity and increases computational cost. In parallel, many existing spatial context models lack explicit modeling the ordering of spatial dependencies, which may limit the availability of relevant context during decoding. To address these issues, we propose the Context Guided Transformer (CGT) entropy model, which estimates probability mass functions of the current frame conditioned on resampled temporal context and dependency-weighted spatial context. A temporal context resampler learns predefined latent queries to extract critical temporal information using transformer encoders, reducing downstream computational overhead. Meanwhile, a teacher-student network is designed as dependency-weighted spatial context assigner to explicitly model the dependency of spatial context order. The teacher generates an attention map to represent token importance and an entropy map to reflect prediction certainty from randomly masked inputs, guiding the student to select the weighted top-k tokens with the highest spatial dependency. During inference, only the student is used to predict undecoded tokens based on high-dependency context. Experimental results demonstrate that our CGT model reduces entropy modeling time by approximately 65% and achieves an 11% BD-Rate reduction compared to the previous state-of-the-art conditional entropy model.</li>
</ul>

<h3>Title: Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yijun Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01862">https://arxiv.org/abs/2508.01862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01862">https://arxiv.org/pdf/2508.01862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01862]] Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models(https://arxiv.org/abs/2508.01862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated remarkable capabilities across diverse tasks, yet they frequently generate hallucinations outputs that are fluent but factually incorrect or unsupported. We propose Counterfactual Probing, a novel approach for detecting and mitigating hallucinations in LLM outputs. Our method dynamically generates counterfactual statements that appear plausible but contain subtle factual errors, then evaluates the model's sensitivity to these perturbations. We hypothesize that genuine knowledge exhibits robustness to counterfactual variations, while hallucinated content shows inconsistent confidence patterns when confronted with plausible alternatives. Our comprehensive evaluation on TruthfulQA, factual statement datasets, and curated hallucination examples demonstrates that counterfactual probing achieves superior detection performance compared to baseline methods, while our adaptive mitigation strategies reduce hallucination scores by an average of 24.5%. The approach requires no model retraining and can be integrated into existing LLM pipelines as a realtime verification mechanism.</li>
</ul>

<h3>Title: Hard-Earned Lessons in Access Control at Scale: Enforcing Identity and Policy Across Trust Boundaries with Reverse Proxies and mTLS</h3>
<ul>
<li><strong>Authors: </strong>Sanjay Singh, Mitendra Mahto</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01863">https://arxiv.org/abs/2508.01863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01863">https://arxiv.org/pdf/2508.01863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01863]] Hard-Earned Lessons in Access Control at Scale: Enforcing Identity and Policy Across Trust Boundaries with Reverse Proxies and mTLS(https://arxiv.org/abs/2508.01863)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>In today's enterprise environment, traditional access methods such as Virtual Private Networks (VPNs) and application-specific Single Sign-On (SSO) often fall short when it comes to securely scaling access for a distributed and dynamic workforce. This paper presents our experience implementing a modern, Zero Trust-aligned architecture that leverages a reverse proxy integrated with Mutual TLS (mTLS) and centralized SSO, along with the key challenges we encountered and lessons learned during its deployment and scaling. This multidimensional solution involves both per-device and per-user authentication, centralized enforcement of security policies, and comprehensive observability, hence enabling organizations to deliver secure and seamless access to their internal applications.</li>
</ul>

<h3>Title: DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization</h3>
<ul>
<li><strong>Authors: </strong>Siran Peng, Haoyuan Zhang, Li Gao, Tianshuo Zhang, Bao Li, Zhen Lei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01873">https://arxiv.org/abs/2508.01873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01873">https://arxiv.org/pdf/2508.01873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01873]] DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization(https://arxiv.org/abs/2508.01873)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, diffusion</a></li>
<li><strong>Abstract: </strong>The rapid evolution of deepfake generation techniques demands robust and accurate face forgery detection algorithms. While determining whether an image has been manipulated remains essential, the ability to precisely localize forgery artifacts has become increasingly important for improving model explainability and fostering user trust. To address this challenge, we propose DiffusionFF, a novel framework that enhances face forgery detection through diffusion-based artifact localization. Our method utilizes a denoising diffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps, which effectively capture subtle traces of manipulation. These DSSIM maps are then fused with high-level semantic features extracted by a pretrained forgery detector, leading to significant improvements in detection accuracy. Extensive experiments on both cross-dataset and intra-dataset benchmarks demonstrate that DiffusionFF not only achieves superior detection performance but also offers precise and fine-grained artifact localization, highlighting its overall effectiveness.</li>
</ul>

<h3>Title: Proactive Constrained Policy Optimization with Preemptive Penalty</h3>
<ul>
<li><strong>Authors: </strong>Ning Yang, Pengyu Wang, Guoqing Liu, Haifeng Zhang, Pin Lyu, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01883">https://arxiv.org/abs/2508.01883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01883">https://arxiv.org/pdf/2508.01883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01883]] Proactive Constrained Policy Optimization with Preemptive Penalty(https://arxiv.org/abs/2508.01883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Safe Reinforcement Learning (RL) often faces significant issues such as constraint violations and instability, necessitating the use of constrained policy optimization, which seeks optimal policies while ensuring adherence to specific constraints like safety. Typically, constrained optimization problems are addressed by the Lagrangian method, a post-violation remedial approach that may result in oscillations and overshoots. Motivated by this, we propose a novel method named Proactive Constrained Policy Optimization (PCPO) that incorporates a preemptive penalty mechanism. This mechanism integrates barrier items into the objective function as the policy nears the boundary, imposing a cost. Meanwhile, we introduce a constraint-aware intrinsic reward to guide boundary-aware exploration, which is activated only when the policy approaches the constraint boundary. We establish theoretical upper and lower bounds for the duality gap and the performance of the PCPO update, shedding light on the method's convergence characteristics. Additionally, to enhance the optimization performance, we adopt a policy iteration approach. An interesting finding is that PCPO demonstrates significant stability in experiments. Experimental results indicate that the PCPO framework provides a robust solution for policy optimization under constraints, with important implications for future research and practical applications.</li>
</ul>

<h3>Title: Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Aldan Creo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01887">https://arxiv.org/abs/2508.01887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01887">https://arxiv.org/pdf/2508.01887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01887]] Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection(https://arxiv.org/abs/2508.01887)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction</a></li>
<li><strong>Abstract: </strong>AI-generated text detectors have become essential tools for maintaining content authenticity, yet their robustness against evasion attacks remains questionable. We present PDFuzz, a novel attack that exploits the discrepancy between visual text layout and extraction order in PDF documents. Our method preserves exact textual content while manipulating character positioning to scramble extraction sequences. We evaluate this approach against the ArguGPT detector using a dataset of human and AI-generated text. Our results demonstrate complete evasion: detector performance drops from (93.6 $\pm$ 1.4) % accuracy and 0.938 $\pm$ 0.014 F1 score to random-level performance ((50.4 $\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity. Our work reveals a vulnerability in current detection systems that is inherent to PDF document structures and underscores the need for implementing sturdy safeguards against such attacks. We make our code publicly available at this https URL.</li>
</ul>

<h3>Title: Optimizing Day-Ahead Energy Trading with Proximal Policy Optimization and Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Navneet Verma, Ying Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01888">https://arxiv.org/abs/2508.01888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01888">https://arxiv.org/pdf/2508.01888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01888]] Optimizing Day-Ahead Energy Trading with Proximal Policy Optimization and Blockchain(https://arxiv.org/abs/2508.01888)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The increasing penetration of renewable energy sources in day-ahead energy markets introduces challenges in balancing supply and demand, ensuring grid resilience, and maintaining trust in decentralized trading systems. This paper proposes a novel framework that integrates the Proximal Policy Optimization (PPO) algorithm, a state-of-the-art reinforcement learning method, with blockchain technology to optimize automated trading strategies for prosumers in day-ahead energy markets. We introduce a comprehensive framework that employs RL agent for multi-objective energy optimization and blockchain for tamper-proof data and transaction management. Simulations using real-world data from the Electricity Reliability Council of Texas (ERCOT) demonstrate the effectiveness of our approach. The RL agent achieves demand-supply balancing within 2\% and maintains near-optimal supply costs for the majority of the operating hours. Moreover, it generates robust battery storage policies capable of handling variability in solar and wind generation. All decisions are recorded on an Algorand-based blockchain, ensuring transparency, auditability, and security - key enablers for trustworthy multi-agent energy trading. Our contributions include a novel system architecture, curriculum learning for robust agent development, and actionable policy insights for practical deployment.</li>
</ul>

<h3>Title: Medical Image De-Identification Resources: Synthetic DICOM Data and Tools for Validation</h3>
<ul>
<li><strong>Authors: </strong>Michael W. Rutherford, Tracy Nolan, Linmin Pei, Ulrike Wagner, Qinyan Pan, Phillip Farmer, Kirk Smith, Benjamin Kopchick, Laura Opsahl-Ong, Granger Sutton, David Clunie, Keyvan Farahani, Fred Prior</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01889">https://arxiv.org/abs/2508.01889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01889">https://arxiv.org/pdf/2508.01889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01889]] Medical Image De-Identification Resources: Synthetic DICOM Data and Tools for Validation(https://arxiv.org/abs/2508.01889)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Medical imaging research increasingly depends on large-scale data sharing to promote reproducibility and train Artificial Intelligence (AI) models. Ensuring patient privacy remains a significant challenge for open-access data sharing. Digital Imaging and Communications in Medicine (DICOM), the global standard data format for medical imaging, encodes both essential clinical metadata and extensive protected health information (PHI) and personally identifiable information (PII). Effective de-identification must remove identifiers, preserve scientific utility, and maintain DICOM validity. Tools exist to perform de-identification, but few assess its effectiveness, and most rely on subjective reviews, limiting reproducibility and regulatory confidence. To address this gap, we developed an openly accessible DICOM dataset infused with synthetic PHI/PII and an evaluation framework for benchmarking image de-identification workflows. The Medical Image de-identification (MIDI) dataset was built using publicly available de-identified data from The Cancer Imaging Archive (TCIA). It includes 538 subjects (216 for validation, 322 for testing), 605 studies, 708 series, and 53,581 DICOM image instances. These span multiple vendors, imaging modalities, and cancer types. Synthetic PHI and PII were embedded into structured data elements, plain text data elements, and pixel data to simulate real-world identity leaks encountered by TCIA curation teams. Accompanying evaluation tools include a Python script, answer keys (known truth), and mapping files that enable automated comparison of curated data against expected transformations. The framework is aligned with the HIPAA Privacy Rule "Safe Harbor" method, DICOM PS3.15 Confidentiality Profiles, and TCIA best practices. It supports objective, standards-driven evaluation of de-identification workflows, promoting safer and more consistent medical image sharing.</li>
</ul>

<h3>Title: Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Istabrak Abbes, Gopeshh Subbaraj, Matthew Riemer, Nizar Islah, Benjamin Therien, Tsuguchika Tabaru, Hiroaki Kingetsu, Sarath Chandar, Irina Rish</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01908">https://arxiv.org/abs/2508.01908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01908">https://arxiv.org/pdf/2508.01908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01908]] Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models(https://arxiv.org/abs/2508.01908)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.</li>
</ul>

<h3>Title: Analyzing The Mirai IoT Botnet and Its Recent Variants: Satori, Mukashi, Moobot, and Sonic</h3>
<ul>
<li><strong>Authors: </strong>Angela Famera, Ben Hilger, Suman Bhunia, Patrick Heil</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01909">https://arxiv.org/abs/2508.01909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01909">https://arxiv.org/pdf/2508.01909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01909]] Analyzing The Mirai IoT Botnet and Its Recent Variants: Satori, Mukashi, Moobot, and Sonic(https://arxiv.org/abs/2508.01909)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Mirai is undoubtedly one of the most significant Internet of Things (IoT) botnet attacks in history. In terms of its detrimental effects, seamless spread, and low detection rate, it surpassed its predecessors. Its developers released the source code, which triggered the development of several variants that combined the old code with newer vulnerabilities found on popular IoT devices. The prominent variants, Satori, Mukashi, Moobot, and Sonic1, together target more than 15 unique known vulnerabilities discovered between 2014-2021. The vulnerabilities include but are not limited to improper input validation, command injections, insufficient credential protection, and out-of-bound writes. With these new attack strategies, Satori compromised more than a quarter million devices within the first twelve hours of its release and peaked at almost 700,000 infected devices. Similarly, Mukashi made more than a hundred million Zyxel NAS devices vulnerable through its new exploits. This article reviews the attack methodologies and impacts of these variants in detail. It summarizes the common vulnerabilities targeted by these variants and analyzes the infection mechanism through vulnerability analysis. This article also provides an overview of possible defense solutions.</li>
</ul>

<h3>Title: A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology</h3>
<ul>
<li><strong>Authors: </strong>Kamal Al-Sabahi, Yousuf Khamis Al Mabsali</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01913">https://arxiv.org/abs/2508.01913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01913">https://arxiv.org/pdf/2508.01913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01913]] A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology(https://arxiv.org/abs/2508.01913)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust</a></li>
<li><strong>Abstract: </strong>Academic publishing, integral to knowledge dissemination and scientific advancement, increasingly faces threats from unethical practices such as unconsented authorship, gift authorship, author ambiguity, and undisclosed conflicts of interest. While existing infrastructures like ORCID effectively disambiguate researcher identities, they fall short in enforcing explicit authorship consent, accurately verifying contributor roles, and robustly detecting conflicts of interest during peer review. To address these shortcomings, this paper introduces a decentralized framework leveraging Self-Sovereign Identity (SSI) and blockchain technology. The proposed model uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to securely verify author identities and contributions, reducing ambiguity and ensuring accurate attribution. A blockchain-based trust registry records authorship consent and peer-review activity immutably. Privacy-preserving cryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support conflict-of-interest detection without revealing sensitive data. Verified authorship metadata and consent records are embedded in publications, increasing transparency. A stakeholder survey of researchers, editors, and reviewers suggests the framework improves ethical compliance and confidence in scholarly communication. This work represents a step toward a more transparent, accountable, and trustworthy academic publishing ecosystem.</li>
</ul>

<h3>Title: Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinting Huang, Michael Hahn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01916">https://arxiv.org/abs/2508.01916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01916">https://arxiv.org/pdf/2508.01916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01916]] Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning(https://arxiv.org/abs/2508.01916)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding internal representations of neural models is a core interest of mechanistic interpretability. Due to its large dimensionality, the representation space can encode various aspects about inputs. To what extent are different aspects organized and encoded in separate subspaces? Is it possible to find these ``natural'' subspaces in a purely unsupervised way? Somewhat surprisingly, we can indeed achieve this and find interpretable subspaces by a seemingly unrelated training objective. Our method, neighbor distance minimization (NDM), learns non-basis-aligned subspaces in an unsupervised manner. Qualitative analysis shows subspaces are interpretable in many cases, and encoded information in obtained subspaces tends to share the same abstract concept across different inputs, making such subspaces similar to ``variables'' used by the model. We also conduct quantitative experiments using known circuits in GPT-2; results show a strong connection between subspaces and circuit variables. We also provide evidence showing scalability to 2B models by finding separate subspaces mediating context and parametric knowledge routing. Viewed more broadly, our findings offer a new perspective on understanding model internals and building circuits.</li>
</ul>

<h3>Title: Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language</h3>
<ul>
<li><strong>Authors: </strong>Jaskaranjeet Singh, Rakesh Thakur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01918">https://arxiv.org/abs/2508.01918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01918">https://arxiv.org/pdf/2508.01918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01918]] Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language(https://arxiv.org/abs/2508.01918)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite the rapid advancement of large language models (LLMs), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs. As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses sparse (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending LLM capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP</li>
</ul>

<h3>Title: InspectVLM: Unified in Theory, Unreliable in Practice</h3>
<ul>
<li><strong>Authors: </strong>Conor Wallace, Isaac Corley, Jonathan Lwowski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01921">https://arxiv.org/abs/2508.01921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01921">https://arxiv.org/pdf/2508.01921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01921]] InspectVLM: Unified in Theory, Unreliable in Practice(https://arxiv.org/abs/2508.01921)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unified vision-language models (VLMs) promise to streamline computer vision pipelines by reframing multiple visual tasks such as classification, detection, and keypoint localization within a single language-driven interface. This architecture is particularly appealing in industrial inspection, where managing disjoint task-specific models introduces complexity, inefficiency, and maintenance overhead. In this paper, we critically evaluate the viability of this unified paradigm using InspectVLM, a Florence-2-based VLM trained on InspectMM, our new large-scale multimodal, multitask inspection dataset. While InspectVLM performs competitively on image-level classification and structured keypoint tasks, we find that it fails to match traditional ResNet-based models in core inspection metrics. Notably, the model exhibits brittle behavior under low prompt variability, produces degenerate outputs for fine-grained object detection, and frequently defaults to memorized language responses regardless of visual input. Our findings suggest that while language-driven unification offers conceptual elegance, current VLMs lack the visual grounding and robustness necessary for deployment in precision critical industrial inspections.</li>
</ul>

<h3>Title: From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation</h3>
<ul>
<li><strong>Authors: </strong>Zhuomin Chen, Jingchao Ni, Hojat Allah Salehi, Xu Zheng, Dongsheng Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01925">https://arxiv.org/abs/2508.01925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01925">https://arxiv.org/pdf/2508.01925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01925]] From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation(https://arxiv.org/abs/2508.01925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved remarkable performance in a wide range of graph-related learning tasks. However, explaining their predictions remains a challenging problem, especially due to the mismatch between the graphs used during training and those encountered during explanation. Most existing methods optimize soft edge masks on weighted graphs to highlight important substructures, but these graphs differ from the unweighted graphs on which GNNs are trained. This distributional shift leads to unreliable gradients and degraded explanation quality, especially when generating small, sparse subgraphs. To address this issue, we propose a novel iterative explanation framework which improves explanation robustness by aligning the model's training data distribution with the weighted graph distribution appeared during explanation. Our method alternates between two phases: explanation subgraph identification and model adaptation. It begins with a relatively large explanation subgraph where soft mask optimization is reliable. Based on this subgraph, we assign importance-aware edge weights to explanatory and non-explanatory edges, and retrain the GNN on these weighted graphs. This process is repeated with progressively smaller subgraphs, forming an iterative refinement procedure. We evaluate our method on multiple benchmark datasets using different GNN backbones and explanation methods. Experimental results show that our method consistently improves explanation quality and can be flexibly integrated with different architectures.</li>
</ul>

<h3>Title: IAUNet: Instance-Aware U-Net</h3>
<ul>
<li><strong>Authors: </strong>Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01928">https://arxiv.org/abs/2508.01928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01928">https://arxiv.org/pdf/2508.01928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01928]] IAUNet: Instance-Aware U-Net(https://arxiv.org/abs/2508.01928)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at this https URL</li>
</ul>

<h3>Title: Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Tom S. Juzek, Zina B. Ward</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01930">https://arxiv.org/abs/2508.01930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01930">https://arxiv.org/pdf/2508.01930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01930]] Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback(https://arxiv.org/abs/2508.01930)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to overuse certain terms like "delve" and "intricate." The exact reasons for these lexical choices, however, have been unclear. Using Meta's Llama model, this study investigates the contribution of Learning from Human Feedback (LHF), under which we subsume Reinforcement Learning from Human Feedback and Direct Preference Optimization. We present a straightforward procedure for detecting the lexical preferences of LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to lexical overuse by experimentally emulating the LHF procedure and demonstrating that participants systematically prefer text variants that include certain words. This lexical overuse can be seen as a sort of misalignment, though our study highlights the potential divergence between the lexical expectations of different populations -- namely LHF workers versus LLM users. Our work contributes to the growing body of research on explainable artificial intelligence and emphasizes the importance of both data and procedural transparency in alignment research.</li>
</ul>

<h3>Title: Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense</h3>
<ul>
<li><strong>Authors: </strong>Kyle Stein, Andrew A. Mahyari, Guillermo Francia III, Eman El-Sheikh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01932">https://arxiv.org/abs/2508.01932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01932">https://arxiv.org/pdf/2508.01932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01932]] Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense(https://arxiv.org/abs/2508.01932)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) and generative AI (GenAI) are increasingly vulnerable to backdoor attacks, where adversaries embed triggers into inputs to cause models to misclassify or misinterpret target labels. Beyond traditional single-trigger scenarios, attackers may inject multiple triggers across various object classes, forming unseen backdoor-object configurations that evade standard detection pipelines. In this paper, we introduce DBOM (Disentangled Backdoor-Object Modeling), a proactive framework that leverages structured disentanglement to identify and neutralize both seen and unseen backdoor threats at the dataset level. Specifically, DBOM factorizes input image representations by modeling triggers and objects as independent primitives in the embedding space through the use of Vision-Language Models (VLMs). By leveraging the frozen, pre-trained encoders of VLMs, our approach decomposes the latent representations into distinct components through a learnable visual prompt repository and prompt prefix tuning, ensuring that the relationships between triggers and objects are explicitly captured. To separate trigger and object representations in the visual prompt repository, we introduce the trigger-object separation and diversity losses that aids in disentangling trigger and object visual features. Next, by aligning image features with feature decomposition and fusion, as well as learned contextual prompt tokens in a shared multimodal space, DBOM enables zero-shot generalization to novel trigger-object pairings that were unseen during training, thereby offering deeper insights into adversarial attack patterns. Experimental results on CIFAR-10 and GTSRB demonstrate that DBOM robustly detects poisoned images prior to downstream training, significantly enhancing the security of DNN training pipelines.</li>
</ul>

<h3>Title: CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yaxuan Li, Yewei Huang, Bijay Gaudel, Hamidreza Jafarnejadsani, Brendan Englot</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01936">https://arxiv.org/abs/2508.01936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01936">https://arxiv.org/pdf/2508.01936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01936]] CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes(https://arxiv.org/abs/2508.01936)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We present a novel multi-altitude camera pose estimation system, addressing the challenges of robust and accurate localization across varied altitudes when only considering sparse image input. The system effectively handles diverse environmental conditions and viewpoint variations by integrating the cross-view transformer, deep features, and structure-from-motion into a unified framework. To benchmark our method and foster further research, we introduce two newly collected datasets specifically tailored for multi-altitude camera pose estimation; datasets of this nature remain rare in the current literature. The proposed framework has been validated through extensive comparative analyses on these datasets, demonstrating that our system achieves superior performance in both accuracy and robustness for multi-altitude sparse pose estimation tasks compared to existing solutions, making it well suited for real-world robotic applications such as aerial navigation, search and rescue, and automated inspection.</li>
</ul>

<h3>Title: Navigating High Dimensional Concept Space with Metalearning</h3>
<ul>
<li><strong>Authors: </strong>Max Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01948">https://arxiv.org/abs/2508.01948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01948">https://arxiv.org/pdf/2508.01948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01948]] Navigating High Dimensional Concept Space with Metalearning(https://arxiv.org/abs/2508.01948)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Rapidly learning abstract concepts from limited examples is a hallmark of human intelligence. This work investigates whether gradient-based meta-learning can equip neural networks with inductive biases for efficient few-shot acquisition of discrete concepts. We compare meta-learning methods against a supervised learning baseline on Boolean tasks generated by a probabilistic context-free grammar (PCFG). By systematically varying concept dimensionality (number of features) and compositionality (depth of grammar recursion), we identify regimes in which meta-learning robustly improves few-shot concept learning. We find improved performance and sample efficiency by training a multilayer perceptron (MLP) across concept spaces increasing in dimensional and compositional complexity. We are able to show that meta-learners are much better able to handle compositional complexity than featural complexity and establish an empirical analysis demonstrating how featural complexity shapes 'concept basins' of the loss landscape, allowing curvature-aware optimization to be more effective than first order methods. We see that we can robustly increase generalization on complex concepts by increasing the number of adaptation steps in meta-SGD, encouraging exploration of rougher loss basins. Overall, this work highlights the intricacies of learning compositional versus featural complexity in high dimensional concept spaces and provides a road to understanding the role of 2nd order methods and extended gradient adaptation in few-shot concept learning.</li>
</ul>

<h3>Title: Kronecker-LoRA: hybrid Kronecker-LoRA adapters for scalable, sustainable fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yixin Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01961">https://arxiv.org/abs/2508.01961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01961">https://arxiv.org/pdf/2508.01961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01961]] Kronecker-LoRA: hybrid Kronecker-LoRA adapters for scalable, sustainable fine-tuning(https://arxiv.org/abs/2508.01961)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning massive pre-trained language models across many tasks demands adapters that are both parameter-efficient and highly expressive. We introduce \textbf{Kron-LoRA}, a two-stage adapter that first factorizes each frozen linear update as a Kronecker product \[ \Delta W = A \otimes B \] and then compresses \[ B \in \mathbb{R}^{d_{B2}\times d_{B1}} \] via an \(r\)-rank LoRA decomposition \(B \approx B_{1}B_{2}\). By leveraging \[ \mathrm{rank}(A \otimes B) \;=\; \mathrm{rank}(A)\,\mathrm{rank}(B), \] Kron-LoRA retains the expressivity of the update while using up to $4\!\times\!$ fewer parameters than a standard rank-8 LoRA adapter. Its compact adapter matrices also quantize to 8- or 4-bit with less accuracy degradation than LoRA, enabling further memory and storage savings for on-device deployment. We benchmark on DistilBERT and Mistral-7B across five tasks (PIQA, HellaSwag, WinoGrande, ARC-Easy, ARC-Challenge) over multiple epochs of adapter-only tuning: on DistilBERT, an 840 K-parameter Kron-LoRA matches LoRA-16's performance, and on Mistral-7B, a 5.7 M-parameter Kron-LoRA rivals LoRA-8 with modest memory savings and only a 3-8\% speed overhead. In sequential fine-tuning from ARC-Challenge to ARC-Easy, Kron-LoRA retains 55.18\% accuracy versus 53.17\% for LoRA-8-despite using only one-quarter of the adapter parameters-underscoring its competitive cross-task transfer performance. By uniting Kronecker structure, low-rank compression, quantization-friendliness, and by providing transparent trade-off analysis, Kron-LoRA offers a scalable, sustainable, and continual-learning-ready solution for multi-task adaptation of large language models.</li>
</ul>

<h3>Title: Accelerating LLM Reasoning via Early Rejection with Partial Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Seyyed Saeid Cheshmi, Azal Ahmad Khan, Xinran Wang, Zirui Liu, Ali Anwar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01969">https://arxiv.org/abs/2508.01969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01969">https://arxiv.org/pdf/2508.01969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01969]] Accelerating LLM Reasoning via Early Rejection with Partial Reward Modeling(https://arxiv.org/abs/2508.01969)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly relied upon for solving complex reasoning tasks in domains such as mathematics, logic, and multi-step question answering. A growing line of work seeks to improve reasoning quality by scaling inference time compute particularly through Process Reward Models (PRMs), used to reward the reasoning at intermediate steps. While effective, these methods introduce substantial computational overhead, especially when generating large numbers of solutions in parallel. In this paper, we investigate whether PRMs can be used mid-generation to provide early signals that enable the rejection of suboptimal candidates before full generation of step is complete. We introduce the hypothesis that PRMs are also Partial Reward Models, meaning that the scores they assign to partially completed reasoning step are predictive of final output quality. This allows for principled early rejection based on intermediate token-level signals. We support this hypothesis both theoretically, by proving that the risk of discarding optimal beams decreases exponentially with generation length and empirically, by demonstrating a strong correlation between partial and final rewards across multiple reward models. On math reasoning benchmarks, our method achieves up to 1.4$\times$-9$\times$ reduction in inference FLOPs without degrading final performance. These results suggest that early rejection is a powerful mechanism for improving the compute-efficiency of reasoning in LLMs.</li>
</ul>

<h3>Title: Improving Hospital Risk Prediction with Knowledge-Augmented Multimodal EHR Modeling</h3>
<ul>
<li><strong>Authors: </strong>Rituparna Datta, Jiaming Cui, Zihan Guan, Rupesh Silwal, Joshua C Eby, Gregory Madden, Anil Vullikanti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01970">https://arxiv.org/abs/2508.01970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01970">https://arxiv.org/pdf/2508.01970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01970]] Improving Hospital Risk Prediction with Knowledge-Augmented Multimodal EHR Modeling(https://arxiv.org/abs/2508.01970)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate prediction of clinical outcomes using Electronic Health Records (EHRs) is critical for early intervention, efficient resource allocation, and improved patient care. EHRs contain multimodal data, including both structured data and unstructured clinical notes that provide rich, context-specific information. In this work, we introduce a unified framework that seamlessly integrates these diverse modalities, leveraging all relevant available information through a two-stage architecture for clinical risk prediction. In the first stage, a fine-tuned Large Language Model (LLM) extracts crucial, task-relevant information from clinical notes, which is enhanced by graph-based retrieval of external domain knowledge from sources such as a medical corpus like PubMed, grounding the LLM's understanding. The second stage combines both unstructured representations and features derived from the structured data to generate the final predictions. This approach supports a wide range of clinical tasks. Here, we demonstrate its effectiveness on 30-day readmission and in-hospital mortality prediction. Experimental results show that our framework achieves strong performance, with AUC scores of $0.84$ and $0.92$, respectively, despite these tasks involving severely imbalanced datasets, with positive rates ranging from approximately $4\%$ to $13\%$. Moreover, it outperforms all existing baselines and clinical practices, including established risk scoring systems. To the best of our knowledge, this is one of the first frameworks for healthcare prediction which enhances the power of an LLM-based graph-guided knowledge retrieval method by combining it with structured data for improved clinical outcome prediction.</li>
</ul>

<h3>Title: Diffusion models for inverse problems</h3>
<ul>
<li><strong>Authors: </strong>Hyungjin Chung, Jeongsol Kim, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01975">https://arxiv.org/abs/2508.01975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01975">https://arxiv.org/pdf/2508.01975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01975]] Diffusion models for inverse problems(https://arxiv.org/abs/2508.01975)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Using diffusion priors to solve inverse problems in imaging have significantly matured over the years. In this chapter, we review the various different approaches that were proposed over the years. We categorize the approaches into the more classic explicit approximation approaches and others, which include variational inference, sequential monte carlo, and decoupled data consistency. We cover the extension to more challenging situations, including blind cases, high-dimensional data, and problems under data scarcity and distribution mismatch. More recent approaches that aim to leverage multimodal information through texts are covered. Through this chapter, we aim to (i) distill the common mathematical threads that connect these algorithms, (ii) systematically contrast their assumptions and performance trade-offs across representative inverse problems, and (iii) spotlight the open theoretical and practical challenges by clarifying the landscape of diffusion model based inverse problem solvers.</li>
</ul>

<h3>Title: TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fan Gao, Cheng Huang, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01977">https://arxiv.org/abs/2508.01977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01977">https://arxiv.org/pdf/2508.01977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01977]] TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models(https://arxiv.org/abs/2508.01977)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: this https URL.</li>
</ul>

<h3>Title: Generative AI-Empowered Secure Communications in Space-Air-Ground Integrated Networks: A Survey and Tutorial</h3>
<ul>
<li><strong>Authors: </strong>Chenbo Hu, Ruichen Zhang, Bo Li, Xu Jiang, Nan Zhao, Marco Di Renzo, Dusit Niyato, Arumugam Nallanathan, George K. Karagiannidis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01983">https://arxiv.org/abs/2508.01983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01983">https://arxiv.org/pdf/2508.01983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01983]] Generative AI-Empowered Secure Communications in Space-Air-Ground Integrated Networks: A Survey and Tutorial(https://arxiv.org/abs/2508.01983)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, robust, generative</a></li>
<li><strong>Abstract: </strong>Space-air-ground integrated networks (SAGINs) face unprecedented security challenges due to their inherent characteristics, such as multidimensional heterogeneity and dynamic topologies. These characteristics fundamentally undermine conventional security methods and traditional artificial intelligence (AI)-driven solutions. Generative AI (GAI) is a transformative approach that can safeguard SAGIN security by synthesizing data, understanding semantics, and making autonomous decisions. This survey fills existing review gaps by examining GAI-empowered secure communications across SAGINs. First, we introduce secured SAGINs and highlight GAI's advantages over traditional AI for security defenses. Then, we explain how GAI mitigates failures of authenticity, breaches of confidentiality, tampering of integrity, and disruptions of availability across the physical, data link, and network layers of SAGINs. Three step-by-step tutorials discuss how to apply GAI to solve specific problems using concrete methods, emphasizing its generative paradigm beyond traditional AI. Finally, we outline open issues and future research directions, including lightweight deployment, adversarial robustness, and cross-domain governance, to provide major insights into GAI's role in shaping next-generation SAGIN security.</li>
</ul>

<h3>Title: IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A</h3>
<ul>
<li><strong>Authors: </strong>Chen Li, Chinthani Sugandhika, Yeo Keat Ee, Eric Peh, Hao Zhang, Hong Yang, Deepu Rajan, Basura Fernando</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01984">https://arxiv.org/abs/2508.01984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01984">https://arxiv.org/pdf/2508.01984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01984]] IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&A(https://arxiv.org/abs/2508.01984)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Existing human motion Q\&A methods rely on explicit program execution, where the requirement for manually defined functional modules may limit the scalability and adaptability. To overcome this, we propose an implicit program-guided motion reasoning (IMoRe) framework that unifies reasoning across multiple query types without manually designed modules. Unlike existing implicit reasoning approaches that infer reasoning operations from question words, our model directly conditions on structured program functions, ensuring a more precise execution of reasoning steps. Additionally, we introduce a program-guided reading mechanism, which dynamically selects multi-level motion representations from a pretrained motion Vision Transformer (ViT), capturing both high-level semantics and fine-grained motion cues. The reasoning module iteratively refines memory representations, leveraging structured program functions to extract relevant information for different query types. Our model achieves state-of-the-art performance on Babel-QA and generalizes to a newly constructed motion Q\&A dataset based on HuMMan, demonstrating its adaptability across different motion reasoning datasets. Code and dataset are available at: this https URL.</li>
</ul>

<h3>Title: Controllable and Stealthy Shilling Attacks via Dispersive Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Shutong Qiao, Wei Yuan, Junliang Yu, Tong Chen, Quoc Viet Hung Nguyen, Hongzhi Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01987">https://arxiv.org/abs/2508.01987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01987">https://arxiv.org/pdf/2508.01987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01987]] Controllable and Stealthy Shilling Attacks via Dispersive Latent Diffusion(https://arxiv.org/abs/2508.01987)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal, diffusion</a></li>
<li><strong>Abstract: </strong>Recommender systems (RSs) are now fundamental to various online platforms, but their dependence on user-contributed data leaves them vulnerable to shilling attacks that can manipulate item rankings by injecting fake users. Although widely studied, most existing attack models fail to meet two critical objectives simultaneously: achieving strong adversarial promotion of target items while maintaining realistic behavior to evade detection. As a result, the true severity of shilling threats that manage to reconcile the two objectives remains underappreciated. To expose this overlooked vulnerability, we present DLDA, a diffusion-based attack framework that can generate highly effective yet indistinguishable fake users by enabling fine-grained control over target promotion. Specifically, DLDA operates in a pre-aligned collaborative embedding space, where it employs a conditional latent diffusion process to iteratively synthesize fake user profiles with precise target item control. To evade detection, DLDA introduces a dispersive regularization mechanism that promotes variability and realism in generated behavioral patterns. Extensive experiments on three real-world datasets and five popular RS models demonstrate that, compared to prior attacks, DLDA consistently achieves stronger item promotion while remaining harder to detect. These results highlight that modern RSs are more vulnerable than previously recognized, underscoring the urgent need for more robust defenses.</li>
</ul>

<h3>Title: Toward Efficient Spiking Transformers: Synapse Pruning Meets Synergistic Learning-Based Compensation</h3>
<ul>
<li><strong>Authors: </strong>Hongze Sun, Wuque Cai, Duo Chen, Shifeng Mao, Jiayi He, Zhenxing Wang, Dezhong Yao, Daqing Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01992">https://arxiv.org/abs/2508.01992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01992">https://arxiv.org/pdf/2508.01992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01992]] Toward Efficient Spiking Transformers: Synapse Pruning Meets Synergistic Learning-Based Compensation(https://arxiv.org/abs/2508.01992)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As a foundational architecture of artificial intelligence models, Transformer has been recently adapted to spiking neural networks with promising performance across various tasks. However, existing spiking Transformer (ST)-based models require a substantial number of parameters and incur high computational costs, thus limiting their deployment in resource-constrained environments. To address these challenges, we propose combining synapse pruning with a synergistic learning-based compensation strategy to derive lightweight ST-based models. Specifically, two types of tailored pruning strategies are introduced to reduce redundancy in the weight matrices of ST blocks: an unstructured $\mathrm{L_{1}P}$ method to induce sparse representations, and a structured DSP method to induce low-rank representations. In addition, we propose an enhanced spiking neuron model, termed the synergistic leaky integrate-and-fire (sLIF) neuron, to effectively compensate for model pruning through synergistic learning between synaptic and intrinsic plasticity mechanisms. Extensive experiments on benchmark datasets demonstrate that the proposed methods significantly reduce model size and computational overhead while maintaining competitive performance. These results validate the effectiveness of the proposed pruning and compensation strategies in constructing efficient and high-performing ST-based models.</li>
</ul>

<h3>Title: Deeply Dual Supervised learning for melanoma recognition</h3>
<ul>
<li><strong>Authors: </strong>Rujosh Polma, Krishnan Menon Iyer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01994">https://arxiv.org/abs/2508.01994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01994">https://arxiv.org/pdf/2508.01994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01994]] Deeply Dual Supervised learning for melanoma recognition(https://arxiv.org/abs/2508.01994)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>As the application of deep learning in dermatology continues to grow, the recognition of melanoma has garnered significant attention, demonstrating potential for improving diagnostic accuracy. Despite advancements in image classification techniques, existing models still face challenges in identifying subtle visual cues that differentiate melanoma from benign lesions. This paper presents a novel Deeply Dual Supervised Learning framework that integrates local and global feature extraction to enhance melanoma recognition. By employing a dual-pathway structure, the model focuses on both fine-grained local features and broader contextual information, ensuring a comprehensive understanding of the image content. The framework utilizes a dual attention mechanism that dynamically emphasizes critical features, thereby reducing the risk of overlooking subtle characteristics of melanoma. Additionally, we introduce a multi-scale feature aggregation strategy to ensure robust performance across varying image resolutions. Extensive experiments on benchmark datasets demonstrate that our framework significantly outperforms state-of-the-art methods in melanoma detection, achieving higher accuracy and better resilience against false positives. This work lays the foundation for future research in automated skin cancer recognition and highlights the effectiveness of dual supervised learning in medical image analysis.</li>
</ul>

<h3>Title: GPU in the Blind Spot: Overlooked Security Risks in Transportation</h3>
<ul>
<li><strong>Authors: </strong>Sefatun-Noor Puspa, Mashrur Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01995">https://arxiv.org/abs/2508.01995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01995">https://arxiv.org/pdf/2508.01995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01995]] GPU in the Blind Spot: Overlooked Security Risks in Transportation(https://arxiv.org/abs/2508.01995)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Graphics processing units (GPUs) are becoming an essential part of the intelligent transportation system (ITS) for enabling video-based and artificial intelligence (AI) based applications. GPUs provide high-throughput and energy-efficient computing for tasks like sensor fusion and roadside video analytics. However, these GPUs are one of the most unmonitored components in terms of security. This makes them vulnerable to cyber and hardware attacks, including unauthorized crypto mining. This paper highlights GPU security as a critical blind spot in transportation cybersecurity. To support this concern, it also presents a case study showing the impact of stealthy unauthorized crypto miners on critical AI workloads, along with a detection strategy. We used a YOLOv8-based video processing pipeline running on an RTX 2060 GPU for the case study. A multi-streaming application was executed while a T-Rex crypto miner ran in the background. We monitored how the miner degraded GPU performance by reducing the frame rate and increasing power consumption, which could be a serious concern for GPUs operating in autonomous vehicles or battery-powered edge devices. We observed measurable impacts using GPU telemetry (nvidia-smi) and Nsight Compute profiling, where frame rate dropped by 50 percent, and power usage increased by up to 90%. To detect, we trained lightweight classifiers using extracted telemetry features. All models achieved high accuracy, precision, recall, and F1-score. This paper raises urgent awareness about GPU observability gaps in ITS and offers a replicable framework for detecting GPU misuse through on-device telemetry.</li>
</ul>

<h3>Title: DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Hammad Atta, Muhammad Zeeshan Baig, Yasir Mehmood, Nadeem Shahzad, Ken Huang, Muhammad Aziz Ul Haq, Muhammad Awais, Kamal Ahmed, Anthony Green</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01997">https://arxiv.org/abs/2508.01997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01997">https://arxiv.org/pdf/2508.01997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01997]] DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems(https://arxiv.org/abs/2508.01997)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, robust, biometric, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement and widespread adoption of generative artificial intelligence (AI) pose significant threats to the integrity of personal identity, including digital cloning, sophisticated impersonation, and the unauthorized monetization of identity-related data. Mitigating these risks necessitates the development of robust AI-generated content detection systems, enhanced legal frameworks, and ethical guidelines. This paper introduces the Digital Identity Rights Framework (DIRF), a structured security and governance model designed to protect behavioral, biometric, and personality-based digital likeness attributes to address this critical need. Structured across nine domains and 63 controls, DIRF integrates legal, technical, and hybrid enforcement mechanisms to secure digital identity consent, traceability, and monetization. We present the architectural foundations, enforcement strategies, and key use cases supporting the need for a unified framework. This work aims to inform platform builders, legal entities, and regulators about the essential controls needed to enforce identity rights in AI-driven systems.</li>
</ul>

<h3>Title: Prompting Large Language Models to Detect Dementia Family Caregivers</h3>
<ul>
<li><strong>Authors: </strong>Md Badsha Biswas, Özlem Uzuner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.01999">https://arxiv.org/abs/2508.01999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.01999">https://arxiv.org/pdf/2508.01999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.01999]] Prompting Large Language Models to Detect Dementia Family Caregivers(https://arxiv.org/abs/2508.01999)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Social media, such as Twitter, provides opportunities for caregivers of dementia patients to share their experiences and seek support for a variety of reasons. Availability of this information online also paves the way for the development of internet-based interventions in their support. However, for this purpose, tweets written by caregivers of dementia patients must first be identified. This paper demonstrates our system for the SMM4H 2025 shared task 3, which focuses on detecting tweets posted by individuals who have a family member with dementia. The task is outlined as a binary classification problem, differentiating between tweets that mention dementia in the context of a family member and those that do not. Our solution to this problem explores large language models (LLMs) with various prompting methods. Our results show that a simple zero-shot prompt on a fine-tuned model yielded the best results. Our final system achieved a macro F1-score of 0.95 on the validation set and the test set. Our full code is available on GitHub.</li>
</ul>

<h3>Title: Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yu Lei, Jiayang Zhao, Yilei Zhao, Zhaoqi Zhang, Linyou Cai, Qianlong Xie, Xingxing Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02002">https://arxiv.org/abs/2508.02002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02002">https://arxiv.org/pdf/2508.02002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02002]] Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization(https://arxiv.org/abs/2508.02002)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Modern auto-bidding systems are required to balance overall performance with diverse advertiser goals and real-world constraints, reflecting the dynamic and evolving needs of the industry. Recent advances in conditional generative models, such as transformers and diffusers, have enabled direct trajectory generation tailored to advertiser preferences, offering a promising alternative to traditional Markov Decision Process-based methods. However, these generative methods face significant challenges, such as the distribution shift between offline and online environments, limited exploration of the action space, and the necessity to meet constraints like marginal Cost-per-Mille (CPM) and Return on Investment (ROI). To tackle these challenges, we propose GRAD (Generative Reward-driven Ad-bidding with Mixture-of-Experts), a scalable foundation model for auto-bidding that combines an Action-Mixture-of-Experts module for diverse bidding action exploration with the Value Estimator of Causal Transformer for constraint-aware optimization. Extensive offline and online experiments demonstrate that GRAD significantly enhances platform revenue, highlighting its effectiveness in addressing the evolving and diverse requirements of modern advertisers. Furthermore, GRAD has been implemented in multiple marketing scenarios at Meituan, one of the world's largest online food delivery platforms, leading to a 2.18% increase in Gross Merchandise Value (GMV) and 10.68% increase in ROI.</li>
</ul>

<h3>Title: Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention</h3>
<ul>
<li><strong>Authors: </strong>Kyungmin Jo, Jooyeol Yun, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02004">https://arxiv.org/abs/2508.02004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02004">https://arxiv.org/pdf/2508.02004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02004]] Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention(https://arxiv.org/abs/2508.02004)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While large-scale text-to-image diffusion models enable the generation of high-quality, diverse images from text prompts, these prompts struggle to capture intricate details, such as textures, preventing the user intent from being reflected. This limitation has led to efforts to generate images conditioned on user-provided images, referred to as image prompts. Recent work modifies the self-attention mechanism to impose image conditions in generated images by replacing or concatenating the keys and values from the image prompt. This enables the self-attention layer to work like a cross-attention layer, generally used to incorporate text prompts. In this paper, we identify two common issues in existing methods of modifying self-attention to generate images that reflect the details of image prompts. First, existing approaches neglect the importance of image prompts in classifier-free guidance. Specifically, current methods use image prompts as both desired and undesired conditions in classifier-free guidance, causing conflicting signals. To resolve this, we propose conflict-free guidance by using image prompts only as desired conditions, ensuring that the generated image faithfully reflects the image prompt. In addition, we observe that the two most common self-attention modifications involve a trade-off between the realism of the generated image and alignment with the image prompt. Specifically, selecting more keys and values from the image prompt improves alignment, while selecting more from the generated image enhances realism. To balance both, we propose an new self-attention modification method, Stratified Attention to jointly use keys and values from both images rather than selecting between them. Through extensive experiments across three image generation tasks, we show that the proposed method outperforms existing image-prompting models in faithfully reflecting the image prompt.</li>
</ul>

<h3>Title: A Comprehensive Analysis of Evolving Permission Usage in Android Apps: Trends, Threats, and Ecosystem Insights</h3>
<ul>
<li><strong>Authors: </strong>Ali Alkinoon, Trung Cuong Dang, Ahod Alghuried, Abdulaziz Alghamdi, Soohyeon Choi, Manar Mohaisen, An Wang, Saeed Salem, David Mohaisen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02008">https://arxiv.org/abs/2508.02008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02008">https://arxiv.org/pdf/2508.02008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02008]] A Comprehensive Analysis of Evolving Permission Usage in Android Apps: Trends, Threats, and Ecosystem Insights(https://arxiv.org/abs/2508.02008)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The proper use of Android app permissions is crucial to the success and security of these apps. Users must agree to permission requests when installing or running their apps. Despite official Android platform documentation on proper permission usage, there are still many cases of permission abuse. This study provides a comprehensive analysis of the Android permission landscape, highlighting trends and patterns in permission requests across various applications from the Google Play Store. By distinguishing between benign and malicious applications, we uncover developers' evolving strategies, with malicious apps increasingly requesting fewer permissions to evade detection, while benign apps request more to enhance functionality. In addition to examining permission trends across years and app features such as advertisements, in-app purchases, content ratings, and app sizes, we leverage association rule mining using the FP-Growth algorithm. This allows us to uncover frequent permission combinations across the entire dataset, specific years, and 16 app genres. The analysis reveals significant differences in permission usage patterns, providing a deeper understanding of co-occurring permissions and their implications for user privacy and app functionality. By categorizing permissions into high-level semantic groups and examining their application across distinct app categories, this study offers a structured approach to analyzing the dynamics within the Android ecosystem. The findings emphasize the importance of continuous monitoring, user education, and regulatory oversight to address permission misuse effectively.</li>
</ul>

<h3>Title: SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wanqi Yang, Yanda Li, Yunchao Wei, Meng Fang, Ling Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02018">https://arxiv.org/abs/2508.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02018">https://arxiv.org/pdf/2508.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02018]] SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models(https://arxiv.org/abs/2508.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large audio-language models (LALMs) have achieved near-human performance in sentence-level transcription and emotion recognition. However, existing evaluations focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speech-based scenarios insufficiently examined. To address this gap, we introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. SpeechR evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats. The multiple-choice version measures answer selection accuracy. The generative version assesses the coherence and logical consistency of reasoning chains. The acoustic-feature version investigates whether variations in stress and emotion affect reasoning performance. Evaluations on eleven state-of-the-art LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities. SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks.</li>
</ul>

<h3>Title: Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure</h3>
<ul>
<li><strong>Authors: </strong>Ziling Wang, Shuya Yang, Jialin Lu, Ka-Ho Chow</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02034">https://arxiv.org/abs/2508.02034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02034">https://arxiv.org/pdf/2508.02034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02034]] Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure(https://arxiv.org/abs/2508.02034)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Face recognition (FR) technologies are increasingly used to power large-scale image retrieval systems, raising serious privacy concerns. Services like Clearview AI and PimEyes allow anyone to upload a facial photo and retrieve a large amount of online content associated with that person. This not only enables identity inference but also exposes their digital footprint, such as social media activity, private photos, and news reports, often without their consent. In response to this emerging threat, we propose Protego, a user-centric privacy protection method that safeguards facial images from such retrieval-based privacy intrusions. Protego encapsulates a user's 3D facial signatures into a pose-invariant 2D representation, which is dynamically deformed into a natural-looking 3D mask tailored to the pose and expression of any facial image of the user, and applied prior to online sharing. Motivated by a critical limitation of existing methods, Protego amplifies the sensitivity of FR models so that protected images cannot be matched even among themselves. Experiments show that Protego significantly reduces retrieval accuracy across a wide range of black-box FR models and performs at least 2x better than existing methods. It also offers unprecedented visual coherence, particularly in video settings where consistency and natural appearance are essential. Overall, Protego contributes to the fight against the misuse of FR for mass surveillance and unsolicited identity tracing.</li>
</ul>

<h3>Title: PhishParrot: LLM-Driven Adaptive Crawling to Unveil Cloaked Phishing Sites</h3>
<ul>
<li><strong>Authors: </strong>Hiroki Nakano, Takashi Koide, Daiki Chiba</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02035">https://arxiv.org/abs/2508.02035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02035">https://arxiv.org/pdf/2508.02035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02035]] PhishParrot: LLM-Driven Adaptive Crawling to Unveil Cloaked Phishing Sites(https://arxiv.org/abs/2508.02035)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Phishing attacks continue to evolve, with cloaking techniques posing a significant challenge to detection efforts. Cloaking allows attackers to display phishing sites only to specific users while presenting legitimate pages to security crawlers, rendering traditional detection systems ineffective. This research proposes PhishParrot, a novel crawling environment optimization system designed to counter cloaking techniques. PhishParrot leverages the contextual analysis capabilities of Large Language Models (LLMs) to identify potential patterns in crawling information, enabling the construction of optimal user profiles capable of bypassing cloaking mechanisms. The system accumulates information on phishing sites collected from diverse environments. It then adapts browser settings and network configurations to match the attacker's target user conditions based on information extracted from similar cases. A 21-day evaluation showed that PhishParrot improved detection accuracy by up to 33.8% over standard analysis systems, yielding 91 distinct crawling environments for diverse conditions targeted by attackers. The findings confirm that the combination of similar-case extraction and LLM-based context analysis is an effective approach for detecting cloaked phishing attacks.</li>
</ul>

<h3>Title: Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time</h3>
<ul>
<li><strong>Authors: </strong>Huihan Li, You Chen, Siyuan Wang, Yixin He, Ninareh Mehrabi, Rahul Gupta, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02037">https://arxiv.org/abs/2508.02037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02037">https://arxiv.org/pdf/2508.02037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02037]] Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time(https://arxiv.org/abs/2508.02037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.</li>
</ul>

<h3>Title: Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Sijia Wang, Ricardo Henao</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02039">https://arxiv.org/abs/2508.02039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02039">https://arxiv.org/pdf/2508.02039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02039]] Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning(https://arxiv.org/abs/2508.02039)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, data-free</a></li>
<li><strong>Abstract: </strong>Increasing concerns for data privacy and other difficulties associated with retrieving source data for model training have created the need for source-free transfer learning, in which one only has access to pre-trained models instead of data from the original source domains. This setting introduces many challenges, as many existing transfer learning methods typically rely on access to source data, which limits their direct applicability to scenarios where source data is unavailable. Further, practical concerns make it more difficult, for instance efficiently selecting models for transfer without information on source data, and transferring without full access to the source models. So motivated, we propose a model recycling framework for parameter-efficient training of models that identifies subsets of related source models to reuse in both white-box and black-box settings. Consequently, our framework makes it possible for Model as a Service (MaaS) providers to build libraries of efficient pre-trained models, thus creating an opportunity for multi-source data-free supervised transfer learning.</li>
</ul>

<h3>Title: Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hui Xie, Haiqin Hu, Lijuan Ding, Qing Li, Yue Sun, Tao Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02043">https://arxiv.org/abs/2508.02043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02043">https://arxiv.org/pdf/2508.02043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02043]] Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction(https://arxiv.org/abs/2508.02043)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Radiotherapy treatment planning often relies on time-consuming, trial-and-error adjustments that heavily depend on the expertise of specialists, while existing deep learning methods face limitations in generalization, prediction accuracy, and clinical applicability. To tackle these challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints Conditional Diffusion Model for end-to-end multi-tumor dose prediction. The model employs LightweightVAE3D to compress high-dimensional CT data and integrates multimodal inputs, including target and organ-at-risk (OAR) masks and beam parameters, within a progressive noise addition and denoising framework. It incorporates conditional features via a multi-head attention mechanism and utilizes a composite loss function combining MSE, conditional terms, and KL divergence to ensure both dosimetric accuracy and compliance with clinical constraints. Evaluation on a large-scale public dataset (2,877 cases) and three external institutional cohorts (450 cases in total) demonstrates that ADDiff-Dose significantly outperforms traditional baselines, achieving an MAE of 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE coefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum dose error to within 0.1 Gy. The average plan generation time per case is reduced to 22 seconds. Ablation studies confirm that the structural encoder enhances compliance with clinical dose constraints by 28.5%. To our knowledge, this is the first study to introduce a conditional diffusion model framework for radiotherapy dose prediction, offering a generalizable and efficient solution for automated treatment planning across diverse tumor sites, with the potential to substantially reduce planning time and improve clinical workflow efficiency.</li>
</ul>

<h3>Title: Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Soyeon Kim, Jindong Wang, Xing Xie, Steven Euijong Whang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02045">https://arxiv.org/abs/2508.02045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02045">https://arxiv.org/pdf/2508.02045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02045]] Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models(https://arxiv.org/abs/2508.02045)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Facts evolve over time, making it essential for Large Language Models (LLMs) to handle time-sensitive factual knowledge accurately and reliably. While factual Time-Sensitive Question-Answering (TSQA) tasks have been widely studied, existing benchmarks often rely on manual curation or a small, fixed set of predefined templates, which restricts scalable and comprehensive TSQA evaluation. To address these challenges, we propose TDBench, a new benchmark that systematically constructs TSQA pairs by harnessing temporal databases and database techniques such as temporal SQL and functional dependencies. We also introduce a fine-grained evaluation metric called time accuracy, which assesses the validity of time references in model explanations alongside traditional answer accuracy to enable a more reliable TSQA evaluation. Extensive experiments on contemporary LLMs show how \ours{} enables scalable and comprehensive TSQA evaluation while reducing the reliance on human labor, complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by enabling LLM evaluation on application-specific data and seamless multi-hop question generation. Code and data are publicly available at: this https URL.</li>
</ul>

<h3>Title: ProCut: LLM Prompt Compression via Attribution Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhentao Xu, Fengyi Li, Albert Chen, Xiaofeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02053">https://arxiv.org/abs/2508.02053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02053">https://arxiv.org/pdf/2508.02053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02053]] ProCut: LLM Prompt Compression via Attribution Estimation(https://arxiv.org/abs/2508.02053)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In large-scale industrial LLM systems, prompt templates often expand to thousands of tokens as teams iteratively incorporate sections such as task instructions, few-shot examples, and heuristic rules to enhance robustness and coverage. This expansion leads to bloated prompts that are difficult to maintain and incur significant inference latency and serving costs. To address this, we introduce Prompt Compression via Attribution Estimation (ProCut), a flexible, LLM-agnostic, training-free framework that compresses prompts through attribution analysis. ProCut segments prompt templates into semantically meaningful units, quantifies their impact on task performance, and prunes low-utility components. Through extensive experiments on five public benchmark datasets and real-world industrial prompts, we show that ProCut achieves substantial prompt size reductions (78% fewer tokens in production) while maintaining or even slightly improving task performance (up to 62% better than alternative methods). We further introduce an LLM-driven attribution estimator that reduces compression latency by over 50%, and demonstrate that ProCut integrates seamlessly with existing prompt-optimization frameworks to produce concise, high-performing prompts.</li>
</ul>

<h3>Title: StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Haoxin Yang, Weihong Chen, Xuemiao Xu, Cheng Xu, Peng Xiao, Cuifeng Sun, Shaoyu Huang, Shengfeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02056">https://arxiv.org/abs/2508.02056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02056">https://arxiv.org/pdf/2508.02056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02056]] StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive Diffusion(https://arxiv.org/abs/2508.02056)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Monocular 3D human pose estimation remains a challenging task due to inherent depth ambiguities and occlusions. Compared to traditional methods based on Transformers or Convolutional Neural Networks (CNNs), recent diffusion-based approaches have shown superior performance, leveraging their probabilistic nature and high-fidelity generation capabilities. However, these methods often fail to account for the spatial and temporal correlations across predicted frames, resulting in limited temporal consistency and inferior accuracy in predicted 3D pose sequences. To address these shortcomings, this paper proposes StarPose, an autoregressive diffusion framework that effectively incorporates historical 3D pose predictions and spatial-temporal physical guidance to significantly enhance both the accuracy and temporal coherence of pose predictions. Unlike existing approaches, StarPose models the 2D-to-3D pose mapping as an autoregressive diffusion process. By synergically integrating previously predicted 3D poses with 2D pose inputs via a Historical Pose Integration Module (HPIM), the framework generates rich and informative historical pose embeddings that guide subsequent denoising steps, ensuring temporally consistent predictions. In addition, a fully plug-and-play Spatial-Temporal Physical Guidance (STPG) mechanism is tailored to refine the denoising process in an iterative manner, which further enforces spatial anatomical plausibility and temporal motion dynamics, rendering robust and realistic pose estimates. Extensive experiments on benchmark datasets demonstrate that StarPose outperforms state-of-the-art methods, achieving superior accuracy and temporal consistency in 3D human pose estimation. Code is available at this https URL.</li>
</ul>

<h3>Title: MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs</h3>
<ul>
<li><strong>Authors: </strong>Guojiang Zhao, Sihang Li, Zixiang Lu, Zheng Cheng, Haitao Lin, Lirong Wu, Hanchen Xia, Hengxing Cai, Wentao Guo, Hongshuai Wang, Mingjun Xu, Siyu Zhu, Guolin Ke, Linfeng Zhang, Zhifeng Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02066">https://arxiv.org/abs/2508.02066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02066">https://arxiv.org/pdf/2508.02066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02066]] MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs(https://arxiv.org/abs/2508.02066)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models(LLMs) have demonstrated remarkable performance across various domains, yet their capabilities in molecular reasoning remain insufficiently explored. Current approaches tend to rely heavily on general-purpose prompting, which lacks domain-specific molecular semantics, while those that use fine-tuning strategies often face challenges with interpretability and reasoning depth. To address these issues, we introduce MolReasoner, a two-stage framework designed to transition LLMs from memorization towards chemical reasoning. First, we propose Mol-SFT, which initializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT) samples generated by GPT-4o and verified for chemical accuracy. Subsequently, Mol-RL applies reinforcement learning with specialized reward functions designed explicitly to align chemical structures with linguistic descriptions, thereby enhancing molecular reasoning capabilities. Our approach notably enhances interpretability, improving the model 's molecular understanding and enabling better generalization. Extensive experiments demonstrate that MolReasoner outperforms existing methods, and marking a significant shift from memorization-based outputs to robust chemical reasoning.</li>
</ul>

<h3>Title: YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Manikanta Kotthapalli, Deepika Ravipati, Reshma Bhatia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02067">https://arxiv.org/abs/2508.02067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02067">https://arxiv.org/pdf/2508.02067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02067]] YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges(https://arxiv.org/abs/2508.02067)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Over the past decade, object detection has advanced significantly, with the YOLO (You Only Look Once) family of models transforming the landscape of real-time vision applications through unified, end-to-end detection frameworks. From YOLOv1's pioneering regression-based detection to the latest YOLOv9, each version has systematically enhanced the balance between speed, accuracy, and deployment efficiency through continuous architectural and algorithmic advancements.. Beyond core object detection, modern YOLO architectures have expanded to support tasks such as instance segmentation, pose estimation, object tracking, and domain-specific applications including medical imaging and industrial automation. This paper offers a comprehensive review of the YOLO family, highlighting architectural innovations, performance benchmarks, extended capabilities, and real-world use cases. We critically analyze the evolution of YOLO models and discuss emerging research directions that extend their impact across diverse computer vision domains.</li>
</ul>

<h3>Title: SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Bang Hu, Changze Lv, Mingjie Li, Yunpeng Liu, Xiaoqing Zheng, Fengzhe Zhang, Wei cao, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02069">https://arxiv.org/abs/2508.02069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02069">https://arxiv.org/pdf/2508.02069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02069]] SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration(https://arxiv.org/abs/2508.02069)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Spiking neural networks (SNNs), inspired by the spiking behavior of biological neurons, offer a distinctive approach for capturing the complexities of temporal data. However, their potential for spatial modeling in multivariate time-series forecasting remains largely unexplored. To bridge this gap, we introduce a brand new SNN architecture, which is among the first to seamlessly integrate graph structural learning with spike-based temporal processing for multivariate time-series forecasting. Specifically, we first embed time features and an adaptive matrix, eliminating the need for predefined graph structures. We then further learn sequence features through the Observation (OBS) Block. Building upon this, our Multi-Scale Spike Aggregation (MSSA) hierarchically aggregates neighborhood information through spiking SAGE layers, enabling multi-hop feature extraction while eliminating the need for floating-point operations. Finally, we propose a Dual-Path Spike Fusion (DSF) Block to integrate spatial graph features and temporal dynamics via a spike-gated mechanism, combining LSTM-processed sequences with spiking self-attention outputs, effectively improve the model accuracy of long sequence datasets. Experiments show that our model surpasses the state-of-the-art SNN-based iSpikformer on all datasets and outperforms traditional temporal models at long horizons, thereby establishing a new paradigm for efficient spatial-temporal modeling.</li>
</ul>

<h3>Title: The SMeL Test: A simple benchmark for media literacy in language models</h3>
<ul>
<li><strong>Authors: </strong>Gustaf Ahdritz, Anat Kleiman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02074">https://arxiv.org/abs/2508.02074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02074">https://arxiv.org/pdf/2508.02074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02074]] The SMeL Test: A simple benchmark for media literacy in language models(https://arxiv.org/abs/2508.02074)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently trusts more reliable sources; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it.</li>
</ul>

<h3>Title: AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization</h3>
<ul>
<li><strong>Authors: </strong>Amitava Das, Abhilekh Borah, Vinija Jain, Aman Chadha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02079">https://arxiv.org/abs/2508.02079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02079">https://arxiv.org/pdf/2508.02079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02079]] AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization(https://arxiv.org/abs/2508.02079)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.</li>
</ul>

<h3>Title: S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Luping Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02082">https://arxiv.org/abs/2508.02082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02082">https://arxiv.org/pdf/2508.02082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02082]] S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained Evaluation Framework(https://arxiv.org/abs/2508.02082)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Radiology report generation (RRG) for diagnostic images, such as chest X-rays, plays a pivotal role in both clinical practice and AI. Traditional free-text reports suffer from redundancy and inconsistent language, complicating the extraction of critical clinical details. Structured radiology report generation (S-RRG) offers a promising solution by organizing information into standardized, concise formats. However, existing approaches often rely on classification or visual question answering (VQA) pipelines that require predefined label sets and produce only fragmented outputs. Template-based approaches, which generate reports by replacing keywords within fixed sentence patterns, further compromise expressiveness and often omit clinically important details. In this work, we present a novel approach to S-RRG that includes dataset construction, model training, and the introduction of a new evaluation framework. We first create a robust chest X-ray dataset (MIMIC-STRUC) that includes disease names, severity levels, probabilities, and anatomical locations, ensuring that the dataset is both clinically relevant and well-structured. We train an LLM-based model to generate standardized, high-quality reports. To assess the generated reports, we propose a specialized evaluation metric (S-Score) that not only measures disease prediction accuracy but also evaluates the precision of disease-specific details, thus offering a clinically meaningful metric for report quality that focuses on elements critical to clinical decision-making and demonstrates a stronger alignment with human assessments. Our approach highlights the effectiveness of structured reports and the importance of a tailored evaluation metric for S-RRG, providing a more clinically relevant measure of report quality.</li>
</ul>

<h3>Title: When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jin Li, Keyu Wang, Shu Yang, Zhuoran Zhang, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02087">https://arxiv.org/abs/2508.02087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02087">https://arxiv.org/pdf/2508.02087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02087]] When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models(https://arxiv.org/abs/2508.02087)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has documented this tendency, the internal mechanisms that enable such behavior remain poorly understood. In this paper, we provide a mechanistic account of how sycophancy arises within LLMs. We first systematically study how user opinions induce sycophancy across different model families. We find that simple opinion statements reliably induce sycophancy, whereas user expertise framing has a negligible impact. Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. We also verify that user authority fails to influence behavior because models do not encode it internally. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These findings highlight that sycophancy is not a surface-level artifact but emerges from a structural override of learned knowledge in deeper layers, with implications for alignment and truthful AI systems.</li>
</ul>

<h3>Title: FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Shida Wang, Chaohu Liu, Yubo Wang, Linli Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02092">https://arxiv.org/abs/2508.02092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02092">https://arxiv.org/pdf/2508.02092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02092]] FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing(https://arxiv.org/abs/2508.02092)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large language models represent significant investments in computation, data, and engineering expertise, making them extraordinarily valuable intellectual assets. Nevertheless, these AI assets remain vulnerable to unauthorized redistribution and commercial exploitation through fine-tuning or black-box deployment. Current fingerprinting approaches face a fundamental trade-off: intrinsic methods require full parameter access, while backdoor-based techniques employ statistically anomalous triggers easily detected and filtered by adversaries. To address these limitations, we introduce FPEdit, a novel knowledge-editing framework that injects semantically coherent natural language fingerprints by modifying a sparse subset of model weights. This ensures stealthy and precise ownership encoding without degrading the core functionality. Extensive experiments show that FPEdit achieves $95$-$100\%$ fingerprint retention under both full-parameter fine-tuning and parameter-efficient adaptation, while preserving performance on 24 downstream benchmarks. Moreover, FPEdit remains robust under quantization, pruning, and stochastic decoding, and can embed 10 fingerprint pairs into LLaMA2-7B in under 10 minutes using less than 32 GB of GPU memory, a $70\%$ reduction in resource requirements compared to existing techniques. These advances establish FPEdit as the first fingerprinting approach to simultaneously achieve robustness against adaptation, resistance to detection, and preservation of model utility, providing a minimally invasive solution for reliable provenance verification of large language models in adversarial deployment scenarios.</li>
</ul>

<h3>Title: VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Xin Eric Wang, Achuta Kadambi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02095">https://arxiv.org/abs/2508.02095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02095">https://arxiv.org/pdf/2508.02095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02095]] VLM4D: Towards Spatiotemporal Awareness in Vision Language Models(https://arxiv.org/abs/2508.02095)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision language models (VLMs) have shown remarkable capabilities in integrating linguistic and visual reasoning but remain fundamentally limited in understanding dynamic spatiotemporal interactions. Humans effortlessly track and reason about object movements, rotations, and perspective shifts-abilities essential for robust dynamic real-world understanding yet notably lacking in current VLMs. In this paper, we introduce VLM4D, the first benchmark specifically designed to evaluate the spatiotemporal reasoning capabilities of VLMs. Our benchmark comprises diverse real-world and synthetic videos accompanied by carefully curated question-answer pairs emphasizing translational and rotational motions, perspective awareness, and motion continuity. Through comprehensive evaluations of state-of-the-art open and closed-source VLMs, we identify significant performance gaps compared to human baselines, highlighting fundamental deficiencies in existing models. Extensive analysis reveals that VLMs struggle particularly with integrating multiple visual cues and maintaining temporal coherence. We further explore promising directions, such as leveraging 4D feature field reconstruction and targeted spatiotemporal supervised fine-tuning, demonstrating their effectiveness in enhancing spatiotemporal comprehension. Our work aims to encourage deeper exploration into improving VLMs' spatial and temporal grounding, paving the way towards more capable and reliable visual intelligence for dynamic environments.</li>
</ul>

<h3>Title: Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Kaiyang Ji, Ye Shi, Zichen Jin, Kangyi Chen, Lan Xu, Yuexin Ma, Jingyi Yu, Jingya Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02106">https://arxiv.org/abs/2508.02106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02106">https://arxiv.org/pdf/2508.02106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02106]] Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis(https://arxiv.org/abs/2508.02106)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Real-time synthesis of physically plausible human interactions remains a critical challenge for immersive VR/AR systems and humanoid robotics. While existing methods demonstrate progress in kinematic motion generation, they often fail to address the fundamental tension between real-time responsiveness, physical feasibility, and safety requirements in dynamic human-machine interactions. We introduce Human-X, a novel framework designed to enable immersive and physically plausible human interactions across diverse entities, including human-avatar, human-humanoid, and human-robot systems. Unlike existing approaches that focus on post-hoc alignment or simplified physics, our method jointly predicts actions and reactions in real-time using an auto-regressive reaction diffusion planner, ensuring seamless synchronization and context-aware responses. To enhance physical realism and safety, we integrate an actor-aware motion tracking policy trained with reinforcement learning, which dynamically adapts to interaction partners' movements while avoiding artifacts like foot sliding and penetration. Extensive experiments on the Inter-X and InterHuman datasets demonstrate significant improvements in motion quality, interaction continuity, and physical plausibility over state-of-the-art methods. Our framework is validated in real-world applications, including virtual reality interface for human-robot interaction, showcasing its potential for advancing human-robot collaboration.</li>
</ul>

<h3>Title: AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Li, Zhongjie Duan, Die Chen, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02107">https://arxiv.org/abs/2508.02107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02107">https://arxiv.org/pdf/2508.02107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02107]] AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation(https://arxiv.org/abs/2508.02107)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite recent advances in photorealistic image generation through large-scale models like FLUX and Stable Diffusion v3, the practical deployment of these architectures remains constrained by their inherent intractability to parameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated efficacy in enabling model customization with minimal parameter overhead, the effective utilization of distributed open-source LoRA modules faces three critical challenges: sparse metadata annotation, the requirement for zero-shot adaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion strategies. To address these limitations, we introduce a novel framework that enables semantic-driven LoRA retrieval and dynamic aggregation through two key components: (1) weight encoding-base LoRA retriever that establishes a shared semantic space between LoRA parameter matrices and text prompts, eliminating dependence on original training data, and (2) fine-grained gated fusion mechanism that computes context-specific fusion weights across network layers and diffusion timesteps to optimally integrate multiple LoRA modules during generation. Our approach achieves significant improvement in image generation perfermance, thereby facilitating scalable and data-efficient enhancement of foundational models. This work establishes a critical bridge between the fragmented landscape of community-developed LoRAs and practical deployment requirements, enabling collaborative model evolution through standardized adapter integration.</li>
</ul>

<h3>Title: Coward: Toward Practical Proactive Federated Backdoor Defense via Collision-based Watermark</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Li, Siying Gu, Yiming Li, Kangjie Chen, Zhili Chen, Tianwei Zhang, Shu-Tao Xia, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02115">https://arxiv.org/abs/2508.02115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02115">https://arxiv.org/pdf/2508.02115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02115]] Coward: Toward Practical Proactive Federated Backdoor Defense via Collision-based Watermark(https://arxiv.org/abs/2508.02115)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, federate, watermark</a></li>
<li><strong>Abstract: </strong>Backdoor detection is currently the mainstream defense against backdoor attacks in federated learning (FL), where malicious clients upload poisoned updates that compromise the global model and undermine the reliability of FL deployments. Existing backdoor detection techniques fall into two categories, including passive and proactive ones, depending on whether the server proactively modifies the global model. However, both have inherent limitations in practice: passive defenses are vulnerable to common non-i.i.d. data distributions and random participation of FL clients, whereas current proactive defenses suffer inevitable out-of-distribution (OOD) bias because they rely on backdoor co-existence effects. To address these issues, we introduce a new proactive defense, dubbed Coward, inspired by our discovery of multi-backdoor collision effects, in which consecutively planted, distinct backdoors significantly suppress earlier ones. In general, we detect attackers by evaluating whether the server-injected, conflicting global watermark is erased during local training rather than retained. Our method preserves the advantages of proactive defenses in handling data heterogeneity (\ie, non-i.i.d. data) while mitigating the adverse impact of OOD bias through a revised detection mechanism. Extensive experiments on benchmark datasets confirm the effectiveness of Coward and its resilience to potential adaptive attacks. The code for our method would be available at this https URL.</li>
</ul>

<h3>Title: SUAD: Solid-Channel Ultrasound Injection Attack and Defense to Voice Assistants</h3>
<ul>
<li><strong>Authors: </strong>Chao Liu, Zhezheng Zhu, Hao Chen, Zhe Chen, Kaiwen Guo, Penghao Wang, Jun Luo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02116">https://arxiv.org/abs/2508.02116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02116">https://arxiv.org/pdf/2508.02116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02116]] SUAD: Solid-Channel Ultrasound Injection Attack and Defense to Voice Assistants(https://arxiv.org/abs/2508.02116)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>As a versatile AI application, voice assistants (VAs) have become increasingly popular, but are vulnerable to security threats. Attackers have proposed various inaudible attacks, but are limited by cost, distance, or LoS. Therefore, we propose \name~Attack, a long-range, cross-barrier, and interference-free inaudible voice attack via solid channels. We begin by thoroughly analyzing the dispersion effect in solid channels, revealing its unique impact on signal propagation. To avoid distortions in voice commands, we design a modular command generation model that parameterizes attack distance, victim audio, and medium dispersion features to adapt to variations in the solid-channel state. Additionally, we propose SUAD Defense, a universal defense that uses ultrasonic perturbation signals to block inaudible voice attacks (IVAs) without impacting normal speech. Since the attack can occur at arbitrary frequencies and times, we propose a training method that randomizes both time and frequency to generate perturbation signals that break ultrasonic commands. Notably, the perturbation signal is modulated to an inaudible frequency without affecting the functionality of voice commands for VAs. Experiments on six smartphones have shown that SUAD Attack achieves activation success rates above 89.8% and SUAD Defense blocks IVAs with success rates exceeding 98%.</li>
</ul>

<h3>Title: Understanding Learning Dynamics Through Structured Representations</h3>
<ul>
<li><strong>Authors: </strong>Saleh Nikooroo, Thomas Engel</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02126">https://arxiv.org/abs/2508.02126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02126">https://arxiv.org/pdf/2508.02126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02126]] Understanding Learning Dynamics Through Structured Representations(https://arxiv.org/abs/2508.02126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While modern deep networks have demonstrated remarkable versatility, their training dynamics remain poorly understood--often driven more by empirical tweaks than architectural insight. This paper investigates how internal structural choices shape the behavior of learning systems. Building on prior efforts that introduced simple architectural constraints, we explore the broader implications of structure for convergence, generalization, and adaptation. Our approach centers on a family of enriched transformation layers that incorporate constrained pathways and adaptive corrections. We analyze how these structures influence gradient flow, spectral sensitivity, and fixed-point behavior--uncovering mechanisms that contribute to training stability and representational regularity. Theoretical analysis is paired with empirical studies on synthetic and structured tasks, demonstrating improved robustness, smoother optimization, and scalable depth behavior. Rather than prescribing fixed templates, we emphasize principles of tractable design that can steer learning behavior in interpretable ways. Our findings support a growing view that architectural design is not merely a matter of performance tuning, but a critical axis for shaping learning dynamics in scalable and trustworthy neural systems.</li>
</ul>

<h3>Title: Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps</h3>
<ul>
<li><strong>Authors: </strong>Mingjie Liu, Hanqing Liu, Chuang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02127">https://arxiv.org/abs/2508.02127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02127">https://arxiv.org/pdf/2508.02127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02127]] Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps(https://arxiv.org/abs/2508.02127)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate object detection under adverse lighting conditions is critical for real-world applications such as autonomous driving. Although neuromorphic event cameras have been introduced to handle these scenarios, adverse lighting often induces distracting reflections from tunnel walls or road surfaces, which frequently lead to false obstacle detections. However, neither RGB nor event data alone is robust enough to address these complexities, and mitigating these issues without additional sensors remains underexplored. To overcome these challenges, we propose leveraging normal maps, directly predicted from monocular RGB images, as robust geometric cues to suppress false positives and enhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection framework that effectively fuses three complementary modalities: monocularly predicted surface normal maps, RGB images, and event streams. To optimize the fusion process, our framework incorporates two key modules: the Adaptive Dual-stream Fusion Module (ADFM), which integrates RGB and normal map features, and the Event-modality Aware Fusion Module (EAFM), which adapts to the high dynamic range characteristics of event data. Extensive evaluations on the DSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly outperforms state-of-the-art methods. Our approach achieves mAP50 improvements of 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing the fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by 7.1% on the PKU-DAVIS-SOD dataset.</li>
</ul>

<h3>Title: Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tai An, Ruwu Cai, Yanzhe Zhang, Yang Liu, Hao Chen, Pengcheng Xie, Sheng Chang, Yiwu Yao, Gongyi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02128">https://arxiv.org/abs/2508.02128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02128">https://arxiv.org/pdf/2508.02128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02128]] Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models(https://arxiv.org/abs/2508.02128)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In the era of large language models (LLMs), N:M sparsity has emerged as a structured compression technique critical for accelerating inference. While prior work has primarily focused on weight sparsity, it often suffers from significant accuracy degradation. Activation sparsity, though promising, is typically training-dependent and faces challenges in generalization. To address these limitations, we introduce Amber Pruner, a training-free N:M activation sparsity method designed specifically for the prefill stage, targeting the acceleration of linear projection layers in LLMs. Extensive experiments across multiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber Pruner can effectively sparsify and accelerate more than 55% of linear computations without requiring model retraining. To further enhance generality and efficiency, we propose Outstanding-sparse, a unified framework that integrates Amber Pruner with post-training W8A8 quantization. Our approach preserves strong performance across a range of downstream tasks, with notable advantages in generative tasks. This work pioneers a new frontier in activation sparsity, providing foundational insights that are poised to guide the co-evolution of algorithms and architectures in the design of next-generation AI systems.</li>
</ul>

<h3>Title: VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yuru Xiao, Zihan Lin, Chao Lu, Deming Zhai, Kui Jiang, Wenbo Zhao, Wei Zhang, Junjun Jiang, Huanran Wang, Xianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02129">https://arxiv.org/abs/2508.02129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02129">https://arxiv.org/pdf/2508.02129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02129]] VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling(https://arxiv.org/abs/2508.02129)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Dynamic urban scene modeling is a rapidly evolving area with broad applications. While current approaches leveraging neural radiance fields or Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity novel view synthesis, they still face significant limitations. These often stem from a dependence on pre-calibrated object tracks or difficulties in accurately modeling fast-moving objects from undersampled capture, particularly due to challenges in handling temporal discontinuities. To overcome these issues, we propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our key insight is to distill robust, temporally consistent priors from a test-time adapted video diffusion model. To ensure precise pose alignment and effective integration of this denoised content, we introduce two core innovations: a joint timestamp optimization strategy that refines interpolated frame poses, and an uncertainty distillation method that adaptively extracts target content while preserving well-reconstructed regions. Extensive experiments demonstrate that our method significantly enhances dynamic modeling, especially for fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view synthesis over baseline approaches.</li>
</ul>

<h3>Title: Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference</h3>
<ul>
<li><strong>Authors: </strong>Kuo Wang, Quanlong Zheng, Junlin Xie, Yanhao Zhang, Jinguo Luo, Haonan Lu, Liang Lin, Fan Zhou, Guanbin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02134">https://arxiv.org/abs/2508.02134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02134">https://arxiv.org/pdf/2508.02134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02134]] Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference(https://arxiv.org/abs/2508.02134)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkable advancements in video understanding tasks. However, constrained by the context length limitation in the underlying LLMs, existing Video-MLLMs typically exhibit suboptimal performance on long video scenarios. To understand extended input frames, common solutions span token compression and streaming inference techniques, which sacrifice feature granularity or inference efficiency. Differently, to efficiently achieve comprehensive understanding of longer frame inputs, we draw ideas from MoE and propose a training-free approach \textbf{Free-MoRef}, which instantly multiplexes the context perception capabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef reconstructs the vision tokens into several short sequences as multi-references. Subsequently, we introduce MoRef-attention, which gathers clues from the multi-reference chunks in parallel to summarize unified query activations. After the shadow layers in LLMs, a reference fusion step is derived to compose a final mixed reasoning sequence with key tokens from parallel chunks, which compensates the cross-reference vision interactions that are neglected in MoRef-attention. By splitting and fusing the long vision token sequences, Free-MoRef achieves improved performance under much lower computing costs in reasoning multiplexed context length, demonstrating strong efficiency and effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that Free-MoRef achieves full perception of 2$\times$ to 8$\times$ longer input frames without compression on a single A100 GPU while keeping instant responses, thereby bringing significant performance gains, even surpassing dedicatedly trained long-video-MLLMs. Codes are available at this https URL</li>
</ul>

<h3>Title: FedLAD: A Linear Algebra Based Data Poisoning Defence for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Qi Xiong, Hai Dong, Nasrin Sohrabi, Zahir Tari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02136">https://arxiv.org/abs/2508.02136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02136">https://arxiv.org/pdf/2508.02136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02136]] FedLAD: A Linear Algebra Based Data Poisoning Defence for Federated Learning(https://arxiv.org/abs/2508.02136)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Sybil attacks pose a significant threat to federated learning, as malicious nodes can collaborate and gain a majority, thereby overwhelming the system. Therefore, it is essential to develop countermeasures that ensure the security of federated learning environments. We present a novel defence method against targeted data poisoning, which is one of the types of Sybil attacks, called Linear Algebra-based Detection (FedLAD). Unlike existing approaches, such as clustering and robust training, which struggle in situations where malicious nodes dominate, FedLAD models the federated learning aggregation process as a linear problem, transforming it into a linear algebra optimisation challenge. This method identifies potential attacks by extracting the independent linear combinations from the original linear combinations, effectively filtering out redundant and malicious elements. Extensive experimental evaluations demonstrate the effectiveness of FedLAD compared to five well-established defence methods: Sherpa, CONTRA, Median, Trimmed Mean, and Krum. Using tasks from both image classification and natural language processing, our experiments confirm that FedLAD is robust and not dependent on specific application settings. The results indicate that FedLAD effectively protects federated learning systems across a broad spectrum of malicious node ratios. Compared to baseline defence methods, FedLAD maintains a low attack success rate for malicious nodes when their ratio ranges from 0.2 to 0.8. Additionally, it preserves high model accuracy when the malicious node ratio is between 0.2 and 0.5. These findings underscore FedLAD's potential to enhance both the reliability and performance of federated learning systems in the face of data poisoning attacks.</li>
</ul>

<h3>Title: TrackletGait: A Robust Framework for Gait Recognition in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Shaoxiong Zhang, Jinkai Zheng, Shangdong Zhu, Chenggang Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02143">https://arxiv.org/abs/2508.02143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02143">https://arxiv.org/pdf/2508.02143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02143]] TrackletGait: A Robust Framework for Gait Recognition in the Wild(https://arxiv.org/abs/2508.02143)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gait recognition aims to identify individuals based on their body shape and walking patterns. Though much progress has been achieved driven by deep learning, gait recognition in real-world surveillance scenarios remains quite challenging to current methods. Conventional approaches, which rely on periodic gait cycles and controlled environments, struggle with the non-periodic and occluded silhouette sequences encountered in the wild. In this paper, we propose a novel framework, TrackletGait, designed to address these challenges in the wild. We propose Random Tracklet Sampling, a generalization of existing sampling methods, which strikes a balance between robustness and representation in capturing diverse walking patterns. Next, we introduce Haar Wavelet-based Downsampling to preserve information during spatial downsampling. Finally, we present a Hardness Exclusion Triplet Loss, designed to exclude low-quality silhouettes by discarding hard triplet samples. TrackletGait achieves state-of-the-art results, with 77.8 and 80.4 rank-1 accuracy on the Gait3D and GREW datasets, respectively, while using only 10.3M backbone parameters. Extensive experiments are also conducted to further investigate the factors affecting gait recognition in the wild.</li>
</ul>

<h3>Title: The Dark Side of Upgrades: Uncovering Security Risks in Smart Contract Upgrades</h3>
<ul>
<li><strong>Authors: </strong>Dingding Wang, Jianting He, Siwei Wu, Yajin Zhou, Lei Wu, Cong Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02145">https://arxiv.org/abs/2508.02145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02145">https://arxiv.org/pdf/2508.02145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02145]] The Dark Side of Upgrades: Uncovering Security Risks in Smart Contract Upgrades(https://arxiv.org/abs/2508.02145)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Smart contract upgrades are increasingly common due to their flexibility in modifying deployed contracts, such as fixing bugs or adding new functionalities. Meanwhile, upgrades compromise the immutability of contracts, introducing significant security concerns. While existing research has explored the security impacts of contract upgrades, these studies are limited in collection of upgrade behaviors and identification of insecurities. To address these limitations, we conduct a comprehensive study on the insecurities of upgrade behaviors. First, we build a dataset containing 83,085 upgraded contracts and 20,902 upgrade chains. To our knowledge, this is the first large-scale dataset about upgrade behaviors, revealing their diversity and exposing gaps in public disclosure. Next, we develop a taxonomy of insecurities based on 37 real-world security incidents, categorizing eight types of upgrade risks and providing the first complete view of upgrade-related insecurities. Finally, we survey public awareness of these risks and existing mitigations. Our findings show that four types of security risks are overlooked by the public and lack mitigation measures. We detect these upgrade risks through a preliminary study, identifying 31,407 related issues - a finding that raises significant concerns.</li>
</ul>

<h3>Title: Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Kuiyuan DIng, Caili Guo, Yang Yang, Zhongtian Du, Walid Saad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02148">https://arxiv.org/abs/2508.02148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02148">https://arxiv.org/pdf/2508.02148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02148]] Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation(https://arxiv.org/abs/2508.02148)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Large-scale models (LSMs) can be an effective framework for semantic representation and understanding, thereby providing a suitable tool for designing semantic communication (SC) systems. However, their direct deployment is often hindered by high computational complexity and resource requirements. In this paper, a novel robust knowledge distillation based semantic communication (RKD-SC) framework is proposed to enable efficient and \textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses two key challenges: determining optimal compact model architectures and effectively transferring knowledge while maintaining robustness against channel noise. First, a knowledge distillation-based lightweight differentiable architecture search (KDL-DARTS) algorithm is proposed. This algorithm integrates knowledge distillation loss and a complexity penalty into the neural architecture search process to identify high-performance, lightweight semantic encoder architectures. Second, a novel two-stage robust knowledge distillation (RKD) algorithm is developed to transfer semantic capabilities from an LSM (teacher) to a compact encoder (student) and subsequently enhance system robustness. To further improve resilience to channel impairments, a channel-aware transformer (CAT) block is introduced as the channel codec, trained under diverse channel conditions with variable-length outputs. Extensive simulations on image classification tasks demonstrate that the RKD-SC framework significantly reduces model parameters while preserving a high degree of the teacher model's performance and exhibiting superior robustness compared to existing methods.</li>
</ul>

<h3>Title: AURORA: Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Luo, Nian Liu, Fahad Shahbaz Khan, Junwei Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02149">https://arxiv.org/abs/2508.02149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02149">https://arxiv.org/pdf/2508.02149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02149]] AURORA: Augmented Understanding via Structured Reasoning and Reinforcement Learning for Reference Audio-Visual Segmentation(https://arxiv.org/abs/2508.02149)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to precisely locate sounding objects by integrating visual, auditory, and textual cues. Existing methods often lack genuine semantic understanding, tending to memorize fixed reasoning patterns. Furthermore, jointly training for reasoning and segmentation can compromise pixel-level precision. To address these issues, we introduce AURORA, a novel framework designed to enhance genuine reasoning and language comprehension in reference audio-visual segmentation. We employ a structured Chain-of-Thought (CoT) prompting mechanism to guide the model through a step-by-step reasoning process and introduce a novel segmentation feature distillation loss to effectively integrate these reasoning abilities without sacrificing segmentation performance. To further cultivate the model's genuine reasoning capabilities, we devise a further two-stage training strategy: first, a ``corrective reflective-style training" stage utilizes self-correction to enhance the quality of reasoning paths, followed by reinforcement learning via Group Reward Policy Optimization (GRPO) to bolster robustness in challenging scenarios. Experiments demonstrate that AURORA achieves state-of-the-art performance on Ref-AVS benchmarks and generalizes effectively to unreferenced segmentation.</li>
</ul>

<h3>Title: AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Die Chen, Zhongjie Duan, Zhiwen Li, Cen Chen, Daoyuan Chen, Yaliang Li, Yinda Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02151">https://arxiv.org/abs/2508.02151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02151">https://arxiv.org/pdf/2508.02151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02151]] AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models(https://arxiv.org/abs/2508.02151)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in text-to-image diffusion models have significantly enhanced both the visual fidelity and semantic controllability of generated images. However, fine-grained control over aesthetic attributes remains challenging, especially when users require continuous and intensity-specific adjustments. Existing approaches often rely on vague textual prompts, which are inherently ambiguous in expressing both the aesthetic semantics and the desired intensity, or depend on costly human preference data for alignment, limiting their scalability and practicality. To address these limitations, we propose AttriCtrl, a plug-and-play framework for precise and continuous control of aesthetic attributes. Specifically, we quantify abstract aesthetics by leveraging semantic similarity from pre-trained vision-language models, and employ a lightweight value encoder that maps scalar intensities in $[0,1]$ to learnable embeddings within diffusion-based generation. This design enables intuitive and customizable aesthetic manipulation, with minimal training overhead and seamless integration into existing generation pipelines. Extensive experiments demonstrate that AttriCtrl achieves accurate control over individual attributes as well as flexible multi-attribute composition. Moreover, it is fully compatible with popular open-source controllable generation frameworks, showcasing strong integration capability and practical utility across diverse generation scenarios.</li>
</ul>

<h3>Title: DreamPainter: Image Background Inpainting for E-commerce Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Sijie Zhao, Jing Cheng, Yaoyao Wu, Hao Xu, Shaohui Jiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02155">https://arxiv.org/abs/2508.02155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02155">https://arxiv.org/pdf/2508.02155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02155]] DreamPainter: Image Background Inpainting for E-commerce Scenarios(https://arxiv.org/abs/2508.02155)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although diffusion-based image genenation has been widely explored and applied, background generation tasks in e-commerce scenarios still face significant challenges. The first challenge is to ensure that the generated products are consistent with the given product inputs while maintaining a reasonable spatial arrangement, harmonious shadows, and reflections between foreground products and backgrounds. Existing inpainting methods fail to address this due to the lack of domain-specific data. The second challenge involves the limitation of relying solely on text prompts for image control, as effective integrating visual information to achieve precise control in inpainting tasks remains underexplored. To address these challenges, we introduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate product instance masks, background reference images, text prompts, and aesthetically pleasing product images. Based on this dataset, we propose DreamPainter, a novel framework that not only utilizes text prompts for control but also flexibly incorporates reference image information as an additional control signal. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, maintaining high product consistency while effectively integrating both text prompt and reference image information.</li>
</ul>

<h3>Title: Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes</h3>
<ul>
<li><strong>Authors: </strong>Tom Fischer, Xiaojie Zhang, Eddy Ilg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02157">https://arxiv.org/abs/2508.02157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02157">https://arxiv.org/pdf/2508.02157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02157]] Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes(https://arxiv.org/abs/2508.02157)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recognizing objects in images is a fundamental problem in computer vision. Although detecting objects in 2D images is common, many applications require determining their pose in 3D space. Traditional category-level methods rely on RGB-D inputs, which may not always be available, or employ two-stage approaches that use separate models and representations for detection and pose estimation. For the first time, we introduce a unified model that integrates detection and pose estimation into a single framework for RGB images by leveraging neural mesh models with learned features and multi-model RANSAC. Our approach achieves state-of-the-art results for RGB category-level pose estimation on REAL275, improving on the current state-of-the-art by 22.9% averaged across all scale-agnostic metrics. Finally, we demonstrate that our unified method exhibits greater robustness compared to single-stage baselines. Our code and models are available at this https URL.</li>
</ul>

<h3>Title: User Trajectory Prediction Unifying Global and Local Temporal Information</h3>
<ul>
<li><strong>Authors: </strong>Wei Hao, Bin Chong, Ronghua Ji, Chen Hou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02161">https://arxiv.org/abs/2508.02161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02161">https://arxiv.org/pdf/2508.02161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02161]] User Trajectory Prediction Unifying Global and Local Temporal Information(https://arxiv.org/abs/2508.02161)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Trajectory prediction is essential for formulating proactive strategies that anticipate user mobility and support advance preparation. Therefore, how to reduce the forecasting error in user trajectory prediction within an acceptable inference time arises as an interesting issue. However, trajectory data contains both global and local temporal information, complicating the extraction of the complete temporal pattern. Moreover, user behavior occurs over different time scales, increasing the difficulty of capturing behavioral patterns. To address these challenges, a trajectory prediction model based on multilayer perceptron (MLP), multi-scale convolutional neural network (MSCNN), and cross-attention (CA) is proposed. Specifically, MLP is used to extract the global temporal information of each feature. In parallel, MSCNN is employed to extract the local temporal information by modeling interactions among features within a local temporal range. Convolutional kernels with different sizes are used in MSCNN to capture temporal information at multiple resolutions, enhancing the model's adaptability to different behavioral patterns. Finally, CA is applied to fuse the global and local temporal information. Experimental results show that our model reduces mean squared error (MSE) by 5.04% and mean absolute error (MAE) by 4.35% compared with ModernTCN in 12-step prediction, while maintaining similar inference time.</li>
</ul>

<h3>Title: After the Party: Navigating the Mapping From Color to Ambient Lighting</h3>
<ul>
<li><strong>Authors: </strong>Florin-Alexandru Vasluianu, Tim Seizinger, Zongwei Wu, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02168">https://arxiv.org/abs/2508.02168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02168">https://arxiv.org/pdf/2508.02168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02168]] After the Party: Navigating the Mapping From Color to Ambient Lighting(https://arxiv.org/abs/2508.02168)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Illumination in practical scenarios is inherently complex, involving colored light sources, occlusions, and diverse material interactions that produce intricate reflectance and shading effects. However, existing methods often oversimplify this challenge by assuming a single light source or uniform, white-balanced lighting, leaving many of these complexities this http URL this paper, we introduce CL3AN, the first large-scale, high-resolution dataset of its kind designed to facilitate the restoration of images captured under multiple Colored Light sources to their Ambient-Normalized counterparts. Through benchmarking, we find that leading approaches often produce artifacts, such as illumination inconsistencies, texture leakage, and color distortion, primarily due to their limited ability to precisely disentangle illumination from reflectance. Motivated by this insight, we achieve such a desired decomposition through a novel learning framework that leverages explicit chromaticity and luminance components guidance, drawing inspiration from the principles of the Retinex model. Extensive evaluations on existing benchmarks and our dataset demonstrate the effectiveness of our approach, showcasing enhanced robustness under non-homogeneous color lighting and material-specific reflectance variations, all while maintaining a highly competitive computational cost. The benchmark, codes, and models are available at this http URL.</li>
</ul>

<h3>Title: GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02172">https://arxiv.org/abs/2508.02172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02172">https://arxiv.org/pdf/2508.02172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02172]] GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting(https://arxiv.org/abs/2508.02172)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Deep classification algorithm for De-identification of DICOM medical images</h3>
<ul>
<li><strong>Authors: </strong>Bufano Michele, Kotter Elmar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02177">https://arxiv.org/abs/2508.02177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02177">https://arxiv.org/pdf/2508.02177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02177]] Deep classification algorithm for De-identification of DICOM medical images(https://arxiv.org/abs/2508.02177)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Background : De-identification of DICOM (Digital Imaging and Communi-cations in Medicine) files is an essential component of medical image research. Personal Identifiable Information (PII) and/or Personal Health Identifying Information (PHI) need to be hidden or removed due to legal reasons. According to the Health Insurance Portability and Accountability Act (HIPAA) and privacy rules, also full-face photographic images and any compa-rable images are direct identifiers and are considered protected health information that also need to be de-identified. Objective : The study aimed to implement a method that permit to de-identify the PII and PHI information present in the header and burned on the pixel data of DICOM. Methods : To execute the de-identification, we implemented an algorithm based on the safe harbor method, defined by HIPAA. Our algorithm uses input customizable parameter to classify and then possibly de-identify individual DICOM tags. Results : The most sensible information, like names, history, personal data and institution were successfully recognized. Conclusions : We developed a python algorithm that is able to classify infor-mation present in a DICOM file. The flexibility provided by the use of customi-zable input parameters, which allow the user to customize the entire process de-pending on the case (e.g., the language), makes the entire program very promis-ing for both everyday use and research purposes. Our code is available at this https URL.</li>
</ul>

<h3>Title: Test-Time Model Adaptation for Quantized Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Zeshuai Deng, Guohao Chen, Shuaicheng Niu, Hui Luo, Shuhai Zhang, Yifan Yang, Renjie Chen, Wei Luo, Mingkui Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02180">https://arxiv.org/abs/2508.02180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02180">https://arxiv.org/pdf/2508.02180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02180]] Test-Time Model Adaptation for Quantized Neural Networks(https://arxiv.org/abs/2508.02180)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Quantizing deep models prior to deployment is a widely adopted technique to speed up inference for various real-time applications, such as autonomous driving. However, quantized models often suffer from severe performance degradation in dynamic environments with potential domain shifts and this degradation is significantly more pronounced compared with their full-precision counterparts, as shown by our theoretical and empirical illustrations. To address the domain shift problem, test-time adaptation (TTA) has emerged as an effective solution by enabling models to learn adaptively from test data. Unfortunately, existing TTA methods are often impractical for quantized models as they typically rely on gradient backpropagation--an operation that is unsupported on quantized models due to vanishing gradients, as well as memory and latency constraints. In this paper, we focus on TTA for quantized models to improve their robustness and generalization ability efficiently. We propose a continual zeroth-order adaptation (ZOA) framework that enables efficient model adaptation using only two forward passes, eliminating the computational burden of existing methods. Moreover, we propose a domain knowledge management scheme to store and reuse different domain knowledge with negligible memory consumption, reducing the interference of different domain knowledge and fostering the knowledge accumulation during long-term adaptation. Experimental results on three classical architectures, including quantized transformer-based and CNN-based models, demonstrate the superiority of our methods for quantized model adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve a 5.0\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The source code is available at this https URL.</li>
</ul>

<h3>Title: CAAD: Context-Aware Adaptive Decoding for Truthful Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Manh Nguyen, Sunil Gupta, Hung Le</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02184">https://arxiv.org/abs/2508.02184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02184">https://arxiv.org/pdf/2508.02184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02184]] CAAD: Context-Aware Adaptive Decoding for Truthful Text Generation(https://arxiv.org/abs/2508.02184)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring truthfulness in large language models remains a critical challenge for reliable text generation. While supervised fine-tuning and reinforcement learning with human feedback have shown promise, they require substantial amount of annotated data and computational resources, limiting scalability. In contrast, decoding-time interventions offer lightweight alternatives without model retraining. However, existing decoding strategies often face issues like prompt sensitivity, limited generalization, or dependence on internal model states. We propose a context-aware adaptive decoding method that leverages a compact reference grounding space, built from as few as 10 annotated examples and comprising pairs of context embeddings and next token logits from truthful responses, to enable retrieval-based logit shaping during inference. At each decoding step, our method retrieves top-N semantically similar contexts and aggregates their associated next token logits to modify the LLM's logits. Across three open-ended question-answering benchmarks, our approach achieves a 2.8 percent average improvement on TruthfulQA and further outperforms existing baselines on both Biographies and WikiQA. Experimental results also demonstrate cross-task generalization, with TruthfulQA-derived grounding enhancing biography generation. Our model-agnostic, scalable, and efficient method requires only a single generation pass, highlighting the potential of context-aware decoding for factual reliability in LLMs.</li>
</ul>

<h3>Title: Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Yanyun Wang, Li Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02186">https://arxiv.org/abs/2508.02186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02186">https://arxiv.org/pdf/2508.02186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02186]] Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training(https://arxiv.org/abs/2508.02186)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial Training (AT) is one of the most effective methods to train robust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off between clean accuracy and adversarial robustness, which is commonly attributed to the more complicated decision boundary caused by the insufficient learning of hard adversarial samples. In this work, we reveal a counterintuitive fact for the first time: From the perspective of perception consistency, hard adversarial samples that can still attack the robust model after AT are already learned better than those successfully defended. Thus, different from previous views, we argue that it is rather the over-sufficient learning of hard adversarial samples that degrades the decision boundary and contributes to the trade-off problem. Specifically, the excessive pursuit of perception consistency would force the model to view the perturbations as noise and ignore the information within them, which should have been utilized to induce a smoother perception transition towards the decision boundary to support its establishment to an appropriate location. In response, we define a new AT objective named Robust Perception, encouraging the model perception to change smoothly with input perturbations, based on which we propose a novel Robust Perception Adversarial Training (RPAT) method, effectively mitigating the current accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate the effectiveness of our method beyond four common baselines and 12 state-of-the-art (SOTA) works. The code is available at this https URL.</li>
</ul>

<h3>Title: Whispering Agents: An event-driven covert communication protocol for the Internet of Agents</h3>
<ul>
<li><strong>Authors: </strong>Kaibo Huang, Yukun Wei, WanSheng Wu, Tianhua Zhang, Zhongliang Yang, Linna Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02188">https://arxiv.org/abs/2508.02188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02188">https://arxiv.org/pdf/2508.02188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02188]] Whispering Agents: An event-driven covert communication protocol for the Internet of Agents(https://arxiv.org/abs/2508.02188)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>The emergence of the Internet of Agents (IoA) introduces critical challenges for communication privacy in sensitive, high-stakes domains. While standard Agent-to-Agent (A2A) protocols secure message content, they are not designed to protect the act of communication itself, leaving agents vulnerable to surveillance and traffic analysis. We find that the rich, event-driven nature of agent dialogues provides a powerful, yet untapped, medium for covert communication. To harness this potential, we introduce and formalize the Covert Event Channel, the first unified model for agent covert communication driven by three interconnected dimensions, which consist of the Storage, Timing,and Behavioral channels. Based on this model, we design and engineer {\Pi}CCAP, a novel protocol that operationalizes this event-driven paradigm. Our comprehensive evaluation demonstrates that {\Pi}CCAP achieves high capacity and robustness while remaining imperceptible to powerful LLM-based wardens, establishing its practical viability. By systematically engineering this channel, our work provides the foundational understanding essential for developing the next generation of monitoring systems and defensive protocols for a secure and trustworthy IoA.</li>
</ul>

<h3>Title: Learning Dynamics of Meta-Learning in Small Model Pretraining</h3>
<ul>
<li><strong>Authors: </strong>David Demitri Africa, Yuval Weiss, Paula Buttery, Richard Diehl Martinez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02189">https://arxiv.org/abs/2508.02189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02189">https://arxiv.org/pdf/2508.02189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02189]] Learning Dynamics of Meta-Learning in Small Model Pretraining(https://arxiv.org/abs/2508.02189)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only better but also more interpretable. We integrate first-order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M-570M params), and evaluate it on a fundamental NLP task with many settings and real-world applications. Compared with vanilla training, our model (i) reaches the same loss up to 1.6x sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and (iii) makes the training dynamics easy to read: first the network's representations fan out ("diversify") and later they collapse into a smaller, shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall in both effective-rank curves and attention-head entropy. The same curves pinpoint which layers specialise earliest and which later reconverge, giving a compact, interpretable signature of meta-adaptation. Code, checkpoints and WandB logs are released.</li>
</ul>

<h3>Title: Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, Hao Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02193">https://arxiv.org/abs/2508.02193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02193">https://arxiv.org/pdf/2508.02193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02193]] Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference(https://arxiv.org/abs/2508.02193)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.</li>
</ul>

<h3>Title: Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems</h3>
<ul>
<li><strong>Authors: </strong>Yebo Peng, Zixiang Liu, Yaoming Li, Zhizhuo Yang, Xinye Xu, Bowen Ye, Weijun Yuan, Zihan Wang, Tong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02208">https://arxiv.org/abs/2508.02208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02208">https://arxiv.org/pdf/2508.02208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02208]] Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems(https://arxiv.org/abs/2508.02208)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the mathematical capability of Large Language Models (LLMs) is a critical yet challenging frontier. Existing benchmarks fall short, particularly for proof-centric problems, as manual creation is unscalable and costly, leaving the true mathematical abilities of LLMs largely unassessed. To overcome these barriers, we propose Proof2Hybrid, the first fully automated framework that synthesizes high-quality, proof-centric benchmarks from natural language mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of converting mathematical proofs into various kinds of questions that are easy to verify. Instructed by this roadmap, we propose a new type of hybrid-formatted questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically designed to enable robust, automatic evaluation while being resilient to guessing and superficial pattern matching inherent in traditional formats. As a demonstration of our framework, we introduce AlgGeoTest, a benchmark for algebraic geometry--a frontier domain of modern mathematics--comprising 456 challenging items. Our extensive evaluations on state-of-the-art LLMs using AlgGeoTest reveal profound deficits in their comprehension of algebraic geometry, providing a more precise measure of their true mathematical capabilities. Our framework and benchmark pave the way for a new wave of in-depth research into the mathematical intelligence of AI systems.</li>
</ul>

<h3>Title: Balancing Information Accuracy and Response Timeliness in Networked LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yigit Turkmen, Baturalp Buyukates, Melih Bastopcu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02209">https://arxiv.org/abs/2508.02209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02209">https://arxiv.org/pdf/2508.02209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02209]] Balancing Information Accuracy and Response Timeliness in Networked LLMs(https://arxiv.org/abs/2508.02209)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have transformed many fields including scientific discovery, content generation, biomedical text mining, and educational technology. However, the substantial requirements for training data, computational resources, and energy consumption pose significant challenges for their practical deployment. A promising alternative is to leverage smaller, specialized language models and aggregate their outputs to improve overall response quality. In this work, we investigate a networked LLM system composed of multiple users, a central task processor, and clusters of topic-specialized LLMs. Each user submits categorical binary (true/false) queries, which are routed by the task processor to a selected cluster of $m$ LLMs. After gathering individual responses, the processor returns a final aggregated answer to the user. We characterize both the information accuracy and response timeliness in this setting, and formulate a joint optimization problem to balance these two competing objectives. Our extensive simulations demonstrate that the aggregated responses consistently achieve higher accuracy than those of individual LLMs. Notably, this improvement is more significant when the participating LLMs exhibit similar standalone performance.</li>
</ul>

<h3>Title: LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</h3>
<ul>
<li><strong>Authors: </strong>Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02215">https://arxiv.org/abs/2508.02215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02215">https://arxiv.org/pdf/2508.02215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02215]] LeanK: Learnable K Cache Channel Pruning for Efficient Decoding(https://arxiv.org/abs/2508.02215)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at this https URL.</li>
</ul>

<h3>Title: Welcome New Doctor: Continual Learning with Expert Consultation and Autoregressive Inference for Whole Slide Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Doanh Cao Bui, Jin Tae Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02220">https://arxiv.org/abs/2508.02220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02220">https://arxiv.org/pdf/2508.02220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02220]] Welcome New Doctor: Continual Learning with Expert Consultation and Autoregressive Inference for Whole Slide Image Analysis(https://arxiv.org/abs/2508.02220)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Whole Slide Image (WSI) analysis, with its ability to reveal detailed tissue structures in magnified views, plays a crucial role in cancer diagnosis and prognosis. Due to their giga-sized nature, WSIs require substantial storage and computational resources for processing and training predictive models. With the rapid increase in WSIs used in clinics and hospitals, there is a growing need for a continual learning system that can efficiently process and adapt existing models to new tasks without retraining or fine-tuning on previous tasks. Such a system must balance resource efficiency with high performance. In this study, we introduce COSFormer, a Transformer-based continual learning framework tailored for multi-task WSI analysis. COSFormer is designed to learn sequentially from new tasks wile avoiding the need to revisit full historical datasets. We evaluate COSFormer on a sequence of seven WSI datasets covering seven organs and six WSI-related tasks under both class-incremental and task-incremental settings. The results demonstrate COSFormer's superior generalizability and effectiveness compared to existing continual learning frameworks, establishing it as a robust solution for continual WSI analysis in clinical applications.</li>
</ul>

<h3>Title: Pigeon-SL: Robust Split Learning Framework for Edge Intelligence under Malicious Clients</h3>
<ul>
<li><strong>Authors: </strong>Sangjun Park, Tony Q.S. Quek, Hyowoon Seo</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02235">https://arxiv.org/abs/2508.02235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02235">https://arxiv.org/pdf/2508.02235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02235]] Pigeon-SL: Robust Split Learning Framework for Edge Intelligence under Malicious Clients(https://arxiv.org/abs/2508.02235)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>Recent advances in split learning (SL) have established it as a promising framework for privacy-preserving, communication-efficient distributed learning at the network edge. However, SL's sequential update process is vulnerable to even a single malicious client, which can significantly degrade model accuracy. To address this, we introduce Pigeon-SL, a novel scheme grounded in the pigeonhole principle that guarantees at least one entirely honest cluster among M clients, even when up to N of them are adversarial. In each global round, the access point partitions the clients into N+1 clusters, trains each cluster independently via vanilla SL, and evaluates their validation losses on a shared dataset. Only the cluster with the lowest loss advances, thereby isolating and discarding malicious updates. We further enhance training and communication efficiency with Pigeon-SL+, which repeats training on the selected cluster to match the update throughput of standard SL. We validate the robustness and effectiveness of our approach under three representative attack models -- label flipping, activation and gradient manipulation -- demonstrating significant improvements in accuracy and resilience over baseline SL methods in future intelligent wireless networks.</li>
</ul>

<h3>Title: Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor</h3>
<ul>
<li><strong>Authors: </strong>Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02240">https://arxiv.org/abs/2508.02240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02240">https://arxiv.org/pdf/2508.02240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02240]] Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor(https://arxiv.org/abs/2508.02240)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have demonstrated remarkable performance in visual generation tasks. However, their low inference speed limits their deployment in low-resource applications. Recent training-free approaches exploit the redundancy of features across timesteps by caching and reusing past representations to accelerate inference. Building on this idea, TaylorSeer instead uses cached features to predict future ones via Taylor expansion. However, its module-level prediction across all transformer blocks (e.g., attention or feedforward modules) requires storing fine-grained intermediate features, leading to notable memory and computation overhead. Moreover, it adopts a fixed caching schedule without considering the varying accuracy of predictions across timesteps, which can lead to degraded outputs when prediction fails. To address these limitations, we propose a novel approach to better leverage Taylor-based acceleration. First, we shift the Taylor prediction target from the module level to the last block level, significantly reducing the number of cached features. Furthermore, observing strong sequential dependencies among Transformer blocks, we propose to use the error between the Taylor-estimated and actual outputs of the first block as an indicator of prediction reliability. If the error is small, we trust the Taylor prediction for the last block; otherwise, we fall back to full computation, thereby enabling a dynamic caching mechanism. Empirical results show that our method achieves a better balance between speed and quality, achieving a 3.17x acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible quality drop. The Project Page is \href{this https URL}{here.}</li>
</ul>

<h3>Title: Isolating Culture Neurons in Multilingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Danial Namazifard, Lukas Galke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02241">https://arxiv.org/abs/2508.02241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02241">https://arxiv.org/pdf/2508.02241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02241]] Isolating Culture Neurons in Multilingual Large Language Models(https://arxiv.org/abs/2508.02241)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their overlap and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that LLMs encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at this https URL .</li>
</ul>

<h3>Title: I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Liu, Junwen Li, Kaiwen Li, Tong Ruan, Chao Wang, Xinyan He, Zongyu Wang, Xuezhi Cao, Jingping Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02243">https://arxiv.org/abs/2508.02243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02243">https://arxiv.org/pdf/2508.02243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02243]] I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking(https://arxiv.org/abs/2508.02243)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal entity linking plays a crucial role in a wide range of applications. Recent advances in large language model-based methods have become the dominant paradigm for this task, effectively leveraging both textual and visual modalities to enhance performance. Despite their success, these methods still face two challenges, including unnecessary incorporation of image data in certain scenarios and the reliance only on a one-time extraction of visual features, which can undermine their effectiveness and accuracy. To address these challenges, we propose a novel LLM-based framework for the multimodal entity linking task, called Intra- and Inter-modal Collaborative Reflections. This framework prioritizes leveraging text information to address the task. When text alone is insufficient to link the correct entity through intra- and inter-modality evaluations, it employs a multi-round iterative strategy that integrates key visual clues from various aspects of the image to support reasoning and enhance matching accuracy. Extensive experiments on three widely used public datasets demonstrate that our framework consistently outperforms current state-of-the-art methods in the task, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively. Our code is available at this https URL.</li>
</ul>

<h3>Title: Semi-Supervised Semantic Segmentation via Derivative Label Propagation</h3>
<ul>
<li><strong>Authors: </strong>Yuanbin Fu, Xiaojie Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02254">https://arxiv.org/abs/2508.02254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02254">https://arxiv.org/pdf/2508.02254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02254]] Semi-Supervised Semantic Segmentation via Derivative Label Propagation(https://arxiv.org/abs/2508.02254)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised semantic segmentation, which leverages a limited set of labeled images, helps to relieve the heavy annotation burden. While pseudo-labeling strategies yield promising results, there is still room for enhancing the reliability of pseudo-labels. Hence, we develop a semi-supervised framework, namely DerProp, equipped with a novel derivative label propagation to rectify imperfect pseudo-labels. Our label propagation method imposes discrete derivative operations on pixel-wise feature vectors as additional regularization, thereby generating strictly regularized similarity metrics. Doing so effectively alleviates the ill-posed problem that identical similarities correspond to different features, through constraining the solution space. Extensive experiments are conducted to verify the rationality of our design, and demonstrate our superiority over other methods. Codes are available at this https URL.</li>
</ul>

<h3>Title: Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders</h3>
<ul>
<li><strong>Authors: </strong>Belen Alastruey, João Maria Janeiro, Alexandre Allauzen, Maha Elbayad, Loïc Barrault, Marta R. Costa-jussà</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02256">https://arxiv.org/abs/2508.02256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02256">https://arxiv.org/pdf/2508.02256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02256]] Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders(https://arxiv.org/abs/2508.02256)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we present a comprehensive study of language interference in encoder-only Transformer models across 83 languages. We construct an interference matrix by training and evaluating small BERT-like models on all possible language pairs, providing a large-scale quantification of cross-lingual interference. Our analysis reveals that interference between languages is asymmetrical and that its patterns do not align with traditional linguistic characteristics, such as language family, nor with proxies like embedding similarity, but instead better relate to script. Finally, we demonstrate that the interference matrix effectively predicts performance on downstream tasks, serving as a tool to better design multilingual models to obtain optimal performance.</li>
</ul>

<h3>Title: Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jia Deng, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02260">https://arxiv.org/abs/2508.02260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02260">https://arxiv.org/pdf/2508.02260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02260]] Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning(https://arxiv.org/abs/2508.02260)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, reinforcement learning with verifiable rewards (RLVR) has been widely used for enhancing the reasoning abilities of large language models (LLMs). A core challenge in RLVR involves managing the exchange between entropy and performance of policies. Despite the importance of this exchange, a fine-grained understanding of when and how this exchange operates most effectively remains limited. To bridge this gap, we conduct a systematic empirical analysis of the entropy-performance exchange mechanism of RLVR across different levels of granularity. Specifically, we first divide the training process into two distinct stages based on entropy dynamics, i.e., rising stage and plateau stage, and then systematically investigate how this mechanism varies across stage-level, instance-level, and token-level granularitiess. Our analysis reveals that, in the rising stage, entropy reduction in negative samples facilitates the learning of effective reasoning patterns, which in turn drives rapid performance gains. Moreover, in the plateau stage, learning efficiency strongly correlates with high-entropy tokens present in low-perplexity samples and those located at the end of sequences. Motivated by these findings, we propose two methods that dynamically adjust the reward signal using perplexity and positional information to focus RL updates on tokens that exhibit high learning potential, achieving improvements compared to the baseline methods on various LLMs.</li>
</ul>

<h3>Title: SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion</h3>
<ul>
<li><strong>Authors: </strong>Rui Qian, Haozhi Cao, Tianchen Deng, Shenghai Yuan, Lihua Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02261">https://arxiv.org/abs/2508.02261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02261">https://arxiv.org/pdf/2508.02261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02261]] SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion(https://arxiv.org/abs/2508.02261)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising task that aims to infer dense geometric and semantic descriptions of a scene from a single image. While recent object-centric paradigms significantly improve efficiency by leveraging flexible 3D Gaussian primitives, they still rely heavily on a large number of randomly initialized primitives, which inevitably leads to 1) inefficient primitive initialization and 2) outlier primitives that introduce erroneous artifacts. In this paper, we propose SplatSSC, a novel framework that resolves these limitations with a depth-guided initialization strategy and a principled Gaussian aggregator. Instead of random initialization, SplatSSC utilizes a dedicated depth branch composed of a Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image and depth features to generate a sparse yet representative set of initial Gaussian primitives. To mitigate noise from outlier primitives, we develop the Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing geometric and semantic predictions during the Gaussian-to-voxel splatting process. Complemented with a specialized Probability Scale Loss, our method achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both latency and memory consumption by more than 9.3%. The code will be released upon acceptance.</li>
</ul>

<h3>Title: Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image Classification and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Peng Zhang, Zhihui Lai, Heng Kong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02265">https://arxiv.org/abs/2508.02265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02265">https://arxiv.org/pdf/2508.02265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02265]] Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image Classification and Segmentation(https://arxiv.org/abs/2508.02265)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Confidence-based pseudo-label selection usually generates overly confident yet incorrect predictions, due to the early misleadingness of model and overfitting inaccurate pseudo-labels in the learning process, which heavily degrades the performance of semi-supervised contrastive learning. Moreover, segmentation and classification tasks are treated independently and the affinity fails to be fully explored. To address these issues, we propose a novel semi-supervised dual-threshold contrastive learning strategy for ultrasound image classification and segmentation, named Hermes. This strategy combines the strengths of contrastive learning with semi-supervised learning, where the pseudo-labels assist contrastive learning by providing additional guidance. Specifically, an inter-task attention and saliency module is also developed to facilitate information sharing between the segmentation and classification tasks. Furthermore, an inter-task consistency learning strategy is designed to align tumor features across both tasks, avoiding negative transfer for reducing features discrepancy. To solve the lack of publicly available ultrasound datasets, we have collected the SZ-TUS dataset, a thyroid ultrasound image dataset. Extensive experiments on two public ultrasound datasets and one private dataset demonstrate that Hermes consistently outperforms several state-of-the-art methods across various semi-supervised settings.</li>
</ul>

<h3>Title: Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Paul Zaha, Lars Böcking, Simeon Allmendinger, Leopold Müller, Niklas Kühl</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02281">https://arxiv.org/abs/2508.02281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02281">https://arxiv.org/pdf/2508.02281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02281]] Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical Image Segmentation(https://arxiv.org/abs/2508.02281)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is crucial for disease diagnosis and treatment planning, yet developing robust segmentation models often requires substantial computational resources and large datasets. Existing research shows that pre-trained and finetuned foundation models can boost segmentation performance. However, questions remain about how particular image preprocessing steps may influence segmentation performance across different medical imaging modalities. In particular, edges-abrupt transitions in pixel intensity-are widely acknowledged as vital cues for object boundaries but have not been systematically examined in the pre-training of foundation models. We address this gap by investigating to which extend pre-training with data processed using computationally efficient edge kernels, such as kirsch, can improve cross-modality segmentation capabilities of a foundation model. Two versions of a foundation model are first trained on either raw or edge-enhanced data across multiple medical imaging modalities, then finetuned on selected raw subsets tailored to specific medical modalities. After systematic investigation using the medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, and XRay, we discover both increased and reduced segmentation performance across modalities using edge-focused pre-training, indicating the need for a selective application of this approach. To guide such selective applications, we propose a meta-learning strategy. It uses standard deviation and image entropy of the raw image to choose between a model pre-trained on edge-enhanced or on raw data for optimal performance. Our experiments show that integrating this meta-learning layer yields an overall segmentation performance improvement across diverse medical imaging tasks by 16.42% compared to models pre-trained on edge-enhanced data only and 19.30% compared to models pre-trained on raw data only.</li>
</ul>

<h3>Title: An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto Insurance Fraud Detection with Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Francis Boabang, Samuel Asante Gyamerah</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP, q-fin.RM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02283">https://arxiv.org/abs/2508.02283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02283">https://arxiv.org/pdf/2508.02283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02283]] An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto Insurance Fraud Detection with Explainable AI(https://arxiv.org/abs/2508.02283)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In insurance fraud prediction, handling class imbalance remains a critical challenge. This paper presents a novel multistage focal loss function designed to enhance the performance of machine learning models in such imbalanced settings by helping to escape local minima and converge to a good solution. Building upon the foundation of the standard focal loss, our proposed approach introduces a dynamic, multi-stage convex and nonconvex mechanism that progressively adjusts the focus on hard-to-classify samples across training epochs. This strategic refinement facilitates more stable learning and improved discrimination between fraudulent and legitimate cases. Through extensive experimentation on a real-world insurance dataset, our method achieved better performance than the traditional focal loss, as measured by accuracy, precision, F1-score, recall and Area Under the Curve (AUC) metrics on the auto insurance dataset. These results demonstrate the efficacy of the multistage focal loss in boosting model robustness and predictive accuracy in highly skewed classification tasks, offering significant implications for fraud detection systems in the insurance industry. An explainable model is included to interpret the results.</li>
</ul>

<h3>Title: Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Jae-Young Kang, Hoonhee Cho, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02288">https://arxiv.org/abs/2508.02288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02288">https://arxiv.org/pdf/2508.02288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02288]] Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection(https://arxiv.org/abs/2508.02288)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D object detection is essential for autonomous systems, enabling precise localization and dimension estimation. While LiDAR and RGB cameras are widely used, their fixed frame rates create perception gaps in high-speed scenarios. Event cameras, with their asynchronous nature and high temporal resolution, offer a solution by capturing motion continuously. The recent approach, which integrates event cameras with conventional sensors for continuous-time detection, struggles in fast-motion scenarios due to its dependency on synchronized sensors. We propose a novel stereo 3D object detection framework that relies solely on event cameras, eliminating the need for conventional 3D sensors. To compensate for the lack of semantic and geometric information in event data, we introduce a dual filter mechanism that extracts both. Additionally, we enhance regression by aligning bounding boxes with object-centric information. Experiments show that our method outperforms prior approaches in dynamic environments, demonstrating the potential of event cameras for robust, continuous-time 3D perception. The code is available at this https URL.</li>
</ul>

<h3>Title: Flexible Automatic Identification and Removal (FAIR)-Pruner: An Efficient Neural Network Pruning Method</h3>
<ul>
<li><strong>Authors: </strong>Chenqing Lin, Mostafa Hussien, Chengyao Yu, Mohamed Cheriet, Osama Abdelrahman, Ruixing Ming</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02291">https://arxiv.org/abs/2508.02291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02291">https://arxiv.org/pdf/2508.02291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02291]] Flexible Automatic Identification and Removal (FAIR)-Pruner: An Efficient Neural Network Pruning Method(https://arxiv.org/abs/2508.02291)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Neural network pruning is a critical compression technique that facilitates the deployment of large-scale neural networks on resource-constrained edge devices, typically by identifying and eliminating redundant or insignificant parameters to reduce computational and memory overhead. This paper proposes the Flexible Automatic Identification and Removal (FAIR)-Pruner, a novel method for neural network structured pruning. Specifically, FAIR-Pruner first evaluates the importance of each unit (e.g., neuron or channel) through the Utilization Score quantified by the Wasserstein distance. To reflect the performance degradation after unit removal, it then introduces the Reconstruction Error, which is computed via the Taylor expansion of the loss function. Finally, FAIR-Pruner identifies superfluous units with negligible impact on model performance by controlling the proposed Tolerance of Difference, which measures differences between unimportant units and those that cause performance degradation. A major advantage of FAIR-Pruner lies in its capacity to automatically determine the layer-wise pruning rates, which yields a more efficient subnetwork structure compared to applying a uniform pruning rate. Another advantage of the FAIR-Pruner is its great one-shot performance without post-pruning fine-tuning. Furthermore, with utilization scores and reconstruction errors, users can flexibly obtain pruned models under different pruning ratios. Comprehensive experimental validation on diverse benchmark datasets (e.g., ImageNet) and various neural network architectures (e.g., VGG) demonstrates that FAIR-Pruner achieves significant model compression while maintaining high accuracy.</li>
</ul>

<h3>Title: Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02293">https://arxiv.org/abs/2508.02293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02293">https://arxiv.org/pdf/2508.02293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02293]] Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning(https://arxiv.org/abs/2508.02293)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>So-called unsupervised anomaly detection is better described as semi-supervised, as it assumes all training data are nominal. This assumption simplifies training but requires manual data curation, introducing bias and limiting adaptability. We propose Confident Meta-learning (CoMet), a novel training strategy that enables deep anomaly detection models to learn from uncurated datasets where nominal and anomalous samples coexist, eliminating the need for explicit filtering. Our approach integrates Soft Confident Learning, which assigns lower weights to low-confidence samples, and Meta-Learning, which stabilizes training by regularizing updates based on training validation loss covariance. This prevents overfitting and enhances robustness to noisy data. CoMet is model-agnostic and can be applied to any anomaly detection method trainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2 with two state-of-the-art models demonstrate the effectiveness of our approach, consistently improving over the baseline methods, remaining insensitive to anomalies in the training set, and setting a new state-of-the-art across all datasets.</li>
</ul>

<h3>Title: Pre-Tactical Flight-Delay and Turnaround Forecasting with Synthetic Aviation Data</h3>
<ul>
<li><strong>Authors: </strong>Abdulmajid Murad, Massimiliano Ruocco</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02294">https://arxiv.org/abs/2508.02294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02294">https://arxiv.org/pdf/2508.02294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02294]] Pre-Tactical Flight-Delay and Turnaround Forecasting with Synthetic Aviation Data(https://arxiv.org/abs/2508.02294)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Access to comprehensive flight operations data remains severely restricted in aviation due to commercial sensitivity and competitive considerations, hindering the development of predictive models for operational planning. This paper investigates whether synthetic data can effectively replace real operational data for training machine learning models in pre-tactical aviation scenarios-predictions made hours to days before operations using only scheduled flight information. We evaluate four state-of-the-art synthetic data generators on three prediction tasks: aircraft turnaround time, departure delays, and arrival delays. Using a Train on Synthetic, Test on Real (TSTR) methodology on over 1.7 million European flight records, we first validate synthetic data quality through fidelity assessments, then assess both predictive performance and the preservation of operational relationships. Our results show that advanced neural network architectures, specifically transformer-based generators, can retain 94-97% of real-data predictive performance while maintaining feature importance patterns informative for operational decision-making. Our analysis reveals that even with real data, prediction accuracy is inherently limited when only scheduled information is available-establishing realistic baselines for pre-tactical forecasting. These findings suggest that high-quality synthetic data can enable broader access to aviation analytics capabilities while preserving commercial confidentiality, though stakeholders must maintain realistic expectations about pre-tactical prediction accuracy given the stochastic nature of flight operations.</li>
</ul>

<h3>Title: Simple Methods Defend RAG Systems Well Against Real-World Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ilias Triantafyllopoulos, Renyi Qu, Salvatore Giorgi, Brenda Curtis, Lyle H. Ungar, João Sedoc</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02296">https://arxiv.org/abs/2508.02296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02296">https://arxiv.org/pdf/2508.02296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02296]] Simple Methods Defend RAG Systems Well Against Real-World Attacks(https://arxiv.org/abs/2508.02296)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Ensuring safety and in-domain responses for Retrieval-Augmented Generation (RAG) systems is paramount in safety-critical applications, yet remains a significant challenge. To address this, we evaluate four methodologies for Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG system only responds to queries confined to the system's knowledge base. Specifically, our evaluation explores two novel dimensionality reduction and feature separation strategies: \textit{PCA}, where top components are selected using explained variance or OOD separability, and an adaptation of \textit{Neural Collapse Feature Separation}. We validate our approach on standard datasets (StackExchange and MSMARCO) and real-world applications (Substance Use and COVID-19), including tests against LLM-simulated and actual attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations of response correctness and relevance, we confirm that an external OOD detector is crucial for maintaining response relevance.</li>
</ul>

<h3>Title: CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment</h3>
<ul>
<li><strong>Authors: </strong>Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, Xiao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02298">https://arxiv.org/abs/2508.02298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02298">https://arxiv.org/pdf/2508.02298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02298]] CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment(https://arxiv.org/abs/2508.02298)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback, helping to mitigate reward hacking. However, current RLVR methods typically treat whole responses as single actions, assigning the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies and inefficient learning. Methods like PPO provide credit assignment through value estimation, but often yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-by-step judgments for each reasoning step, but they require high-quality process supervision labels and are time-consuming when applied in online reinforcement learning (RL). To overcome these limitations, we introduce a simple but efficient method Credit Assignment Policy Optimization (CAPO). Given a reasoning response rollout from the policy model, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass, thereby providing verifiable token-level rewards to refine the tokens that were originally assigned identical rule-based rewards. This enables more fine-grained credit assignment in an effective way. Furthermore, to enhance the accuracy and robustness of CAPO, we employ voting mechanisms that scale with the number of generated critiques. Extensive experiments using different backbones like Llama and Qwen models and in different sizes show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across six challenging mathematical benchmarks and three out-of-domain benchmarks.</li>
</ul>

<h3>Title: Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Seletkov, Sophie Starck, Ayhan Can Erdur, Yundi Zhang, Daniel Rueckert, Rickmer Braren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02307">https://arxiv.org/abs/2508.02307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02307">https://arxiv.org/pdf/2508.02307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02307]] Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment(https://arxiv.org/abs/2508.02307)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Reliable preclinical disease risk assessment is essential to move public healthcare from reactive treatment to proactive identification and prevention. However, image-based risk prediction algorithms often consider one condition at a time and depend on hand-crafted features obtained through segmentation tools. We propose a whole-body self-supervised representation learning method for the preclinical disease risk assessment under a competing risk modeling. This approach outperforms whole-body radiomics in multiple diseases, including cardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive pulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a preclinical screening scenario and subsequently combining with cardiac MRI, it sharpens further the prediction for CVD subgroups: ischemic heart disease (IHD), hypertensive diseases (HD), and stroke. The results indicate the translational potential of whole-body representations as a standalone screening modality and as part of a multi-modal framework within clinical workflows for early personalized risk stratification. The code is available at this https URL</li>
</ul>

<h3>Title: LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training</h3>
<ul>
<li><strong>Authors: </strong>Sikui Zhang, Guangze Gao, Ziyun Gan, Chunfeng Yuan, Zefeng Lin, Houwen Peng, Bing Li, Weiming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02308">https://arxiv.org/abs/2508.02308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02308">https://arxiv.org/pdf/2508.02308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02308]] LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training(https://arxiv.org/abs/2508.02308)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) experience significant performance degradation when the input exceeds the pretraining context window, primarily due to the out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent studies mitigate this problem by remapping OOD positions into the in-distribution range with fixed mapping strategies, ignoring the dynamic relationship between input length and the model's effective context window. To this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a training-free method that fully utilizes the model's effective context window for adaptive long-context scaling in LLMs. Motivated by the left-skewed frequency distribution of relative positions, LaMPE establishes a dynamic relationship between mapping length and input length through a parametric scaled sigmoid function to adaptively allocate positional capacity across varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention mechanism that strategically allocates positional resolution across different sequence regions to capture both fine-grained locality and long-range dependencies. Our method can be seamlessly applied to a wide range of RoPE-based LLMs without training. Extensive experiments on three representative LLMs across five mainstream long-context benchmarks demonstrate that LaMPE achieves significant performance improvements compared to existing length extrapolation methods. The code will be released at this https URL.</li>
</ul>

<h3>Title: A Survey on Data Security in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kang Chen, Xiuze Zhou, Yuanguo Lin, Jinhe Su, Yuanhui Yu, Li Shen, Fan Lin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02312">https://arxiv.org/abs/2508.02312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02312">https://arxiv.org/pdf/2508.02312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02312]] A Survey on Data Security in Large Language Models(https://arxiv.org/abs/2508.02312)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, robust, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), now a foundation in advancing natural language processing, power applications such as text generation, machine translation, and conversational systems. Despite their transformative potential, these models inherently rely on massive amounts of training data, often collected from diverse and uncurated sources, which exposes them to serious data security risks. Harmful or malicious data can compromise model behavior, leading to issues such as toxic output, hallucinations, and vulnerabilities to threats such as prompt injection or data poisoning. As LLMs continue to be integrated into critical real-world systems, understanding and addressing these data-centric security risks is imperative to safeguard user trust and system reliability. This survey offers a comprehensive overview of the main data security risks facing LLMs and reviews current defense strategies, including adversarial training, RLHF, and data augmentation. Additionally, we categorize and analyze relevant datasets used for assessing robustness and security across different domains, providing guidance for future research. Finally, we highlight key research directions that focus on secure model updates, explainability-driven defenses, and effective governance frameworks, aiming to promote the safe and responsible development of LLM technology. This work aims to inform researchers, practitioners, and policymakers, driving progress toward data security in LLMs.</li>
</ul>

<h3>Title: NMS: Efficient Edge DNN Training via Near-Memory Sampling on Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Boran Zhao, Haiduo Huang, Qiwei Dang, Wenzhe Zhao, Tian Xia, Pengju Ren</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02313">https://arxiv.org/abs/2508.02313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02313">https://arxiv.org/pdf/2508.02313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02313]] NMS: Efficient Edge DNN Training via Near-Memory Sampling on Manifolds(https://arxiv.org/abs/2508.02313)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Training deep neural networks (DNNs) on edge devices has attracted increasing attention due to its potential to address challenges related to domain adaptation and privacy preservation. However, DNNs typically rely on large datasets for training, which results in substantial energy consumption, making the training in edge devices impractical. Some dataset compression methods have been proposed to solve this challenge. For instance, the coreset selection and dataset distillation reduce the training cost by selecting and generating representative samples respectively. Nevertheless, these methods have two significant defects: (1) The necessary of leveraging a DNN model to evaluate the quality of representative samples, which inevitably introduces inductive bias of DNN, resulting in a severe generalization issue; (2) All training images require multiple accesses to the DDR via long-distance PCB connections, leading to substantial energy overhead. To address these issues, inspired by the nonlinear manifold stationary of the human brain, we firstly propose a DNN-free sample-selecting algorithm, called DE-SNE, to improve the generalization issue. Secondly, we innovatively utilize the near-memory computing technique to implement DE-SNE, thus only a small fraction of images need to access the DDR via long-distance PCB. It significantly reduces DDR energy consumption. As a result, we build a novel expedited DNN training system with a more efficient in-place Near-Memory Sampling characteristic for edge devices, dubbed NMS. As far as we know, our NMS is the first DNN-free near-memory sampling technique that can effectively alleviate generalization issues and significantly reduce DDR energy caused by dataset access. The experimental results show that our NMS outperforms the current state-of-the-art (SOTA) approaches, namely DQ, DQAS, and NeSSA, in model accuracy.</li>
</ul>

<h3>Title: VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo</h3>
<ul>
<li><strong>Authors: </strong>Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02317">https://arxiv.org/abs/2508.02317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02317">https://arxiv.org/pdf/2508.02317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02317]] VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo(https://arxiv.org/abs/2508.02317)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.</li>
</ul>

<h3>Title: Is Uncertainty Quantification a Viable Alternative to Learned Deferral?</h3>
<ul>
<li><strong>Authors: </strong>Anna M. Wundram, Christian F. Baumgartner</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02319">https://arxiv.org/abs/2508.02319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02319">https://arxiv.org/pdf/2508.02319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02319]] Is Uncertainty Quantification a Viable Alternative to Learned Deferral?(https://arxiv.org/abs/2508.02319)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) holds the potential to dramatically improve patient care. However, it is not infallible, necessitating human-AI-collaboration to ensure safe implementation. One aspect of AI safety is the models' ability to defer decisions to a human expert when they are likely to misclassify autonomously. Recent research has focused on methods that learn to defer by optimising a surrogate loss function that finds the optimal trade-off between predicting a class label or deferring. However, during clinical translation, models often face challenges such as data shift. Uncertainty quantification methods aim to estimate a model's confidence in its predictions. However, they may also be used as a deferral strategy which does not rely on learning from specific training distribution. We hypothesise that models developed to quantify uncertainty are more robust to out-of-distribution (OOD) input than learned deferral models that have been trained in a supervised fashion. To investigate this hypothesis, we constructed an extensive evaluation study on a large ophthalmology dataset, examining both learned deferral models and established uncertainty quantification methods, assessing their performance in- and out-of-distribution. Specifically, we evaluate their ability to accurately classify glaucoma from fundus images while deferring cases with a high likelihood of error. We find that uncertainty quantification methods may be a promising choice for AI deferral.</li>
</ul>

<h3>Title: CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02322">https://arxiv.org/abs/2508.02322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02322">https://arxiv.org/pdf/2508.02322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02322]] CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis(https://arxiv.org/abs/2508.02322)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level pruning, merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under pruning ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.</li>
</ul>

<h3>Title: Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth Distillation from Single Images</h3>
<ul>
<li><strong>Authors: </strong>Philipp Wulff, Felix Wimbauer, Dominik Muhle, Daniel Cremers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02323">https://arxiv.org/abs/2508.02323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02323">https://arxiv.org/pdf/2508.02323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02323]] Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth Distillation from Single Images(https://arxiv.org/abs/2508.02323)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Volumetric scene reconstruction from a single image is crucial for a broad range of applications like autonomous driving and robotics. Recent volumetric reconstruction methods achieve impressive results, but generally require expensive 3D ground truth or multi-view supervision. We propose to leverage pre-trained 2D diffusion models and depth prediction models to generate synthetic scene geometry from a single image. This can then be used to distill a feed-forward scene reconstruction model. Our experiments on the challenging KITTI-360 and Waymo datasets demonstrate that our method matches or outperforms state-of-the-art baselines that use multi-view supervision, and offers unique advantages, for example regarding dynamic scenes.</li>
</ul>

<h3>Title: CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions</h3>
<ul>
<li><strong>Authors: </strong>Ziteng Wang, Siqi Yang, Limeng Qiao, Lin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02329">https://arxiv.org/abs/2508.02329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02329">https://arxiv.org/pdf/2508.02329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02329]] CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions(https://arxiv.org/abs/2508.02329)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite the success of Vision-Language Models (VLMs) like CLIP in aligning vision and language, their proficiency in detailed, fine-grained visual comprehension remains a key challenge. We present CLIP-IN, a novel framework that bolsters CLIP's fine-grained perception through two core innovations. Firstly, we leverage instruction-editing datasets, originally designed for image manipulation, as a unique source of hard negative image-text pairs. Coupled with a symmetric hard negative contrastive loss, this enables the model to effectively distinguish subtle visual-semantic differences. Secondly, CLIP-IN incorporates long descriptive captions, utilizing rotary positional encodings to capture rich semantic context often missed by standard CLIP. Our experiments demonstrate that CLIP-IN achieves substantial gains on the MMVP benchmark and various fine-grained visual recognition tasks, without compromising robust zero-shot performance on broader classification and retrieval tasks. Critically, integrating CLIP-IN's visual representations into Multimodal Large Language Models significantly reduces visual hallucinations and enhances reasoning abilities. This work underscores the considerable potential of synergizing targeted, instruction-based contrastive learning with comprehensive descriptive information to elevate the fine-grained understanding of VLMs.</li>
</ul>

<h3>Title: BOOST: Bayesian Optimization with Optimal Kernel and Acquisition Function Selection Technique</h3>
<ul>
<li><strong>Authors: </strong>Joon-Hyun Park, Mujin Cheon, Dong-Yeun Koh</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02332">https://arxiv.org/abs/2508.02332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02332">https://arxiv.org/pdf/2508.02332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02332]] BOOST: Bayesian Optimization with Optimal Kernel and Acquisition Function Selection Technique(https://arxiv.org/abs/2508.02332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The performance of Bayesian optimization (BO), a highly sample-efficient method for expensive black-box problems, is critically governed by the selection of its hyperparameters, including the kernel and acquisition functions. This presents a challenge: an inappropriate combination of these can lead to poor performance and wasted evaluations. While individual improvements to kernel functions (e.g., tree-based kernels, deep kernel learning) and acquisition functions (e.g., multi-step lookahead, tree-based planning) have been explored, the joint and autonomous selection of the best pair of these fundamental hyperparameters has been overlooked. This forces practitioners to rely on heuristics or costly manual training. We propose a simple yet effective framework, BOOST (Bayesian Optimization with Optimal Kernel and Acquisition Function Selection Technique), that automates this selection. BOOST utilizes a lightweight, offline evaluation stage to predict the performance of various kernel-acquisition function pairs and identify the most suitable configuration before expensive evaluations. BOOST partitions data-in-hand into two subsets: a reference subset and a query subset, and it prepares all possible kernel-acquisition pairs from the user's chosen candidates. For each configuration, BOOST conducts internal BO runs using the reference subset, evaluating how effectively each pair guides the search toward the optimum in the unknown query subset, thereby identifying the configuration with the best retrospective performance for future optimization. Experiments on both synthetic benchmark functions and real-world hyperparameter optimization tasks demonstrate that BOOST consistently outperforms standard BO approaches with fixed hyperparameters, highlighting its effectiveness and robustness in diverse problem landscapes.</li>
</ul>

<h3>Title: Correspondence-Free Fast and Robust Spherical Point Pattern Registration</h3>
<ul>
<li><strong>Authors: </strong>Anik Sarker, Alan T. Asbeck</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02339">https://arxiv.org/abs/2508.02339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02339">https://arxiv.org/pdf/2508.02339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02339]] Correspondence-Free Fast and Robust Spherical Point Pattern Registration(https://arxiv.org/abs/2508.02339)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing methods for rotation estimation between two spherical ($\mathbb{S}^2$) patterns typically rely on spherical cross-correlation maximization between two spherical function. However, these approaches exhibit computational complexities greater than cubic $O(n^3)$ with respect to rotation space discretization and lack extensive evaluation under significant outlier contamination. To this end, we propose a rotation estimation algorithm between two spherical patterns with linear time complexity $O(n)$. Unlike existing spherical-function-based methods, we explicitly represent spherical patterns as discrete 3D point sets on the unit sphere, reformulating rotation estimation as a spherical point-set alignment (i.e., Wahba problem for 3D unit vectors). Given the geometric nature of our formulation, our spherical pattern alignment algorithm naturally aligns with the Wahba problem framework for 3D unit vectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical Pattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a hybrid approach (SPMC+FRS) that combines the advantages of the previous two methods. Our experiments demonstrate that in the $\mathbb{S}^2$ domain and in correspondence-free settings, our algorithms are over 10x faster and over 10x more accurate than current state-of-the-art methods for the Wahba problem with outliers. We validate our approach through extensive simulations on a new dataset of spherical patterns, the ``Robust Vector Alignment Dataset. "Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud Registration (PCR) and (ii) rotation estimation for spherical images.</li>
</ul>

<h3>Title: Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search</h3>
<ul>
<li><strong>Authors: </strong>Fan Hu, Zijie Xin, Xirong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02340">https://arxiv.org/abs/2508.02340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02340">https://arxiv.org/pdf/2508.02340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02340]] Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search(https://arxiv.org/abs/2508.02340)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Ad-hoc Video Search (AVS) involves using a textual query to search for multiple relevant videos in a large collection of unlabeled short videos. The main challenge of AVS is the visual diversity of relevant videos. A simple query such as "Find shots of a man and a woman dancing together indoors" can span a multitude of environments, from brightly lit halls and shadowy bars to dance scenes in black-and-white animations. It is therefore essential to retrieve relevant videos as comprehensively as possible. Current solutions for the AVS task primarily fuse multiple features into one or more common spaces, yet overlook the need for diverse spaces. To fully exploit the expressive capability of individual features, we propose LPD, short for Learning Partially Decorrelated common spaces. LPD incorporates two key innovations: feature-specific common space construction and the de-correlation loss. Specifically, LPD learns a separate common space for each video and text feature, and employs de-correlation loss to diversify the ordering of negative samples across different spaces. To enhance the consistency of multi-space convergence, we designed an entropy-based fair multi-space triplet ranking loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces highlight its ability to enhance result diversity.</li>
</ul>

<h3>Title: MicroMix: Efficient Mixed-Precision Quantization with Microscaling Formats for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenyuan Liu, Haoqian Meng, Yilun Luo, Peng Zhang, Xindian Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02343">https://arxiv.org/abs/2508.02343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02343">https://arxiv.org/pdf/2508.02343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02343]] MicroMix: Efficient Mixed-Precision Quantization with Microscaling Formats for Large Language Models(https://arxiv.org/abs/2508.02343)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Quantization significantly accelerates inference in large language models (LLMs) by replacing original high-precision matrices with low-precision counterparts. Recent advances in weight-activation quantization have primarily focused on mapping both weights and activations to the INT4 format. Although the new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x speedup over FP16, existing INT4-based kernels fail to fully exploit this capability due to mismatched data formats. To bridge this gap, we propose MicroMix, a co-designed mixed-precision quantization algorithm and matrix multiplication kernel based on Microscaling (MX) data formats. Tailored for the Blackwell architecture, the MicroMix kernel supports arbitrary combinations of MXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a favorable trade-off between accuracy and efficiency for each linear layer, we introduce quantization thresholds that identify activation elements where lower-precision formats (MXFP4 or MXFP6) incur excessive quantization error. Our algorithm selectively allocates higher-precision channels to preserve accuracy while maintaining compute efficiency. MicroMix achieves competitive or superior performance across diverse downstream tasks, including zero-shot and few-shot learning, language modeling, code generation, and mathematical reasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX 5090) GPUs, our kernel delivers at least 20% faster execution than TensorRT-FP8. Furthermore, when applied to various Llama and Qwen models, MicroMix consistently improves prefill latency and memory efficiency across a range of batch sizes compared to TensorRT baselines. Our code is available at this https URL.</li>
</ul>

<h3>Title: mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera</h3>
<ul>
<li><strong>Authors: </strong>Byeonggyu Park, Hee-Yeun Kim, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seong-Woo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02348">https://arxiv.org/abs/2508.02348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02348">https://arxiv.org/pdf/2508.02348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02348]] mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera(https://arxiv.org/abs/2508.02348)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban environments poses a significant challenge for autonomous driving systems. While mmWave radar has demonstrated potential for detecting objects in such scenarios, the 2D radar point cloud (PCD) data is susceptible to distortions caused by multipath reflections, making accurate spatial inference difficult. Additionally, although camera images provide high-resolution visual information, they lack depth perception and cannot directly observe objects in NLoS regions. In this paper, we propose a novel framework that interprets radar PCD through road layout inferred from camera for localization of NLoS pedestrians. The proposed method leverages visual information from the camera to interpret 2D radar PCD, enabling spatial scene reconstruction. The effectiveness of the proposed approach is validated through experiments conducted using a radar-camera system mounted on a real vehicle. The localization performance is evaluated using a dataset collected in outdoor NLoS driving environments, demonstrating the practical applicability of the method.</li>
</ul>

<h3>Title: Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zhang, Shu Yang, Junchao Wu, Derek F. Wong, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02360">https://arxiv.org/abs/2508.02360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02360">https://arxiv.org/pdf/2508.02360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02360]] Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models(https://arxiv.org/abs/2508.02360)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models on a political topic will significantly manipulate their political stance on various issues and unintentionally affect their stance on unrelated topics. While previous studies have proposed this issue, there is still a lack of understanding regarding the internal representations of these stances and the mechanisms that lead to unintended cross-topic generalization. In this paper, we systematically explore the internal mechanisms underlying this phenomenon from a neuron-level perspective and how to mitigate the cross-topic generalization of political fine-tuning. Firstly, we propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify two distinct types of political neurons: general political neurons, which govern stance across multiple political topics, and topic-specific neurons} that affect the model's political stance on individual topics. We find the existence of these political neuron types across four models and datasets through activation patching experiments. Leveraging these insights, we introduce InhibitFT, an inhibition-based fine-tuning method, effectively mitigating the cross-topic stance generalization. Experimental results demonstrate the robustness of identified neuron types across various models and datasets, and show that InhibitFT significantly reduces the cross-topic stance generalization by 20% on average, while preserving topic-specific performance. Moreover, we demonstrate that selectively inhibiting only 5% of neurons is sufficient to effectively mitigate the cross-topic stance generalization.</li>
</ul>

<h3>Title: Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering</h3>
<ul>
<li><strong>Authors: </strong>Xu Wang, Shengeng Tang, Fei Wang, Lechao Cheng, Dan Guo, Feng Xue, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02362">https://arxiv.org/abs/2508.02362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02362">https://arxiv.org/pdf/2508.02362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02362]] Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering(https://arxiv.org/abs/2508.02362)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generating semantically coherent and visually accurate talking faces requires bridging the gap between linguistic meaning and facial articulation. Although audio-driven methods remain prevalent, their reliance on high-quality paired audio visual data and the inherent ambiguity in mapping acoustics to lip motion pose significant challenges in terms of scalability and robustness. To address these issues, we propose Text2Lip, a viseme-centric framework that constructs an interpretable phonetic-visual bridge by embedding textual input into structured viseme sequences. These mid-level units serve as a linguistically grounded prior for lip motion prediction. Furthermore, we design a progressive viseme-audio replacement strategy based on curriculum learning, enabling the model to gradually transition from real audio to pseudo-audio reconstructed from enhanced viseme features via cross-modal attention. This allows for robust generation in both audio-present and audio-free scenarios. Finally, a landmark-guided renderer synthesizes photorealistic facial videos with accurate lip synchronization. Extensive evaluations show that Text2Lip outperforms existing approaches in semantic fidelity, visual realism, and modality robustness, establishing a new paradigm for controllable and flexible talking face generation. Our project homepage is this https URL.</li>
</ul>

<h3>Title: Transport-Guided Rectified Flow Inversion: Improved Image Editing Using Optimal Transport Theory</h3>
<ul>
<li><strong>Authors: </strong>Marian Lupascu, Mihai-Sorin Stupariu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02363">https://arxiv.org/abs/2508.02363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02363">https://arxiv.org/pdf/2508.02363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02363]] Transport-Guided Rectified Flow Inversion: Improved Image Editing Using Optimal Transport Theory(https://arxiv.org/abs/2508.02363)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Effective image inversion in rectified flow models - mapping real images to editable latent representations - is crucial for practical image editing applications; however, achieving optimal balance between reconstruction fidelity and editing flexibility remains a fundamental challenge. In this work, we introduce the Optimal Transport Inversion Pipeline (OTIP), a zero-shot framework that leverages optimal transport theory to guide the inversion process in rectified flow models. Our underlying hypothesis is that incorporating transport-based guidance during the reverse diffusion process can effectively balance reconstruction accuracy and editing controllability through principled trajectory optimization. The method computes optimal transport paths between image and noise distributions while maintaining computational efficiency. Our approach achieves high-fidelity reconstruction with LPIPS scores of 0.001 and SSIM of 0.992 on face editing benchmarks, demonstrating superior preservation of fine-grained details compared to existing methods. We evaluate the framework across multiple editing tasks, observing 7.8% to 12.9% improvements in reconstruction loss over RF-Inversion on the LSUN-Bedroom and LSUN-Church datasets, respectively. For semantic face editing, our method achieves an 11.2% improvement in identity preservation and a 1.6% enhancement in perceptual quality, while maintaining computational efficiency comparable to baseline approaches. Qualitatively, our method produces visually compelling edits with superior semantic consistency and fine-grained detail preservation across diverse editing scenarios. Code is available at: this https URL</li>
</ul>

<h3>Title: A Novel Sliced Fused Gromov-Wasserstein Distance</h3>
<ul>
<li><strong>Authors: </strong>Moritz Piening, Robert Beinert</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02364">https://arxiv.org/abs/2508.02364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02364">https://arxiv.org/pdf/2508.02364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02364]] A Novel Sliced Fused Gromov-Wasserstein Distance(https://arxiv.org/abs/2508.02364)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Gromov--Wasserstein (GW) distance and its fused extension (FGW) are powerful tools for comparing heterogeneous data. Their computation is, however, challenging since both distances are based on non-convex, quadratic optimal transport (OT) problems. Leveraging 1D OT, a sliced version of GW has been proposed to lower the computational burden. Unfortunately, this sliced version is restricted to Euclidean geometry and loses invariance to isometries, strongly limiting its application in practice. To overcome these issues, we propose a novel slicing technique for GW as well as for FGW that is based on an appropriate lower bound, hierarchical OT, and suitable quadrature rules for the underlying 1D OT problems. Our novel sliced FGW significantly reduces the numerical effort while remaining invariant to isometric transformations and allowing the comparison of arbitrary geometries. We show that our new distance actually defines a pseudo-metric for structured spaces that bounds FGW from below and study its interpolation properties between sliced Wasserstein and GW. Since we avoid the underlying quadratic program, our sliced distance is numerically more robust and reliable than the original GW and FGW distance; especially in the context of shape retrieval and graph isomorphism testing.</li>
</ul>

<h3>Title: Language Model Guided Reinforcement Learning in Quantitative Trading</h3>
<ul>
<li><strong>Authors: </strong>Adam Darmanin, Vince Vella</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-fin.TR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02366">https://arxiv.org/abs/2508.02366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02366">https://arxiv.org/pdf/2508.02366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02366]] Language Model Guided Reinforcement Learning in Quantitative Trading(https://arxiv.org/abs/2508.02366)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Algorithmic trading requires short-term decisions aligned with long-term financial goals. While reinforcement learning (RL) has been explored for such tactical decisions, its adoption remains limited by myopic behavior and opaque policy rationale. In contrast, large language models (LLMs) have recently demonstrated strategic reasoning and multi-modal financial signal interpretation when guided by well-designed prompts. We propose a hybrid system where LLMs generate high-level trading strategies to guide RL agents in their actions. We evaluate (i) the rationale of LLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and Maximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results show improved return and risk metrics over standard RL.</li>
</ul>

<h3>Title: TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage Recognition System for Transportation Unit Identification</h3>
<ul>
<li><strong>Authors: </strong>Emre Gülsoylu, André Kelm, Lennart Bengtson, Matthias Hirsch, Christian Wilms, Tim Rolff, Janick Edinger, Simone Frintrop</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02372">https://arxiv.org/abs/2508.02372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02372">https://arxiv.org/pdf/2508.02372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02372]] TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage Recognition System for Transportation Unit Identification(https://arxiv.org/abs/2508.02372)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Identifying transportation units (TUs) is essential for improving the efficiency of port logistics. However, progress in this field has been hindered by the lack of publicly available benchmark datasets that capture the diversity and dynamics of real-world port environments. To address this gap, we present the TRUDI dataset-a comprehensive collection comprising 35,034 annotated instances across five categories: container, tank container, trailer, ID text, and logo. The images were captured at operational ports using both ground-based and aerial cameras, under a wide variety of lighting and weather conditions. For the identification of TUs-which involves reading the 11-digit alphanumeric ID typically painted on each unit-we introduce TITUS, a dedicated pipeline that operates in three stages: (1) segmenting the TU instances, (2) detecting the location of the ID text, and (3) recognising and validating the extracted ID. Unlike alternative systems, which often require similar scenes, specific camera angles or gate setups, our evaluation demonstrates that TITUS reliably identifies TUs from a range of camera perspectives and in varying lighting and weather conditions. By making the TRUDI dataset publicly available, we provide a robust benchmark that enables the development and comparison of new approaches. This contribution supports digital transformation efforts in multipurpose ports and helps to increase the efficiency of entire logistics chains.</li>
</ul>

<h3>Title: Analysis of Publicly Accessible Operational Technology and Associated Risks</h3>
<ul>
<li><strong>Authors: </strong>Matthew Rodda, Vasilios Mavroudis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02375">https://arxiv.org/abs/2508.02375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02375">https://arxiv.org/pdf/2508.02375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02375]] Analysis of Publicly Accessible Operational Technology and Associated Risks(https://arxiv.org/abs/2508.02375)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Operational Technology (OT) is an integral component of critical national infrastructure, enabling automation and control in industries such as energy, manufacturing, and transportation. However, OT networks, systems, and devices have been designed and deployed prioritising functionality rather than security. This leads to inherent vulnerabilities in many deployed systems when operational misconfigurations expose them to the internet. This report provides an up-to-date overview of the OT threat landscape exposed to the public internet and studies the affected protocols, vendors, software, and the geographic distribution of systems. Our findings reveal nearly 70,000 exposed OT devices globally, with significant concentrations in North America and Europe. Analysis of prevalent protocols (e.g., ModbusTCP, EtherNet/IP, S7) shows that many devices expose detailed identifying information, including outdated firmware versions with known critical vulnerabilities that remain unpatched for years after disclosure. Furthermore, we demonstrate how automated analysis of screenshots can uncover exposed graphical interfaces of Human Machine Interfaces (HMIs) and Supervisory Control and Data Acquisition (SCADA) systems, highlighting diverse pathways for potential unauthorized access and underscoring the risks to industrial processes and critical infrastructure.</li>
</ul>

<h3>Title: Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction: A Pruning Framework for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zuxin Ma, Yunhe Cui, Yongbin Qin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02381">https://arxiv.org/abs/2508.02381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02381">https://arxiv.org/pdf/2508.02381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02381]] Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction: A Pruning Framework for LLMs(https://arxiv.org/abs/2508.02381)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Non-uniform structured network pruning methods can effectively reduce Large Language Model (LLM) size by eliminating redundant channels or layers, offering lower performance degradation than uniform strategies. However, existing non-uniform methods rely heavily on manually designed pruning policies (e.g., layer importance and scaling factors), and therefore cannot efficiently adapt to scenarios with dynamic pruning ratio requirements. Additionly, a critical bottleneck -- the time-consuming evaluation of pruning policies -- further limits the feasibility of iteratively and dynamically finding optimal pruning policies. To address these limitations, we propose PPF (Predictive Pruning Framework), a novel pruning framework for LLMs that eliminates manual design dependencies via second-level performance prediction. PPF not only supports real-time pruning decisions under dynamic pruning ratios but is also applicable to static pruning scenarios. It employs an agent for producing adaptive and real-time pruning actions, while a lightweight performance predictor that can evaluate a pruning policy in seconds, significantly speeding up the iterative optimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can generate dynamic/static pruning policies and it reduces perplexity by up to 33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods, outperforming manually designed pruning policies. The performance predictor achieves second-level performance prediction with high accuracy (prediction error < 0.0011). It reduces the mean evaluation latency from minute-level (1 minute and 38.02 seconds of test-set evaluation methods) to second-level (1.52 second), achieving over 64 times speedup. Our code will be available at this https URL .</li>
</ul>

<h3>Title: Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Feng, Hebei Gao, Hong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02386">https://arxiv.org/abs/2508.02386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02386">https://arxiv.org/pdf/2508.02386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02386]] Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection(https://arxiv.org/abs/2508.02386)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised instance segmentation and object detection. COLER first uses our developed CutOnce to generate coarse pseudo labels, then enables the detector to learn from these masks. CutOnce applies Normalized Cut only once and does not rely on any clustering methods, but it can generate multiple object masks in an image. We have designed several novel yet simple modules that not only allow CutOnce to fully leverage the object discovery capabilities of self-supervised models, but also free it from reliance on mask post-processing. During training, COLER achieves strong performance without requiring specially designed loss functions for pseudo labels, and its performance is further improved through self-training. COLER is a zero-shot unsupervised model that outperforms previous state-of-the-art methods on multiple this http URL believe our method can help advance the field of unsupervised object localization.</li>
</ul>

<h3>Title: $ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise</h3>
<ul>
<li><strong>Authors: </strong>Jialiang Wang, Xiong Zhou, Deming Zhai, Junjun Jiang, Xiangyang Ji, Xianming Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02387">https://arxiv.org/abs/2508.02387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02387">https://arxiv.org/pdf/2508.02387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02387]] $ε$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise(https://arxiv.org/abs/2508.02387)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Noisy labels pose a common challenge for training accurate deep neural networks. To mitigate label noise, prior studies have proposed various robust loss functions to achieve noise tolerance in the presence of label noise, particularly symmetric losses. However, they usually suffer from the underfitting issue due to the overly strict symmetric condition. In this work, we propose a simple yet effective approach for relaxing the symmetric condition, namely $\epsilon$-softmax, which simply modifies the outputs of the softmax layer to approximate one-hot vectors with a controllable error $\epsilon$. Essentially, $\epsilon$-softmax not only acts as an alternative for the softmax layer, but also implicitly plays the crucial role in modifying the loss function. We prove theoretically that $\epsilon$-softmax can achieve noise-tolerant learning with controllable excess risk bound for almost any loss function. Recognizing that $\epsilon$-softmax-enhanced losses may slightly reduce fitting ability on clean datasets, we further incorporate them with one symmetric loss, thereby achieving a better trade-off between robustness and effective learning. Extensive experiments demonstrate the superiority of our method in mitigating synthetic and real-world label noise. The code is available at this https URL.</li>
</ul>

<h3>Title: CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02401">https://arxiv.org/abs/2508.02401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02401">https://arxiv.org/pdf/2508.02401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02401]] CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation(https://arxiv.org/abs/2508.02401)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs. To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: this https URL.</li>
</ul>

<h3>Title: Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yimeng Liu, Maolin Gan, Huaili Zeng, Li Liu, Younsuk Dong, Zhichao Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02409">https://arxiv.org/abs/2508.02409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02409">https://arxiv.org/pdf/2508.02409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02409]] Hydra: Accurate Multi-Modal Leaf Wetness Sensing with mm-Wave and Camera Fusion(https://arxiv.org/abs/2508.02409)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Leaf Wetness Duration (LWD), the time that water remains on leaf surfaces, is crucial in the development of plant diseases. Existing LWD detection lacks standardized measurement techniques, and variations across different plant characteristics limit its effectiveness. Prior research proposes diverse approaches, but they fail to measure real natural leaves directly and lack resilience in various environmental conditions. This reduces the precision and robustness, revealing a notable practical application and effectiveness gap in real-world agricultural settings. This paper presents Hydra, an innovative approach that integrates millimeter-wave (mm-Wave) radar with camera technology to detect leaf wetness by determining if there is water on the leaf. We can measure the time to determine the LWD based on this detection. Firstly, we design a Convolutional Neural Network (CNN) to selectively fuse multiple mm-Wave depth images with an RGB image to generate multiple feature images. Then, we develop a transformer-based encoder to capture the inherent connection among the multiple feature images to generate a feature map, which is further fed to a classifier for detection. Moreover, we augment the dataset during training to generalize our model. Implemented using a frequency-modulated continuous-wave (FMCW) radar within the 76 to 81 GHz band, Hydra's performance is meticulously evaluated on plants, demonstrating the potential to classify leaf wetness with up to 96% accuracy across varying scenarios. Deploying Hydra in the farm, including rainy, dawn, or poorly light nights, it still achieves an accuracy rate of around 90%.</li>
</ul>

<h3>Title: HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wang, Hao Si, Fan Zhang, Xiaoya Zhou, Dengdi Sun, Wanli Lyu, Qingquan Yang, Jin Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02411">https://arxiv.org/abs/2508.02411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02411">https://arxiv.org/pdf/2508.02411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02411]] HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis(https://arxiv.org/abs/2508.02411)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multivariate time series analysis has long been one of the key research topics in the field of artificial intelligence. However, analyzing complex time series data remains a challenging and unresolved problem due to its high dimensionality, dynamic nature, and complex interactions among variables. Inspired by the strong structural modeling capability of hypergraphs, this paper proposes a novel hypergraph-based time series transformer backbone network, termed HGTS-Former, to address the multivariate coupling in time series data. Specifically, given the multivariate time series signal, we first normalize and embed each patch into tokens. Then, we adopt the multi-head self-attention to enhance the temporal representation of each patch. The hierarchical hypergraphs are constructed to aggregate the temporal patterns within each channel and fine-grained relations between different variables. After that, we convert the hyperedge into node features through the EdgeToNode module and adopt the feed-forward network to further enhance the output features. Extensive experiments conducted on two multivariate time series tasks and eight datasets fully validated the effectiveness of our proposed HGTS-Former. The source code will be released on this https URL.</li>
</ul>

<h3>Title: ASMR: Angular Support for Malfunctioning Client Resilience in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Mirko Konstantin, Moritz Fuchs, Anirban Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02414">https://arxiv.org/abs/2508.02414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02414">https://arxiv.org/pdf/2508.02414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02414]] ASMR: Angular Support for Malfunctioning Client Resilience in Federated Learning(https://arxiv.org/abs/2508.02414)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) allows the training of deep neural networks in a distributed and privacy-preserving manner. However, this concept suffers from malfunctioning updates sent by the attending clients that cause global model performance degradation. Reasons for this malfunctioning might be technical issues, disadvantageous training data, or malicious attacks. Most of the current defense mechanisms are meant to require impractical prerequisites like knowledge about the number of malfunctioning updates, which makes them unsuitable for real-world applications. To counteract these problems, we introduce a novel method called Angular Support for Malfunctioning Client Resilience (ASMR), that dynamically excludes malfunctioning clients based on their angular distance. Our novel method does not require any hyperparameters or knowledge about the number of malfunctioning clients. Our experiments showcase the detection capabilities of ASMR in an image classification task on a histopathological dataset, while also presenting findings on the significance of dynamically adapting decision boundaries.</li>
</ul>

<h3>Title: Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens</h3>
<ul>
<li><strong>Authors: </strong>Haohan Zheng, Zhenguo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02419">https://arxiv.org/abs/2508.02419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02419">https://arxiv.org/pdf/2508.02419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02419]] Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens(https://arxiv.org/abs/2508.02419)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) have demonstrated remarkable multimodal comprehension and reasoning capabilities, but they still suffer from severe object hallucination. Previous studies primarily attribute the flaw to linguistic prior caused by the scale mismatch between visual encoders and large language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs, generating descriptions inconsistent with visual cues. However, through an in-depth investigation of the hallucinated mechanisms, we empirically reveal a previously overlooked phenomenon: LVLMs may ignore not only visual information but also textual modality during hallucination, a behavior termed as modality bias, which indicates that LVLMs struggle to simultaneously attend to both visual and textual modalities, leading to fragmented understanding of user-provided instructions. Based on this observation, we propose a simple yet effective training-free method to mitigate object hallucination. Concretely, we intervene and adjust the attention weights of textual and visual tokens, balancing cross-modal compatibility for better alignment with user intentions. Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's overreliance on its parametric knowledge, synergistically enhancing our attention manipulation. Extensive experiments confirm the widespread presence of modality bias in LVLMs. Notably, our method effectively mitigates hallucination across multiple open-source LVLMs and benchmarks, highlighting its generalizability and efficacy.</li>
</ul>

<h3>Title: AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications</h3>
<ul>
<li><strong>Authors: </strong>Robin Nowak, Patrick Figge, Carolin Haeussler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02430">https://arxiv.org/abs/2508.02430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02430">https://arxiv.org/pdf/2508.02430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02430]] AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications(https://arxiv.org/abs/2508.02430)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Measuring innovation often relies on context-specific proxies and on expert evaluation. Hence, empirical innovation research is often limited to settings where such data is available. We investigate how large language models (LLMs) can be leveraged to overcome the constraints of manual expert evaluations and assist researchers in measuring innovation. We design an LLM framework that reliably approximates domain experts' assessment of innovation from unstructured text data. We demonstrate the performance and broad applicability of this framework through two studies in different contexts: (1) the innovativeness of software application updates and (2) the originality of user-generated feedback and improvement ideas in product reviews. We compared the performance (F1-score) and reliability (consistency rate) of our LLM framework against alternative measures used in prior innovation studies, and to state-of-the-art machine learning- and deep learning-based models. The LLM framework achieved higher F1-scores than the other approaches, and its results are highly consistent (i.e., results do not change across runs). This article equips R&D personnel in firms, as well as researchers, reviewers, and editors, with the knowledge and tools to effectively use LLMs for measuring innovation and evaluating the performance of LLM-based innovation measures. In doing so, we discuss, the impact of important design decisions-including model selection, prompt engineering, training data size, training data distribution, and parameter settings-on performance and reliability. Given the challenges inherent in using human expert evaluation and existing text-based measures, our framework has important implications for harnessing LLMs as reliable, increasingly accessible, and broadly applicable research tools for measuring innovation.</li>
</ul>

<h3>Title: SoftPUF: a Software-Based Blockchain Framework using PUF and Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>S M Mostaq Hossain, Sheikh Ghafoor, Kumar Yelamarthi, Venkata Prasanth Yanambaka</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02438">https://arxiv.org/abs/2508.02438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02438">https://arxiv.org/pdf/2508.02438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02438]] SoftPUF: a Software-Based Blockchain Framework using PUF and Machine Learning(https://arxiv.org/abs/2508.02438)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Physically Unclonable Function (PUF) offers a secure and lightweight alternative to traditional cryptography for authentication due to their unique device fingerprint. However, their dependence on specialized hardware hinders their adoption in diverse applications. This paper proposes a novel blockchain framework that leverages SoftPUF, a software-based approach mimicking PUF. SoftPUF addresses the hardware limitations of traditional PUF, enabling secure and efficient authentication for a broader range of devices within a blockchain network. The framework utilizes a machine learning model trained on PUF data to generate unique, software-based keys for each device. These keys serve as secure identifiers for authentication on the blockchain, eliminating the need for dedicated hardware. This approach facilitates the integration of legacy devices from various domains, including cloud-based solutions, into the blockchain network. Additionally, the framework incorporates well-established defense mechanisms to ensure robust security against various attacks. This combined approach paves the way for secure and scalable authentication in diverse blockchain-based applications. Additionally, to ensure robust security, the system incorporates well-established defense mechanisms against various attacks, including 51%, phishing, routing, and Sybil attacks, into the blockchain network. This combined approach paves the way for secure and efficient authentication in a wider range of blockchain-based applications.</li>
</ul>

<h3>Title: Glioblastoma Overall Survival Prediction With Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yin Lin, iccardo Barbieri, Domenico Aquino, Giuseppe Lauria, Marina Grisoli, Elena De Momi, Alberto Redaelli, Simona Ferrante</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02439">https://arxiv.org/abs/2508.02439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02439">https://arxiv.org/pdf/2508.02439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02439]] Glioblastoma Overall Survival Prediction With Vision Transformers(https://arxiv.org/abs/2508.02439)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Glioblastoma is one of the most aggressive and common brain tumors, with a median survival of 10-15 months. Predicting Overall Survival (OS) is critical for personalizing treatment strategies and aligning clinical decisions with patient outcomes. In this study, we propose a novel Artificial Intelligence (AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images, exploiting Vision Transformers (ViTs) to extract hidden features directly from MRI images, eliminating the need of tumor segmentation. Unlike traditional approaches, our method simplifies the workflow and reduces computational resource requirements. The proposed model was evaluated on the BRATS dataset, reaching an accuracy of 62.5% on the test set, comparable to the top-performing methods. Additionally, it demonstrated balanced performance across precision, recall, and F1 score, overcoming the best model in these metrics. The dataset size limits the generalization of the ViT which typically requires larger datasets compared to convolutional neural networks. This limitation in generalization is observed across all the cited studies. This work highlights the applicability of ViTs for downsampled medical imaging tasks and establishes a foundation for OS prediction models that are computationally efficient and do not rely on segmentation.</li>
</ul>

<h3>Title: LatentPrompt: Optimizing Promts in Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Bystroński, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02452">https://arxiv.org/abs/2508.02452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02452">https://arxiv.org/pdf/2508.02452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02452]] LatentPrompt: Optimizing Promts in Latent Space(https://arxiv.org/abs/2508.02452)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances have shown that optimizing prompts for Large Language Models (LLMs) can significantly improve task performance, yet many optimization techniques rely on heuristics or manual exploration. We present LatentPrompt, a model-agnostic framework for prompt optimization that leverages latent semantic space to automatically generate, evaluate, and refine candidate prompts without requiring hand-crafted rules. Beginning with a set of seed prompts, our method embeds them in a continuous latent space and systematically explores this space to identify prompts that maximize task-specific performance. In a proof-of-concept study on the Financial PhraseBank sentiment classification benchmark, LatentPrompt increased classification accuracy by approximately 3 percent after a single optimization cycle. The framework is broadly applicable, requiring only black-box access to an LLM and an automatic evaluation metric, making it suitable for diverse domains and tasks.</li>
</ul>

<h3>Title: Thwart Me If You Can: An Empirical Analysis of Android Platform Armoring Against Stalkerware</h3>
<ul>
<li><strong>Authors: </strong>Malvika Jadhav, Wenxuan Bao, Vincent Bindschaedler</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02454">https://arxiv.org/abs/2508.02454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02454">https://arxiv.org/pdf/2508.02454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02454]] Thwart Me If You Can: An Empirical Analysis of Android Platform Armoring Against Stalkerware(https://arxiv.org/abs/2508.02454)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense</a></li>
<li><strong>Abstract: </strong>Stalkerware is a serious threat to individuals' privacy that is receiving increased attention from the security and privacy research communities. Existing works have largely focused on studying leading stalkerware apps, dual-purpose apps, monetization of stalkerware, or the experience of survivors. However, there remains a need to understand potential defenses beyond the detection-and-removal approach, which may not necessarily be effective in the context of stalkerware. In this paper, we perform a systematic analysis of a large corpus of recent Android stalkerware apps. We combine multiple analysis techniques to quantify stalkerware behaviors and capabilities and how these evolved over time. Our primary goal is understanding: how (and whether) recent Android platform changes -- largely designed to improve user privacy -- have thwarted stalkerware functionality; how stalkerware may have adapted as a result; and what we may conclude about potential defenses. Our investigation reveals new insights into tactics used by stalkerware and may inspire alternative defense strategies.</li>
</ul>

<h3>Title: Experimental Evaluation of Post-Quantum Homomorphic Encryption for Privacy-Preserving V2X Communication</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Al Mamun, Kyle Yates, Antsa Rakotondrafara, Mashrur Chowdhury, Ryann Cartor, Shuhong Gao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02461">https://arxiv.org/abs/2508.02461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02461">https://arxiv.org/pdf/2508.02461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02461]] Experimental Evaluation of Post-Quantum Homomorphic Encryption for Privacy-Preserving V2X Communication(https://arxiv.org/abs/2508.02461)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Intelligent Transportation Systems (ITS) fundamentally rely on vehicle-generated data for applications such as congestion monitoring and route optimization, making the preservation of user privacy a critical challenge. Homomorphic Encryption (HE) offers a promising solution by enabling computation on encrypted data without revealing underlying content. This study presents the first real-world experimental evaluation of three post-quantum secure HE schemes, i.e., Brakerski-Fan-Vercauteren (BFV), Brakerski-Gentry-Vaikuntanathan (BGV), and Cheon-Kim-Kim-Song (CKKS), for vehicular communication scenarios. Two representative privacy-preserving use cases are considered: encrypted vehicle counting and average speed aggregation. Experiments are conducted over both Wi-Fi and Ethernet to assess performance under wireless and wired vehicle-to-everything (V2X) settings. Results show that BFV and BGV are suitable for latency-tolerant applications such as intersection monitoring and regional traffic analysis, with total end-to-end latencies under 10 seconds. While CKKS experiences higher overhead, it remains viable for periodic encrypted aggregation of numerical data. The experimental results demonstrate that HE can be feasibly deployed in ITS environments under 128-bit post-quantum security, provided that scheme-specific latency constraints are considered. This reinforces its potential to serve as a foundational tool for secure and privacy-preserving V2X data processing.</li>
</ul>

<h3>Title: SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yonghuang Wu, Wenwen Zeng, Xuan Xie, Chengqian Zhao, Guoqing Wu, Jinhua Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02464">https://arxiv.org/abs/2508.02464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02464">https://arxiv.org/pdf/2508.02464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02464]] SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models(https://arxiv.org/abs/2508.02464)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models like Segment Anything Model (SAM) excel in promptable segmentation but suffer from an intent gap: they segment only explicitly prompted objects, failing to generalize to semantically related instances implicitly desired by users. This limitation is critical in domains with dense homogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual prompts typically yield incomplete results, rendering dense annotations impractical due to prohibitive cost. To bridge this gap, we introduce SAMPO (Segment Anything Model with Preference Optimization), a novel framework that teaches visual foundation models to infer high-level categorical intent from sparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO optimizes models to implicitly capture target-class characteristics through preference optimization. This approach, which operates without dependency on language models, enables robust multi-object segmentation even under sparse prompting and demonstrates superior data efficiency during fine-tuning. Validated on three medical segmentation tasks, SAMPO achieves state-of-the-art performance: on challenging tasks like PanNuke-T2, our method, when fine-tuned with only 10% of the training data, significantly outperforms all existing methods trained on the full 100% dataset, achieving an improvement of over 9 percentage points compared to the best baseline. Our work establishes a new paradigm for intent-aware alignment in visual foundation models, removing dependencies on auxiliary prompt generators or language-model-assisted preference learning.</li>
</ul>

<h3>Title: PoseGuard: Pose-Guided Generation with Safety Guardrails</h3>
<ul>
<li><strong>Authors: </strong>Kongxin Wang, Jie Zhang, Peigui Qi, Kunsheng Tang, Tianwei Zhang, Wenbo Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02476">https://arxiv.org/abs/2508.02476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02476">https://arxiv.org/pdf/2508.02476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02476]] PoseGuard: Pose-Guided Generation with Safety Guardrails(https://arxiv.org/abs/2508.02476)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Pose-guided video generation has become a powerful tool in creative industries, exemplified by frameworks like Animate Anyone. However, conditioning generation on specific poses introduces serious risks, such as impersonation, privacy violations, and NSFW content creation. To address these challenges, we propose $\textbf{PoseGuard}$, a safety alignment framework for pose-guided generation. PoseGuard is designed to suppress unsafe generations by degrading output quality when encountering malicious poses, while maintaining high-fidelity outputs for benign inputs. We categorize unsafe poses into three representative types: discriminatory gestures such as kneeling or offensive salutes, sexually suggestive poses that lead to NSFW content, and poses imitating copyrighted celebrity movements. PoseGuard employs a dual-objective training strategy combining generation fidelity with safety alignment, and uses LoRA-based fine-tuning for efficient, parameter-light updates. To ensure adaptability to evolving threats, PoseGuard supports pose-specific LoRA fusion, enabling flexible and modular updates when new unsafe poses are identified. We further demonstrate the generalizability of PoseGuard to facial landmark-guided generation. Extensive experiments validate that PoseGuard effectively blocks unsafe generations, maintains generation quality for benign inputs, and remains robust against slight pose variations.</li>
</ul>

<h3>Title: Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions</h3>
<ul>
<li><strong>Authors: </strong>Jaehyuk Heo, Pilsung Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02477">https://arxiv.org/abs/2508.02477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02477">https://arxiv.org/pdf/2508.02477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02477]] Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions(https://arxiv.org/abs/2508.02477)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in image anomaly detection have extended unsupervised learning-based models from single-class settings to multi-class frameworks, aiming to improve efficiency in training time and model storage. When a single model is trained to handle multiple classes, it often underperforms compared to class-specific models in terms of per-class detection accuracy. Accordingly, previous studies have primarily focused on narrowing this performance gap. However, the way class information is used, or not used, remains a relatively understudied factor that could influence how detection thresholds are defined in multi-class image anomaly detection. These thresholds, whether class-specific or class-agnostic, significantly affect detection outcomes. In this study, we identify and formalize the requirements that a multi-class image anomaly detection model must satisfy under different conditions, depending on whether class labels are available during training and evaluation. We then re-examine existing methods under these criteria. To meet these challenges, we propose Hierarchical Coreset (HierCore), a novel framework designed to satisfy all defined requirements. HierCore operates effectively even without class labels, leveraging a hierarchical memory bank to estimate class-wise decision criteria for anomaly detection. We empirically validate the applicability and robustness of existing methods and HierCore under four distinct scenarios, determined by the presence or absence of class labels in the training and evaluation phases. The experimental results demonstrate that HierCore consistently meets all requirements and maintains strong, stable performance across all settings, highlighting its practical potential for real-world multi-class anomaly detection tasks.</li>
</ul>

<h3>Title: MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding</h3>
<ul>
<li><strong>Authors: </strong>Wenwen Zeng, Yonghuang Wu, Yifan Chen, Xuan Xie, Chengqian Zhao, Feiyu Yin, Guoqing Wu, Jinhua Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02480">https://arxiv.org/abs/2508.02480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02480">https://arxiv.org/pdf/2508.02480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02480]] MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding(https://arxiv.org/abs/2508.02480)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Reconstructing dynamic videos from fMRI is important for understanding visual cognition and enabling vivid brain-computer interfaces. However, current methods are critically limited to single-shot clips, failing to address the multi-shot nature of real-world experiences. Multi-shot reconstruction faces fundamental challenges: fMRI signal mixing across shots, the temporal resolution mismatch between fMRI and video obscuring rapid scene changes, and the lack of dedicated multi-shot fMRI-video datasets. To overcome these limitations, we propose a novel divide-and-decode framework for multi-shot fMRI video reconstruction. Our core innovations are: (1) A shot boundary predictor module explicitly decomposing mixed fMRI signals into shot-specific segments. (2) Generative keyframe captioning using LLMs, which decodes robust textual descriptions from each segment, overcoming temporal blur by leveraging high-level semantics. (3) Novel large-scale data synthesis (20k samples) from existing datasets. Experimental results demonstrate our framework outperforms state-of-the-art methods in multi-shot reconstruction fidelity. Ablation studies confirm the critical role of fMRI decomposition and semantic captioning, with decomposition significantly improving decoded caption CLIP similarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI reconstruction, enabling accurate recovery of complex visual narratives through explicit decomposition and semantic prompting.</li>
</ul>

<h3>Title: Toward Using Machine Learning as a Shape Quality Metric for Liver Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Khoa Tuan Nguyen, Gaeun Oh, Ho-min Park, Francesca Tozzi, Wouter Willaert, Joris Vankerschaver, Niki Rashidian, Wesley De Neve</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02482">https://arxiv.org/abs/2508.02482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02482">https://arxiv.org/pdf/2508.02482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02482]] Toward Using Machine Learning as a Shape Quality Metric for Liver Point Cloud Generation(https://arxiv.org/abs/2508.02482)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>While 3D medical shape generative models such as diffusion models have shown promise in synthesizing diverse and anatomically plausible structures, the absence of ground truth makes quality evaluation challenging. Existing evaluation metrics commonly measure distributional distances between training and generated sets, while the medical field requires assessing quality at the individual level for each generated shape, which demands labor-intensive expert review. In this paper, we investigate the use of classical machine learning (ML) methods and PointNet as an alternative, interpretable approach for assessing the quality of generated liver shapes. We sample point clouds from the surfaces of the generated liver shapes, extract handcrafted geometric features, and train a group of supervised ML and PointNet models to classify liver shapes as good or bad. These trained models are then used as proxy discriminators to assess the quality of synthetic liver shapes produced by generative models. Our results show that ML-based shape classifiers provide not only interpretable feedback but also complementary insights compared to expert evaluation. This suggests that ML classifiers can serve as lightweight, task-relevant quality metrics in 3D organ shape generation, supporting more transparent and clinically aligned evaluation protocols in medical shape modeling.</li>
</ul>

<h3>Title: Federated Graph Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Yuming Ai, Xunkai Li, Jiaqi Chao, Bowen Fan, Zhengyu Wu, Yinlin Zhu, Rong-Hua Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02485">https://arxiv.org/abs/2508.02485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02485">https://arxiv.org/pdf/2508.02485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02485]] Federated Graph Unlearning(https://arxiv.org/abs/2508.02485)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>The demand for data privacy has led to the development of frameworks like Federated Graph Learning (FGL), which facilitate decentralized model training. However, a significant operational challenge in such systems is adhering to the right to be forgotten. This principle necessitates robust mechanisms for two distinct types of data removal: the selective erasure of specific entities and their associated knowledge from local subgraphs and the wholesale removal of a user's entire dataset and influence. Existing methods often struggle to fully address both unlearning requirements, frequently resulting in incomplete data removal or the persistence of residual knowledge within the system. This work introduces a unified framework, conceived to provide a comprehensive solution to these challenges. The proposed framework employs a bifurcated strategy tailored to the specific unlearning request. For fine-grained Meta Unlearning, it uses prototype gradients to direct the initial local forgetting process, which is then refined by generating adversarial graphs to eliminate any remaining data traces among affected clients. In the case of complete client unlearning, the framework utilizes adversarial graph generation exclusively to purge the departed client's contributions from the remaining network. Extensive experiments on multiple benchmark datasets validate the proposed approach. The framework achieves substantial improvements in model prediction accuracy across both client and meta-unlearning scenarios when compared to existing methods. Furthermore, additional studies confirm its utility as a plug-in module, where it materially enhances the predictive capabilities and unlearning effectiveness of other established methods.</li>
</ul>

<h3>Title: From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks</h3>
<ul>
<li><strong>Authors: </strong>Shuzhou Yuan, Zhan Qu, Mario Tawfelis, Michael Färber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02502">https://arxiv.org/abs/2508.02502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02502">https://arxiv.org/pdf/2508.02502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02502]] From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks(https://arxiv.org/abs/2508.02502)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong linguistic capabilities, but little is known about how they encode psycholinguistic knowledge across languages. We investigate whether and how LLMs exhibit human-like psycholinguistic responses under different linguistic identities using two tasks: sound symbolism and word valence. We evaluate two models, Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and bilingual prompting in English, Dutch, and Chinese. Behaviorally, both models adjust their outputs based on prompted language identity, with Qwen showing greater sensitivity and sharper distinctions between Dutch and Chinese. Probing analysis reveals that psycholinguistic signals become more decodable in deeper layers, with Chinese prompts yielding stronger and more stable valence representations than Dutch. Our results demonstrate that language identity conditions both output behavior and internal representations in LLMs, providing new insights into their application as models of cross-linguistic cognition.</li>
</ul>

<h3>Title: Modular Arithmetic: Language Models Solve Math Digit by Digit</h3>
<ul>
<li><strong>Authors: </strong>Tanja Baeumel, Daniil Gurgurov, Yusser al Ghussin, Josef van Genabith, Simon Ostermann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02513">https://arxiv.org/abs/2508.02513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02513">https://arxiv.org/pdf/2508.02513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02513]] Modular Arithmetic: Language Models Solve Math Digit by Digit(https://arxiv.org/abs/2508.02513)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While recent work has begun to uncover the internal strategies that Large Language Models (LLMs) employ for simple arithmetic tasks, a unified understanding of their underlying mechanisms is still lacking. We extend recent findings showing that LLMs represent numbers in a digit-wise manner and present evidence for the existence of digit-position-specific circuits that LLMs use to perform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that operate independently on different digit positions (units, tens, hundreds). Notably, such circuits exist independently of model size and of tokenization strategy, i.e. both for models that encode longer numbers digit-by-digit and as one token. Using Feature Importance and Causal Interventions, we identify and validate the digit-position-specific circuits, revealing a compositional and interpretable structure underlying the solving of arithmetic problems in LLMs. Our interventions selectively alter the model's prediction at targeted digit positions, demonstrating the causal role of digit-position circuits in solving arithmetic tasks.</li>
</ul>

<h3>Title: PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhan Qu, Shuzhou Yuan, Michael Färber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02515">https://arxiv.org/abs/2508.02515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02515">https://arxiv.org/pdf/2508.02515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02515]] PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs(https://arxiv.org/abs/2508.02515)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across four families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a reward signal, we fine-tune three lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts.</li>
</ul>

<h3>Title: AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via Multi-modal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yao Lai, Souradip Poddar, Sungyoung Lee, Guojin Chen, Mengkang Hu, Bei Yu, Ping Luo, David Z. Pan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02518">https://arxiv.org/abs/2508.02518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02518">https://arxiv.org/pdf/2508.02518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02518]] AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via Multi-modal LLMs(https://arxiv.org/abs/2508.02518)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite advances in analog design automation, analog front-end design still heavily depends on expert intuition and iterative simulations, underscoring critical gaps in fully automated optimization for performance-critical applications. Recently, the rapid development of Large Language Models (LLMs) has brought new promise to analog design automation. However, existing work remains in its early stages, and holistic joint optimization for practical end-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a unified multimodal LLM-based framework that integrates generative capabilities and optimization techniques to jointly explore circuit topologies and optimize device sizing, automatically generating performance-specific, fully sized schematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning LLMs on high-quality synthesized circuit data and introduces a multimodal diagnosis and repair workflow based on functional specifications and waveform images. By leveraging LLMs to interpret generated circuit netlists, AnalogCoder-Pro automates the extraction of critical design parameters and the formulation of parameter spaces, establishing an end-to-end workflow for simultaneous topology generation and device sizing optimization. Extensive experiments demonstrate that these orthogonal approaches significantly improve the success rate of analog circuit design and enhance circuit performance.</li>
</ul>

<h3>Title: Transportation Cyber Incident Awareness through Generative AI-Based Incident Analysis and Retrieval-Augmented Question-Answering Systems</h3>
<ul>
<li><strong>Authors: </strong>Ostonya Thomas, Muhaimin Bin Munir, Jean-Michel Tine, Mizanur Rahman, Yuchen Cai, Khandakar Ashrafi Akbar, Md Nahiyan Uddin, Latifur Khan, Trayce Hockstad, Mashrur Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02523">https://arxiv.org/abs/2508.02523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02523">https://arxiv.org/pdf/2508.02523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02523]] Transportation Cyber Incident Awareness through Generative AI-Based Incident Analysis and Retrieval-Augmented Question-Answering Systems(https://arxiv.org/abs/2508.02523)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Technological advancements have revolutionized numerous industries, including transportation. While digitalization, automation, and connectivity have enhanced safety and efficiency, they have also introduced new vulnerabilities. With 95% of data breaches attributed to human error, promoting cybersecurity awareness in transportation is increasingly critical. Despite numerous cyberattacks on transportation systems worldwide, comprehensive and centralized records of these incidents remain scarce. To address this gap and enhance cyber awareness, this paper presents a large language model (LLM) based approach to extract and organize transportation related cyber incidents from publicly available datasets. A key contribution of this work is the use of generative AI to transform unstructured, heterogeneous cyber incident data into structured formats. Incidents were sourced from the Center for Strategic & International Studies (CSIS) List of Significant Cyber Incidents, the University of Maryland Cyber Events Database (UMCED), the European Repository of Cyber Incidents (EuRepoC), the Maritime Cyber Attack Database (MCAD), and the U.S. DOT Transportation Cybersecurity and Resiliency (TraCR) Examples of Cyber Attacks in Transportation (2018 to 2022). These were classified by a fine tuned LLM into five transportation modes: aviation, maritime, rail, road, and multimodal, forming a transportation specific cyber incident database. Another key contribution of this work is the development of a Retrieval Augmented Generation question answering system, designed to enhance accessibility and practical use by enabling users to query the curated database for specific details on transportation related cyber incidents. By leveraging LLMs for both data extraction and user interaction, this study contributes a novel, accessible tool for improving cybersecurity awareness in the transportation sector.</li>
</ul>

<h3>Title: I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2</h3>
<ul>
<li><strong>Authors: </strong>Jack Merullo, Arjun Khurana, Oliver McLaughlin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02527">https://arxiv.org/abs/2508.02527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02527">https://arxiv.org/pdf/2508.02527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02527]] I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2(https://arxiv.org/abs/2508.02527)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate proficiency on phonetic tasks, such as rhyming, without explicit phonetic or auditory grounding. In this work, we investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic information. Our results suggest that Llama uses a rich internal model of phonemes to complete phonetic tasks. We provide evidence for high-level organization of phoneme representations in its latent space. In doing so, we also identify a ``phoneme mover head" which promotes phonetic information during rhyming tasks. We visualize the output space of this head and find that, while notable differences exist, Llama learns a model of vowels similar to the standard IPA vowel chart for humans, despite receiving no direct supervision to do so.</li>
</ul>

<h3>Title: Understanding the Risks of Asphalt Art on the Reliability of Surveillance Perception Systems</h3>
<ul>
<li><strong>Authors: </strong>Jin Ma, Abyad Enan, Long Cheng, Mashrur Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02530">https://arxiv.org/abs/2508.02530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02530">https://arxiv.org/pdf/2508.02530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02530]] Understanding the Risks of Asphalt Art on the Reliability of Surveillance Perception Systems(https://arxiv.org/abs/2508.02530)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Artistic crosswalks featuring asphalt art, introduced by different organizations in recent years, aim to enhance the visibility and safety of pedestrians. However, their visual complexity may interfere with surveillance systems that rely on vision-based object detection models. In this study, we investigate the impact of asphalt art on pedestrian detection performance of a pretrained vision-based object detection model. We construct realistic crosswalk scenarios by compositing various street art patterns into a fixed surveillance scene and evaluate the model's performance in detecting pedestrians on asphalt-arted crosswalks under both benign and adversarial conditions. A benign case refers to pedestrian crosswalks painted with existing normal asphalt art, whereas an adversarial case involves digitally crafted or altered asphalt art perpetrated by an attacker. Our results show that while simple, color-based designs have minimal effect, complex artistic patterns, particularly those with high visual salience, can significantly degrade pedestrian detection performance. Furthermore, we demonstrate that adversarially crafted asphalt art can be exploited to deliberately obscure real pedestrians or generate non-existent pedestrian detections. These findings highlight a potential vulnerability in urban vision-based pedestrian surveillance systems and underscore the importance of accounting for environmental visual variations when designing robust pedestrian perception models.</li>
</ul>

<h3>Title: Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Karan Reddy, Mayukha Pal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02532">https://arxiv.org/abs/2508.02532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02532">https://arxiv.org/pdf/2508.02532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02532]] Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction(https://arxiv.org/abs/2508.02532)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Standard transformer-based language models, while powerful for general text, often struggle with the fine-grained syntax and entity relationships in complex technical, engineering documents. To address this, we propose the Contextual Graph Transformer (CGT), a hybrid neural architecture that combines Graph Neural Networks (GNNs) and Transformers for domain-specific question answering. CGT constructs a dynamic graph over input tokens using sequential, skip-gram, and semantic similarity edges, which is processed by GATv2Conv layers for local structure learning. These enriched embeddings are then passed to a Transformer encoder to capture global dependencies. Unlike generic large models, technical domains often require specialized language models with stronger contextualization and structure awareness. CGT offers a parameter-efficient solution for such use cases. Integrated into a Retrieval-Augmented Generation (RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from CGTs ability to jointly model structural token interactions and long-range semantic coherence. The model is trained from scratch using a two-phase approach: pretraining on general text followed by fine-tuning on domain-specific manuals. This highlights CGTs adaptability to technical language, enabling better grounding, entity tracking, and retrieval-augmented responses in real-world applications.</li>
</ul>

<h3>Title: Communication and Computation Efficient Split Federated Learning in O-RAN</h3>
<ul>
<li><strong>Authors: </strong>Shunxian Gu, Chaoqun You, Bangbang Ren, Deke Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02534">https://arxiv.org/abs/2508.02534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02534">https://arxiv.org/pdf/2508.02534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02534]] Communication and Computation Efficient Split Federated Learning in O-RAN(https://arxiv.org/abs/2508.02534)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The hierarchical architecture of Open Radio Access Network (O-RAN) has enabled a new Federated Learning (FL) paradigm that trains models using data from non- and near-real-time (near-RT) Radio Intelligent Controllers (RICs). However, the ever-increasing model size leads to longer training time, jeopardizing the deadline requirements for both non-RT and near-RT RICs. To address this issue, split federated learning (SFL) offers an approach by offloading partial model layers from near-RT-RIC to high-performance non-RT-RIC. Nonetheless, its deployment presents two challenges: (i) Frequent data/gradient transfers between near-RT-RIC and non-RT-RIC in SFL incur significant communication cost in O-RAN. (ii) Proper allocation of computational and communication resources in O-RAN is vital to satisfying the deadline and affects SFL convergence. Therefore, we propose SplitMe, an SFL framework that exploits mutual learning to alternately and independently train the near-RT-RIC's model and the non-RT-RIC's inverse model, eliminating frequent transfers. The ''inverse'' of the inverse model is derived via a zeroth-order technique to integrate the final model. Then, we solve a joint optimization problem for SplitMe to minimize overall resource costs with deadline-aware selection of near-RT-RICs and adaptive local updates. Our numerical results demonstrate that SplitMe remarkably outperforms FL frameworks like SFL, FedAvg and O-RANFed regarding costs and convergence.</li>
</ul>

<h3>Title: Nicknames for Group Signatures</h3>
<ul>
<li><strong>Authors: </strong>Guillaume Quispe, Pierre Jouvelot, Gerard Memmi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02543">https://arxiv.org/abs/2508.02543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02543">https://arxiv.org/pdf/2508.02543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02543]] Nicknames for Group Signatures(https://arxiv.org/abs/2508.02543)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Nicknames for Group Signatures (NGS) is a new signature scheme that extends Group Signatures (GS) with Signatures with Flexible Public Keys (SFPK). Via GS, each member of a group can sign messages on behalf of the group without revealing his identity, except to a designated auditor. Via SFPK, anyone can create new identities for a particular user, enabling anonymous transfers with only the intended recipient able to trace these new identities. To prevent the potential abuses that this anonymity brings, NGS integrates flexible public keys into the GS framework to support auditable transfers. In addition to introducing NGS, we describe its security model and provide a mathematical construction proved secure in the Random Oracle Model. As a practical NGS use case, we build NickHat, a blockchain-based token-exchange prototype system on top of Ethereum.</li>
</ul>

<h3>Title: What are you sinking? A geometric approach on attention sink</h3>
<ul>
<li><strong>Authors: </strong>Valeria Ruscio, Umberto Nanni, Fabrizio Silvestri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02546">https://arxiv.org/abs/2508.02546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02546">https://arxiv.org/pdf/2508.02546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02546]] What are you sinking? A geometric approach on attention sink(https://arxiv.org/abs/2508.02546)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Attention sink (AS) is a consistent pattern in transformer attention maps where certain tokens (often special tokens or positional anchors) disproportionately attract attention from other tokens. We show that in transformers, AS is not an architectural artifact, but it is the manifestation of a fundamental geometric principle: the establishment of reference frames that anchor representational spaces. We analyze several architectures and identify three distinct reference frame types, centralized, distributed, and bidirectional, that correlate with the attention sink phenomenon. We show that they emerge during the earliest stages of training as optimal solutions to the problem of establishing stable coordinate systems in high-dimensional spaces. We show the influence of architecture components, particularly position encoding implementations, on the specific type of reference frame. This perspective transforms our understanding of transformer attention mechanisms and provides insights for both architecture design and the relationship with AS.</li>
</ul>

<h3>Title: PrivAR: Real-Time Privacy Protection for Location-Based Augmented Reality Applications</h3>
<ul>
<li><strong>Authors: </strong>Shafizur Rahman Seeam, Ye Zheng, Zhengxiong Li, Yidan Hu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02551">https://arxiv.org/abs/2508.02551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02551">https://arxiv.org/pdf/2508.02551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02551]] PrivAR: Real-Time Privacy Protection for Location-Based Augmented Reality Applications(https://arxiv.org/abs/2508.02551)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Location-based augmented reality (LB-AR) applications, such as Pokémon Go, stream sub-second GPS updates to deliver responsive and immersive user experiences. However, this high-frequency location reporting introduces serious privacy risks. Protecting privacy in LB-AR is significantly more challenging than in traditional location-based services (LBS), as it demands real-time location protection with strong per-location and trajectory-level privacy guaranteed while maintaining low latency and high quality of service (QoS). Existing methods fail to meet these combined demands. To fill the gap, we present PrivAR, the first client-side privacy framework for real-time LB-AR. PrivAR introduces two lightweight mechanisms: (i) Planar Staircase Mechanism (PSM) which designs a staircase-shaped distribution to generate noisy location with strong per-location privacy and low expected error; and (ii) Thresholded Reporting with PSM (TR-PSM), a selective scheme that releases a noisy location update only when a displacement exceeds a private threshold, enabling many-to-one mappings for enhanced trace-level privacy while preserving high QoS. We present theoretical analysis, extensive experiments on two public datasets and our proprietary GeoTrace dataset, and validate PrivAR on a Pokémon-Go-style prototype. Results show PrivAR improves QoS (Gamescore) by up to 50%, while increasing attacker error by 1.8x over baseline with an additional 0.06 milliseconds runtime overhead.</li>
</ul>

<h3>Title: Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02556">https://arxiv.org/abs/2508.02556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02556">https://arxiv.org/pdf/2508.02556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02556]] Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks(https://arxiv.org/abs/2508.02556)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into overlapping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than transformer-based models, making them well-suited for real-world deployment.</li>
</ul>

<h3>Title: Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction</h3>
<ul>
<li><strong>Authors: </strong>Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02558">https://arxiv.org/abs/2508.02558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02558">https://arxiv.org/pdf/2508.02558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02558]] Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction(https://arxiv.org/abs/2508.02558)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.</li>
</ul>

<h3>Title: Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application</h3>
<ul>
<li><strong>Authors: </strong>Nys Tjade Siegel, James H. Cole, Mohamad Habes, Stefan Haufe, Kerstin Ritter, Marc-André Schulz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02560">https://arxiv.org/abs/2508.02560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02560">https://arxiv.org/pdf/2508.02560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02560]] Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application(https://arxiv.org/abs/2508.02560)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Trustworthy interpretation of deep learning models is critical for neuroimaging applications, yet commonly used Explainable AI (XAI) methods lack rigorous validation, risking misinterpretation. We performed the first large-scale, systematic comparison of XAI methods on ~45,000 structural brain MRIs using a novel XAI validation framework. This framework establishes verifiable ground truth by constructing prediction tasks with known signal sources - from localized anatomical features to subject-specific clinical lesions - without artificially altering input images. Our analysis reveals systematic failures in two of the most widely used methods: GradCAM consistently failed to localize predictive features, while Layer-wise Relevance Propagation generated extensive, artifactual explanations that suggest incompatibility with neuroimaging data characteristics. Our results indicate that these failures stem from a domain mismatch, where methods with design principles tailored to natural images require substantial adaptation for neuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad, which makes fewer assumptions about data structure, proved consistently accurate, suggesting its conceptual simplicity makes it more robust to this domain shift. These findings highlight the need for domain-specific adaptation and validation of XAI methods, suggest that interpretations from prior neuroimaging studies using standard XAI methodology warrant re-evaluation, and provide urgent guidance for practical application of XAI in neuroimaging.</li>
</ul>

<h3>Title: Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Javier Fumanal-Idocin, Raquel Fernandez-Peralta, Javier Andreu-Perez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02566">https://arxiv.org/abs/2508.02566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02566">https://arxiv.org/pdf/2508.02566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02566]] Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification(https://arxiv.org/abs/2508.02566)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Dynamic feature selection (DFS) offers a compelling alternative to traditional, static feature selection by adapting the selected features to each individual sample. Unlike classical methods that apply a uniform feature set, DFS customizes feature selection per sample, providing insight into the decision-making process for each case. DFS is especially significant in settings where decision transparency is key, i.e., clinical decisions; however, existing methods use opaque models, which hinder their applicability in real-life scenarios. This paper introduces a novel approach leveraging a rule-based system as a base classifier for the DFS process, which enhances decision interpretability compared to neural estimators. We also show how this method provides a quantitative measure of uncertainty for each feature query and can make the feature selection process computationally lighter by constraining the feature search space. We also discuss when greedy selection of conditional mutual information is equivalent to selecting features that minimize the difference with respect to the global model predictions. Finally, we demonstrate the competitive performance of our rule-based DFS approach against established and state-of-the-art greedy and RL methods, which are mostly considered opaque, compared to our explainable rule-based system.</li>
</ul>

<h3>Title: Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jérémie Dentan, Davide Buscaldi, Sonia Vanier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02573">https://arxiv.org/abs/2508.02573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02573">https://arxiv.org/pdf/2508.02573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02573]] Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs(https://arxiv.org/abs/2508.02573)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding. We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization.</li>
</ul>

<h3>Title: EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Eman Alamoudi, Ellis Solaiman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02574">https://arxiv.org/abs/2508.02574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02574">https://arxiv.org/pdf/2508.02574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02574]] EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare(https://arxiv.org/abs/2508.02574)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Arabic-language patient feedback remains under-analysed because dialect diversity and scarce aspect-level sentiment labels hinder automated assessment. To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that merges ChatGPT pseudo-labelling with targeted human review to build the first explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label (positive, negative, or neutral), forming a pioneering Arabic dataset aligned with healthcare themes, with ChatGPT-generated rationales provided for each label to enhance transparency. To evaluate the impact of annotation quality on model performance, we created three versions of the training data: a fully supervised set with all labels reviewed by humans, a semi-supervised set with 50% human review, and an unsupervised set with only machine-generated labels. We fine-tuned two transformer models on these datasets for both aspect and sentiment classification. Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels. Reducing the number of aspect classes notably improved classification metrics across the board. These findings demonstrate an effective, scalable approach to Arabic aspect-based sentiment analysis (SA) in healthcare, combining large language model annotation with human expertise to produce a robust and explainable dataset. Future directions include generalisation across hospitals, prompt refinement, and interpretable data-driven modelling.</li>
</ul>

<h3>Title: MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification</h3>
<ul>
<li><strong>Authors: </strong>Ming Pok Ng, Junqi Jiang, Gabriel Freedman, Antonio Rago, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02584">https://arxiv.org/abs/2508.02584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02584">https://arxiv.org/pdf/2508.02584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02584]] MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification(https://arxiv.org/abs/2508.02584)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.</li>
</ul>

<h3>Title: CharBench: Evaluating the Role of Tokenization in Character-Level Tasks</h3>
<ul>
<li><strong>Authors: </strong>Omri Uzan, Yuval Pinter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02591">https://arxiv.org/abs/2508.02591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02591">https://arxiv.org/pdf/2508.02591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02591]] CharBench: Evaluating the Role of Tokenization in Character-Level Tasks(https://arxiv.org/abs/2508.02591)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tasks that require character-level reasoning, such as counting or locating characters within words, remain challenging for contemporary language models. A common conjecture is that language models' reliance on subword units, rather than characters, contributes to their struggles with character-level tasks, yet recent studies offer conflicting conclusions about the role of tokenization, leaving its impact unclear. To address this gap, we introduce CharBench, a comprehensive benchmark of character-level tasks that is two orders of magnitude larger than existing alternatives. We evaluate a diverse range of leading open-weight and proprietary models on CharBench and find that it presents a significant challenge to modern LLMs, with an average accuracy of 43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic properties of words and their segmentations into tokens correspond to model performance. For counting tasks, we find that tokenization properties are weakly correlated with correctness, while the length of the queried word and the actual character count play a more significant part. In contrast, for tasks requiring intra-word positional understanding, performance is negatively correlated with the length of the token containing the queried character, suggesting that longer tokens obscure character position information for LLMs. We encourage future work to build on the benchmark and evaluation methodology introduced here as tools for improving model performance on such tasks.</li>
</ul>

<h3>Title: StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in Low-Data Regimes</h3>
<ul>
<li><strong>Authors: </strong>Siyi Liu, Yujia Zheng, Yongqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02601">https://arxiv.org/abs/2508.02601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02601">https://arxiv.org/pdf/2508.02601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02601]] StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in Low-Data Regimes(https://arxiv.org/abs/2508.02601)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The application of machine learning on tabular data in specialized domains is severely limited by data scarcity. While generative models offer a solution, traditional methods falter in low-data regimes, and recent Large Language Models (LLMs) often ignore the explicit dependency structure of tabular data, leading to low-fidelity synthetics. To address these limitations, we introduce StructSynth, a novel framework that integrates the generative power of LLMs with robust structural control. StructSynth employs a two-stage architecture. First, it performs explicit structure discovery to learn a Directed Acyclic Graph (DAG) from the available data. Second, this learned structure serves as a high-fidelity blueprint to steer the LLM's generation process, forcing it to adhere to the learned feature dependencies and thereby ensuring the generated data respects the underlying structure by design. Our extensive experiments demonstrate that StructSynth produces synthetic data with significantly higher structural integrity and downstream utility than state-of-the-art methods. It proves especially effective in challenging low-data scenarios, successfully navigating the trade-off between privacy preservation and statistical fidelity.</li>
</ul>

<h3>Title: ReMoMask: Retrieval-Augmented Masked Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhengdao Li, Siheng Wang, Zeyu Zhang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02605">https://arxiv.org/abs/2508.02605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02605">https://arxiv.org/pdf/2508.02605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02605]] ReMoMask: Retrieval-Augmented Masked Motion Generation(https://arxiv.org/abs/2508.02605)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: this https URL. Website: this https URL.</li>
</ul>

<h3>Title: DeepKoopFormer: A Koopman Enhanced Transformer Based Architecture for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Ali Forootani, Mohammad Khosravi, Masoud Barati</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02616">https://arxiv.org/abs/2508.02616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02616">https://arxiv.org/pdf/2508.02616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02616]] DeepKoopFormer: A Koopman Enhanced Transformer Based Architecture for Time Series Forecasting(https://arxiv.org/abs/2508.02616)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Time series forecasting plays a vital role across scientific, industrial, and environmental domains, especially when dealing with high-dimensional and nonlinear systems. While Transformer-based models have recently achieved state-of-the-art performance in long-range forecasting, they often suffer from interpretability issues and instability in the presence of noise or dynamical uncertainty. In this work, we propose DeepKoopFormer, a principled forecasting framework that combines the representational power of Transformers with the theoretical rigor of Koopman operator theory. Our model features a modular encoder-propagator-decoder structure, where temporal dynamics are learned via a spectrally constrained, linear Koopman operator in a latent space. We impose structural guarantees-such as bounded spectral radius, Lyapunov based energy regularization, and orthogonal parameterization to ensure stability and interpretability. Comprehensive evaluations are conducted on both synthetic dynamical systems, real-world climate dataset (wind speed and surface pressure), financial time series (cryptocurrency), and electricity generation dataset using the Python package that is prepared for this purpose. Across all experiments, DeepKoopFormer consistently outperforms standard LSTM and baseline Transformer models in terms of accuracy, robustness to noise, and long-term forecasting stability. These results establish DeepKoopFormer as a flexible, interpretable, and robust framework for forecasting in high dimensional and dynamical settings.</li>
</ul>

<h3>Title: Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation</h3>
<ul>
<li><strong>Authors: </strong>Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02618">https://arxiv.org/abs/2508.02618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02618">https://arxiv.org/pdf/2508.02618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02618]] Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation(https://arxiv.org/abs/2508.02618)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this "attention hacking", we propose "Interaction Distillation", a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher model's interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM.</li>
</ul>

<h3>Title: Pointer: Linear-Complexity Long-Range Modeling without Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Zixi Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02631">https://arxiv.org/abs/2508.02631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02631">https://arxiv.org/pdf/2508.02631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02631]] Pointer: Linear-Complexity Long-Range Modeling without Pre-training(https://arxiv.org/abs/2508.02631)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We introduce Pointer, a novel architecture that achieves linear $O(NK)$ complexity for long-range sequence modeling while maintaining superior performance without requiring pre-training. Unlike standard attention mechanisms that compute $O(N^2)$ pairwise interactions, our approach uses layer-wise pointer chaining where each layer's pointer selection depends on previous layer's pointer positions, creating explicit long-distance connections through pointer chains. We demonstrate that this architecture achieves $2$--$10\times$ speedup on long sequences compared to standard transformers, maintains $>95\%$ accuracy on copy tasks at distances up to 2048 tokens, and learns interpretable pointer patterns that reveal structured dependency modeling. Our experiments on efficiency benchmarks, long-range dependency tasks, and interpretability analysis show that Pointer offers a compelling alternative to attention mechanisms for scenarios requiring efficient long-range modeling without pre-training dependencies.</li>
</ul>

<h3>Title: Test Set Quality in Multilingual LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Kranti Chalamalasetti, Gabriel Bernier-Colborne, Yvan Gauthier, Sowmya Vajjala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02635">https://arxiv.org/abs/2508.02635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02635">https://arxiv.org/pdf/2508.02635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02635]] Test Set Quality in Multilingual LLM Evaluation(https://arxiv.org/abs/2508.02635)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.</li>
</ul>

<h3>Title: Evaluating Variance in Visual Question Answering Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Nikitha SR</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02645">https://arxiv.org/abs/2508.02645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02645">https://arxiv.org/pdf/2508.02645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02645]] Evaluating Variance in Visual Question Answering Benchmarks(https://arxiv.org/abs/2508.02645)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have emerged as powerful tools for visual question answering (VQA), enabling reasoning and contextual understanding across visual and textual modalities. Despite their advancements, the evaluation of MLLMs on VQA benchmarks often relies on point estimates, overlooking the significant variance in performance caused by factors such as stochastic model outputs, training seed sensitivity, and hyperparameter configurations. This paper critically examines these issues by analyzing variance across 14 widely used VQA benchmarks, covering diverse tasks such as visual reasoning, text understanding, and commonsense reasoning. We systematically study the impact of training seed, framework non-determinism, model scale, and extended instruction finetuning on performance variability. Additionally, we explore Cloze-style evaluation as an alternate assessment strategy, studying its effectiveness in reducing stochasticity and improving reliability across benchmarks. Our findings highlight the limitations of current evaluation practices and advocate for variance-aware methodologies to foster more robust and reliable development of MLLMs.</li>
</ul>

<h3>Title: LOST: Low-rank and Sparse Pre-training for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Li, Lu Yin, Li Shen, Jinjin Xu, Liwu Xu, Tianjin Huang, Wenwu Wang, Shiwei Liu, Xilu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02668">https://arxiv.org/abs/2508.02668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02668">https://arxiv.org/pdf/2508.02668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02668]] LOST: Low-rank and Sparse Pre-training for Large Language Models(https://arxiv.org/abs/2508.02668)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have achieved remarkable performance across a wide range of tasks, their massive scale incurs prohibitive computational and memory costs for pre-training from scratch. Recent studies have investigated the use of low-rank parameterization as a means of reducing model size and training cost. In this context, sparsity is often employed as a complementary technique to recover important information lost in low-rank compression by capturing salient features in the residual space. However, existing approaches typically combine low-rank and sparse components in a simplistic or ad hoc manner, often resulting in undesirable performance degradation compared to full-rank training. In this paper, we propose \textbf{LO}w-rank and \textbf{S}parse pre-\textbf{T}raining (\textbf{LOST}) for LLMs, a novel method that ingeniously integrates low-rank and sparse structures to enable effective training of LLMs from scratch under strict efficiency constraints. LOST applies singular value decomposition to weight matrices, preserving the dominant low-rank components, while allocating the remaining singular values to construct channel-wise sparse components to complement the expressiveness of low-rank training. We evaluate LOST on LLM pretraining ranging from 60M to 7B parameters. Our experiments show that LOST achieves competitive or superior performance compared to full-rank models, while significantly reducing both memory and compute overhead. Moreover, Code is available at \href{this https URL}{LOST Repo}</li>
</ul>

<h3>Title: Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Li, Liang Wang, Chao Wang, Siyu Zhou, Jing Jiang, Yan Peng, Guodong Long</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.02671">https://arxiv.org/abs/2508.02671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.02671">https://arxiv.org/pdf/2508.02671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.02671]] Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models(https://arxiv.org/abs/2508.02671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>For CLIP-based prompt tuning, introducing more data as additional knowledge for enhancing fine-tuning process is proved to be an effective approach. Existing data amplification strategies for prompt tuning typically rely on external knowledge (e.g., large language models or pre-structured knowledge bases), resulting in higher costs for data collection and processing, while generally ignoring further utilization of features in image modality. To address this, we propose Augmentation-driven Prompt Tuning (AugPT), a self-contained distillation-based prompt tuning approach using only internal augmentation on raw dataset to better exploit known features. Specifically, AugPT employs self-supervised augmentation on unlabeled images in the training set, and introduces a novel gating mechanism based on consensus test, reusing the pre-trained prompt tuning backbone model to spontaneously filter noisy samples, further enhancing the quality of augmented views. Extensive experiments validate that AugPT simultaneously enhances model performance and generalization capability without using appended external knowledge. The code of AugPT is available at: this https URL .</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
