<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-02</h1>
<h3>Title: Hypertokens: Holographic Associative Memory in Tokenized LLMs</h3>
<ul>
<li><strong>Authors: </strong>Christopher James Augeri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00002">https://arxiv.org/abs/2507.00002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00002">https://arxiv.org/pdf/2507.00002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00002]] Hypertokens: Holographic Associative Memory in Tokenized LLMs(https://arxiv.org/abs/2507.00002)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit remarkable capabilities but suffer from apparent precision loss, reframed here as information spreading. This reframing shifts the problem from computational precision to an information-theoretic communication issue. We address the K:V and V:K memory problem in LLMs by introducing HDRAM (Holographically Defined Random Access Memory), a symbolic memory framework treating transformer latent space as a spread-spectrum channel. Built upon hypertokens, structured symbolic codes integrating classical error-correcting codes (ECC), holographic computing, and quantum-inspired search, HDRAM recovers distributed information through principled despreading. These phase-coherent memory addresses enable efficient key-value operations and Grover-style search in latent space. By combining ECC grammar with compressed sensing and Krylov subspace alignment, HDRAM significantly improves associative retrieval without architectural changes, demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can fortify transformer architectures.</li>
</ul>

<h3>Title: Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE</h3>
<ul>
<li><strong>Authors: </strong>Eyhab Al-Masri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00003">https://arxiv.org/abs/2507.00003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00003">https://arxiv.org/pdf/2507.00003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00003]] Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE(https://arxiv.org/abs/2507.00003)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, explainability</a></li>
<li><strong>Abstract: </strong>This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework for interpretable intrusion detection in IoT environments. By integrating Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the system decomposes prediction confidence into truth (T), falsity (F), and indeterminacy (I) components, enabling uncertainty quantification and abstention. Predictions with high indeterminacy are flagged for review using both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that misclassified samples exhibit significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24). The use of indeterminacy as a proxy for uncertainty enables informed abstention and targeted review-particularly valuable in edge deployments. Figures and tables validate the correlation between I-scores and error likelihood, supporting more trustworthy, human-in-the-loop AI decisions. This work shows that neutrosophic logic enhances both accuracy and explainability, providing a practical foundation for trust-aware AI in edge and fog-based IoT security systems.</li>
</ul>

<h3>Title: A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search</h3>
<ul>
<li><strong>Authors: </strong>Austin R. Ellis-Mohr, Anuj K. Nayak, Lav R. Varshney</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00004">https://arxiv.org/abs/2507.00004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00004">https://arxiv.org/pdf/2507.00004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00004]] A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search(https://arxiv.org/abs/2507.00004)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demand considerable computational, energy, and financial resources during both training and deployment. While scaling laws for training have guided much of the field's recent progress, inference costs now represent a significant and growing component of the overall resource burden, particularly for reasoning-focused models. Existing characterizations of compute-optimality that consider model size, dataset size, and inference tokens in isolation or in fixed combinations risk overlooking more efficient operating points. We introduce directed stochastic skill search (DS3), a general framework that represents inference as stochastic traversal over a learned skill graph. From a simplified yet expressive instantiation, we derive closed-form expressions for task success and compute cost across a wide range of inference strategies -- including chain-of-thought (CoT) and tree-of-thought (ToT) -- enabling comparative analysis as a function of task difficulty and model capability. To that end, we extend a prior first-principles tripartite graph framework of LLM training to incorporate inference, and separately bridge DS3 with empirical methods that characterize LLM scaling behavior. We theoretically recover empirically observed patterns, including: linear accuracy scaling with logarithmic compute; variation in preferred inference strategies as a function of task difficulty and model capability; emergent behavior elicited by reasoning even when performance plateaus under parameter scaling; and both best-of-N (BoN) and majority voting behavior captured within a unified analytical framework. By explicitly characterizing training-inference interdependencies, our framework deepens theoretical understanding and supports principled algorithmic design and resource allocation.</li>
</ul>

<h3>Title: Towards Undistillable Models by Minimizing Conditional Mutual Information</h3>
<ul>
<li><strong>Authors: </strong>Linfeng Ye, Shayan Mohajer Hamidi, En-hui Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00012">https://arxiv.org/abs/2507.00012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00012">https://arxiv.org/pdf/2507.00012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00012]] Towards Undistillable Models by Minimizing Conditional Mutual Information(https://arxiv.org/abs/2507.00012)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>A deep neural network (DNN) is said to be undistillable if, when used as a black-box input-output teacher, it cannot be distilled through knowledge distillation (KD). In this case, the distilled student (referred to as the knockoff student) does not outperform a student trained independently with label smoothing (LS student) in terms of prediction accuracy. To protect intellectual property of DNNs, it is desirable to build undistillable DNNs. To this end, it is first observed that an undistillable DNN may have the trait that each cluster of its output probability distributions in response to all sample instances with the same label should be highly concentrated to the extent that each cluster corresponding to each label should ideally collapse into one probability distribution. Based on this observation and by measuring the concentration of each cluster in terms of conditional mutual information (CMI), a new training method called CMI minimized (CMIM) method is proposed, which trains a DNN by jointly minimizing the conventional cross entropy (CE) loss and the CMI values of all temperature scaled clusters across the entire temperature spectrum. The resulting CMIM model is shown, by extensive experiments, to be undistillable by all tested KD methods existing in the literature. That is, the knockoff students distilled by these KD methods from the CMIM model underperform the respective LS students. In addition, the CMIM model is also shown to performs better than the model trained with the CE loss alone in terms of their own prediction accuracy.</li>
</ul>

<h3>Title: SWE-Bench-CL: Continual Learning for Coding Agents</h3>
<ul>
<li><strong>Authors: </strong>Thomas Joshi, Shayan Chowdhury, Fatih Uysal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00014">https://arxiv.org/abs/2507.00014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00014">https://arxiv.org/pdf/2507.00014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00014]] SWE-Bench-CL: Continual Learning for Coding Agents(https://arxiv.org/abs/2507.00014)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta score -- to capture the stability-plasticity trade-off. We outline a rigorous experimental protocol comparing memory-enabled and memory-disabled agents across diverse Python repositories. All code and data are publicly available at this https URL, providing the community with a reproducible platform for developing more adaptive and robust AI agents in software engineering.</li>
</ul>

<h3>Title: Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications</h3>
<ul>
<li><strong>Authors: </strong>Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Guisheng Liao, Xuekang Liu, Fabio Roli, Carsten Maple</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00015">https://arxiv.org/abs/2507.00015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00015">https://arxiv.org/pdf/2507.00015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00015]] Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications(https://arxiv.org/abs/2507.00015)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, transformer</a></li>
<li><strong>Abstract: </strong>The remarkable success of transformers across various fields such as natural language processing and computer vision has paved the way for their applications in automatic modulation classification, a critical component in the communication systems of Internet of Things (IoT) devices. However, it has been observed that transformer-based classification of radio signals is susceptible to subtle yet sophisticated adversarial attacks. To address this issue, we have developed a defensive strategy for transformer-based modulation classification systems to counter such adversarial attacks. In this paper, we propose a novel vision transformer (ViT) architecture by introducing a new concept known as adversarial indicator (AdvI) token to detect adversarial attacks. To the best of our knowledge, this is the first work to propose an AdvI token in ViT to defend against adversarial attacks. Integrating an adversarial training method with a detection mechanism using AdvI token, we combine a training time defense and running time defense in a unified neural network model, which reduces architectural complexity of the system compared to detecting adversarial perturbations using separate models. We investigate into the operational principles of our method by examining the attention mechanism. We show the proposed AdvI token acts as a crucial element within the ViT, influencing attention weights and thereby highlighting regions or features in the input data that are potentially suspicious or anomalous. Through experimental results, we demonstrate that our approach surpasses several competitive methods in handling white-box attack scenarios, including those utilizing the fast gradient method, projected gradient descent attacks and basic iterative method.</li>
</ul>

<h3>Title: Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections</h3>
<ul>
<li><strong>Authors: </strong>Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00018">https://arxiv.org/abs/2507.00018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00018">https://arxiv.org/pdf/2507.00018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00018]] Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections(https://arxiv.org/abs/2507.00018)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training processes are essential phases in grounding pre-trained language models to real-world tasks, with learning from demonstrations or preference signals playing a crucial role in this adaptation. We present a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. Through rigorous mathematical derivation, we demonstrate that both SFT and preference learning methods like Direct Preference Optimization (DPO) operate within the same optimal policy-reward subspace, with SFT representing a special case of implicit reward learning. Our analysis reveals a critical limitation in conventional SFT: the KL divergence term in distribution matching becomes constant with respect to the policy during optimization, failing to constrain model updates. To address this, we propose a simple yet effective learning rate reduction approach that yields significant performance improvements (up to \textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in instruction following tasks. Additionally, we derive alternative SFT objectives from various f-divergence functions that preserve the KL term during optimization, further enhancing post-DPO model performance. Finally, we extend the theoretical relationship between LLM logits and Q-functions from preference learning to the SFT context, providing mathematical derivations and experimental validation.</li>
</ul>

<h3>Title: Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods</h3>
<ul>
<li><strong>Authors: </strong>Marcio Borges, Felipe Pereira, Michel Tosin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00020">https://arxiv.org/abs/2507.00020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00020">https://arxiv.org/pdf/2507.00020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00020]] Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods(https://arxiv.org/abs/2507.00020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study uses a Variational Autoencoder method to enhance the efficiency and applicability of Markov Chain Monte Carlo (McMC) methods by generating broader-spectrum prior proposals. Traditional approaches, such as the Karhunen-Lo√®ve Expansion (KLE), require previous knowledge of the covariance function, often unavailable in practical applications. The VAE framework enables a data-driven approach to flexibly capture a broader range of correlation structures in Bayesian inverse problems, particularly subsurface flow modeling. The methodology is tested on a synthetic groundwater flow inversion problem, where pressure data is used to estimate permeability fields. Numerical experiments demonstrate that the VAE-based parameterization achieves comparable accuracy to KLE when the correlation length is known and outperforms KLE when the assumed correlation length deviates from the true value. Moreover, the VAE approach significantly reduces stochastic dimensionality, improving computational efficiency. The results suggest that leveraging deep generative models in McMC methods can lead to more adaptable and efficient Bayesian inference in high-dimensional problems.</li>
</ul>

<h3>Title: GLU Attention Improve Transformer</h3>
<ul>
<li><strong>Authors: </strong>Zehao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00022">https://arxiv.org/abs/2507.00022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00022">https://arxiv.org/pdf/2507.00022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00022]] GLU Attention Improve Transformer(https://arxiv.org/abs/2507.00022)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Gated Linear Units (GLU) have shown great potential in enhancing neural network performance. In this paper, I introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention. My experiments demonstrate that GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs. GLU Attention is lightweight and can seamlessly integrate with other technologies, such as Flash Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention (MHA) variants such as Grouped-Query Attention (GQA). This project is open-sourced at github.</li>
</ul>

<h3>Title: AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity</h3>
<ul>
<li><strong>Authors: </strong>Yeyong Yu, Xilei Bian, Jie Xiong, Xing Wu, Quan Qian</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00024">https://arxiv.org/abs/2507.00024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00024">https://arxiv.org/pdf/2507.00024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00024]] AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity(https://arxiv.org/abs/2507.00024)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the growing demand for novel materials, machine learning-driven inverse design methods face significant challenges in reconciling the high-dimensional materials composition space with limited experimental data. Existing approaches suffer from two major limitations: (I) machine learning models often lack reliability in high-dimensional spaces, leading to prediction biases during the design process; (II) these models fail to effectively incorporate domain expert knowledge, limiting their capacity to support knowledge-guided inverse design. To address these challenges, we introduce AIMatDesign, a reinforcement learning framework that addresses these limitations by augmenting experimental data using difference-based algorithms to build a trusted experience pool, accelerating model convergence. To enhance model reliability, an automated refinement strategy guided by large language models (LLMs) dynamically corrects prediction inconsistencies, reinforcing alignment between reward signals and state value functions. Additionally, a knowledge-based reward function leverages expert domain rules to improve stability and efficiency during training. Our experiments demonstrate that AIMatDesign significantly surpasses traditional machine learning and reinforcement learning methods in discovery efficiency, convergence speed, and success rates. Among the numerous candidates proposed by AIMatDesign, experimental synthesis of representative Zr-based alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\% elongation, closely matching predictions. Moreover, the framework accurately captured the trend of yield strength variation with composition, demonstrating its reliability and potential for closed-loop materials discovery.</li>
</ul>

<h3>Title: ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiale Ding, Xiang Zheng, Cong Wang, Wei-Bin Lee, Xingjun Ma, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00026">https://arxiv.org/abs/2507.00026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00026">https://arxiv.org/pdf/2507.00026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00026]] ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models(https://arxiv.org/abs/2507.00026)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly deployed as black-box components in real-world applications, evaluating their safety-especially under adversarial prompting-has become critical. Arguably, effective safety evaluations should be adaptive, evolving with LLM capabilities, and also cover a broad spectrum of harmful topics and real-world scenarios to fully expose potential vulnerabilities. Existing manual safety benchmarks, built on handcrafted adversarial prompts, are limited by their static nature and the intensive labor required to update them, making it difficult to keep pace with rapidly advancing LLMs. In contrast, automated adversarial prompt generation offers a promising path toward adaptive evaluation. However, current methods often suffer from insufficient adversarial topic coverage (topic-level diversity) and weak alignment with real-world contexts. These shortcomings stem from the exploration-exploitation dilemma in black-box optimization and a lack of real-world contextualization, resulting in adversarial prompts that are both topically narrow and scenario-repetitive. To address these issues, we propose Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses multi-objective reinforcement learning to fine-tune an adversarial LLM for generating topically diverse and contextually rich adversarial prompts. Experiments show that ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with notable improvements in integrated evaluation metrics. We hope ROSE represents a step toward more practical and reality-oriented safety evaluation of LLMs. WARNING: This paper contains examples of potentially harmful text.</li>
</ul>

<h3>Title: LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing</h3>
<ul>
<li><strong>Authors: </strong>Wenbing Li, Zikai Song, Hang Zhou, Yunyao Zhang, Junqing Yu, Wei Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00029">https://arxiv.org/abs/2507.00029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00029">https://arxiv.org/pdf/2507.00029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00029]] LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing(https://arxiv.org/abs/2507.00029)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts (MoE) for adapting large language models (LLMs) to multiple tasks still exhibit prevailing limitations: they either swap entire attention/feed-forward layers for switch experts or bolt on parallel expert branches, diluting parameter efficiency and task fidelity. We propose the LoRA-Mixer, a modular and lightweight MoE framework that integrates LoRA experts. Our core innovation lies in replacing the projection matrices of the attention module's input/output linear layers with dynamically routed, task-specific LoRA experts. This design ensures seamless compatibility with diverse foundation models, including transformers and state space models (SSMs), by leveraging their inherent linear projection structures. The framework supports two operational paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a novel hard-soft routing strategy, or (2) direct deployment of pre-trained, frozen LoRA modules sourced from external repositories. To enable robust router training with limited data while ensuring stable routing decisions and maximizing expert reuse, we introduce an adaptive Specialization Balance Loss (SBL) that jointly optimizes expert balance and task-specific alignment. Extensive experiments on seven benchmark datasets, including MedQA, CoLA, SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base models, respectively. Compared with state-of-the-art methods, LoRA-Mixer achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively, using only 48% of the parameters, demonstrating its efficiency and strong performance.</li>
</ul>

<h3>Title: Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru</h3>
<ul>
<li><strong>Authors: </strong>Chuan Li, Jiang You, Hassine Moungla, Vincent Gauthier, Miguel Nunez-del-Prado, Hugo Alatrista-Salas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00031">https://arxiv.org/abs/2507.00031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00031">https://arxiv.org/pdf/2507.00031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00031]] Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru(https://arxiv.org/abs/2507.00031)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate modeling of human mobility is critical for understanding epidemic spread and deploying timely interventions. In this work, we leverage a large-scale spatio-temporal dataset collected from Peru's national Digital Contact Tracing (DCT) application during the COVID-19 pandemic to forecast mobility flows across urban regions. A key challenge lies in the spatial sparsity of hourly mobility counts across hexagonal grid cells, which limits the predictive power of conventional time series models. To address this, we propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN) technique that augments each cell's features with aggregated signals from its immediate H3 neighbors. We evaluate this strategy on three forecasting backbones: NLinear, PatchTST, and K-U-Net, under various historical input lengths. Experimental results show that SPN consistently improves forecasting performance, achieving up to 9.85 percent reduction in test MSE. Our findings demonstrate that spatial smoothing of sparse mobility signals provides a simple yet effective path toward robust spatio-temporal forecasting during public health crises.</li>
</ul>

<h3>Title: Moment Sampling in Video LLMs for Long-Form Video QA</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Chasmai, Gauri Jagatap, Gouthaman KV, Grant Van Horn, Subhransu Maji, Andrea Fanelli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00033">https://arxiv.org/abs/2507.00033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00033">https://arxiv.org/pdf/2507.00033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00033]] Moment Sampling in Video LLMs for Long-Form Video QA(https://arxiv.org/abs/2507.00033)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in video large language models (Video LLMs) have significantly advanced the field of video question answering (VideoQA). While existing methods perform well on short videos, they often struggle with long-range reasoning in longer videos. To scale Video LLMs for longer video content, frame sub-sampling (selecting frames at regular intervals) is commonly used. However, this approach is suboptimal, often leading to the loss of crucial frames or the inclusion of redundant information from multiple similar frames. Missing key frames impairs the model's ability to answer questions accurately, while redundant frames lead the model to focus on irrelevant video segments and increase computational resource consumption. In this paper, we investigate the use of a general-purpose text-to-video moment retrieval model to guide the frame sampling process. We propose "moment sampling", a novel, model-agnostic approach that enables the model to select the most relevant frames according to the context of the question. Specifically, we employ a lightweight moment retrieval model to prioritize frame selection. By focusing on the frames most pertinent to the given question, our method enhances long-form VideoQA performance in Video LLMs. Through extensive experiments on four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we demonstrate the effectiveness of the proposed approach.</li>
</ul>

<h3>Title: Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing</h3>
<ul>
<li><strong>Authors: </strong>Lucas Potin, Rosa Figueiredo, Vincent Labatut, Christine Largeron</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00039">https://arxiv.org/abs/2507.00039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00039">https://arxiv.org/pdf/2507.00039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00039]] Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing(https://arxiv.org/abs/2507.00039)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Graph classification aims to categorize graphs based on their structural and attribute features, with applications in diverse fields such as social network analysis and bioinformatics. Among the methods proposed to solve this task, those relying on patterns (i.e. subgraphs) provide good explainability, as the patterns used for classification can be directly interpreted. To identify meaningful patterns, a standard approach is to use a quality measure, i.e. a function that evaluates the discriminative power of each pattern. However, the literature provides tens of such measures, making it difficult to select the most appropriate for a given application. Only a handful of surveys try to provide some insight by comparing these measures, and none of them specifically focuses on graphs. This typically results in the systematic use of the most widespread measures, without thorough evaluation. To address this issue, we present a comparative analysis of 38 quality measures from the literature. We characterize them theoretically, based on four mathematical properties. We leverage publicly available datasets to constitute a benchmark, and propose a method to elaborate a gold standard ranking of the patterns. We exploit these resources to perform an empirical comparison of the measures, both in terms of pattern ranking and classification performance. Moreover, we propose a clustering-based preprocessing step, which groups patterns appearing in the same graphs to enhance classification performance. Our experimental results demonstrate the effectiveness of this step, reducing the number of patterns to be processed while achieving comparable performance. Additionally, we show that some popular measures widely used in the literature are not associated with the best results.</li>
</ul>

<h3>Title: MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations</h3>
<ul>
<li><strong>Authors: </strong>Mehmet Yigit Avci, Pedro Borges, Paul Wright, Mehmet Yigitsoy, Sebastien Ourselin, Jorge Cardoso</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00043">https://arxiv.org/abs/2507.00043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00043">https://arxiv.org/pdf/2507.00043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00043]] MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations(https://arxiv.org/abs/2507.00043)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate interpretation of Magnetic Resonance Imaging scans in clinical systems is based on a precise understanding of image contrast. This contrast is primarily governed by acquisition parameters, such as echo time and repetition time, which are stored in the DICOM metadata. To simplify contrast identification, broad labels such as T1-weighted or T2-weighted are commonly used, but these offer only a coarse approximation of the underlying acquisition settings. In many real-world datasets, such labels are entirely missing, leaving raw acquisition parameters as the only indicators of contrast. Adding to this challenge, the available metadata is often incomplete, noisy, or inconsistent. The lack of reliable and standardized metadata complicates tasks such as image interpretation, retrieval, and integration into clinical workflows. Furthermore, robust contrast-aware representations are essential to enable more advanced clinical applications, such as achieving modality-invariant representations and data harmonization. To address these challenges, we propose MR-CLIP, a multimodal contrastive learning framework that aligns MR images with their DICOM metadata to learn contrast-aware representations, without relying on manual labels. Trained on a diverse clinical dataset that spans various scanners and protocols, MR-CLIP captures contrast variations across acquisitions and within scans, enabling anatomy-invariant representations. We demonstrate its effectiveness in cross-modal retrieval and contrast classification, highlighting its scalability and potential for further clinical applications. The code and weights are publicly available at this https URL.</li>
</ul>

<h3>Title: HistoART: Histopathology Artifact Detection and Reporting Tool</h3>
<ul>
<li><strong>Authors: </strong>Seyed Kahaki, Alexander R. Webber, Ghada Zamzmi, Adarsh Subbaswamy, Rucha Deshpande, Aldo Badano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00044">https://arxiv.org/abs/2507.00044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00044">https://arxiv.org/pdf/2507.00044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00044]] HistoART: Histopathology Artifact Detection and Reporting Tool(https://arxiv.org/abs/2507.00044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to digitize tissue specimens for detailed, high-resolution examination; however, other diagnostic approaches, such as liquid biopsy and molecular testing, are also utilized based on the cancer type and clinical context. While WSI has revolutionized digital histopathology by enabling automated, precise analysis, it remains vulnerable to artifacts introduced during slide preparation and scanning. These artifacts can compromise downstream image analysis. To address this challenge, we propose and compare three robust artifact detection approaches for WSIs: (1) a foundation model-based approach (FMA) using a fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach (KBA) leveraging handcrafted features from texture, color, and frequency-based metrics. The methods target six common artifact types: tissue folds, out-of-focus regions, air bubbles, tissue damage, marker traces, and blood contamination. Evaluations were conducted on 50,000+ image patches from diverse scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]), outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978]) and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into actionable insights, we developed a quality report scorecard that quantifies high-quality patches and visualizes artifact distributions.</li>
</ul>

<h3>Title: CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Chenguang Wang, Yijun Liang, Xiyao Wang, Yuhang Zhou, Xiyang Wu, Yuqing Zhang, Ruiyi Zhang, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00045">https://arxiv.org/abs/2507.00045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00045">https://arxiv.org/pdf/2507.00045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00045]] CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning(https://arxiv.org/abs/2507.00045)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives? To answer this question, we investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task. CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.</li>
</ul>

<h3>Title: Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process</h3>
<ul>
<li><strong>Authors: </strong>Akshansh Mishra, Eyob Mesele Sefene, Shivraman Thapliyal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00046">https://arxiv.org/abs/2507.00046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00046">https://arxiv.org/pdf/2507.00046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00046]] Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process(https://arxiv.org/abs/2507.00046)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.</li>
</ul>

<h3>Title: VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Binesh Sadanandan, Vahid Behzadan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00052">https://arxiv.org/abs/2507.00052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00052">https://arxiv.org/pdf/2507.00052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00052]] VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models(https://arxiv.org/abs/2507.00052)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) hold great promise for streamlining labour-intensive medical imaging workflows, yet systematic security evaluations in clinical settings remain scarce. We introduce VSF--Med, an end-to-end vulnerability-scoring framework for medical VLMs that unites three novel components: (i) a rich library of sophisticated text-prompt attack templates targeting emerging threat vectors; (ii) imperceptible visual perturbations calibrated by structural similarity (SSIM) thresholds to preserve clinical realism; and (iii) an eight-dimensional rubric evaluated by two independent judge LLMs, whose raw scores are consolidated via z-score normalization to yield a 0--32 composite risk metric. Built entirely on publicly available datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000 adversarial variants from 5,000 radiology images and enables reproducible benchmarking of any medical VLM with a single command. Our consolidated analysis reports mean z-score shifts of $0.90\sigma$ for persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness, and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs. Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection attacks.</li>
</ul>

<h3>Title: MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Zhong, Daniel Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00068">https://arxiv.org/abs/2507.00068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00068">https://arxiv.org/pdf/2507.00068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00068]] MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding(https://arxiv.org/abs/2507.00068)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While multi-modal learning has advanced significantly, current approaches often treat modalities separately, creating inconsistencies in representation and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization via Textual Alignment), a theoretically-grounded framework that unifies visual and auditory inputs into a structured textual space for seamless processing with large language models. MANTA addresses four key challenges: (1) semantic alignment across modalities with information-theoretic optimization, (2) adaptive temporal synchronization for varying information densities, (3) hierarchical content representation for multi-scale understanding, and (4) context-aware retrieval of sparse information from long sequences. We formalize our approach within a rigorous mathematical framework, proving its optimality for context selection under token constraints. Extensive experiments on the challenging task of Long Video Question Answering show that MANTA improves state-of-the-art models by up to 22.6% in overall accuracy, with particularly significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement) and cross-modal understanding (25.1% improvement). Our framework introduces novel density estimation techniques for redundancy minimization while preserving rare signals, establishing new foundations for unifying multimodal representations through structured text.</li>
</ul>

<h3>Title: Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap</h3>
<ul>
<li><strong>Authors: </strong>Yifan Sun, Yushan Liang, Zhen Zhang, Jiaye Teng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00075">https://arxiv.org/abs/2507.00075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00075">https://arxiv.org/pdf/2507.00075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00075]] Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap(https://arxiv.org/abs/2507.00075)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-improvement is among the most prominent techniques within the realm of large language models (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In this paper, we theoretically model the training dynamics of self-improvement via the concept of solver-verifier gap. This is inspired by the conjecture that the performance enhancement of self-improvement stems from the gap between LLM's solver capability and verifier capability. Based on the theoretical framework, we further introduce how to predict the ultimate power of self-improvement using only information from the first few training epochs. We empirically validate the effectiveness of the theoretical model on various LLMs and datasets. Beyond self-improvement, we extend our analysis to investigate how external data influences these dynamics within the framework. Notably, we find that under limited external data regimes, such external data can be utilized at any stage without significantly affecting final performances, which accords with the empirical observations.</li>
</ul>

<h3>Title: The language of time: a language model perspective on time-series foundation models</h3>
<ul>
<li><strong>Authors: </strong>Yi Xie, Yun Xiong, Zejian Shi, Hao Niu, Zhengfu Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00078">https://arxiv.org/abs/2507.00078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00078">https://arxiv.org/pdf/2507.00078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00078]] The language of time: a language model perspective on time-series foundation models(https://arxiv.org/abs/2507.00078)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the rise of large language models, the paradigm of training foundation models with massive parameter counts on vast datasets has been adopted in multiple domains to achieve remarkable success. Time series foundation models represent a significant extension of this paradigm, demonstrating exceptional expressive power, generalization, and cross-domain transferability. However, this gives rise to a fundamental paradox: time series data reflect distinct dynamical systems, making cross-domain transfer intuitively implausible, yet this is contradicted by the models' empirical success. To resolve this paradox, this paper investigates, from both theoretical and experimental perspectives, the representation learning mechanisms and generalization capabilities of patch-based time series foundation models. We argue that such models are not merely applying a new architecture but are fundamentally generalizing the representation paradigm of language models by extending deterministic vector-based representations to latent probabilistic distributional forms. Our theoretical analysis supports this framework by demonstrating that continuous time-series patches can be faithfully quantized into a discrete vocabulary whose key statistical properties are highly consistent with those of natural language. This generalization allows time series models to inherit the robust representation and transfer abilities of large language models, thereby explaining their superior performance in temporal tasks. Ultimately, our work provides a rigorous theoretical cornerstone for understanding, evaluating, and improving the safety and reliability of large-scale time series foundation models.</li>
</ul>

<h3>Title: Online Meal Detection Based on CGM Data Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Ali Tavasoli, Heman Shakeri</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.AO, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00080">https://arxiv.org/abs/2507.00080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00080">https://arxiv.org/pdf/2507.00080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00080]] Online Meal Detection Based on CGM Data Dynamics(https://arxiv.org/abs/2507.00080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>We utilize dynamical modes as features derived from Continuous Glucose Monitoring (CGM) data to detect meal events. By leveraging the inherent properties of underlying dynamics, these modes capture key aspects of glucose variability, enabling the identification of patterns and anomalies associated with meal consumption. This approach not only improves the accuracy of meal detection but also enhances the interpretability of the underlying glucose dynamics. By focusing on dynamical features, our method provides a robust framework for feature extraction, facilitating generalization across diverse datasets and ensuring reliable performance in real-world applications. The proposed technique offers significant advantages over traditional approaches, improving detection accuracy,</li>
</ul>

<h3>Title: Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission</h3>
<ul>
<li><strong>Authors: </strong>Faranaksadat Solat, Joohyung Lee, Mohamed Seif, Dusit Niyato, H. Vincent Poor</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00082">https://arxiv.org/abs/2507.00082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00082">https://arxiv.org/pdf/2507.00082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00082]] Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission(https://arxiv.org/abs/2507.00082)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Hybrid Language Models (HLMs) combine the low-latency efficiency of Small Language Models (SLMs) on edge devices with the high accuracy of Large Language Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM inference, HLMs reduce latency and communication by invoking LLMs only when local SLM predictions are uncertain, i.e., when token-level confidence is low or entropy is high. However, ambiguous or low-confidence predictions still require frequent offloading to the LLM, leading to significant communication overhead in bandwidth-constrained settings. To address this, we propose FedHLM, a communication-efficient HLM framework that integrates uncertainty-aware inference with Federated Learning (FL). FedHLM's key innovation lies in collaboratively learning token-level uncertainty thresholds that govern when LLM assistance is needed. Rather than using static or manually tuned thresholds, FedHLM employs FL to optimize these thresholds in a privacy-preserving, distributed manner. Additionally, it leverages embedding-based token representations for Peer-to-Peer (P2P) resolution, enabling clients to reuse tokens inferred by semantically similar peers without engaging the LLM. We further introduce hierarchical model aggregation: edge servers refine local routing policies through client updates, while cross-cluster coordination aligns global decision boundaries. This layered design captures recurring uncertainty patterns, reducing redundant LLM queries. Experiments on large-scale news classification tasks show that FedHLM reduces LLM transmissions by over 95 percent with negligible accuracy loss, making it well-suited for scalable and efficient edge-AI applications.</li>
</ul>

<h3>Title: A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism</h3>
<ul>
<li><strong>Authors: </strong>Ruiyuan Jiang, Dongyao Jia, Eng Gee Lim, Pengfei Fan, Yuli Zhang, Shangbo Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00085">https://arxiv.org/abs/2507.00085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00085">https://arxiv.org/pdf/2507.00085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00085]] A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism(https://arxiv.org/abs/2507.00085)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate traffic prediction is essential for Intelligent Transportation Systems (ITS), yet current methods struggle with the inherent complexity and non-linearity of traffic dynamics, making it difficult to integrate spatial and temporal characteristics. Furthermore, existing approaches use static techniques to address non-stationary and anomalous historical data, which limits adaptability and undermines data smoothing. To overcome these challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative framework for network-level traffic speed prediction. GFEN introduces a novel topological spatiotemporal graph fusion technique that meticulously extracts and merges spatial and temporal correlations from both data distribution and network topology using trainable methods, enabling the modeling of multi-scale spatiotemporal features. Additionally, GFEN employs a hybrid methodology combining a k-th order difference-based mathematical framework with an attention-based deep learning structure to adaptively smooth historical observations and dynamically mitigate data anomalies and non-stationarity. Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods by approximately 6.3% in prediction accuracy and exhibits convergence rates nearly twice as fast as recent hybrid models, confirming its superior performance and potential to significantly enhance traffic prediction system efficiency.</li>
</ul>

<h3>Title: pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation</h3>
<ul>
<li><strong>Authors: </strong>Jiale Zhao, Pengzhi Mao, Kaifei Wang, Yiming Li, Yaping Peng, Ranfei Chen, Shuqi Lu, Xiaohong Ji, Jiaxiang Ding, Xin Zhang, Yucheng Liao, Weinan E, Weijie Zhang, Han Wen, Hao Chi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00087">https://arxiv.org/abs/2507.00087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00087">https://arxiv.org/pdf/2507.00087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00087]] pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation(https://arxiv.org/abs/2507.00087)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning has advanced mass spectrometry data interpretation, yet most models remain feature extractors rather than unified scoring frameworks. We present pUniFind, the first large-scale multimodal pre-trained model in proteomics that integrates end-to-end peptide-spectrum scoring with open, zero-shot de novo sequencing. Trained on over 100 million open search-derived spectra, pUniFind aligns spectral and peptide modalities via cross modality prediction and outperforms traditional engines across diverse datasets, particularly achieving a 42.6 percent increase in the number of identified peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind identifies 60 percent more PSMs than existing de novo methods despite a 300-fold larger search space. A deep learning based quality control module further recovers 38.5 percent additional peptides including 1,891 mapped to the genome but absent from reference proteomes while preserving full fragment ion coverage. These results establish a unified, scalable deep learning framework for proteomic analysis, offering improved sensitivity, modification coverage, and interpretability.</li>
</ul>

<h3>Title: A new machine learning framework for occupational accidents forecasting with safety inspections integration</h3>
<ul>
<li><strong>Authors: </strong>Aho Yapi, Pierre Latouche, Arnaud Guillin, Yan Bailly</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00089">https://arxiv.org/abs/2507.00089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00089">https://arxiv.org/pdf/2507.00089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00089]] A new machine learning framework for occupational accidents forecasting with safety inspections integration(https://arxiv.org/abs/2507.00089)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a generic framework for short-term occupational accident forecasting that leverages safety inspections and models accident occurrences as binary time series. The approach generates daily predictions, which are then aggregated into weekly safety assessments to better inform decision making. To ensure the reliability and operational applicability of the forecasts, we apply a sliding-window cross-validation procedure specifically designed for time series data, combined with an evaluation based on aggregated period-level metrics. Several machine learning algorithms, including logistic regression, tree-based models, and neural networks, are trained and systematically compared within this framework. Unlike the other approaches, the long short-term memory (LSTM) network outperforms the other approaches and detects the upcoming high-risk periods with a balanced accuracy of 0.86, confirming the robustness of our methodology and demonstrating that a binary time series model can anticipate these critical periods based on safety inspections. The proposed methodology converts routine safety inspection data into clear weekly risk scores, detecting the periods when accidents are most likely. Decision-makers can integrate these scores into their planning tools to classify inspection priorities, schedule targeted interventions, and funnel resources to the sites or shifts classified as highest risk, stepping in before incidents occur and getting the greatest return on safety investments.</li>
</ul>

<h3>Title: Generating Heterogeneous Multi-dimensional Data : A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Corbeau Michael, Claeys Emmanuelle, Serrurier Mathieu, Zarat√© Pascale</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00090">https://arxiv.org/abs/2507.00090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00090">https://arxiv.org/pdf/2507.00090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00090]] Generating Heterogeneous Multi-dimensional Data : A Comparative Study(https://arxiv.org/abs/2507.00090)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Allocation of personnel and material resources is highly sensible in the case of firefighter interventions. This allocation relies on simulations to experiment with various scenarios. The main objective of this allocation is the global optimization of the firefighters response. Data generation is then mandatory to study various scenarios In this study, we propose to compare different data generation methods. Methods such as Random Sampling, Tabular Variational Autoencoders, standard Generative Adversarial Networks, Conditional Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are examined to ascertain their efficacy in capturing the intricacies of firefighter interventions. Traditional evaluation metrics often fall short in capturing the nuanced requirements of synthetic datasets for real-world scenarios. To address this gap, an evaluation of synthetic data quality is conducted using a combination of domain-specific metrics tailored to the firefighting domain and standard measures such as the Wasserstein distance. Domain-specific metrics include response time distribution, spatial-temporal distribution of interventions, and accidents representation. These metrics are designed to assess data variability, the preservation of fine and complex correlations and anomalies such as event with a very low occurrence, the conformity with the initial statistical distribution and the operational relevance of the synthetic data. The distribution has the particularity of being highly unbalanced, none of the variables following a Gaussian distribution, adding complexity to the data generation process.</li>
</ul>

<h3>Title: AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets</h3>
<ul>
<li><strong>Authors: </strong>Ailiya Borjigin, Wei Zhou, Cong He</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00096">https://arxiv.org/abs/2507.00096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00096">https://arxiv.org/pdf/2507.00096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00096]] AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets(https://arxiv.org/abs/2507.00096)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Alternative Assets tokenization is transforming non-traditional financial instruments are represented and traded on the web. However, ensuring trustworthiness in web-based tokenized ecosystems poses significant challenges, from verifying off-chain asset data to enforcing regulatory compliance. This paper proposes an AI-governed agent architecture that integrates intelligent agents with blockchain to achieve web-trustworthy tokenization of alternative assets. In the proposed architecture, autonomous agents orchestrate the tokenization process (asset verification, valuation, compliance checking, and lifecycle management), while an AI-driven governance layer monitors agent behavior and enforces trust through adaptive policies and cryptoeconomic incentives. We demonstrate that this approach enhances transparency, security, and compliance in asset tokenization, addressing key concerns around data authenticity and fraud. A case study on tokenizing real estate assets illustrates how the architecture mitigates risks (e.g., fraudulent listings and money laundering) through real-time AI anomaly detection and on-chain enforcement. Our evaluation and analysis suggest that combining AI governance with multi-agent systems and blockchain can significantly bolster trust in tokenized asset ecosystems. This work offers a novel framework for trustworthy asset tokenization on the web and provides insights for practitioners aiming to deploy secure, compliant tokenization platforms.</li>
</ul>

<h3>Title: Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series</h3>
<ul>
<li><strong>Authors: </strong>Bernd Hofmann, Patrick Bruendl, Huong Giang Nguyen, Joerg Franke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00102">https://arxiv.org/abs/2507.00102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00102">https://arxiv.org/pdf/2507.00102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00102]] Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series(https://arxiv.org/abs/2507.00102)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Ensuring consistent product quality in modern manufacturing is crucial, particularly in safety-critical applications. Conventional quality control approaches, reliant on manually defined thresholds and features, lack adaptability to the complexity and variability inherent in production data and necessitate extensive domain expertise. Conversely, data-driven methods, such as machine learning, demonstrate high detection performance but typically function as black-box models, thereby limiting their acceptance in industrial environments where interpretability is paramount. This paper introduces a methodology for industrial fault detection, which is both data-driven and transparent. The approach integrates a supervised machine learning model for multi-class fault classification, Shapley Additive Explanations for post-hoc interpretability, and a do-main-specific visualisation technique that maps model explanations to operator-interpretable features. Furthermore, the study proposes an evaluation methodology that assesses model explanations through quantitative perturbation analysis and evaluates visualisations by qualitative expert assessment. The approach was applied to the crimping process, a safety-critical joining technique, using a dataset of univariate, discrete time series. The system achieves a fault detection accuracy of 95.9 %, and both quantitative selectivity analysis and qualitative expert evaluations confirmed the relevance and inter-pretability of the generated explanations. This human-centric approach is designed to enhance trust and interpretability in data-driven fault detection, thereby contributing to applied system design in industrial quality control.</li>
</ul>

<h3>Title: AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise</h3>
<ul>
<li><strong>Authors: </strong>Hasan Yiƒüit</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET, cs.IT, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00145">https://arxiv.org/abs/2507.00145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00145">https://arxiv.org/pdf/2507.00145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00145]] AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise(https://arxiv.org/abs/2507.00145)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform entropy directly from physical noise, eliminating the need for bulky quantum devices or expensive laboratory-grade RF receivers. Instead, it relies on a low-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and then emits 32-bit high-entropy streams without any quantization step. Unlike deterministic or trained artificial intelligence random number generators (RNGs), our dynamic inner-outer network couples adaptive natural sources and reseeding, yielding truly unpredictable and autonomous sequences. Generated numbers pass the NIST SP 800-22 battery better than a CPU-based method. It also passes nineteen bespoke statistical tests for both bit- and integer-level analysis. All results satisfy cryptographic standards, while forward and backward prediction experiments reveal no exploitable biases. The model's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft cores, as well as suitable for other resource-constrained platforms. By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG broadens the reach of high-integrity random number generators across secure systems, cryptographic protocols, embedded and edge devices, stochastic simulations, and server applications that need randomness.</li>
</ul>

<h3>Title: Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data</h3>
<ul>
<li><strong>Authors: </strong>Ekaterina Borisova, Fabio Barth, Nils Feldhus, Raia Abu Ahmad, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Sebastian M√∂ller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00152">https://arxiv.org/abs/2507.00152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00152">https://arxiv.org/pdf/2507.00152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00152]] Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data(https://arxiv.org/abs/2507.00152)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Tables are among the most widely used tools for representing structured data in research, business, medicine, and education. Although LLMs demonstrate strong performance in downstream tasks, their efficiency in processing tabular data remains underexplored. In this paper, we investigate the effectiveness of both text-based and multimodal LLMs on table understanding tasks through a cross-domain and cross-modality evaluation. Specifically, we compare their performance on tables from scientific vs. non-scientific contexts and examine their robustness on tables represented as images vs. text. Additionally, we conduct an interpretability analysis to measure context usage and input relevance. We also introduce the TableEval benchmark, comprising 3017 tables from scholarly publications, Wikipedia, and financial reports, where each table is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX. Our findings indicate that while LLMs maintain robustness across table modalities, they face significant challenges when processing scientific tables.</li>
</ul>

<h3>Title: Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics</h3>
<ul>
<li><strong>Authors: </strong>Peter Mortimer, Mirko Maehlisch</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00153">https://arxiv.org/abs/2507.00153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00153">https://arxiv.org/pdf/2507.00153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00153]] Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics(https://arxiv.org/abs/2507.00153)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>The performance of leaning-based perception algorithms suffer when deployed in out-of-distribution and underrepresented environments. Outdoor robots are particularly susceptible to rapid changes in visual scene appearance due to dynamic lighting, seasonality and weather effects that lead to scenes underrepresented in the training data of the learning-based perception system. In this conceptual paper, we focus on preparing our autonomous vehicle for deployment in snow-filled environments. We propose a novel method for diffusion-based image augmentation to more closely represent the deployment environment in our training data. Diffusion-based image augmentations rely on the public availability of vision foundation models learned on internet-scale datasets. The diffusion-based image augmentations allow us to take control over the semantic distribution of the ground surfaces in the training data and to fine-tune our model for its deployment environment. We employ open vocabulary semantic segmentation models to filter out augmentation candidates that contain hallucinations. We believe that diffusion-based image augmentations can be extended to many other environments apart from snow surfaces, like sandy environments and volcanic terrains.</li>
</ul>

<h3>Title: Prompting as Scientific Inquiry</h3>
<ul>
<li><strong>Authors: </strong>Ari Holtzman, Chenhao Tan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00163">https://arxiv.org/abs/2507.00163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00163">https://arxiv.org/pdf/2507.00163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00163]] Prompting as Scientific Inquiry(https://arxiv.org/abs/2507.00163)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Prompting is the primary method by which we study and control large language models. It is also one of the most powerful: nearly every major capability attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was first unlocked through prompting. Yet prompting is rarely treated as science and is frequently frowned upon as alchemy. We argue that this is a category error. If we treat LLMs as a new kind of complex and opaque organism that is trained rather than programmed, then prompting is not a workaround: it is behavioral science. Mechanistic interpretability peers into the neural substrate, prompting probes the model in its native interface: language. We contend that prompting is not inferior, but rather a key component in the science of LLMs.</li>
</ul>

<h3>Title: SelvaBox: A high-resolution dataset for tropical tree crown detection</h3>
<ul>
<li><strong>Authors: </strong>Hugo Baudchon, Arthur Ouaknine, Martin Weiss, M√©lisande Teng, Thomas R. Walla, Antoine Caron-Guay, Christopher Pal, Etienne Lalibert√©</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00170">https://arxiv.org/abs/2507.00170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00170">https://arxiv.org/pdf/2507.00170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00170]] SelvaBox: A high-resolution dataset for tropical tree crown detection(https://arxiv.org/abs/2507.00170)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting individual tree crowns in tropical forests is essential to study these complex and crucial ecosystems impacted by human interventions and climate change. However, tropical crowns vary widely in size, structure, and pattern and are largely overlapping and intertwined, requiring advanced remote sensing methods applied to high-resolution imagery. Despite growing interest in tropical tree crown detection, annotated datasets remain scarce, hindering robust model development. We introduce SelvaBox, the largest open-access dataset for tropical tree crown detection in high-resolution drone imagery. It spans three countries and contains more than 83,000 manually labeled crowns - an order of magnitude larger than all previous tropical forest datasets combined. Extensive benchmarks on SelvaBox reveal two key findings: (1) higher-resolution inputs consistently boost detection accuracy; and (2) models trained exclusively on SelvaBox achieve competitive zero-shot detection performance on unseen tropical tree crown datasets, matching or exceeding competing methods. Furthermore, jointly training on SelvaBox and three other datasets at resolutions from 3 to 10 cm per pixel within a unified multi-resolution pipeline yields a detector ranking first or second across all evaluated datasets. Our dataset, code, and pre-trained weights are made public.</li>
</ul>

<h3>Title: Graph-Based Deep Learning for Component Segmentation of Maize Plants</h3>
<ul>
<li><strong>Authors: </strong>J. I. Ru√≠z, A. M√©ndez, E. Rodr√≠guez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00182">https://arxiv.org/abs/2507.00182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00182">https://arxiv.org/pdf/2507.00182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00182]] Graph-Based Deep Learning for Component Segmentation of Maize Plants(https://arxiv.org/abs/2507.00182)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In precision agriculture, one of the most important tasks when exploring crop production is identifying individual plant components. There are several attempts to accomplish this task by the use of traditional 2D imaging, 3D reconstructions, and Convolutional Neural Networks (CNN). However, they have several drawbacks when processing 3D data and identifying individual plant components. Therefore, in this work, we propose a novel Deep Learning architecture to detect components of individual plants on Light Detection and Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on the concept of Graph Neural Networks (GNN), and feature enhancing with Principal Component Analysis (PCA). For this, each point is taken as a vertex and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established, thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used to further increase the features of each point. Finally, Graph Attention Networks (GAT) are applied to classify visible phenotypic components of the plant, such as the leaf, stem, and soil. This study demonstrates that our graph-based deep learning approach enhances segmentation accuracy for identifying individual plant components, achieving percentages above 80% in the IoU average, thus outperforming other existing models based on point clouds.</li>
</ul>

<h3>Title: Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros</h3>
<ul>
<li><strong>Authors: </strong>Jacob Schrum, Olivia Kilday, Emilio Salas, Bess Hagan, Reid Williams</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00184">https://arxiv.org/abs/2507.00184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00184">https://arxiv.org/pdf/2507.00184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00184]] Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros(https://arxiv.org/abs/2507.00184)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Recent research shows how diffusion models can unconditionally generate tile-based game levels, but use of diffusion models for text-to-level generation is underexplored. There are practical considerations for creating a usable model: caption/level pairs are needed, as is a text embedding model, and a way of generating entire playable levels, rather than individual scenes. We present strategies to automatically assign descriptive captions to an existing level dataset, and train diffusion models using both pretrained text encoders and simple transformer models trained from scratch. Captions are automatically assigned to generated levels so that the degree of overlap between input and output captions can be compared. We also assess the diversity and playability of the resulting levels. Results are compared with an unconditional diffusion model and a generative adversarial network, as well as the text-to-level approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model uses a simple transformer model for text embedding, and takes less time to train than diffusion models employing more complex text encoders, indicating that reliance on larger language models is not necessary. We also present a GUI allowing designers to construct long levels from model-generated scenes.</li>
</ul>

<h3>Title: Plug. Play. Persist. Inside a Ready-to-Go Havoc C2 Infrastructure</h3>
<ul>
<li><strong>Authors: </strong>Alessio Di Santo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00189">https://arxiv.org/abs/2507.00189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00189">https://arxiv.org/pdf/2507.00189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00189]] Plug. Play. Persist. Inside a Ready-to-Go Havoc C2 Infrastructure(https://arxiv.org/abs/2507.00189)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>This analysis focuses on a single Azure-hosted Virtual Machine at this http URL that the adversary converted into an all-in-one delivery, staging and Command-and-Control node. The host advertises an out-of-date Apache 2.4.52 instance whose open directory exposes phishing lures, PowerShell loaders, Reflective Shell-Code, compiled Havoc Demon implants and a toolbox of lateral-movement binaries; the same server also answers on 8443/80 for encrypted beacon traffic. The web tier is riddled with publicly documented critical vulnerabilities, that would have allowed initial code-execution had the attackers not already owned the device. Initial access is delivered through an HTML file that, once de-obfuscated, perfectly mimics Google Unusual sign-in attempt notification and funnels victims toward credential collection. A PowerShell command follows: it disables AMSI in-memory, downloads a Base64-encoded stub, allocates RWX pages and starts the shell-code without ever touching disk. That stub reconstructs a DLL in memory using the Reflective-Loader technique and hands control to Havoc Demon implant. Every Demon variant-32- and 64-bit alike-talks to the same backend, resolves Windows APIs with hashed look-ups, and hides its activity behind indirect syscalls. Runtime telemetry shows interests in registry under Image File Execution Options, deliberate queries to Software Restriction Policy keys, and heavy use of Crypto DLLs to protect payloads and C2 traffic. The attacker toolkit further contains Chisel, PsExec, Doppelganger and Whisker, some of them re-compiled under user directories that leak the developer personas tonzking123 and thobt. Collectively the findings paint a picture of a technically adept actor who values rapid re-tooling over deep operational security, leaning on Havoc modularity and on legitimate cloud services to blend malicious flows into ordinary enterprise traffic.</li>
</ul>

<h3>Title: What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness</h3>
<ul>
<li><strong>Authors: </strong>Kumar Kshitij Patel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00195">https://arxiv.org/abs/2507.00195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00195">https://arxiv.org/pdf/2507.00195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00195]] What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness(https://arxiv.org/abs/2507.00195)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>This thesis contributes to the theoretical understanding of local update algorithms, especially Local SGD, in distributed and federated optimization under realistic models of data heterogeneity. A central focus is on the bounded second-order heterogeneity assumption, which is shown to be both necessary and sufficient for local updates to outperform centralized or mini-batch methods in convex and non-convex settings. The thesis establishes tight upper and lower bounds in several regimes for various local update algorithms and characterizes the min-max complexity of multiple problem classes. At its core is a fine-grained consensus-error-based analysis framework that yields sharper finite-time convergence bounds under third-order smoothness and relaxed heterogeneity assumptions. The thesis also extends to online federated learning, providing fundamental regret bounds under both first-order and bandit feedback. Together, these results clarify when and why local updates offer provable advantages, and the thesis serves as a self-contained guide for analyzing Local SGD in heterogeneous environments.</li>
</ul>

<h3>Title: LineRetriever: Planning-Aware Observation Reduction for Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Imene Kerboua, Sahar Omidi Shayegan, Megh Thakkar, Xing Han L√π, Massimo Caccia, V√©ronique Eglin, Alexandre Aussem, J√©r√©my Espinas, Alexandre Lacoste</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00210">https://arxiv.org/abs/2507.00210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00210">https://arxiv.org/pdf/2507.00210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00210]] LineRetriever: Planning-Aware Observation Reduction for Web Agents(https://arxiv.org/abs/2507.00210)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models have demonstrated impressive capabilities in web navigation tasks, the extensive context of web pages, often represented as DOM or Accessibility Tree (AxTree) structures, frequently exceeds model context limits. Current approaches like bottom-up truncation or embedding-based retrieval lose critical information about page state and action history. This is particularly problematic for adaptive planning in web agents, where understanding the current state is essential for determining future actions. We hypothesize that embedding models lack sufficient capacity to capture plan-relevant information, especially when retrieving content that supports future action prediction. This raises a fundamental question: how can retrieval methods be optimized for adaptive planning in web navigation tasks? In response, we introduce \textit{LineRetriever}, a novel approach that leverages a language model to identify and retrieve observation lines most relevant to future navigation steps. Unlike traditional retrieval methods that focus solely on semantic similarity, \textit{LineRetriever} explicitly considers the planning horizon, prioritizing elements that contribute to action prediction. Our experiments demonstrate that \textit{LineRetriever} can reduce the size of the observation at each step for the web agent while maintaining consistent performance within the context limitations.</li>
</ul>

<h3>Title: Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mads Henrichsen, Rasmus Krebs</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00214">https://arxiv.org/abs/2507.00214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00214">https://arxiv.org/pdf/2507.00214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00214]] Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning(https://arxiv.org/abs/2507.00214)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Standard classification models often map inputs directly to labels without explicit reasoning, potentially limiting their performance, robustness, and interpretability. This paper introduces a novel two-stage approach to enhance text classification by leveraging Large Language Model (LLM)-generated reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model (henceforth Llama-R-Gen) on a general-purpose reasoning dataset (syvai/reasoning-gen) to generate textual reasoning (R) given a question and its answer. In the second stage, this generally trained Llama-R-Gen is used offline to create an augmented training dataset for a downstream generative model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the input text (Q) and is trained to output the generated reasoning (R) immediately followed by the predicted emotion (A). We demonstrate this methodology on the dair-ai/emotion dataset for emotion classification. Our experiments show that the generative model trained to output reasoning and the emotion (Classifier Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy (for emotion prediction) compared to a baseline generative model trained solely to output the emotion (Classifier Q->A), highlighting the strong generalization capabilities of the reasoning generation and the benefit of explicit reasoning training. This work underscores the potential of LLM-generated reasonings for creating richer training datasets, thereby improving the performance of diverse downstream NLP tasks and providing explicit explanations.</li>
</ul>

<h3>Title: PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Peilin He, James Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00230">https://arxiv.org/abs/2507.00230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00230">https://arxiv.org/pdf/2507.00230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00230]] PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction(https://arxiv.org/abs/2507.00230)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust, federate, watermark</a></li>
<li><strong>Abstract: </strong>Reconstructing high-quality images from low-resolution inputs using Residual Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in collaborative scenarios where centralized training poses significant privacy risks, including data leakage and inference attacks, as well as high computational costs. We propose a novel Privacy-Preserving Federated Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image reconstruction. PPFL-RDSN integrates Federated Learning (FL), local differential privacy, and robust model watermarking techniques, ensuring data remains secure on local devices, safeguarding sensitive information, and maintaining model authenticity without revealing underlying data. Empirical evaluations show that PPFL-RDSN achieves comparable performance to the state-of-the-art centralized methods while reducing computational burdens, and effectively mitigates security and privacy vulnerabilities, making it a practical solution for secure and privacy-preserving collaborative computer vision applications.</li>
</ul>

<h3>Title: Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations</h3>
<ul>
<li><strong>Authors: </strong>Jiztom Kavalakkatt Francis, Matthew J Darr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00234">https://arxiv.org/abs/2507.00234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00234">https://arxiv.org/pdf/2507.00234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00234]] Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations(https://arxiv.org/abs/2507.00234)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we present a novel framework for enhancing model interpretability by integrating heatmaps produced separately by ResNet and a restructured 2D Transformer with globally weighted input saliency. We address the critical problem of spatial-temporal misalignment in existing interpretability methods, where convolutional networks fail to capture global context and Transformers lack localized precision - a limitation that impedes actionable insights in safety-critical domains like healthcare and industrial monitoring. Our method merges gradient-weighted activation maps (ResNet) and Transformer attention rollout into a unified visualization, achieving full spatial-temporal alignment while preserving real-time performance. Empirical evaluations on clinical (ECG arrhythmia detection) and industrial (energy consumption prediction) datasets demonstrate significant improvements: the hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy Appliance dataset-outperforming standalone ResNet, Transformer, and InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L (0.650) scores. By formalizing interpretability as causal fidelity and spatial-temporal alignment, our approach bridges the gap between technical outputs and stakeholder understanding, offering a scalable solution for transparent, time-aware decision-making.</li>
</ul>

<h3>Title: Linearly Decoding Refused Knowledge in Aligned Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aryan Shrivastava, Ari Holtzman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00239">https://arxiv.org/abs/2507.00239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00239">https://arxiv.org/pdf/2507.00239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00239]] Linearly Decoding Refused Knowledge in Aligned Language Models(https://arxiv.org/abs/2507.00239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Most commonly used language models (LMs) are instruction-tuned and aligned using a combination of fine-tuning and reinforcement learning, causing them to refuse users requests deemed harmful by the model. However, jailbreak prompts can often bypass these refusal mechanisms and elicit harmful responses. In this work, we study the extent to which information accessed via jailbreak prompts is decodable using linear probes trained on LM hidden states. We show that a great deal of initially refused information is linearly decodable. For example, across models, the response of a jailbroken LM for the average IQ of a country can be predicted by a linear probe with Pearson correlations exceeding $0.8$. Surprisingly, we find that probes trained on base models (which do not refuse) sometimes transfer to their instruction-tuned versions and are capable of revealing information that jailbreaks decode generatively, suggesting that the internal representations of many refused properties persist from base LMs through instruction-tuning. Importantly, we show that this information is not merely "leftover" in instruction-tuned models, but is actively used by them: we find that probe-predicted values correlate with LM generated pairwise comparisons, indicating that the information decoded by our probes align with suppressed generative behavior that may be expressed more subtly in other downstream tasks. Overall, our results suggest that instruction-tuning does not wholly eliminate or even relocate harmful information in representation space-they merely suppress its direct expression, leaving it both linearly accessible and indirectly influential in downstream behavior.</li>
</ul>

<h3>Title: VOCAL: Visual Odometry via ContrAstive Learning</h3>
<ul>
<li><strong>Authors: </strong>Chi-Yao Huang, Zeel Bhatt, Yezhou Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00243">https://arxiv.org/abs/2507.00243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00243">https://arxiv.org/pdf/2507.00243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00243]] VOCAL: Visual Odometry via ContrAstive Learning(https://arxiv.org/abs/2507.00243)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Breakthroughs in visual odometry (VO) have fundamentally reshaped the landscape of robotics, enabling ultra-precise camera state estimation that is crucial for modern autonomous systems. Despite these advances, many learning-based VO techniques rely on rigid geometric assumptions, which often fall short in interpretability and lack a solid theoretical basis within fully data-driven frameworks. To overcome these limitations, we introduce VOCAL (Visual Odometry via ContrAstive Learning), a novel framework that reimagines VO as a label ranking challenge. By integrating Bayesian inference with a representation learning framework, VOCAL organizes visual features to mirror camera states. The ranking mechanism compels similar camera states to converge into consistent and spatially coherent representations within the latent space. This strategic alignment not only bolsters the interpretability of the learned features but also ensures compatibility with multimodal data sources. Extensive evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability and flexibility, pushing VO toward more general and explainable spatial intelligence.</li>
</ul>

<h3>Title: Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition</h3>
<ul>
<li><strong>Authors: </strong>Nikita Nikitin, Eugene Fomin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00248">https://arxiv.org/abs/2507.00248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00248">https://arxiv.org/pdf/2507.00248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00248]] Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition(https://arxiv.org/abs/2507.00248)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We present a novel framework for real-time sign language recognition using lightweight DNNs trained on limited data. Our system addresses key challenges in sign language recognition, including data scarcity, high computational costs, and discrepancies in frame rates between training and inference environments. By encoding sign language specific parameters, such as handshape, palm orientation, movement, and location into vectorized inputs, and leveraging MediaPipe for landmark extraction, we achieve highly separable input data representations. Our DNN architecture, optimized for sub 10MB deployment, enables accurate classification of 343 signs with less than 10ms latency on edge devices. The data annotation platform 'slait data' facilitates structured labeling and vector extraction. Our model achieved 92% accuracy in isolated sign recognition and has been integrated into the 'slait ai' web application, where it demonstrates stable inference.</li>
</ul>

<h3>Title: Impact of Fine-Tuning Methods on Memorization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jie Hou, Chuxiong Wu, Lannan Luo, Qiang Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00258">https://arxiv.org/abs/2507.00258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00258">https://arxiv.org/pdf/2507.00258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00258]] Impact of Fine-Tuning Methods on Memorization in Large Language Models(https://arxiv.org/abs/2507.00258)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>As the capabilities of pre-trained large language models (LLMs) continue to advance, the "pre-train and fine-tune" paradigm has become increasingly mainstream, leading to the development of various fine-tuning methods. However, the privacy risks arising from memorization during fine-tuning have received relatively little attention. To address this gap, we categorize popular fine-tuning approaches and assess their impact on memorization through the lens of membership inference attacks (MIAs). Our results show that, compared to parameter-based fine-tuning, prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs. Furthermore, prompt-based methods maintain low memorization regardless of model scale. These findings suggest that parameter-based fine-tuning is more prone to leaking private information, whereas prompt-based fine-tuning serves as a more privacy-preserving option.</li>
</ul>

<h3>Title: Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Amr Abourayya, Jens Kleesiek, Bharat Rao, Michael Kamp</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00259">https://arxiv.org/abs/2507.00259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00259">https://arxiv.org/pdf/2507.00259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00259]] Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning(https://arxiv.org/abs/2507.00259)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Data heterogeneity is a central challenge in federated learning, and personalized federated learning (PFL) aims to address it by tailoring models to each client's distribution. Yet many PFL methods fail to outperform local or centralized baselines, suggesting a mismatch between the collaboration they enforce and the structure of the data. We propose an approach based on adaptive collaboration, where clients decide adaptively not only how much to rely on others, but also whom to trust at the level of individual examples. We instantiate this principle in FEDMOSAIC, a federated co-training method in which clients exchange predictions over a shared unlabeled dataset. This enables fine-grained trust decisions that are difficult to achieve with parameter sharing alone. Each client adjusts its loss weighting based on the agreement between private and public data, and contributes to global pseudo-labels in proportion to its estimated per-example confidence. Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across diverse non-IID settings, and we provide convergence guarantees under standard assumptions. Our results demonstrate the potential of data-aware collaboration for robust and effective personalization.</li>
</ul>

<h3>Title: Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections</h3>
<ul>
<li><strong>Authors: </strong>Vignesh Ram Nithin Kappagantula, Shayan Hassantabar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00263">https://arxiv.org/abs/2507.00263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00263">https://arxiv.org/pdf/2507.00263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00263]] Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections(https://arxiv.org/abs/2507.00263)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of vacation rental (VR) platforms has led to an increasing volume of property images, often uploaded without structured categorization. This lack of organization poses significant challenges for travelers attempting to understand the spatial layout of a property, particularly when multiple rooms of the same type are present. To address this issue, we introduce an effective approach for solving the room scene discovery and grouping problem, as well as identifying bed types within each bedroom group. This grouping is valuable for travelers to comprehend the spatial organization, layout, and the sleeping configuration of the property. We propose a computationally efficient machine learning pipeline characterized by low latency and the ability to perform effectively with sample-efficient learning, making it well-suited for real-time and data-scarce environments. The pipeline integrates a supervised room-type detection model, a supervised overlap detection model to identify the overlap similarity between two images, and a clustering algorithm to group the images of the same space together using the similarity scores. Additionally, the pipeline maps each bedroom group to the corresponding bed types specified in the property's metadata, based on the visual content present in the group's images using a Multi-modal Large Language Model (MLLM) model. We evaluate the aforementioned models individually and also assess the pipeline in its entirety, observing strong performance that significantly outperforms established approaches such as contrastive learning and clustering with pretrained embeddings.</li>
</ul>

<h3>Title: Examining Reject Relations in Stimulus Equivalence Simulations</h3>
<ul>
<li><strong>Authors: </strong>Alexis Carrillo, Asieh Abolpour Mofrad, Anis Yazidi, Moises Betancort</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00265">https://arxiv.org/abs/2507.00265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00265">https://arxiv.org/pdf/2507.00265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00265]] Examining Reject Relations in Stimulus Equivalence Simulations(https://arxiv.org/abs/2507.00265)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Simulations offer a valuable tool for exploring stimulus equivalence (SE), yet the potential of reject relations to disrupt the assessment of equivalence class formation is contentious. This study investigates the role of reject relations in the acquisition of stimulus equivalence using computational models. We examined feedforward neural networks (FFNs), bidirectional encoder representations from transformers (BERT), and generative pre-trained transformers (GPT) across 18 conditions in matching-to-sample (MTS) simulations. Conditions varied in training structure (linear series, one-to-many, and many-to-one), relation type (select-only, reject-only, and select-reject), and negative comparison selection (standard and biased). A probabilistic agent served as a benchmark, embodying purely associative learning. The primary goal was to determine whether artificial neural networks could demonstrate equivalence class formation or whether their performance reflected associative learning. Results showed that reject relations influenced agent performance. While some agents achieved high accuracy on equivalence tests, particularly with reject relations and biased negative comparisons, this performance was comparable to the probabilistic agent. These findings suggest that artificial neural networks, including transformer models, may rely on associative strategies rather than SE. This underscores the need for careful consideration of reject relations and more stringent criteria in computational models of equivalence.</li>
</ul>

<h3>Title: Self-Supervised Multiview Xray Matching</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Dabboussi, Malo Huard, Yann Gousseau, Pietro Gori</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00287">https://arxiv.org/abs/2507.00287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00287">https://arxiv.org/pdf/2507.00287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00287]] Self-Supervised Multiview Xray Matching(https://arxiv.org/abs/2507.00287)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Accurate interpretation of multi-view radiographs is crucial for diagnosing fractures, muscular injuries, and other anomalies. While significant advances have been made in AI-based analysis of single images, current methods often struggle to establish robust correspondences between different X-ray views, an essential capability for precise clinical evaluations. In this work, we present a novel self-supervised pipeline that eliminates the need for manual annotation by automatically generating a many-to-many correspondence matrix between synthetic X-ray views. This is achieved using digitally reconstructed radiographs (DRR), which are automatically derived from unannotated CT volumes. Our approach incorporates a transformer-based training phase to accurately predict correspondences across two or more X-ray views. Furthermore, we demonstrate that learning correspondences among synthetic X-ray views can be leveraged as a pretraining strategy to enhance automatic multi-view fracture detection on real data. Extensive evaluations on both synthetic and real X-ray datasets show that incorporating correspondences improves performance in multi-view fracture classification.</li>
</ul>

<h3>Title: MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic</h3>
<ul>
<li><strong>Authors: </strong>Yujun Zhang, Runlong Li, Xiaoxiang Liang, Xinhao Yang, Tian Su, Bo Liu, Yan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00304">https://arxiv.org/abs/2507.00304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00304">https://arxiv.org/pdf/2507.00304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00304]] MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic(https://arxiv.org/abs/2507.00304)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>The abnormal fluctuations in network traffic may indicate potential security threats or system failures. Therefore, efficient network traffic prediction and anomaly detection methods are crucial for network security and traffic management. This paper proposes a novel network traffic prediction and anomaly detection model, MamNet, which integrates time-domain modeling and frequency-domain feature extraction. The model first captures the long-term dependencies of network traffic through the Mamba module (time-domain modeling), and then identifies periodic fluctuations in the traffic using Fourier Transform (frequency-domain feature extraction). In the feature fusion layer, multi-scale information is integrated to enhance the model's ability to detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and CAIDA datasets demonstrate that MamNet outperforms several recent mainstream models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an improvement of approximately 2% to 4% in detection performance for complex traffic patterns and long-term trend detection. The results indicate that MamNet effectively captures anomalies in network traffic across different time scales and is suitable for anomaly detection tasks in network security and traffic management. Future work could further optimize the model structure by incorporating external network event information, thereby improving the model's adaptability and stability in complex network environments.</li>
</ul>

<h3>Title: Open-ended Scientific Discovery via Bayesian Surprise</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Agarwal, Bodhisattwa Prasad Majumder, Reece Adamson, Megha Chakravorty, Satvika Reddy Gavireddy, Aditya Parashar, Harshit Surana, Bhavana Dalvi Mishra, Andrew McCallum, Ashish Sabharwal, Peter Clark</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00310">https://arxiv.org/abs/2507.00310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00310">https://arxiv.org/pdf/2507.00310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00310]] Open-ended Scientific Discovery via Bayesian Surprise(https://arxiv.org/abs/2507.00310)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The promise of autonomous scientific discovery (ASD) hinges not only on answering questions, but also on knowing which questions to ask. Most recent works in ASD explore the use of large language models (LLMs) in goal-driven settings, relying on human-specified research questions to guide hypothesis generation. However, scientific discovery may be accelerated further by allowing the AI system to drive exploration by its own criteria. The few existing approaches in open-ended ASD select hypotheses based on diversity heuristics or subjective proxies for human interestingness, but the former struggles to meaningfully navigate the typically vast hypothesis space, and the latter suffers from imprecise definitions. This paper presents AutoDS -- a method for open-ended ASD that instead drives scientific exploration using Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior beliefs about a hypothesis to its posterior beliefs after gathering experimental results. To efficiently explore the space of nested hypotheses, our method employs a Monte Carlo tree search (MCTS) strategy with progressive widening using surprisal as the reward function. We evaluate AutoDS in the setting of data-driven discovery across 21 real-world datasets spanning domains such as biology, economics, finance, and behavioral science. Our results demonstrate that under a fixed budget, AutoDS substantially outperforms competitors by producing 5--29\% more discoveries deemed surprising by the LLM. Our human evaluation further finds that two-thirds of AutoDS discoveries are surprising to the domain experts, suggesting this is an important step forward towards building open-ended ASD systems.</li>
</ul>

<h3>Title: $Œº^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Siyou Li, Pengyao Qin, Huanan Wu, Dong Nie, Arun J. Thirunavukarasu, Juntao Yu, Le Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00316">https://arxiv.org/abs/2507.00316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00316">https://arxiv.org/pdf/2507.00316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00316]] $Œº^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation(https://arxiv.org/abs/2507.00316)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale $\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasetdemonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited data for RRG tasks.</li>
</ul>

<h3>Title: Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video</h3>
<ul>
<li><strong>Authors: </strong>Alexander Moore, Amar Saini, Kylie Cancilla, Doug Poland, Carmen Carrano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00339">https://arxiv.org/abs/2507.00339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00339">https://arxiv.org/pdf/2507.00339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00339]] Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video(https://arxiv.org/abs/2507.00339)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene. We introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. MOVi-MC-AC contributes to the growing literature of object detection, tracking, and segmentation by including two new contributions to the deep learning for computer vision world. Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content. The full dataset is available at this https URL ,</li>
</ul>

<h3>Title: Addressing malware family concept drift with triplet autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Numan Halit Guldemir, Oluwafemi Olukoya, Jes√∫s Mart√≠nez-del-Rinc√≥n</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00348">https://arxiv.org/abs/2507.00348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00348">https://arxiv.org/pdf/2507.00348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00348]] Addressing malware family concept drift with triplet autoencoder(https://arxiv.org/abs/2507.00348)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Machine learning is increasingly vital in cybersecurity, especially in malware detection. However, concept drift, where the characteristics of malware change over time, poses a challenge for maintaining the efficacy of these detection systems. Concept drift can occur in two forms: the emergence of entirely new malware families and the evolution of existing ones. This paper proposes an innovative method to address the former, focusing on effectively identifying new malware families. Our approach leverages a supervised autoencoder combined with triplet loss to differentiate between known and new malware families. We create clear and robust clusters that enhance the accuracy and resilience of malware family classification by utilizing this metric learning technique and the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm. The effectiveness of our method is validated using an Android malware dataset and a Windows portable executable (PE) malware dataset, showcasing its capability to sustain model performance within the dynamic landscape of emerging malware threats. Our results demonstrate a significant improvement in detecting new malware families, offering a reliable solution for ongoing cybersecurity challenges.</li>
</ul>

<h3>Title: Question Decomposition for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Paul J. L. Ammann, Jonas Golde, Alan Akbik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00355">https://arxiv.org/abs/2507.00355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00355">https://arxiv.org/pdf/2507.00355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00355]] Question Decomposition for Retrieval-Augmented Generation(https://arxiv.org/abs/2507.00355)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Grounding large language models (LLMs) in verifiable external sources is a well-established strategy for generating reliable answers. Retrieval-augmented generation (RAG) is one such approach, particularly effective for tasks like question answering: it retrieves passages that are semantically related to the question and then conditions the model on this evidence. However, multi-hop questions, such as "Which company among NVIDIA, Apple, and Google made the biggest profit in 2023?," challenge RAG because relevant facts are often distributed across multiple documents rather than co-occurring in one source, making it difficult for standard RAG to retrieve sufficient information. To address this, we propose a RAG pipeline that incorporates question decomposition: (i) an LLM decomposes the original query into sub-questions, (ii) passages are retrieved for each sub-question, and (iii) the merged candidate pool is reranked to improve the coverage and precision of the retrieved evidence. We show that question decomposition effectively assembles complementary documents, while reranking reduces noise and promotes the most relevant passages before answer generation. Although reranking itself is standard, we show that pairing an off-the-shelf cross-encoder reranker with LLM-driven question decomposition bridges the retrieval gap on multi-hop questions and provides a practical, drop-in enhancement, without any extra training or specialized indexing. We evaluate our approach on the MultiHop-RAG and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy (F1: +11.6%) over standard RAG baselines.</li>
</ul>

<h3>Title: An Improved U-Net Model for Offline handwriting signature denoising</h3>
<ul>
<li><strong>Authors: </strong>Wanghui Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00365">https://arxiv.org/abs/2507.00365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00365">https://arxiv.org/pdf/2507.00365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00365]] An Improved U-Net Model for Offline handwriting signature denoising(https://arxiv.org/abs/2507.00365)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Handwriting signatures, as an important means of identity recognition, are widely used in multiple fields such as financial transactions, commercial contracts and personal affairs due to their legal effect and uniqueness. In forensic science appraisals, the analysis of offline handwriting signatures requires the appraiser to provide a certain number of signature samples, which are usually derived from various historical contracts or archival materials. However, the provided handwriting samples are often mixed with a large amount of interfering information, which brings severe challenges to handwriting identification work. This study proposes a signature handwriting denoising model based on the improved U-net structure, aiming to enhance the robustness of the signature recognition system. By introducing discrete wavelet transform and PCA transform, the model's ability to suppress noise has been enhanced. The experimental results show that this modelis significantly superior to the traditional methods in denoising effect, can effectively improve the clarity and readability of the signed images, and provide more reliable technical support for signature analysis and recognition.</li>
</ul>

<h3>Title: PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching</h3>
<ul>
<li><strong>Authors: </strong>Xin Yang (1 and 2), Ruiming Du (3), Hanyang Huang (1 and 2), Jiayang Xie (1 and 2), Pengyao Xie (1 and 2), Leisen Fang (1 and 2), Ziyue Guo (1 and 2), Nanjun Jiang (4), Yu Jiang (5), Haiyan Cen (1 and 2) ((1) College of Biosystems Engineering and Food Science, Zhejiang University, (2) Key Laboratory of Spectroscopy Sensing, Ministry of Agriculture and Rural Affairs, (3) Department of Biological and Environmental Engineering, Cornell University, (4) Amway (China) Botanical R and D Center, (5) Horticulture Section, School of Integrative Plant Science, Cornell AgriTech)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00371">https://arxiv.org/abs/2507.00371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00371">https://arxiv.org/pdf/2507.00371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00371]] PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching(https://arxiv.org/abs/2507.00371)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Organ segmentation of plant point clouds is a prerequisite for the high-resolution and accurate extraction of organ-level phenotypic traits. Although the fast development of deep learning has boosted much research on segmentation of plant point clouds, the existing techniques for organ segmentation still face limitations in resolution, segmentation accuracy, and generalizability across various plant species. In this study, we proposed a novel approach called plant segmentation neural radiance fields (PlantSegNeRF), aiming to directly generate high-precision instance point clouds from multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF performed 2D instance segmentation on the multi-view images to generate instance masks for each organ with a corresponding ID. The multi-view instance IDs corresponding to the same plant organ were then matched and refined using a specially designed instance matching module. The instance NeRF was developed to render an implicit scene, containing color, density, semantic and instance information. The implicit scene was ultimately converted into high-precision plant instance point clouds based on the volume density. The results proved that in semantic segmentation of point clouds, PlantSegNeRF outperformed the commonly used methods, demonstrating an average improvement of 16.1%, 18.3%, 17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the second-best results on structurally complex datasets. More importantly, PlantSegNeRF exhibited significant advantages in plant point cloud instance segmentation tasks. Across all plant datasets, it achieved average improvements of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively. This study extends the organ-level plant phenotyping and provides a high-throughput way to supply high-quality 3D data for the development of large-scale models in plant science.</li>
</ul>

<h3>Title: MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Xie, Ziang Zhang, Zhenyu Weng, Yuesheng Zhu, Guibo Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00377">https://arxiv.org/abs/2507.00377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00377">https://arxiv.org/pdf/2507.00377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00377]] MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis(https://arxiv.org/abs/2507.00377)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements in deep learning for medical image segmentation are often limited by the scarcity of high-quality training this http URL diffusion models provide a potential solution by generating synthetic images, their effectiveness in medical imaging remains constrained due to their reliance on large-scale medical datasets and the need for higher image quality. To address these challenges, we present MedDiff-FT, a controllable medical image generation method that fine-tunes a diffusion foundation model to produce medical images with structural dependency and domain specificity in a data-efficient manner. During inference, a dynamic adaptive guiding mask enforces spatial constraints to ensure anatomically coherent synthesis, while a lightweight stochastic mask generator enhances diversity through hierarchical randomness injection. Additionally, an automated quality assessment protocol filters suboptimal outputs using feature-space metrics, followed by mask corrosion to refine fidelity. Evaluated on five medical segmentation datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's segmentation performance by an average of 1% in Dice score. The framework effectively balances generation quality, diversity, and computational efficiency, offering a practical solution for medical data augmentation. The code is available at this https URL.</li>
</ul>

<h3>Title: Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics</h3>
<ul>
<li><strong>Authors: </strong>Vojtƒõch Lanz, Jan Hajiƒç jr</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00380">https://arxiv.org/abs/2507.00380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00380">https://arxiv.org/pdf/2507.00380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00380]] Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics(https://arxiv.org/abs/2507.00380)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The idea that Gregorian melodies are constructed from some vocabulary of segments has long been a part of chant scholarship. This so-called "centonisation" theory has received much musicological criticism, but frequent re-use of certain melodic segments has been observed in chant melodies, and the intractable number of possible segmentations allowed the option that some undiscovered segmentation exists that will yet prove the value of centonisation, and recent empirical results have shown that segmentations can outperform music-theoretical features in mode classification. Inspired by the fact that Gregorian chant was memorised, we search for an optimal unsupervised segmentation of chant melody using nested hierarchical Pitman-Yor language models. The segmentation we find achieves state-of-the-art performance in mode classification. Modeling a monk memorising the melodies from one liturgical manuscript, we then find empirical evidence for the link between mode classification and memory efficiency, and observe more formulaic areas at the beginnings and ends of melodies corresponding to the practical role of modality in performance. However, the resulting segmentations themselves indicate that even such a memory-optimal segmentation is not what is understood as centonisation.</li>
</ul>

<h3>Title: Causal Prompting for Implicit Sentiment Analysis with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jing Ren, Wenhao Zhou, Bowen Li, Mujie Liu, Nguyen Linh Dan Le, Jiade Cen, Liping Chen, Ziqi Xu, Xiwei Xu, Xiaodong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00389">https://arxiv.org/abs/2507.00389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00389">https://arxiv.org/pdf/2507.00389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00389]] Causal Prompting for Implicit Sentiment Analysis with Large Language Models(https://arxiv.org/abs/2507.00389)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied rather than explicitly stated, requiring models to perform deeper reasoning over subtle contextual cues. While recent prompting-based methods using Large Language Models (LLMs) have shown promise in ISA, they often rely on majority voting over chain-of-thought (CoT) reasoning paths without evaluating their causal validity, making them susceptible to internal biases and spurious correlations. To address this challenge, we propose CAPITAL, a causal prompting framework that incorporates front-door adjustment into CoT reasoning. CAPITAL decomposes the overall causal effect into two components: the influence of the input prompt on the reasoning chains, and the impact of those chains on the final output. These components are estimated using encoder-based clustering and the NWGM approximation, with a contrastive learning objective used to better align the encoder's representation with the LLM's reasoning space. Experiments on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently outperforms strong prompting baselines in both accuracy and robustness, particularly under adversarial conditions. This work offers a principled approach to integrating causal inference into LLM prompting and highlights its benefits for bias-aware sentiment reasoning. The source code and case study are available at: this https URL.</li>
</ul>

<h3>Title: MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE</h3>
<ul>
<li><strong>Authors: </strong>Geng Zhang, Yuxuan Han, Yuxuan Lou, Wangbo Zhao, Yiqi Zhang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00390">https://arxiv.org/abs/2507.00390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00390">https://arxiv.org/pdf/2507.00390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00390]] MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE(https://arxiv.org/abs/2507.00390)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) enables efficient scaling of large language models by activating only a subset of experts per input token. However, deploying MoE-based models incurs significant memory overhead due to the need to retain all experts in memory. While structured pruning is promising to reduce memory costs, existing methods often show suboptimal performance and unstable degradation in three dimensions: model architectures, calibration data sources, and calibration sample sizes. This paper proposes Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that replaces redundant experts with lightweight novices to achieve effective and robust model compression. MoNE evaluates expert redundancy based on two metrics: access frequency and output variance. Experts exhibiting low usage and stable outputs are pruned and replaced with lightweight novices-unbiased estimations of their original outputs-minimizing performance degradation. Extensive experiments demonstrate that MoNE consistently outperforms baseline methods with minimal accuracy degradation across the three dimensions, confirming its effectiveness and robustness. Notably, it improves the average zero shot accuracy across nine downstream tasks by up to 2.71 under 25\% pruning ratio and 3.61 under 50\% pruning. The code is available at this https URL.</li>
</ul>

<h3>Title: Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space</h3>
<ul>
<li><strong>Authors: </strong>Yingping Liang, Yutao Hu, Wenqi Shao, Ying Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00392">https://arxiv.org/abs/2507.00392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00392">https://arxiv.org/pdf/2507.00392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00392]] Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space(https://arxiv.org/abs/2507.00392)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Feature matching plays a fundamental role in many computer vision tasks, yet existing methods heavily rely on scarce and clean multi-view image collections, which constrains their generalization to diverse and challenging scenarios. Moreover, conventional feature encoders are typically trained on single-view 2D images, limiting their capacity to capture 3D-aware correspondences. In this paper, we propose a novel two-stage framework that lifts 2D images to 3D space, named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and diverse single-view images. To be specific, in the first stage, we learn a 3D-aware feature encoder using a combination of multi-view image synthesis and 3D feature Gaussian representation, which injects 3D geometry knowledge into the encoder. In the second stage, a novel-view rendering strategy, combined with large-scale synthetic data generation from single-view images, is employed to learn a feature decoder for robust feature matching, thus achieving generalization across diverse domains. Extensive experiments demonstrate that our method achieves superior generalization across zero-shot evaluation benchmarks, highlighting the effectiveness of the proposed framework for robust feature matching.</li>
</ul>

<h3>Title: HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism</h3>
<ul>
<li><strong>Authors: </strong>Geng Zhang, Shenggan Cheng, Xuanlei Zhao, Ziming Liu, Yang You</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00394">https://arxiv.org/abs/2507.00394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00394">https://arxiv.org/pdf/2507.00394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00394]] HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism(https://arxiv.org/abs/2507.00394)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As transformer sequence lengths grow, existing pipeline parallelisms incur suboptimal performance due to the quadratic attention computation and the substantial memory overhead. To relieve these challenges, we propose HelixPipe, a novel pipeline parallelism for long sequence transformer training. First, HelixPipe introduces attention parallel partition, which schedules attention computations of different micro batches across different pipeline stages in parallel, reducing pipeline bubbles. Second, it employs a two-fold first-in-last-out micro batch schedule to balance memory usage and overlap communication with computation. Additionally, HelixPipe utilizes recomputation without attention and chunked MLP to mitigate fragmentation and enable longer sequences. Experiments demonstrate that HelixPipe gains increasing advantages with longer sequence lengths, and outperforms existing methods in throughput and scalability across varying pipeline sizes, model sizes, and cluster configurations. Notably, it achieves a 26\% speedup over baseline methods when training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available at this https URL.</li>
</ul>

<h3>Title: Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains</h3>
<ul>
<li><strong>Authors: </strong>Xin Xu, Eibe Frank, Geoffrey Holmes</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00401">https://arxiv.org/abs/2507.00401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00401">https://arxiv.org/pdf/2507.00401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00401]] Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains(https://arxiv.org/abs/2507.00401)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We investigate cross-domain few-shot learning under the constraint that fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible -- a scenario that is increasingly common in practical use cases. Handling the low-quality and static embeddings produced by frozen, "black-box" backbones leads to a problem representation of few-shot classification as a series of multiple instance verification (MIV) tasks. Inspired by this representation, we introduce a novel approach to few-shot domain adaptation, named the "MIV-head", akin to a classification head that is agnostic to any pretrained backbone and computationally efficient. The core components designed for the MIV-head, when trained on few-shot data from a target domain, collectively yield strong performance on test data from that domain. Importantly, it does so without fine-tuning the backbone, and within the "meta-testing" phase. Experimenting under various settings and on an extension of the Meta-dataset benchmark for cross-domain few-shot image classification, using representative off-the-shelf convolutional neural network and vision transformer backbones pretrained on ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when compared to state-of-the-art "adapter" (or partially fine-tuning) methods applied to the same backbones, while incurring substantially lower adaptation cost. We also find well-known "classification head" approaches lag far behind in terms of accuracy. Ablation study empirically justifies the core components of our approach. We share our code at this https URL.</li>
</ul>

<h3>Title: Diffusion Disambiguation Models for Partial Label Learning</h3>
<ul>
<li><strong>Authors: </strong>Jinfu Fan, Xiaohui Zhong, Kangrui Ren, Jiangnan Li, Linqing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00411">https://arxiv.org/abs/2507.00411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00411">https://arxiv.org/pdf/2507.00411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00411]] Diffusion Disambiguation Models for Partial Label Learning(https://arxiv.org/abs/2507.00411)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning from ambiguous labels is a long-standing problem in practical machine learning applications. The purpose of \emph{partial label learning} (PLL) is to identify the ground-truth label from a set of candidate labels associated with a given instance. Inspired by the remarkable performance of diffusion models in various generation tasks, this paper explores their potential to denoise ambiguous labels through the reverse denoising process. Therefore, this paper reformulates the label disambiguation problem from the perspective of generative models, where labels are generated by iteratively refining initial random guesses. This perspective enables the diffusion model to learn how label information is generated stochastically. By modeling the generation uncertainty, we can use the maximum likelihood estimate of the label for classification inference. However, such ambiguous labels lead to a mismatch between instance and label, which reduces the quality of generated data. To address this issue, this paper proposes a \emph{diffusion disambiguation model for PLL} (DDMP), which first uses the potential complementary information between instances and labels to construct pseudo-clean labels for initial diffusion training. Furthermore, a transition-aware matrix is introduced to estimate the potential ground-truth labels, which are dynamically updated during the diffusion generation. During training, the ground-truth label is progressively refined, improving the classifier. Experiments show the advantage of the DDMP and its suitability for PLL.</li>
</ul>

<h3>Title: Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Wenjin Mo, Zhiyuan Li, Minghong Fang, Mingwei Fang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00423">https://arxiv.org/abs/2507.00423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00423">https://arxiv.org/pdf/2507.00423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00423]] Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning(https://arxiv.org/abs/2507.00423)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, membership infer, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) allows multiple clients to collaboratively train a global machine learning model with coordination from a central server, without needing to share their raw data. This approach is particularly appealing in the era of privacy regulations like the GDPR, leading many prominent companies to adopt it. However, FL's distributed nature makes it susceptible to poisoning attacks, where malicious clients, controlled by an attacker, send harmful data to compromise the model. Most existing poisoning attacks in FL aim to degrade the model's integrity, such as reducing its accuracy, with limited attention to privacy concerns from these attacks. In this study, we introduce FedPoisonMIA, a novel poisoning membership inference attack targeting FL. FedPoisonMIA involves malicious clients crafting local model updates to infer membership information. Additionally, we propose a robust defense mechanism to mitigate the impact of FedPoisonMIA attacks. Extensive experiments across various datasets demonstrate the attack's effectiveness, while our defense approach reduces its impact to a degree.</li>
</ul>

<h3>Title: Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows</h3>
<ul>
<li><strong>Authors: </strong>Ruixiang Zhang, Shuangfei Zhai, Jiatao Gu, Yizhe Zhang, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Josh Susskind, Navdeep Jaitly</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00425">https://arxiv.org/abs/2507.00425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00425">https://arxiv.org/pdf/2507.00425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00425]] Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows(https://arxiv.org/abs/2507.00425)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Autoregressive models have driven remarkable progress in language modeling. Their foundational reliance on discrete tokens, unidirectional context, and single-pass decoding, while central to their success, also inspires the exploration of a design space that could offer new axes of modeling flexibility. In this work, we explore an alternative paradigm, shifting language modeling from a discrete token space to a continuous latent space. We propose a novel framework TarFlowLM, that employs transformer-based autoregressive normalizing flows to model these continuous representations. This approach unlocks substantial flexibility, enabling the construction of models that can capture global bi-directional context through stacked, alternating-direction autoregressive transformations, support block-wise generation with flexible token patch sizes, and facilitate a hierarchical multi-pass generation process. We further propose new mixture-based coupling transformations designed to capture complex dependencies within the latent space shaped by discrete data, and demonstrate theoretical connections to conventional discrete autoregressive models. Extensive experiments on language modeling benchmarks demonstrate strong likelihood performance and highlight the flexible modeling capabilities inherent in our framework.</li>
</ul>

<h3>Title: DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Pan, Dan Xu, Qiong Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00429">https://arxiv.org/abs/2507.00429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00429">https://arxiv.org/pdf/2507.00429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00429]] DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting(https://arxiv.org/abs/2507.00429)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Developing a unified pipeline that enables users to remove, re-texture, or replace objects in a versatile manner is crucial for text-guided 3D inpainting. However, there are still challenges in performing multiple 3D inpainting tasks within a unified framework: 1) Single reference inpainting methods lack robustness when dealing with views that are far from the reference view. 2) Appearance inconsistency arises when independently inpainting multi-view images with 2D diffusion priors; 3) Geometry inconsistency limits performance when there are significant geometric changes in the inpainting regions. To tackle these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting pipeline that leverages diffusion models to propagate consistent appearance and geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy for selecting multiple reference views to reduce errors during propagation. Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that propagates attention features from the selected reference views to other views via diffusion models to maintain appearance consistency. Furthermore, DiGA3D introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to further improve the geometric consistency of inpainted 3D scenes. Extensive experiments on multiple 3D inpainting tasks demonstrate the effectiveness of our method. The project page is available at this https URL.</li>
</ul>

<h3>Title: Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Su, Xiner Li, Masatoshi Uehara, Sunwoo Kim, Yulai Zhao, Gabriele Scalia, Ehsan Hajiramezanali, Tommaso Biancalani, Degui Zhi, Shuiwang Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00445">https://arxiv.org/abs/2507.00445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00445">https://arxiv.org/pdf/2507.00445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00445]] Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design(https://arxiv.org/abs/2507.00445)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We address the problem of fine-tuning diffusion models for reward-guided generation in biomolecular design. While diffusion models have proven highly effective in modeling complex, high-dimensional data distributions, real-world applications often demand more than high-fidelity generation, requiring optimization with respect to potentially non-differentiable reward functions such as physics-based simulation or rewards based on scientific knowledge. Although RL methods have been explored to fine-tune diffusion models for such objectives, they often suffer from instability, low sample efficiency, and mode collapse due to their on-policy nature. In this work, we propose an iterative distillation-based fine-tuning framework that enables diffusion models to optimize for arbitrary reward functions. Our method casts the problem as policy distillation: it collects off-policy data during the roll-in phase, simulates reward-based soft-optimal policies during roll-out, and updates the model by minimizing the KL divergence between the simulated soft-optimal policy and the current model policy. Our off-policy formulation, combined with KL divergence minimization, enhances training stability and sample efficiency compared to existing RL-based methods. Empirical results demonstrate the effectiveness and superior reward optimization of our approach across diverse tasks in protein, small molecule, and regulatory DNA design.</li>
</ul>

<h3>Title: Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Zhan, Jianan Zhao, Zhaocheng Zhu, Jian Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00449">https://arxiv.org/abs/2507.00449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00449">https://arxiv.org/pdf/2507.00449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00449]] Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention(https://arxiv.org/abs/2507.00449)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively. In this work, we focus on analyzing and improving the long-context modeling capabilities of SSMs. We show that the widely used synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities of real-world long-context modeling. To address this limitation, we extend the associative recall to a novel synthetic task, \emph{joint recall}, which requires a model to recall the value associated with a key given in a specified context. Theoretically, we prove that SSMs do not have the expressiveness to solve multi-query joint recall in sub-quadratic time complexity. To resolve this issue, we propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has the expressiveness to solve multi-query joint recall with sub-quadratic computation. To bridge the gap between theoretical analysis and real-world applications, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is further tailored to natural language domains. Extensive experiments on both synthetic and real-world long-context benchmarks show that HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA).</li>
</ul>

<h3>Title: Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ankit Kashyap</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00453">https://arxiv.org/abs/2507.00453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00453">https://arxiv.org/pdf/2507.00453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00453]] Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling(https://arxiv.org/abs/2507.00453)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a Transformer architecture for long-context language modeling that combines global attention with two biologically inspired components: chunked local attention and a gated FIFO memory mechanism. This unified attention block allows the model to efficiently handle both short-range and long-range dependencies without increasing attention cost quadratically. The memory module persistently stores past token representations using a gated update mechanism inspired by recurrent networks. Rotary positional encoding is applied per attention head to enable directionally disentangled, scale-invariant positional signals. The architecture is implemented entirely from scratch in PyTorch, with no reliance on high-level libraries, enabling transparent and modular experimentation. Our model offers a lightweight and extensible design for tasks such as dialogue modeling, code completion, and document understanding.</li>
</ul>

<h3>Title: Pitfalls of Evaluating Language Models with Open Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Md. Najib Hasan (1), Mohammad Fakhruddin Babar (2), Souvika Sarkar (1), Monowar Hasan (2), Santu Karmaker (3) ((1) Wichita State University, (2) Washington State University, (3) University of Central Florida)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00460">https://arxiv.org/abs/2507.00460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00460">https://arxiv.org/pdf/2507.00460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00460]] Pitfalls of Evaluating Language Models with Open Benchmarks(https://arxiv.org/abs/2507.00460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer standardized, transparent protocols that facilitate the fair comparison, reproducibility, and iterative advancement of Language Models (LMs). However, their openness also introduces critical and underexplored pitfalls. This study exposes these weaknesses by systematically constructing ``cheating'' models -- smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets -- which achieve top rankings on a prominent open, holistic benchmark (HELM) despite poor generalization and limited practical utility. Our findings underscore three key insights: \ca high leaderboard performance on open benchmarks may not always reflect real-world effectiveness; \cb private or dynamic benchmarks must complement open evaluations to safeguard integrity; and \cc a fundamental reevaluation of current benchmarking practices is essential to ensure robust and trustworthy LM assessments.</li>
</ul>

<h3>Title: Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jizhou Han, Chenhao Ding, SongLin Dong, Yuhang He, Xinyuan Gao, Yihong Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00462">https://arxiv.org/abs/2507.00462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00462">https://arxiv.org/pdf/2507.00462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00462]] Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation(https://arxiv.org/abs/2507.00462)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual-language models (VLMs) like CLIP exhibit strong generalization but struggle with distribution shifts at test time. Existing training-free test-time adaptation (TTA) methods operate strictly within CLIP's original feature space, relying on high-confidence samples while overlooking the potential of low-confidence ones. We propose MS-TTA, a training-free approach that enhances feature representations beyond CLIP's space using a single-step k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA improves feature compactness and class separability, leading to more stable adaptation. Additionally, a cache of refined embeddings further enhances inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms state-of-the-art training-free TTA methods, achieving robust adaptation without requiring additional training.</li>
</ul>

<h3>Title: Diversity Conscious Refined Random Forest</h3>
<ul>
<li><strong>Authors: </strong>Sijan Bhattarai, Saurav Bhandari, Girija Bhusal, Saroj Shakya, Tapendra Pandey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00467">https://arxiv.org/abs/2507.00467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00467">https://arxiv.org/pdf/2507.00467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00467]] Diversity Conscious Refined Random Forest(https://arxiv.org/abs/2507.00467)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Random Forest (RF) is a widely used ensemble learning technique known for its robust classification performance across diverse domains. However, it often relies on hundreds of trees and all input features, leading to high inference cost and model redundancy. In this work, our goal is to grow trees dynamically only on informative features and then enforce maximal diversity by clustering and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest Classifier that iteratively refines itself by first removing the least informative features and then analytically determines how many new trees should be grown, followed by correlation-based clustering to remove redundant trees. The classification accuracy of our model was compared against the standard RF on the same number of trees. Experiments on 8 multiple benchmark datasets, including binary and multiclass datasets, demonstrate that the proposed model achieves improved accuracy compared to standard RF.</li>
</ul>

<h3>Title: Bisecle: Binding and Separation in Continual Learning for Video Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yue Tan, Xiaoqian Hu, Hao Xue, Celso De Melo, Flora D. Salim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00469">https://arxiv.org/abs/2507.00469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00469">https://arxiv.org/pdf/2507.00469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00469]] Bisecle: Binding and Separation in Continual Learning for Video Language Understanding(https://arxiv.org/abs/2507.00469)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Frontier vision-language models (VLMs) have made remarkable improvements in video understanding tasks. However, real-world videos typically exist as continuously evolving data streams (e.g., dynamic scenes captured by wearable glasses), necessitating models to continually adapt to shifting data distributions and novel scenarios. Considering the prohibitive computational costs of fine-tuning models on new tasks, usually, a small subset of parameters is updated while the bulk of the model remains frozen. This poses new challenges to existing continual learning frameworks in the context of large multimodal foundation models, i.e., catastrophic forgetting and update conflict. While the foundation models struggle with parameter-efficient continual learning, the hippocampus in the human brain has evolved highly efficient mechanisms for memory formation and consolidation. Inspired by the rapid Binding and pattern separation mechanisms in the hippocampus, in this work, we propose Bisecle for video-language continual learning, where a multi-directional supervision module is used to capture more cross-modal relationships and a contrastive prompt learning scheme is designed to isolate task-specific knowledge to facilitate efficient memory storage. Binding and separation processes further strengthen the ability of VLMs to retain complex experiences, enabling robust and efficient continual learning in video understanding tasks. We perform a thorough evaluation of the proposed Bisecle, demonstrating its ability to mitigate forgetting and enhance cross-task generalization on several VideoQA benchmarks.</li>
</ul>

<h3>Title: ARIG: Autoregressive Interactive Head Generation for Real-time Conversations</h3>
<ul>
<li><strong>Authors: </strong>Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, Xiaoming Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00472">https://arxiv.org/abs/2507.00472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00472">https://arxiv.org/pdf/2507.00472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00472]] ARIG: Autoregressive Interactive Head Generation for Real-time Conversations(https://arxiv.org/abs/2507.00472)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.</li>
</ul>

<h3>Title: ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Yaofei Duan, Yuhao Huang, Xin Yang, Luyi Han, Xinyu Xie, Zhiyuan Zhu, Ping He, Ka-Hou Chan, Ligang Cui, Sio-Kei Im, Dong Ni, Tao Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00474">https://arxiv.org/abs/2507.00474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00474">https://arxiv.org/pdf/2507.00474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00474]] ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis(https://arxiv.org/abs/2507.00474)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep learning-based diagnostic models often suffer performance drops due to distribution shifts between training (source) and test (target) domains. Collecting and labeling sufficient target domain data for model retraining represents an optimal solution, yet is limited by time and scarce resources. Active learning (AL) offers an efficient approach to reduce annotation costs while maintaining performance, but struggles to handle the challenge posed by distribution variations across different datasets. In this study, we propose a novel unsupervised Active learning framework for Domain Adaptation, named ADAptation, which efficiently selects informative samples from multi-domain data pools under limited annotation budget. As a fundamental step, our method first utilizes the distribution homogenization capabilities of diffusion models to bridge cross-dataset gaps by translating target images into source-domain style. We then introduce two key innovations: (a) a hypersphere-constrained contrastive learning network for compact feature clustering, and (b) a dual-scoring mechanism that quantifies and balances sample uncertainty and representativeness. Extensive experiments on four breast ultrasound datasets (three public and one in-house/multi-center) across five common deep classifiers demonstrate that our method surpasses existing strong AL-based competitors, validating its effectiveness and generalization for clinical domain adaptation. The code is available at the anonymized link: this https URL.</li>
</ul>

<h3>Title: Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kiyoung Om, Kyuil Sim, Taeyoung Yun, Hyeongyu Kang, Jinkyoo Park</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00480">https://arxiv.org/abs/2507.00480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00480">https://arxiv.org/pdf/2507.00480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00480]] Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization(https://arxiv.org/abs/2507.00480)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optimizing high-dimensional black-box functions under black-box constraints is a pervasive task in a wide range of scientific and engineering problems. These problems are typically harder than unconstrained problems due to hard-to-find feasible regions. While Bayesian optimization (BO) methods have been developed to solve such problems, they often struggle with the curse of dimensionality. Recently, generative model-based approaches have emerged as a promising alternative for constrained optimization. However, they suffer from poor scalability and are vulnerable to mode collapse, particularly when the target distribution is highly multi-modal. In this paper, we propose a new framework to overcome these challenges. Our method iterates through two stages. First, we train flow-based models to capture the data distribution and surrogate models that predict both function values and constraint violations with uncertainty quantification. Second, we cast the candidate selection problem as a posterior inference problem to effectively search for promising candidates that have high objective values while not violating the constraints. During posterior inference, we find that the posterior distribution is highly multi-modal and has a large plateau due to constraints, especially when constraint feedback is given as binary indicators of feasibility. To mitigate this issue, we amortize the sampling from the posterior distribution in the latent space of flow-based models, which is much smoother than that in the data space. We empirically demonstrate that our method achieves superior performance on various synthetic and real-world constrained black-box optimization tasks. Our code is publicly available \href{this https URL}{here}.</li>
</ul>

<h3>Title: PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00485">https://arxiv.org/abs/2507.00485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00485">https://arxiv.org/pdf/2507.00485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00485]] PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning(https://arxiv.org/abs/2507.00485)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) is widely used in tasks where agents interact with an environment to maximize rewards. Building on this foundation, Safe Reinforcement Learning (Safe RL) incorporates a cost metric alongside the reward metric, ensuring that agents adhere to safety constraints during decision-making. In this paper, we identify that Safe RL is vulnerable to backdoor attacks, which can manipulate agents into performing unsafe actions. First, we introduce the relevant concepts and evaluation metrics for backdoor attacks in Safe RL. It is the first attack framework in the Safe RL field that involves both Positive and Negative Action sample (PNAct) is to implant backdoors, where positive action samples provide reference actions and negative action samples indicate actions to be avoided. We theoretically point out the properties of PNAct and design an attack algorithm. Finally, we conduct experiments to evaluate the effectiveness of our proposed backdoor attack framework, evaluating it with the established metrics. This paper highlights the potential risks associated with Safe RL and underscores the feasibility of such attacks. Our code and supplementary material are available at this https URL.</li>
</ul>

<h3>Title: Just Noticeable Difference for Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Zijian Chen, Yuan Tian, Yuze Sun, Wei Sun, Zicheng Zhang, Weisi Lin, Guangtao Zhai, Wenjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00490">https://arxiv.org/abs/2507.00490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00490">https://arxiv.org/pdf/2507.00490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00490]] Just Noticeable Difference for Large Multimodal Models(https://arxiv.org/abs/2507.00490)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Just noticeable difference (JND), the minimum change that the human visual system (HVS) can perceive, has been studied for decades. Although recent work has extended this line of research into machine vision, there has been a scarcity of studies systematically exploring its perceptual boundaries across multiple tasks and stimulus types, particularly in the current era of rapidly advancing large multimodal models (LMMs), where studying the multifaceted capabilities of models has become a mainstream focus. Moreover, the perceptual defects of LMMs are not investigated thoroughly, resulting in potential security issues and suboptimal response efficiency. In this paper, we take an initial attempt and demonstrate that there exist significant visual blind spots in current LMMs. To systemically quantify this characteristic, we propose a new concept, {\bf LMM-JND}, together with its determination pipeline. Targeting uncovering the behavior commonalities in HVS-aligned visual perception tasks, we delve into several LMM families and construct a large-scale dataset, named VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12 distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle with basic comparison queries and fall significantly short of human-level visual performance. We further explore the effects of vision and language backbones and find a notable correlation between their design philosophy that may instruct the future refinement of LMMs for their visual acuity. Together, our research underscores the significance of LMM-JND as a unique perspective for studying LMMs, and predictable LMM-JND is crucial for security concerns. This work will be available at this https URL.</li>
</ul>

<h3>Title: Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Fenil R. Doshi, Thomas Fel, Talia Konkle, George Alvarez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00493">https://arxiv.org/abs/2507.00493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00493">https://arxiv.org/pdf/2507.00493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00493]] Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models(https://arxiv.org/abs/2507.00493)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Humans are able to recognize objects based on both local texture cues and the configuration of object parts, yet contemporary vision models primarily harvest local texture cues, yielding brittle, non-compositional features. Work on shape-vs-texture bias has pitted shape and texture representations in opposition, measuring shape relative to texture, ignoring the possibility that models (and humans) can simultaneously rely on both types of cues, and obscuring the absolute quality of both types of representation. We therefore recast shape evaluation as a matter of absolute configural competence, operationalized by the Configural Shape Score (CSS), which (i) measures the ability to recognize both images in Object-Anagram pairs that preserve local texture while permuting global part arrangement to depict different object categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii) uncovers a broad spectrum of configural sensitivity with fully self-supervised and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes reveal that (iii) high-CSS networks depend on long-range interactions: radius-controlled attention masks abolish performance showing a distinctive U-shaped integration profile, and representational-similarity analyses expose a mid-depth transition from local to global coding. A BagNet control remains at chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that configural shape score also predicts other shape-dependent evals. Overall, we propose that the path toward truly robust, generalizable, and human-like vision systems may not lie in forcing an artificial choice between shape and texture, but rather in architectural and learning frameworks that seamlessly integrate both local-texture and global configural shape.</li>
</ul>

<h3>Title: ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation</h3>
<ul>
<li><strong>Authors: </strong>JianChao Zhao, Songlin Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00502">https://arxiv.org/abs/2507.00502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00502">https://arxiv.org/pdf/2507.00502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00502]] ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation(https://arxiv.org/abs/2507.00502)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Continual Test-Time Adaptation (CTTA) aims to enable models to adapt on-the-fly to a stream of unlabeled data under evolving distribution shifts. However, existing CTTA methods typically rely on shared model parameters across all domains, making them vulnerable to feature entanglement and catastrophic forgetting in the presence of large or non-stationary domain shifts. To address this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an \emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples domain-general and domain-specific knowledge via a dual-branch expert design with token-guided feature separation, and dynamically expands its expert pool based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that detects distribution changes in real-time using frequency-domain cues. Extensive experiments demonstrate the superiority of ExPaMoE across diverse CTTA scenarios. We evaluate our method on standard benchmarks including CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and realistic CTTA benchmark built from multiple ImageNet-derived datasets, to better reflect long-term adaptation under complex domain evolution. ExPaMoE consistently outperforms prior arts, showing strong robustness, scalability, and resistance to forgetting.</li>
</ul>

<h3>Title: LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Haoran Lou, Chunxiao Fan, Ziyan Liu, Yuexin Wu, Xinxiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00505">https://arxiv.org/abs/2507.00505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00505">https://arxiv.org/pdf/2507.00505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00505]] LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs(https://arxiv.org/abs/2507.00505)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The architecture of multimodal large language models (MLLMs) commonly connects a vision encoder, often based on CLIP-ViT, to a large language model. While CLIP-ViT works well for capturing global image features, it struggles to model local relationships between adjacent patches, leading to weaker visual representation, which in turn affects the detailed understanding ability of MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial visual tokens} to the original visual tokens to enhance the visual representation. Our approach offers three key advantages: 1)We propose a novel Projector, which uses convolutional kernels to derive visual spatial tokens from ViT patch features, simulating two visual spatial ordering approaches: ``from central region to global" and ``from abstract to specific". Then, a cross-attention mechanism is applied to fuse fine-grained visual information, enriching the overall visual representation. 2) We present two model variants: LLaVA-SP-Cropping, which focuses on detail features through progressive cropping, and LLaVA-SP-Pooling, which captures global semantics through adaptive pooling, enabling the model to handle diverse visual understanding tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA, achieves significant performance improvements across various multimodal benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple tasks with nearly identical inference latency. The code and models are available at \href{this https URL}{\texttt{this https URL}}.</li>
</ul>

<h3>Title: SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yunfei Xie, Yuxuan Cheng, Juncheng Wu, Haoyu Zhang, Yuyin Zhou, Shoudong Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00506">https://arxiv.org/abs/2507.00506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00506">https://arxiv.org/pdf/2507.00506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00506]] SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning(https://arxiv.org/abs/2507.00506)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in adapting vision-language pre-training models like CLIP for person re-identification (ReID) tasks often rely on complex adapter design or modality-specific tuning while neglecting cross-modal interaction, leading to high computational costs or suboptimal alignment. To address these limitations, we propose a simple yet effective framework named Selective Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and robustness against real-world perturbations. Our method introduces two key innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a lightweight module that dynamically injects discriminative visual features into text prompts via a cross-modal gating mechanism. Moreover, the proposed Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training strategy that enforces invariant feature alignment under random image perturbations by regularizing consistency between original and augmented cross-modal embeddings. Extensive experiments are conducted on several popular benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID, and P-DukeMTMC, which demonstrate the impressive performance of the proposed method. Notably, our framework eliminates heavy adapters while maintaining efficient inference, achieving an optimal trade-off between performance and computational overhead. The code will be released upon acceptance.</li>
</ul>

<h3>Title: TeamCMU at Touch√©: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search</h3>
<ul>
<li><strong>Authors: </strong>To Eun Kim, Jo√£o Coelho, Gbemileke Onilude, Jai Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00509">https://arxiv.org/abs/2507.00509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00509">https://arxiv.org/pdf/2507.00509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00509]] TeamCMU at Touch√©: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search(https://arxiv.org/abs/2507.00509)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, steal, generative, large language model</a></li>
<li><strong>Abstract: </strong>As conversational search engines increasingly adopt generation-based paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), the integration of advertisements into generated responses presents both commercial opportunities and challenges for user experience. Unlike traditional search, where advertisements are clearly delineated, generative systems blur the boundary between informational content and promotional material, raising concerns around transparency and trust. In this work, we propose a modular pipeline for advertisement management in RAG-based conversational systems, consisting of an ad-rewriter for seamless ad integration and a robust ad-classifier for detection. We leverage synthetic data to train high-performing classifiers, which are then used to guide two complementary ad-integration strategies: supervised fine-tuning of the ad-rewriter and a best-of-N sampling approach that selects the least detectable ad-integrated response among multiple candidates. Our evaluation focuses on two core questions: the effectiveness of ad classifiers in detecting diverse ad integration strategies, and the training methods that best support coherent, minimally intrusive ad insertion. Experimental results show that our ad-classifier, trained on synthetic advertisement data inspired by marketing strategies and enhanced through curriculum learning, achieves robust detection performance. Additionally, we demonstrate that classifier-guided optimization, through both fine-tuning and best-of-N sampling, significantly improves ad stealth, enabling more seamless integration. These findings contribute an adversarial co-evolution framework for developing more sophisticated ad-aware generative search systems and robust ad classifiers.</li>
</ul>

<h3>Title: Cyber Attacks Detection, Prevention, and Source Localization in Digital Substation Communication using Hybrid Statistical-Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Nicola Cibin, Bas Mulder, Herman Carstens, Peter Palensky, Alexandru ≈ûtefanov</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00522">https://arxiv.org/abs/2507.00522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00522">https://arxiv.org/pdf/2507.00522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00522]] Cyber Attacks Detection, Prevention, and Source Localization in Digital Substation Communication using Hybrid Statistical-Deep Learning(https://arxiv.org/abs/2507.00522)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>The digital transformation of power systems is accelerating the adoption of IEC 61850 standard. However, its communication protocols, including Sampled Values (SV), lack built-in security features such as authentication and encryption, making them vulnerable to malicious packet injection. Such cyber attacks can delay fault clearance or trigger unintended circuit breaker operations. While most existing research focuses on detecting cyber attacks in digital substations, intrusion prevention systems have been disregarded because of the risk of potential communication network disruptions. This paper proposes a novel method using hybrid statistical-deep learning for the detection, prevention, and source localization of IEC 61850 SV injection attacks. The method uses exponentially modified Gaussian distributions to model communication network latency and long short-term memory and Elman recurrent neural network to detect anomalous variations in the estimated probability distributions. It effectively discards malicious SV frames with minimal processing overhead and latency, maintains robustness against communication network latency variation and time-synchronization issues, and guarantees a near-zero false positive rate in non-attack scenarios. Comprehensive validation is conducted on three testbeds involving industrial-grade devices, hardware-in-the-loop simulations, virtualized intelligent electronic devices and merging units, and high-fidelity emulated communication networks. Results demonstrate the method's suitability for practical deployment in IEC 61850-compliant digital substations.</li>
</ul>

<h3>Title: Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Djamahl Etchegaray, Yuxia Fu, Zi Huang, Yadan Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00525">https://arxiv.org/abs/2507.00525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00525">https://arxiv.org/pdf/2507.00525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00525]] Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving(https://arxiv.org/abs/2507.00525)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Interpretable communication is essential for safe and trustworthy autonomous driving, yet current vision-language models (VLMs) often operate under idealized assumptions and struggle to capture user intent in real-world scenarios. Existing driving-oriented VQA datasets are limited to full-scene descriptions or waypoint prediction, preventing the assessment of whether VLMs can respond to localized user-driven queries. We introduce Box-QAymo, a box-referring dataset and benchmark designed to both evaluate and finetune VLMs on spatial and temporal reasoning over user-specified objects. Users express intent by drawing bounding boxes, offering a fast and intuitive interface for focused queries in complex scenes. Specifically, we propose a hierarchical evaluation protocol that begins with binary sanity-check questions to assess basic model capacities, and progresses to (1) attribute prediction for box-referred objects, (2) motion understanding of target instances, and (3) spatiotemporal motion reasoning over inter-object dynamics across frames. To support this, we crowd-sourced fine-grained object classes and visual attributes that reflect the complexity drivers encounter, and extract object trajectories to construct temporally grounded QA pairs. Rigorous quality control through negative sampling, temporal consistency checks, and difficulty-aware balancing guarantee dataset robustness and diversity. Our comprehensive evaluation reveals significant limitations in current VLMs when queried about perception questions, highlighting the gap in achieving real-world performance. This work provides a foundation for developing more robust and interpretable autonomous driving systems that can communicate effectively with users under real-world conditions. Project page and dataset are available at this https URL.</li>
</ul>

<h3>Title: NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data</h3>
<ul>
<li><strong>Authors: </strong>Tahir Javed, Kaushal Bhogale, Mitesh M. Khapra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00534">https://arxiv.org/abs/2507.00534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00534">https://arxiv.org/pdf/2507.00534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00534]] NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data(https://arxiv.org/abs/2507.00534)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce Nirantar, a comprehensive framework for evaluating continual learning (CL) in multilingual and multi-domain ASR. Designed to reflect real-world CL challenges, Nirantar leverages data collected incrementally across 22 languages and 208 districts in India through natural episodes. This enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL), and the novel Language-Incremental Domain-Incremental Learning (LIDIL) scenarios. Unlike prior work that relies on simulated episodes, Nirantar presents dynamic, non-uniform language and domain shifts, making it an ideal testbed for CL research. With 3250 hours of human-transcribed speech, including 1720 hours newly introduced in this work, our framework enables systematic benchmarking of CL methods. We evaluate existing approaches and demonstrate that no single method performs consistently well, underscoring the need for more robust CL strategies.</li>
</ul>

<h3>Title: Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation</h3>
<ul>
<li><strong>Authors: </strong>Feng Lin, Marco Chen, Haokui Zhang, Xiaotian Yu, Guangming Lu, Rong Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00537">https://arxiv.org/abs/2507.00537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00537">https://arxiv.org/pdf/2507.00537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00537]] Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation(https://arxiv.org/abs/2507.00537)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper studies the role of attention heads in CLIP's image encoder. While CLIP has exhibited robust performance across diverse applications, we hypothesize that certain attention heads negatively affect final representations and that ablating them can improve performance in downstream tasks. To capitalize on this insight, we propose a simple yet effective method, called Attention Ablation Technique (AAT), to suppress the contribution of specific heads by manipulating attention weights. By integrating two alternative strategies tailored for different application scenarios, AAT systematically identifies and ablates detrimental attention heads to enhance representation quality. Experiments demonstrate that AAT consistently improves downstream task performance across various domains, boosting recall rate by up to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight the potential of AAT to effectively refine large-scale vision-language models with virtually no increase in inference cost.</li>
</ul>

<h3>Title: Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction</h3>
<ul>
<li><strong>Authors: </strong>Shixiao Wang, Yifan Zhuang, Runsheng Zhang, Zhijun Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00540">https://arxiv.org/abs/2507.00540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00540">https://arxiv.org/pdf/2507.00540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00540]] Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction(https://arxiv.org/abs/2507.00540)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper proposes a user semantic intent modeling algorithm based on Capsule Networks to address the problem of insufficient accuracy in intent recognition for human-computer interaction. The method represents semantic features in input text through a vectorized capsule structure. It uses a dynamic routing mechanism to transfer information across multiple capsule layers. This helps capture hierarchical relationships and part-whole structures between semantic entities more effectively. The model uses a convolutional feature extraction module as the low-level encoder. After generating initial semantic capsules, it forms high-level abstract intent representations through an iterative routing process. To further enhance performance, a margin-based mechanism is introduced into the loss function. This improves the model's ability to distinguish between intent classes. Experiments are conducted using a public natural language understanding dataset. Multiple mainstream models are used for comparison. Results show that the proposed model outperforms traditional methods and other deep learning structures in terms of accuracy, F1-score, and intent detection rate. The study also analyzes the effect of the number of dynamic routing iterations on model performance. A convergence curve of the loss function during training is provided. These results verify the stability and effectiveness of the proposed method in semantic modeling. Overall, this study presents a new structured modeling approach to improve intent recognition under complex semantic conditions.</li>
</ul>

<h3>Title: Out-of-distribution detection in 3D applications: a review</h3>
<ul>
<li><strong>Authors: </strong>Zizhao Li, Xueyang Kang, Joseph West, Kourosh Khoshelham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00570">https://arxiv.org/abs/2507.00570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00570">https://arxiv.org/pdf/2507.00570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00570]] Out-of-distribution detection in 3D applications: a review(https://arxiv.org/abs/2507.00570)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The ability to detect objects that are not prevalent in the training set is a critical capability in many 3D applications, including autonomous driving. Machine learning methods for object recognition often assume that all object categories encountered during inference belong to a closed set of classes present in the training data. This assumption limits generalization to the real world, as objects not seen during training may be misclassified or entirely ignored. As part of reliable AI, OOD detection identifies inputs that deviate significantly from the training distribution. This paper provides a comprehensive overview of OOD detection within the broader scope of trustworthy and uncertain AI. We begin with key use cases across diverse domains, introduce benchmark datasets spanning multiple modalities, and discuss evaluation metrics. Next, we present a comparative analysis of OOD detection methodologies, exploring model structures, uncertainty indicators, and distributional distance taxonomies, alongside uncertainty calibration techniques. Finally, we highlight promising research directions, including adversarially robust OOD detection and failure identification, particularly relevant to 3D applications. The paper offers both theoretical and practical insights into OOD detection, showcasing emerging research opportunities such as 3D vision integration. These insights help new researchers navigate the field more effectively, contributing to the development of reliable, safe, and robust AI systems.</li>
</ul>

<h3>Title: Foundation Models for Clinical Records at Health System Scale</h3>
<ul>
<li><strong>Authors: </strong>Haresh Rengaraj Rajamohan, Xiang Gao, Weicheng Zhu, Shih-Lun Huang, Long Chen, Kyunghyun Cho, Cem M. Deniz, Narges Razavian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00574">https://arxiv.org/abs/2507.00574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00574">https://arxiv.org/pdf/2507.00574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00574]] Foundation Models for Clinical Records at Health System Scale(https://arxiv.org/abs/2507.00574)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Large-scale pretraining has transformed modeling of language and other data types, but its potential remains underexplored in healthcare with structured electronic health records (EHRs). We present a novel generative pretraining strategy for sequential EHR data using next-visit event prediction. Our model learns to autoregressively generate various tokenized clinical events for the next visit based on patient history and inherently handles the joint prediction of heterogeneous data types. Additionally, we introduce regularization on predicting repeated events and highlight a key pitfall in EHR-based foundation model evaluations: repeated event tokens can inflate performance metrics when new onsets are not distinguished from subsequent occurrences. Our model is evaluated via zero-shot prediction for forecasting dementia and knee osteoarthritis incidence within 2 and 5 years, and the model performance rivals a fully fine-tuned masked pretrained Transformer baseline, demonstrating that our approach captures complex clinical dependencies without requiring costly task-specific fine-tuning.</li>
</ul>

<h3>Title: BadViM: Backdoor Attack against Vision Mamba</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Wu, Liyan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00577">https://arxiv.org/abs/2507.00577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00577">https://arxiv.org/pdf/2507.00577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00577]] BadViM: Backdoor Attack against Vision Mamba(https://arxiv.org/abs/2507.00577)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, transformer</a></li>
<li><strong>Abstract: </strong>Vision State Space Models (SSMs), particularly architectures like Vision Mamba (ViM), have emerged as promising alternatives to Vision Transformers (ViTs). However, the security implications of this novel architecture, especially their vulnerability to backdoor attacks, remain critically underexplored. Backdoor attacks aim to embed hidden triggers into victim models, causing the model to misclassify inputs containing these triggers while maintaining normal behavior on clean inputs. This paper investigates the susceptibility of ViM to backdoor attacks by introducing BadViM, a novel backdoor attack framework specifically designed for Vision Mamba. The proposed BadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency sensitivity patterns of the victim model to create stealthy, distributed triggers. To maximize attack efficacy, we propose a Hidden State Alignment loss that strategically manipulates the internal representations of model by aligning the hidden states of backdoor images with those of target classes. Extensive experimental results demonstrate that BadViM achieves superior attack success rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits remarkable resilience against common defensive measures, including PatchDrop, PatchShuffle and JPEG compression, which typically neutralize normal backdoor attacks.</li>
</ul>

<h3>Title: AI-Generated Video Detection via Perceptual Straightening</h3>
<ul>
<li><strong>Authors: </strong>Christian Intern√≤, Robert Geirhos, Markus Olhofer, Sunny Liu, Barbara Hammer, David Klindt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00583">https://arxiv.org/abs/2507.00583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00583">https://arxiv.org/pdf/2507.00583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00583]] AI-Generated Video Detection via Perceptual Straightening(https://arxiv.org/abs/2507.00583)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the "perceptual straightening" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.</li>
</ul>

<h3>Title: Similarity Memory Prior is All You Need for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tang Hao, Guo ZhiQing, Wang LieJun, Liu Chao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00585">https://arxiv.org/abs/2507.00585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00585">https://arxiv.org/pdf/2507.00585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00585]] Similarity Memory Prior is All You Need for Medical Image Segmentation(https://arxiv.org/abs/2507.00585)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, it has been found that "grandmother cells" in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on this https URL.</li>
</ul>

<h3>Title: The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)</h3>
<ul>
<li><strong>Authors: </strong>Linard Arquint, Samarth Kishor, Jason R. Koenig, Joey Dodds, Daniel Kroening, Peter M√ºller</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00595">https://arxiv.org/abs/2507.00595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00595">https://arxiv.org/pdf/2507.00595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00595]] The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)(https://arxiv.org/abs/2507.00595)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Existing program verifiers can prove advanced properties about security protocol implementations, but are difficult to scale to large codebases because of the manual effort required. We develop a novel methodology called *Diodon* that addresses this challenge by splitting the codebase into the protocol implementation (the *Core*) and the remainder (the *Application*). This split allows us to apply powerful semi-automated verification techniques to the security-critical Core, while fully-automatic static analyses scale the verification to the entire codebase by ensuring that the Application cannot invalidate the security properties proved for the Core. The static analyses achieve that by proving *I/O independence*, i.e., that the I/O operations within the Application are independent of the Core's security-relevant data (such as keys), and that the Application meets the Core's requirements. We have proved Diodon sound by first showing that we can safely allow the Application to perform I/O independent of the security protocol, and second that manual verification and static analyses soundly compose. We evaluate Diodon on two case studies: an implementation of the signed Diffie-Hellman key exchange and a large (100k+ LoC) production Go codebase implementing a key exchange protocol for which we obtained secrecy and injective agreement guarantees by verifying a Core of about 1% of the code with the auto-active program verifier Gobra in less than three person months.</li>
</ul>

<h3>Title: Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based</h3>
<ul>
<li><strong>Authors: </strong>Shuangquan Lyu, Yingnan Deng, Guiran Liu, Zhen Qi, Ruotong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00601">https://arxiv.org/abs/2507.00601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00601">https://arxiv.org/pdf/2507.00601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00601]] Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based(https://arxiv.org/abs/2507.00601)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model's original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method's applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.</li>
</ul>

<h3>Title: Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies</h3>
<ul>
<li><strong>Authors: </strong>Tao Xiong, Xavier Hu, Wenyan Fan, Shengyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00606">https://arxiv.org/abs/2507.00606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00606">https://arxiv.org/pdf/2507.00606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00606]] Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies(https://arxiv.org/abs/2507.00606)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised this http URL experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.</li>
</ul>

<h3>Title: Integrating Network and Attack Graphs for Service-Centric Impact Analysis</h3>
<ul>
<li><strong>Authors: </strong>Joni Herttuainen, Vesa Kuikka, Kimmo K. Kaski</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00637">https://arxiv.org/abs/2507.00637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00637">https://arxiv.org/pdf/2507.00637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00637]] Integrating Network and Attack Graphs for Service-Centric Impact Analysis(https://arxiv.org/abs/2507.00637)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>We present a novel methodology for modelling, visualising, and analysing cyber threats, attack paths, as well as their impact on user services in enterprise or infrastructure networks of digital devices and services they provide. Using probabilistic methods to track the propagation of an attack through attack graphs, via the service or application layers, and on physical communication networks, our model enables us to analyse cyber attacks at different levels of detail. Understanding the propagation of an attack within a service among microservices and its spread between different services or application servers could help detect and mitigate it early. We demonstrate that this network-based influence spreading modelling approach enables the evaluation of diverse attack scenarios and the development of protection and mitigation measures, taking into account the criticality of services from the user's perspective. This methodology could also aid security specialists and system administrators in making well-informed decisions regarding risk mitigation strategies.</li>
</ul>

<h3>Title: Cooperative Sheaf Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Andr√© Ribeiro, Ana Luiza Ten√≥rio, Juan Belieni, Amauri H. Souza, Diego Mesquita</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00647">https://arxiv.org/abs/2507.00647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00647">https://arxiv.org/pdf/2507.00647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00647]] Cooperative Sheaf Neural Networks(https://arxiv.org/abs/2507.00647)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Sheaf diffusion has recently emerged as a promising design pattern for graph representation learning due to its inherent ability to handle heterophilic data and avoid oversmoothing. Meanwhile, cooperative message passing has also been proposed as a way to enhance the flexibility of information diffusion by allowing nodes to independently choose whether to propagate/gather information from/to neighbors. A natural question ensues: is sheaf diffusion capable of exhibiting this cooperative behavior? Here, we provide a negative answer to this question. In particular, we show that existing sheaf diffusion methods fail to achieve cooperative behavior due to the lack of message directionality. To circumvent this limitation, we introduce the notion of cellular sheaves over directed graphs and characterize their in- and out-degree Laplacians. We leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs). Theoretically, we characterize the receptive field of CSNN and show it allows nodes to selectively attend (listen) to arbitrarily far nodes while ignoring all others in their path, potentially mitigating oversquashing. Our experiments show that CSNN presents overall better performance compared to prior art on sheaf diffusion as well as cooperative graph neural networks.</li>
</ul>

<h3>Title: GANs Secretly Perform Approximate Bayesian Model Selection</h3>
<ul>
<li><strong>Authors: </strong>Maurizio Filippone, Marius P. Linhard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00651">https://arxiv.org/abs/2507.00651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00651">https://arxiv.org/pdf/2507.00651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00651]] GANs Secretly Perform Approximate Bayesian Model Selection(https://arxiv.org/abs/2507.00651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) are popular and successful generative models. Despite their success, optimization is notoriously challenging and they require regularization against overfitting. In this work, we explain the success and limitations of GANs by interpreting them as probabilistic generative models. This interpretation enables us to view GANs as Bayesian neural networks with partial stochasticity, allowing us to establish conditions of universal approximation. We can then cast the adversarial-style optimization of several variants of GANs as the optimization of a proxy for the marginal likelihood. Taking advantage of the connection between marginal likelihood optimization and Occam's razor, we can define regularization and optimization strategies to smooth the loss landscape and search for solutions with minimum description length, which are associated with flat minima and good generalization. The results on a wide range of experiments indicate that these strategies lead to performance improvements and pave the way to a deeper understanding of regularization strategies for GANs.</li>
</ul>

<h3>Title: Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yilun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00653">https://arxiv.org/abs/2507.00653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00653">https://arxiv.org/pdf/2507.00653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00653]] Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models(https://arxiv.org/abs/2507.00653)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The escalating computational costs of Large Language Model (LLM) inference have become a critical barrier to their widespread and sustainable deployment. While existing optimization strategies are effective, they are predominantly based on statistical heuristics or architectural modifications, lacking a guiding cognitive theory to manage the inference process itself. This paper aims to bridge this gap by introducing a novel paradigm: the Cognitive Load-Aware Inference (CLAI) framework, which operationalizes principles from Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$, and $GCL_{LLM}$), thereby reframing the inference process as a cognitive economics optimization problem: based on the intrinsic complexity of a problem ($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a fine-tuned model that internalizes these principles for spontaneous cognitive economy. Across a range of benchmarks in complex reasoning, long-context question answering, and code generation, our methods achieve significant reductions in token consumption (up to 45\%) without sacrificing accuracy. Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose difficult problems, a key characteristic of human expert cognition. This work demonstrates that by emulating the brain's resource management strategies, we can build more efficient, robust, and capable artificial intelligence systems.</li>
</ul>

<h3>Title: Neural Augmented Kalman Filters for Road Network assisted GNSS positioning</h3>
<ul>
<li><strong>Authors: </strong>Hans van Gorp, Davide Belli, Amir Jalalirad, Bence Major</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00654">https://arxiv.org/abs/2507.00654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00654">https://arxiv.org/pdf/2507.00654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00654]] Neural Augmented Kalman Filters for Road Network assisted GNSS positioning(https://arxiv.org/abs/2507.00654)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Global Navigation Satellite System (GNSS) provides critical positioning information globally, but its accuracy in dense urban environments is often compromised by multipath and non-line-of-sight errors. Road network data can be used to reduce the impact of these errors and enhance the accuracy of a positioning system. Previous works employing road network data are either limited to offline applications, or rely on Kalman Filter (KF) heuristics with little flexibility and robustness. We instead propose training a Temporal Graph Neural Network (TGNN) to integrate road network information into a KF. The TGNN is designed to predict the correct road segment and its associated uncertainty to be used in the measurement update step of the KF. We validate our approach with real-world GNSS data and open-source road networks, observing a 29% decrease in positioning error for challenging scenarios compared to a GNSS-only KF. To the best of our knowledge, ours is the first deep learning-based approach jointly employing road network data and GNSS measurements to determine the user position on Earth.</li>
</ul>

<h3>Title: LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment</h3>
<ul>
<li><strong>Authors: </strong>Juelin Zhu, Shuaibang Peng, Long Wang, Hanlin Tan, Yu Liu, Maojun Zhang, Shen Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00659">https://arxiv.org/abs/2507.00659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00659">https://arxiv.org/pdf/2507.00659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00659]] LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment(https://arxiv.org/abs/2507.00659)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We propose a novel method for aerial visual localization over low Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method LoD-Loc has shown promising localization results leveraging LoD models. However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the majority of available models and those many countries plan to construct nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD city models could unlock drones' potential for global urban localization. To address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine strategy using explicit silhouette alignment to achieve accurate localization over low-LoD city models in the air. Specifically, given a query image, LoD-Loc v2 first applies a building segmentation network to shape building silhouettes. Then, in the coarse pose selection stage, we construct a pose cost volume by uniformly sampling pose hypotheses around a prior pose to represent the pose probability distribution. Each cost of the volume measures the degree of alignment between the projected and predicted silhouettes. We select the pose with maximum value as the coarse pose. In the fine pose estimation stage, a particle filtering method incorporating a multi-beam tracking approach is used to efficiently explore the hypothesis space and obtain the final pose estimation. To further facilitate research in this field, we release two datasets with LoD1 city models covering 10.7 km , along with real RGB queries and ground-truth pose annotations. Experimental results show that LoD-Loc v2 improves estimation accuracy with high-LoD models and enables localization with low-LoD models for the first time. Moreover, it outperforms state-of-the-art baselines by large margins, even surpassing texture-model-based methods, and broadens the convergence basin to accommodate larger prior errors.</li>
</ul>

<h3>Title: SAFER: Probing Safety in Reward Models with Sparse Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Sihang Li, Wei Shi, Ziyuan Xie, Tao Liang, Guojun Ma, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00665">https://arxiv.org/abs/2507.00665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00665">https://arxiv.org/pdf/2507.00665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00665]] SAFER: Probing Safety in Reward Models with Sparse Autoencoder(https://arxiv.org/abs/2507.00665)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at this https URL. \textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.}</li>
</ul>

<h3>Title: A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Edward Effendy, Kuan-Wei Tseng, Rei Kawakami</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00676">https://arxiv.org/abs/2507.00676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00676">https://arxiv.org/pdf/2507.00676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00676]] A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation(https://arxiv.org/abs/2507.00676)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Accepted in the ICIP 2025 We present a novel transformer-based framework for whole-body grasping that addresses both pose generation and motion infilling, enabling realistic and stable object interactions. Our pipeline comprises three stages: Grasp Pose Generation for full-body grasp generation, Temporal Infilling for smooth motion continuity, and a LiftUp Transformer that refines downsampled joints back to high-resolution markers. To overcome the scarcity of hand-object interaction data, we introduce a data-efficient Generalized Pretraining stage on large, diverse motion datasets, yielding robust spatio-temporal representations transferable to grasping tasks. Experiments on the GRAB dataset show that our method outperforms state-of-the-art baselines in terms of coherence, stability, and visual realism. The modular design also supports easy adaptation to other human-motion applications.</li>
</ul>

<h3>Title: Diffusion Classifier Guidance for Non-robust Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Philipp Vaeth, Dibyanshu Kumar, Benjamin Paassen, Magda Gregorov√°</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00687">https://arxiv.org/abs/2507.00687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00687">https://arxiv.org/pdf/2507.00687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00687]] Diffusion Classifier Guidance for Non-robust Classifiers(https://arxiv.org/abs/2507.00687)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Classifier guidance is intended to steer a diffusion process such that a given classifier reliably recognizes the generated data point as a certain class. However, most classifier guidance approaches are restricted to robust classifiers, which were specifically trained on the noise of the diffusion forward process. We extend classifier guidance to work with general, non-robust, classifiers that were trained without noise. We analyze the sensitivity of both non-robust and robust classifiers to noise of the diffusion process on the standard CelebA data set, the specialized SportBalls data set and the high-dimensional real-world CelebA-HQ data set. Our findings reveal that non-robust classifiers exhibit significant accuracy degradation under noisy conditions, leading to unstable guidance gradients. To mitigate these issues, we propose a method that utilizes one-step denoised image predictions and implements stabilization techniques inspired by stochastic optimization methods, such as exponential moving averages. Experimental results demonstrate that our approach improves the stability of classifier guidance while maintaining sample diversity and visual quality. This work contributes to advancing conditional sampling techniques in generative models, enabling a broader range of classifiers to be used as guidance classifiers.</li>
</ul>

<h3>Title: Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack</h3>
<ul>
<li><strong>Authors: </strong>Keke Tang, Ziyong Du, Weilong Peng, Xiaofei Wang, Peican Zhu, Ligang Liu, Zhihong Tian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00690">https://arxiv.org/abs/2507.00690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00690">https://arxiv.org/pdf/2507.00690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00690]] Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack(https://arxiv.org/abs/2507.00690)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks on point clouds often impose strict geometric constraints to preserve plausibility; however, such constraints inherently limit transferability and undefendability. While deformation offers an alternative, existing unstructured approaches may introduce unnatural distortions, making adversarial point clouds conspicuous and undermining their plausibility. In this paper, we propose CageAttack, a cage-based deformation framework that produces natural adversarial point clouds. It first constructs a cage around the target object, providing a structured basis for smooth, natural-looking deformation. Perturbations are then applied to the cage vertices, which seamlessly propagate to the point cloud, ensuring that the resulting deformations remain intrinsic to the object and preserve plausibility. Extensive experiments on seven 3D deep neural network classifiers across three datasets show that CageAttack achieves a superior balance among transferability, undefendability, and plausibility, outperforming state-of-the-art methods. Codes will be made public upon acceptance.</li>
</ul>

<h3>Title: Rectifying Magnitude Neglect in Linear Attention</h3>
<ul>
<li><strong>Authors: </strong>Qihang Fan, Huaibo Huang, Yuang Ai, ran He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00698">https://arxiv.org/abs/2507.00698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00698">https://arxiv.org/pdf/2507.00698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00698]] Rectifying Magnitude Neglect in Linear Attention(https://arxiv.org/abs/2507.00698)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query. This prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose Magnitude-Aware Linear Attention (MALA), which modifies the computation of Linear Attention to fully incorporate the Query's magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. We evaluate the effectiveness of MALA on multiple tasks, including image classification, object detection, instance segmentation, semantic segmentation, natural language processing, speech recognition, and image generation. Our MALA achieves strong results on all of these tasks. Code will be available at this https URL</li>
</ul>

<h3>Title: SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Chong Zhang, Xichao Liu, Yibing Zhan, Dapeng Tao, Jun Ni</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00701">https://arxiv.org/abs/2507.00701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00701">https://arxiv.org/pdf/2507.00701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00701]] SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval(https://arxiv.org/abs/2507.00701)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in spaceborne GNSS missions have produced extensive global datasets, providing a robust basis for deep learning-based significant wave height (SWH) retrieval. While existing deep learning models predominantly utilize CYGNSS data with four-channel information, they often adopt single-channel inputs or simple channel concatenation without leveraging the benefits of cross-channel information interaction during training. To address this limitation, a novel spatial-channel attention-based network, namely SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each channel of the DDMs are modeled as independent attention heads, enabling the fusion of spatial and channel-wise information. For auxiliary parameters, a lightweight attention mechanism is designed to assign weights along the spatial and channel dimensions. The final feature integrates both spatial and channel-level characteristics. Model performance is evaluated using four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC buoy observations. The code is available at this https URL.</li>
</ul>

<h3>Title: BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Zeming Chen, Hang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00707">https://arxiv.org/abs/2507.00707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00707">https://arxiv.org/pdf/2507.00707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00707]] BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving(https://arxiv.org/abs/2507.00707)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Multi-view image generation in autonomous driving demands consistent 3D scene understanding across camera views. Most existing methods treat this problem as a 2D image set generation task, lacking explicit 3D modeling. However, we argue that a structured representation is crucial for scene generation, especially for autonomous driving applications. This paper proposes BEV-VAE for consistent and controllable view synthesis. BEV-VAE first trains a multi-view image variational autoencoder for a compact and unified BEV latent space and then generates the scene with a latent diffusion transformer. BEV-VAE supports arbitrary view generation given camera configurations, and optionally 3D layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance in both 3D consistent reconstruction and generation. The code is available at: this https URL.</li>
</ul>

<h3>Title: Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories</h3>
<ul>
<li><strong>Authors: </strong>Jhouben Cuesta-Ramirez, Samuel Beaussant, Mehdi Mounsif</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00711">https://arxiv.org/abs/2507.00711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00711">https://arxiv.org/pdf/2507.00711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00711]] Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories(https://arxiv.org/abs/2507.00711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) trained via Reinforcement Learning (RL) have recently achieved impressive results on reasoning benchmarks. Yet, growing evidence shows that these models often generate longer but ineffective chains of thought (CoTs), calling into question whether benchmark gains reflect real reasoning improvements. We present new evidence of overthinking, where models disregard correct solutions even when explicitly provided, instead continuing to generate unnecessary reasoning steps that often lead to incorrect conclusions. Experiments on three state-of-the-art models using the AIME2024 math benchmark reveal critical limitations in these models ability to integrate corrective information, posing new challenges for achieving robust and interpretable reasoning.</li>
</ul>

<h3>Title: AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Elizabeth Fons, Elena Kochkina, Rachneet Kaur, Zhen Zeng, Berowne Hlavaty, Charese Smiley, Svitlana Vyetrenko, Manuela Veloso</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00718">https://arxiv.org/abs/2507.00718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00718">https://arxiv.org/pdf/2507.00718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00718]] AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation(https://arxiv.org/abs/2507.00718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the potential of large language models (LLMs) to generate financial reports from time series data. We propose a framework encompassing prompt engineering, model selection, and evaluation. We introduce an automated highlighting system to categorize information within the generated reports, differentiating between insights derived directly from time series data, stemming from financial reasoning, and those reliant on external knowledge. This approach aids in evaluating the factual grounding and reasoning capabilities of the models. Our experiments, utilizing both data from the real stock market indices and synthetic time series, demonstrate the capability of LLMs to produce coherent and informative financial reports.</li>
</ul>

<h3>Title: Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features</h3>
<ul>
<li><strong>Authors: </strong>Linghui Zhu, Yiming Li, Haiqin Weng, Yan Liu, Tianwei Zhang, Shu-Tao Xia, Zhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00724">https://arxiv.org/abs/2507.00724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00724">https://arxiv.org/pdf/2507.00724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00724]] Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features(https://arxiv.org/abs/2507.00724)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Large vision models achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property for its owner. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to these personalized models. However, in this paper, we reveal that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized models by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by the output differences between the shadow and victim models. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously.</li>
</ul>

<h3>Title: Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN</h3>
<ul>
<li><strong>Authors: </strong>Arthur Thuy, Ekaterina Loginova, Dries F. Benoit</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00736">https://arxiv.org/abs/2507.00736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00736">https://arxiv.org/pdf/2507.00736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00736]] Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN(https://arxiv.org/abs/2507.00736)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Recent years have seen growing interest in Question Difficulty Estimation (QDE) using natural language processing techniques. Question difficulty is often represented using discrete levels, framing the task as ordinal regression due to the inherent ordering from easiest to hardest. However, the literature has neglected the ordinal nature of the task, relying on classification or discretized regression models, with specialized ordinal regression methods remaining unexplored. Furthermore, evaluation metrics are tightly coupled to the modeling paradigm, hindering cross-study comparability. While some metrics fail to account for the ordinal structure of difficulty levels, none adequately address class imbalance, resulting in biased performance assessments. This study addresses these limitations by benchmarking three types of model outputs -- discretized regression, classification, and ordinal regression -- using the balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly captures ordinality and class imbalance. In addition to using popular ordinal regression methods, we propose OrderedLogitNN, extending the ordered logit model from econometrics to neural networks. We fine-tune BERT on the RACE++ and ARC datasets and find that OrderedLogitNN performs considerably better on complex tasks. The balanced DRPS offers a robust and fair evaluation metric for discrete-level QDE, providing a principled foundation for future research.</li>
</ul>

<h3>Title: Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network</h3>
<ul>
<li><strong>Authors: </strong>An Le, Hung Nguyen, Sungbal Seo, You-Suk Bae, Truong Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00739">https://arxiv.org/abs/2507.00739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00739">https://arxiv.org/pdf/2507.00739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00739]] Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network(https://arxiv.org/abs/2507.00739)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This work introduces a novel biorthogonal tunable wavelet unit constructed using a lifting scheme that relaxes both the orthogonality and equal filter length constraints, providing greater flexibility in filter design. The proposed unit enhances convolution, pooling, and downsampling operations, leading to improved image classification and anomaly detection in convolutional neural networks (CNN). When integrated into an 18-layer residual neural network (ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12% and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its effectiveness in capturing fine-grained details. Similar improvements were observed in ResNet-34. For anomaly detection in the hazelnut category of the MVTec Anomaly Detection dataset, the proposed method achieved competitive and wellbalanced performance in both segmentation and detection tasks, outperforming existing approaches in terms of accuracy and robustness.</li>
</ul>

<h3>Title: Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds</h3>
<ul>
<li><strong>Authors: </strong>Craig S Wright</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00740">https://arxiv.org/abs/2507.00740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00740">https://arxiv.org/pdf/2507.00740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00740]] Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds(https://arxiv.org/abs/2507.00740)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>This paper presents a complete formal specification, protocol description, and mathematical proof structure for Simplified Payment Verification (SPV) as originally defined in the Bitcoin whitepaper \cite{nakamoto2008}. In stark contrast to the misrepresentations proliferated by popular implementations, we show that SPV is not only secure under bounded adversarial assumptions but strictly optimal for digital cash systems requiring scalable and verifiable transaction inclusion. We reconstruct the SPV protocol from first principles, grounding its verification model in symbolic automata, Merkle membership relations, and chain-of-proof dominance predicates. Through rigorous probabilistic and game-theoretic analysis, we derive the economic bounds within which the protocol operates securely and verify its liveness and safety properties under partial connectivity, hostile relay networks, and adversarial propagation delay. Our specification further introduces low-bandwidth optimisations such as adaptive polling and compressed header synchronisation while preserving correctness. This document serves both as a blueprint for secure SPV implementation and a rebuttal of common misconceptions surrounding non-validating clients.</li>
</ul>

<h3>Title: Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports</h3>
<ul>
<li><strong>Authors: </strong>Carlos Caminha, Maria de Lourdes M. Silva, Iago C. Chaves, Felipe T. Brito, Victor A. E. Farias, Javam C. Machado</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00742">https://arxiv.org/abs/2507.00742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00742">https://arxiv.org/pdf/2507.00742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00742]] Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports(https://arxiv.org/abs/2507.00742)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Computer manufacturers offer platforms for users to describe device faults using textual reports such as "My screen is flickering". Identifying the faulty component from the report is essential for automating tests and improving user experience. However, such reports are often ambiguous and lack detail, making this task challenging. Large Language Models (LLMs) have shown promise in addressing such issues. This study evaluates 27 open-source models (1B-72B parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot, Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted 98,948 inferences, processing over 51 million input tokens and generating 13 million output tokens. We achieve f1-score up to 0.76. Results show that three models offer the best balance between size and performance: mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and gemma-2-2b-it, that offer competitive performance with lower VRAM usage, enabling efficient inference on end-user devices as modern laptops or smartphones with NPUs.</li>
</ul>

<h3>Title: Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Bob Zhang, Haoran Li, Tao Zhang, Cilin Yan, Jiayin Cai, Xiaolong Jiang, Yanbin Hao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00748">https://arxiv.org/abs/2507.00748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00748">https://arxiv.org/pdf/2507.00748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00748]] Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning(https://arxiv.org/abs/2507.00748)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding in single-image scenarios with textual references. However, their performance degrades when handling real-world applications involving complex multi-image compositions and multimodal instructions, which reveals limitations in cross-image reasoning and generalization. To address these challenges, we adopt a Reinforcement Learning (RL) based post-training strategy to improve the reasoning performance of MLLMs in multi-image grounding tasks. Our approach begins with synthesizing high-quality chain-of-thought (CoT) data for cold-start initialization, followed by supervised fine-tuning (SFT) using low-rank adaptation (LoRA). The cold-start training stage enables the model to identify correct solutions. Subsequently, we perform rejection sampling using the merged SFT model to curate high-quality RL data and leverage rule-based RL to guide the model toward optimal reasoning paths. Extensive experimental results demonstrate the effectiveness of our approach, achieving +9.04\% improvements on MIG-Bench and +4.98\% improvements on several out-of-domain reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach exhibits strong generalization in multi-image perception, with gains of +3.1\% and +2.4\% over the base model on subsets of the BLINK and MMIU benchmarks, respectively.</li>
</ul>

<h3>Title: Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hao Xing, Kai Zhe Boey, Yuankai Wu, Darius Burschka, Gordon Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00752">https://arxiv.org/abs/2507.00752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00752">https://arxiv.org/pdf/2507.00752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00752]] Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation(https://arxiv.org/abs/2507.00752)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts. Extensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.</li>
</ul>

<h3>Title: Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Selim Kuzucu, Muhammad Ferjad Naeem, Anna Kukleva, Federico Tombari, Bernt Schiele</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00754">https://arxiv.org/abs/2507.00754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00754">https://arxiv.org/pdf/2507.00754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00754]] Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs(https://arxiv.org/abs/2507.00754)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Model (LLMs) blocks with Vision Transformers (ViTs) holds immense promise for vision-only tasks by leveraging the rich semantic knowledge and reasoning capabilities of LLMs. However, a fundamental challenge lies in the inherent modality mismatch between text-centric pretraining of LLMs and vision-centric training of ViTs. Direct fusion often fails to fully exploit the LLM's potential and suffers from unstable finetuning. As a result, LLM blocks are kept frozen while only the vision components are learned. As a remedy to these challenges, we introduce Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges this modality mismatch through a synergistic pre-training strategy. LUViT co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and (2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM block using the MAE objective. This joint optimization guides the ViT to produce LLM-aligned features and the LLM to effectively interpret visual information. We demonstrate through extensive experiments that LUViT significantly improves performance on various downstream vision tasks, showcasing a more effective and efficient pathway to harness LLM knowledge for visual understanding.</li>
</ul>

<h3>Title: Towards Open-World Human Action Segmentation Using Graph Convolutional Networks</h3>
<ul>
<li><strong>Authors: </strong>Hao Xing, Kai Zhe Boey, Gordon Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00756">https://arxiv.org/abs/2507.00756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00756">https://arxiv.org/pdf/2507.00756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00756]] Towards Open-World Human Action Segmentation Using Graph Convolutional Networks(https://arxiv.org/abs/2507.00756)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Human-object interaction segmentation is a fundamental task of daily activity understanding, which plays a crucial role in applications such as assistive robotics, healthcare, and autonomous systems. Most existing learning-based methods excel in closed-world action segmentation, they struggle to generalize to open-world scenarios where novel actions emerge. Collecting exhaustive action categories for training is impractical due to the dynamic diversity of human activities, necessitating models that detect and segment out-of-distribution actions without manual annotation. To address this issue, we formally define the open-world action segmentation problem and propose a structured framework for detecting and segmenting unseen actions. Our framework introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional Network (EPGCN) with a novel decoder module for robust spatiotemporal feature upsampling. 2) Mixup-based training to synthesize out-of-distribution data, eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss that groups in-distribution actions while distancing out-of-distribution samples. We evaluate our framework on two challenging human-object interaction recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets. Experimental results demonstrate significant improvements over state-of-the-art action segmentation models across multiple open-set evaluation metrics, achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and out-of-distribution detection performances (AUROC), respectively. Additionally, we conduct an in-depth ablation study to assess the impact of each proposed component, identifying the optimal framework configuration for open-world action segmentation.</li>
</ul>

<h3>Title: A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Yu, Anirbit Ghosh, Tobias Sebastian Finn, Rossella Arcucci, Marc Bocquet, Sibo Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00761">https://arxiv.org/abs/2507.00761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00761">https://arxiv.org/pdf/2507.00761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00761]] A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model(https://arxiv.org/abs/2507.00761)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Thanks to recent advances in generative AI, computers can now simulate realistic and complex natural processes. We apply this capability to predict how wildfires spread, a task made difficult by the unpredictable nature of fire and the variety of environmental conditions it depends on. In this study, We present the first denoising diffusion model for predicting wildfire spread, a new kind of AI framework that learns to simulate fires not just as one fixed outcome, but as a range of possible scenarios. By doing so, it accounts for the inherent uncertainty of wildfire dynamics, a feature that traditional models typically fail to represent. Unlike deterministic approaches that generate a single prediction, our model produces ensembles of forecasts that reflect physically meaningful distributions of where fire might go next. This technology could help us develop smarter, faster, and more reliable tools for anticipating wildfire behavior, aiding decision-makers in fire risk assessment and response planning.</li>
</ul>

<h3>Title: LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing</h3>
<ul>
<li><strong>Authors: </strong>Daniel Fein, Sebastian Russo, Violet Xiang, Kabir Jolly, Rafael Rafailov, Nick Haber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00769">https://arxiv.org/abs/2507.00769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00769">https://arxiv.org/pdf/2507.00769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00769]] LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing(https://arxiv.org/abs/2507.00769)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at this https URL, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.</li>
</ul>

<h3>Title: Generative AI and the future of scientometrics: current topics and future questions</h3>
<ul>
<li><strong>Authors: </strong>Benedetto Lepori, Jens Peter Andersen, Karsten Donnay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00783">https://arxiv.org/abs/2507.00783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00783">https://arxiv.org/pdf/2507.00783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00783]] Generative AI and the future of scientometrics: current topics and future questions(https://arxiv.org/abs/2507.00783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The aim of this paper is to review the use of GenAI in scientometrics, and to begin a debate on the broader implications for the field. First, we provide an introduction on GenAI's generative and probabilistic nature as rooted in distributional linguistics. And we relate this to the debate on the extent to which GenAI might be able to mimic human 'reasoning'. Second, we leverage this distinction for a critical engagement with recent experiments using GenAI in scientometrics, including topic labelling, the analysis of citation contexts, predictive applications, scholars' profiling, and research assessment. GenAI shows promise in tasks where language generation dominates, such as labelling, but faces limitations in tasks that require stable semantics, pragmatic reasoning, or structured domain knowledge. However, these results might become quickly outdated. Our recommendation is, therefore, to always strive to systematically compare the performance of different GenAI models for specific tasks. Third, we inquire whether, by generating large amounts of scientific language, GenAI might have a fundamental impact on our field by affecting textual characteristics used to measure science, such as authors, words, and references. We argue that careful empirical work and theoretical reflection will be essential to remain capable of interpreting the evolving patterns of knowledge production.</li>
</ul>

<h3>Title: OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection</h3>
<ul>
<li><strong>Authors: </strong>Ziji Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00789">https://arxiv.org/abs/2507.00789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00789">https://arxiv.org/pdf/2507.00789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00789]] OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection(https://arxiv.org/abs/2507.00789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models often struggle to achieve accurate semantic alignment between generated images and text prompts while maintaining efficiency for deployment on resource-constrained hardware. Existing approaches either incur substantial computational overhead through noise optimization or compromise semantic fidelity by aggressively pruning tokens. In this work, we propose OptiPrune, a unified framework that combines distribution-aware initial noise optimization with similarity-based token pruning to address both challenges simultaneously. Specifically, (1) we introduce a distribution-aware noise optimization module guided by attention scores to steer the initial latent noise toward semantically meaningful regions, mitigating issues such as subject neglect and feature entanglement; (2) we design a hardware-efficient token pruning strategy that selects representative base tokens via patch-wise similarity, injects randomness to enhance generalization, and recovers pruned tokens using maximum similarity copying before attention operations. Our method preserves the Gaussian prior during noise optimization and enables efficient inference without sacrificing alignment quality. Experiments on benchmark datasets, including Animal-Animal, demonstrate that OptiPrune achieves state-of-the-art prompt-image consistency with significantly reduced computational cost.</li>
</ul>

<h3>Title: LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling</h3>
<ul>
<li><strong>Authors: </strong>Huaqiu Li, Yong Wang, Tongwen Huang, Hailang Huang, Haoqian Wang, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00790">https://arxiv.org/abs/2507.00790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00790">https://arxiv.org/pdf/2507.00790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00790]] LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling(https://arxiv.org/abs/2507.00790)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data will be available at this https URL.</li>
</ul>

<h3>Title: TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Minye Shao, Xingyu Miao, Haoran Duan, Zeyu Wang, Jingkun Chen, Yawen Huang, Xian Wu, Jingjing Deng, Yang Long, Yefeng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00802">https://arxiv.org/abs/2507.00802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00802">https://arxiv.org/pdf/2507.00802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00802]] TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency(https://arxiv.org/abs/2507.00802)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>3D medical image generation is essential for data augmentation and patient privacy, calling for reliable and efficient models suited for clinical practice. However, current methods suffer from limited anatomical fidelity, restricted axial length, and substantial computational cost, placing them beyond reach for regions with limited resources and infrastructure. We introduce TRACE, a framework that generates 3D medical images with spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach. TRACE models sequential 2D slices as video frame pairs, combining segmentation priors and radiology reports for anatomical alignment, incorporating optical flow to sustain temporal coherence. During inference, an overlapping-frame strategy links frame pairs into a flexible length sequence, reconstructed into a spatiotemporally and anatomically aligned 3D volume. Experimental results demonstrate that TRACE effectively balances computational efficiency with preserving anatomical fidelity and spatiotemporal consistency. Code is available at: this https URL.</li>
</ul>

<h3>Title: Many LLMs Are More Utilitarian Than One</h3>
<ul>
<li><strong>Authors: </strong>Anita Keshmirian, Razan Baltaji, Babak Hemmatian, Hadi Asghari, Lav R. Varshney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00814">https://arxiv.org/abs/2507.00814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00814">https://arxiv.org/pdf/2507.00814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00814]] Many LLMs Are More Utilitarian Than One(https://arxiv.org/abs/2507.00814)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Moral judgment is integral to large language model (LLM) alignment and social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function collectively during collaboration, compared to individual agents. In human moral judgment, group deliberation leads to a utilitarian boost: a tendency to endorse norm violations that maximize benefits for the greatest number of people despite harms. We study whether a similar dynamic emerges in multi-agent LLM systems. We tested six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reasoned independently, and (2) Group, where they engaged in multi-turn discussions in pairs or triads. In personal moral dilemmas, where agents must decide to directly harm one individual to maximize the utility for others, all models found moral violations to be more acceptable when part of a group than individually, similar to human experiments. Some models endorsed actions that maximized overall well-being, even if they benefited strangers over familiar individuals. Others became more willing to violate moral norms in groups. However, while human groups show a similar action bias, the mechanism for their utilitarian boost differs from LLMs. Whereas the human shift comes from heightened sensitivity to decision outcomes, LLM groups show either reduced norm sensitivity or enhanced impartiality. This suggests that while the surface behavior of LLM collectives mimics human group reasoning, the underlying drivers differ. We discuss the implications for AI alignment, multi-agent design, and artificial moral reasoning.</li>
</ul>

<h3>Title: CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Zhang, Rui Hu, Qing Guo, Wei Yang Bryan Lim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00817">https://arxiv.org/abs/2507.00817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00817">https://arxiv.org/pdf/2507.00817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00817]] CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs(https://arxiv.org/abs/2507.00817)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Video Multimodal Large Language Models (V-MLLMs) have shown impressive capabilities in temporal reasoning and cross-modal understanding, yet their vulnerability to adversarial attacks remains underexplored due to unique challenges: complex cross-modal reasoning mechanisms, temporal dependencies, and computational constraints. We present CAVALRY-V (Cross-modal Language-Vision Adversarial Yielding for Videos), a novel framework that directly targets the critical interface between visual perception and language generation in V-MLLMs. Our approach introduces two key innovations: (1) a dual-objective semantic-visual loss function that simultaneously disrupts the model's text generation logits and visual representations to undermine cross-modal integration, and (2) a computationally efficient two-stage generator framework that combines large-scale pre-training for cross-model transferability with specialized fine-tuning for spatiotemporal coherence. Empirical evaluation on comprehensive video understanding benchmarks demonstrates that CAVALRY-V significantly outperforms existing attack methods, achieving 22.8% average improvement over the best baseline attacks on both commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5, InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves flexibility through implicit temporal coherence modeling rather than explicit regularization, enabling significant performance improvements even on image understanding (34.4% average gain). This capability demonstrates CAVALRY-V's potential as a foundational approach for adversarial research across multimodal systems.</li>
</ul>

<h3>Title: Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Yasser El Jarida, Youssef Iraqi, Loubna Mekouar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00822">https://arxiv.org/abs/2507.00822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00822">https://arxiv.org/pdf/2507.00822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00822]] Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data(https://arxiv.org/abs/2507.00822)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate particle size distribution (PSD) measurement is important in industries such as mining, pharmaceuticals, and fertilizer manufacturing, significantly influencing product quality and operational efficiency. Traditional PSD methods like sieve analysis and laser diffraction are manual, time-consuming, and limited by particle overlap. Recent developments in convolutional neural networks (CNNs) enable automated, real-time PSD estimation directly from particle images. In this work, we present a CNN-based methodology trained on realistic synthetic particle imagery generated using Blender's advanced rendering capabilities. Synthetic data sets using this method can replicate various industrial scenarios by systematically varying particle shapes, textures, lighting, and spatial arrangements that closely resemble the actual configurations. We evaluated three CNN-based architectures, ResNet-50, InceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10, d50, d90). Results demonstrated comparable accuracy across models, with EfficientNet-B0 achieving the best computational efficiency suitable for real-time industrial deployment. This approach shows the effectiveness of realistic synthetic data for robust CNN training, which offers significant potential for automated industrial PSD monitoring. The code is released at : this https URL</li>
</ul>

<h3>Title: High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery</h3>
<ul>
<li><strong>Authors: </strong>Hongxing Peng, Lide Chen, Hui Zhu, Yan Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00825">https://arxiv.org/abs/2507.00825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00825">https://arxiv.org/pdf/2507.00825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00825]] High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery(https://arxiv.org/abs/2507.00825)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial challenges, including small target sizes, high-density distributions, and cluttered backgrounds in UAV imagery. Current algorithms often depend on hand-crafted components like anchor boxes, which demand fine-tuning and exhibit limited generalization, and Non-Maximum Suppression (NMS), which is threshold-sensitive and prone to misclassifying dense objects. These generic architectures thus struggle to adapt to aerial imaging characteristics, resulting in performance limitations. Moreover, emerging end-to-end frameworks have yet to effectively mitigate these aerial-specific this http URL address these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time Detection Transformer framework tailored for UAVs. First, we introduce the High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone. HFESNet preserves critical high-frequency spatial details to extract robust semantic features, thereby improving discriminative capability for small and occluded targets in complex backgrounds. Second, our Efficient Small Object Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with minimal computational overhead, significantly boosting small object detection. Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware Positional Encoding (GAPE) modules enhance the detector's decoder stability and localization accuracy, effectively optimizing bounding boxes and providing explicit spatial priors for dense scenes. Experiments on the VisDrone dataset demonstrate that HEGS-DETR achieves a 5.1\% AP$_{50}$ and 3.8\% AP increase over the baseline, while maintaining real-time speed and reducing parameter count by 4M.</li>
</ul>

<h3>Title: A Technique for the Detection of PDF Tampering or Forgery</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Grobler, Sheunesu Makura, Hein Venter</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00827">https://arxiv.org/abs/2507.00827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00827">https://arxiv.org/pdf/2507.00827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00827]] A Technique for the Detection of PDF Tampering or Forgery(https://arxiv.org/abs/2507.00827)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark</a></li>
<li><strong>Abstract: </strong>Tampering or forgery of digital documents has become widespread, most commonly through altering images without any malicious intent such as enhancing the overall appearance of the image. However, there are occasions when tampering of digital documents can have negative consequences, such as financial fraud and reputational damage. Tampering can occur through altering a digital document's text or editing an image's pixels. Many techniques have been developed to detect whether changes have been made to a document. Most of these techniques rely on generating hashes or watermarking the document. These techniques, however, have limitations in that they cannot detect alterations to portable document format (PDF) signatures or other non-visual aspects, such as metadata. This paper presents a new technique that can be used to detect tampering within a PDF document by utilizing the PDF document's file page objects. The technique employs a prototype that can detect changes to a PDF document, such as changes made to the text, images, or metadata of the said file.</li>
</ul>

<h3>Title: On the Surprising Efficacy of LLMs for Penetration-Testing</h3>
<ul>
<li><strong>Authors: </strong>Andreas Happe, J√ºrgen Cito</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00829">https://arxiv.org/abs/2507.00829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00829">https://arxiv.org/pdf/2507.00829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00829]] On the Surprising Efficacy of LLMs for Penetration-Testing(https://arxiv.org/abs/2507.00829)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a critical examination of the surprising efficacy of Large Language Models (LLMs) in penetration testing. The paper thoroughly reviews the evolution of LLMs and their rapidly expanding capabilities which render them increasingly suitable for complex penetration testing operations. It systematically details the historical adoption of LLMs in both academic research and industry, showcasing their application across various offensive security tasks and covering broader phases of the cyber kill chain. Crucially, the analysis also extends to the observed adoption of LLMs by malicious actors, underscoring the inherent dual-use challenge of this technology within the security landscape. The unexpected effectiveness of LLMs in this context is elucidated by several key factors: the strong alignment between penetration testing's reliance on pattern-matching and LLMs' core strengths, their inherent capacity to manage uncertainty in dynamic environments, and cost-effective access to competent pre-trained models through LLM providers. The current landscape of LLM-aided penetration testing is categorized into interactive 'vibe-hacking' and the emergence of fully autonomous systems. The paper identifies and discusses significant obstacles impeding wider adoption and safe deployment. These include critical issues concerning model reliability and stability, paramount safety and security concerns, substantial monetary and ecological costs, implications for privacy and digital sovereignty, complex questions of accountability, and profound ethical dilemmas. This comprehensive review and analysis provides a foundation for discussion on future research directions and the development of robust safeguards at the intersection of AI and security.</li>
</ul>

<h3>Title: Stylometry recognizes human and LLM-generated texts in short samples</h3>
<ul>
<li><strong>Authors: </strong>Karol Przystalski, Jan K. Argasi≈Ñski, Iwona Grabska-Gradzi≈Ñska, Jeremi K. Ochab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00838">https://arxiv.org/abs/2507.00838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00838">https://arxiv.org/pdf/2507.00838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00838]] Stylometry recognizes human and LLM-generated texts in short samples(https://arxiv.org/abs/2507.00838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans, addressing issues of model attribution, intellectual property, and ethical AI use. Stylometry has been used extensively to characterise the style and attribute authorship of texts. By applying it to LLM-generated texts, we identify their emergent writing patterns. The paper involves creating a benchmark dataset based on Wikipedia, with (a) human-written term summaries, (b) texts generated purely by LLMs (GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods (Dipper, T5). The 10-sentence long texts were classified by tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based (our own pipeline) stylometric features that encode lexical, grammatical, syntactic, and punctuation patterns. The cross-validated results reached a performance of up to .87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary classification, with the particular example of Wikipedia and GPT-4 reaching up to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed features characteristic of the encyclopaedic text type, individual overused words, as well as a greater grammatical standardisation of LLMs with respect to human-written texts. These results show -- crucially, in the context of the increasingly sophisticated LLMs -- that it is possible to distinguish machine- from human-generated texts at least for a well-defined text type.</li>
</ul>

<h3>Title: Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing</h3>
<ul>
<li><strong>Authors: </strong>Keiichiro Kimura, Hiroki Kuzuno, Yoshiaki Shiraishi, Masakatu Morii</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00847">https://arxiv.org/abs/2507.00847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00847">https://arxiv.org/pdf/2507.00847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00847]] Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing(https://arxiv.org/abs/2507.00847)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Bluetooth is a pervasive wireless communication technology used by billions of devices for short-range connectivity. The security of Bluetooth relies on the pairing process, where devices establish shared long-term keys for secure communications. However, many commercial Bluetooth devices implement automatic pairing functions to improve user convenience, creating a previously unexplored attack surface. We present Stealtooth, a novel attack that abuses unknown vulnerabilities in the automatic pairing functions in commercial Bluetooth devices to achieve completely silent device link key overwriting. The Stealtooth attack leverages the fact that Bluetooth audio devices automatically transition to pairing mode under specific conditions, enabling attackers to hijack pairing processes without user awareness or specialized tools. We also extend the attack into the MitM Stealtooth attack, combining automatic pairing abuse with power-saving mode techniques to enable man-in-the-middle attacks. We evaluate the attacks against 10 commercial Bluetooth devices from major manufacturers, demonstrating widespread vulnerabilities across diverse device types and manufacturers. Our practical implementation requires only commodity hardware and open-source software, highlighting the low barrier to entry for attackers. We propose defenses both device and protocol levels, including enhanced user notifications and standardized automatic pairing guidelines. Our findings reveal a critical tension between security and usability, showing that current automatic pairing implementations create systematic vulnerabilities. We responsibly disclosed our findings to affected vendors, with several already releasing patches.</li>
</ul>

<h3>Title: UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection</h3>
<ul>
<li><strong>Authors: </strong>Wei Li, Jiaman Tang, Yang Li, Beihao Xia, Ligang Tan, Hongmao Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00849">https://arxiv.org/abs/2507.00849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00849">https://arxiv.org/pdf/2507.00849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00849]] UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection(https://arxiv.org/abs/2507.00849)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicle (UAV) object detection has been widely used in traffic management, agriculture, emergency rescue, etc. However, it faces significant challenges, including occlusions, small object sizes, and irregular shapes. These challenges highlight the necessity for a robust and efficient multimodal UAV object detection method. Mamba has demonstrated considerable potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a multimodal UAV object detection framework based on Mamba architectures. To improve geometric adaptability, we propose the Deformable Token Mamba Block (DTMB) to generate deformable tokens by incorporating adaptive patches from deformable convolutions alongside normal patches from normal convolutions, which serve as the inputs to the Mamba Block. To optimize the multimodal feature complementarity, we design two separate DTMBs for the RGB and infrared (IR) modalities, with the outputs from both DTMBs integrated into the Mamba Block for feature extraction and into the Fusion Mamba Block for feature fusion. Additionally, to improve multiscale object detection, especially for small objects, we stack four DTMBs at different scales to produce multiscale feature representations, which are then sent to the Detection Neck for Mamba (DNM). The DNM module, inspired by the YOLO series, includes modifications to the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In particular, we employ cross-enhanced spatial attention before the DTMB and cross-channel attention after the Fusion Mamba Block to extract more discriminative features. Experimental results on the DroneVehicle dataset show that our method outperforms the baseline OAFA method by 3.6% in the mAP metric. Codes will be released at this https URL.</li>
</ul>

<h3>Title: Aligning Learning and Endogenous Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Rares Cristian, Pavithra Harsha, Georgia Perakis, Brian Quanz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00851">https://arxiv.org/abs/2507.00851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00851">https://arxiv.org/pdf/2507.00851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00851]] Aligning Learning and Endogenous Decision-Making(https://arxiv.org/abs/2507.00851)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Many of the observations we make are biased by our decisions. For instance, the demand of items is impacted by the prices set, and online checkout choices are influenced by the assortments presented. The challenge in decision-making under this setting is the lack of counterfactual information, and the need to learn it instead. We introduce an end-to-end method under endogenous uncertainty to train ML models to be aware of their downstream, enabling their effective use in the decision-making stage. We further introduce a robust optimization variant that accounts for uncertainty in ML models -- specifically by constructing uncertainty sets over the space of ML models and optimizing actions to protect against worst-case predictions. We prove guarantees that this robust approach can capture near-optimal decisions with high probability as a function of data. Besides this, we also introduce a new class of two-stage stochastic optimization problems to the end-to-end learning framework that can now be addressed through our framework. Here, the first stage is an information-gathering problem to decide which random variable to poll and gain information about before making a second-stage decision based off of it. We present several computational experiments for pricing and inventory assortment/recommendation problems. We compare against existing methods in online learning/bandits/offline reinforcement learning and show our approach has consistent improved performance over these. Just as in the endogenous setting, the model's prediction also depends on the first-stage decision made. While this decision does not affect the random variable in this setting, it does affect the correct point forecast that should be made.</li>
</ul>

<h3>Title: Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Sadat Daneshmand</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00852">https://arxiv.org/abs/2507.00852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00852">https://arxiv.org/pdf/2507.00852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00852]] Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting(https://arxiv.org/abs/2507.00852)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Flexible manufacturing systems in Industry 4.0 require robots capable of handling objects in unstructured environments without rigid positioning constraints. This paper presents a computer vision system that enables industrial robots to detect and grasp pen components in arbitrary orientations without requiring structured trays, while maintaining robust performance under varying lighting conditions. We implement and evaluate a Mask R-CNN-based approach on a complete pen manufacturing line at ZHAW, addressing three critical challenges: object detection without positional constraints, robustness to extreme lighting variations, and reliable performance with cost-effective cameras. Our system achieves 95% detection accuracy across diverse lighting conditions while eliminating the need for structured component placement, demonstrating a 30% reduction in setup time and significant improvement in manufacturing flexibility. The approach is validated through extensive testing under four distinct lighting scenarios, showing practical applicability for real-world industrial deployment.</li>
</ul>

<h3>Title: SafeMap: Robust HD Map Construction from Incomplete Observations</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshuai Hao, Lingdong Kong, Rong Yin, Pengwei Wang, Jing Zhang, Yunfeng Diao, Shu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00861">https://arxiv.org/abs/2507.00861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00861">https://arxiv.org/pdf/2507.00861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00861]] SafeMap: Robust HD Map Construction from Incomplete Observations(https://arxiv.org/abs/2507.00861)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>Robust high-definition (HD) map construction is vital for autonomous driving, yet existing methods often struggle with incomplete multi-view camera data. This paper presents SafeMap, a novel framework specifically designed to secure accuracy even when certain camera views are missing. SafeMap integrates two key components: the Gaussian-based Perspective View Reconstruction (G-PVR) module and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module. G-PVR leverages prior knowledge of view importance to dynamically prioritize the most informative regions based on the relationships among available camera views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV representations derived from incomplete observations. Together, these components facilitate the end-to-end map reconstruction and robust HD map generation. SafeMap is easy to implement and integrates seamlessly into existing systems, offering a plug-and-play solution for enhanced robustness. Experimental results demonstrate that SafeMap significantly outperforms previous methods in both complete and incomplete scenarios, highlighting its superior performance and reliability.</li>
</ul>

<h3>Title: Is Visual in-Context Learning for Compositional Medical Tasks within Reach?</h3>
<ul>
<li><strong>Authors: </strong>Simon Rei√ü, Zdravko Marinov, Alexander Jaus, Constantin Seibold, M. Saquib Sarfraz, Erik Rodner, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00868">https://arxiv.org/abs/2507.00868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00868">https://arxiv.org/pdf/2507.00868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00868]] Is Visual in-Context Learning for Compositional Medical Tasks within Reach?(https://arxiv.org/abs/2507.00868)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the potential of visual in-context learning to enable a single model to handle multiple tasks and adapt to new tasks during test time without re-training. Unlike previous approaches, our focus is on training in-context learners to adapt to sequences of tasks, rather than individual tasks. Our goal is to solve complex tasks that involve multiple intermediate steps using a single model, allowing users to define entire vision pipelines flexibly at test time. To achieve this, we first examine the properties and limitations of visual in-context learning architectures, with a particular focus on the role of codebooks. We then introduce a novel method for training in-context learners using a synthetic compositional task generation engine. This engine bootstraps task sequences from arbitrary segmentation datasets, enabling the training of visual in-context learners for compositional tasks. Additionally, we investigate different masking-based training objectives to gather insights into how to train models better for solving complex, compositional tasks. Our exploration not only provides important insights especially for multi-modal medical task sequences but also highlights challenges that need to be addressed.</li>
</ul>

<h3>Title: TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation</h3>
<ul>
<li><strong>Authors: </strong>Xi Xuan, King-kui Sin, Yufei Zhou, Chunyu Kit</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00875">https://arxiv.org/abs/2507.00875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00875">https://arxiv.org/pdf/2507.00875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00875]] TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation(https://arxiv.org/abs/2507.00875)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent systems empowered by large language models (LLMs) have demonstrated remarkable capabilities in a wide range of downstream applications, including machine translation. However, the potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures. In this work, we introduce TransLaw, a novel multi-agent framework implemented for real-world Hong Kong case law translation. It employs three specialized agents, namely, Translator, Annotator, and Proofreader, to collaboratively produce translations for high accuracy in legal meaning, appropriateness in style, and adequate coherence and cohesion in structure. This framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services. We evaluated its performance using 13 open-source and commercial LLMs as agents and obtained interesting findings, including that it surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, yet trails human experts in contextualizing complex terminology and stylistic naturalness. Our platform website is available at CityUHK, and our bilingual judgment corpus used for the evaluation is available at Hugging Face.</li>
</ul>

<h3>Title: NN-Former: Rethinking Graph Structure in Neural Architecture Representation</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Xu, Haokui Zhang, Yaowei Wang, Wei Zeng, Shiliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00880">https://arxiv.org/abs/2507.00880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00880">https://arxiv.org/pdf/2507.00880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00880]] NN-Former: Rethinking Graph Structure in Neural Architecture Representation(https://arxiv.org/abs/2507.00880)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The growing use of deep learning necessitates efficient network design and deployment, making neural predictors vital for estimating attributes such as accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers have shown promising performance in representing neural architectures. However, each of both methods has its disadvantages. GNNs lack the capabilities to represent complicated features, while transformers face poor generalization when the depth of architecture grows. To mitigate the above issues, we rethink neural architecture topology and show that sibling nodes are pivotal while overlooked in previous research. We thus propose a novel predictor leveraging the strengths of GNNs and transformers to learn the enhanced topology. We introduce a novel token mixer that considers siblings, and a new channel mixer named bidirectional graph isomorphism feed-forward network. Our approach consistently achieves promising performance in both accuracy and latency prediction, providing valuable insights for learning Directed Acyclic Graph (DAG) topology. The code is available at this https URL.</li>
</ul>

<h3>Title: Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Aditya Tomar, Nihar Ranjan Sahoo, Ashish Mittal, Rudra Murthy, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00883">https://arxiv.org/abs/2507.00883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00883">https://arxiv.org/pdf/2507.00883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00883]] Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations(https://arxiv.org/abs/2507.00883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Although mathematics is often considered culturally neutral, the way mathematical problems are presented can carry implicit cultural context. Existing benchmarks like GSM8K are predominantly rooted in Western norms, including names, currencies, and everyday scenarios. In this work, we create culturally adapted variants of the GSM8K test set for five regions Africa, India, China, Korea, and Japan using prompt-based transformations followed by manual verification. We evaluate six large language models (LLMs), ranging from 8B to 72B parameters, across five prompting strategies to assess their robustness to cultural variation in math problem presentation. Our findings reveal a consistent performance gap: models perform best on the original US-centric dataset and comparatively worse on culturally adapted versions. However, models with reasoning capabilities are more resilient to these shifts, suggesting that deeper reasoning helps bridge cultural presentation gaps in mathematical tasks</li>
</ul>

<h3>Title: MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Wang, Xianhe Tang, Pufeng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00891">https://arxiv.org/abs/2507.00891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00891">https://arxiv.org/pdf/2507.00891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00891]] MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes(https://arxiv.org/abs/2507.00891)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Memes are widely used in online social interactions, providing vivid, intuitive, and often humorous means to express intentions and emotions. Existing dialogue datasets are predominantly limited to either manually annotated or pure-text conversations, lacking the expressiveness and contextual nuance that multimodal interactions this http URL address these challenges, we introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue dataset with contextually retrieved memes. Our dataset combines a large-scale, MLLM-annotated meme library with dialogues auto-generated by dual agents across diverse scenarios. We introduce a retrieval framework and adaptive threshold to ensure contextually relevant, naturally spaced meme usage. Experiments demonstrate the effectiveness of our approach in generating contextually appropriate and diverse meme-incorporated dialogues, offering a scalable and privacy-preserving resource for advancing multimodal conversational AI.</li>
</ul>

<h3>Title: TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality</h3>
<ul>
<li><strong>Authors: </strong>Carlos Vonessen, Charles Harris, Miruna Cretu, Pietro Li√≤</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00899">https://arxiv.org/abs/2507.00899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00899">https://arxiv.org/pdf/2507.00899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00899]] TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality(https://arxiv.org/abs/2507.00899)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art models for 3D molecular generation are based on significant inductive biases, SE(3), permutation equivariance to respect symmetry and graph message-passing networks to capture local chemistry, yet the generated molecules still struggle with physical plausibility. We introduce TABASCO which relaxes these assumptions: The model has a standard non-equivariant transformer architecture, treats atoms in a molecule as sequences and reconstructs bonds deterministically after generation. The absence of equivariant layers and message passing allows us to significantly simplify the model architecture and scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves state-of-the-art PoseBusters validity and delivers inference roughly 10x faster than the strongest baseline, while exhibiting emergent rotational equivariance despite symmetry not being hard-coded. Our work offers a blueprint for training minimalist, high-throughput generative models suited to specialised tasks such as structure- and pharmacophore-based drug design. We provide a link to our implementation at this http URL.</li>
</ul>

<h3>Title: The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses</h3>
<ul>
<li><strong>Authors: </strong>Fabio Correa Xavier</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00907">https://arxiv.org/abs/2507.00907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00907">https://arxiv.org/pdf/2507.00907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00907]] The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses(https://arxiv.org/abs/2507.00907)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, generative</a></li>
<li><strong>Abstract: </strong>In a world where deepfakes and cloned voices are emerging as sophisticated attack vectors, organizations require a new security mindset: Sensorial Zero Trust [9]. This article presents a scientific analysis of the need to systematically doubt information perceived through the senses, establishing rigorous verification protocols to mitigate the risks of fraud based on generative artificial intelligence. Key concepts, such as Out-of-Band verification, Vision-Language Models (VLMs) as forensic collaborators, cryptographic provenance, and human training, are integrated into a framework that extends Zero Trust principles to human sensory information. The approach is grounded in empirical findings and academic research, emphasizing that in an era of AI-generated realities, even our eyes and ears can no longer be implicitly trusted without verification. Leaders are called to foster a culture of methodological skepticism to protect organizational integrity in this new threat landscape.</li>
</ul>

<h3>Title: Privacy-Preserving Quantized Federated Learning with Diverse Precision</h3>
<ul>
<li><strong>Authors: </strong>Dang Qua Nguyen, Morteza Hashemi, Erik Perrins, Sergiy A. Vorobyov, David J. Love, Taejoon Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00920">https://arxiv.org/abs/2507.00920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00920">https://arxiv.org/pdf/2507.00920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00920]] Privacy-Preserving Quantized Federated Learning with Diverse Precision(https://arxiv.org/abs/2507.00920)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has emerged as a promising paradigm for distributed machine learning, enabling collaborative training of a global model across multiple local devices without requiring them to share raw data. Despite its advancements, FL is limited by factors such as: (i) privacy risks arising from the unprotected transmission of local model updates to the fusion center (FC) and (ii) decreased learning utility caused by heterogeneity in model quantization resolution across participating devices. Prior work typically addresses only one of these challenges because maintaining learning utility under both privacy risks and quantization heterogeneity is a non-trivial task. In this paper, our aim is therefore to improve the learning utility of a privacy-preserving FL that allows clusters of devices with different quantization resolutions to participate in each FL round. Specifically, we introduce a novel stochastic quantizer (SQ) that is designed to simultaneously achieve differential privacy (DP) and minimum quantization error. Notably, the proposed SQ guarantees bounded distortion, unlike other DP approaches. To address quantization heterogeneity, we introduce a cluster size optimization technique combined with a linear fusion approach to enhance model aggregation accuracy. Numerical simulations validate the benefits of our approach in terms of privacy protection and learning utility compared to the conventional LaplaceSQ-FL algorithm.</li>
</ul>

<h3>Title: MVP: Winning Solution to SMP Challenge 2025 Video Track</h3>
<ul>
<li><strong>Authors: </strong>Liliang Ye (1), Yunyao Zhang (1), Yafeng Wu (1), Yi-Ping Phoebe Chen (2), Junqing Yu (1), Wei Yang (1), Zikai Song (1) ((1) Huazhong University of Science and Technology, Wuhan, China, (2) La Trobe University, Melbourne, Australia)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00950">https://arxiv.org/abs/2507.00950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00950">https://arxiv.org/pdf/2507.00950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00950]] MVP: Winning Solution to SMP Challenge 2025 Video Track(https://arxiv.org/abs/2507.00950)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Social media platforms serve as central hubs for content dissemination, opinion expression, and public engagement across diverse modalities. Accurately predicting the popularity of social media videos enables valuable applications in content recommendation, trend detection, and audience engagement. In this paper, we present Multimodal Video Predictor (MVP), our winning solution to the Video Track of the SMP Challenge 2025. MVP constructs expressive post representations by integrating deep video features extracted from pretrained models with user metadata and contextual information. The framework applies systematic preprocessing techniques, including log-transformations and outlier removal, to improve model robustness. A gradient-boosted regression model is trained to capture complex patterns across modalities. Our approach ranked first in the official evaluation of the Video Track, demonstrating its effectiveness and reliability for multimodal video popularity prediction on social platforms. The source code is available at this https URL.</li>
</ul>

<h3>Title: Benchmarking the Discovery Engine</h3>
<ul>
<li><strong>Authors: </strong>Jack Foxabbott, Arush Tagade, Andrew Cusick, Robbie McCorkell, Leo McKee-Reid, Jugal Patel, Jamie Rumbelow, Jessica Rumbelow, Zohreh Shams</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00964">https://arxiv.org/abs/2507.00964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00964">https://arxiv.org/pdf/2507.00964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00964]] Benchmarking the Discovery Engine(https://arxiv.org/abs/2507.00964)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The Discovery Engine is a general purpose automated system for scientific discovery, which combines machine learning with state-of-the-art ML interpretability to enable rapid and robust scientific insight across diverse datasets. In this paper, we benchmark the Discovery Engine against five recent peer-reviewed scientific publications applying machine learning across medicine, materials science, social science, and environmental science. In each case, the Discovery Engine matches or exceeds prior predictive performance while also generating deeper, more actionable insights through rich interpretability artefacts. These results demonstrate its potential as a new standard for automated, interpretable scientific modelling that enables complex knowledge discovery from data.</li>
</ul>

<h3>Title: Surgical Neural Radiance Fields from One Image</h3>
<ul>
<li><strong>Authors: </strong>Alberto Neri, Maximilan Fehrentz, Veronica Penza, Leonardo S. Mattos, Nazim Haouchine</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00969">https://arxiv.org/abs/2507.00969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00969">https://arxiv.org/pdf/2507.00969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00969]] Surgical Neural Radiance Fields from One Image(https://arxiv.org/abs/2507.00969)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D reconstruction and view synthesis, yet their reliance on extensive multi-view data limits their application in surgical intraoperative settings where only limited data is available. In particular, collecting such extensive data intraoperatively is impractical due to time constraints. This work addresses this challenge by leveraging a single intraoperative image and preoperative data to train NeRF efficiently for surgical scenarios. Methods: We leverage preoperative MRI data to define the set of camera viewpoints and images needed for robust and unobstructed training. Intraoperatively, the appearance of the surgical image is transferred to the pre-constructed training set through neural style transfer, specifically combining WTC2 and STROTSS to prevent over-stylization. This process enables the creation of a dataset for instant and fast single-image NeRF training. Results: The method is evaluated with four clinical neurosurgical cases. Quantitative comparisons to NeRF models trained on real surgical microscope images demonstrate strong synthesis agreement, with similarity metrics indicating high reconstruction fidelity and stylistic alignment. When compared with ground truth, our method demonstrates high structural similarity, confirming good reconstruction quality and texture preservation. Conclusion: Our approach demonstrates the feasibility of single-image NeRF training in surgical settings, overcoming the limitations of traditional multi-view methods.</li>
</ul>

<h3>Title: Reasoning as an Adaptive Defense for Safety</h3>
<ul>
<li><strong>Authors: </strong>Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00971">https://arxiv.org/abs/2507.00971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00971">https://arxiv.org/pdf/2507.00971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00971]] Reasoning as an Adaptive Defense for Safety(https://arxiv.org/abs/2507.00971)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Reasoning methods that adaptively allocate test-time compute have advanced LLM performance on easy to verify domains such as math and code. In this work, we study how to utilize this approach to train models that exhibit a degree of robustness to safety vulnerabilities, and show that doing so can provide benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners for Safety), a reinforcement learning (RL) approach that trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. To build TARS, we identify three critical design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as too many refusals, and (3) a reward function to prevent degeneration of reasoning capabilities during training. Models trained with TARS exhibit adaptive behaviors by spending more compute on ambiguous queries, leading to better safety-refusal trade-offs. They also internally learn to better distinguish between safe and unsafe prompts and attain greater robustness to both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an effective, open recipe for training LLMs against jailbreaks and harmful requests by reasoning per prompt.</li>
</ul>

<h3>Title: RTMap: Real-Time Recursive Mapping with Change Detection and Localization</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Du, Sheng Yang, Lingxuan Wang, Zhenghua Hou, Chengying Cai, Zhitao Tan, Mingxia Chen, Shi-Sheng Huang, Qiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00980">https://arxiv.org/abs/2507.00980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00980">https://arxiv.org/pdf/2507.00980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00980]] RTMap: Real-Time Recursive Mapping with Change Detection and Localization(https://arxiv.org/abs/2507.00980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While recent online HD mapping methods relieve burdened offline pipelines and solve map freshness, they remain limited by perceptual inaccuracies, occlusion in dense traffic, and an inability to fuse multi-agent observations. We propose RTMap to enhance these single-traversal methods by persistently crowdsourcing a multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap simultaneously addresses three core challenges in an end-to-end fashion: (1) Uncertainty-aware positional modeling for HD map elements, (2) probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3) real-time detection for possible road structural changes. Experiments on several public autonomous driving datasets demonstrate our solid performance on both the prior-aided map quality and the localization accuracy, demonstrating our effectiveness of robustly serving downstream prediction and planning modules while gradually improving the accuracy and freshness of the crowdsourced prior-map asynchronously. Our source-code will be made publicly available at this https URL (Camera ready version incorporating reviewer suggestions will be updated soon).</li>
</ul>

<h3>Title: Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Jack Nugent, Siyang Wu, Zeyu Ma, Beining Han, Meenal Parakh, Abhishek Joshi, Lingjie Mei, Alexander Raistrick, Xinyuan Li, Jia Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00981">https://arxiv.org/abs/2507.00981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00981">https://arxiv.org/pdf/2507.00981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00981]] Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations(https://arxiv.org/abs/2507.00981)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed substantial progress on monocular depth estimation, particularly as measured by the success of large models on standard benchmarks. However, performance on standard benchmarks does not offer a complete assessment, because most evaluate accuracy but not robustness. In this work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which enables systematic robustness evaluation. PDE uses procedural generation to create 3D scenes that test robustness to various controlled perturbations, including object, camera, material and lighting changes. Our analysis yields interesting findings on what perturbations are challenging for state-of-the-art depth models, which we hope will inform further research. Code and data are available at this https URL.</li>
</ul>

<h3>Title: Discourse Heuristics For Paradoxically Moral Self-Correction</h3>
<ul>
<li><strong>Authors: </strong>Guangliang Liu, Zimo Qi, Xitong Zhang, Kristen Marie Johnson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00985">https://arxiv.org/abs/2507.00985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00985">https://arxiv.org/pdf/2507.00985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00985]] Discourse Heuristics For Paradoxically Moral Self-Correction(https://arxiv.org/abs/2507.00985)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Moral self-correction has emerged as a promising approach for aligning the output of Large Language Models (LLMs) with human moral values. However, moral self-correction techniques are subject to two primary paradoxes. First, despite empirical and theoretical evidence to support the effectiveness of self-correction, this LLM capability only operates at a superficial level. Second, while LLMs possess the capability of self-diagnosing immoral aspects of their output, they struggle to identify the cause of this moral inconsistency during their self-correction process. To better understand and address these paradoxes, we analyze the discourse constructions in fine-tuning corpora designed to enhance moral self-correction, uncovering the existence of the heuristics underlying effective constructions. We demonstrate that moral self-correction relies on discourse constructions that reflect heuristic shortcuts, and that the presence of these heuristic shortcuts during self-correction leads to inconsistency when attempting to enhance both self-correction and self-diagnosis capabilities jointly. Based on our findings, we propose a solution to improve moral self-correction by leveraging the heuristics of curated datasets. We also highlight the generalization challenges of this capability, particularly in terms of learning from situated context and model scales.</li>
</ul>

<h3>Title: UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yuanrui Wang, Cong Han, YafeiLi, Zhipeng Jin, Xiawei Li, SiNan Du, Wen Tao, Yi Yang, shuanglong li, Chun Yuan, Liu Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00992">https://arxiv.org/abs/2507.00992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00992">https://arxiv.org/pdf/2507.00992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00992]] UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis(https://arxiv.org/abs/2507.00992)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Text-to-image generation has greatly advanced content creation, yet accurately rendering visual text remains a key challenge due to blurred glyphs, semantic drift, and limited style control. Existing methods often rely on pre-rendered glyph images as conditions, but these struggle to retain original font styles and color cues, necessitating complex multi-branch designs that increase model overhead and reduce flexibility. To address these issues, we propose a segmentation-guided framework that uses pixel-level visual text masks -- rich in glyph shape, color, and spatial detail -- as unified conditional inputs. Our method introduces two core components: (1) a fine-tuned bilingual segmentation model for precise text mask extraction, and (2) a streamlined diffusion model augmented with adaptive glyph conditioning and a region-specific loss to preserve textual fidelity in both content and style. Our approach achieves state-of-the-art performance on the AnyText benchmark, significantly surpassing prior methods in both Chinese and English settings. To enable more rigorous evaluation, we also introduce two new benchmarks: GlyphMM-benchmark for testing layout and glyph consistency in complex typesetting, and MiniText-benchmark for assessing generation quality in small-scale text regions. Experimental results show that our model outperforms existing methods by a large margin in both scenarios, particularly excelling at small text rendering and complex layout preservation, validating its strong generalization and deployment readiness.</li>
</ul>

<h3>Title: La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America</h3>
<ul>
<li><strong>Authors: </strong>Mar√≠a Grandury, Javier Aula-Blasco, J√∫lia Falc√£o, Cl√©mentine Fourrier, Miguel Gonz√°lez, Gonzalo Mart√≠nez, Gonzalo Santamar√≠a, Rodrigo Agerri, Nuria Aldama, Luis Chiruzzo, Javier Conde, Helena G√≥mez, Marta Guerrero, Guido Ivetta, Natalia L√≥pez, Flor Miriam Plaza-del-Arco, Mar√≠a Teresa Mart√≠n-Valdivia, Helena Montoro, Carmen Mu√±oz, Pedro Reviriego, Leire Rosado, Alejandro Vaca, Mar√≠a Estrella Vallecillo-Rodr√≠guez, Jorge Vallego, Irune Zubiaga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.00999">https://arxiv.org/abs/2507.00999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.00999">https://arxiv.org/pdf/2507.00999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.00999]] La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America(https://arxiv.org/abs/2507.00999)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Leaderboards showcase the current capabilities and limitations of Large Language Models (LLMs). To motivate the development of LLMs that represent the linguistic and cultural diversity of the Spanish-speaking community, we present La Leaderboard, the first open-source leaderboard to evaluate generative LLMs in languages and language varieties of Spain and Latin America. La Leaderboard is a community-driven project that aims to establish an evaluation standard for everyone interested in developing LLMs for the Spanish-speaking community. This initial version combines 66 datasets in Basque, Catalan, Galician, and different Spanish varieties, showcasing the evaluation results of 50 models. To encourage community-driven development of leaderboards in other languages, we explain our methodology, including guidance on selecting the most suitable evaluation setup for each downstream task. In particular, we provide a rationale for using fewer few-shot examples than typically found in the literature, aiming to reduce environmental impact and facilitate access to reproducible results for a broader research community.</li>
</ul>

<h3>Title: ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention</h3>
<ul>
<li><strong>Authors: </strong>Yuhong Chou, Zehao Liu, Ruijie Zhu, Xinyi Wan, Tianjian Li, Congying Chu, Qian Liu, Jibin Wu, Zejun Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01004">https://arxiv.org/abs/2507.01004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01004">https://arxiv.org/pdf/2507.01004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01004]] ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention(https://arxiv.org/abs/2507.01004)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.</li>
</ul>

<h3>Title: DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zhe Kong, Le Li, Yong Zhang, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming Wei, Guanying Chen, Wenhan Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01012">https://arxiv.org/abs/2507.01012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01012">https://arxiv.org/pdf/2507.01012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01012]] DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution(https://arxiv.org/abs/2507.01012)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Real-world video super-resolution (VSR) presents significant challenges due to complex and unpredictable degradations. Although some recent methods utilize image diffusion models for VSR and have shown improved detail generation capabilities, they still struggle to produce temporally consistent frames. We attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address this issue. However, due to the intrinsic image-animation characteristics of SVD, it is challenging to generate fine details using only low-quality videos. To tackle this problem, we propose DAM-VSR, an appearance and motion disentanglement framework for VSR. This framework disentangles VSR into appearance enhancement and motion control problems. Specifically, appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. This disentanglement fully leverages the generative prior of video diffusion models and the detail generation capabilities of image super-resolution models. Furthermore, equipped with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art performance on real-world data and AIGC data, demonstrating its powerful detail generation capabilities.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
