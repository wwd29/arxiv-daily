<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Towards Fast and Scalable Private Inference. (arXiv:2307.04077v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04077">http://arxiv.org/abs/2307.04077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04077] Towards Fast and Scalable Private Inference](http://arxiv.org/abs/2307.04077) #secure</code></li>
<li>Summary: <p>Privacy and security have rapidly emerged as first order design constraints.
Users now demand more protection over who can see their data (confidentiality)
as well as how it is used (control). Here, existing cryptographic techniques
for security fall short: they secure data when stored or communicated but must
decrypt it for computation. Fortunately, a new paradigm of computing exists,
which we refer to as privacy-preserving computation (PPC). Emerging PPC
technologies can be leveraged for secure outsourced computation or to enable
two parties to compute without revealing either users' secret data. Despite
their phenomenal potential to revolutionize user protection in the digital age,
the realization has been limited due to exorbitant computational,
communication, and storage overheads.
</p></li>
</ul>

<p>This paper reviews recent efforts on addressing various PPC overheads using
private inference (PI) in neural network as a motivating application. First,
the problem and various technologies, including homomorphic encryption (HE),
secret sharing (SS), garbled circuits (GCs), and oblivious transfer (OT), are
introduced. Next, a characterization of their overheads when used to implement
PI is covered. The characterization motivates the need for both GCs and HE
accelerators. Then two solutions are presented: HAAC for accelerating GCs and
RPU for accelerating HE. To conclude, results and effects are shown with a
discussion on what future work is needed to overcome the remaining overheads of
PI.
</p>

<h2>security</h2>
<h3>Title: FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction. (arXiv:2307.03990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03990">http://arxiv.org/abs/2307.03990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03990] FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction](http://arxiv.org/abs/2307.03990) #security</code></li>
<li>Summary: <p>DeepFake based digital facial forgery is threatening public media security,
especially when lip manipulation has been used in talking face generation, and
the difficulty of fake video detection is further improved. By only changing
lip shape to match the given speech, the facial features of identity are hard
to be discriminated in such fake talking face videos. Together with the lack of
attention on audio stream as the prior knowledge, the detection failure of fake
talking face videos also becomes inevitable. It's found that the optical flow
of the fake talking face video is disordered especially in the lip region while
the optical flow of the real video changes regularly, which means the motion
feature from optical flow is useful to capture manipulation cues. In this
study, a fake talking face detection network (FTFDNet) is proposed by
incorporating visual, audio and motion features using an efficient cross-modal
fusion (CMF) module. Furthermore, a novel audio-visual attention mechanism
(AVAM) is proposed to discover more informative features, which can be
seamlessly integrated into any audio-visual CNN architecture by modularization.
With the additional AVAM, the proposed FTFDNet is able to achieve a better
detection performance than other state-of-the-art DeepFake video detection
methods not only on the established fake talking face detection dataset (FTFDD)
but also on the DeepFake video detection datasets (DFDC and DF-TIMIT).
</p></li>
</ul>

<h3>Title: A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing. (arXiv:2307.04245v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04245">http://arxiv.org/abs/2307.04245</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04245] A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing](http://arxiv.org/abs/2307.04245) #security</code></li>
<li>Summary: <p>Optical Character Recognition (OCR) technology finds applications in
digitizing books and unstructured documents, along with applications in other
domains such as mobility statistics, law enforcement, traffic, security
systems, etc. The state-of-the-art methods work well with the OCR with printed
text on license plates, shop names, etc. However, applications such as printed
textbooks and handwritten texts have limited accuracy with existing techniques.
The reason may be attributed to similar-looking characters and variations in
handwritten characters. Since these issues are challenging to address with OCR
technologies exclusively, we propose a post-processing approach using Natural
Language Processing (NLP) tools. This work presents an end-to-end pipeline that
first performs OCR on the handwritten or printed text and then improves its
accuracy using NLP.
</p></li>
</ul>

<h3>Title: From Lemons to Peaches: Improving Security ROI through Security Chaos Engineering. (arXiv:2307.03796v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03796">http://arxiv.org/abs/2307.03796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03796] From Lemons to Peaches: Improving Security ROI through Security Chaos Engineering](http://arxiv.org/abs/2307.03796) #security</code></li>
<li>Summary: <p>Traditional information security presents a poor ROI: payoffs only manifest
when attacks are successfully prevented. In a reality where attacks are
inevitable, subpar returns are therefore inevitable. The emerging paradigm of
Security Chaos Engineering offers a more remunerative and reliable ROI by
minimizing attack impacts and generating valuable evidence to inform continuous
improvement of system design and operation.
</p></li>
</ul>

<h3>Title: A Novel Pseudo-Random Number Generator Based on Multi-Objective Optimization for Image-Cryptographic Applications. (arXiv:2307.03911v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03911">http://arxiv.org/abs/2307.03911</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03911] A Novel Pseudo-Random Number Generator Based on Multi-Objective Optimization for Image-Cryptographic Applications](http://arxiv.org/abs/2307.03911) #security</code></li>
<li>Summary: <p>Pseudo-random number generators (PRNGs) play an important role to ensure the
security and confidentiality of image cryptographic algorithms. Their primary
function is to generate a sequence of numbers that possesses unpredictability
and randomness, which is crucial for the algorithms to work effectively and
provide the desired level of security. However, traditional PRNGs frequently
encounter limitations like insufficient randomness, predictability, and
vulnerability to cryptanalysis attacks. To overcome these limitations, we
propose a novel method namely an elliptic curve genetic algorithm (ECGA) for
the construction of an image-dependent pseudo-random number generator (IDPRNG)
that merges elliptic curves (ECs) and a multi-objective genetic algorithm
(MOGA). The ECGA consists of two primary stages. First, we generate an EC-based
initial sequence of random numbers using pixels of a plain-image and parameters
of an EC, that depart from traditional methods of population initialization. In
our proposed approach, the image itself serves as the seed for the initial
population in the genetic algorithm optimization, taking into account the
image-dependent nature of cryptographic applications. This allows the PRNG to
adapt its behavior to the unique characteristics of the input image, leading to
enhanced security and improved resistance against differential attacks.
Furthermore, the use of a good initial population reduces the number of
generations required by a genetic algorithm, which results in decreased
computational cost. In the second stage, we use well-known operations of a
genetic algorithm to optimize the generated sequence by maximizing a
multi-objective fitness function that is based on both the information entropy
and the period of the PRNG. By combining elliptic curves and genetic
algorithms, we enhance the randomness and security of the ECGA.
</p></li>
</ul>

<h3>Title: Enhancing Room Security and Automating Class Attendance Using ID Cards. (arXiv:2307.03926v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03926">http://arxiv.org/abs/2307.03926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03926] Enhancing Room Security and Automating Class Attendance Using ID Cards](http://arxiv.org/abs/2307.03926) #security</code></li>
<li>Summary: <p>With the rapid advancements in technology, automation has emerged as the
future of human endeavors. From simple tasks like attendance management to
complex security systems, automation has the potential to revolutionize various
aspects of our lives. This research paper explores the implementation of a
method aimed at enhancing room security in hostels and automating class
attendance using ID cards. In this study, we propose a system that utilizes the
unique identity information stored in ID cards for various security and
check-in tasks. By integrating RFID (Radio-Frequency Identification) reader
technology, GSM modules, Node MCU, and Arduino, we create a comprehensive
solution. The RFID reader scans the ID card, extracting the relevant
information and verifying the user's identity. The data is then transmitted via
the GSM module to a central database, ensuring real-time monitoring and
security measures. Moreover, the system also enables the automation of class
attendance. By utilizing the same ID cards, students can simply tap their cards
on a reader placed in the classroom. This information is recorded
automatically, eliminating the need for manual attendance taking and reducing
errors and time consumption. This research project highlights the practical
implementation of ID card technology to enhance room security in hostels and
automate class attendance processes. By leveraging the power of automation, we
aim to streamline administrative tasks, improve security measures, and optimize
efficiency in educational institutions and other relevant settings.
</p></li>
</ul>

<h3>Title: Secrets Revealed in Container Images: An Internet-wide Study on Occurrence and Impact. (arXiv:2307.03958v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03958">http://arxiv.org/abs/2307.03958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03958] Secrets Revealed in Container Images: An Internet-wide Study on Occurrence and Impact](http://arxiv.org/abs/2307.03958) #security</code></li>
<li>Summary: <p>Containerization allows bundling applications and their dependencies into a
single image. The containerization framework Docker eases the use of this
concept and enables sharing images publicly, gaining high momentum. However, it
can lead to users creating and sharing images that include private keys or API
secrets-either by mistake or out of negligence. This leakage impairs the
creator's security and that of everyone using the image. Yet, the extent of
this practice and how to counteract it remains unclear.
</p></li>
</ul>

<p>In this paper, we analyze 337,171 images from Docker Hub and 8,076 other
private registries unveiling that 8.5% of images indeed include secrets.
Specifically, we find 52,107 private keys and 3,158 leaked API secrets, both
opening a large attack surface, i.e., putting authentication and
confidentiality of privacy-sensitive data at stake and even allow active
attacks. We further document that those leaked keys are used in the wild: While
we discovered 1,060 certificates relying on compromised keys being issued by
public certificate authorities, based on further active Internet measurements,
we find 275,269 TLS and SSH hosts using leaked private keys for authentication.
To counteract this issue, we discuss how our methodology can be used to prevent
secret leakage and reuse.
</p>

<h2>privacy</h2>
<h3>Title: Bounding data reconstruction attacks with the hypothesis testing interpretation of differential privacy. (arXiv:2307.03928v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03928">http://arxiv.org/abs/2307.03928</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03928] Bounding data reconstruction attacks with the hypothesis testing interpretation of differential privacy](http://arxiv.org/abs/2307.03928) #privacy</code></li>
<li>Summary: <p>We explore Reconstruction Robustness (ReRo), which was recently proposed as
an upper bound on the success of data reconstruction attacks against machine
learning models. Previous research has demonstrated that differential privacy
(DP) mechanisms also provide ReRo, but so far, only asymptotic Monte Carlo
estimates of a tight ReRo bound have been shown. Directly computable ReRo
bounds for general DP mechanisms are thus desirable. In this work, we establish
a connection between hypothesis testing DP and ReRo and derive closed-form,
analytic or numerical ReRo bounds for the Laplace and Gaussian mechanisms and
their subsampled variants.
</p></li>
</ul>

<h3>Title: On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise. (arXiv:2307.04208v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04208">http://arxiv.org/abs/2307.04208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04208] On the Challenges of Deploying Privacy-Preserving Synthetic Data in the Enterprise](http://arxiv.org/abs/2307.04208) #privacy</code></li>
<li>Summary: <p>Generative AI technologies are gaining unprecedented popularity, causing a
mix of excitement and apprehension through their remarkable capabilities. In
this paper, we study the challenges associated with deploying synthetic data, a
subfield of Generative AI. Our focus centers on enterprise deployment, with an
emphasis on privacy concerns caused by the vast amount of personal and highly
sensitive data. We identify 40+ challenges and systematize them into five main
groups -- i) generation, ii) infrastructure &amp; architecture, iii) governance,
iv) compliance &amp; regulation, and v) adoption. Additionally, we discuss a
strategic and systematic approach that enterprises can employ to effectively
address the challenges and achieve their goals by establishing trust in the
implemented solutions.
</p></li>
</ul>

<h3>Title: Towards Assumption-free Bias Mitigation. (arXiv:2307.04105v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04105">http://arxiv.org/abs/2307.04105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04105] Towards Assumption-free Bias Mitigation](http://arxiv.org/abs/2307.04105) #privacy</code></li>
<li>Summary: <p>Despite the impressive prediction ability, machine learning models show
discrimination towards certain demographics and suffer from unfair prediction
behaviors. To alleviate the discrimination, extensive studies focus on
eliminating the unequal distribution of sensitive attributes via multiple
approaches. However, due to privacy concerns, sensitive attributes are often
either unavailable or missing in real-world scenarios. Therefore, several
existing works alleviate the bias without sensitive attributes. Those studies
face challenges, either in inaccurate predictions of sensitive attributes or
the need to mitigate unequal distribution of manually defined non-sensitive
attributes related to bias. The latter requires strong assumptions about the
correlation between sensitive and non-sensitive attributes. As data
distribution and task goals vary, the strong assumption on non-sensitive
attributes may not be valid and require domain expertise. In this work, we
propose an assumption-free framework to detect the related attributes
automatically by modeling feature interaction for bias mitigation. The proposed
framework aims to mitigate the unfair impact of identified biased feature
interactions. Experimental results on four real-world datasets demonstrate that
our proposed framework can significantly alleviate unfair prediction behaviors
by considering biased feature interactions.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Marine Debris Detection in Satellite Surveillance using Attention Mechanisms. (arXiv:2307.04128v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04128">http://arxiv.org/abs/2307.04128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04128] Marine Debris Detection in Satellite Surveillance using Attention Mechanisms](http://arxiv.org/abs/2307.04128) #protect</code></li>
<li>Summary: <p>Marine debris is an important issue for environmental protection, but current
methods for locating marine debris are yet limited. In order to achieve higher
efficiency and wider applicability in the localization of Marine debris, this
study tries to combine the instance segmentation of YOLOv7 with different
attention mechanisms and explores the best model. By utilizing a labelled
dataset consisting of satellite images containing ocean debris, we examined
three attentional models including lightweight coordinate attention, CBAM
(combining spatial and channel focus), and bottleneck transformer (based on
self-attention). Box detection assessment revealed that CBAM achieved the best
outcome (F1 score of 77%) compared to coordinate attention (F1 score of 71%)
and YOLOv7/bottleneck transformer (both F1 scores around 66%). Mask evaluation
showed CBAM again leading with an F1 score of 73%, whereas coordinate attention
and YOLOv7 had comparable performances (around F1 score of 68%/69%) and
bottleneck transformer lagged behind at F1 score of 56%. These findings suggest
that CBAM offers optimal suitability for detecting marine debris. However, it
should be noted that the bottleneck transformer detected some areas missed by
manual annotation and displayed better mask precision for larger debris pieces,
signifying potentially superior practical performance.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Adversarial Self-Attack Defense and Spatial-Temporal Relation Mining for Visible-Infrared Video Person Re-Identification. (arXiv:2307.03903v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03903">http://arxiv.org/abs/2307.03903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03903] Adversarial Self-Attack Defense and Spatial-Temporal Relation Mining for Visible-Infrared Video Person Re-Identification](http://arxiv.org/abs/2307.03903) #defense</code></li>
<li>Summary: <p>In visible-infrared video person re-identification (re-ID), extracting
features not affected by complex scenes (such as modality, camera views,
pedestrian pose, background, etc.) changes, and mining and utilizing motion
information are the keys to solving cross-modal pedestrian identity matching.
To this end, the paper proposes a new visible-infrared video person re-ID
method from a novel perspective, i.e., adversarial self-attack defense and
spatial-temporal relation mining. In this work, the changes of views, posture,
background and modal discrepancy are considered as the main factors that cause
the perturbations of person identity features. Such interference information
contained in the training samples is used as an adversarial perturbation. It
performs adversarial attacks on the re-ID model during the training to make the
model more robust to these unfavorable factors. The attack from the adversarial
perturbation is introduced by activating the interference information contained
in the input samples without generating adversarial samples, and it can be thus
called adversarial self-attack. This design allows adversarial attack and
defense to be integrated into one framework. This paper further proposes a
spatial-temporal information-guided feature representation network to use the
information in video sequences. The network cannot only extract the information
contained in the video-frame sequences but also use the relation of the local
information in space to guide the network to extract more robust features. The
proposed method exhibits compelling performance on large-scale cross-modality
video datasets. The source code of the proposed method will be released at
https://github.com/lhf12278/xxx.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03798">http://arxiv.org/abs/2307.03798</a></li>
<li>Code URL: <a href="https://github.com/matfrei/clipmasterprints">https://github.com/matfrei/clipmasterprints</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03798] CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution](http://arxiv.org/abs/2307.03798) #attack</code></li>
<li>Summary: <p>Models leveraging both visual and textual data such as Contrastive
Language-Image Pre-training (CLIP), are increasingly gaining importance. In
this work, we show that despite their versatility, such models are vulnerable
to what we refer to as fooling master images. Fooling master images are capable
of maximizing the confidence score of a CLIP model for a significant number of
widely varying prompts, while being unrecognizable for humans. We demonstrate
how fooling master images can be mined by searching the latent space of
generative models by means of an evolution strategy or stochastic gradient
descent. We investigate the properties of the mined fooling master images, and
find that images trained on a small number of image captions potentially
generalize to a much larger number of semantically related captions. Further,
we evaluate two possible mitigation strategies and find that vulnerability to
fooling master examples is closely related to a modality gap in contrastive
pre-trained multi-modal networks. From the perspective of vulnerability to
off-manifold attacks, we therefore argue for the mitigation of modality gaps in
CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints
are available at https://github.com/matfrei/CLIPMasterPrints.
</p></li>
</ul>

<h3>Title: GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty. (arXiv:2307.04099v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04099">http://arxiv.org/abs/2307.04099</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04099] GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty](http://arxiv.org/abs/2307.04099) #attack</code></li>
<li>Summary: <p>Adversarial examples (AE) with good transferability enable practical
black-box attacks on diverse target models, where insider knowledge about the
target models is not required. Previous methods often generate AE with no or
very limited transferability; that is, they easily overfit to the particular
architecture and feature representation of the source, white-box model and the
generated AE barely work for target, black-box models. In this paper, we
propose a novel approach to enhance AE transferability using Gradient Norm
Penalty (GNP). It drives the loss function optimization procedure to converge
to a flat region of local optima in the loss landscape. By attacking 11
state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we
empirically show that GNP is very effective in generating AE with high
transferability. We also demonstrate that it is very flexible in that it can be
easily integrated with other gradient based methods for stronger transfer-based
attacks.
</p></li>
</ul>

<h3>Title: Attacking (EC)DSA scheme with ephemeral keys sharing specific bits. (arXiv:2307.03979v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03979">http://arxiv.org/abs/2307.03979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03979] Attacking (EC)DSA scheme with ephemeral keys sharing specific bits](http://arxiv.org/abs/2307.03979) #attack</code></li>
<li>Summary: <p>In this paper, we present a deterministic attack on (EC)DSA signature scheme,
providing that several signatures are known such that the corresponding
ephemeral keys share a certain amount of bits without knowing their value. By
eliminating the shared blocks of bits between the ephemeral keys, we get a
lattice of dimension equal to the number of signatures having a vector
containing the private key. We compute an upper bound for the distance of this
vector from a target vector, and next, using Kannan's enumeration algorithm, we
determine it and hence the secret key. The attack can be made highly efficient
by appropriately selecting the number of shared bits and the number of
signatures.
</p></li>
</ul>

<h3>Title: Intrusion Resilience Systems for Modern Vehicles. (arXiv:2307.04184v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04184">http://arxiv.org/abs/2307.04184</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04184] Intrusion Resilience Systems for Modern Vehicles](http://arxiv.org/abs/2307.04184) #attack</code></li>
<li>Summary: <p>Current vehicular Intrusion Detection and Prevention Systems either incur
high false-positive rates or do not capture zero-day vulnerabilities, leading
to safety-critical risks. In addition, prevention is limited to few primitive
options like dropping network packets or extreme options, e.g., ECU Bus-off
state. To fill this gap, we introduce the concept of vehicular Intrusion
Resilience Systems (IRS) that ensures the resilience of critical applications
despite assumed faults or zero-day attacks, as long as threat assumptions are
met. IRS enables running a vehicular application in a replicated way, i.e., as
a Replicated State Machine, over several ECUs, and then requiring the
replicated processes to reach a form of Byzantine agreement before changing
their local state. Our study rides the mutation of modern vehicular
environments, which are closing the gap between simple and resource-constrained
"real-time and embedded systems", and complex and powerful "information
technology" ones. It shows that current vehicle (e.g., Zonal) architectures and
networks are becoming plausible for such modular fault and intrusion tolerance
solutions,deemed too heavy in the past. Our evaluation on a simulated
Automotive Ethernet network running two state-of-the-art agreement protocols
(Damysus and Hotstuff) shows that the achieved latency and throughout are
feasible for many Automotive applications.
</p></li>
</ul>

<h3>Title: Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03761">http://arxiv.org/abs/2307.03761</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03761] Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks](http://arxiv.org/abs/2307.03761) #attack</code></li>
<li>Summary: <p>In the era of digital transformation, systems monitored by the Industrial
Internet of Things (IIoTs) generate large amounts of Multivariate Time Series
(MTS) data through heterogeneous sensor networks. While this data facilitates
condition monitoring and anomaly detection, the increasing complexity and
interdependencies within the sensor network pose significant challenges for
anomaly detection. Despite progress in this field, much of the focus has been
on point anomalies and contextual anomalies, with lesser attention paid to
collective anomalies. A less addressed but common variant of collective
anomalies is when the abnormal collective behavior is caused by shifts in
interrelationships within the system. This can be due to abnormal environmental
conditions like overheating, improper operational settings resulting from
cyber-physical attacks, or system-level faults. To address these challenges,
this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a
graph-based anomaly detection framework that leverages the attention mechanism
to construct a continuous graph representation of multivariate time series by
inferring dynamic edges between time series. DyGATAD incorporates an operating
condition-aware reconstruction combined with a topology-based anomaly score,
thereby enhancing the detection ability of relationship shifts. We evaluate the
performance of DyGATAD using both a synthetic dataset with controlled varying
fault severity levels and an industrial-scale multiphase flow facility
benchmark featuring various fault types with different detection difficulties.
Our proposed approach demonstrated superior performance in collective anomaly
detection for sensor networks, showing particular strength in early-stage fault
detection, even in the case of faults with minimal severity.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03838">http://arxiv.org/abs/2307.03838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03838] RADAR: Robust AI-Text Detection via Adversarial Learning](http://arxiv.org/abs/2307.03838) #robust</code></li>
<li>Summary: <p>Recent advances in large language models (LLMs) and the intensifying
popularity of ChatGPT-like applications have blurred the boundary of
high-quality text generation between humans and machines. However, in addition
to the anticipated revolutionary changes to our technology and society, the
difficulty of distinguishing LLM-generated texts (AI-text) from human-generated
texts poses new challenges of misuse and fairness, such as fake content
generation, plagiarism, and false accusation of innocent writers. While
existing works show that current AI-text detectors are not robust to LLM-based
paraphrasing, this paper aims to bridge this gap by proposing a new framework
called RADAR, which jointly trains a Robust AI-text Detector via Adversarial
leaRning. RADAR is based on adversarial training of a paraphraser and a
detector. The paraphraser's goal is to generate realistic contents to evade
AI-text detection. RADAR uses the feedback from the detector to update the
paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly
2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,
experimental results show that RADAR significantly outperforms existing AI-text
detection methods, especially when paraphrasing is in place. We also identify
the strong transferability of RADAR from instruction-tuned LLMs to other LLMs,
and evaluate the improved capability of RADAR via GPT-3.5.
</p></li>
</ul>

<h3>Title: Robust Ranking Explanations. (arXiv:2307.04024v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04024">http://arxiv.org/abs/2307.04024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04024] Robust Ranking Explanations](http://arxiv.org/abs/2307.04024) #robust</code></li>
<li>Summary: <p>Robust explanations of machine learning models are critical to establish
human trust in the models. Due to limited cognition capability, most humans can
only interpret the top few salient features. It is critical to make top salient
features robust to adversarial attacks, especially those against the more
vulnerable gradient-based explanations. Existing defense measures robustness
using $\ell_p$-norms, which have weaker protection power. We define explanation
thickness for measuring salient features ranking stability, and derive
tractable surrogate bounds of the thickness to design the \textit{R2ET}
algorithm to efficiently maximize the thickness and anchor top salient
features. Theoretically, we prove a connection between R2ET and adversarial
training. Experiments with a wide spectrum of network architectures and data
modalities, including brain networks, demonstrate that R2ET attains higher
explanation robustness under stealthy attacks while retaining accuracy.
</p></li>
</ul>

<h3>Title: A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness. (arXiv:2307.03803v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03803">http://arxiv.org/abs/2307.03803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03803] A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness](http://arxiv.org/abs/2307.03803) #robust</code></li>
<li>Summary: <p>The robustness of deep neural networks (DNNs) against adversarial attacks has
been studied extensively in hopes of both better understanding how deep
learning models converge and in order to ensure the security of these models in
safety-critical applications. Adversarial training is one approach to
strengthening DNNs against adversarial attacks, and has been shown to offer a
means for doing so at the cost of applying computationally expensive training
methods to the entire model. To better understand these attacks and facilitate
more efficient adversarial training, in this paper we develop a novel
theoretical framework that investigates how the adversarial robustness of a
subnetwork contributes to the robustness of the entire network. To do so we
first introduce the concept of semirobustness, which is a measure of the
adversarial robustness of a subnetwork. Building on this concept, we then
provide a theoretical analysis to show that if a subnetwork is semirobust and
there is a sufficient dependency between it and each subsequent layer in the
network, then the remaining layers are also guaranteed to be robust. We
validate these findings empirically across multiple DNN architectures,
datasets, and adversarial attacks. Experiments show the ability of a robust
subnetwork to promote full-network robustness, and investigate the layer-wise
dependencies required for this full-network robustness to be achieved.
</p></li>
</ul>

<h3>Title: Controlling Chaotic Maps using Next-Generation Reservoir Computing. (arXiv:2307.03813v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03813">http://arxiv.org/abs/2307.03813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03813] Controlling Chaotic Maps using Next-Generation Reservoir Computing](http://arxiv.org/abs/2307.03813) #robust</code></li>
<li>Summary: <p>In this work, we combine nonlinear system control techniques with
next-generation reservoir computing, a best-in-class machine learning approach
for predicting the behavior of dynamical systems. We demonstrate the
performance of the controller in a series of control tasks for the chaotic
H\'enon map, including controlling the system between unstable fixed-points,
stabilizing the system to higher order periodic orbits, and to an arbitrary
desired state. We show that our controller succeeds in these tasks, requires
only 10 data points for training, can control the system to a desired
trajectory in a single iteration, and is robust to noise and modeling error.
</p></li>
</ul>

<h3>Title: Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04033">http://arxiv.org/abs/2307.04033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04033] Learning Variational Neighbor Labels for Test-Time Domain Generalization](http://arxiv.org/abs/2307.04033) #robust</code></li>
<li>Summary: <p>This paper strives for domain generalization, where models are trained
exclusively on source domains before being deployed at unseen target domains.
We follow the strict separation of source training and target testing but
exploit the value of the unlabeled target data itself during inference. We make
three contributions. First, we propose probabilistic pseudo-labeling of target
samples to generalize the source-trained model to the target domain at test
time. We formulate the generalization at test time as a variational inference
problem by modeling pseudo labels as distributions to consider the uncertainty
during generalization and alleviate the misleading signal of inaccurate pseudo
labels. Second, we learn variational neighbor labels that incorporate the
information of neighboring target samples to generate more robust pseudo
labels. Third, to learn the ability to incorporate more representative target
information and generate more precise and robust variational neighbor labels,
we introduce a meta-generalization stage during training to simulate the
generalization procedure. Experiments on six widely-used datasets demonstrate
the benefits, abilities, and effectiveness of our proposal.
</p></li>
</ul>

<h3>Title: Learning Space-Time Continuous Neural PDEs from Partially Observed States. (arXiv:2307.04110v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04110">http://arxiv.org/abs/2307.04110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04110] Learning Space-Time Continuous Neural PDEs from Partially Observed States](http://arxiv.org/abs/2307.04110) #robust</code></li>
<li>Summary: <p>We introduce a novel grid-independent model for learning partial differential
equations (PDEs) from noisy and partial observations on irregular
spatiotemporal grids. We propose a space-time continuous latent neural PDE
model with an efficient probabilistic framework and a novel encoder design for
improved data efficiency and grid independence. The latent state dynamics are
governed by a PDE model that combines the collocation method and the method of
lines. We employ amortized variational inference for approximate posterior
estimation and utilize a multiple shooting technique for enhanced training
speed and stability. Our model demonstrates state-of-the-art performance on
complex synthetic and real-world datasets, overcoming limitations of previous
approaches and effectively handling partially-observed data. The proposed model
outperforms recent methods, showing its potential to advance data-driven PDE
modeling and enabling robust, grid-independent modeling of complex
partially-observed dynamic processes.
</p></li>
</ul>

<h3>Title: A Deep Learning Framework for Solving Hyperbolic Partial Differential Equations: Part I. (arXiv:2307.04121v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04121">http://arxiv.org/abs/2307.04121</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04121] A Deep Learning Framework for Solving Hyperbolic Partial Differential Equations: Part I](http://arxiv.org/abs/2307.04121) #robust</code></li>
<li>Summary: <p>Physics informed neural networks (PINNs) have emerged as a powerful tool to
provide robust and accurate approximations of solutions to partial differential
equations (PDEs). However, PINNs face serious difficulties and challenges when
trying to approximate PDEs with dominant hyperbolic character. This research
focuses on the development of a physics informed deep learning framework to
approximate solutions to nonlinear PDEs that can develop shocks or
discontinuities without any a-priori knowledge of the solution or the location
of the discontinuities. The work takes motivation from finite element method
that solves for solution values at nodes in the discretized domain and use
these nodal values to obtain a globally defined solution field. Built on the
rigorous mathematical foundations of the discontinuous Galerkin method, the
framework naturally handles imposition of boundary conditions
(Neumann/Dirichlet), entropy conditions, and regularity requirements. Several
numerical experiments and validation with analytical solutions demonstrate the
accuracy, robustness, and effectiveness of the proposed framework.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: CA-CentripetalNet: A novel anchor-free deep learning framework for hardhat wearing detection. (arXiv:2307.04103v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04103">http://arxiv.org/abs/2307.04103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04103] CA-CentripetalNet: A novel anchor-free deep learning framework for hardhat wearing detection](http://arxiv.org/abs/2307.04103) #extraction</code></li>
<li>Summary: <p>Automatic hardhat wearing detection can strengthen the safety management in
construction sites, which is still challenging due to complicated video
surveillance scenes. To deal with the poor generalization of previous deep
learning based methods, a novel anchor-free deep learning framework called
CA-CentripetalNet is proposed for hardhat wearing detection. Two novel schemes
are proposed to improve the feature extraction and utilization ability of
CA-CentripetalNet, which are vertical-horizontal corner pooling and bounding
constrained center attention. The former is designed to realize the
comprehensive utilization of marginal features and internal features. The
latter is designed to enforce the backbone to pay attention to internal
features, which is only used during the training rather than during the
detection. Experimental results indicate that the CA-CentripetalNet achieves
better performance with the 86.63% mAP (mean Average Precision) with less
memory consumption at a reasonable speed than the existing deep learning based
methods, especially in case of small-scale hardhats and non-worn-hardhats.
</p></li>
</ul>

<h3>Title: Linguistic representations for fewer-shot relation extraction across domains. (arXiv:2307.03823v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03823">http://arxiv.org/abs/2307.03823</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03823] Linguistic representations for fewer-shot relation extraction across domains](http://arxiv.org/abs/2307.03823) #extraction</code></li>
<li>Summary: <p>Recent work has demonstrated the positive impact of incorporating linguistic
representations as additional context and scaffolding on the in-domain
performance of several NLP tasks. We extend this work by exploring the impact
of linguistic representations on cross-domain performance in a few-shot
transfer setting. An important question is whether linguistic representations
enhance generalizability by providing features that function as cross-domain
pivots. We focus on the task of relation extraction on three datasets of
procedural text in two domains, cooking and materials science. Our approach
augments a popular transformer-based architecture by alternately incorporating
syntactic and semantic graphs constructed by freely available off-the-shelf
tools. We examine their utility for enhancing generalization, and investigate
whether earlier findings, e.g. that semantic representations can be more
helpful than syntactic ones, extend to relation extraction in multiple domains.
We find that while the inclusion of these graphs results in significantly
higher performance in few-shot transfer, both types of graph exhibit roughly
equivalent utility.
</p></li>
</ul>

<h3>Title: MDACE: MIMIC Documents Annotated with Code Evidence. (arXiv:2307.03859v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03859">http://arxiv.org/abs/2307.03859</a></li>
<li>Code URL: <a href="https://github.com/3mcloud/MDACE">https://github.com/3mcloud/MDACE</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03859] MDACE: MIMIC Documents Annotated with Code Evidence](http://arxiv.org/abs/2307.03859) #extraction</code></li>
<li>Summary: <p>We introduce a dataset for evidence/rationale extraction on an extreme
multi-label classification task over long medical documents. One such task is
Computer-Assisted Coding (CAC) which has improved significantly in recent
years, thanks to advances in machine learning technologies. Yet simply
predicting a set of final codes for a patient encounter is insufficient as CAC
systems are required to provide supporting textual evidence to justify the
billing codes. A model able to produce accurate and reliable supporting
evidence for each code would be a tremendous benefit. However, a human
annotated code evidence corpus is extremely difficult to create because it
requires specialized knowledge. In this paper, we introduce MDACE, the first
publicly available code evidence dataset, which is built on a subset of the
MIMIC-III clinical records. The dataset -- annotated by professional medical
coders -- consists of 302 Inpatient charts with 3,934 evidence spans and 52
Profee charts with 5,563 evidence spans. We implemented several evidence
extraction methods based on the EffectiveCAN model (Liu et al., 2021) to
establish baseline performance on this dataset. MDACE can be used to evaluate
code evidence extraction methods for CAC systems, as well as the accuracy and
interpretability of deep learning models for multi-label classification. We
believe that the release of MDACE will greatly improve the understanding and
application of deep learning technologies for medical coding and document
classification.
</p></li>
</ul>

<h3>Title: HistRED: A Historical Document-Level Relation Extraction Dataset. (arXiv:2307.04285v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04285">http://arxiv.org/abs/2307.04285</a></li>
<li>Code URL: <a href="https://huggingface.co/datasets/Soyoung/HistRED">https://huggingface.co/datasets/Soyoung/HistRED</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04285] HistRED: A Historical Document-Level Relation Extraction Dataset](http://arxiv.org/abs/2307.04285) #extraction</code></li>
<li>Summary: <p>Despite the extensive applications of relation extraction (RE) tasks in
various domains, little has been explored in the historical context, which
contains promising data across hundreds and thousands of years. To promote the
historical RE research, we present HistRED constructed from Yeonhaengnok.
Yeonhaengnok is a collection of records originally written in Hanja, the
classical Chinese writing, which has later been translated into Korean. HistRED
provides bilingual annotations such that RE can be performed on Korean and
Hanja texts. In addition, HistRED supports various self-contained subtexts with
different lengths, from a sentence level to a document level, supporting
diverse context settings for researchers to evaluate the robustness of their RE
models. To demonstrate the usefulness of our dataset, we propose a bilingual RE
model that leverages both Korean and Hanja contexts to predict relations
between entities. Our model outperforms monolingual baselines on HistRED,
showing that employing multiple language contexts supplements the RE
predictions. The dataset is publicly available at:
https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning over a Wireless Network: Distributed User Selection through Random Access. (arXiv:2307.03758v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03758">http://arxiv.org/abs/2307.03758</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03758] Federated Learning over a Wireless Network: Distributed User Selection through Random Access](http://arxiv.org/abs/2307.03758) #federate</code></li>
<li>Summary: <p>User selection has become crucial for decreasing the communication costs of
federated learning (FL) over wireless networks. However, centralized user
selection causes additional system complexity. This study proposes a network
intrinsic approach of distributed user selection that leverages the radio
resource competition mechanism in random access. Taking the carrier sensing
multiple access (CSMA) mechanism as an example of random access, we manipulate
the contention window (CW) size to prioritize certain users for obtaining radio
resources in each round of training. Training data bias is used as a target
scenario for FL with user selection. Prioritization is based on the distance
between the newly trained local model and the global model of the previous
round. To avoid excessive contribution by certain users, a counting mechanism
is used to ensure fairness. Simulations with various datasets demonstrate that
this method can rapidly achieve convergence similar to that of the centralized
user selection approach.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Is ChatGPT a Good Personality Recognizer? A Preliminary Study. (arXiv:2307.03952v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03952">http://arxiv.org/abs/2307.03952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03952] Is ChatGPT a Good Personality Recognizer? A Preliminary Study](http://arxiv.org/abs/2307.03952) #fair</code></li>
<li>Summary: <p>In recent years, personality has been regarded as a valuable personal factor
being incorporated into numerous tasks such as sentiment analysis and product
recommendation. This has led to widespread attention to text-based personality
recognition task, which aims to identify an individual's personality based on
given text. Considering that ChatGPT has recently exhibited remarkable
abilities on various natural language processing tasks, we provide a
preliminary evaluation of ChatGPT on text-based personality recognition task
for generating effective personality data. Concretely, we employ a variety of
prompting strategies to explore ChatGPT's ability in recognizing personality
from given text, especially the level-oriented prompting strategy we designed
for guiding ChatGPT in analyzing given text at a specified level. We compare
the performance of ChatGPT on two representative real-world datasets with
traditional neural network, fine-tuned RoBERTa, and corresponding
state-of-the-art task-specific model. The experimental results show that
ChatGPT with zero-shot chain-of-thought prompting exhibits impressive
personality recognition ability. Triggered by zero-shot chain-of-thought
prompting, ChatGPT outperforms fine-tuned RoBERTa on the two datasets and is
capable to provide natural language explanations through text-based logical
reasoning. Furthermore, relative to zero-shot chain-of-thought prompting,
zero-shot level-oriented chain-of-thought prompting enhances the personality
prediction ability of ChatGPT and reduces the performance gap between ChatGPT
and corresponding state-of-the-art task-specific model. Besides, we also
conduct experiments to observe the fairness of ChatGPT when identifying
personality and discover that ChatGPT shows unfairness to some sensitive
demographic attributes such as gender and age.
</p></li>
</ul>

<h3>Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data. (arXiv:2307.04303v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04303">http://arxiv.org/abs/2307.04303</a></li>
<li>Code URL: <a href="https://github.com/anthonysicilia/equitable-dialogue-acl2023">https://github.com/anthonysicilia/equitable-dialogue-acl2023</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04303] Learning to Generate Equitable Text in Dialogue from Biased Training Data](http://arxiv.org/abs/2307.04303) #fair</code></li>
<li>Summary: <p>The ingrained principles of fairness in a dialogue system's decision-making
process and generated responses are crucial for user engagement, satisfaction,
and task achievement. Absence of equitable and inclusive principles can hinder
the formation of common ground, which in turn negatively impacts the overall
performance of the system. For example, misusing pronouns in a user interaction
may cause ambiguity about the intended subject. Yet, there is no comprehensive
study of equitable text generation in dialogue. Aptly, in this work, we use
theories of computational learning to study this problem. We provide formal
definitions of equity in text generation, and further, prove formal connections
between learning human-likeness and learning equity: algorithms for improving
equity ultimately reduce to algorithms for improving human-likeness (on
augmented data). With this insight, we also formulate reasonable conditions
under which text generation algorithms can learn to generate equitable text
without any modifications to the biased training data on which they learn. To
exemplify our theory in practice, we look at a group of algorithms for the
GuessWhat?! visual dialogue game and, using this example, test our theory
empirically. Our theory accurately predicts relative-performance of multiple
algorithms in generating equitable text as measured by both human and automated
evaluation.
</p></li>
</ul>

<h3>Title: Fairness-Aware Graph Neural Networks: A Survey. (arXiv:2307.03929v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03929">http://arxiv.org/abs/2307.03929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03929] Fairness-Aware Graph Neural Networks: A Survey](http://arxiv.org/abs/2307.03929) #fair</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have become increasingly important due to their
representational power and state-of-the-art predictive performance on many
fundamental learning tasks. Despite this success, GNNs suffer from fairness
issues that arise as a result of the underlying graph data and the fundamental
aggregation mechanism that lies at the heart of the large class of GNN models.
In this article, we examine and categorize fairness techniques for improving
the fairness of GNNs. Previous work on fair GNN models and techniques are
discussed in terms of whether they focus on improving fairness during a
preprocessing step, during training, or in a post-processing phase.
Furthermore, we discuss how such techniques can be used together whenever
appropriate, and highlight the advantages and intuition as well. We also
introduce an intuitive taxonomy for fairness evaluation metrics including
graph-level fairness, neighborhood-level fairness, embedding-level fairness,
and prediction-level fairness metrics. In addition, graph datasets that are
useful for benchmarking the fairness of GNN models are summarized succinctly.
Finally, we highlight key open problems and challenges that remain to be
addressed.
</p></li>
</ul>

<h3>Title: On The Impact of Machine Learning Randomness on Group Fairness. (arXiv:2307.04138v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04138">http://arxiv.org/abs/2307.04138</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04138] On The Impact of Machine Learning Randomness on Group Fairness](http://arxiv.org/abs/2307.04138) #fair</code></li>
<li>Summary: <p>Statistical measures for group fairness in machine learning reflect the gap
in performance of algorithms across different groups. These measures, however,
exhibit a high variance between different training instances, which makes them
unreliable for empirical evaluation of fairness. What causes this high
variance? We investigate the impact on group fairness of different sources of
randomness in training neural networks. We show that the variance in group
fairness measures is rooted in the high volatility of the learning process on
under-represented groups. Further, we recognize the dominant source of
randomness as the stochasticity of data order during training. Based on these
findings, we show how one can control group-level accuracy (i.e., model
fairness), with high efficiency and negligible impact on the model's overall
performance, by simply changing the data order for a single epoch.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03887">http://arxiv.org/abs/2307.03887</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03887] Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining](http://arxiv.org/abs/2307.03887) #interpretability</code></li>
<li>Summary: <p>In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model's output to specific
features of the data. One such of these methods is the prototypical part
network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
this method often learns to classify from spurious or inconsistent parts of the
image. Hoping to remedy this, we take inspiration from the recent developments
in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns to
identify non-spurious prototypes. In place of a full RL update, we propose the
reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet),
which adds an additional three steps to the ProtoPNet training loop. The first
two steps are reward-based reweighting and reselection, which align prototypes
with human feedback. The final step is retraining to realign the model's
features with the updated prototypes. We find that R3-ProtoPNet improves the
overall consistency and meaningfulness of the prototypes, but lower the test
predictive accuracy when used independently. When multiple R3-ProtoPNets are
incorporated into an ensemble, we find an increase in test predictive
performance while maintaining interpretability.
</p></li>
</ul>

<h3>Title: Histopathology Whole Slide Image Analysis with Heterogeneous Graph Representation Learning. (arXiv:2307.04189v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04189">http://arxiv.org/abs/2307.04189</a></li>
<li>Code URL: <a href="https://github.com/hku-medai/wsi-hgnn">https://github.com/hku-medai/wsi-hgnn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04189] Histopathology Whole Slide Image Analysis with Heterogeneous Graph Representation Learning](http://arxiv.org/abs/2307.04189) #interpretability</code></li>
<li>Summary: <p>Graph-based methods have been extensively applied to whole-slide
histopathology image (WSI) analysis due to the advantage of modeling the
spatial relationships among different entities. However, most of the existing
methods focus on modeling WSIs with homogeneous graphs (e.g., with homogeneous
node type). Despite their successes, these works are incapable of mining the
complex structural relations between biological entities (e.g., the diverse
interaction among different cell types) in the WSI. We propose a novel
heterogeneous graph-based framework to leverage the inter-relationships among
different types of nuclei for WSI analysis. Specifically, we formulate the WSI
as a heterogeneous graph with "nucleus-type" attribute to each node and a
semantic similarity attribute to each edge. We then present a new
heterogeneous-graph edge attribute transformer (HEAT) to take advantage of the
edge and node heterogeneity during massage aggregating. Further, we design a
new pseudo-label-based semantic-consistent pooling mechanism to obtain
graph-level features, which can mitigate the over-parameterization issue of
conventional cluster-based pooling. Additionally, observing the limitations of
existing association-based localization methods, we propose a causal-driven
approach attributing the contribution of each node to improve the
interpretability of our framework. Extensive experiments on three public TCGA
benchmark datasets demonstrate that our framework outperforms the
state-of-the-art methods with considerable margins on various tasks. Our codes
are available at https://github.com/HKU-MedAI/WSI-HGNN.
</p></li>
</ul>

<h3>Title: Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04075">http://arxiv.org/abs/2307.04075</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04075] Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data](http://arxiv.org/abs/2307.04075) #interpretability</code></li>
<li>Summary: <p>Due to the high heterogeneity and clinical characteristics of cancer, there
are significant differences in multi-omics data and clinical features among
subtypes of different cancers. Therefore, the identification and discovery of
cancer subtypes are crucial for the diagnosis, treatment, and prognosis of
cancer. In this study, we proposed a generalization framework based on
attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze
cancer multi-omics data for the identification and characterization of cancer
subtypes. AMUCL framework includes a unsupervised multi-head attention
mechanism, which deeply extracts multi-omics data features. Importantly, a
decoupled contrastive learning model (DMACL) based on a multi-head attention
mechanism is proposed to learn multi-omics data features and clusters and
identify new cancer subtypes. This unsupervised contrastive learning method
clusters subtypes by calculating the similarity between samples in the feature
space and sample space of multi-omics data. Compared to 11 other deep learning
models, the DMACL model achieved a C-index of 0.002, a Silhouette score of
0.801, and a Davies Bouldin Score of 0.38 on a single-cell multi-omics dataset.
On a cancer multi-omics dataset, the DMACL model obtained a C-index of 0.016, a
Silhouette score of 0.688, and a Davies Bouldin Score of 0.46, and obtained the
most reliable cancer subtype clustering results for each type of cancer.
Finally, we used the DMACL model in the AMUCL framework to reveal six cancer
subtypes of AML. By analyzing the GO functional enrichment, subtype-specific
biological functions, and GSEA of AML, we further enhanced the interpretability
of cancer subtype analysis based on the generalizable AMUCL framework.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant. (arXiv:2307.04276v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04276">http://arxiv.org/abs/2307.04276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04276] Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant](http://arxiv.org/abs/2307.04276) #explainability</code></li>
<li>Summary: <p>Automated Essay scoring has been explored as a research and industry problem
for over 50 years. It has drawn a lot of attention from the NLP community
because of its clear educational value as a research area that can engender the
creation of valuable time-saving tools for educators around the world. Yet,
these tools are generally focused on detecting good grammar, spelling mistakes,
and organization quality but tend to fail at incorporating persuasiveness
features in their final assessment. The responsibility to give actionable
feedback to the student to improve the strength of their arguments is left
solely on the teacher's shoulders. In this work, we present a transformer-based
architecture capable of achieving above-human accuracy in annotating
argumentative writing discourse elements for their persuasiveness quality and
we expand on planned future work investigating the explainability of our model
so that actionable feedback can be offered to the student and thus potentially
enable a partnership between the teacher's advice and the machine's advice.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Unsupervised 3D out-of-distribution detection with latent diffusion models. (arXiv:2307.03777v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03777">http://arxiv.org/abs/2307.03777</a></li>
<li>Code URL: <a href="https://github.com/marksgraham/ddpm-ood">https://github.com/marksgraham/ddpm-ood</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03777] Unsupervised 3D out-of-distribution detection with latent diffusion models](http://arxiv.org/abs/2307.03777) #diffusion</code></li>
<li>Summary: <p>Methods for out-of-distribution (OOD) detection that scale to 3D data are
crucial components of any real-world clinical deep learning system. Classic
denoising diffusion probabilistic models (DDPMs) have been recently proposed as
a robust way to perform reconstruction-based OOD detection on 2D datasets, but
do not trivially scale to 3D data. In this work, we propose to use Latent
Diffusion Models (LDMs), which enable the scaling of DDPMs to high-resolution
3D medical data. We validate the proposed approach on near- and far-OOD
datasets and compare it to a recently proposed, 3D-enabled approach using
Latent Transformer Models (LTMs). Not only does the proposed LDM-based approach
achieve statistically significant better performance, it also shows less
sensitivity to the underlying latent representation, more favourable memory
scaling, and produces better spatial anomaly maps. Code is available at
https://github.com/marksgraham/ddpm-ood
</p></li>
</ul>

<h3>Title: Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation. (arXiv:2307.03833v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03833">http://arxiv.org/abs/2307.03833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03833] Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation](http://arxiv.org/abs/2307.03833) #diffusion</code></li>
<li>Summary: <p>Learning-based methods have dominated the 3D human pose estimation (HPE)
tasks with significantly better performance in most benchmarks than traditional
optimization-based methods. Nonetheless, 3D HPE in the wild is still the
biggest challenge of learning-based models, whether with 2D-3D lifting,
image-to-3D, or diffusion-based methods, since the trained networks implicitly
learn camera intrinsic parameters and domain-based 3D human pose distributions
and estimate poses by statistical average. On the other hand, the
optimization-based methods estimate results case-by-case, which can predict
more diverse and sophisticated human poses in the wild. By combining the
advantages of optimization-based and learning-based methods, we propose the
Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the
problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO
achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mm
without training with any 2D-3D or image-3D pairs. Moreover, our
single-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE
$42.6$mm on cross-dataset evaluation, which even outperforms learning-based
methods trained on 3DPW.
</p></li>
</ul>

<h3>Title: TractGeoNet: A geometric deep learning framework for pointwise analysis of tract microstructure to predict language assessment performance. (arXiv:2307.03982v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03982">http://arxiv.org/abs/2307.03982</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03982] TractGeoNet: A geometric deep learning framework for pointwise analysis of tract microstructure to predict language assessment performance](http://arxiv.org/abs/2307.03982) #diffusion</code></li>
<li>Summary: <p>We propose a geometric deep-learning-based framework, TractGeoNet, for
performing regression using diffusion magnetic resonance imaging (dMRI)
tractography and associated pointwise tissue microstructure measurements. By
employing a point cloud representation, TractGeoNet can directly utilize
pointwise tissue microstructure and positional information from all points
within a fiber tract. To improve regression performance, we propose a novel
loss function, the Paired-Siamese Regression loss, which encourages the model
to focus on accurately predicting the relative differences between regression
label scores rather than just their absolute values. In addition, we propose a
Critical Region Localization algorithm to identify highly predictive anatomical
regions within the white matter fiber tracts for the regression task. We
evaluate the effectiveness of the proposed method by predicting individual
performance on two neuropsychological assessments of language using a dataset
of 20 association white matter fiber tracts from 806 subjects from the Human
Connectome Project. The results demonstrate superior prediction performance of
TractGeoNet compared to several popular regression models. Of the twenty tracts
studied, we find that the left arcuate fasciculus tract is the most highly
predictive of the two studied language performance assessments. The localized
critical regions are widespread and distributed across both hemispheres and all
cerebral lobes, including areas of the brain considered important for language
function such as superior and anterior temporal regions, pars opercularis, and
precentral gyrus. Overall, TractGeoNet demonstrates the potential of geometric
deep learning to enhance the study of the brain's white matter fiber tracts and
to relate their structure to human traits such as language performance.
</p></li>
</ul>

<h3>Title: Stimulating the Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling. (arXiv:2307.03992v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03992">http://arxiv.org/abs/2307.03992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03992] Stimulating the Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling](http://arxiv.org/abs/2307.03992) #diffusion</code></li>
<li>Summary: <p>Image denoising is a fundamental problem in computational photography, where
achieving high-quality perceptual performance with low distortion is highly
demanding. Current methods either struggle with perceptual performance or
suffer from significant distortion. Recently, the emerging diffusion model
achieves state-of-the-art performance in various tasks, and its denoising
mechanism demonstrates great potential for image denoising. However,
stimulating diffusion models for image denoising is not straightforward and
requires solving several critical problems. On the one hand, the input
inconsistency hinders the connection of diffusion models and image denoising.
On the other hand, the content inconsistency between the generated image and
the desired denoised image introduces additional distortion. To tackle these
problems, we present a novel strategy called Diffusion Model for Image
Denoising (DMID) by understanding and rethinking the diffusion model from a
denoising perspective. Our DMID strategy includes an adaptive embedding method
that embeds the noisy image into a pre-trained diffusion model, and an adaptive
ensembling method that reduces distortion in the denoised image. Our DMID
strategy achieves state-of-the-art performance on all distortion-based and
perceptual metrics, for both Gaussian and real-world image denoising.
</p></li>
</ul>

<h3>Title: Measuring the Success of Diffusion Models at Imitating Human Artists. (arXiv:2307.04028v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04028">http://arxiv.org/abs/2307.04028</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04028] Measuring the Success of Diffusion Models at Imitating Human Artists](http://arxiv.org/abs/2307.04028) #diffusion</code></li>
<li>Summary: <p>Modern diffusion models have set the state-of-the-art in AI image generation.
Their success is due, in part, to training on Internet-scale data which often
includes copyrighted work. This prompts questions about the extent to which
these models learn from, imitate, or copy the work of human artists. This work
suggests that tying copyright liability to the capabilities of the model may be
useful given the evolving ecosystem of generative models. Specifically, much of
the legal analysis of copyright and generative systems focuses on the use of
protected data for training. As a result, the connections between data,
training, and the system are often obscured. In our approach, we consider
simple image classification techniques to measure a model's ability to imitate
specific artists. Specifically, we use Contrastive Language-Image Pretrained
(CLIP) encoders to classify images in a zero-shot fashion. Our process first
prompts a model to imitate a specific artist. Then, we test whether CLIP can be
used to reclassify the artist (or the artist's work) from the imitation. If
these tests match the imitation back to the original artist, this suggests the
model can imitate that artist's expression. Our approach is simple and
quantitative. Furthermore, it uses standard techniques and does not require
additional training. We demonstrate our approach with an audit of Stable
Diffusion's capacity to imitate 70 professional digital artists with
copyrighted work online. When Stable Diffusion is prompted to imitate an artist
from this set, we find that the artist can be identified from the imitation
with an average accuracy of 81.0%. Finally, we also show that a sample of the
artist's work can be matched to these imitation images with a high degree of
statistical reliability. Overall, these results suggest that Stable Diffusion
is broadly successful at imitating individual human artists.
</p></li>
</ul>

<h3>Title: DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer. (arXiv:2307.04157v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04157">http://arxiv.org/abs/2307.04157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04157] DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer](http://arxiv.org/abs/2307.04157) #diffusion</code></li>
<li>Summary: <p>Neural Style Transfer (NST) is the field of study applying neural techniques
to modify the artistic appearance of a content image to match the style of a
reference style image. Traditionally, NST methods have focused on texture-based
image edits, affecting mostly low level information and keeping most image
structures the same. However, style-based deformation of the content is
desirable for some styles, especially in cases where the style is abstract or
the primary concept of the style is in its deformed rendition of some content.
With the recent introduction of diffusion models, such as Stable Diffusion, we
can access far more powerful image generation techniques, enabling new
possibilities. In our work, we propose using this new class of models to
perform style transfer while enabling deformable style transfer, an elusive
capability in previous models. We show how leveraging the priors of these
models can expose new artistic controls at inference time, and we document our
findings in exploring this new direction for the field of style transfer.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Context-aware Pedestrian Trajectory Prediction with Multimodal Transformer. (arXiv:2307.03786v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03786">http://arxiv.org/abs/2307.03786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03786] Context-aware Pedestrian Trajectory Prediction with Multimodal Transformer](http://arxiv.org/abs/2307.03786) #transformer</code></li>
<li>Summary: <p>We propose a novel solution for predicting future trajectories of
pedestrians. Our method uses a multimodal encoder-decoder transformer
architecture, which takes as input both pedestrian locations and ego-vehicle
speeds. Notably, our decoder predicts the entire future trajectory in a
single-pass and does not perform one-step-ahead prediction, which makes the
method effective for embedded edge deployment. We perform detailed experiments
and evaluate our method on two popular datasets, PIE and JAAD. Quantitative
results demonstrate the superiority of our proposed model over the current
state-of-the-art, which consistently achieves the lowest error for 3 time
horizons of 0.5, 1.0 and 1.5 seconds. Moreover, the proposed method is
significantly faster than the state-of-the-art for the two datasets of PIE and
JAAD. Lastly, ablation experiments demonstrate the impact of the key multimodal
configuration of our method.
</p></li>
</ul>

<h3>Title: VS-TransGRU: A Novel Transformer-GRU-based Framework Enhanced by Visual-Semantic Fusion for Egocentric Action Anticipation. (arXiv:2307.03918v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03918">http://arxiv.org/abs/2307.03918</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03918] VS-TransGRU: A Novel Transformer-GRU-based Framework Enhanced by Visual-Semantic Fusion for Egocentric Action Anticipation](http://arxiv.org/abs/2307.03918) #transformer</code></li>
<li>Summary: <p>Egocentric action anticipation is a challenging task that aims to make
advanced predictions of future actions from current and historical observations
in the first-person view. Most existing methods focus on improving the model
architecture and loss function based on the visual input and recurrent neural
network to boost the anticipation performance. However, these methods, which
merely consider visual information and rely on a single network architecture,
gradually reach a performance plateau. In order to fully understand what has
been observed and capture the dependencies between current observations and
future actions well enough, we propose a novel visual-semantic fusion enhanced
and Transformer GRU-based action anticipation framework in this paper. Firstly,
high-level semantic information is introduced to improve the performance of
action anticipation for the first time. We propose to use the semantic features
generated based on the class labels or directly from the visual observations to
augment the original visual features. Secondly, an effective visual-semantic
fusion module is proposed to make up for the semantic gap and fully utilize the
complementarity of different modalities. Thirdly, to take advantage of both the
parallel and autoregressive models, we design a Transformer based encoder for
long-term sequential modeling and a GRU-based decoder for flexible iteration
decoding. Extensive experiments on two large-scale first-person view datasets,
i.e., EPIC-Kitchens and EGTEA Gaze+, validate the effectiveness of our proposed
method, which achieves new state-of-the-art performance, outperforming previous
approaches by a large margin.
</p></li>
</ul>

<h3>Title: Camouflaged Object Detection with Feature Grafting and Distractor Aware. (arXiv:2307.03943v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03943">http://arxiv.org/abs/2307.03943</a></li>
<li>Code URL: <a href="https://github.com/syxvision/fdnet">https://github.com/syxvision/fdnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03943] Camouflaged Object Detection with Feature Grafting and Distractor Aware](http://arxiv.org/abs/2307.03943) #transformer</code></li>
<li>Summary: <p>The task of Camouflaged Object Detection (COD) aims to accurately segment
camouflaged objects that integrated into the environment, which is more
challenging than ordinary detection as the texture between the target and
background is visually indistinguishable. In this paper, we proposed a novel
Feature Grafting and Distractor Aware network (FDNet) to handle the COD task.
Specifically, we use CNN and Transformer to encode multi-scale images in
parallel. In order to better explore the advantages of the two encoders, we
design a cross-attention-based Feature Grafting Module to graft features
extracted from Transformer branch into CNN branch, after which the features are
aggregated in the Feature Fusion Module. A Distractor Aware Module is designed
to explicitly model the two possible distractors in the COD task to refine the
coarse camouflage map. We also proposed the largest artificial camouflaged
object dataset which contains 2000 images with annotations, named ACOD2K. We
conducted extensive experiments on four widely used benchmark datasets and the
ACOD2K dataset. The results show that our method significantly outperforms
other state-of-the-art methods. The code and the ACOD2K will be available at
https://github.com/syxvision/FDNet.
</p></li>
</ul>

<h3>Title: Random Position Adversarial Patch for Vision Transformers. (arXiv:2307.04066v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04066">http://arxiv.org/abs/2307.04066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04066] Random Position Adversarial Patch for Vision Transformers](http://arxiv.org/abs/2307.04066) #transformer</code></li>
<li>Summary: <p>Previous studies have shown the vulnerability of vision transformers to
adversarial patches, but these studies all rely on a critical assumption: the
attack patches must be perfectly aligned with the patches used for linear
projection in vision transformers. Due to this stringent requirement, deploying
adversarial patches for vision transformers in the physical world becomes
impractical, unlike their effectiveness on CNNs. This paper proposes a novel
method for generating an adversarial patch (G-Patch) that overcomes the
alignment constraint, allowing the patch to launch a targeted attack at any
position within the field of view. Specifically, instead of directly optimizing
the patch using gradients, we employ a GAN-like structure to generate the
adversarial patch. Our experiments show the effectiveness of the adversarial
patch in achieving universal attacks on vision transformers, both in digital
and physical-world scenarios. Additionally, further analysis reveals that the
generated adversarial patch exhibits robustness to brightness restriction,
color transfer, and random noise. Real-world attack experiments validate the
effectiveness of the G-Patch to launch robust attacks even under some very
challenging conditions.
</p></li>
</ul>

<h3>Title: Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers. (arXiv:2307.04129v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04129">http://arxiv.org/abs/2307.04129</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04129] Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers](http://arxiv.org/abs/2307.04129) #transformer</code></li>
<li>Summary: <p>This paper addresses the problem of cross-modal object tracking from RGB
videos and event data. Rather than constructing a complex cross-modal fusion
network, we explore the great potential of a pre-trained vision Transformer
(ViT). Particularly, we delicately investigate plug-and-play training
augmentations that encourage the ViT to bridge the vast distribution gap
between the two modalities, enabling comprehensive cross-modal information
interaction and thus enhancing its ability. Specifically, we propose a mask
modeling strategy that randomly masks a specific modality of some tokens to
enforce the interaction between tokens from different modalities interacting
proactively. To mitigate network oscillations resulting from the masking
strategy and further amplify its positive effect, we then theoretically propose
an orthogonal high-rank loss to regularize the attention matrix. Extensive
experiments demonstrate that our plug-and-play training augmentation techniques
can significantly boost state-of-the-art one-stream and twostream trackers to a
large extent in terms of both tracking precision and success rate. Our new
perspective and findings will potentially bring insights to the field of
leveraging powerful pre-trained ViTs to model cross-modal data. The code will
be publicly available.
</p></li>
</ul>

<h3>Title: Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition. (arXiv:2307.04132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04132">http://arxiv.org/abs/2307.04132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04132] Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition](http://arxiv.org/abs/2307.04132) #transformer</code></li>
<li>Summary: <p>In this work, following the intuition that adverbs describing scene-sequences
are best identified by reasoning over high-level concepts of object-behavior,
we propose the design of a new framework that reasons over object-behaviours
extracted from raw-video-clips to recognize the clip's corresponding
adverb-types. Importantly, while previous works for general scene
adverb-recognition assume knowledge of the clips underlying action-types, our
method is directly applicable in the more general problem setting where the
action-type of a video-clip is unknown. Specifically, we propose a novel
pipeline that extracts human-interpretable object-behaviour-facts from raw
video clips and propose novel symbolic and transformer based reasoning methods
that operate over these extracted facts to identify adverb-types. Experiment
results demonstrate that our proposed methods perform favourably against the
previous state-of-the-art. Additionally, to support efforts in symbolic
video-processing, we release two new datasets of object-behaviour-facts
extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
</p></li>
</ul>

<h3>Title: A Survey and Approach to Chart Classification. (arXiv:2307.04147v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04147">http://arxiv.org/abs/2307.04147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04147] A Survey and Approach to Chart Classification](http://arxiv.org/abs/2307.04147) #transformer</code></li>
<li>Summary: <p>Charts represent an essential source of visual information in documents and
facilitate a deep understanding and interpretation of information typically
conveyed numerically. In the scientific literature, there are many charts, each
with its stylistic differences. Recently the document understanding community
has begun to address the problem of automatic chart understanding, which begins
with chart classification. In this paper, we present a survey of the current
state-of-the-art techniques for chart classification and discuss the available
datasets and their supported chart types. We broadly classify these
contributions as traditional approaches based on ML, CNN, and Transformers.
Furthermore, we carry out an extensive comparative performance analysis of
CNN-based and transformer-based approaches on the recently published CHARTINFO
UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The
data set includes 15 different chart categories, including 22,923 training
images and 13,260 test images. We have implemented a vision-based transformer
model that produces state-of-the-art results in chart classification.
</p></li>
</ul>

<h3>Title: SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04192">http://arxiv.org/abs/2307.04192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04192] SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering](http://arxiv.org/abs/2307.04192) #transformer</code></li>
<li>Summary: <p>Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
</p></li>
</ul>

<h3>Title: Bidirectional Attention as a Mixture of Continuous Word Experts. (arXiv:2307.04057v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04057">http://arxiv.org/abs/2307.04057</a></li>
<li>Code URL: <a href="https://github.com/yixinw-lab/attention-uai">https://github.com/yixinw-lab/attention-uai</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04057] Bidirectional Attention as a Mixture of Continuous Word Experts](http://arxiv.org/abs/2307.04057) #transformer</code></li>
<li>Summary: <p>Bidirectional attention $\unicode{x2013}$ composed of self-attention with
positional encodings and the masked language model (MLM) objective
$\unicode{x2013}$ has emerged as a key component of modern large language
models (LLMs). Despite its empirical success, few studies have examined its
statistical underpinnings: What statistical model is bidirectional attention
implicitly fitting? What sets it apart from its non-attention predecessors? We
explore these questions in this paper. The key observation is that fitting a
single-layer single-head bidirectional attention, upon reparameterization, is
equivalent to fitting a continuous bag of words (CBOW) model with
mixture-of-experts (MoE) weights. Further, bidirectional attention with
multiple heads and multiple layers is equivalent to stacked MoEs and a mixture
of MoEs, respectively. This statistical viewpoint reveals the distinct use of
MoE in bidirectional attention, which aligns with its practical effectiveness
in handling heterogeneous data. It also suggests an immediate extension to
categorical tabular data, if we view each word location in a sentence as a
tabular feature. Across empirical studies, we find that this extension
outperforms existing tabular extensions of transformers in out-of-distribution
(OOD) generalization. Finally, this statistical perspective of bidirectional
attention enables us to theoretically characterize when linear word analogies
are present in its word embeddings. These analyses show that bidirectional
attention can require much stronger assumptions to exhibit linear word
analogies than its non-attention predecessors.
</p></li>
</ul>

<h3>Title: inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03854">http://arxiv.org/abs/2307.03854</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03854] inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data](http://arxiv.org/abs/2307.03854) #transformer</code></li>
<li>Summary: <p>The real-time crash likelihood prediction model is an essential component of
the proactive traffic safety management system. Over the years, numerous
studies have attempted to construct a crash likelihood prediction model in
order to enhance traffic safety, but mostly on freeways. In the majority of the
existing studies, researchers have primarily employed a deep learning-based
framework to identify crash potential. Lately, Transformer has emerged as a
potential deep neural network that fundamentally operates through
attention-based mechanisms. Transformer has several functional benefits over
extant deep learning models such as Long Short-Term Memory (LSTM), Convolution
Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term
dependencies in a data sequence. Secondly, Transformer can parallelly process
all elements in a data sequence during training. Finally, Transformer does not
have the vanishing gradient issue. Realizing the immense possibility of
Transformer, this paper proposes inTersection-Transformer (inTformer), a
time-embedded attention-based Transformer model that can effectively predict
intersection crash likelihood in real-time. The proposed model was evaluated
using connected vehicle data extracted from INRIX's Signal Analytics Platform.
The data was parallelly formatted and stacked at different timesteps to develop
nine inTformer models. The best inTformer model achieved a sensitivity of 73%.
This model was also compared to earlier studies on crash likelihood prediction
at intersections and with several established deep learning models trained on
the same connected vehicle dataset. In every scenario, this inTformer
outperformed the benchmark models confirming the viability of the proposed
inTformer architecture.
</p></li>
</ul>

<h3>Title: When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment. (arXiv:2307.03864v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03864">http://arxiv.org/abs/2307.03864</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03864] When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment](http://arxiv.org/abs/2307.03864) #transformer</code></li>
<li>Summary: <p>Reinforcement learning (RL) algorithms face two distinct challenges: learning
effective representations of past and present observations, and determining how
actions influence future returns. Both challenges involve modeling long-term
dependencies. The transformer architecture has been very successful to solve
problems that involve long-term dependencies, including in the RL domain.
However, the underlying reason for the strong performance of Transformer-based
RL methods remains unclear: is it because they learn effective memory, or
because they perform effective credit assignment? After introducing formal
definitions of memory length and credit assignment length, we design simple
configurable tasks to measure these distinct quantities. Our empirical results
reveal that Transformers can enhance the memory capacity of RL algorithms,
scaling up to tasks that require memorizing observations $1500$ steps ago.
However, Transformers do not improve long-term credit assignment. In summary,
our results provide an explanation for the success of Transformers in RL, while
also highlighting an important area for future research and benchmark design.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Synthesizing Forestry Images Conditioned on Plant Phenotype Using a Generative Adversarial Network. (arXiv:2307.03789v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03789">http://arxiv.org/abs/2307.03789</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03789] Synthesizing Forestry Images Conditioned on Plant Phenotype Using a Generative Adversarial Network](http://arxiv.org/abs/2307.03789) #generative</code></li>
<li>Summary: <p>Plant phenology and phenotype prediction using remote sensing data is
increasingly gaining the attention of the plant science community to improve
agricultural productivity. In this work, we generate synthetic forestry images
that satisfy certain phenotypic attributes, viz. canopy greenness. The
greenness index of plants describes a particular vegetation type in a mixed
forest. Our objective is to develop a Generative Adversarial Network (GAN) to
synthesize forestry images conditioned on this continuous attribute, i.e.,
greenness of vegetation, over a specific region of interest. The training data
is based on the automated digital camera imagery provided by the National
Ecological Observatory Network (NEON) and processed by the PhenoCam Network.
The synthetic images generated by our method are also used to predict another
phenotypic attribute, viz., redness of plants. The Structural SIMilarity (SSIM)
index is utilized to assess the quality of the synthetic images. The greenness
and redness indices of the generated synthetic images are compared against that
of the original images using Root Mean Squared Error (RMSE) in order to
evaluate their accuracy and integrity. Moreover, the generalizability and
scalability of our proposed GAN model is determined by effectively transforming
it to generate synthetic images for other forest sites and vegetation types.
</p></li>
</ul>

<h3>Title: Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation. (arXiv:2307.03869v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03869">http://arxiv.org/abs/2307.03869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03869] Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation](http://arxiv.org/abs/2307.03869) #generative</code></li>
<li>Summary: <p>Significant progress has recently been made in creative applications of large
pre-trained models for downstream tasks in 3D vision, such as text-to-shape
generation. This motivates our investigation of how these pre-trained models
can be used effectively to generate 3D shapes from sketches, which has largely
remained an open challenge due to the limited sketch-shape paired datasets and
the varying level of abstraction in the sketches. We discover that conditioning
a 3D generative model on the features (obtained from a frozen large pre-trained
vision model) of synthetic renderings during training enables us to effectively
generate 3D shapes from sketches at inference time. This suggests that the
large pre-trained vision model features carry semantic signals that are
resilient to domain shifts, i.e., allowing us to use only RGB renderings, but
generalizing to sketches at inference time. We conduct a comprehensive set of
experiments investigating different design factors and demonstrate the
effectiveness of our straightforward approach for generation of multiple 3D
shapes per each input sketch regardless of their level of abstraction without
requiring any paired datasets during training.
</p></li>
</ul>

<h3>Title: StyleGAN3: Generative Networks for Improving the Equivariance of Translation and Rotation. (arXiv:2307.03898v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03898">http://arxiv.org/abs/2307.03898</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03898] StyleGAN3: Generative Networks for Improving the Equivariance of Translation and Rotation](http://arxiv.org/abs/2307.03898) #generative</code></li>
<li>Summary: <p>StyleGAN can use style to affect facial posture and identity features, and
noise to affect hair, wrinkles, skin color and other details. Among these, the
outcomes of the picture processing will vary slightly between different
versions of styleGAN. As a result, the comparison of performance differences
between styleGAN2 and the two modified versions of styleGAN3 will be the main
focus of this study. We used the FFHQ dataset as the dataset and FID, EQ-T, and
EQ-R were used to be the assessment of the model. In the end, we discovered
that Stylegan3 version is a better generative network to improve the
equivariance. Our findings have a positive impact on the creation of animation
and videos.
</p></li>
</ul>

<h3>Title: Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04081">http://arxiv.org/abs/2307.04081</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04081] Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance](http://arxiv.org/abs/2307.04081) #generative</code></li>
<li>Summary: <p>Score-based Generative Models (SGMs) are a popular family of deep generative
models that achieves leading image generation quality. Earlier studies have
extended SGMs to tackle class-conditional generation by coupling an
unconditional SGM with the guidance of a trained classifier. Nevertheless, such
classifier-guided SGMs do not always achieve accurate conditional generation,
especially when trained with fewer labeled data. We argue that the issue is
rooted in unreliable gradients of the classifier and the inability to fully
utilize unlabeled data during training. We then propose to improve
classifier-guided SGMs by letting the classifier calibrate itself. Our key idea
is to use principles from energy-based models to convert the classifier as
another view of the unconditional SGM. Then, existing loss for the
unconditional SGM can be adopted to calibrate the classifier using both labeled
and unlabeled data. Empirical results validate that the proposed approach
significantly improves the conditional generation quality across different
percentages of labeled data. The improved performance makes the proposed
approach consistently superior to other conditional SGMs when using fewer
labeled data. The results confirm the potential of the proposed approach for
generative modeling with limited labeled data.
</p></li>
</ul>

<h3>Title: Can Generative Large Language Models Perform ASR Error Correction?. (arXiv:2307.04172v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04172">http://arxiv.org/abs/2307.04172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04172] Can Generative Large Language Models Perform ASR Error Correction?](http://arxiv.org/abs/2307.04172) #generative</code></li>
<li>Summary: <p>ASR error correction continues to serve as an important part of
post-processing for speech recognition systems. Traditionally, these models are
trained with supervised training using the decoding results of the underlying
ASR system and the reference text. This approach is computationally intensive
and the model needs to be re-trained when switching the underlying ASR model.
Recent years have seen the development of large language models and their
ability to perform natural language processing tasks in a zero-shot manner. In
this paper, we take ChatGPT as an example to examine its ability to perform ASR
error correction in the zero-shot or 1-shot settings. We use the ASR N-best
list as model input and propose unconstrained error correction and N-best
constrained error correction methods. Results on a Conformer-Transducer model
and the pre-trained Whisper model show that we can largely improve the ASR
system performance with error correction using the powerful ChatGPT model.
</p></li>
</ul>

<h3>Title: ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04251">http://arxiv.org/abs/2307.04251</a></li>
<li>Code URL: <a href="https://github.com/iamgmujtaba/scholar_search">https://github.com/iamgmujtaba/scholar_search</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04251] ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey](http://arxiv.org/abs/2307.04251) #generative</code></li>
<li>Summary: <p>ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc.
</p></li>
</ul>

<h3>Title: Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks. (arXiv:2307.04065v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04065">http://arxiv.org/abs/2307.04065</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04065] Large-scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks](http://arxiv.org/abs/2307.04065) #generative</code></li>
<li>Summary: <p>We present a non-convex optimization algorithm metaheuristic, based on the
training of a deep generative network, which enables effective searching within
continuous, ultra-high dimensional landscapes. During network training,
populations of sampled local gradients are utilized within a customized loss
function to evolve the network output distribution function towards one peak at
high-performing optima. The deep network architecture is tailored to support
progressive growth over the course of training, which allows the algorithm to
manage the curse of dimensionality characteristic of high-dimensional
landscapes. We apply our concept to a range of standard optimization problems
with dimensions as high as one thousand and show that our method performs
better with fewer function evaluations compared to state-of-the-art algorithm
benchmarks. We also discuss the role of deep network over-parameterization,
loss function engineering, and proper network architecture selection in
optimization, and why the required batch size of sampled local gradients is
independent of problem dimension. These concepts form the foundation for a new
class of algorithms that utilize customizable and expressive deep generative
networks to solve non-convex optimization problems.
</p></li>
</ul>

<h3>Title: Restricted Generative Projection for One-Class Classification and Anomaly Detection. (arXiv:2307.04097v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04097">http://arxiv.org/abs/2307.04097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04097] Restricted Generative Projection for One-Class Classification and Anomaly Detection](http://arxiv.org/abs/2307.04097) #generative</code></li>
<li>Summary: <p>We present a simple framework for one-class classification and anomaly
detection. The core idea is to learn a mapping to transform the unknown
distribution of training (normal) data to a known target distribution.
Crucially, the target distribution should be sufficiently simple, compact, and
informative. The simplicity is to ensure that we can sample from the
distribution easily, the compactness is to ensure that the decision boundary
between normal data and abnormal data is clear and reliable, and the
informativeness is to ensure that the transformed data preserve the important
information of the original data. Therefore, we propose to use truncated
Gaussian, uniform in hypersphere, uniform on hypersphere, or uniform between
hyperspheres, as the target distribution. We then minimize the distance between
the transformed data distribution and the target distribution while keeping the
reconstruction error for the original data small enough. Comparative studies on
multiple benchmark datasets verify the effectiveness of our methods in
comparison to baselines.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models. (arXiv:2307.03762v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03762">http://arxiv.org/abs/2307.03762</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03762] Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models](http://arxiv.org/abs/2307.03762) #large language model</code></li>
<li>Summary: <p>In this perspective paper, we first comprehensively review existing
evaluations of Large Language Models (LLMs) using both standardized tests and
ability-oriented benchmarks. We pinpoint several problems with current
evaluation methods that tend to overstate the capabilities of LLMs. We then
articulate what artificial general intelligence should encompass beyond the
capabilities of LLMs. We propose four characteristics of generally intelligent
agents: 1) they can perform unlimited tasks; 2) they can generate new tasks
within a context; 3) they operate based on a value system that underpins task
generation; and 4) they have a world model reflecting reality, which shapes
their interaction with the world. Building on this viewpoint, we highlight the
missing pieces in artificial general intelligence, that is, the unity of
knowing and acting. We argue that active engagement with objects in the real
world delivers more robust signals for forming conceptual representations.
Additionally, knowledge acquisition isn't solely reliant on passive input but
requires repeated trials and errors. We conclude by outlining promising future
research directions in the field of artificial general intelligence.
</p></li>
</ul>

<h3>Title: Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task. (arXiv:2307.03972v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03972">http://arxiv.org/abs/2307.03972</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03972] Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task](http://arxiv.org/abs/2307.03972) #large language model</code></li>
<li>Summary: <p>Large-scale language models (LLMs) has shown remarkable capability in various
of Natural Language Processing (NLP) tasks and attracted lots of attention
recently. However, some studies indicated that large language models fail to
achieve promising result beyond the state-of-the-art models in English
grammatical error correction (GEC) tasks. In this report, we aim to explore the
how large language models perform on Chinese grammatical error correction tasks
and provide guidance for future work. We conduct experiments with 3 different
LLMs of different model scale on 4 Chinese GEC dataset. Our experimental
results indicate that the performances of LLMs on automatic evaluation metrics
falls short of the previous sota models because of the problem of
over-correction. Furthermore, we also discover notable variations in the
performance of LLMs when evaluated on different data distributions. Our
findings demonstrates that further investigation is required for the
application of LLMs on Chinese GEC task.
</p></li>
</ul>

<h3>Title: A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. (arXiv:2307.03987v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03987">http://arxiv.org/abs/2307.03987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03987] A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation](http://arxiv.org/abs/2307.03987) #large language model</code></li>
<li>Summary: <p>Recently developed large language models have achieved remarkable success in
generating fluent and coherent text. However, these models often tend to
'hallucinate' which critically hampers their reliability. In this work, we
address this crucial problem and propose an approach that actively detects and
mitigates hallucinations during the generation process. Specifically, we first
identify the candidates of potential hallucination leveraging the model's logit
output values, check their correctness through a validation procedure, mitigate
the detected hallucinations, and then continue with the generation process.
Through extensive experiments with the 'article generation task', we first
demonstrate the individual efficacy of our detection and mitigation techniques.
Specifically, the detection technique achieves a recall of 88% and the
mitigation technique successfully mitigates 57.6% of the correctly detected
hallucinations. Importantly, our mitigation technique does not introduce new
hallucinations even in the case of incorrectly detected hallucinations, i.e.,
false positives. Then, we show that the proposed active detection and
mitigation approach successfully reduces the hallucinations of the GPT-3 model
from 47.5% to 14.5% on average. In summary, our work contributes to improving
the reliability and trustworthiness of large language models, a crucial step en
route to enabling their widespread adoption in real-world applications.
</p></li>
</ul>

<h3>Title: Assessing the efficacy of large language models in generating accurate teacher responses. (arXiv:2307.04274v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04274">http://arxiv.org/abs/2307.04274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04274] Assessing the efficacy of large language models in generating accurate teacher responses](http://arxiv.org/abs/2307.04274) #large language model</code></li>
<li>Summary: <p>(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on
Innovative Use of NLP for Building Educational Applications on generation of
teacher language in educational dialogues. Following the structure of the
shared task, in this study, we attempt to assess the generative abilities of
large language models in providing informative and helpful insights to
students, thereby simulating the role of a knowledgeable teacher. To this end,
we present an extensive evaluation of several benchmarking generative models,
including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and
fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we
fine-tuned the Flan-T5 model using reinforcement learning. Our experimental
findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of
GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.
</p></li>
</ul>

<p>We hypothesize that several dataset characteristics, including sampling,
representativeness, and dialog completeness, pose significant challenges to
fine-tuning, thus contributing to the poor generalizability of the fine-tuned
models. Finally, we note the need for these generative models to be evaluated
with a metric that relies not only on dialog coherence and matched language
modeling distribution but also on the model's ability to showcase pedagogical
skills.
</p>

<h2>segmentation</h2>
<h3>Title: Edge-Aware Mirror Network for Camouflaged Object Detection. (arXiv:2307.03932v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03932">http://arxiv.org/abs/2307.03932</a></li>
<li>Code URL: <a href="https://github.com/sdy1999/eamnet">https://github.com/sdy1999/eamnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03932] Edge-Aware Mirror Network for Camouflaged Object Detection](http://arxiv.org/abs/2307.03932) #segmentation</code></li>
<li>Summary: <p>Existing edge-aware camouflaged object detection (COD) methods normally
output the edge prediction in the early stage. However, edges are important and
fundamental factors in the following segmentation task. Due to the high visual
similarity between camouflaged targets and the surroundings, edge prior
predicted in early stage usually introduces erroneous foreground-background and
contaminates features for segmentation. To tackle this problem, we propose a
novel Edge-aware Mirror Network (EAMNet), which models edge detection and
camouflaged object segmentation as a cross refinement process. More
specifically, EAMNet has a two-branch architecture, where a
segmentation-induced edge aggregation module and an edge-induced integrity
aggregation module are designed to cross-guide the segmentation branch and edge
detection branch. A guided-residual channel attention module which leverages
the residual connection and gated convolution finally better extracts
structural details from low-level features. Quantitative and qualitative
experiment results show that EAMNet outperforms existing cutting-edge baselines
on three widely used COD datasets. Codes are available at
https://github.com/sdy1999/EAMNet.
</p></li>
</ul>

<h3>Title: Building and Road Segmentation Using EffUNet and Transfer Learning Approach. (arXiv:2307.03980v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03980">http://arxiv.org/abs/2307.03980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03980] Building and Road Segmentation Using EffUNet and Transfer Learning Approach](http://arxiv.org/abs/2307.03980) #segmentation</code></li>
<li>Summary: <p>In city, information about urban objects such as water supply, railway lines,
power lines, buildings, roads, etc., is necessary for city planning. In
particular, information about the spread of these objects, locations and
capacity is needed for the policymakers to make impactful decisions. This
thesis aims to segment the building and roads from the aerial image captured by
the satellites and UAVs. Many different architectures have been proposed for
the semantic segmentation task and UNet being one of them. In this thesis, we
propose a novel architecture based on Google's newly proposed EfficientNetV2 as
an encoder for feature extraction with UNet decoder for constructing the
segmentation map. Using this approach we achieved a benchmark score for the
Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153
respectively.
</p></li>
</ul>

<h3>Title: BPNet: B\'ezier Primitive Segmentation on 3D Point Clouds. (arXiv:2307.04013v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04013">http://arxiv.org/abs/2307.04013</a></li>
<li>Code URL: <a href="https://github.com/bizerfr/bpnet">https://github.com/bizerfr/bpnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04013] BPNet: B\'ezier Primitive Segmentation on 3D Point Clouds](http://arxiv.org/abs/2307.04013) #segmentation</code></li>
<li>Summary: <p>This paper proposes BPNet, a novel end-to-end deep learning framework to
learn B\'ezier primitive segmentation on 3D point clouds. The existing works
treat different primitive types separately, thus limiting them to finite shape
categories. To address this issue, we seek a generalized primitive segmentation
on point clouds. Taking inspiration from B\'ezier decomposition on NURBS
models, we transfer it to guide point cloud segmentation casting off primitive
types. A joint optimization framework is proposed to learn B\'ezier primitive
segmentation and geometric fitting simultaneously on a cascaded architecture.
Specifically, we introduce a soft voting regularizer to improve primitive
segmentation and propose an auto-weight embedding module to cluster point
features, making the network more robust and generic. We also introduce a
reconstruction module where we successfully process multiple CAD models with
different primitives simultaneously. We conducted extensive experiments on the
synthetic ABC dataset and real-scan datasets to validate and compare our
approach with different baseline methods. Experiments show superior performance
over previous work in terms of segmentation, with a substantially faster
inference speed.
</p></li>
</ul>

<h3>Title: CMDFusion: Bidirectional Fusion Network with Cross-modality Knowledge Distillation for LIDAR Semantic Segmentation. (arXiv:2307.04091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04091">http://arxiv.org/abs/2307.04091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04091] CMDFusion: Bidirectional Fusion Network with Cross-modality Knowledge Distillation for LIDAR Semantic Segmentation](http://arxiv.org/abs/2307.04091) #segmentation</code></li>
<li>Summary: <p>2D RGB images and 3D LIDAR point clouds provide complementary knowledge for
the perception system of autonomous vehicles. Several 2D and 3D fusion methods
have been explored for the LIDAR semantic segmentation task, but they suffer
from different problems. 2D-to-3D fusion methods require strictly paired data
during inference, which may not be available in real-world scenarios, while
3D-to-2D fusion methods cannot explicitly make full use of the 2D information.
Therefore, we propose a Bidirectional Fusion Network with Cross-Modality
Knowledge Distillation (CMDFusion) in this work. Our method has two
contributions. First, our bidirectional fusion scheme explicitly and implicitly
enhances the 3D feature via 2D-to-3D fusion and 3D-to-2D fusion, respectively,
which surpasses either one of the single fusion schemes. Second, we distillate
the 2D knowledge from a 2D network (Camera branch) to a 3D network (2D
knowledge branch) so that the 3D network can generate 2D information even for
those points not in the FOV (field of view) of the camera. In this way, RGB
images are not required during inference anymore since the 2D knowledge branch
provides 2D information according to the 3D LIDAR input. We show that our
CMDFusion achieves the best performance among all fusion-based methods on
SemanticKITTI and nuScenes datasets. The code will be released at
https://github.com/Jun-CEN/CMDFusion.
</p></li>
</ul>

<h3>Title: Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets. (arXiv:2307.04101v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04101">http://arxiv.org/abs/2307.04101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04101] Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets](http://arxiv.org/abs/2307.04101) #segmentation</code></li>
<li>Summary: <p>The development of remote sensing and deep learning techniques has enabled
building semantic segmentation with high accuracy and efficiency. Despite their
success in different tasks, the discussions on the impact of spatial resolution
on deep learning based building semantic segmentation are quite inadequate,
which makes choosing a higher cost-effective data source a big challenge. To
address the issue mentioned above, in this study, we create remote sensing
images among three study areas into multiple spatial resolutions by
super-resolution and down-sampling. After that, two representative deep
learning architectures: UNet and FPN, are selected for model training and
testing. The experimental results obtained from three cities with two deep
learning models indicate that the spatial resolution greatly influences
building segmentation results, and with a better cost-effectiveness around
0.3m, which we believe will be an important insight for data selection and
preparation.
</p></li>
</ul>

<h3>Title: Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird's Eye View. (arXiv:2307.04106v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04106">http://arxiv.org/abs/2307.04106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04106] Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird's Eye View](http://arxiv.org/abs/2307.04106) #segmentation</code></li>
<li>Summary: <p>Recent vision-only perception models for autonomous driving achieved
promising results by encoding multi-view image features into Bird's-Eye-View
(BEV) space. A critical step and the main bottleneck of these methods is
transforming image features into the BEV coordinate frame. This paper focuses
on leveraging geometry information, such as depth, to model such feature
transformation. Existing works rely on non-parametric depth distribution
modeling leading to significant memory consumption, or ignore the geometry
information to address this problem. In contrast, we propose to use parametric
depth distribution modeling for feature transformation. We first lift the 2D
image features to the 3D space defined for the ego vehicle via a predicted
parametric depth distribution for each pixel in each view. Then, we aggregate
the 3D feature volume based on the 3D space occupancy derived from depth to the
BEV frame. Finally, we use the transformed features for downstream tasks such
as object detection and semantic segmentation. Existing semantic segmentation
methods do also suffer from an hallucination problem as they do not take
visibility information into account. This hallucination can be particularly
problematic for subsequent modules such as control and planning. To mitigate
the issue, our method provides depth uncertainty and reliable visibility-aware
estimations. We further leverage our parametric depth modeling to present a
novel visibility-aware evaluation metric that, when taken into account, can
mitigate the hallucination problem. Extensive experiments on object detection
and semantic segmentation on the nuScenes datasets demonstrate that our method
outperforms existing methods on both tasks.
</p></li>
</ul>

<h3>Title: A Novel Explainable Artificial Intelligence Model in Image Classification problem. (arXiv:2307.04137v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04137">http://arxiv.org/abs/2307.04137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04137] A Novel Explainable Artificial Intelligence Model in Image Classification problem](http://arxiv.org/abs/2307.04137) #segmentation</code></li>
<li>Summary: <p>In recent years, artificial intelligence is increasingly being applied widely
in many different fields and has a profound and direct impact on human life.
Following this is the need to understand the principles of the model making
predictions. Since most of the current high-precision models are black boxes,
neither the AI scientist nor the end-user deeply understands what's going on
inside these models. Therefore, many algorithms are studied for the purpose of
explaining AI models, especially those in the problem of image classification
in the field of computer vision such as LIME, CAM, GradCAM. However, these
algorithms still have limitations such as LIME's long execution time and CAM's
confusing interpretation of concreteness and clarity. Therefore, in this paper,
we propose a new method called Segmentation - Class Activation Mapping (SeCAM)
that combines the advantages of these algorithms above, while at the same time
overcoming their disadvantages. We tested this algorithm with various models,
including ResNet50, Inception-v3, VGG16 from ImageNet Large Scale Visual
Recognition Challenge (ILSVRC) data set. Outstanding results when the algorithm
has met all the requirements for a specific explanation in a remarkably concise
time.
</p></li>
</ul>

<h3>Title: Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04149">http://arxiv.org/abs/2307.04149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04149] Latent Graph Attention for Enhanced Spatial Context](http://arxiv.org/abs/2307.04149) #segmentation</code></li>
<li>Summary: <p>Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
</p></li>
</ul>

<h3>Title: Mx2M: Masked Cross-Modality Modeling in Domain Adaptation for 3D Semantic Segmentation. (arXiv:2307.04231v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04231">http://arxiv.org/abs/2307.04231</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04231] Mx2M: Masked Cross-Modality Modeling in Domain Adaptation for 3D Semantic Segmentation](http://arxiv.org/abs/2307.04231) #segmentation</code></li>
<li>Summary: <p>Existing methods of cross-modal domain adaptation for 3D semantic
segmentation predict results only via 2D-3D complementarity that is obtained by
cross-modal feature matching. However, as lacking supervision in the target
domain, the complementarity is not always reliable. The results are not ideal
when the domain gap is large. To solve the problem of lacking supervision, we
introduce masked modeling into this task and propose a method Mx2M, which
utilizes masked cross-modality modeling to reduce the large domain gap. Our
Mx2M contains two components. One is the core solution, cross-modal removal and
prediction (xMRP), which makes the Mx2M adapt to various scenarios and provides
cross-modal self-supervision. The other is a new way of cross-modal feature
matching, the dynamic cross-modal filter (DxMF) that ensures the whole method
dynamically uses more suitable 2D-3D complementarity. Evaluation of the Mx2M on
three DA scenarios, including Day/Night, USA/Singapore, and A2D2/SemanticKITTI,
brings large improvements over previous methods on many metrics.
</p></li>
</ul>

<h3>Title: Convex Decomposition of Indoor Scenes. (arXiv:2307.04246v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04246">http://arxiv.org/abs/2307.04246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04246] Convex Decomposition of Indoor Scenes](http://arxiv.org/abs/2307.04246) #segmentation</code></li>
<li>Summary: <p>We describe a method to parse a complex, cluttered indoor scene into
primitives which offer a parsimonious abstraction of scene structure. Our
primitives are simple convexes. Our method uses a learned regression procedure
to parse a scene into a fixed number of convexes from RGBD input, and can
optionally accept segmentations to improve the decomposition. The result is
then polished with a descent method which adjusts the convexes to produce a
very good fit, and greedily removes superfluous primitives. Because the entire
scene is parsed, we can evaluate using traditional depth, normal, and
segmentation error metrics. Our evaluation procedure demonstrates that the
error from our primitive representation is comparable to that of predicting
depth from a single image.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
