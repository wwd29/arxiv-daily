<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-20</h1>
<h3>Title: Nteasee: A mixed methods study of expert and general population perspectives on deploying AI for health in African countries</h3>
<ul>
<li><strong>Authors: </strong>Mercy Nyamewaa Asiedu, Iskandar Haykel, Awa Dieng, Kerrie Kauer, Tousif Ahmed, Florence Ofori, Charisma Chan, Stephen Pfohl, Negar Rostamzadeh, Katherine Heller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12197">https://arxiv.org/abs/2409.12197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12197">https://arxiv.org/pdf/2409.12197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12197]] Nteasee: A mixed methods study of expert and general population perspectives on deploying AI for health in African countries(https://arxiv.org/abs/2409.12197)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) for health has the potential to significantly change and improve healthcare. However in most African countries, identifying culturally and contextually attuned approaches for deploying these solutions is not well understood. To bridge this gap, we conduct a qualitative study to investigate the best practices, fairness indicators, and potential biases to mitigate when deploying AI for health in African countries, as well as explore opportunities where artificial intelligence could make a positive impact in health. We used a mixed methods approach combining in-depth interviews (IDIs) and surveys. We conduct 1.5-2 hour long IDIs with 50 experts in health, policy, and AI across 17 countries, and through an inductive approach we conduct a qualitative thematic analysis on expert IDI responses. We administer a blinded 30-minute survey with case studies to 672 general population participants across 5 countries in Africa and analyze responses on quantitative scales, statistically comparing responses by country, age, gender, and level of familiarity with AI. We thematically summarize open-ended responses from surveys. Our results find generally positive attitudes, high levels of trust, accompanied by moderate levels of concern among general population participants for AI usage for health in Africa. This contrasts with expert responses, where major themes revolved around trust/mistrust, ethical concerns, and systemic barriers to integration, among others. This work presents the first-of-its-kind qualitative research study of the potential of AI for health in Africa from an algorithmic fairness angle, with perspectives from both experts and the general population. We hope that this work guides policymakers and drives home the need for further research and the inclusion of general population perspectives in decision-making around AI usage.</li>
</ul>

<h3>Title: ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video</h3>
<ul>
<li><strong>Authors: </strong>Han Ling, Yinghui Sun, Quansen Sun, Yuhui Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12202">https://arxiv.org/abs/2409.12202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12202">https://arxiv.org/pdf/2409.12202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12202]] ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video(https://arxiv.org/abs/2409.12202)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Perceiving and understanding 3D motion is a core technology in fields such as autonomous driving, robots, and motion prediction. This paper proposes a 3D motion perception method called ScaleFlow++ that is easy to generalize. With just a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and motion-in-depth (MID). Most existing methods directly regress MID from two RGB frames or optical flow, resulting in inaccurate and unstable results. Our key insight is cross-scale matching, which extracts deep motion clues by matching objects in pairs of images at different scales. Unlike previous methods, ScaleFlow++ integrates optical flow and MID estimation into a unified architecture, estimating optical flow and MID end-to-end based on feature matching. Moreover, we also proposed modules such as global initialization network, global iterative optimizer, and hybrid training pipeline to integrate global motion information, reduce the number of iterations, and prevent overfitting during training. On KITTI, ScaleFlow++ achieved the best monocular scene flow estimation performance, reducing SF-all from 6.21 to 5.79. The evaluation of MID even surpasses RGBD-based methods. In addition, ScaleFlow++ has achieved stunning zero-shot generalization performance in both rigid and nonrigid scenes. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Mixture of Diverse Size Experts</h3>
<ul>
<li><strong>Authors: </strong>Manxi Sun, Wei Liu, Jian Luan, Pengzhi Gao, Bin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12210">https://arxiv.org/abs/2409.12210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12210">https://arxiv.org/pdf/2409.12210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12210]] Mixture of Diverse Size Experts(https://arxiv.org/abs/2409.12210)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Sparsely-Activated Mixture-of-Experts (MoE) has gained increasing popularity for scaling up large language models (LLMs) without exploding computational costs. Despite its success, the current design faces a challenge where all experts have the same size, limiting the ability of tokens to choose the experts with the most appropriate size for generating the next token. In this paper, we propose the Mixture of Diverse Size Experts (MoDSE), a new MoE architecture with layers designed to have experts of different sizes. Our analysis of difficult token generation tasks shows that experts of various sizes achieve better predictions, and the routing path of the experts tends to be stable after a training period. However, having experts of diverse sizes can lead to uneven workload distribution. To tackle this limitation, we introduce an expert-pair allocation strategy to evenly distribute the workload across multiple GPUs. Comprehensive evaluations across multiple benchmarks demonstrate the effectiveness of MoDSE, as it outperforms existing MoEs by allocating the parameter budget to experts adaptively while maintaining the same total parameter size and the number of experts.</li>
</ul>

<h3>Title: SemAI: Semantic Artificial Intelligence-enhanced DNA storage for Internet-of-Things</h3>
<ul>
<li><strong>Authors: </strong>Wenfeng Wu, Luping Xiang, Qiang Liu, Kun Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12213">https://arxiv.org/abs/2409.12213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12213">https://arxiv.org/pdf/2409.12213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12213]] SemAI: Semantic Artificial Intelligence-enhanced DNA storage for Internet-of-Things(https://arxiv.org/abs/2409.12213)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In the wake of the swift evolution of technologies such as the Internet of Things (IoT), the global data landscape undergoes an exponential surge, propelling DNA storage into the spotlight as a prospective medium for contemporary cloud storage applications. This paper introduces a Semantic Artificial Intelligence-enhanced DNA storage (SemAI-DNA) paradigm, distinguishing itself from prevalent deep learning-based methodologies through two key modifications: 1) embedding a semantic extraction module at the encoding terminus, facilitating the meticulous encoding and storage of nuanced semantic information; 2) conceiving a forethoughtful multi-reads filtering model at the decoding terminus, leveraging the inherent multi-copy propensity of DNA molecules to bolster system fault tolerance, coupled with a strategically optimized decoder's architectural framework. Numerical results demonstrate the SemAI-DNA's efficacy, attaining 2.61 dB Peak Signal-to-Noise Ratio (PSNR) gain and 0.13 improvement in Structural Similarity Index (SSIM) over conventional deep learning-based approaches.</li>
</ul>

<h3>Title: ARTICLE: Annotator Reliability Through In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Sujan Dutta, Deepak Pandita, Tharindu Cyril Weerasooriya, Marcos Zampieri, Christopher M. Homan, Ashiqur R. KhudaBukhsh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12218">https://arxiv.org/abs/2409.12218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12218">https://arxiv.org/pdf/2409.12218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12218]] ARTICLE: Annotator Reliability Through In-Context Learning(https://arxiv.org/abs/2409.12218)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ensuring annotator quality in training and evaluation data is a key piece of machine learning in NLP. Tasks such as sentiment analysis and offensive speech detection are intrinsically subjective, creating a challenging scenario for traditional quality assessment approaches because it is hard to distinguish disagreement due to poor work from that due to differences of opinions between sincere annotators. With the goal of increasing diverse perspectives in annotation while ensuring consistency, we propose \texttt{ARTICLE}, an in-context learning (ICL) framework to estimate annotation quality through self-consistency. We evaluate this framework on two offensive speech datasets using multiple LLMs and compare its performance with traditional methods. Our findings indicate that \texttt{ARTICLE} can be used as a robust method for identifying reliable annotators, hence improving data quality.</li>
</ul>

<h3>Title: Sparks of Artificial General Intelligence(AGI) in Semiconductor Material Science: Early Explorations into the Next Frontier of Generative AI-Assisted Electron Micrograph Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sakhinana Sagar Srinivas, Geethan Sannidhi, Sreeja Gangasani, Chidaksh Ravuru, Venkataramana Runkana</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12244">https://arxiv.org/abs/2409.12244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12244">https://arxiv.org/pdf/2409.12244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12244]] Sparks of Artificial General Intelligence(AGI) in Semiconductor Material Science: Early Explorations into the Next Frontier of Generative AI-Assisted Electron Micrograph Analysis(https://arxiv.org/abs/2409.12244)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Characterizing materials with electron micrographs poses significant challenges for automated labeling due to the complex nature of nanomaterial structures. To address this, we introduce a fully automated, end-to-end pipeline that leverages recent advances in Generative AI. It is designed for analyzing and understanding the microstructures of semiconductor materials with effectiveness comparable to that of human experts, contributing to the pursuit of Artificial General Intelligence (AGI) in nanomaterial identification. Our approach utilizes Large MultiModal Models (LMMs) such as GPT-4V, alongside text-to-image models like DALLE-3. We integrate a GPT-4 guided Visual Question Answering (VQA) method to analyze nanomaterial images, generate synthetic nanomaterial images via DALLE-3, and employ in-context learning with few-shot prompting in GPT-4V for accurate nanomaterial identification. Our method surpasses traditional techniques by enhancing the precision of nanomaterial identification and optimizing the process for high-throughput screening.</li>
</ul>

<h3>Title: MQA-KEAL: Multi-hop Question Answering under Knowledge Editing for Arabic Language</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Asif Ali, Nawal Daftardar, Mutayyaba Waheed, Jianbin Qin, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12257">https://arxiv.org/abs/2409.12257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12257">https://arxiv.org/pdf/2409.12257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12257]] MQA-KEAL: Multi-hop Question Answering under Knowledge Editing for Arabic Language(https://arxiv.org/abs/2409.12257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant capabilities across numerous application domains. A key challenge is to keep these models updated with latest available information, which limits the true potential of these models for the end-applications. Although, there have been numerous attempts for LLMs Knowledge Editing (KE), i.e., to edit the LLMs prior knowledge and in turn test it via Multi-hop Question Answering (MQA), yet so far these studies are primarily focused on English language. To bridge this gap, in this paper we propose: Multi-hop Questioning Answering under Knowledge Editing for Arabic Language (MQA-KEAL). MQA-KEAL stores knowledge edits as structured knowledge units in the external memory. In order to solve multi-hop question, it first uses task-decomposition to decompose the question into smaller sub-problems. Later for each sub-problem, it iteratively queries the external memory and/or target LLM in order to generate the final response. In addition, we also contribute MQUAKE-AR (Arabic translation of English benchmark MQUAKE), as well as a new benchmark MQA-AEVAL for rigorous performance evaluation of MQA under KE for Arabic language. Experimentation evaluation reveals MQA-KEAL outperforms the baseline models by a significant margin.</li>
</ul>

<h3>Title: WiLoR: End-to-end 3D Hand Localization and Reconstruction in-the-wild</h3>
<ul>
<li><strong>Authors: </strong>Rolandos Alexandros Potamias, Jinglei Zhang, Jiankang Deng, Stefanos Zafeiriou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12259">https://arxiv.org/abs/2409.12259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12259">https://arxiv.org/pdf/2409.12259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12259]] WiLoR: End-to-end 3D Hand Localization and Reconstruction in-the-wild(https://arxiv.org/abs/2409.12259)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In recent years, 3D hand pose estimation methods have garnered significant attention due to their extensive applications in human-computer interaction, virtual reality, and robotics. In contrast, there has been a notable gap in hand detection pipelines, posing significant challenges in constructing effective real-world multi-hand reconstruction systems. In this work, we present a data-driven pipeline for efficient multi-hand reconstruction in the wild. The proposed pipeline is composed of two components: a real-time fully convolutional hand localization and a high-fidelity transformer-based 3D hand reconstruction model. To tackle the limitations of previous methods and build a robust and stable detection network, we introduce a large-scale dataset with over than 2M in-the-wild hand images with diverse lighting, illumination, and occlusion conditions. Our approach outperforms previous methods in both efficiency and accuracy on popular 2D and 3D benchmarks. Finally, we showcase the effectiveness of our pipeline to achieve smooth 3D hand tracking from monocular videos, without utilizing any temporal components. Code, models, and dataset are available this https URL.</li>
</ul>

<h3>Title: Detecting LGBTQ+ Instances of Cyberbullying</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Arslan, Manuel Sandoval Madrigal, Mohammed Abuhamad, Deborah L. Hall, Yasin N. Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12263">https://arxiv.org/abs/2409.12263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12263">https://arxiv.org/pdf/2409.12263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12263]] Detecting LGBTQ+ Instances of Cyberbullying(https://arxiv.org/abs/2409.12263)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Social media continues to have an impact on the trajectory of humanity. However, its introduction has also weaponized keyboards, allowing the abusive language normally reserved for in-person bullying to jump onto the screen, i.e., cyberbullying. Cyberbullying poses a significant threat to adolescents globally, affecting the mental health and well-being of many. A group that is particularly at risk is the LGBTQ+ community, as researchers have uncovered a strong correlation between identifying as LGBTQ+ and suffering from greater online harassment. Therefore, it is critical to develop machine learning models that can accurately discern cyberbullying incidents as they happen to LGBTQ+ members. The aim of this study is to compare the efficacy of several transformer models in identifying cyberbullying targeting LGBTQ+ individuals. We seek to determine the relative merits and demerits of these existing methods in addressing complex and subtle kinds of cyberbullying by assessing their effectiveness with real social media data.</li>
</ul>

<h3>Title: Mastering Chess with a Transformer Model</h3>
<ul>
<li><strong>Authors: </strong>Daniel Monroe, The Leela Chess Zero Team</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12272">https://arxiv.org/abs/2409.12272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12272">https://arxiv.org/pdf/2409.12272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12272]] Mastering Chess with a Transformer Model(https://arxiv.org/abs/2409.12272)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models have demonstrated impressive capabilities when trained at scale, excelling at difficult cognitive tasks requiring complex reasoning and rational decision-making. In this paper, we explore the application of transformer models to chess, focusing on the critical role of the position encoding within the attention mechanism. We show that in chess, transformers endowed with a sufficiently versatile position encoding can match existing chess-playing models at a fraction of the computational cost. Our architecture significantly outperforms AlphaZero at 8x fewer FLOPS and matches prior grandmaster-level transformer-based agents at 30x fewer FLOPS.</li>
</ul>

<h3>Title: Making Large Language Models into World Models with Precondition and Effect Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Kaige Xie, Ian Yang, John Gunerli, Mark Riedl</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12278">https://arxiv.org/abs/2409.12278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12278">https://arxiv.org/pdf/2409.12278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12278]] Making Large Language Models into World Models with Precondition and Effect Knowledge(https://arxiv.org/abs/2409.12278)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>World models, which encapsulate the dynamics of how actions affect environments, are foundational to the functioning of intelligent agents. In this work, we explore the potential of Large Language Models (LLMs) to operate as world models. Although LLMs are not inherently designed to model real-world dynamics, we show that they can be induced to perform two critical world model functions: determining the applicability of an action based on a given world state, and predicting the resulting world state upon action execution. This is achieved by fine-tuning two separate LLMs-one for precondition prediction and another for effect prediction-while leveraging synthetic data generation techniques. Through human-participant studies, we validate that the precondition and effect knowledge generated by our models aligns with human understanding of world dynamics. We also analyze the extent to which the world model trained on our synthetic data results in an inferred state space that supports the creation of action chains, a necessary property for planning.</li>
</ul>

<h3>Title: MetaPix: A Data-Centric AI Development Platform for Efficient Management and Utilization of Unstructured Computer Vision Data</h3>
<ul>
<li><strong>Authors: </strong>Sai Vishwanath Venkatesh, Atra Akandeh, Madhu Lokanath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12289">https://arxiv.org/abs/2409.12289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12289">https://arxiv.org/pdf/2409.12289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12289]] MetaPix: A Data-Centric AI Development Platform for Efficient Management and Utilization of Unstructured Computer Vision Data(https://arxiv.org/abs/2409.12289)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In today's world of advanced AI technologies, data management is a critical component of any AI/ML solution. Effective data management is vital for the creation and maintenance of high-quality, diverse datasets, which significantly enhance predictive capabilities and lead to smarter business solutions. In this work, we introduce MetaPix, a Data-centric AI platform offering comprehensive data management solutions specifically designed for unstructured data. MetaPix offers robust tools for data ingestion, processing, storage, versioning, governance, and discovery. The platform operates on four key concepts: DataSources, Datasets, Extensions and Extractors. A DataSource serves as MetaPix top level asset, representing a narrow-scoped source of data for a specific use. Datasets are MetaPix second level object, structured collections of data. Extractors are internal tools integrated into MetaPix's backend processing, facilitate data processing and enhancement. Additionally, MetaPix supports extensions, enabling integration with external third-party tools to enhance platform functionality. This paper delves into each MetaPix concept in detail, illustrating how they collectively contribute to the platform's objectives. By providing a comprehensive solution for managing and utilizing unstructured computer vision data, MetaPix equips organizations with a powerful toolset to develop AI applications effectively.</li>
</ul>

<h3>Title: Provable In-Context Learning of Linear Systems and Linear Elliptic PDEs with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Frank Cole, Yulong Lu, Riley O'Neill, Tianhao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12293">https://arxiv.org/abs/2409.12293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12293">https://arxiv.org/pdf/2409.12293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12293]] Provable In-Context Learning of Linear Systems and Linear Elliptic PDEs with Transformers(https://arxiv.org/abs/2409.12293)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Foundation models for natural language processing, powered by the transformer architecture, exhibit remarkable in-context learning (ICL) capabilities, allowing pre-trained models to adapt to downstream tasks using few-shot prompts without updating their weights. Recently, transformer-based foundation models have also emerged as versatile tools for solving scientific problems, particularly in the realm of partial differential equations (PDEs). However, the theoretical foundations of the ICL capabilities in these scientific models remain largely unexplored. This work develops a rigorous error analysis for transformer-based ICL applied to solution operators associated with a family of linear elliptic PDEs. We first demonstrate that a linear transformer, defined by a linear self-attention layer, can provably learn in-context to invert linear systems arising from the spatial discretization of PDEs. This is achieved by deriving theoretical scaling laws for the prediction risk of the proposed linear transformers in terms of spatial discretization size, the number of training tasks, and the lengths of prompts used during training and inference. These scaling laws also enable us to establish quantitative error bounds for learning PDE solutions. Furthermore, we quantify the adaptability of the pre-trained transformer on downstream PDE tasks that experience distribution shifts in both tasks (represented by PDE coefficients) and input covariates (represented by the source term). To analyze task distribution shifts, we introduce a novel concept of task diversity and characterize the transformer's prediction error in terms of the magnitude of task shift, assuming sufficient diversity in the pre-training tasks. We also establish sufficient conditions to ensure task diversity. Finally, we validate the ICL-capabilities of transformers through extensive numerical experiments.</li>
</ul>

<h3>Title: Self-Supervised Pre-training Tasks for an fMRI Time-series Transformer in Autism Detection</h3>
<ul>
<li><strong>Authors: </strong>Yinchi Zhou, Peiyu Duan, Yuexi Du, Nicha C. Dvornek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12304">https://arxiv.org/abs/2409.12304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12304">https://arxiv.org/pdf/2409.12304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12304]] Self-Supervised Pre-training Tasks for an fMRI Time-series Transformer in Autism Detection(https://arxiv.org/abs/2409.12304)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Autism Spectrum Disorder (ASD) is a neurodevelopmental condition that encompasses a wide variety of symptoms and degrees of impairment, which makes the diagnosis and treatment challenging. Functional magnetic resonance imaging (fMRI) has been extensively used to study brain activity in ASD, and machine learning methods have been applied to analyze resting state fMRI (rs-fMRI) data. However, fewer studies have explored the recent transformer-based models on rs-fMRI data. Given the superiority of transformer models in capturing long-range dependencies in sequence data, we have developed a transformer-based self-supervised framework that directly analyzes time-series fMRI data without computing functional connectivity. To address over-fitting in small datasets and enhance the model performance, we propose self-supervised pre-training tasks to reconstruct the randomly masked fMRI time-series data, investigating the effects of various masking strategies. We then finetune the model for the ASD classification task and evaluate it using two public datasets and five-fold cross-validation with different amounts of training data. The experiments show that randomly masking entire ROIs gives better model performance than randomly masking time points in the pre-training step, resulting in an average improvement of 10.8% for AUC and 9.3% for subject accuracy compared with the transformer model trained from scratch across different levels of training data availability. Our code is available on GitHub.</li>
</ul>

<h3>Title: Understanding Implosion in Text-to-Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Wenxin Ding, Cathy Y. Li, Shawn Shan, Ben Y. Zhao, Haitao Zheng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12314">https://arxiv.org/abs/2409.12314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12314">https://arxiv.org/pdf/2409.12314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12314]] Understanding Implosion in Text-to-Image Generative Models(https://arxiv.org/abs/2409.12314)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent works show that text-to-image generative models are surprisingly vulnerable to a variety of poisoning attacks. Empirical results find that these models can be corrupted by altering associations between individual text prompts and associated visual features. Furthermore, a number of concurrent poisoning attacks can induce "model implosion," where the model becomes unable to produce meaningful images for unpoisoned prompts. These intriguing findings highlight the absence of an intuitive framework to understand poisoning attacks on these models. In this work, we establish the first analytical framework on robustness of image generative models to poisoning attacks, by modeling and analyzing the behavior of the cross-attention mechanism in latent diffusion models. We model cross-attention training as an abstract problem of "supervised graph alignment" and formally quantify the impact of training data by the hardness of alignment, measured by an Alignment Difficulty (AD) metric. The higher the AD, the harder the alignment. We prove that AD increases with the number of individual prompts (or concepts) poisoned. As AD grows, the alignment task becomes increasingly difficult, yielding highly distorted outcomes that frequently map meaningful text prompts to undefined or meaningless visual representations. As a result, the generative model implodes and outputs random, incoherent images at large. We validate our analytical framework through extensive experiments, and we confirm and explain the unexpected (and unexplained) effect of model implosion while producing new, unforeseen insights. Our work provides a useful tool for studying poisoning attacks against diffusion models and their defenses.</li>
</ul>

<h3>Title: A large-scale study of performance and equity of commercial remote identity verification technologies across demographics</h3>
<ul>
<li><strong>Authors: </strong>Kaniz Fatima, Michael Schuckers, Gerardo Cruz-Ortiz, Daqing Hou, Sandip Purnapatra, Tiffany Andrews, Ambuj Neupane, Brandeis Marshall, Stephanie Schuckers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12318">https://arxiv.org/abs/2409.12318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12318">https://arxiv.org/pdf/2409.12318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12318]] A large-scale study of performance and equity of commercial remote identity verification technologies across demographics(https://arxiv.org/abs/2409.12318)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric, fair</a></li>
<li><strong>Abstract: </strong>As more types of transactions move online, there is an increasing need to verify someone's identity remotely. Remote identity verification (RIdV) technologies have emerged to fill this need. RIdV solutions typically use a smart device to validate an identity document like a driver's license by comparing a face selfie to the face photo on the document. Recent research has been focused on ensuring that biometric systems work fairly across demographic groups. This study assesses five commercial RIdV solutions for equity across age, gender, race/ethnicity, and skin tone across 3,991 test subjects. This paper employs statistical methods to discern whether the RIdV result across demographic groups is statistically distinguishable. Two of the RIdV solutions were equitable across all demographics, while two RIdV solutions had at least one demographic that was inequitable. For example, the results for one technology had a false negative rate of 10.5% +/- 4.5% and its performance for each demographic category was within the error bounds, and, hence, were equitable. The other technologies saw either poor overall performance or inequitable performance. For one of these, participants of the race Black/African American (B/AA) as well as those with darker skin tones (Monk scale 7/8/9/10) experienced higher false rejections. Finally, one technology demonstrated more favorable but inequitable performance for the Asian American and Pacific Islander (AAPI) demographic. This study confirms that it is necessary to evaluate products across demographic groups to fully understand the performance of remote identity verification technologies.</li>
</ul>

<h3>Title: Large Language Models Are Strong Audio-Visual Speech Recognition Learners</h3>
<ul>
<li><strong>Authors: </strong>Umberto Cappellazzo, Minsu Kim, Honglie Chen, Pingchuan Ma, Stavros Petridis, Daniele Falavigna, Alessio Brutti, Maja Pantic</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12319">https://arxiv.org/abs/2409.12319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12319">https://arxiv.org/pdf/2409.12319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12319]] Large Language Models Are Strong Audio-Visual Speech Recognition Learners(https://arxiv.org/abs/2409.12319)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have recently become a focal point of research due to their formidable multimodal understanding capabilities. For example, in the audio and speech domains, an LLM can be equipped with (automatic) speech recognition (ASR) abilities by just concatenating the audio tokens, computed with an audio encoder, and the text tokens to achieve state-of-the-art results. On the contrary, tasks like visual and audio-visual speech recognition (VSR/AVSR), which also exploit noise-invariant lip movement information, have received little or no attention. To bridge this gap, we propose Llama-AVSR, a new MLLM with strong audio-visual speech recognition capabilities. It leverages pre-trained audio and video encoders to produce modality-specific tokens which, together with the text tokens, are processed by a pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an auto-regressive fashion. Llama-AVSR requires a small number of trainable parameters as only modality-specific projectors and LoRA modules are trained whereas the multi-modal encoders and LLM are kept frozen. We evaluate our proposed approach on LRS3, the largest public AVSR benchmark, and we achieve new state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.81% and 0.77%, respectively. To bolster our results, we investigate the key factors that underpin the effectiveness of Llama-AVSR: the choice of the pre-trained encoders and LLM, the efficient integration of LoRA modules, and the optimal performance-efficiency trade-off obtained via modality-aware compression rates.</li>
</ul>

<h3>Title: ReFu: Recursive Fusion for Exemplar-Free 3D Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Yi Yang, Lei Zhong, Huiping Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12326">https://arxiv.org/abs/2409.12326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12326">https://arxiv.org/pdf/2409.12326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12326]] ReFu: Recursive Fusion for Exemplar-Free 3D Class-Incremental Learning(https://arxiv.org/abs/2409.12326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce a novel Recursive Fusion model, dubbed ReFu, designed to integrate point clouds and meshes for exemplar-free 3D Class-Incremental Learning, where the model learns new 3D classes while retaining knowledge of previously learned ones. Unlike existing methods that either rely on storing historical data to mitigate forgetting or focus on single data modalities, ReFu eliminates the need for exemplar storage while utilizing the complementary strengths of both point clouds and meshes. To achieve this, we introduce a recursive method which continuously accumulates knowledge by updating the regularized auto-correlation matrix. Furthermore, we propose a fusion module, featuring a Pointcloud-guided Mesh Attention Layer that learns correlations between the two modalities. This mechanism effectively integrates point cloud and mesh features, leading to more robust and stable continual learning. Experiments across various datasets demonstrate that our proposed framework outperforms existing methods in 3D class-incremental learning. Project Page: this https URL</li>
</ul>

<h3>Title: SplitVAEs: Decentralized scenario generation from siloed data for stochastic optimization problems</h3>
<ul>
<li><strong>Authors: </strong>H M Mohaimanul Islam, Huynh Q. N. Vo, Paritosh Ramanan</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12328">https://arxiv.org/abs/2409.12328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12328">https://arxiv.org/pdf/2409.12328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12328]] SplitVAEs: Decentralized scenario generation from siloed data for stochastic optimization problems(https://arxiv.org/abs/2409.12328)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Stochastic optimization problems in large-scale multi-stakeholder networked systems (e.g., power grids and supply chains) rely on data-driven scenarios to encapsulate complex spatiotemporal interdependencies. However, centralized aggregation of stakeholder data is challenging due to the existence of data silos resulting from computational and logistical bottlenecks. In this paper, we present SplitVAEs, a decentralized scenario generation framework that leverages variational autoencoders to generate high-quality scenarios without moving stakeholder data. With the help of experiments on distributed memory systems, we demonstrate the broad applicability of SplitVAEs in a variety of domain areas that are dominated by a large number of stakeholders. Our experiments indicate that SplitVAEs can learn spatial and temporal interdependencies in large-scale networks to generate scenarios that match the joint historical distribution of stakeholder data in a decentralized manner. Our experiments show that SplitVAEs deliver robust performance compared to centralized, state-of-the-art benchmark methods while significantly reducing data transmission costs, leading to a scalable, privacy-enhancing alternative to scenario generation.</li>
</ul>

<h3>Title: FuzzEval: Assessing Fuzzers on Generating Context-Sensitive Inputs</h3>
<ul>
<li><strong>Authors: </strong>S Mahmudul Hasan, Polina Kozyreva, Endadul Hoque</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12331">https://arxiv.org/abs/2409.12331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12331">https://arxiv.org/pdf/2409.12331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12331]] FuzzEval: Assessing Fuzzers on Generating Context-Sensitive Inputs(https://arxiv.org/abs/2409.12331)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cryptographic protocols form the backbone of modern security systems, yet vulnerabilities persist within their implementations. Traditional testing techniques, including fuzzing, have struggled to effectively identify vulnerabilities in cryptographic libraries due to their reliance on context-sensitive inputs. This paper presents a comprehensive evaluation of eleven state-of-the-art fuzzers' ability to generate context-sensitive inputs for testing a cryptographic standard, PKCS#1-v1.5, across thirteen implementations. Our study reveals nuanced performance differences among the fuzzers in terms of the validity and diversity of the produced inputs. This investigation underscores the limitations of existing fuzzers in handling context-sensitive inputs. These findings are expected to drive further research and development in this area.</li>
</ul>

<h3>Title: Provable Privacy Guarantee for Individual Identities and Locations in Large-Scale Contact Tracing</h3>
<ul>
<li><strong>Authors: </strong>Tyler Nicewarner, Wei Jiang, Aniruddha Gokhale, Dan Lin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12341">https://arxiv.org/abs/2409.12341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12341">https://arxiv.org/pdf/2409.12341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12341]] Provable Privacy Guarantee for Individual Identities and Locations in Large-Scale Contact Tracing(https://arxiv.org/abs/2409.12341)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The task of infectious disease contact tracing is crucial yet challenging, especially when meeting strict privacy requirements. Previous attempts in this area have had limitations in terms of applicable scenarios and efficiency. Our paper proposes a highly scalable, practical contact tracing system called PREVENT that can work with a variety of location collection methods to gain a comprehensive overview of a person's trajectory while ensuring the privacy of individuals being tracked, without revealing their plain text locations to any party, including servers. Our system is very efficient and can provide real-time query services for large-scale datasets with millions of locations. This is made possible by a newly designed secret-sharing based architecture that is tightly integrated into unique private space partitioning trees. Notably, our experimental results on both real and synthetic datasets demonstrate that our system introduces negligible performance overhead compared to traditional contact tracing methods. PREVENT could be a game-changer in the fight against infectious diseases and set a new standard for privacy-preserving location tracking.</li>
</ul>

<h3>Title: Extracting Memorized Training Data via Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Ellen Su, Anu Vellore, Amy Chang, Raffaele Mura, Blaine Nelson, Paul Kassianik, Amin Karbasi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12367">https://arxiv.org/abs/2409.12367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12367">https://arxiv.org/pdf/2409.12367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12367]] Extracting Memorized Training Data via Decomposition(https://arxiv.org/abs/2409.12367)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, extraction, large language model</a></li>
<li><strong>Abstract: </strong>The widespread use of Large Language Models (LLMs) in society creates new information security challenges for developers, organizations, and end-users alike. LLMs are trained on large volumes of data, and their susceptibility to reveal the exact contents of the source training datasets poses security and safety risks. Although current alignment procedures restrict common risky behaviors, they do not completely prevent LLMs from leaking data. Prior work demonstrated that LLMs may be tricked into divulging training data by using out-of-distribution queries or adversarial techniques. In this paper, we demonstrate a simple, query-based decompositional method to extract news articles from two frontier LLMs. We use instruction decomposition techniques to incrementally extract fragments of training data. Out of 3723 New York Times articles, we extract at least one verbatim sentence from 73 articles, and over 20% of verbatim sentences from 6 articles. Our analysis demonstrates that this method successfully induces the LLM to generate texts that are reliable reproductions of news articles, meaning that they likely originate from the source training dataset. This method is simple, generalizable, and does not fine-tune or change the production model. If replicable at scale, this training data extraction methodology could expose new LLM security and safety vulnerabilities, including privacy risks and unauthorized data leaks. These implications require careful consideration from model development to its end-use.</li>
</ul>

<h3>Title: Communication-Efficient Federated Low-Rank Update Algorithm and its Connection to Implicit Regularization</h3>
<ul>
<li><strong>Authors: </strong>Haemin Park, Diego Klabjan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12371">https://arxiv.org/abs/2409.12371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12371">https://arxiv.org/pdf/2409.12371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12371]] Communication-Efficient Federated Low-Rank Update Algorithm and its Connection to Implicit Regularization(https://arxiv.org/abs/2409.12371)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) faces significant challenges related to communication efficiency and heterogeneity. To address these issues, we explore the potential of using low-rank updates. Our theoretical analysis reveals that client's loss exhibits a higher rank structure (gradients span higher rank subspace of Hessian) compared to the server's loss. Based on this insight, we hypothesize that constraining client-side optimization to a low-rank subspace could provide an implicit regularization effect. Consequently, we propose FedLoRU, a general low-rank update framework for federated learning. Our framework enforces low-rank client-side updates and accumulates these updates to form a higher-rank model. Additionally, variants of FedLoRU can adapt to environments with statistical and model heterogeneity by employing multiple or hierarchical low-rank updates. Experimental results demonstrate that FedLoRU performs comparably to full-rank algorithms and exhibits robustness to heterogeneous and large numbers of clients.</li>
</ul>

<h3>Title: Enhancing 3D Robotic Vision Robustness by Minimizing Adversarial Mutual Information through a Curriculum Training Approach</h3>
<ul>
<li><strong>Authors: </strong>Nastaran Darabi, Dinithi Jayasuriya, Devashri Naik, Theja Tulabandhula, Amit Ranjan Trivedi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12379">https://arxiv.org/abs/2409.12379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12379">https://arxiv.org/pdf/2409.12379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12379]] Enhancing 3D Robotic Vision Robustness by Minimizing Adversarial Mutual Information through a Curriculum Training Approach(https://arxiv.org/abs/2409.12379)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Adversarial attacks exploit vulnerabilities in a model's decision boundaries through small, carefully crafted perturbations that lead to significant mispredictions. In 3D vision, the high dimensionality and sparsity of data greatly expand the attack surface, making 3D vision particularly vulnerable for safety-critical robotics. To enhance 3D vision's adversarial robustness, we propose a training objective that simultaneously minimizes prediction loss and mutual information (MI) under adversarial perturbations to contain the upper bound of misprediction errors. This approach simplifies handling adversarial examples compared to conventional methods, which require explicit searching and training on adversarial samples. However, minimizing prediction loss conflicts with minimizing MI, leading to reduced robustness and catastrophic forgetting. To address this, we integrate curriculum advisors in the training setup that gradually introduce adversarial objectives to balance training and prevent models from being overwhelmed by difficult cases early in the process. The advisors also enhance robustness by encouraging training on diverse MI examples through entropy regularizers. We evaluated our method on ModelNet40 and KITTI using PointNet, DGCNN, SECOND, and PointTransformers, achieving 2-5% accuracy gains on ModelNet40 and a 5-10% mAP improvement in object detection. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Privacy-Preserving Student Learning with Differentially Private Data-Free Distillation</h3>
<ul>
<li><strong>Authors: </strong>Bochao Liu, Jianghu Lu, Pengju Wang, Junjie Zhang, Dan Zeng, Zhenxing Qian, Shiming Ge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12384">https://arxiv.org/abs/2409.12384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12384">https://arxiv.org/pdf/2409.12384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12384]] Privacy-Preserving Student Learning with Differentially Private Data-Free Distillation(https://arxiv.org/abs/2409.12384)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, data-free</a></li>
<li><strong>Abstract: </strong>Deep learning models can achieve high inference accuracy by extracting rich knowledge from massive well-annotated data, but may pose the risk of data privacy leakage in practical deployment. In this paper, we present an effective teacher-student learning approach to train privacy-preserving deep learning models via differentially private data-free distillation. The main idea is generating synthetic data to learn a student that can mimic the ability of a teacher well-trained on private data. In the approach, a generator is first pretrained in a data-free manner by incorporating the teacher as a fixed discriminator. With the generator, massive synthetic data can be generated for model training without exposing data privacy. Then, the synthetic data is fed into the teacher to generate private labels. Towards this end, we propose a label differential privacy algorithm termed selective randomized response to protect the label information. Finally, a student is trained on the synthetic data with the supervision of private labels. In this way, both data privacy and label privacy are well protected in a unified framework, leading to privacy-preserving models. Extensive experiments and analysis clearly demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Look Through Masks: Towards Masked Face Recognition with De-Occlusion Distillation</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Li, Shiming Ge, Daichi Zhang, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12385">https://arxiv.org/abs/2409.12385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12385">https://arxiv.org/pdf/2409.12385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12385]] Look Through Masks: Towards Masked Face Recognition with De-Occlusion Distillation(https://arxiv.org/abs/2409.12385)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many real-world applications today like video surveillance and urban governance need to address the recognition of masked faces, where content replacement by diverse masks often brings in incomplete appearance and ambiguous representation, leading to a sharp drop in accuracy. Inspired by recent progress on amodal perception, we propose to migrate the mechanism of amodal completion for the task of masked face recognition with an end-to-end de-occlusion distillation framework, which consists of two modules. The \textit{de-occlusion} module applies a generative adversarial network to perform face completion, which recovers the content under the mask and eliminates appearance ambiguity. The \textit{distillation} module takes a pre-trained general face recognition model as the teacher and transfers its knowledge to train a student for completed faces using massive online synthesized face pairs. Especially, the teacher knowledge is represented with structural relations among instances in multiple orders, which serves as a posterior regularization to enable the adaptation. In this way, the knowledge can be fully distilled and transferred to identify masked faces. Experiments on synthetic and realistic datasets show the efficacy of the proposed approach.</li>
</ul>

<h3>Title: A Novel Perspective for Multi-modal Multi-label Skin Lesion Classification</h3>
<ul>
<li><strong>Authors: </strong>Yuan Zhang, Yutong Xie, Hu Wang, Jodie C Avery, M Louise Hull, Gustavo Carneiro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12390">https://arxiv.org/abs/2409.12390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12390">https://arxiv.org/pdf/2409.12390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12390]] A Novel Perspective for Multi-modal Multi-label Skin Lesion Classification(https://arxiv.org/abs/2409.12390)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The efficacy of deep learning-based Computer-Aided Diagnosis (CAD) methods for skin diseases relies on analyzing multiple data modalities (i.e., clinical+dermoscopic images, and patient metadata) and addressing the challenges of multi-label classification. Current approaches tend to rely on limited multi-modal techniques and treat the multi-label problem as a multiple multi-class problem, overlooking issues related to imbalanced learning and multi-label correlation. This paper introduces the innovative Skin Lesion Classifier, utilizing a Multi-modal Multi-label TransFormer-based model (SkinM2Former). For multi-modal analysis, we introduce the Tri-Modal Cross-attention Transformer (TMCT) that fuses the three image and metadata modalities at various feature levels of a transformer encoder. For multi-label classification, we introduce a multi-head attention (MHA) module to learn multi-label correlations, complemented by an optimisation that handles multi-label and imbalanced learning problems. SkinM2Former achieves a mean average accuracy of 77.27% and a mean diagnostic accuracy of 77.85% on the public Derm7pt dataset, outperforming state-of-the-art (SOTA) methods.</li>
</ul>

<h3>Title: Selecting a classification performance measure: matching the measure to the problem</h3>
<ul>
<li><strong>Authors: </strong>David J. Hand, Peter Christen, Sumayya Ziyad</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12391">https://arxiv.org/abs/2409.12391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12391">https://arxiv.org/pdf/2409.12391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12391]] Selecting a classification performance measure: matching the measure to the problem(https://arxiv.org/abs/2409.12391)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The problem of identifying to which of a given set of classes objects belong is ubiquitous, occurring in many research domains and application areas, including medical diagnosis, financial decision making, online commerce, and national security. But such assignments are rarely completely perfect, and classification errors occur. This means it is necessary to compare classification methods and algorithms to decide which is ``best'' for any particular problem. However, just as there are many different classification methods, so there are many different ways of measuring their performance. It is thus vital to choose a measure of performance which matches the aims of the research or application. This paper is a contribution to the growing literature on the relative merits of different performance measures. Its particular focus is the critical importance of matching the properties of the measure to the aims for which the classification is being made.</li>
</ul>

<h3>Title: Small Language Models are Equation Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Bumjun Kim, Kunha Lee, Juyeon Kim, Sangam Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12393">https://arxiv.org/abs/2409.12393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12393">https://arxiv.org/pdf/2409.12393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12393]] Small Language Models are Equation Reasoners(https://arxiv.org/abs/2409.12393)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning has enabled Large Language Model (LLM) to achieve remarkable performance in various NLP tasks, including arithmetic problem-solving. However, this success does not generalize to small language model (sLM) like T5, due to their limited capacity and absence of emergent abilities associated with larger models. Recent works to enhance sLM through knowledge distillation have yielded some improvements but still face significant limitations, particularly high ambiguity from the variability in natural language expressions and substantial computational costs. In this paper, we investigate why sLM perform poorly on arithmetic reasoning tasks and hypothesize that natural language format variability introduces high ambiguity for these smaller models. Based on this hypothesis, we conduct experiments with equation-only format, which is a reasoning format that unifies arithmetic reasoning previously expressed in natural language formats into mathematical equations. Experiment results demonstrate that equation-only format effectively boosts the arithmetic reasoning abilities of sLM, especially in very small models like T5-Tiny.</li>
</ul>

<h3>Title: ITPatch: An Invisible and Triggered Physical Adversarial Patch against Traffic Sign Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shuai Yuan, Hongwei Li, Xingshuo Han, Guowen Xu, Wenbo Jiang, Tao Ni, Qingchuan Zhao, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12394">https://arxiv.org/abs/2409.12394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12394">https://arxiv.org/pdf/2409.12394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12394]] ITPatch: An Invisible and Triggered Physical Adversarial Patch against Traffic Sign Recognition(https://arxiv.org/abs/2409.12394)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Physical adversarial patches have emerged as a key adversarial attack to cause misclassification of traffic sign recognition (TSR) systems in the real world. However, existing adversarial patches have poor stealthiness and attack all vehicles indiscriminately once deployed. In this paper, we introduce an invisible and triggered physical adversarial patch (ITPatch) with a novel attack vector, i.e., fluorescent ink, to advance the state-of-the-art. It applies carefully designed fluorescent perturbations to a target sign, an attacker can later trigger a fluorescent effect using invisible ultraviolet light, causing the TSR system to misclassify the sign and potentially resulting in traffic accidents. We conducted a comprehensive evaluation to investigate the effectiveness of ITPatch, which shows a success rate of 98.31% in low-light conditions. Furthermore, our attack successfully bypasses five popular defenses and achieves a success rate of 96.72%.</li>
</ul>

<h3>Title: LMT-Net: Lane Model Transformer Network for Automated HD Mapping from Sparse Vehicle Observations</h3>
<ul>
<li><strong>Authors: </strong>Michael Mink, Thomas Monninger, Steffen Staab</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12409">https://arxiv.org/abs/2409.12409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12409">https://arxiv.org/pdf/2409.12409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12409]] LMT-Net: Lane Model Transformer Network for Automated HD Mapping from Sparse Vehicle Observations(https://arxiv.org/abs/2409.12409)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In autonomous driving, High Definition (HD) maps provide a complete lane model that is not limited by sensor range and occlusions. However, the generation and upkeep of HD maps involves periodic data collection and human annotations, limiting scalability. To address this, we investigate automating the lane model generation and the use of sparse vehicle observations instead of dense sensor measurements. For our approach, a pre-processing step generates polylines by aligning and aggregating observed lane boundaries. Aligned driven traces are used as starting points for predicting lane pairs defined by the left and right boundary points. We propose Lane Model Transformer Network (LMT-Net), an encoder-decoder neural network architecture that performs polyline encoding and predicts lane pairs and their connectivity. A lane graph is formed by using predicted lane pairs as nodes and predicted lane connectivity as edges. We evaluate the performance of LMT-Net on an internal dataset that consists of multiple vehicle observations as well as human annotations as Ground Truth (GT). The evaluation shows promising results and demonstrates superior performance compared to the implemented baseline on both highway and non-highway Operational Design Domain (ODD).</li>
</ul>

<h3>Title: Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation</h3>
<ul>
<li><strong>Authors: </strong>Chen Liang, Zhifan Feng, Zihe Liu, Wenbin Jiang, Jinan Xu, Yufeng Chen, Yong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12411">https://arxiv.org/abs/2409.12411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12411">https://arxiv.org/pdf/2409.12411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12411]] Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation(https://arxiv.org/abs/2409.12411)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-thought prompting significantly boosts the reasoning ability of large language models but still faces three issues: hallucination problem, restricted interpretability, and uncontrollable generation. To address these challenges, we present AgentCOT, a llm-based autonomous agent framework, which can solve complex problems in an agent-style manner by multiple round LLM generation. At each step, AgentCOT selects an action and executes it to yield an intermediate result with supporting evidence. In addition, we integrate the step's index into the reasoning process to form a graph structure for complex inference logic. We introduce two new strategies to enhance the performance of AgentCOT.We conduct extensive experiments to verify the effectiveness of our method on six common benchmarks. Results exhibit that our method brings in substantial improvements over current competitive approaches.</li>
</ul>

<h3>Title: Domain-stratified Training for Cross-organ and Cross-scanner Adenocarcinoma Segmentation in the COSAS 2024 Challenge</h3>
<ul>
<li><strong>Authors: </strong>Huang Jiayan, Ji Zheng, Kuang Jinbo, Xu Shuoyu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12418">https://arxiv.org/abs/2409.12418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12418">https://arxiv.org/pdf/2409.12418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12418]] Domain-stratified Training for Cross-organ and Cross-scanner Adenocarcinoma Segmentation in the COSAS 2024 Challenge(https://arxiv.org/abs/2409.12418)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This manuscript presents an image segmentation algorithm developed for the Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation (COSAS 2024) challenge. We adopted an organ-stratified and scanner-stratified approach to train multiple Upernet-based segmentation models and subsequently ensembled the results. Despite the challenges posed by the varying tumor characteristics across different organs and the differing imaging conditions of various scanners, our method achieved a final test score of 0.7643 for Task 1 and 0.8354 for Task 2. These results demonstrate the adaptability and efficacy of our approach across diverse conditions. Our model's ability to generalize across various datasets underscores its potential for real-world applications.</li>
</ul>

<h3>Title: Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large Language Models Iteratively without Gold Labels</h3>
<ul>
<li><strong>Authors: </strong>Chaoqun Liu, Qin Chao, Wenxuan Zhang, Xiaobao Wu, Boyang Li, Anh Tuan Luu, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12425">https://arxiv.org/abs/2409.12425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12425">https://arxiv.org/pdf/2409.12425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12425]] Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large Language Models Iteratively without Gold Labels(https://arxiv.org/abs/2409.12425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance through supervised fine-tuning or in-context learning using gold labels. However, this paradigm is limited by the availability of gold labels, while in certain scenarios, LLMs may need to perform tasks that are too complex for humans to provide such labels. To tackle this challenge, this study explores whether solely utilizing unlabeled data can elicit strong model capabilities. We propose a new paradigm termed zero-to-strong generalization. We iteratively prompt LLMs to annotate unlabeled data and retain high-quality labels by filtering. Surprisingly, we obverse that this iterative process gradually unlocks LLMs' potential on downstream tasks. Our experiments on extensive classification and reasoning tasks confirm the effectiveness of our proposed framework. Our analysis indicates that this paradigm is effective for both in-context learning and fine-tuning, and for various model sizes.</li>
</ul>

<h3>Title: Sustainable Visions: Unsupervised Machine Learning Insights on Global Development Goals</h3>
<ul>
<li><strong>Authors: </strong>Alberto García-Rodríguez, Matias Núñez, Miguel Robles Pérez, Tzipe Govezensky, Rafael A. Barrio, Carlos Gershenson, Kimmo K. Kaski, Julia Tagüeña</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12427">https://arxiv.org/abs/2409.12427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12427">https://arxiv.org/pdf/2409.12427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12427]] Sustainable Visions: Unsupervised Machine Learning Insights on Global Development Goals(https://arxiv.org/abs/2409.12427)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The United Nations 2030 Agenda for Sustainable Development outlines 17 goals to address global challenges. However, progress has been slower than expected and, consequently, there is a need to investigate the reasons behind this fact. In this study, we used a novel data-driven methodology to analyze data from 107 countries (2000$-$2022) using unsupervised machine learning techniques. Our analysis reveals strong positive and negative correlations between certain SDGs. The findings show that progress toward the SDGs is heavily influenced by geographical, cultural and socioeconomic factors, with no country on track to achieve all goals by 2030. This highlights the need for a region specific, systemic approach to sustainable development that acknowledges the complex interdependencies of the goals and the diverse capacities of nations. Our approach provides a robust framework for developing efficient and data-informed strategies, to promote cooperative and targeted initiatives for sustainable progress.</li>
</ul>

<h3>Title: Is it Still Fair? A Comparative Evaluation of Fairness Algorithms through the Lens of Covariate Drift</h3>
<ul>
<li><strong>Authors: </strong>Oscar Blessed Deho, Michael Bewong, Selasi Kwashie, Jiuyong Li, Jixue Liu, Lin Liu, Srecko Joksimovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12428">https://arxiv.org/abs/2409.12428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12428">https://arxiv.org/pdf/2409.12428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12428]] Is it Still Fair? A Comparative Evaluation of Fairness Algorithms through the Lens of Covariate Drift(https://arxiv.org/abs/2409.12428)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Over the last few decades, machine learning (ML) applications have grown exponentially, yielding several benefits to society. However, these benefits are tempered with concerns of discriminatory behaviours exhibited by ML models. In this regard, fairness in machine learning has emerged as a priority research area. Consequently, several fairness metrics and algorithms have been developed to mitigate against discriminatory behaviours that ML models may possess. Yet still, very little attention has been paid to the problem of naturally occurring changes in data patterns (\textit{aka} data distributional drift), and its impact on fairness algorithms and metrics. In this work, we study this problem comprehensively by analyzing 4 fairness-unaware baseline algorithms and 7 fairness-aware algorithms, carefully curated to cover the breadth of its typology, across 5 datasets including public and proprietary data, and evaluated them using 3 predictive performance and 10 fairness metrics. In doing so, we show that (1) data distributional drift is not a trivial occurrence, and in several cases can lead to serious deterioration of fairness in so-called fair models; (2) contrary to some existing literature, the size and direction of data distributional drift is not correlated to the resulting size and direction of unfairness; and (3) choice of, and training of fairness algorithms is impacted by the effect of data distributional drift which is largely ignored in the literature. Emanating from our findings, we synthesize several policy implications of data distributional drift on fairness algorithms that can be very relevant to stakeholders and practitioners.</li>
</ul>

<h3>Title: FlexiTex: Enhancing Texture Generation with Visual Guidance</h3>
<ul>
<li><strong>Authors: </strong>DaDong Jiang, Xianghui Yang, Zibo Zhao, Sheng Zhang, Jiaao Yu, Zeqiang Lai, Shaoxiong Yang, Chunchao Guo, Xiaobo Zhou, Zhihui Ke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12431">https://arxiv.org/abs/2409.12431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12431">https://arxiv.org/pdf/2409.12431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12431]] FlexiTex: Enhancing Texture Generation with Visual Guidance(https://arxiv.org/abs/2409.12431)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent texture generation methods achieve impressive results due to the powerful generative prior they leverage from large-scale text-to-image diffusion models. However, abstract textual prompts are limited in providing global textural or shape information, which results in the texture generation methods producing blurry or inconsistent patterns. To tackle this, we present FlexiTex, embedding rich information via visual guidance to generate a high-quality texture. The core of FlexiTex is the Visual Guidance Enhancement module, which incorporates more specific information from visual guidance to reduce ambiguity in the text prompt and preserve high-frequency details. To further enhance the visual guidance, we introduce a Direction-Aware Adaptation module that automatically designs direction prompts based on different camera poses, avoiding the Janus problem and maintaining semantically global consistency. Benefiting from the visual guidance, FlexiTex produces quantitatively and qualitatively sound results, demonstrating its potential to advance texture generation for real-world applications.</li>
</ul>

<h3>Title: Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhou, Delong Chen, Samuel Cahyawijaya, Xufeng Duan, Zhenguang G. Cai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12435">https://arxiv.org/abs/2409.12435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12435">https://arxiv.org/pdf/2409.12435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12435]] Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language Models(https://arxiv.org/abs/2409.12435)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a novel analysis that leverages linguistic minimal pairs to probe the internal linguistic representations of Large Language Models (LLMs). By measuring the similarity between LLM activation differences across minimal pairs, we quantify the and gain insight into the linguistic knowledge captured by LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs in three languages, reveal properties of linguistic similarity from four key aspects: consistency across LLMs, relation to theoretical categorizations, dependency to semantic context, and cross-lingual alignment of relevant phenomena. Our findings suggest that 1) linguistic similarity is significantly influenced by training data exposure, leading to higher cross-LLM agreement in higher-resource languages. 2) Linguistic similarity strongly aligns with fine-grained theoretical linguistic categories but weakly with broader ones. 3) Linguistic similarity shows a weak correlation with semantic similarity, showing its context-dependent nature. 4) LLMs exhibit limited cross-lingual alignment in their understanding of relevant linguistic phenomena. This work demonstrates the potential of minimal pairs as a window into the neural representations of language in LLMs, shedding light on the relationship between LLMs and linguistic theory.</li>
</ul>

<h3>Title: Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Zhou, Abbas Ghaddar, Ge Zhang, Liheng Ma, Yaochen Hu, Soumyasundar Pal, Mark Coates, Bin Wang, Yingxue Zhang, Jianye Hao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12437">https://arxiv.org/abs/2409.12437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12437">https://arxiv.org/pdf/2409.12437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12437]] Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data(https://arxiv.org/abs/2409.12437)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities. Our extensive experiments, conducted on two established natural language reasoning tasks -- inductive reasoning and spatial reasoning -- demonstrate that supervised fine-tuning (SFT) with synthetic graph-based reasoning data effectively enhances LLMs' reasoning performance without compromising their effectiveness on other standard evaluation benchmarks.</li>
</ul>

<h3>Title: Incremental and Data-Efficient Concept Formation to Support Masked Word Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xin Lian, Nishant Baglodi, Christopher J. MacLellan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12440">https://arxiv.org/abs/2409.12440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12440">https://arxiv.org/pdf/2409.12440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12440]] Incremental and Data-Efficient Concept Formation to Support Masked Word Prediction(https://arxiv.org/abs/2409.12440)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces Cobweb4L, a novel approach for efficient language model learning that supports masked word prediction. The approach builds on Cobweb, an incremental system that learns a hierarchy of probabilistic concepts. Each concept stores the frequencies of words that appear in instances tagged with that concept label. The system utilizes an attribute value representation to encode words and their surrounding context into instances. Cobweb4L uses the information theoretic variant of category utility and a new performance mechanism that leverages multiple concepts to generate predictions. We demonstrate that with these extensions it significantly outperforms prior Cobweb performance mechanisms that use only a single node to generate predictions. Further, we demonstrate that Cobweb4L learns rapidly and achieves performance comparable to and even superior to Word2Vec. Next, we show that Cobweb4L and Word2Vec outperform BERT in the same task with less training data. Finally, we discuss future work to make our conclusions more robust and inclusive.</li>
</ul>

<h3>Title: Domain Generalization for Endoscopic Image Segmentation by Disentangling Style-Content Information and SuperPixel Consistency</h3>
<ul>
<li><strong>Authors: </strong>Mansoor Ali Teevno, Rafael Martinez-Garcia-Pena, Gilberto Ochoa-Ruiz, Sharib Ali</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12450">https://arxiv.org/abs/2409.12450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12450">https://arxiv.org/pdf/2409.12450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12450]] Domain Generalization for Endoscopic Image Segmentation by Disentangling Style-Content Information and SuperPixel Consistency(https://arxiv.org/abs/2409.12450)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Frequent monitoring is necessary to stratify individuals based on their likelihood of developing gastrointestinal (GI) cancer precursors. In clinical practice, white-light imaging (WLI) and complementary modalities such as narrow-band imaging (NBI) and fluorescence imaging are used to assess risk areas. However, conventional deep learning (DL) models show degraded performance due to the domain gap when a model is trained on one modality and tested on a different one. In our earlier approach, we used a superpixel-based method referred to as "SUPRA" to effectively learn domain-invariant information using color and space distances to generate groups of pixels. One of the main limitations of this earlier work is that the aggregation does not exploit structural information, making it suboptimal for segmentation tasks, especially for polyps and heterogeneous color distributions. Therefore, in this work, we propose an approach for style-content disentanglement using instance normalization and instance selective whitening (ISW) for improved domain generalization when combined with SUPRA. We evaluate our approach on two datasets: EndoUDA Barrett's Esophagus and EndoUDA polyps, and compare its performance with three state-of-the-art (SOTA) methods. Our findings demonstrate a notable enhancement in performance compared to both baseline and SOTA methods across the target domain data. Specifically, our approach exhibited improvements of 14%, 10%, 8%, and 18% over the baseline and three SOTA methods on the polyp dataset. Additionally, it surpassed the second-best method (EndoUDA) on the Barrett's Esophagus dataset by nearly 2%.</li>
</ul>

<h3>Title: CodePlan: Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Wen, Jian Guan, Hongning Wang, Wei Wu, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12452">https://arxiv.org/abs/2409.12452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12452">https://arxiv.org/pdf/2409.12452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12452]] CodePlan: Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning(https://arxiv.org/abs/2409.12452)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of large language models (LLMs) on traditional natural language processing tasks, their planning ability remains a critical bottleneck in tackling complex multi-step reasoning tasks. Existing approaches mainly rely on prompting or task-specific fine-tuning, often suffering from weak robustness and cross-task generalization. To address the limitation, we introduce CODEPLAN, a scalable paradigm that empowers LLMs to generate and follow code-form plans pseudocode that outlines high-level, structured reasoning processes. By leveraging the structured and versatile nature of code, CODEPLAN effectively captures the rich semantics and control flows inherent to sophisticated reasoning. Importantly, CODEPLAN allows the automatic extraction of code-form plans from massive, wide-ranging text corpora without the need for curated, task-specific datasets. This enables it to scale up efficiently and improve reasoning capabilities across diverse scenarios. To train CODEPLAN, we construct a large-scale dataset of 2M examples that integrate code-form plans with standard prompt-response pairs from existing corpora. With minimal computation overhead during both training and inference, CODEPLAN achieves a 25.1% relative improvement compared with directly generating responses, averaged across 13 challenging multi-step reasoning benchmarks, spanning mathematical reasoning, symbolic reasoning, instruction-following, multi-hop QA, and decision-making tasks. Further analysis reveals CODEPLAN's increasing performance gains on more complex reasoning tasks, as well as significant data efficiency thanks to its generalization ability.</li>
</ul>

<h3>Title: FoME: A Foundation Model for EEG using Adaptive Temporal-Lateral Attention Scaling</h3>
<ul>
<li><strong>Authors: </strong>Enze Shi, Kui Zhao, Qilong Yuan, Jiaqi Wang, Huawen Hu, Sigang Yu, Shu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12454">https://arxiv.org/abs/2409.12454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12454">https://arxiv.org/pdf/2409.12454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12454]] FoME: A Foundation Model for EEG using Adaptive Temporal-Lateral Attention Scaling(https://arxiv.org/abs/2409.12454)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Electroencephalography (EEG) is a vital tool to measure and record brain activity in neuroscience and clinical applications, yet its potential is constrained by signal heterogeneity, low signal-to-noise ratios, and limited labeled datasets. In this paper, we propose FoME (Foundation Model for EEG), a novel approach using adaptive temporal-lateral attention scaling to address above-mentioned challenges. FoME is pre-trained on a diverse 1.7TB dataset of scalp and intracranial EEG recordings, comprising 745M parameters trained for 1,096k steps. Our model introduces two key innovations: a time-frequency fusion embedding technique and an adaptive time-lateral attention scaling (ATLAS) mechanism. These components synergistically capture complex temporal and spectral EEG dynamics, enabling FoME to adapt to varying patterns across diverse data streams and facilitate robust multi-channel modeling. Evaluations across four downstream tasks demonstrate FoME's superior performance in classification and forecasting applications, consistently achieving state-of-the-art results. To conclude, FoME establishes a new paradigm for EEG analysis, offering a versatile foundation that advances brain-computer interfaces, clinical diagnostics, and cognitive research across neuroscience and related fields. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Bayesian-Optimized One-Step Diffusion Model with Knowledge Distillation for Real-Time 3D Human Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Sibo Tian, Minghui Zheng, Xiao Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12456">https://arxiv.org/abs/2409.12456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12456">https://arxiv.org/pdf/2409.12456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12456]] Bayesian-Optimized One-Step Diffusion Model with Knowledge Distillation for Real-Time 3D Human Motion Prediction(https://arxiv.org/abs/2409.12456)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Human motion prediction is a cornerstone of human-robot collaboration (HRC), as robots need to infer the future movements of human workers based on past motion cues to proactively plan their motion, ensuring safety in close collaboration scenarios. The diffusion model has demonstrated remarkable performance in predicting high-quality motion samples with reasonable diversity, but suffers from a slow generative process which necessitates multiple model evaluations, hindering real-world applications. To enable real-time prediction, in this work, we propose training a one-step multi-layer perceptron-based (MLP-based) diffusion model for motion prediction using knowledge distillation and Bayesian optimization. Our method contains two steps. First, we distill a pretrained diffusion-based motion predictor, TransFusion, directly into a one-step diffusion model with the same denoiser architecture. Then, to further reduce the inference time, we remove the computationally expensive components from the original denoiser and use knowledge distillation once again to distill the obtained one-step diffusion model into an even smaller model based solely on MLPs. Bayesian optimization is used to tune the hyperparameters for training the smaller diffusion model. Extensive experimental studies are conducted on benchmark datasets, and our model can significantly improve the inference speed, achieving real-time prediction without noticeable degradation in performance.</li>
</ul>

<h3>Title: Familiarity-aware Evidence Compression for Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12468">https://arxiv.org/abs/2409.12468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12468">https://arxiv.org/pdf/2409.12468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12468]] Familiarity-aware Evidence Compression for Retrieval Augmented Generation(https://arxiv.org/abs/2409.12468)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) improves large language models (LMs) by incorporating non-parametric knowledge through evidence retrieval from external sources. However, it often struggles to filter out inconsistent and irrelevant information that can distract the LM from its tasks. While compressing the retrieved evidence with a compression model aims to address this issue, the compressed evidence may still be unfamiliar to the target model used for downstream task, potentially failing to utilize the evidence effectively. We propose FaviComp (Familiarity-aware Evidence Compression), a novel training-free evidence compression technique that makes retrieved evidence more familiar to the target model, while seamlessly integrating parametric knowledge from the model. Specifically, FaviComp proactively lowers the perplexity of the compressed evidence with regard to the target model by combining token probabilities from both the compression model and the target model to generate context that is more familiar to the target model. This approach balances the integration of parametric and non-parametric knowledge, which is especially helpful in complex tasks where the retrieved evidence set may not contain all the necessary information. Experimental results demonstrate that FaviComp consistently outperforms existing baselines in multiple open-domain QA datasets, achieving high compression rates and showcasing the effective integration of both parametric and non-parametric knowledge.</li>
</ul>

<h3>Title: HSIGene: A Foundation Model For Hyperspectral Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Li Pang, Datao Tang, Shuang Xu, Deyu Meng, Xiangyong Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12470">https://arxiv.org/abs/2409.12470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12470">https://arxiv.org/pdf/2409.12470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12470]] HSIGene: A Foundation Model For Hyperspectral Image Generation(https://arxiv.org/abs/2409.12470)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Hyperspectral image (HSI) plays a vital role in various fields such as agriculture and environmental monitoring. However, due to the expensive acquisition cost, the number of hyperspectral images is limited, degenerating the performance of downstream tasks. Although some recent studies have attempted to employ diffusion models to synthesize HSIs, they still struggle with the scarcity of HSIs, affecting the reliability and diversity of the generated images. Some studies propose to incorporate multi-modal data to enhance spatial diversity, but the spectral fidelity cannot be ensured. In addition, existing HSI synthesis models are typically uncontrollable or only support single-condition control, limiting their ability to generate accurate and reliable HSIs. To alleviate these issues, we propose HSIGene, a novel HSI generation foundation model which is based on latent diffusion and supports multi-condition control, allowing for more precise and reliable HSI generation. To enhance the spatial diversity of the training data while preserving spectral fidelity, we propose a new data augmentation method based on spatial super-resolution, in which HSIs are upscaled first, and thus abundant training patches could be obtained by cropping the high-resolution HSIs. In addition, to improve the perceptual quality of the augmented data, we introduce a novel two-stage HSI super-resolution framework, which first applies RGB bands super-resolution and then utilizes our proposed Rectangular Guided Attention Network (RGAN) for guided HSI super-resolution. Experiments demonstrate that the proposed model is capable of generating a vast quantity of realistic HSIs for downstream tasks such as denoising and super-resolution. The code and models are available at this https URL.</li>
</ul>

<h3>Title: TEAM: Temporal Adversarial Examples Attack Model against Network Intrusion Detection System Applied to RNN</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Liu, Dengpan Ye, Long Tang, Yunming Zhang, Jiacheng Deng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12472">https://arxiv.org/abs/2409.12472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12472">https://arxiv.org/pdf/2409.12472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12472]] TEAM: Temporal Adversarial Examples Attack Model against Network Intrusion Detection System Applied to RNN(https://arxiv.org/abs/2409.12472)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>With the development of artificial intelligence, neural networks play a key role in network intrusion detection systems (NIDS). Despite the tremendous advantages, neural networks are susceptible to adversarial attacks. To improve the reliability of NIDS, many research has been conducted and plenty of solutions have been proposed. However, the existing solutions rarely consider the adversarial attacks against recurrent neural networks (RNN) with time steps, which would greatly affect the application of NIDS in real world. Therefore, we first propose a novel RNN adversarial attack model based on feature reconstruction called \textbf{T}emporal adversarial \textbf{E}xamples \textbf{A}ttack \textbf{M}odel \textbf{(TEAM)}, which applied to time series data and reveals the potential connection between adversarial and time steps in RNN. That is, the past adversarial examples within the same time steps can trigger further attacks on current or future original examples. Moreover, TEAM leverages Time Dilation (TD) to effectively mitigates the effect of temporal among adversarial examples within the same time steps. Experimental results show that in most attack categories, TEAM improves the misjudgment rate of NIDS on both black and white boxes, making the misjudgment rate reach more than 96.68%. Meanwhile, the maximum increase in the misjudgment rate of the NIDS for subsequent original samples exceeds 95.57%.</li>
</ul>

<h3>Title: CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling Acceleration in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Junlin Lv, Yuan Feng, Xike Xie, Xin Jia, Qirong Peng, Guiming Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12490">https://arxiv.org/abs/2409.12490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12490">https://arxiv.org/pdf/2409.12490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12490]] CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling Acceleration in LLMs(https://arxiv.org/abs/2409.12490)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have achieved notable success across various domains, yet efficient inference is still limited by the quadratic computation complexity of the attention mechanism. The inference consists of prefilling and decoding phases. Although several attempts have been made to accelerate decoding, the inefficiency of the prefilling phase, especially for long-context tasks, remains a challenge. In this paper, we observe a locality in query criticality during the prefilling phase of long-context processing: adjacent query tokens tend to focus on similar subsets of the past Key-Value (KV) cache. Based on this observation, we propose CritiPrefill, a criticality-based segment-wise prefilling method. This method partitions the input sequence's queries and KV cache into segments and blocks, utilizing a segment-wise algorithm to estimate the query criticality. By pruning non-critical computations between query segments and cache blocks in the self-attention mechanism, the prefilling process can be significantly accelerated. Extensive evaluations on multiple long-context datasets show up to 2.7x speedup on Llama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100 GPU, with minimal quality degradation.</li>
</ul>

<h3>Title: End-to-end Open-vocabulary Video Visual Relationship Detection using Multi-modal Prompting</h3>
<ul>
<li><strong>Authors: </strong>Yongqi Wang, Shuo Yang, Xinxiao Wu, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12499">https://arxiv.org/abs/2409.12499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12499">https://arxiv.org/pdf/2409.12499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12499]] End-to-end Open-vocabulary Video Visual Relationship Detection using Multi-modal Prompting(https://arxiv.org/abs/2409.12499)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Open-vocabulary video visual relationship detection aims to expand video visual relationship detection beyond annotated categories by detecting unseen relationships between both seen and unseen objects in videos. Existing methods usually use trajectory detectors trained on closed datasets to detect object trajectories, and then feed these trajectories into large-scale pre-trained vision-language models to achieve open-vocabulary classification. Such heavy dependence on the pre-trained trajectory detectors limits their ability to generalize to novel object categories, leading to performance degradation. To address this challenge, we propose to unify object trajectory detection and relationship classification into an end-to-end open-vocabulary framework. Under this framework, we propose a relationship-aware open-vocabulary trajectory detector. It primarily consists of a query-based Transformer decoder, where the visual encoder of CLIP is distilled for frame-wise open-vocabulary object detection, and a trajectory associator. To exploit relationship context during trajectory detection, a relationship query is embedded into the Transformer decoder, and accordingly, an auxiliary relationship loss is designed to enable the decoder to perceive the relationships between objects explicitly. Moreover, we propose an open-vocabulary relationship classifier that leverages the rich semantic knowledge of CLIP to discover novel relationships. To adapt CLIP well to relationship classification, we design a multi-modal prompting method that employs spatio-temporal visual prompting for visual representation and vision-guided language prompting for language input. Extensive experiments on two public datasets, VidVRD and VidOR, demonstrate the effectiveness of our framework. Our framework is also applied to a more difficult cross-dataset scenario to further demonstrate its generalization ability.</li>
</ul>

<h3>Title: LLMR: Knowledge Distillation with a Large Language Model-Induced Reward</h3>
<ul>
<li><strong>Authors: </strong>Dongheng Li, Yongchang Hao, Lili Mou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12500">https://arxiv.org/abs/2409.12500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12500">https://arxiv.org/pdf/2409.12500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12500]] LLMR: Knowledge Distillation with a Large Language Model-Induced Reward(https://arxiv.org/abs/2409.12500)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have become increasingly popular and demonstrated remarkable performance in various natural language processing (NLP) tasks. However, these models are typically computationally expensive and difficult to be deployed in resource-constrained environments. In this paper, we propose LLMR, a novel knowledge distillation (KD) method based on a reward function induced from large language models. We conducted experiments on multiple datasets in the dialogue generation and summarization tasks. Empirical results demonstrate that our LLMR approach consistently outperforms traditional KD methods in different tasks and datasets.</li>
</ul>

<h3>Title: Towards Low-latency Event-based Visual Recognition with Hybrid Step-wise Distillation Spiking Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Xian Zhong, Shengwang Hu, Wenxuan Liu, Wenxin Huang, Jianhao Ding, Zhaofei Yu, Tiejun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12507">https://arxiv.org/abs/2409.12507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12507">https://arxiv.org/pdf/2409.12507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12507]] Towards Low-latency Event-based Visual Recognition with Hybrid Step-wise Distillation Spiking Neural Networks(https://arxiv.org/abs/2409.12507)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Spiking neural networks (SNNs) have garnered significant attention for their low power consumption and high biological interpretability. Their rich spatio-temporal information processing capability and event-driven nature make them ideally well-suited for neuromorphic datasets. However, current SNNs struggle to balance accuracy and latency in classifying these datasets. In this paper, we propose Hybrid Step-wise Distillation (HSD) method, tailored for neuromorphic datasets, to mitigate the notable decline in performance at lower time steps. Our work disentangles the dependency between the number of event frames and the time steps of SNNs, utilizing more event frames during the training stage to improve performance, while using fewer event frames during the inference stage to reduce latency. Nevertheless, the average output of SNNs across all time steps is susceptible to individual time step with abnormal outputs, particularly at extremely low time steps. To tackle this issue, we implement Step-wise Knowledge Distillation (SKD) module that considers variations in the output distribution of SNNs at each time step. Empirical evidence demonstrates that our method yields competitive performance in classification tasks on neuromorphic datasets, especially at lower time steps. Our code will be available at: {this https URL}.</li>
</ul>

<h3>Title: Scaling FP8 training to trillion-token LLMs</h3>
<ul>
<li><strong>Authors: </strong>Maxim Fishman, Brian Chmiel, Ron Banner, Daniel Soudry</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12517">https://arxiv.org/abs/2409.12517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12517">https://arxiv.org/pdf/2409.12517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12517]] Scaling FP8 training to trillion-token LLMs(https://arxiv.org/abs/2409.12517)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens -- a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter durations. We trace these instabilities to outlier amplification by the SwiGLU activation function. Interestingly, we show, both analytically and empirically, that this amplification happens only over prolonged training periods, and link it to a SwiGLU weight alignment process. To address this newly identified issue, we introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training without altering function behavior. We also demonstrate, for the first time, FP8 quantization of both Adam optimizer moments. Combining these innovations, we successfully train a 7B parameter model using FP8 precision on 256 Intel Gaudi2 accelerators, achieving on-par results with the BF16 baseline while delivering up to a $\sim 34 \%$ throughput improvement.</li>
</ul>

<h3>Title: Prompting Segment Anything Model with Domain-Adaptive Prototype for Generalizable Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhikai Wei, Wenhui Dong, Peilin Zhou, Yuliang Gu, Zhou Zhao, Yongchao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12522">https://arxiv.org/abs/2409.12522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12522">https://arxiv.org/pdf/2409.12522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12522]] Prompting Segment Anything Model with Domain-Adaptive Prototype for Generalizable Medical Image Segmentation(https://arxiv.org/abs/2409.12522)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning based methods often suffer from performance degradation caused by domain shift. In recent years, many sophisticated network structures have been designed to tackle this problem. However, the advent of large model trained on massive data, with its exceptional segmentation capability, introduces a new perspective for solving medical segmentation problems. In this paper, we propose a novel Domain-Adaptive Prompt framework for fine-tuning the Segment Anything Model (termed as DAPSAM) to address single-source domain generalization (SDG) in segmenting medical images. DAPSAM not only utilizes a more generalization-friendly adapter to fine-tune the large model, but also introduces a self-learning prototype-based prompt generator to enhance model's generalization ability. Specifically, we first merge the important low-level features into intermediate features before feeding to each adapter, followed by an attention filter to remove redundant information. This yields more robust image embeddings. Then, we propose using a learnable memory bank to construct domain-adaptive prototypes for prompt generation, helping to achieve generalizable medical image segmentation. Extensive experimental results demonstrate that our DAPSAM achieves state-of-the-art performance on two SDG medical image segmentation tasks with different modalities. The code is available at this https URL.</li>
</ul>

<h3>Title: Denoising Reuse: Exploiting Inter-frame Motion Consistency for Efficient Video Latent Generation</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Wang, Shuo Yan, Yixuan Chen, Yujiang Wang, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Robert P. Dick, Qin Lv, Fan Yang, Tun Lu, Ning Gu, Li Shang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12532">https://arxiv.org/abs/2409.12532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12532">https://arxiv.org/pdf/2409.12532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12532]] Denoising Reuse: Exploiting Inter-frame Motion Consistency for Efficient Video Latent Generation(https://arxiv.org/abs/2409.12532)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video generation using diffusion-based models is constrained by high computational costs due to the frame-wise iterative diffusion process. This work presents a Diffusion Reuse MOtion (Dr. Mo) network to accelerate latent video generation. Our key discovery is that coarse-grained noises in earlier denoising steps have demonstrated high motion consistency across consecutive video frames. Following this observation, Dr. Mo propagates those coarse-grained noises onto the next frame by incorporating carefully designed, lightweight inter-frame motions, eliminating massive computational redundancy in frame-wise diffusion models. The more sensitive and fine-grained noises are still acquired via later denoising steps, which can be essential to retain visual qualities. As such, deciding which intermediate steps should switch from motion-based propagations to denoising can be a crucial problem and a key tradeoff between efficiency and quality. Dr. Mo employs a meta-network named Denoising Step Selector (DSS) to dynamically determine desirable intermediate steps across video frames. Extensive evaluations on video generation and editing tasks have shown that Dr. Mo can substantially accelerate diffusion models in video tasks with improved visual qualities.</li>
</ul>

<h3>Title: Deep Probability Segmentation: Are segmentation models probability estimators?</h3>
<ul>
<li><strong>Authors: </strong>Simone Fassio, Simone Monaco, Daniele Apiletti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12535">https://arxiv.org/abs/2409.12535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12535">https://arxiv.org/pdf/2409.12535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12535]] Deep Probability Segmentation: Are segmentation models probability estimators?(https://arxiv.org/abs/2409.12535)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning has revolutionized various fields by enabling highly accurate predictions and estimates. One important application is probabilistic prediction, where models estimate the probability of events rather than deterministic outcomes. This approach is particularly relevant and, therefore, still unexplored for segmentation tasks where each pixel in an image needs to be classified. Conventional models often overlook the probabilistic nature of labels, but accurate uncertainty estimation is crucial for improving the reliability and applicability of models. In this study, we applied Calibrated Probability Estimation (CaPE) to segmentation tasks to evaluate its impact on model calibration. Our results indicate that while CaPE improves calibration, its effect is less pronounced compared to classification tasks, suggesting that segmentation models can inherently provide better probability estimates. We also investigated the influence of dataset size and bin optimization on the effectiveness of calibration. Our results emphasize the expressive power of segmentation models as probability estimators and incorporate probabilistic reasoning, which is crucial for applications requiring precise uncertainty quantification.</li>
</ul>

<h3>Title: Improving Cone-Beam CT Image Quality with Knowledge Distillation-Enhanced Diffusion Model in Imbalanced Data Settings</h3>
<ul>
<li><strong>Authors: </strong>Joonil Hwang, Sangjoon Park, NaHyeon Park, Seungryong Cho, Jin Sung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12539">https://arxiv.org/abs/2409.12539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12539">https://arxiv.org/pdf/2409.12539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12539]] Improving Cone-Beam CT Image Quality with Knowledge Distillation-Enhanced Diffusion Model in Imbalanced Data Settings(https://arxiv.org/abs/2409.12539)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In radiation therapy (RT), the reliance on pre-treatment computed tomography (CT) images encounter challenges due to anatomical changes, necessitating adaptive planning. Daily cone-beam CT (CBCT) imaging, pivotal for therapy adjustment, falls short in tissue density accuracy. To address this, our innovative approach integrates diffusion models for CT image generation, offering precise control over data synthesis. Leveraging a self-training method with knowledge distillation, we maximize CBCT data during therapy, complemented by sparse paired fan-beam CTs. This strategy, incorporated into state-of-the-art diffusion-based models, surpasses conventional methods like Pix2pix and CycleGAN. A meticulously curated dataset of 2800 paired CBCT and CT scans, supplemented by 4200 CBCT scans, undergoes preprocessing and teacher model training, including the Brownian Bridge Diffusion Model (BBDM). Pseudo-label CT images are generated, resulting in a dataset combining 5600 CT images with corresponding CBCT images. Thorough evaluation using MSE, SSIM, PSNR and LPIPS demonstrates superior performance against Pix2pix and CycleGAN. Our approach shows promise in generating high-quality CT images from CBCT scans in RT.</li>
</ul>

<h3>Title: Profiling Patient Transcript Using Large Language Model Reasoning Augmentation for Alzheimer's Disease Detection</h3>
<ul>
<li><strong>Authors: </strong>Chin-Po Chen, Jeng-Lin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12541">https://arxiv.org/abs/2409.12541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12541">https://arxiv.org/pdf/2409.12541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12541]] Profiling Patient Transcript Using Large Language Model Reasoning Augmentation for Alzheimer's Disease Detection(https://arxiv.org/abs/2409.12541)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) stands as the predominant cause of dementia, characterized by a gradual decline in speech and language capabilities. Recent deep-learning advancements have facilitated automated AD detection through spontaneous speech. However, common transcript-based detection methods directly model text patterns in each utterance without a global view of the patient's linguistic characteristics, resulting in limited discriminability and interpretability. Despite the enhanced reasoning abilities of large language models (LLMs), there remains a gap in fully harnessing the reasoning ability to facilitate AD detection and model interpretation. Therefore, we propose a patient-level transcript profiling framework leveraging LLM-based reasoning augmentation to systematically elicit linguistic deficit attributes. The summarized embeddings of the attributes are integrated into an Albert model for AD detection. The framework achieves 8.51\% ACC and 8.34\% F1 improvements on the ADReSS dataset compared to the baseline without reasoning augmentation. Our further analysis shows the effectiveness of our identified linguistic deficit attributes and the potential to use LLM for AD detection interpretation.</li>
</ul>

<h3>Title: Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Peng, Jiajun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12545">https://arxiv.org/abs/2409.12545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12545">https://arxiv.org/pdf/2409.12545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12545]] Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment(https://arxiv.org/abs/2409.12545)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is an effective model compression method that can transfer the internal capabilities of large language models (LLMs) to smaller ones. However, the multi-modal probability distribution predicted by teacher LLMs causes difficulties for student models to learn. In this paper, we first demonstrate the importance of multi-modal distribution alignment with experiments and then highlight the inefficiency of existing KD approaches in learning multi-modal distributions. To address this problem, we propose Ranking Loss based Knowledge Distillation (RLKD), which encourages the consistency of the ranking of peak predictions between the teacher and student models. By incorporating word-level ranking loss, we ensure excellent compatibility with existing distillation objectives while fully leveraging the fine-grained information between different categories in peaks of two predicted distribution. Experimental results demonstrate that our method enables the student model to better learn the multi-modal distributions of the teacher model, leading to a significant performance improvement in various downstream tasks.</li>
</ul>

<h3>Title: Hidden in Plain Sound: Environmental Backdoor Poisoning Attacks on Whisper, and Mitigations</h3>
<ul>
<li><strong>Authors: </strong>Jonatan Bartolini, Todor Stoyanov, Alberto Giaretta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12553">https://arxiv.org/abs/2409.12553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12553">https://arxiv.org/pdf/2409.12553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12553]] Hidden in Plain Sound: Environmental Backdoor Poisoning Attacks on Whisper, and Mitigations(https://arxiv.org/abs/2409.12553)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, transformer</a></li>
<li><strong>Abstract: </strong>Thanks to the popularisation of transformer-based models, speech recognition (SR) is gaining traction in various application fields, such as industrial and robotics environments populated with mission-critical devices. While transformer-based SR can provide various benefits for simplifying human-machine interfacing, the research on the cybersecurity aspects of these models is lacklustre. In particular, concerning backdoor poisoning attacks. In this paper, we propose a new poisoning approach that maps different environmental trigger sounds to target phrases of different lengths, during the fine-tuning phase. We test our approach on Whisper, one of the most popular transformer-based SR model, showing that it is highly vulnerable to our attack, under several testing conditions. To mitigate the attack proposed in this paper, we investigate the use of Silero VAD, a state-of-the-art voice activity detection (VAD) model, as a defence mechanism. Our experiments show that it is possible to use VAD models to filter out malicious triggers and mitigate our attacks, with a varying degree of success, depending on the type of trigger sound and testing conditions.</li>
</ul>

<h3>Title: RAD-Bench: Evaluating Large Language Models Capabilities in Retrieval Augmented Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Lin Kuo, Feng-Ting Liao, Mu-Wei Hsieh, Fu-Chieh Chang, Po-Chun Hsu, Da-Shan Shiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12558">https://arxiv.org/abs/2409.12558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12558">https://arxiv.org/pdf/2409.12558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12558]] RAD-Bench: Evaluating Large Language Models Capabilities in Retrieval Augmented Dialogues(https://arxiv.org/abs/2409.12558)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In real-world applications with Large Language Models (LLMs), external retrieval mechanisms - such as Search-Augmented Generation (SAG), tool utilization, and Retrieval-Augmented Generation (RAG) - are often employed to enhance the quality of augmented generations in dialogues. These approaches often come with multi-turn dialogue, where each interaction is enriched by relevant information retrieved from external sources. Existing benchmarks either assess LLMs' chat abilities in multi-turn dialogues or their use of retrieval for augmented responses in single-turn settings. However, there is a gap in evaluating LLMs' ability to leverage retrieval for more precise responses across multiple turns. To address this limitation, we introduce RAD-Bench (Retrieval Augmented Dialogue), a benchmark designed to evaluate LLMs' capabilities in multi-turn dialogues following retrievals, essential for their deployment in context-rich applications. RAD-Bench evaluates two key abilities of LLMs: Retrieval Synthesis and Retrieval Reasoning. These are measured using discriminative questions and retrieved contexts, and corresponding reference answers, assessing how effectively LLMs integrate and reason with context to maintain and enhance conversation quality over multiple turns. Our evaluation results on commonly used LLMs reveal that model performance deteriorates as additional layers of conditions or constraints are applied across conversation turns, even when accurate retrieved contexts are provided.</li>
</ul>

<h3>Title: InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, Quanzeng You</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12568">https://arxiv.org/abs/2409.12568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12568">https://arxiv.org/pdf/2409.12568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12568]] InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning(https://arxiv.org/abs/2409.12568)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Pre-training on large-scale, high-quality datasets is crucial for enhancing the reasoning capabilities of Large Language Models (LLMs), especially in specialized domains such as mathematics. Despite the recognized importance, the Multimodal LLMs (MLLMs) field currently lacks a comprehensive open-source pre-training dataset specifically designed for mathematical reasoning. To address this gap, we introduce InfiMM-WebMath-40B, a high-quality dataset of interleaved image-text documents. It comprises 24 million web pages, 85 million associated image URLs, and 40 billion text tokens, all meticulously extracted and filtered from CommonCrawl. We provide a detailed overview of our data collection and processing pipeline. To demonstrate the robustness of InfiMM-WebMath-40B, we conducted evaluations in both text-only and multimodal settings. Our evaluations on text-only benchmarks show that, despite utilizing only 40 billion tokens, our dataset significantly enhances the performance of our 1.3B model, delivering results comparable to DeepSeekMath-1.3B, which uses 120 billion tokens for the same model size. Nevertheless, with the introduction of our multi-modal math pre-training dataset, our models set a new state-of-the-art among open-source models on multi-modal math benchmarks such as MathVerse and We-Math. We release our data at this https URL.</li>
</ul>

<h3>Title: Deep Transfer Hashing for Adaptive Learning on Federated Streaming Data</h3>
<ul>
<li><strong>Authors: </strong>Manuel Röder, Frank-Michael Schleif</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12575">https://arxiv.org/abs/2409.12575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12575">https://arxiv.org/pdf/2409.12575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12575]] Deep Transfer Hashing for Adaptive Learning on Federated Streaming Data(https://arxiv.org/abs/2409.12575)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>This extended abstract explores the integration of federated learning with deep transfer hashing for distributed prediction tasks, emphasizing resource-efficient client training from evolving data streams. Federated learning allows multiple clients to collaboratively train a shared model while maintaining data privacy - by incorporating deep transfer hashing, high-dimensional data can be converted into compact hash codes, reducing data transmission size and network loads. The proposed framework utilizes transfer learning, pre-training deep neural networks on a central server, and fine-tuning on clients to enhance model accuracy and adaptability. A selective hash code sharing mechanism using a privacy-preserving global memory bank further supports client fine-tuning. This approach addresses challenges in previous research by improving computational efficiency and scalability. Practical applications include Car2X event predictions, where a shared model is collectively trained to recognize traffic patterns, aiding in tasks such as traffic density assessment and accident detection. The research aims to develop a robust framework that combines federated learning, deep transfer hashing and transfer learning for efficient and secure downstream task execution.</li>
</ul>

<h3>Title: StoryMaker: Towards Holistic Consistent Characters in Text-to-image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhengguang Zhou, Jing Li, Huaxia Li, Nemo Chen, Xu Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12576">https://arxiv.org/abs/2409.12576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12576">https://arxiv.org/pdf/2409.12576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12576]] StoryMaker: Towards Holistic Consistent Characters in Text-to-image Generation(https://arxiv.org/abs/2409.12576)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tuning-free personalized image generation methods have achieved significant success in maintaining facial consistency, i.e., identities, even with multiple characters. However, the lack of holistic consistency in scenes with multiple characters hampers these methods' ability to create a cohesive narrative. In this paper, we introduce StoryMaker, a personalization solution that preserves not only facial consistency but also clothing, hairstyles, and body consistency, thus facilitating the creation of a story through a series of images. StoryMaker incorporates conditions based on face identities and cropped character images, which include clothing, hairstyles, and bodies. Specifically, we integrate the facial identity information with the cropped character images using the Positional-aware Perceiver Resampler (PPR) to obtain distinct character features. To prevent intermingling of multiple characters and the background, we separately constrain the cross-attention impact regions of different characters and the background using MSE loss with segmentation masks. Additionally, we train the generation network conditioned on poses to promote decoupling from poses. A LoRA is also employed to enhance fidelity and quality. Experiments underscore the effectiveness of our approach. StoryMaker supports numerous applications and is compatible with other societal plug-ins. Our source codes and model weights are available at this https URL.</li>
</ul>

<h3>Title: LLMs Can Check Their Own Results to Mitigate Hallucinations in Traffic Understanding Tasks</h3>
<ul>
<li><strong>Authors: </strong>Malsha Ashani Mahawatta Dona, Beatriz Cabrero-Daniel, Yinan Yu, Christian Berger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12580">https://arxiv.org/abs/2409.12580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12580">https://arxiv.org/pdf/2409.12580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12580]] LLMs Can Check Their Own Results to Mitigate Hallucinations in Traffic Understanding Tasks(https://arxiv.org/abs/2409.12580)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Today's Large Language Models (LLMs) have showcased exemplary capabilities, ranging from simple text generation to advanced image processing. Such models are currently being explored for in-vehicle services such as supporting perception tasks in Advanced Driver Assistance Systems (ADAS) or Autonomous Driving (AD) systems, given the LLMs' capabilities to process multi-modal data. However, LLMs often generate nonsensical or unfaithful information, known as ``hallucinations'': a notable issue that needs to be mitigated. In this paper, we systematically explore the adoption of SelfCheckGPT to spot hallucinations by three state-of-the-art LLMs (GPT-4o, LLaVA, and Llama3) when analysing visual automotive data from two sources: Waymo Open Dataset, from the US, and PREPER CITY dataset, from Sweden. Our results show that GPT-4o is better at generating faithful image captions than LLaVA, whereas the former demonstrated leniency in mislabeling non-hallucinated content as hallucinations compared to the latter. Furthermore, the analysis of the performance metrics revealed that the dataset type (Waymo or PREPER CITY) did not significantly affect the quality of the captions or the effectiveness of hallucination detection. However, the models showed better performance rates over images captured during daytime, compared to during dawn, dusk or night. Overall, the results show that SelfCheckGPT and its adaptation can be used to filter hallucinations in generated traffic-related image captions for state-of-the-art LLMs.</li>
</ul>

<h3>Title: Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Ballout, Ulf Krumnack, Gunther Heidemann, Kai-Uwe Kühnberger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12586">https://arxiv.org/abs/2409.12586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12586">https://arxiv.org/pdf/2409.12586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12586]] Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights(https://arxiv.org/abs/2409.12586)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enhancing small language models for real-life application deployment is a significant challenge facing the research community. Due to the difficulties and costs of using large language models, researchers are seeking ways to effectively deploy task-specific small models. In this work, we introduce a simple yet effective knowledge distillation method to improve the performance of small language models. Our approach utilizes a teacher model with approximately 3 billion parameters to identify the most influential tokens in its decision-making process. These tokens are extracted from the input based on their attribution scores relative to the output, using methods like saliency maps. These important tokens are then provided as rationales to a student model, aiming to distill the knowledge of the teacher model. This method has proven to be effective, as demonstrated by testing it on four diverse datasets, where it shows improvement over both standard fine-tuning methods and state-of-the-art knowledge distillation models. Furthermore, we explore explanations of the success of the model by analyzing the important tokens extracted from the teacher model. Our findings reveal that in 68\% of cases, specifically in datasets where labels are part of the answer, such as multiple-choice questions, the extracted tokens are part of the ground truth.</li>
</ul>

<h3>Title: LARE: Latent Augmentation using Regional Embedding with Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Kosuke Sakurai, Tatsuya Ishii, Ryotaro Shimizu, Linxin Song, Masayuki Goto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12597">https://arxiv.org/abs/2409.12597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12597">https://arxiv.org/pdf/2409.12597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12597]] LARE: Latent Augmentation using Regional Embedding with Vision-Language Model(https://arxiv.org/abs/2409.12597)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, considerable research has been conducted on vision-language models that handle both image and text data; these models are being applied to diverse downstream tasks, such as "image-related chat," "image recognition by instruction," and "answering visual questions." Vision-language models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), are also high-performance image classifiers that are being developed into domain adaptation methods that can utilize language information to extend into unseen domains. However, because these VLMs embed images as a single point in a unified embedding space, there is room for improvement in the classification accuracy. Therefore, in this study, we proposed the Latent Augmentation using Regional Embedding (LARE), which embeds the image as a region in the unified embedding space learned by the VLM. By sampling the augmented image embeddings from within this latent region, LARE enables data augmentation to various unseen domains, not just to specific unseen domains. LARE achieves robust image classification for domains in and out using augmented image embeddings to fine-tune VLMs. We demonstrate that LARE outperforms previous fine-tuning models in terms of image classification accuracy on three benchmarks. We also demonstrate that LARE is a more robust and general model that is valid under multiple conditions, such as unseen domains, small amounts of data, and imbalanced data.</li>
</ul>

<h3>Title: Enhancing SLM via ChatGPT and Dataset Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Tom Pieper, Mohamad Ballout, Ulf Krumnack, Gunther Heidemann, Kai-Uwe Kühnberger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12599">https://arxiv.org/abs/2409.12599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12599">https://arxiv.org/pdf/2409.12599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12599]] Enhancing SLM via ChatGPT and Dataset Augmentation(https://arxiv.org/abs/2409.12599)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the enhancement of small language models through strategic dataset augmentation via ChatGPT-3.5-Turbo, in the domain of Natural Language Inference (NLI). By employing knowledge distillation-based techniques and synthetic dataset augmentation, we aim to bridge the performance gap between large language models (LLMs) and small language models (SLMs) without the immense cost of human annotation. Our methods involve two forms of rationale generation--information extraction and informed reasoning--to enrich the ANLI dataset. We then fine-tune T5-Small on these augmented datasets, evaluating its performance against an established benchmark. Our findings reveal that the incorporation of synthetic rationales significantly improves the model's ability to comprehend natural language, leading to 1.3\% and 2.3\% higher classification accuracy, respectively, on the ANLI dataset, demonstrating the potential of leveraging LLMs for dataset augmentation. This approach not only enhances the performance of smaller models on complex tasks but also introduces a cost-effective method for fine-tuning smaller language models. By advancing our understanding of knowledge distillation and fine-tuning strategies, this work contributes to the ongoing effort to create more capable and efficient NLP systems.</li>
</ul>

<h3>Title: CF-GO-Net: A Universal Distribution Learner via Characteristic Function Networks with Graph Optimizers</h3>
<ul>
<li><strong>Authors: </strong>Zeyang Yu, Shengxi Li, Danilo Mandic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12610">https://arxiv.org/abs/2409.12610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12610">https://arxiv.org/pdf/2409.12610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12610]] CF-GO-Net: A Universal Distribution Learner via Characteristic Function Networks with Graph Optimizers(https://arxiv.org/abs/2409.12610)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Generative models aim to learn the distribution of datasets, such as images, so as to be able to generate samples that statistically resemble real data. However, learning the underlying probability distribution can be very challenging and intractable. To this end, we introduce an approach which employs the characteristic function (CF), a probabilistic descriptor that directly corresponds to the distribution. However, unlike the probability density function (pdf), the characteristic function not only always exists, but also provides an additional degree of freedom, hence enhances flexibility in learning distributions. This removes the critical dependence on pdf-based assumptions, which limit the applicability of traditional methods. While several works have attempted to use CF in generative modeling, they often impose strong constraints on the training process. In contrast, our approach calculates the distance between query points in the CF domain, which is an unconstrained and well defined problem. Next, to deal with the sampling strategy, which is crucial to model performance, we propose a graph neural network (GNN)-based optimizer for the sampling process, which identifies regions where the difference between CFs is most significant. In addition, our method allows the use of a pre-trained model, such as a well-trained autoencoder, and is capable of learning directly in its feature space, without modifying its parameters. This offers a flexible and robust approach to generative modeling, not only provides broader applicability and improved performance, but also equips any latent space world with the ability to become a generative model.</li>
</ul>

<h3>Title: Enhancing Perception of Key Changes in Remote Sensing Image Change Captioning</h3>
<ul>
<li><strong>Authors: </strong>Cong Yang, Zuchao Li, Hongzan Jiao, Zhi Gao, Lefei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12612">https://arxiv.org/abs/2409.12612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12612">https://arxiv.org/pdf/2409.12612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12612]] Enhancing Perception of Key Changes in Remote Sensing Image Change Captioning(https://arxiv.org/abs/2409.12612)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, while significant progress has been made in remote sensing image change captioning, existing methods fail to filter out areas unrelated to actual changes, making models susceptible to irrelevant features. In this article, we propose a novel multimodal framework for remote sensing image change captioning, guided by Key Change Features and Instruction-tuned (KCFI). This framework aims to fully leverage the intrinsic knowledge of large language models through visual instructions and enhance the effectiveness and accuracy of change features using pixel-level change detection tasks. Specifically, KCFI includes a ViTs encoder for extracting bi-temporal remote sensing image features, a key feature perceiver for identifying critical change areas, a pixel-level change detection decoder to constrain key change features, and an instruction-tuned decoder based on a large language model. Moreover, to ensure that change description and change detection tasks are jointly optimized, we employ a dynamic weight-averaging strategy to balance the losses between the two tasks. We also explore various feature combinations for visual fine-tuning instructions and demonstrate that using only key change features to guide the large language model is the optimal choice. To validate the effectiveness of our approach, we compare it against several state-of-the-art change captioning methods on the LEVIR-CC dataset, achieving the best performance. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Santosh Kumar Radha, Yasamin Nouri Jelyani, Ara Ghukasyan, Oktay Goktas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12618">https://arxiv.org/abs/2409.12618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12618">https://arxiv.org/pdf/2409.12618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12618]] Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning(https://arxiv.org/abs/2409.12618)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating "thought"-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.</li>
</ul>

<h3>Title: CamelEval: Advancing Culturally Aligned Arabic Language Models and Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Zhaozhi Qian, Faroq Altam, Muhammad Saleh Saeed Alqurishi, Riad Souissi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12623">https://arxiv.org/abs/2409.12623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12623">https://arxiv.org/pdf/2409.12623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12623]] CamelEval: Advancing Culturally Aligned Arabic Language Models and Benchmarks(https://arxiv.org/abs/2409.12623)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are the cornerstones of modern artificial intelligence systems. This paper introduces Juhaina, a Arabic-English bilingual LLM specifically designed to align with the values and preferences of Arabic speakers. Juhaina inherently supports advanced functionalities such as instruction following, open-ended question answering, information provisioning, and text processing. Our model contains 9.24 billion parameters and is trained on a context window of up to 8,192 tokens. This paper details the creation process of Juhaina and provides an extensive empirical evaluation. Furthermore, we identify the limitations of widely-adopted Open Arabic LLM Leaderboard (OALL) and propose a new evaluation benchmark, CamelEval. Our findings demonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as the Llama and Gemma families, in generating helpful responses in Arabic, providing factually accurate information about the region, and understanding nuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI technologies, serving over 400 million Arabic speakers by offering LLMs that not only communicate in their language but also comprehend their culture. We publicly release all models on Huggingface \url{this https URL}.</li>
</ul>

<h3>Title: Green Federated Learning: A new era of Green Aware AI</h3>
<ul>
<li><strong>Authors: </strong>Dipanwita Thakur, Antonella Guzzo, Giancarlo Fortino</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12626">https://arxiv.org/abs/2409.12626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12626">https://arxiv.org/pdf/2409.12626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12626]] Green Federated Learning: A new era of Green Aware AI(https://arxiv.org/abs/2409.12626)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The development of AI applications, especially in large-scale wireless networks, is growing exponentially, alongside the size and complexity of the architectures used. Particularly, machine learning is acknowledged as one of today's most energy-intensive computational applications, posing a significant challenge to the environmental sustainability of next-generation intelligent systems. Achieving environmental sustainability entails ensuring that every AI algorithm is designed with sustainability in mind, integrating green considerations from the architectural phase onwards. Recently, Federated Learning (FL), with its distributed nature, presents new opportunities to address this need. Hence, it's imperative to elucidate the potential and challenges stemming from recent FL advancements and their implications for sustainability. Moreover, it's crucial to furnish researchers, stakeholders, and interested parties with a roadmap to navigate and understand existing efforts and gaps in green-aware AI algorithms. This survey primarily aims to achieve this objective by identifying and analyzing over a hundred FL works, assessing their contributions to green-aware artificial intelligence for sustainable environments, with a specific focus on IoT research. It delves into current issues in green federated learning from an energy-efficient standpoint, discussing potential challenges and future prospects for green IoT application research.</li>
</ul>

<h3>Title: Counterfactual Explanations for Clustering Models</h3>
<ul>
<li><strong>Authors: </strong>Aurora Spagnol, Kacper Sokol, Pietro Barbiero, Marc Langheinrich, Martin Gjoreski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12632">https://arxiv.org/abs/2409.12632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12632">https://arxiv.org/pdf/2409.12632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12632]] Counterfactual Explanations for Clustering Models(https://arxiv.org/abs/2409.12632)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Clustering algorithms rely on complex optimisation processes that may be difficult to comprehend, especially for individuals who lack technical expertise. While many explainable artificial intelligence techniques exist for supervised machine learning, unsupervised learning -- and clustering in particular -- has been largely neglected. To complicate matters further, the notion of a ``true'' cluster is inherently challenging to define. These facets of unsupervised learning and its explainability make it difficult to foster trust in such methods and curtail their adoption. To address these challenges, we propose a new, model-agnostic technique for explaining clustering algorithms with counterfactual statements. Our approach relies on a novel soft-scoring method that captures the spatial information utilised by clustering models. It builds upon a state-of-the-art Bayesian counterfactual generator for supervised learning to deliver high-quality explanations. We evaluate its performance on five datasets and two clustering algorithms, and demonstrate that introducing soft scores to guide counterfactual search significantly improves the results.</li>
</ul>

<h3>Title: EFA-YOLO: An Efficient Feature Attention Model for Fire and Flame Detection</h3>
<ul>
<li><strong>Authors: </strong>Weichao Pan, Xu Wang, Wenqing Huan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12635">https://arxiv.org/abs/2409.12635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12635">https://arxiv.org/pdf/2409.12635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12635]] EFA-YOLO: An Efficient Feature Attention Model for Fire and Flame Detection(https://arxiv.org/abs/2409.12635)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>As a natural disaster with high suddenness and great destructiveness, fire has long posed a major threat to human society and ecological environment. In recent years, with the rapid development of smart city and Internet of Things (IoT) technologies, fire detection systems based on deep learning have gradually become a key means to cope with fire hazards. However, existing fire detection models still have many challenges in terms of detection accuracy and real-time performance in complex contexts. To address these issues, we propose two key modules: EAConv (Efficient Attention Convolution) and EADown (Efficient Attention Downsampling). The EAConv module significantly improves the feature extraction efficiency by combining an efficient attention mechanism with depth-separable convolution, while the EADown module enhances the accuracy and efficiency of feature downsampling by utilizing spatial and channel attention mechanisms in combination with pooling operations. Based on these two modules, we design an efficient and lightweight flame detection model, EFA-YOLO (Efficient Feature Attention YOLO). Experimental results show that EFA-YOLO has a model parameter quantity of only 1.4M, GFLOPs of 4.6, and the inference time per image on the CPU is only 22.19 ms. Compared with existing mainstream models (e.g., YOLOv5, YOLOv8, YOLOv9, and YOLOv10), EFA-YOLO exhibits a significant enhancement in detection accuracy (mAP) and inference speed, with model parameter amount is reduced by 94.6 and the inference speed is improved by 88 times.</li>
</ul>

<h3>Title: Image inpainting for corrupted images by using the semi-super resolution GAN</h3>
<ul>
<li><strong>Authors: </strong>Mehrshad Momen-Tayefeh, Mehrdad Momen-Tayefeh, Amir Ali Ghafourian Ghahramani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12636">https://arxiv.org/abs/2409.12636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12636">https://arxiv.org/pdf/2409.12636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12636]] Image inpainting for corrupted images by using the semi-super resolution GAN(https://arxiv.org/abs/2409.12636)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Image inpainting is a valuable technique for enhancing images that have been corrupted. The primary challenge in this research revolves around the extent of corruption in the input image that the deep learning model must restore. To address this challenge, we introduce a Generative Adversarial Network (GAN) for learning and replicating the missing pixels. Additionally, we have developed a distinct variant of the Super-Resolution GAN (SRGAN), which we refer to as the Semi-SRGAN (SSRGAN). Furthermore, we leveraged three diverse datasets to assess the robustness and accuracy of our proposed model. Our training process involves varying levels of pixel corruption to attain optimal accuracy and generate high-quality images.</li>
</ul>

<h3>Title: Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries</h3>
<ul>
<li><strong>Authors: </strong>Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, Rohan Anil, Ethan Dyer, Siamak Shakeri, Roopali Vij, Harsh Mehta, Vinay Ramasesh, Quoc Le, Ed Chi, Yifeng Lu, Orhan Firat, Angeliki Lazaridou, Jean-Baptiste Lespiau, Nithya Attaluri, Kate Olszewska</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12640">https://arxiv.org/abs/2409.12640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12640">https://arxiv.org/pdf/2409.12640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12640]] Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries(https://arxiv.org/abs/2409.12640)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Michelangelo: a minimal, synthetic, and unleaked long-context reasoning evaluation for large language models which is also easy to automatically score. This evaluation is derived via a novel, unifying framework for evaluations over arbitrarily long contexts which measure the model's ability to do more than retrieve a single piece of information from its context. The central idea of the \frameworkname framework (\frameworkshort) is to construct tasks which require a model to ``chisel away'' the irrelevant information in the context, revealing a latent structure in the context. To verify a model's understanding of this latent structure, we query the model for details of the structure. Using \frameworkshort, we produce three diagnostic long-context evaluations across code and natural-language domains intended to provide a stronger signal of long-context language model capabilities. We perform evaluations on several state-of-the-art models and demonstrate both that a) the proposed evaluations are high-signal and b) that there is significant room for improvement in synthesizing long-context information.</li>
</ul>

<h3>Title: Deep generative models as an adversarial attack strategy for tabular machine learning</h3>
<ul>
<li><strong>Authors: </strong>Salijona Dyrmishi, Mihaela Cătălina Stoian, Eleonora Giunchiglia, Maxime Cordy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12642">https://arxiv.org/abs/2409.12642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12642">https://arxiv.org/pdf/2409.12642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12642]] Deep generative models as an adversarial attack strategy for tabular machine learning(https://arxiv.org/abs/2409.12642)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Deep Generative Models (DGMs) have found application in computer vision for generating adversarial examples to test the robustness of machine learning (ML) systems. Extending these adversarial techniques to tabular ML presents unique challenges due to the distinct nature of tabular data and the necessity to preserve domain constraints in adversarial examples. In this paper, we adapt four popular tabular DGMs into adversarial DGMs (AdvDGMs) and evaluate their effectiveness in generating realistic adversarial examples that conform to domain constraints.</li>
</ul>

<h3>Title: Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards</h3>
<ul>
<li><strong>Authors: </strong>Furkan Şahinuç, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12656">https://arxiv.org/abs/2409.12656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12656">https://arxiv.org/pdf/2409.12656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12656]] Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards(https://arxiv.org/abs/2409.12656)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scientific leaderboards are standardized ranking systems that facilitate evaluating and comparing competitive methods. Typically, a leaderboard is defined by a task, dataset, and evaluation metric (TDM) triple, allowing objective performance assessment and fostering innovation through benchmarking. However, the exponential increase in publications has made it infeasible to construct and maintain these leaderboards manually. Automatic leaderboard construction has emerged as a solution to reduce manual labor. Existing datasets for this task are based on the community-contributed leaderboards without additional curation. Our analysis shows that a large portion of these leaderboards are incomplete, and some of them contain incorrect information. In this work, we present SciLead, a manually-curated Scientific Leaderboard dataset that overcomes the aforementioned problems. Building on this dataset, we propose three experimental settings that simulate real-world scenarios where TDM triples are fully defined, partially defined, or undefined during leaderboard construction. While previous research has only explored the first setting, the latter two are more representative of real-world applications. To address these diverse settings, we develop a comprehensive LLM-based framework for constructing leaderboards. Our experiments and analysis reveal that various LLMs often correctly identify TDM triples while struggling to extract result values from publications. We make our code and data publicly available.</li>
</ul>

<h3>Title: Exploring the topics, sentiments and hate speech in the Spanish information environment</h3>
<ul>
<li><strong>Authors: </strong>ALEJANDRO BUITRAGO LOPEZ, Javier Pastor-Galindo, José Antonio Ruipérez-Valiente</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12658">https://arxiv.org/abs/2409.12658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12658">https://arxiv.org/pdf/2409.12658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12658]] Exploring the topics, sentiments and hate speech in the Spanish information environment(https://arxiv.org/abs/2409.12658)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the digital era, the internet and social media have transformed communication but have also facilitated the spread of hate speech and disinformation, leading to radicalization, polarization, and toxicity. This is especially concerning for media outlets due to their significant role in shaping public discourse. This study examines the topics, sentiments, and hate prevalence in 337,807 response messages (website comments and tweets) to news from five Spanish media outlets (La Vanguardia, ABC, El País, El Mundo, and 20 Minutos) in January 2021. These public reactions were originally labeled as distinct types of hate by experts following an original procedure, and they are now classified into three sentiment values (negative, neutral, or positive) and main topics. The BERTopic unsupervised framework was used to extract 81 topics, manually named with the help of Large Language Models (LLMs) and grouped into nine primary categories. Results show social issues (22.22%), expressions and slang (20.35%), and political issues (11.80%) as the most discussed. Content is mainly negative (62.7%) and neutral (28.57%), with low positivity (8.73%). Toxic narratives relate to conversation expressions, gender, feminism, and COVID-19. Despite low levels of hate speech (3.98%), the study confirms high toxicity in online responses to social and political topics.</li>
</ul>

<h3>Title: Enhancing Construction Site Safety: A Lightweight Convolutional Network for Effective Helmet Detection</h3>
<ul>
<li><strong>Authors: </strong>Mujadded Al Rabbani Alif</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12669">https://arxiv.org/abs/2409.12669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12669">https://arxiv.org/pdf/2409.12669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12669]] Enhancing Construction Site Safety: A Lightweight Convolutional Network for Effective Helmet Detection(https://arxiv.org/abs/2409.12669)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>In the realm of construction safety, the detection of personal protective equipment, such as helmets, plays a critical role in preventing workplace injuries. This paper details the development and evaluation of convolutional neural networks (CNNs) designed for the accurate classification of helmet presence on construction sites. Initially, a simple CNN model comprising one convolutional block and one fully connected layer was developed, yielding modest results. To enhance its performance, the model was progressively refined, first by extending the architecture to include an additional convolutional block and a fully connected layer. Subsequently, batch normalization and dropout techniques were integrated, aiming to mitigate overfitting and improve the model's generalization capabilities. The performance of these models is methodically analyzed, revealing a peak F1-score of 84\%, precision of 82\%, and recall of 86\% with the most advanced configuration of the first study phase. Despite these improvements, the accuracy remained suboptimal, thus setting the stage for further architectural and operational enhancements. This work lays a foundational framework for ongoing adjustments and optimization in automated helmet detection technology, with future enhancements expected to address the limitations identified during these initial experiments.</li>
</ul>

<h3>Title: Text2Traj2Text: Learning-by-Synthesis Framework for Contextual Captioning of Human Movement Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Hikaru Asano, Ryo Yonetani, Taiki Sekii, Hiroki Ouchi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12670">https://arxiv.org/abs/2409.12670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12670">https://arxiv.org/pdf/2409.12670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12670]] Text2Traj2Text: Learning-by-Synthesis Framework for Contextual Captioning of Human Movement Trajectories(https://arxiv.org/abs/2409.12670)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents Text2Traj2Text, a novel learning-by-synthesis framework for captioning possible contexts behind shopper's trajectory data in retail stores. Our work will impact various retail applications that need better customer understanding, such as targeted advertising and inventory management. The key idea is leveraging large language models to synthesize a diverse and realistic collection of contextual captions as well as the corresponding movement trajectories on a store map. Despite learned from fully synthesized data, the captioning model can generalize well to trajectories/captions created by real human subjects. Our systematic evaluation confirmed the effectiveness of the proposed framework over competitive approaches in terms of ROUGE and BERT Score metrics.</li>
</ul>

<h3>Title: (Un)certainty of (Un)fairness: Preference-Based Selection of Certainly Fair Decision-Makers</h3>
<ul>
<li><strong>Authors: </strong>Manh Khoi Duong, Stefan Conrad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12677">https://arxiv.org/abs/2409.12677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12677">https://arxiv.org/pdf/2409.12677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12677]] (Un)certainty of (Un)fairness: Preference-Based Selection of Certainly Fair Decision-Makers(https://arxiv.org/abs/2409.12677)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness metrics are used to assess discrimination and bias in decision-making processes across various domains, including machine learning models and human decision-makers in real-world applications. This involves calculating the disparities between probabilistic outcomes among social groups, such as acceptance rates between male and female applicants. However, traditional fairness metrics do not account for the uncertainty in these processes and lack of comparability when two decision-makers exhibit the same disparity. Using Bayesian statistics, we quantify the uncertainty of the disparity to enhance discrimination assessments. We represent each decision-maker, whether a machine learning model or a human, by its disparity and the corresponding uncertainty in that disparity. We define preferences over decision-makers and utilize brute-force to choose the optimal decision-maker according to a utility function that ranks decision-makers based on these preferences. The decision-maker with the highest utility score can be interpreted as the one for whom we are most certain that it is fair.</li>
</ul>

<h3>Title: Semi-Supervised Semantic Segmentation with Professional and General Training</h3>
<ul>
<li><strong>Authors: </strong>Yuting Hong, Hui Xiao, Huazheng Hao, Xiaojie Qiu, Baochen Yao, Chengbin Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12680">https://arxiv.org/abs/2409.12680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12680">https://arxiv.org/pdf/2409.12680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12680]] Semi-Supervised Semantic Segmentation with Professional and General Training(https://arxiv.org/abs/2409.12680)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>With the advancement of convolutional neural networks, semantic segmentation has achieved remarkable progress. The training of such networks heavily relies on image annotations, which are very expensive to obtain. Semi-supervised learning can utilize both labeled data and unlabeled data with the help of pseudo-labels. However, in many real-world scenarios where classes are imbalanced, majority classes often play a dominant role during training and the learning quality of minority classes can be undermined. To overcome this limitation, we propose a synergistic training framework, including a professional training module to enhance minority class learning and a general training module to learn more comprehensive semantic information. Based on a pixel selection strategy, they can iteratively learn from each other to reduce error accumulation and coupling. In addition, a dual contrastive learning with anchors is proposed to guarantee more distinct decision boundaries. In experiments, our framework demonstrates superior performance compared to state-of-the-art methods on benchmark datasets.</li>
</ul>

<h3>Title: Connecting Ideas in 'Lower-Resource' Scenarios: NLP for National Varieties, Creoles and Other Low-resource Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Aditya Joshi, Diptesh Kanojia, Heather Lent, Hour Kaing, Haiyue Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12683">https://arxiv.org/abs/2409.12683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12683">https://arxiv.org/pdf/2409.12683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12683]] Connecting Ideas in 'Lower-Resource' Scenarios: NLP for National Varieties, Creoles and Other Low-resource Scenarios(https://arxiv.org/abs/2409.12683)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite excellent results on benchmarks over a small subset of languages, large language models struggle to process text from languages situated in `lower-resource' scenarios such as dialects/sociolects (national or social varieties of a language), Creoles (languages arising from linguistic contact between multiple languages) and other low-resource languages. This introductory tutorial will identify common challenges, approaches, and themes in natural language processing (NLP) research for confronting and overcoming the obstacles inherent to data-poor contexts. By connecting past ideas to the present field, this tutorial aims to ignite collaboration and cross-pollination between researchers working in these scenarios. Our notion of `lower-resource' broadly denotes the outstanding lack of data required for model training - and may be applied to scenarios apart from the three covered in the tutorial.</li>
</ul>

<h3>Title: A dynamic vision sensor object recognition model based on trainable event-driven convolution and spiking attention mechanism</h3>
<ul>
<li><strong>Authors: </strong>Peng Zheng, Qian Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12691">https://arxiv.org/abs/2409.12691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12691">https://arxiv.org/pdf/2409.12691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12691]] A dynamic vision sensor object recognition model based on trainable event-driven convolution and spiking attention mechanism(https://arxiv.org/abs/2409.12691)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) are well-suited for processing event streams from Dynamic Visual Sensors (DVSs) due to their use of sparse spike-based coding and asynchronous event-driven computation. To extract features from DVS objects, SNNs commonly use event-driven convolution with fixed kernel parameters. These filters respond strongly to features in specific orientations while disregarding others, leading to incomplete feature extraction. To improve the current event-driven convolution feature extraction capability of SNNs, we propose a DVS object recognition model that utilizes a trainable event-driven convolution and a spiking attention mechanism. The trainable event-driven convolution is proposed in this paper to update its convolution kernel through gradient descent. This method can extract local features of the event stream more efficiently than traditional event-driven convolution. Furthermore, the spiking attention mechanism is used to extract global dependence features. The classification performances of our model are better than the baseline methods on two neuromorphic datasets including MNIST-DVS and the more complex CIFAR10-DVS. Moreover, our model showed good classification ability for short event streams. It was shown that our model can improve the performance of event-driven convolutional SNNs for DVS objects.</li>
</ul>

<h3>Title: Exploring Large Language Models for Product Attribute Value Identification</h3>
<ul>
<li><strong>Authors: </strong>Kassem Sabeh, Mouna Kacimi, Johann Gamper, Robert Litschko, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12695">https://arxiv.org/abs/2409.12695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12695">https://arxiv.org/pdf/2409.12695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12695]] Exploring Large Language Models for Product Attribute Value Identification(https://arxiv.org/abs/2409.12695)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Product attribute value identification (PAVI) involves automatically identifying attributes and their values from product information, enabling features like product search, recommendation, and comparison. Existing methods primarily rely on fine-tuning pre-trained language models, such as BART and T5, which require extensive task-specific training data and struggle to generalize to new attributes. This paper explores large language models (LLMs), such as LLaMA and Mistral, as data-efficient and robust alternatives for PAVI. We propose various strategies: comparing one-step and two-step prompt-based approaches in zero-shot settings and utilizing parametric and non-parametric knowledge through in-context learning examples. We also introduce a dense demonstration retriever based on a pre-trained T5 model and perform instruction fine-tuning to explicitly train LLMs on task-specific instructions. Extensive experiments on two product benchmarks show that our two-step approach significantly improves performance in zero-shot settings, and instruction fine-tuning further boosts performance when using training data, demonstrating the practical benefits of using LLMs for PAVI.</li>
</ul>

<h3>Title: Generation and Editing of Mandrill Faces: Application to Sex Editing and Assessment</h3>
<ul>
<li><strong>Authors: </strong>Nicolas M. Dibot, Julien P. Renoult, William Puech</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12705">https://arxiv.org/abs/2409.12705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12705">https://arxiv.org/pdf/2409.12705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12705]] Generation and Editing of Mandrill Faces: Application to Sex Editing and Assessment(https://arxiv.org/abs/2409.12705)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI has seen major developments in recent years, enhancing the realism of synthetic images, also known as computer-generated images. In addition, generative AI has also made it possible to modify specific image characteristics through image editing. Previous work has developed methods based on generative adversarial networks (GAN) for generating realistic images, in particular faces, but also to modify specific features. However, this work has never been applied to specific animal species. Moreover, the assessment of the results has been generally done subjectively, rather than quantitatively. In this paper, we propose an approach based on methods for generating images of faces of male or female mandrills, a non-human primate. The main novelty of proposed method is the ability to edit their sex by identifying a sex axis in the latent space of a specific GAN. In addition, we have developed an assessment of the sex levels based on statistical features extracted from real image distributions. The experimental results we obtained from a specific database are not only realistic, but also accurate, meeting a need for future work in behavioral experiments with wild mandrills.</li>
</ul>

<h3>Title: SeqRisk: Transformer-augmented latent variable model for improved survival prediction with longitudinal data</h3>
<ul>
<li><strong>Authors: </strong>Mine Öğretir, Miika Koskinen, Juha Sinisalo, Risto Renkonen, Harri Lähdesmäki</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12709">https://arxiv.org/abs/2409.12709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12709">https://arxiv.org/pdf/2409.12709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12709]] SeqRisk: Transformer-augmented latent variable model for improved survival prediction with longitudinal data(https://arxiv.org/abs/2409.12709)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>In healthcare, risk assessment of different patient outcomes has for long time been based on survival analysis, i.e.\ modeling time-to-event associations. However, conventional approaches rely on data from a single time-point, making them suboptimal for fully leveraging longitudinal patient history and capturing temporal regularities. Focusing on clinical real-world data and acknowledging its challenges, we utilize latent variable models to effectively handle irregular, noisy, and sparsely observed longitudinal data. We propose SeqRisk, a method that combines variational autoencoder (VAE) or longitudinal VAE (LVAE) with a transformer encoder and Cox proportional hazards module for risk prediction. SeqRisk captures long-range interactions, improves patient trajectory representations, enhances predictive accuracy and generalizability, as well as provides partial explainability for sample population characteristics in attempts to identify high-risk patients. We demonstrate that SeqRisk performs competitively compared to existing approaches on both simulated and real-world datasets.</li>
</ul>

<h3>Title: Optical Flow Matters: an Empirical Comparative Study on Fusing Monocular Extracted Modalities for Better Steering</h3>
<ul>
<li><strong>Authors: </strong>Fouad Makiyeh, Mark Bastourous, Anass Bairouk, Wei Xiao, Mirjana Maras, Tsun-Hsuan Wangb, Marc Blanchon, Ramin Hasani, Patrick Chareyre, Daniela Rus</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12716">https://arxiv.org/abs/2409.12716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12716">https://arxiv.org/pdf/2409.12716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12716]] Optical Flow Matters: an Empirical Comparative Study on Fusing Monocular Extracted Modalities for Better Steering(https://arxiv.org/abs/2409.12716)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous vehicle navigation is a key challenge in artificial intelligence, requiring robust and accurate decision-making processes. This research introduces a new end-to-end method that exploits multimodal information from a single monocular camera to improve the steering predictions for self-driving cars. Unlike conventional models that require several sensors which can be costly and complex or rely exclusively on RGB images that may not be robust enough under different conditions, our model significantly improves vehicle steering prediction performance from a single visual sensor. By focusing on the fusion of RGB imagery with depth completion information or optical flow data, we propose a comprehensive framework that integrates these modalities through both early and hybrid fusion techniques. We use three distinct neural network models to implement our approach: Convolution Neural Network - Neutral Circuit Policy (CNN-NCP) , Variational Auto Encoder - Long Short-Term Memory (VAE-LSTM) , and Neural Circuit Policy architecture VAE-NCP. By incorporating optical flow into the decision-making process, our method significantly advances autonomous navigation. Empirical results from our comparative study using Boston driving data show that our model, which integrates image and motion information, is robust and reliable. It outperforms state-of-the-art approaches that do not use optical flow, reducing the steering estimation error by 31%. This demonstrates the potential of optical flow data, combined with advanced neural network architectures (a CNN-based structure for fusing data and a Recurrence-based network for inferring a command from latent space), to enhance the performance of autonomous vehicles steering estimation.</li>
</ul>

<h3>Title: LLM-Measure: Generating Valid, Consistent, and Reproducible Text-Based Measures for Social Science Research</h3>
<ul>
<li><strong>Authors: </strong>Yi Yang, Hanyu Duan, Jiaxin Liu, Kar Yan Tam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12722">https://arxiv.org/abs/2409.12722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12722">https://arxiv.org/pdf/2409.12722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12722]] LLM-Measure: Generating Valid, Consistent, and Reproducible Text-Based Measures for Social Science Research(https://arxiv.org/abs/2409.12722)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The increasing use of text as data in social science research necessitates the development of valid, consistent, reproducible, and efficient methods for generating text-based concept measures. This paper presents a novel method that leverages the internal hidden states of large language models (LLMs) to generate these concept measures. Specifically, the proposed method learns a concept vector that captures how the LLM internally represents the target concept, then estimates the concept value for text data by projecting the text's LLM hidden states onto the concept vector. Three replication studies demonstrate the method's effectiveness in producing highly valid, consistent, and reproducible text-based measures across various social science research contexts, highlighting its potential as a valuable tool for the research community.</li>
</ul>

<h3>Title: Edu-Values: Towards Evaluating the Chinese Education Values of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Peiyi Zhang, Yazhou Zhang, Bo Wang, Lu Rong, Jing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12739">https://arxiv.org/abs/2409.12739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12739">https://arxiv.org/pdf/2409.12739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12739]] Edu-Values: Towards Evaluating the Chinese Education Values of Large Language Models(https://arxiv.org/abs/2409.12739)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the recent evolution of large language models (LLMs), concerns about aligning such models with human values have grown. Previous research has primarily focused on assessing LLMs' performance in terms of the Helpful, Honest, Harmless (3H) basic principles, while often overlooking their alignment with educational values in the Chinese context. To fill this gap, we present Edu-Values, the first Chinese education values evaluation benchmark designed to measure LLMs' alignment ability across seven dimensions: professional ideology, cultural literacy, educational knowledge and skills, education laws and regulations, teachers' professional ethics, basic competencies, and subject knowledge. We meticulously design and compile 1,418 questions, including multiple-choice, multi-modal question answering, subjective analysis, adversarial prompts, and questions on traditional Chinese culture. We conduct both human evaluation and automatic evaluation over 11 state-of-the-art (SoTA) LLMs, and highlight three main findings: (1) due to differences in educational culture, Chinese LLMs significantly outperform English LLMs, with Qwen 2 ranking the first with a score of 81.37; (2) LLMs perform well in subject knowledge and teaching skills but struggle with teachers' professional ethics and basic competencies; (3) LLMs excel at multiple-choice questions but perform poorly on subjective analysis and multi-modal tasks. This demonstrates the effectiveness and potential of the proposed benchmark. Our dataset is available at this https URL.</li>
</ul>

<h3>Title: Fine Tuning Large Language Models for Medicine: The Role and Importance of Direct Parameter Optimization</h3>
<ul>
<li><strong>Authors: </strong>Thomas Savage, Stephen Ma, Abdessalem Boukil, Vishwesh Patel, Ekanath Rangan, Ivan Rodriguez, Jonathan H Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12741">https://arxiv.org/abs/2409.12741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12741">https://arxiv.org/pdf/2409.12741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12741]] Fine Tuning Large Language Models for Medicine: The Role and Importance of Direct Parameter Optimization(https://arxiv.org/abs/2409.12741)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) fine tuning is underutilized in the field of medicine. Two of the most common methods of fine tuning are Supervised Fine Tuning (SFT) and Direct Parameter Optimization (DPO), but there is little guidance informing users when to use either technique. In this investigation, we compare the performance of SFT and DPO for five common natural language tasks in medicine: Classification with text data, Classification with numeric data, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT alone is sufficient for Classification with text data, whereas DPO improves performance for the more complex tasks of Clinical Reasoning, Summarization and Clinical Triage. Our results establish the role and importance of DPO fine tuning within medicine, and consequently call attention to current software gaps that prevent widespread deployment of this technique.</li>
</ul>

<h3>Title: COCO-Occ: A Benchmark for Occluded Panoptic Segmentation and Image Understanding</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Wei, Jun Wang, Abhir Bhalerao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12760">https://arxiv.org/abs/2409.12760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12760">https://arxiv.org/pdf/2409.12760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12760]] COCO-Occ: A Benchmark for Occluded Panoptic Segmentation and Image Understanding(https://arxiv.org/abs/2409.12760)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>To help address the occlusion problem in panoptic segmentation and image understanding, this paper proposes a new large-scale dataset, COCO-Occ, which is derived from the COCO dataset by manually labelling the COCO images into three perceived occlusion levels. Using COCO-Occ, we systematically assess and quantify the impact of occlusion on panoptic segmentation on samples having different levels of occlusion. Comparative experiments with SOTA panoptic models demonstrate that the presence of occlusion significantly affects performance with higher occlusion levels resulting in notably poorer performance. Additionally, we propose a straightforward yet effective method as an initial attempt to leverage the occlusion annotation using contrastive learning to render a model that learns a more robust representation capturing different severities of occlusion. Experimental results demonstrate that the proposed approach boosts the performance of the baseline model and achieves SOTA performance on the proposed COCO-Occ dataset.</li>
</ul>

<h3>Title: Towards AI-enabled Cyber Threat Assessment in the Health Sector</h3>
<ul>
<li><strong>Authors: </strong>Patrizia Heinl, Andrius Patapovas, Michael Pilgermann</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12765">https://arxiv.org/abs/2409.12765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12765">https://arxiv.org/pdf/2409.12765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12765]] Towards AI-enabled Cyber Threat Assessment in the Health Sector(https://arxiv.org/abs/2409.12765)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Cyber attacks on the healthcare industry can have tremendous consequences and the attack surface expands continuously. In order to handle the steadily rising workload, an expanding amount of analog processes in healthcare institutions is digitized. Despite regulations becoming stricter, not all existing infrastructure is sufficiently protected against cyber attacks. With an increasing number of devices and digital processes, the system and network landscape becomes more complex and harder to manage and therefore also more difficult to protect. The aim of this project is to introduce an AI-enabled platform that collects security relevant information from the outside of a health organization, analyzes it, delivers a risk score and supports decision makers in healthcare institutions to optimize investment choices for security measures. Therefore, an architecture of such a platform is designed, relevant information sources are identified, and AI methods for relevant data collection, selection, and risk scoring are explored.</li>
</ul>

<h3>Title: The Robustness of Spiking Neural Networks in Communication and its Application towards Network Efficiency in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Manh V. Nguyen, Liang Zhao, Bobin Deng, William Severa, Honghui Xu, Shaoen Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12769">https://arxiv.org/abs/2409.12769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12769">https://arxiv.org/pdf/2409.12769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12769]] The Robustness of Spiking Neural Networks in Communication and its Application towards Network Efficiency in Federated Learning(https://arxiv.org/abs/2409.12769)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) have recently gained significant interest in on-chip learning in embedded devices and emerged as an energy-efficient alternative to conventional Artificial Neural Networks (ANNs). However, to extend SNNs to a Federated Learning (FL) setting involving collaborative model training, the communication between the local devices and the remote server remains the bottleneck, which is often restricted and costly. In this paper, we first explore the inherent robustness of SNNs under noisy communication in FL. Building upon this foundation, we propose a novel Federated Learning with Top-K Sparsification (FLTS) algorithm to reduce the bandwidth usage for FL training. We discover that the proposed scheme with SNNs allows more bandwidth savings compared to ANNs without impacting the model's accuracy. Additionally, the number of parameters to be communicated can be reduced to as low as 6 percent of the size of the original model. We further improve the communication efficiency by enabling dynamic parameter compression during model training. Extensive experiment results demonstrate that our proposed algorithms significantly outperform the baselines in terms of communication cost and model accuracy and are promising for practical network-efficient FL with SNNs.</li>
</ul>

<h3>Title: Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Pignatelli, Johan Ferret, Tim Rockäschel, Edward Grefenstette, Davide Paglieri, Samuel Coward, Laura Toni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12798">https://arxiv.org/abs/2409.12798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12798">https://arxiv.org/pdf/2409.12798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12798]] Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL(https://arxiv.org/abs/2409.12798)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The temporal credit assignment problem is a central challenge in Reinforcement Learning (RL), concerned with attributing the appropriate influence to each actions in a trajectory for their ability to achieve a goal. However, when feedback is delayed and sparse, the learning signal is poor, and action evaluation becomes harder. Canonical solutions, such as reward shaping and options, require extensive domain knowledge and manual intervention, limiting their scalability and applicability. In this work, we lay the foundations for Credit Assignment with Language Models (CALM), a novel approach that leverages Large Language Models (LLMs) to automate credit assignment via reward shaping and options discovery. CALM uses LLMs to decompose a task into elementary subgoals and assess the achievement of these subgoals in state-action transitions. Every time an option terminates, a subgoal is achieved, and CALM provides an auxiliary reward. This additional reward signal can enhance the learning process when the task reward is sparse and delayed without the need for human-designed rewards. We provide a preliminary evaluation of CALM using a dataset of human-annotated demonstrations from MiniHack, suggesting that LLMs can be effective in assigning credit in zero-shot settings, without examples or LLM fine-tuning. Our preliminary results indicate that the knowledge of LLMs is a promising prior for credit assignment in RL, facilitating the transfer of human knowledge into value functions.</li>
</ul>

<h3>Title: Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV -- Extended Abstract</h3>
<ul>
<li><strong>Authors: </strong>Matej Fabijanić, Nadir Kapetanović, Nikola Mišković</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12813">https://arxiv.org/abs/2409.12813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12813">https://arxiv.org/pdf/2409.12813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12813]] Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV -- Extended Abstract(https://arxiv.org/abs/2409.12813)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The process of fish cage inspections, which is a necessary maintenance task at any fish farm, be it small scale or industrial, is a task that has the potential to be fully automated. Replacing trained divers who perform regular inspections with autonomous marine vehicles would lower the costs of manpower and remove the risks associated with humans performing underwater inspections. Achieving such a level of autonomy implies developing an image processing algorithm that is capable of estimating the state of biofouling buildup. The aim of this work is to propose a complete solution for automating the said inspection process; from developing an autonomous control algorithm for an ROV, to automatically segmenting images of fish cages, and accurately estimating the state of biofouling. The first part is achieved by modifying a commercially available ROV with an acoustic SBL positioning system and developing a closed-loop control system. The second part is realized by implementing a proposed biofouling estimation framework, which relies on AI to perform image segmentation, and by processing images using established computer vision methods to obtain a rough estimate of the distance of the ROV from the fish cage. This also involved developing a labeling tool in order to create a dataset of images for the neural network performing the semantic segmentation to be trained on. The experimental results show the viability of using an ROV fitted with an acoustic transponder for autonomous missions, and demonstrate the biofouling estimation framework's ability to provide accurate assessments, alongside satisfactory distance estimation capabilities. In conclusion, the achieved biofouling estimation accuracy showcases clear potential for use in the aquaculture industry.</li>
</ul>

<h3>Title: Automated Linear Disturbance Mapping via Semantic Segmentation of Sentinel-2 Imagery</h3>
<ul>
<li><strong>Authors: </strong>Andrew M. Nagel, Anne Webster, Christopher Henry, Christopher Storie, Ignacio San-Miguel Sanchez, Olivier Tsui, Jason Duffe, Andy Dean</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12817">https://arxiv.org/abs/2409.12817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12817">https://arxiv.org/pdf/2409.12817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12817]] Automated Linear Disturbance Mapping via Semantic Segmentation of Sentinel-2 Imagery(https://arxiv.org/abs/2409.12817)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>In Canada's northern regions, linear disturbances such as roads, seismic exploration lines, and pipelines pose a significant threat to the boreal woodland caribou population (Rangifer tarandus). To address the critical need for management of these disturbances, there is a strong emphasis on developing mapping approaches that accurately identify forest habitat fragmentation. The traditional approach is manually generating maps, which is time-consuming and lacks the capability for frequent updates. Instead, applying deep learning methods to multispectral satellite imagery offers a cost-effective solution for automated and regularly updated map production. Deep learning models have shown promise in extracting paved roads in urban environments when paired with high-resolution (<0.5m) imagery, but their effectiveness for general linear feature extraction in forested areas from lower resolution imagery remains underexplored. This research employs a deep convolutional neural network model based on the VGGNet16 architecture for semantic segmentation of lower resolution (10m) Sentinel-2 satellite imagery, creating precise multi-class linear disturbance maps. The model is trained using ground-truth label maps sourced from the freely available Alberta Institute of Biodiversity Monitoring Human Footprint dataset, specifically targeting the Boreal and Taiga Plains ecozones in Alberta, Canada. Despite challenges in segmenting lower resolution imagery, particularly for thin linear disturbances like seismic exploration lines that can exhibit a width of 1-3 pixels in Sentinel-2 imagery, our results demonstrate the effectiveness of the VGGNet model for accurate linear disturbance retrieval. By leveraging the freely available Sentinel-2 imagery, this work advances cost-effective automated mapping techniques for identifying and monitoring linear disturbance fragmentation.</li>
</ul>

<h3>Title: FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists</h3>
<ul>
<li><strong>Authors: </strong>Tenghao Huang, Donghee Lee, John Sweeney, Jiatong Shi, Emily Steliotes, Matthew Lange, Jonathan May, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12832">https://arxiv.org/abs/2409.12832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12832">https://arxiv.org/pdf/2409.12832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12832]] FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists(https://arxiv.org/abs/2409.12832)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address the challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. To facilitate research in this area, we introduce the FoodPuzzle, a challenging benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.</li>
</ul>

<h3>Title: USBIPS Framework: Protecting Hosts from Malicious USB Peripherals</h3>
<ul>
<li><strong>Authors: </strong>Chun-Yi Wang, Fu-Hau Hsu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12850">https://arxiv.org/abs/2409.12850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12850">https://arxiv.org/pdf/2409.12850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12850]] USBIPS Framework: Protecting Hosts from Malicious USB Peripherals(https://arxiv.org/abs/2409.12850)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>USB-based attacks have increased in complexity in recent years. Modern attacks incorporate a wide range of attack vectors, from social engineering to signal injection. The security community is addressing these challenges using a growing set of fragmented defenses. Regardless of the vector of a USB-based attack, the most important risks concerning most people and enterprises are service crashes and data loss. The host OS manages USB peripherals, and malicious USB peripherals, such as those infected with BadUSB, can crash a service or steal data from the OS. Although USB firewalls have been proposed to thwart malicious USB peripherals, such as USBFilter and USBGuard, they cannot prevent real-world intrusions. This paper focuses on building a security framework called USBIPS within OSs to defend against malicious USB peripherals. This includes major efforts to explore the nature of malicious behavior and build persistent protection from USB-based intrusions. We first present a behavior-based detection mechanism focusing on attacks integrated into USB peripherals. We then introduce the novel idea of an allowlisting-based method for USB access control. We finally develop endpoint detection and response system to build the first generic security framework that thwarts USB-based intrusion. Within a centralized threat analysis framework, it provides persistent protection and may detect unknown malicious behavior. By addressing key security and performance challenges, these efforts help modern OSs against attacks from untrusted USB peripherals.</li>
</ul>

<h3>Title: Enhancing E-commerce Product Title Translation with Retrieval-Augmented Generation and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bryan Zhang, Taichi Nakatani, Stephan Walter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12880">https://arxiv.org/abs/2409.12880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12880">https://arxiv.org/pdf/2409.12880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12880]] Enhancing E-commerce Product Title Translation with Retrieval-Augmented Generation and Large Language Models(https://arxiv.org/abs/2409.12880)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>E-commerce stores enable multilingual product discovery which require accurate product title translation. Multilingual large language models (LLMs) have shown promising capacity to perform machine translation tasks, and it can also enhance and translate product titles cross-lingually in one step. However, product title translation often requires more than just language conversion because titles are short, lack context, and contain specialized terminology. This study proposes a retrieval-augmented generation (RAG) approach that leverages existing bilingual product information in e-commerce by retrieving similar bilingual examples and incorporating them as few-shot prompts to enhance LLM-based product title translation. Experiment results show that our proposed RAG approach improve product title translation quality with chrF score gains of up to 15.3% for language pairs where the LLM has limited proficiency.</li>
</ul>

<h3>Title: On the Hardness of Decentralized Multi-Agent Policy Evaluation under Byzantine Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hairi, Minghong Fang, Zifan Zhang, Alvaro Velasquez, Jia Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12882">https://arxiv.org/abs/2409.12882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12882">https://arxiv.org/pdf/2409.12882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12882]] On the Hardness of Decentralized Multi-Agent Policy Evaluation under Byzantine Attacks(https://arxiv.org/abs/2409.12882)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In this paper, we study a fully-decentralized multi-agent policy evaluation problem, which is an important sub-problem in cooperative multi-agent reinforcement learning, in the presence of up to $f$ faulty agents. In particular, we focus on the so-called Byzantine faulty model with model poisoning setting. In general, policy evaluation is to evaluate the value function of any given policy. In cooperative multi-agent system, the system-wide rewards are usually modeled as the uniform average of rewards from all agents. We investigate the multi-agent policy evaluation problem in the presence of Byzantine agents, particularly in the setting of heterogeneous local rewards. Ideally, the goal of the agents is to evaluate the accumulated system-wide rewards, which are uniform average of rewards of the normal agents for a given policy. It means that all agents agree upon common values (the consensus part) and furthermore, the consensus values are the value functions (the convergence part). However, we prove that this goal is not achievable. Instead, we consider a relaxed version of the problem, where the goal of the agents is to evaluate accumulated system-wide reward, which is an appropriately weighted average reward of the normal agents. We further prove that there is no correct algorithm that can guarantee that the total number of positive weights exceeds $|\mathcal{N}|-f $, where $|\mathcal{N}|$ is the number of normal agents. Towards the end, we propose a Byzantine-tolerant decentralized temporal difference algorithm that can guarantee asymptotic consensus under scalar function approximation. We then empirically test the effective of the proposed algorithm.</li>
</ul>

<h3>Title: Improving Prototypical Parts Abstraction for Case-Based Reasoning Explanations Designed for the Kidney Stone Type Recognition</h3>
<ul>
<li><strong>Authors: </strong>Daniel Flores-Araiza, Francisco Lopez-Tiro, Clément Larose, Salvador Hinojosa, Andres Mendez-Vazquez, Miguel Gonzalez-Mendoza, Gilberto Ochoa-Ruiz, Christian Daul</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12883">https://arxiv.org/abs/2409.12883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12883">https://arxiv.org/pdf/2409.12883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12883]] Improving Prototypical Parts Abstraction for Case-Based Reasoning Explanations Designed for the Kidney Stone Type Recognition(https://arxiv.org/abs/2409.12883)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The in-vivo identification of the kidney stone types during an ureteroscopy would be a major medical advance in urology, as it could reduce the time of the tedious renal calculi extraction process, while diminishing infection risks. Furthermore, such an automated procedure would make possible to prescribe anti-recurrence treatments immediately. Nowadays, only few experienced urologists are able to recognize the kidney stone types in the images of the videos displayed on a screen during the endoscopy. Thus, several deep learning (DL) models have recently been proposed to automatically recognize the kidney stone types using ureteroscopic images. However, these DL models are of black box nature whicl limits their applicability in clinical settings. This contribution proposes a case-based reasoning DL model which uses prototypical parts (PPs) and generates local and global descriptors. The PPs encode for each class (i.e., kidney stone type) visual feature information (hue, saturation, intensity and textures) similar to that used by biologists. The PPs are optimally generated due a new loss function used during the model training. Moreover, the local and global descriptors of PPs allow to explain the decisions ("what" information, "where in the images") in an understandable way for biologists and urologists. The proposed DL model has been tested on a database including images of the six most widespread kidney stone types. The overall average classification accuracy was 90.37. When comparing this results with that of the eight other DL models of the kidney stone state-of-the-art, it can be seen that the valuable gain in explanability was not reached at the expense of accuracy which was even slightly increased with respect to that (88.2) of the best method of the literature. These promising and interpretable results also encourage urologists to put their trust in AI-based solutions.</li>
</ul>

<h3>Title: Hypersphere Secure Sketch Revisited: Probabilistic Linear Regression Attack on IronMask in Multiple Usage</h3>
<ul>
<li><strong>Authors: </strong>Pengxu Zhu, Lei Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12884">https://arxiv.org/abs/2409.12884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12884">https://arxiv.org/pdf/2409.12884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12884]] Hypersphere Secure Sketch Revisited: Probabilistic Linear Regression Attack on IronMask in Multiple Usage(https://arxiv.org/abs/2409.12884)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack, biometric</a></li>
<li><strong>Abstract: </strong>Protection of biometric templates is a critical and urgent area of focus. IronMask demonstrates outstanding recognition performance while protecting facial templates against existing known attacks. In high-level, IronMask can be conceptualized as a fuzzy commitment scheme building on the hypersphere directly. We devise an attack on IronMask targeting on the security notion of renewability. Our attack, termed as Probabilistic Linear Regression Attack, utilizes the linearity of underlying used error correcting code. This attack is the first algorithm to successfully recover the original template when getting multiple protected templates in acceptable time and requirement of storage. We implement experiments on IronMask applied to protect ArcFace that well verify the validity of our attacks. Furthermore, we carry out experiments in noisy environments and confirm that our attacks are still applicable. Finally, we put forward two strategies to mitigate this type of attacks.</li>
</ul>

<h3>Title: Knowledge-Based Domain-Oriented Data Augmentation for Enhancing Unsupervised Sentence Embedding</h3>
<ul>
<li><strong>Authors: </strong>Peichao Lai, Zhengfeng Zhang, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12887">https://arxiv.org/abs/2409.12887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12887">https://arxiv.org/pdf/2409.12887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12887]] Knowledge-Based Domain-Oriented Data Augmentation for Enhancing Unsupervised Sentence Embedding(https://arxiv.org/abs/2409.12887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recently, unsupervised sentence embedding models have received significant attention in downstream natural language processing tasks. Using large language models (LLMs) for data augmentation has led to considerable improvements in previous studies. Nevertheless, these strategies emphasize data augmentation with extensive generic corpora, neglecting the consideration of few-shot domain data. The synthesized data lacks fine-grained information and may introduce negative sample noise. This study introduces a novel pipeline-based data augmentation method that leverages LLM to synthesize the domain-specific dataset. It produces both positive and negative samples through entity- and quantity-aware augmentation, utilizing an entity knowledge graph to synthesize samples with fine-grained semantic distinctions, increasing training sample diversity and relevance. We then present a Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model to reduce synthetic data noise and improve model discrimination to reduce negative sample noise. Experimental results demonstrate that our approach achieves state-of-the-art semantic textual similarity performance with fewer synthetic data samples and lesser LLM parameters, demonstrating its efficiency and robustness in varied backbones.</li>
</ul>

<h3>Title: Recognition of Harmful Phytoplankton from Microscopic Images using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Aymane Khaldi, Rohaifa Khaldi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12900">https://arxiv.org/abs/2409.12900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12900">https://arxiv.org/pdf/2409.12900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12900]] Recognition of Harmful Phytoplankton from Microscopic Images using Deep Learning(https://arxiv.org/abs/2409.12900)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Monitoring plankton distribution, particularly harmful phytoplankton, is vital for preserving aquatic ecosystems, regulating the global climate, and ensuring environmental protection. Traditional methods for monitoring are often time-consuming, expensive, error-prone, and unsuitable for large-scale applications, highlighting the need for accurate and efficient automated systems. In this study, we evaluate several state-of-the-art CNN models, including ResNet, ResNeXt, DenseNet, and EfficientNet, using three transfer learning approaches: linear probing, fine-tuning, and a combined approach, to classify eleven harmful phytoplankton genera from microscopic images. The best performance was achieved by ResNet-50 using the fine-tuning approach, with an accuracy of 96.97%. The results also revealed that the models struggled to differentiate between four harmful phytoplankton types with similar morphological features.</li>
</ul>

<h3>Title: Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Samragh, Iman Mirzadeh, Keivan Alizadeh Vahid, Fartash Faghri, Minsik Cho, Moin Nabi, Devang Naik, Mehrdad Farajtabar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12903">https://arxiv.org/abs/2409.12903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12903">https://arxiv.org/pdf/2409.12903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12903]] Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization(https://arxiv.org/abs/2409.12903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The pre-training phase of language models often begins with randomly initialized parameters. With the current trends in scaling models, training their large number of parameters can be extremely slow and costly. In contrast, small language models are less expensive to train, but they often cannot achieve the accuracy of large models. In this paper, we explore an intriguing idea to connect these two different regimes: Can we develop a method to initialize large language models using smaller pre-trained models? Will such initialization bring any benefits in terms of training time and final accuracy? In this paper, we introduce HyperCloning, a method that can expand the parameters of a pre-trained language model to those of a larger model with increased hidden dimensions. Our method ensures that the larger model retains the functionality of the smaller model. As a result, the larger model already inherits the predictive power and accuracy of the smaller model before the training starts. We demonstrate that training such an initialized model results in significant savings in terms of GPU hours required for pre-training large language models.</li>
</ul>

<h3>Title: Defending against Reverse Preference Attacks is Difficult</h3>
<ul>
<li><strong>Authors: </strong>Domenic Rosati, Giles Edkins, Harsh Raj, David Atanasov, Subhabrata Majumdar, Janarthanan Rajendran, Frank Rudzicz, Hassan Sajjad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12914">https://arxiv.org/abs/2409.12914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12914">https://arxiv.org/pdf/2409.12914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12914]] Defending against Reverse Preference Attacks is Difficult(https://arxiv.org/abs/2409.12914)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>While there has been progress towards aligning Large Language Models (LLMs) with human values and ensuring safe behaviour at inference time, safety-aligned LLMs are known to be vulnerable to training-time attacks such as supervised fine-tuning (SFT) on harmful datasets. In this paper, we ask if LLMs are vulnerable to adversarial reinforcement learning. Motivated by this goal, we propose Reverse Preference Attacks (RPA), a class of attacks to make LLMs learn harmful behavior using adversarial reward during reinforcement learning from human feedback (RLHF). RPAs expose a critical safety gap of safety-aligned LLMs in RL settings: they easily explore the harmful text generation policies to optimize adversarial reward. To protect against RPAs, we explore a host of mitigation strategies. Leveraging Constrained Markov-Decision Processes, we adapt a number of mechanisms to defend against harmful fine-tuning attacks into the RL setting. Our experiments show that ``online" defenses that are based on the idea of minimizing the negative log likelihood of refusals -- with the defender having control of the loss function -- can effectively protect LLMs against RPAs. However, trying to defend model weights using ``offline" defenses that operate under the assumption that the defender has no control over the loss function are less effective in the face of RPAs. These findings show that attacks done using RL can be used to successfully undo safety alignment in open-weight LLMs and use them for malicious purposes.</li>
</ul>

<h3>Title: Training Language Models to Self-Correct via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, Aleksandra Faust</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12917">https://arxiv.org/abs/2409.12917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12917">https://arxiv.org/pdf/2409.12917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12917]] Training Language Models to Self-Correct via Reinforcement Learning(https://arxiv.org/abs/2409.12917)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a more capable model or other forms of supervision. To this end, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.</li>
</ul>

<h3>Title: MaskMol: Knowledge-guided Molecular Image Pre-Training Framework for Activity Cliffs</h3>
<ul>
<li><strong>Authors: </strong>Zhixiang Cheng, Hongxin Xiang, Pengsen Ma, Li Zeng, Xin Jin, Xixi Yang, Jianxin Lin, Yang Deng, Bosheng Song, Xinxin Feng, Changhui Deng, Xiangxiang Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12926">https://arxiv.org/abs/2409.12926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12926">https://arxiv.org/pdf/2409.12926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12926]] MaskMol: Knowledge-guided Molecular Image Pre-Training Framework for Activity Cliffs(https://arxiv.org/abs/2409.12926)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Activity cliffs, which refer to pairs of molecules that are structurally similar but show significant differences in their potency, can lead to model representation collapse and make the model challenging to distinguish them. Our research indicates that as molecular similarity increases, graph-based methods struggle to capture these nuances, whereas image-based approaches effectively retain the distinctions. Thus, we developed MaskMol, a knowledge-guided molecular image self-supervised learning framework. MaskMol accurately learns the representation of molecular images by considering multiple levels of molecular knowledge, such as atoms, bonds, and substructures. By utilizing pixel masking tasks, MaskMol extracts fine-grained information from molecular images, overcoming the limitations of existing deep learning models in identifying subtle structural changes. Experimental results demonstrate MaskMol's high accuracy and transferability in activity cliff estimation and compound potency prediction across 20 different macromolecular targets, outperforming 25 state-of-the-art deep learning and machine learning approaches. Visualization analyses reveal MaskMol's high biological interpretability in identifying activity cliff-relevant molecular substructures. Notably, through MaskMol, we identified candidate EP4 inhibitors that could be used to treat tumors. This study not only raises awareness about activity cliffs but also introduces a novel method for molecular image representation learning and virtual screening, advancing drug discovery and providing new insights into structure-activity relationships (SAR).</li>
</ul>

<h3>Title: LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning</h3>
<ul>
<li><strong>Authors: </strong>Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin Cao, Liangcai Gao, Zhi Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12929">https://arxiv.org/abs/2409.12929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12929">https://arxiv.org/pdf/2409.12929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12929]] LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning(https://arxiv.org/abs/2409.12929)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we present a novel approach, called LogicPro, to enhance Large Language Models (LLMs) complex Logical reasoning through Program Examples. We do this effectively by simply utilizing widely available algorithmic problems and their code solutions. First, we constructed diverse test samples input based on algorithmic questions and code solutions. Then, we designed different complex reasoning questions based on algorithmic problems and test samples. Finally, combining the intermediate variable outputs of the code solutions and the complex reasoning questions, we derived the reasoning process and the final answer. With this approach, we can construct a dataset that is sufficiently difficult (all models are ineffective), diverse (synthesized from 2,360 different algorithmic questions), and scalable (building different test samples and collecting more algorithmic questions). In addition, we obtain a high-quality reasoning process guided by the values of intermediate variables. As a result, our approach achieves significant improvements in multiple models for the BBH$^{27}$, GSM8K, HellSwag, Logicqa, Reclor, and RTE datasets, outperforming a wide range of existing reasoning datasets.</li>
</ul>

<h3>Title: Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, Manaal Faruqui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12941">https://arxiv.org/abs/2409.12941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12941">https://arxiv.org/pdf/2409.12941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12941]] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation(https://arxiv.org/abs/2409.12941)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.</li>
</ul>

<h3>Title: Revisiting Semi-supervised Adversarial Robustness via Noise-aware Online Robust Distillation</h3>
<ul>
<li><strong>Authors: </strong>Tsung-Han Wu, Hung-Ting Su, Shang-Tse Chen, Winston H. Hsu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12946">https://arxiv.org/abs/2409.12946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12946">https://arxiv.org/pdf/2409.12946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12946]] Revisiting Semi-supervised Adversarial Robustness via Noise-aware Online Robust Distillation(https://arxiv.org/abs/2409.12946)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The robust self-training (RST) framework has emerged as a prominent approach for semi-supervised adversarial training. To explore the possibility of tackling more complicated tasks with even lower labeling budgets, unlike prior approaches that rely on robust pretrained models, we present SNORD - a simple yet effective framework that introduces contemporary semi-supervised learning techniques into the realm of adversarial training. By enhancing pseudo labels and managing noisy training data more effectively, SNORD showcases impressive, state-of-the-art performance across diverse datasets and labeling budgets, all without the need for pretrained models. Compared to full adversarial supervision, SNORD achieves a 90% relative robust accuracy under epsilon = 8/255 AutoAttack, requiring less than 0.1%, 2%, and 10% labels for CIFAR-10, CIFAR-100, and TinyImageNet-200, respectively. Additional experiments confirm the efficacy of each component and demonstrate the adaptability of integrating SNORD with existing adversarial pretraining strategies to further bolster robustness.</li>
</ul>

<h3>Title: Re-Introducing LayerNorm: Geometric Meaning, Irreversibility and a Comparative Study with RMSNorm</h3>
<ul>
<li><strong>Authors: </strong>Akshat Gupta, Atahan Ozdemir, Gopala Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12951">https://arxiv.org/abs/2409.12951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12951">https://arxiv.org/pdf/2409.12951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12951]] Re-Introducing LayerNorm: Geometric Meaning, Irreversibility and a Comparative Study with RMSNorm(https://arxiv.org/abs/2409.12951)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Layer normalization is a pivotal step in the transformer architecture. This paper delves into the less explored geometric implications of this process, examining how LayerNorm influences the norm and orientation of hidden vectors in the representation space. We show that the definition of LayerNorm is innately linked to the uniform vector, defined as $\boldsymbol{1} = [1, 1, 1, 1, \cdots, 1]^T \in \mathbb{R}^d$. We then show that the standardization step in LayerNorm can be understood in three simple steps: (i) remove the component of a vector along the uniform vector, (ii) normalize the remaining vector, and (iii) scale the resultant vector by $\sqrt{d}$, where $d$ is the dimensionality of the representation space. We also introduce the property of "irreversibility" for LayerNorm, where we show that the information lost during the normalization process cannot be recovered. In other words, unlike batch normalization, LayerNorm cannot learn an identity transform. While we present possible arguments for removing the component along the uniform vector, the choice of removing this component seems arbitrary and not well motivated by the original authors. To evaluate the usefulness of this step, we compare the hidden representations of LayerNorm-based LLMs with models trained using RMSNorm and show that all LLMs naturally align representations orthogonal to the uniform vector, presenting the first mechanistic evidence that removing the component along the uniform vector in LayerNorm is a redundant step. Our findings support the use of RMSNorm over LayerNorm as it is not only more computationally efficient with comparable downstream performance, but also learns a similar distribution of hidden representations that operate orthogonal to the uniform vector.</li>
</ul>

<h3>Title: The Gaussian Discriminant Variational Autoencoder (GdVAE): A Self-Explainable Model with Counterfactual Explanations</h3>
<ul>
<li><strong>Authors: </strong>Anselm Haselhoff, Kevin Trelenberg, Fabian Küppers, Jonas Schneider</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12952">https://arxiv.org/abs/2409.12952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12952">https://arxiv.org/pdf/2409.12952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12952]] The Gaussian Discriminant Variational Autoencoder (GdVAE): A Self-Explainable Model with Counterfactual Explanations(https://arxiv.org/abs/2409.12952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual counterfactual explanation (CF) methods modify image concepts, e.g, shape, to change a prediction to a predefined outcome while closely resembling the original query image. Unlike self-explainable models (SEMs) and heatmap techniques, they grant users the ability to examine hypothetical "what-if" scenarios. Previous CF methods either entail post-hoc training, limiting the balance between transparency and CF quality, or demand optimization during inference. To bridge the gap between transparent SEMs and CF methods, we introduce the GdVAE, a self-explainable model based on a conditional variational autoencoder (CVAE), featuring a Gaussian discriminant analysis (GDA) classifier and integrated CF explanations. Full transparency is achieved through a generative classifier that leverages class-specific prototypes for the downstream task and a closed-form solution for CFs in the latent space. The consistency of CFs is improved by regularizing the latent space with the explainer function. Extensive comparisons with existing approaches affirm the effectiveness of our method in producing high-quality CF explanations while preserving transparency. Code and models are public.</li>
</ul>

<h3>Title: JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Zhecan Wang, Junzhang Liu, Chia-Wei Tang, Hani Alomari, Anushka Sivakumar, Rui Sun, Wenhao Li, Md. Atabuzzaman, Hammad Ayyubi, Haoxuan You, Alvi Ishmam, Kai-Wei Chang, Shih-Fu Chang, Chris Thomas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12953">https://arxiv.org/abs/2409.12953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12953">https://arxiv.org/pdf/2409.12953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12953]] JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images(https://arxiv.org/abs/2409.12953)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing vision-language understanding benchmarks largely consist of images of objects in their usual contexts. As a consequence, recent multimodal large language models can perform well with only a shallow visual understanding by relying on background language biases. Thus, strong performance on these benchmarks does not necessarily correlate with strong visual understanding. In this paper, we release JourneyBench, a comprehensive human-annotated benchmark of generated images designed to assess the model's fine-grained multimodal reasoning abilities across five tasks: complementary multimodal chain of thought, multi-image VQA, imaginary image captioning, VQA with hallucination triggers, and fine-grained retrieval with sample-specific distractors. Unlike existing benchmarks, JourneyBench explicitly requires fine-grained multimodal reasoning in unusual imaginary scenarios where language bias and holistic image gist are insufficient. We benchmark state-of-the-art models on JourneyBench and analyze performance along a number of fine-grained dimensions. Results across all five tasks show that JourneyBench is exceptionally challenging for even the best models, indicating that models' visual reasoning abilities are not as strong as they first appear. We discuss the implications of our findings and propose avenues for further research.</li>
</ul>

<h3>Title: 3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, Liang Pan, Dahua Lin, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12957">https://arxiv.org/abs/2409.12957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12957">https://arxiv.org/pdf/2409.12957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12957]] 3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion(https://arxiv.org/abs/2409.12957)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>The increasing demand for high-quality 3D assets across various industries necessitates efficient and automated 3D content creation. Despite recent advancements in 3D generative models, existing methods still face challenges with optimization speed, geometric fidelity, and the lack of assets for physically based rendering (PBR). In this paper, we introduce 3DTopia-XL, a scalable native 3D generative model designed to overcome these limitations. 3DTopia-XL leverages a novel primitive-based 3D representation, PrimX, which encodes detailed shape, albedo, and material field into a compact tensorial format, facilitating the modeling of high-resolution geometry with PBR assets. On top of the novel representation, we propose a generative framework based on Diffusion Transformer (DiT), which comprises 1) Primitive Patch Compression, 2) and Latent Primitive Diffusion. 3DTopia-XL learns to generate high-quality 3D assets from textual or visual inputs. We conduct extensive qualitative and quantitative experiments to demonstrate that 3DTopia-XL significantly outperforms existing methods in generating high-quality 3D assets with fine-grained textures and materials, efficiently bridging the quality gap between generative models and real-world applications.</li>
</ul>

<h3>Title: MURI: High-Quality Instruction Tuning Datasets for Low-Resource Languages via Reverse Instructions</h3>
<ul>
<li><strong>Authors: </strong>Abdullatif Köksal, Marion Thaler, Ayyoob Imani, Ahmet Üstün, Anna Korhonen, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12958">https://arxiv.org/abs/2409.12958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12958">https://arxiv.org/pdf/2409.12958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12958]] MURI: High-Quality Instruction Tuning Datasets for Low-Resource Languages via Reverse Instructions(https://arxiv.org/abs/2409.12958)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction tuning enhances large language models (LLMs) by aligning them with human preferences across diverse tasks. Traditional approaches to create instruction tuning datasets face serious challenges for low-resource languages due to their dependence on data annotation. This work introduces a novel method, Multilingual Reverse Instructions (MURI), which generates high-quality instruction tuning datasets for low-resource languages without requiring human annotators or pre-existing multilingual models. Utilizing reverse instructions and a translation pipeline, MURI produces instruction-output pairs from existing human-written texts in low-resource languages. This method ensures cultural relevance and diversity by sourcing texts from different native domains and applying filters to eliminate inappropriate content. Our dataset, MURI-IT, includes more than 2 million instruction-output pairs across 200 languages. Evaluation by native speakers and fine-tuning experiments with mT5 models demonstrate the approach's effectiveness for both NLU and open-ended generation. We publicly release datasets and models at this https URL.</li>
</ul>

<h3>Title: MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines</h3>
<ul>
<li><strong>Authors: </strong>Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12959">https://arxiv.org/abs/2409.12959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12959">https://arxiv.org/pdf/2409.12959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12959]] MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines(https://arxiv.org/abs/2409.12959)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has paved the way for AI search engines, e.g., SearchGPT, showcasing a new paradigm in human-internet interaction. However, most current AI search engines are limited to text-only settings, neglecting the multimodal user queries and the text-image interleaved nature of website information. Recently, Large Multimodal Models (LMMs) have made impressive strides. Yet, whether they can function as AI search engines remains under-explored, leaving the potential of LMMs in multimodal search an open question. To this end, we first design a delicate pipeline, MMSearch-Engine, to empower any LMMs with multimodal search capabilities. On top of this, we introduce MMSearch, a comprehensive evaluation benchmark to assess the multimodal search performance of LMMs. The curated dataset contains 300 manually collected instances spanning 14 subfields, which involves no overlap with the current LMMs' training data, ensuring the correct answer can only be obtained within searching. By using MMSearch-Engine, the LMMs are evaluated by performing three individual tasks (requery, rerank, and summarization), and one challenging end-to-end task with a complete searching process. We conduct extensive experiments on closed-source and open-source LMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best results, which surpasses the commercial product, Perplexity Pro, in the end-to-end task, demonstrating the effectiveness of our proposed pipeline. We further present error analysis to unveil current LMMs still struggle to fully grasp the multimodal search tasks, and conduct ablation study to indicate the potential of scaling test-time computation for AI search engine. We hope MMSearch may provide unique insights to guide the future development of multimodal AI search engine. Project Page: this https URL</li>
</ul>

<h3>Title: LVCD: Reference-based Lineart Video Colorization with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhitong Huang, Mohan Zhang, Jing Liao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12960">https://arxiv.org/abs/2409.12960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12960">https://arxiv.org/pdf/2409.12960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12960]] LVCD: Reference-based Lineart Video Colorization with Diffusion Models(https://arxiv.org/abs/2409.12960)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose the first video diffusion framework for reference-based lineart video colorization. Unlike previous works that rely solely on image generative models to colorize lineart frame by frame, our approach leverages a large-scale pretrained video diffusion model to generate colorized animation videos. This approach leads to more temporally consistent results and is better equipped to handle large motions. Firstly, we introduce Sketch-guided ControlNet which provides additional control to finetune an image-to-video diffusion model for controllable video synthesis, enabling the generation of animation videos conditioned on lineart. We then propose Reference Attention to facilitate the transfer of colors from the reference frame to other frames containing fast and expansive motions. Finally, we present a novel scheme for sequential sampling, incorporating the Overlapped Blending Module and Prev-Reference Attention, to extend the video diffusion model beyond its original fixed-length limitation for long video colorization. Both qualitative and quantitative results demonstrate that our method significantly outperforms state-of-the-art techniques in terms of frame and video quality, as well as temporal consistency. Moreover, our method is capable of generating high-quality, long temporal-consistent animation videos with large motions, which is not achievable in previous works. Our code and model are available at this https URL.</li>
</ul>

<h3>Title: CLAIR-A: Leveraging Large Language Models to Judge Audio Captions</h3>
<ul>
<li><strong>Authors: </strong>Tsung-Han Wu, Joseph E. Gonzalez, Trevor Darrell, David M. Chan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12962">https://arxiv.org/abs/2409.12962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12962">https://arxiv.org/pdf/2409.12962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12962]] CLAIR-A: Leveraging Large Language Models to Judge Audio Captions(https://arxiv.org/abs/2409.12962)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Automated Audio Captioning (AAC) task asks models to generate natural language descriptions of an audio input. Evaluating these machine-generated audio captions is a complex task that requires considering diverse factors, among them, auditory scene understanding, sound-object inference, temporal coherence, and the environmental context of the scene. While current methods focus on specific aspects, they often fail to provide an overall score that aligns well with human judgment. In this work, we propose CLAIR-A, a simple and flexible method that leverages the zero-shot capabilities of large language models (LLMs) to evaluate candidate audio captions by directly asking LLMs for a semantic distance score. In our evaluations, CLAIR-A better predicts human judgements of quality compared to traditional metrics, with a 5.8% relative accuracy improvement compared to the domain-specific FENSE metric and up to 11% over the best general-purpose measure on the Clotho-Eval dataset. Moreover, CLAIR-A offers more transparency by allowing the language model to explain the reasoning behind its scores, with these explanations rated up to 30% better by human evaluators than those provided by baseline methods. CLAIR-A is made publicly available at this https URL.</li>
</ul>

<h3>Title: Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner</h3>
<ul>
<li><strong>Authors: </strong>Yuzhang Shang, Bingxin Xu, Weitai Kang, Mu Cai, Yuheng Li, Zehao Wen, Zhen Dong, Kurt Keutzer, Yong Jae Lee, Yan Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.12963">https://arxiv.org/abs/2409.12963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.12963">https://arxiv.org/pdf/2409.12963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.12963]] Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner(https://arxiv.org/abs/2409.12963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Advancements in Large Language Models (LLMs) inspire various strategies for integrating video modalities. A key approach is Video-LLMs, which incorporate an optimizable interface linking sophisticated video encoders to LLMs. However, due to computation and data limitations, these Video-LLMs are typically pre-trained to process only short videos, limiting their broader application for understanding longer video content. Additionally, fine-tuning Video-LLMs to handle longer videos is cost-prohibitive. Consequently, it becomes essential to explore the interpolation of Video-LLMs under a completely training-free setting. In this paper, we first identify the primary challenges in interpolating Video-LLMs: (1) the video encoder and modality alignment projector are fixed, preventing the integration of additional frames into Video-LLMs, and (2) the LLM backbone is limited in its content length capabilities, which complicates the processing of an increased number of video tokens. To address these challenges, we propose a specific INTerPolation method for Video-LLMs (INTP-Video-LLMs). We introduce an alternative video token rearrangement technique that circumvents limitations imposed by the fixed video encoder and alignment projector. Furthermore, we introduce a training-free LLM context window extension method to enable Video-LLMs to understand a correspondingly increased number of visual tokens.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
