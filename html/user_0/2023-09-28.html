<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Eve Said Yes: AirBone Authentication for Head-Wearable Smart Voice Assistant. (arXiv:2309.15203v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15203">http://arxiv.org/abs/2309.15203</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15203]] Eve Said Yes: AirBone Authentication for Head-Wearable Smart Voice Assistant(http://arxiv.org/abs/2309.15203)</code></li>
<li>Summary: <p>Recent advances in machine learning and natural language processing have
fostered the enormous prosperity of smart voice assistants and their services,
e.g., Alexa, Google Home, Siri, etc. However, voice spoofing attacks are deemed
to be one of the major challenges of voice control security, and never stop
evolving such as deep-learning-based voice conversion and speech synthesis
techniques. To solve this problem outside the acoustic domain, we focus on
head-wearable devices, such as earbuds and virtual reality (VR) headsets, which
are feasible to continuously monitor the bone-conducted voice in the vibration
domain. Specifically, we identify that air and bone conduction (AC/BC) from the
same vocalization are coupled (or concurrent) and user-level unique, which
makes them suitable behavior and biometric factors for multi-factor
authentication (MFA). The legitimate user can defeat acoustic domain and even
cross-domain spoofing samples with the proposed two-stage AirBone
authentication. The first stage answers \textit{whether air and bone conduction
utterances are time domain consistent (TC)} and the second stage runs
\textit{bone conduction speaker recognition (BC-SR)}. The security level is
hence increased for two reasons: (1) current acoustic attacks on smart voice
assistants cannot affect bone conduction, which is in the vibration domain; (2)
even for advanced cross-domain attacks, the unique bone conduction features can
detect adversary's impersonation and machine-induced vibration. Finally,
AirBone authentication has good usability (the same level as voice
authentication) compared with traditional MFA and those specially designed to
enhance smart voice security. Our experimental results show that the proposed
AirBone authentication is usable and secure, and can be easily equipped by
commercial off-the-shelf head wearables with good user experience.
</p></li>
</ul>

<h3>Title: SOCI^+: An Enhanced Toolkit for Secure OutsourcedComputation on Integers. (arXiv:2309.15406v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15406">http://arxiv.org/abs/2309.15406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15406]] SOCI^+: An Enhanced Toolkit for Secure OutsourcedComputation on Integers(http://arxiv.org/abs/2309.15406)</code></li>
<li>Summary: <p>Secure outsourced computation is critical for cloud computing to safeguard
data confidentiality and ensure data usability. Recently, secure outsourced
computation schemes following a twin-server architecture based on partially
homomorphic cryptosystems have received increasing attention. The Secure
Outsourced Computation on Integers (SOCI) [1] toolkit is the state-of-the-art
among these schemes which can perform secure computation on integers without
requiring the costly bootstrapping operation as in fully homomorphic
encryption; however, SOCI suffers from relatively large computation and
communication overhead. In this paper, we propose SOCI+ which significantly
improves the performance of SOCI. Specifically, SOCI+ employs a novel (2,
2)-threshold Paillier cryptosystem with fast encryption and decryption as its
cryptographic primitive, and supports a suite of efficient secure arithmetic
computation on integers protocols, including a secure multiplication protocol
(SMUL), a secure comparison protocol (SCMP), a secure sign bit-acquisition
protocol (SSBA), and a secure division protocol (SDIV), all based on the (2,
2)-threshold Paillier cryptosystem with fast encryption and decryption. In
addition, SOCI+ incorporates an offline and online computation mechanism to
further optimize its performance. We perform rigorous theoretical analysis to
prove the correctness and security of SOCI+. Compared with SOCI, our
experimental evaluation shows that SOCI+ is up to 5.4 times more efficient in
computation and 40% less in communication overhead.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Critical Infrastructure Security Goes to Space: Leveraging Lessons Learned on the Ground. (arXiv:2309.15232v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15232">http://arxiv.org/abs/2309.15232</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15232]] Critical Infrastructure Security Goes to Space: Leveraging Lessons Learned on the Ground(http://arxiv.org/abs/2309.15232)</code></li>
<li>Summary: <p>Space systems enable essential communications, navigation, imaging and
sensing for a variety of domains, including agriculture, commerce,
transportation, and emergency operations by first responders. Protecting the
cybersecurity of these critical infrastructure systems is essential. While the
space environment brings unique constraints to managing cybersecurity risks,
lessons learned about risks and effective defenses in other critical
infrastructure domains can help us to design effective defenses for space
systems. In particular, discoveries regarding cybersecurity for industrial
control systems (ICS) for energy, manufacturing, transportation, and the
consumer and industrial Internet of Things (IoT) offer insights into
cybersecurity for the space domain. This paper provides an overview of ICS and
space system commonalities, lessons learned about cybersecurity for ICS that
can be applied to space systems, and recommendations for future research and
development to secure increasingly critical space systems.
</p></li>
</ul>

<h3>Title: Evaluation and Analysis of Standard Security Technology in V2X Communication -- Exploring ECQV Implicit Certificate Cracking. (arXiv:2309.15340v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15340">http://arxiv.org/abs/2309.15340</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15340]] Evaluation and Analysis of Standard Security Technology in V2X Communication -- Exploring ECQV Implicit Certificate Cracking(http://arxiv.org/abs/2309.15340)</code></li>
<li>Summary: <p>In IEEE 1609.2 and IEEE 1609.2.1 standards for Vehicle-to-everything (V2X)
secure communication, various security algorithms based on Elliptic Curve
Cryptography (ECC) have been adopted and designed. To enhance the efficiency of
the Security Credential Management System (SCMS), this study evaluates the
computational time for key generation, key expansion, signature generation, and
signature verification under different security strengths. This study discusses
relevant techniques based on Elliptic Curve Qu-Vanstone (ECQV) implicit
certificates, analyzes the length of uncompressed elliptic curve points,
compressed elliptic curve points, explicit certificates, and implicit
certificates. Furthermore, this study proposes mathematical models to
demonstrate the probability of ECQV cracking and provides suggestions for
mitigating ECQV cracking risks.
</p></li>
</ul>

<h3>Title: Raij\=u: Reinforcement Learning-Guided Post-Exploitation for Automating Security Assessment of Network Systems. (arXiv:2309.15518v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15518">http://arxiv.org/abs/2309.15518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15518]] Raij\=u: Reinforcement Learning-Guided Post-Exploitation for Automating Security Assessment of Network Systems(http://arxiv.org/abs/2309.15518)</code></li>
<li>Summary: <p>In order to assess the risks of a network system, it is important to
investigate the behaviors of attackers after successful exploitation, which is
called post-exploitation. Although there are various efficient tools supporting
post-exploitation implementation, no application can automate this process.
Most of the steps of this process are completed by experts who have profound
knowledge of security, known as penetration testers or pen-testers. To this
end, our study proposes the Raij\=u framework, a Reinforcement Learning
(RL)-driven automation approach that assists pen-testers in quickly
implementing the process of post-exploitation for security-level evaluation in
network systems. We implement two RL algorithms, Advantage Actor-Critic (A2C)
and Proximal Policy Optimization (PPO), to train specialized agents capable of
making intelligent actions, which are Metasploit modules to automatically
launch attacks of privileges escalation, gathering hashdump, and lateral
movement. By leveraging RL, we aim to empower these agents with the ability to
autonomously select and execute actions that can exploit vulnerabilities in
target systems. This approach allows us to automate certain aspects of the
penetration testing workflow, making it more efficient and responsive to
emerging threats and vulnerabilities. The experiments are performed in four
real environments with agents trained in thousands of episodes. The agents
automatically select actions and launch attacks on the environments and achieve
over 84\% of successful attacks with under 55 attack steps given. Moreover, the
A2C algorithm has proved extremely effective in the selection of proper actions
for automation of post-exploitation.
</p></li>
</ul>

<h3>Title: Cyber Security Requirements for Platforms Enhancing AI Reproducibility. (arXiv:2309.15525v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15525">http://arxiv.org/abs/2309.15525</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15525]] Cyber Security Requirements for Platforms Enhancing AI Reproducibility(http://arxiv.org/abs/2309.15525)</code></li>
<li>Summary: <p>Scientific research is increasingly reliant on computational methods, posing
challenges for ensuring research reproducibility. This study focuses on the
field of artificial intelligence (AI) and introduces a new framework for
evaluating AI platforms for reproducibility from a cyber security standpoint to
address the security challenges associated with AI research. Using this
framework, five popular AI reproducibility platforms; Floydhub, BEAT, Codalab,
Kaggle, and OpenML were assessed. The analysis revealed that none of these
platforms fully incorporates the necessary cyber security measures essential
for robust reproducibility. Kaggle and Codalab, however, performed better in
terms of implementing cyber security measures covering aspects like security,
privacy, usability, and trust. Consequently, the study provides tailored
recommendations for different user scenarios, including individual researchers,
small laboratories, and large corporations. It emphasizes the importance of
integrating specific cyber security features into AI platforms to address the
challenges associated with AI reproducibility, ultimately advancing
reproducibility in this field. Moreover, the proposed framework can be applied
beyond AI platforms, serving as a versatile tool for evaluating a wide range of
systems and applications from a cyber security perspective.
</p></li>
</ul>

<h3>Title: Grain-128PLE: Generic Physical-Layer Encryption for IoT Networks. (arXiv:2309.15569v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15569">http://arxiv.org/abs/2309.15569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15569]] Grain-128PLE: Generic Physical-Layer Encryption for IoT Networks(http://arxiv.org/abs/2309.15569)</code></li>
<li>Summary: <p>Physical layer security (PLS) encompasses techniques proposed at the physical
layer to achieve information security objectives while requiring a minimal
resource footprint. The channel coding-based secrecy and signal
modulation-based encryption approaches are reliant on certain channel
conditions or a certain communications protocol stack to operate on, which
prevents them from being a generic solution. This paper presents Grain-128PLE,
a lightweight physical layer encryption (PLE) scheme that is derived from the
Grain-128AEAD v2 stream cipher. The Grain-128PLE stream cipher performs
encryption and decryption at the physical layer, in between the channel coding
and signal modulation processes. This placement, like that of the A5 stream
cipher that had been used in the GSM communications standard, makes it a
generic solution for providing data confidentiality in IoT networks. The design
of Grain-128PLE maintains the structure of the main building blocks of the
original Grain-128AEAD v2 stream cipher, evaluated for its security strength
during NIST's recent Lightweight Cryptography competition, and is therefore
expected to achieve similar levels of security.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Contrastive Continual Multi-view Clustering with Filtered Structural Fusion. (arXiv:2309.15135v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15135">http://arxiv.org/abs/2309.15135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15135]] Contrastive Continual Multi-view Clustering with Filtered Structural Fusion(http://arxiv.org/abs/2309.15135)</code></li>
<li>Summary: <p>Multi-view clustering thrives in applications where views are collected in
advance by extracting consistent and complementary information among views.
However, it overlooks scenarios where data views are collected sequentially,
i.e., real-time data. Due to privacy issues or memory burden, previous views
are not available with time in these situations. Some methods are proposed to
handle it but are trapped in a stability-plasticity dilemma. In specific, these
methods undergo a catastrophic forgetting of prior knowledge when a new view is
attained. Such a catastrophic forgetting problem (CFP) would cause the
consistent and complementary information hard to get and affect the clustering
performance. To tackle this, we propose a novel method termed Contrastive
Continual Multi-view Clustering with Filtered Structural Fusion (CCMVC-FSF).
Precisely, considering that data correlations play a vital role in clustering
and prior knowledge ought to guide the clustering process of a new view, we
develop a data buffer with fixed size to store filtered structural information
and utilize it to guide the generation of a robust partition matrix via
contrastive learning. Furthermore, we theoretically connect CCMVC-FSF with
semi-supervised learning and knowledge distillation. Extensive experiments
exhibit the excellence of the proposed method.
</p></li>
</ul>

<h3>Title: Position and Orientation-Aware One-Shot Learning for Medical Action Recognition from Signal Data. (arXiv:2309.15635v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15635">http://arxiv.org/abs/2309.15635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15635]] Position and Orientation-Aware One-Shot Learning for Medical Action Recognition from Signal Data(http://arxiv.org/abs/2309.15635)</code></li>
<li>Summary: <p>In this work, we propose a position and orientation-aware one-shot learning
framework for medical action recognition from signal data. The proposed
framework comprises two stages and each stage includes signal-level image
generation (SIG), cross-attention (CsA), dynamic time warping (DTW) modules and
the information fusion between the proposed privacy-preserved position and
orientation features. The proposed SIG method aims to transform the raw
skeleton data into privacy-preserved features for training. The CsA module is
developed to guide the network in reducing medical action recognition bias and
more focusing on important human body parts for each specific action, aimed at
addressing similar medical action related issues. Moreover, the DTW module is
employed to minimize temporal mismatching between instances and further improve
model performance. Furthermore, the proposed privacy-preserved
orientation-level features are utilized to assist the position-level features
in both of the two stages for enhancing medical action recognition performance.
Extensive experimental results on the widely-used and well-known NTU RGB+D 60,
NTU RGB+D 120, and PKU-MMD datasets all demonstrate the effectiveness of the
proposed method, which outperforms the other state-of-the-art methods with
general dataset partitioning by 2.7%, 6.2% and 4.1%, respectively.
</p></li>
</ul>

<h3>Title: SANGEA: Scalable and Attributed Network Generation. (arXiv:2309.15648v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15648">http://arxiv.org/abs/2309.15648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15648]] SANGEA: Scalable and Attributed Network Generation(http://arxiv.org/abs/2309.15648)</code></li>
<li>Summary: <p>The topic of synthetic graph generators (SGGs) has recently received much
attention due to the wave of the latest breakthroughs in generative modelling.
However, many state-of-the-art SGGs do not scale well with the graph size.
Indeed, in the generation process, all the possible edges for a fixed number of
nodes must often be considered, which scales in $\mathcal{O}(N^2)$, with $N$
being the number of nodes in the graph. For this reason, many state-of-the-art
SGGs are not applicable to large graphs. In this paper, we present SANGEA, a
sizeable synthetic graph generation framework which extends the applicability
of any SGG to large graphs. By first splitting the large graph into
communities, SANGEA trains one SGG per community, then links the community
graphs back together to create a synthetic large graph. Our experiments show
that the graphs generated by SANGEA have high similarity to the original graph,
in terms of both topology and node feature distribution. Additionally, these
generated graphs achieve high utility on downstream tasks such as link
prediction. Finally, we provide a privacy assessment of the generated graphs to
show that, even though they have excellent utility, they also achieve
reasonable privacy scores.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Disinformation Detection: An Evolving Challenge in the Age of LLMs. (arXiv:2309.15847v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15847">http://arxiv.org/abs/2309.15847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15847]] Disinformation Detection: An Evolving Challenge in the Age of LLMs(http://arxiv.org/abs/2309.15847)</code></li>
<li>Summary: <p>The advent of generative Large Language Models (LLMs) such as ChatGPT has
catalyzed transformative advancements across multiple domains. However,
alongside these advancements, they have also introduced potential threats. One
critical concern is the misuse of LLMs by disinformation spreaders, leveraging
these models to generate highly persuasive yet misleading content that
challenges the disinformation detection system. This work aims to address this
issue by answering three research questions: (1) To what extent can the current
disinformation detection technique reliably detect LLM-generated
disinformation? (2) If traditional techniques prove less effective, can LLMs
themself be exploited to serve as a robust defense against advanced
disinformation? and, (3) Should both these strategies falter, what novel
approaches can be proposed to counter this burgeoning threat effectively? A
holistic exploration for the formation and detection of disinformation is
conducted to foster this line of research.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Defending Against Physical Adversarial Patch Attacks on Infrared Human Detection. (arXiv:2309.15519v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15519">http://arxiv.org/abs/2309.15519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15519]] Defending Against Physical Adversarial Patch Attacks on Infrared Human Detection(http://arxiv.org/abs/2309.15519)</code></li>
<li>Summary: <p>Infrared detection is an emerging technique for safety-critical tasks owing
to its remarkable anti-interference capability. However, recent studies have
revealed that it is vulnerable to physically-realizable adversarial patches,
posing risks in its real-world applications. To address this problem, we are
the first to investigate defense strategies against adversarial patch attacks
on infrared detection, especially human detection. We have devised a
straightforward defense strategy, patch-based occlusion-aware detection (POD),
which efficiently augments training samples with random patches and
subsequently detects them. POD not only robustly detects people but also
identifies adversarial patch locations. Surprisingly, while being extremely
computationally efficient, POD easily generalizes to state-of-the-art
adversarial patch attacks that are unseen during training. Furthermore, POD
improves detection precision even in a clean (i.e., no-patch) situation due to
the data augmentation effect. Evaluation demonstrated that POD is robust to
adversarial patches of various shapes and sizes. The effectiveness of our
baseline approach is shown to be a viable defense mechanism for real-world
infrared human detection systems, paving the way for exploring future research
directions.
</p></li>
</ul>

<h3>Title: LivDet2023 -- Fingerprint Liveness Detection Competition: Advancing Generalization. (arXiv:2309.15578v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15578">http://arxiv.org/abs/2309.15578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15578]] LivDet2023 -- Fingerprint Liveness Detection Competition: Advancing Generalization(http://arxiv.org/abs/2309.15578)</code></li>
<li>Summary: <p>The International Fingerprint Liveness Detection Competition (LivDet) is a
biennial event that invites academic and industry participants to prove their
advancements in Fingerprint Presentation Attack Detection (PAD). This edition,
LivDet2023, proposed two challenges, Liveness Detection in Action and
Fingerprint Representation, to evaluate the efficacy of PAD embedded in
verification systems and the effectiveness and compactness of feature sets. A
third, hidden challenge is the inclusion of two subsets in the training set
whose sensor information is unknown, testing participants ability to generalize
their models. Only bona fide fingerprint samples were provided to participants,
and the competition reports and assesses the performance of their algorithms
suffering from this limitation in data availability.
</p></li>
</ul>

<h3>Title: Breaking NoC Anonymity using Flow Correlation Attack. (arXiv:2309.15687v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15687">http://arxiv.org/abs/2309.15687</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15687]] Breaking NoC Anonymity using Flow Correlation Attack(http://arxiv.org/abs/2309.15687)</code></li>
<li>Summary: <p>Network-on-Chip (NoC) is widely used as the internal communication fabric in
today's multicore System-on-Chip (SoC) designs. Security of the on-chip
communication is crucial because exploiting any vulnerability in shared NoC
would be a goldmine for an attacker. NoC security relies on effective
countermeasures against diverse attacks. We investigate the security strength
of existing anonymous routing protocols in NoC architectures. Specifically,
this paper makes two important contributions. We show that the existing
anonymous routing is vulnerable to machine learning (ML) based flow correlation
attacks on NoCs. We propose a lightweight anonymous routing that use traffic
obfuscation techniques which can defend against ML-based flow correlation
attacks. Experimental studies using both real and synthetic traffic reveal that
our proposed attack is successful against state-of-the-art anonymous routing in
NoC architectures with a high accuracy (up to 99%) for diverse traffic
patterns, while our lightweight countermeasure can defend against ML-based
attacks with minor hardware and performance overhead.
</p></li>
</ul>

<h3>Title: PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling. (arXiv:2309.15375v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15375">http://arxiv.org/abs/2309.15375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15375]] PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling(http://arxiv.org/abs/2309.15375)</code></li>
<li>Summary: <p>An electrocardiogram (ECG or EKG) is a medical test that measures the heart's
electrical activity. ECGs are often used to diagnose and monitor a wide range
of heart conditions, including arrhythmias, heart attacks, and heart failure.
On the one hand, the conventional ECG requires clinical measurement, which
restricts its deployment to medical facilities. On the other hand, single-lead
ECG has become popular on wearable devices using administered procedures. An
alternative to ECG is Photoplethysmography (PPG), which uses non-invasive,
low-cost optical methods to measure cardiac physiology, making it a suitable
option for capturing vital heart signs in daily life. As a result, it has
become increasingly popular in health monitoring and is used in various
clinical and commercial wearable devices. While ECG and PPG correlate strongly,
the latter does not offer significant clinical diagnostic value. Here, we
propose a subject-independent attention-based deep state-space model to
translate PPG signals to corresponding ECG waveforms. The model is highly
data-efficient by incorporating prior knowledge in terms of probabilistic
graphical models. Notably, the model enables the detection of atrial
fibrillation (AFib), the most common heart rhythm disorder in adults, by
complementing ECG's accuracy with continuous PPG monitoring. We evaluated the
model on 55 subjects from the MIMIC III database. Quantitative and qualitative
experimental results demonstrate the effectiveness and efficiency of our
approach.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: VPA: Fully Test-Time Visual Prompt Adaptation. (arXiv:2309.15251v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15251">http://arxiv.org/abs/2309.15251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15251]] VPA: Fully Test-Time Visual Prompt Adaptation(http://arxiv.org/abs/2309.15251)</code></li>
<li>Summary: <p>Textual prompt tuning has demonstrated significant performance improvements
in adapting natural language processing models to a variety of downstream tasks
by treating hand-engineered prompts as trainable parameters. Inspired by the
success of textual prompting, several studies have investigated the efficacy of
visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA),
the first framework that generalizes visual prompting with test-time
adaptation. VPA introduces a small number of learnable tokens, enabling fully
test-time and storage-efficient adaptation without necessitating source-domain
information. We examine our VPA design under diverse adaptation settings,
encompassing single-image, batched-image, and pseudo-label adaptation. We
evaluate VPA on multiple tasks, including out-of-distribution (OOD)
generalization, corruption robustness, and domain adaptation. Experimental
results reveal that VPA effectively enhances OOD generalization by 3.3% across
various models, surpassing previous test-time approaches. Furthermore, we show
that VPA improves corruption robustness by 6.5% compared to strong baselines.
Finally, we demonstrate that VPA also boosts domain adaptation performance by
relatively 5.2%. Our VPA also exhibits marked effectiveness in improving the
robustness of zero-shot recognition for vision-language models.
</p></li>
</ul>

<h3>Title: KDD-LOAM: Jointly Learned Keypoint Detector and Descriptors Assisted LiDAR Odometry and Mapping. (arXiv:2309.15394v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15394">http://arxiv.org/abs/2309.15394</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15394]] KDD-LOAM: Jointly Learned Keypoint Detector and Descriptors Assisted LiDAR Odometry and Mapping(http://arxiv.org/abs/2309.15394)</code></li>
<li>Summary: <p>Sparse keypoint matching based on distinct 3D feature representations can
improve the efficiency and robustness of point cloud registration. Existing
learning-based 3D descriptors and keypoint detectors are either independent or
loosely coupled, so they cannot fully adapt to each other. In this work, we
propose a tightly coupled keypoint detector and descriptor (TCKDD) based on a
multi-task fully convolutional network with a probabilistic detection loss. In
particular, this self-supervised detection loss fully adapts the keypoint
detector to any jointly learned descriptors and benefits the self-supervised
learning of descriptors. Extensive experiments on both indoor and outdoor
datasets show that our TCKDD achieves state-of-the-art performance in point
cloud registration. Furthermore, we design a keypoint detector and
descriptors-assisted LiDAR odometry and mapping framework (KDD-LOAM), whose
real-time odometry relies on keypoint descriptor matching-based RANSAC. The
sparse keypoints are further used for efficient scan-to-map registration and
mapping. Experiments on KITTI dataset demonstrate that KDD-LOAM significantly
surpasses LOAM and shows competitive performance in odometry.
</p></li>
</ul>

<h3>Title: Cross-Dataset Experimental Study of Radar-Camera Fusion in Bird's-Eye View. (arXiv:2309.15465v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15465">http://arxiv.org/abs/2309.15465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15465]] Cross-Dataset Experimental Study of Radar-Camera Fusion in Bird's-Eye View(http://arxiv.org/abs/2309.15465)</code></li>
<li>Summary: <p>By exploiting complementary sensor information, radar and camera fusion
systems have the potential to provide a highly robust and reliable perception
system for advanced driver assistance systems and automated driving functions.
Recent advances in camera-based object detection offer new radar-camera fusion
possibilities with bird's eye view feature maps. In this work, we propose a
novel and flexible fusion network and evaluate its performance on two datasets:
nuScenes and View-of-Delft. Our experiments reveal that while the camera branch
needs large and diverse training data, the radar branch benefits more from a
high-performance radar. Using transfer learning, we improve the camera's
performance on the smaller dataset. Our results further demonstrate that the
radar-camera fusion approach significantly outperforms the camera-only and
radar-only baselines.
</p></li>
</ul>

<h3>Title: The Robust Semantic Segmentation UNCV2023 Challenge Results. (arXiv:2309.15478v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15478">http://arxiv.org/abs/2309.15478</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15478]] The Robust Semantic Segmentation UNCV2023 Challenge Results(http://arxiv.org/abs/2309.15478)</code></li>
<li>Summary: <p>This paper outlines the winning solutions employed in addressing the MUAD
uncertainty quantification challenge held at ICCV 2023. The challenge was
centered around semantic segmentation in urban environments, with a particular
focus on natural adversarial scenarios. The report presents the results of 19
submitted entries, with numerous techniques drawing inspiration from
cutting-edge uncertainty quantification methodologies presented at prominent
conferences in the fields of computer vision and machine learning and journals
over the past few years. Within this document, the challenge is introduced,
shedding light on its purpose and objectives, which primarily revolved around
enhancing the robustness of semantic segmentation in urban scenes under varying
natural adversarial conditions. The report then delves into the top-performing
solutions. Moreover, the document aims to provide a comprehensive overview of
the diverse solutions deployed by all participants. By doing so, it seeks to
offer readers a deeper insight into the array of strategies that can be
leveraged to effectively handle the inherent uncertainties associated with
autonomous driving and semantic segmentation, especially within urban
environments.
</p></li>
</ul>

<h3>Title: Transferability of Representations Learned using Supervised Contrastive Learning Trained on a Multi-Domain Dataset. (arXiv:2309.15486v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15486">http://arxiv.org/abs/2309.15486</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15486]] Transferability of Representations Learned using Supervised Contrastive Learning Trained on a Multi-Domain Dataset(http://arxiv.org/abs/2309.15486)</code></li>
<li>Summary: <p>Contrastive learning has shown to learn better quality representations than
models trained using cross-entropy loss. They also transfer better to
downstream datasets from different domains. However, little work has been done
to explore the transferability of representations learned using contrastive
learning when trained on a multi-domain dataset. In this paper, a study has
been conducted using the Supervised Contrastive Learning framework to learn
representations from the multi-domain DomainNet dataset and then evaluate the
transferability of the representations learned on other downstream datasets.
The fixed feature linear evaluation protocol will be used to evaluate the
transferability on 7 downstream datasets that were chosen across different
domains. The results obtained are compared to a baseline model that was trained
using the widely used cross-entropy loss. Empirical results from the
experiments showed that on average, the Supervised Contrastive Learning model
performed 6.05% better than the baseline model on the 7 downstream datasets.
The findings suggest that Supervised Contrastive Learning models can
potentially learn more robust representations that transfer better across
domains than cross-entropy models when trained on a multi-domain dataset.
</p></li>
</ul>

<h3>Title: Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15564">http://arxiv.org/abs/2309.15564</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15564]] Jointly Training Large Autoregressive Multimodal Models(http://arxiv.org/abs/2309.15564)</code></li>
<li>Summary: <p>In recent years, advances in the large-scale pretraining of language and
text-to-image models have revolutionized the field of machine learning. Yet,
integrating these two modalities into a single, robust model capable of
generating seamless multimodal outputs remains a significant challenge. To
address this gap, we present the Joint Autoregressive Mixture (JAM) framework,
a modular approach that systematically fuses existing text and image generation
models. We also introduce a specialized, data-efficient instruction-tuning
strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned
model demonstrates unparalleled performance in generating high-quality
multimodal outputs and represents the first model explicitly designed for this
purpose.
</p></li>
</ul>

<h3>Title: HPL-ViT: A Unified Perception Framework for Heterogeneous Parallel LiDARs in V2V. (arXiv:2309.15572v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15572">http://arxiv.org/abs/2309.15572</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15572]] HPL-ViT: A Unified Perception Framework for Heterogeneous Parallel LiDARs in V2V(http://arxiv.org/abs/2309.15572)</code></li>
<li>Summary: <p>To develop the next generation of intelligent LiDARs, we propose a novel
framework of parallel LiDARs and construct a hardware prototype in our
experimental platform, DAWN (Digital Artificial World for Natural). It
emphasizes the tight integration of physical and digital space in LiDAR
systems, with networking being one of its supported core features. In the
context of autonomous driving, V2V (Vehicle-to-Vehicle) technology enables
efficient information sharing between different agents which significantly
promotes the development of LiDAR networks. However, current research operates
under an ideal situation where all vehicles are equipped with identical LiDAR,
ignoring the diversity of LiDAR categories and operating frequencies. In this
paper, we first utilize OpenCDA and RLS (Realistic LiDAR Simulation) to
construct a novel heterogeneous LiDAR dataset named OPV2V-HPL. Additionally, we
present HPL-ViT, a pioneering architecture designed for robust feature fusion
in heterogeneous and dynamic scenarios. It uses a graph-attention Transformer
to extract domain-specific features for each agent, coupled with a
cross-attention mechanism for the final fusion. Extensive experiments on
OPV2V-HPL demonstrate that HPL-ViT achieves SOTA (state-of-the-art) performance
in all settings and exhibits outstanding generalization capabilities.
</p></li>
</ul>

<h3>Title: Learning Spatial-Temporal Regularized Tensor Sparse RPCA for Background Subtraction. (arXiv:2309.15576v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15576">http://arxiv.org/abs/2309.15576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15576]] Learning Spatial-Temporal Regularized Tensor Sparse RPCA for Background Subtraction(http://arxiv.org/abs/2309.15576)</code></li>
<li>Summary: <p>Video background subtraction is one of the fundamental problems in computer
vision that aims to segment all moving objects. Robust principal component
analysis has been identified as a promising unsupervised paradigm for
background subtraction tasks in the last decade thanks to its competitive
performance in a number of benchmark datasets. Tensor robust principal
component analysis variations have improved background subtraction performance
further. However, because moving object pixels in the sparse component are
treated independently and do not have to adhere to spatial-temporal
structured-sparsity constraints, performance is reduced for sequences with
dynamic backgrounds, camouflaged, and camera jitter problems. In this work, we
present a spatial-temporal regularized tensor sparse RPCA algorithm for precise
background subtraction. Within the sparse component, we impose spatial-temporal
regularizations in the form of normalized graph-Laplacian matrices. To do this,
we build two graphs, one across the input tensor spatial locations and the
other across its frontal slices in the time domain. While maximizing the
objective function, we compel the tensor sparse component to serve as the
spatiotemporal eigenvectors of the graph-Laplacian matrices. The disconnected
moving object pixels in the sparse component are preserved by the proposed
graph-based regularizations since they both comprise of spatiotemporal
subspace-based structure. Additionally, we propose a unique objective function
that employs batch and online-based optimization methods to jointly maximize
the background-foreground and spatial-temporal regularization components.
Experiments are performed on six publicly available background subtraction
datasets that demonstrate the superior performance of the proposed algorithm
compared to several existing methods. Our source code will be available very
soon.
</p></li>
</ul>

<h3>Title: Physics Inspired Hybrid Attention for SAR Target Recognition. (arXiv:2309.15697v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15697">http://arxiv.org/abs/2309.15697</a></li>
<li>Code URL: https://github.com/xai4sar/piha</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15697]] Physics Inspired Hybrid Attention for SAR Target Recognition(http://arxiv.org/abs/2309.15697)</code></li>
<li>Summary: <p>There has been a recent emphasis on integrating physical models and deep
neural networks (DNNs) for SAR target recognition, to improve performance and
achieve a higher level of physical interpretability. The attributed scattering
center (ASC) parameters garnered the most interest, being considered as
additional input data or features for fusion in most methods. However, the
performance greatly depends on the ASC optimization result, and the fusion
strategy is not adaptable to different types of physical information.
Meanwhile, the current evaluation scheme is inadequate to assess the model's
robustness and generalizability. Thus, we propose a physics inspired hybrid
attention (PIHA) mechanism and the once-for-all (OFA) evaluation protocol to
address the above issues. PIHA leverages the high-level semantics of physical
information to activate and guide the feature group aware of local semantics of
target, so as to re-weight the feature importance based on knowledge prior. It
is flexible and generally applicable to various physical models, and can be
integrated into arbitrary DNNs without modifying the original architecture. The
experiments involve a rigorous assessment using the proposed OFA, which entails
training and validating a model on either sufficient or limited data and
evaluating on multiple test sets with different data distributions. Our method
outperforms other state-of-the-art approaches in 12 test scenarios with same
ASC parameters. Moreover, we analyze the working mechanism of PIHA and evaluate
various PIHA enabled DNNs. The experiments also show PIHA is effective for
different physical information. The source code together with the adopted
physical information is available at https://github.com/XAI4SAR.
</p></li>
</ul>

<h3>Title: Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback. (arXiv:2309.15762v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15762">http://arxiv.org/abs/2309.15762</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15762]] Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback(http://arxiv.org/abs/2309.15762)</code></li>
<li>Summary: <p>We propose a method for adapting neural networks to distribution shifts at
test-time. In contrast to training-time robustness mechanisms that attempt to
anticipate and counter the shift, we create a closed-loop system and make use
of a test-time feedback signal to adapt a network on the fly. We show that this
loop can be effectively implemented using a learning-based function, which
realizes an amortized optimizer for the network. This leads to an adaptation
method, named Rapid Network Adaptation (RNA), that is notably more flexible and
orders of magnitude faster than the baselines. Through a broad set of
experiments using various adaptation signals and target tasks, we study the
efficiency and flexibility of this method. We perform the evaluations using
various datasets (Taskonomy, Replica, ScanNet, Hypersim, COCO, ImageNet), tasks
(depth, optical flow, semantic segmentation, classification), and distribution
shifts (Cross-datasets, 2D and 3D Common Corruptions) with promising results.
We end with a discussion on general formulations for handling distribution
shifts and our observations from comparing with similar approaches from other
domains.
</p></li>
</ul>

<h3>Title: Joint-YODNet: A Light-weight Object Detector for UAVs to Achieve Above 100fps. (arXiv:2309.15782v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15782">http://arxiv.org/abs/2309.15782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15782]] Joint-YODNet: A Light-weight Object Detector for UAVs to Achieve Above 100fps(http://arxiv.org/abs/2309.15782)</code></li>
<li>Summary: <p>Small object detection via UAV (Unmanned Aerial Vehicle) images captured from
drones and radar is a complex task with several formidable challenges. This
domain encompasses numerous complexities that impede the accurate detection and
localization of small objects. To address these challenges, we propose a novel
method called JointYODNet for UAVs to detect small objects, leveraging a joint
loss function specifically designed for this task. Our method revolves around
the development of a joint loss function tailored to enhance the detection
performance of small objects. Through extensive experimentation on a diverse
dataset of UAV images captured under varying environmental conditions, we
evaluated different variations of the loss function and determined the most
effective formulation. The results demonstrate that our proposed joint loss
function outperforms existing methods in accurately localizing small objects.
Specifically, our method achieves a recall of 0.971, and a F1Score of 0.975,
surpassing state-of-the-art techniques. Additionally, our method achieves a
mAP@.5(%) of 98.6, indicating its robustness in detecting small objects across
varying scales
</p></li>
</ul>

<h3>Title: Partial Transport for Point-Cloud Registration. (arXiv:2309.15787v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15787">http://arxiv.org/abs/2309.15787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15787]] Partial Transport for Point-Cloud Registration(http://arxiv.org/abs/2309.15787)</code></li>
<li>Summary: <p>Point cloud registration plays a crucial role in various fields, including
robotics, computer graphics, and medical imaging. This process involves
determining spatial relationships between different sets of points, typically
within a 3D space. In real-world scenarios, complexities arise from non-rigid
movements and partial visibility, such as occlusions or sensor noise, making
non-rigid registration a challenging problem. Classic non-rigid registration
methods are often computationally demanding, suffer from unstable performance,
and, importantly, have limited theoretical guarantees. The optimal transport
problem and its unbalanced variations (e.g., the optimal partial transport
problem) have emerged as powerful tools for point-cloud registration,
establishing a strong benchmark in this field. These methods view point clouds
as empirical measures and provide a mathematically rigorous way to quantify the
`correspondence' between (the transformed) source and target points. In this
paper, we approach the point-cloud registration problem through the lens of
optimal transport theory and first propose a comprehensive set of non-rigid
registration methods based on the optimal partial transportation problem.
Subsequently, leveraging the emerging work on efficient solutions to the
one-dimensional optimal partial transport problem, we extend our proposed
algorithms via slicing to gain significant computational efficiency, resulting
in fast and robust non-rigid registration algorithms. We demonstrate the
effectiveness of our proposed methods and compare them against baselines on
various 3D and 2D non-rigid registration problems where the source and target
point clouds are corrupted by random noise.
</p></li>
</ul>

<h3>Title: Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution. (arXiv:2309.15609v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15609">http://arxiv.org/abs/2309.15609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15609]] Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution(http://arxiv.org/abs/2309.15609)</code></li>
<li>Summary: <p>This paper presents an end-to-end solution for the creation of fully
automated conference meeting transcripts and their machine translations into
various languages. This tool has been developed at the World Intellectual
Property Organization (WIPO) using in-house developed speech-to-text (S2T) and
machine translation (MT) components. Beyond describing data collection and
fine-tuning, resulting in a highly customized and robust system, this paper
describes the architecture and evolution of the technical components as well as
highlights the business impact and benefits from the user side. We also point
out particular challenges in the evolution and adoption of the system and how
the new approach created a new product and replaced existing established
workflows in conference management documentation.
</p></li>
</ul>

<h3>Title: Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization. (arXiv:2309.15686v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15686">http://arxiv.org/abs/2309.15686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15686]] Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization(http://arxiv.org/abs/2309.15686)</code></li>
<li>Summary: <p>Incorporating longer context has been shown to benefit machine translation,
but the inclusion of context in end-to-end speech translation (E2E-ST) remains
under-studied. To bridge this gap, we introduce target language context in
E2E-ST, enhancing coherence and overcoming memory constraints of extended audio
segments. Additionally, we propose context dropout to ensure robustness to the
absence of context, and further improve performance by adding speaker
information. Our proposed contextual E2E-ST outperforms the isolated
utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational
speech, contextual information primarily contributes to capturing context
style, as well as resolving anaphora and named entities.
</p></li>
</ul>

<h3>Title: Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting. (arXiv:2309.15169v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15169">http://arxiv.org/abs/2309.15169</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15169]] Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting(http://arxiv.org/abs/2309.15169)</code></li>
<li>Summary: <p>Multivariate time series (MTS) forecasting involves predicting future time
series data based on historical observations. Existing research primarily
emphasizes the development of complex spatial-temporal models that capture
spatial dependencies and temporal correlations among time series variables
explicitly. However, recent advances have been impeded by challenges relating
to data scarcity and model robustness. To address these issues, we propose
Spatial-Temporal Masked Autoencoders (STMAE), an MTS forecasting framework that
leverages masked autoencoders to enhance the performance of spatial-temporal
baseline models. STMAE consists of two learning stages. In the pretraining
stage, an encoder-decoder architecture is employed. The encoder processes the
partially visible MTS data produced by a novel dual-masking strategy, including
biased random walk-based spatial masking and patch-based temporal masking.
Subsequently, the decoders aim to reconstruct the masked counterparts from both
spatial and temporal perspectives. The pretraining stage establishes a
challenging pretext task, compelling the encoder to learn robust
spatial-temporal patterns. In the fine-tuning stage, the pretrained encoder is
retained, and the original decoder from existing spatial-temporal models is
appended for forecasting. Extensive experiments are conducted on multiple MTS
benchmarks. The promising results demonstrate that integrating STMAE into
various spatial-temporal models can largely enhance their MTS forecasting
capability.
</p></li>
</ul>

<h3>Title: Scaling Representation Learning from Ubiquitous ECG with State-Space Models. (arXiv:2309.15292v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15292">http://arxiv.org/abs/2309.15292</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15292]] Scaling Representation Learning from Ubiquitous ECG with State-Space Models(http://arxiv.org/abs/2309.15292)</code></li>
<li>Summary: <p>Ubiquitous sensing from wearable devices in the wild holds promise for
enhancing human well-being, from diagnosing clinical conditions and measuring
stress to building adaptive health promoting scaffolds. But the large volumes
of data therein across heterogeneous contexts pose challenges for conventional
supervised learning approaches. Representation Learning from biological signals
is an emerging realm catalyzed by the recent advances in computational modeling
and the abundance of publicly shared databases. The electrocardiogram (ECG) is
the primary researched modality in this context, with applications in health
monitoring, stress and affect estimation. Yet, most studies are limited by
small-scale controlled data collection and over-parameterized architecture
choices. We introduce \textbf{WildECG}, a pre-trained state-space model for
representation learning from ECG signals. We train this model in a
self-supervised manner with 275,000 10s ECG recordings collected in the wild
and evaluate it on a range of downstream tasks. The proposed model is a robust
backbone for ECG analysis, providing competitive performance on most of the
tasks considered, while demonstrating efficacy in low-resource regimes. The
code and pre-trained weights are shared publicly at
https://github.com/klean2050/tiles_ecg_model.
</p></li>
</ul>

<h3>Title: MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15312">http://arxiv.org/abs/2309.15312</a></li>
<li>Code URL: https://github.com/thrungroup/maptree</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15312]] MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees(http://arxiv.org/abs/2309.15312)</code></li>
<li>Summary: <p>Decision trees remain one of the most popular machine learning models today,
largely due to their out-of-the-box performance and interpretability. In this
work, we present a Bayesian approach to decision tree induction via maximum a
posteriori inference of a posterior distribution over trees. We first
demonstrate a connection between maximum a posteriori inference of decision
trees and AND/OR search. Using this connection, we propose an AND/OR search
algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori
tree. Lastly, we demonstrate the empirical performance of the maximum a
posteriori tree both on synthetic data and in real world settings. On 16 real
world datasets, MAPTree either outperforms baselines or demonstrates comparable
performance but with much smaller trees. On a synthetic dataset, MAPTree also
demonstrates greater robustness to noise and better generalization than
existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree
faster than existing sampling approaches and, in contrast with those
algorithms, is able to provide a certificate of optimality. The code for our
experiments is available at https://github.com/ThrunGroup/maptree.
</p></li>
</ul>

<h3>Title: Neural Stochastic Differential Equations for Robust and Explainable Analysis of Electromagnetic Unintended Radiated Emissions. (arXiv:2309.15386v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15386">http://arxiv.org/abs/2309.15386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15386]] Neural Stochastic Differential Equations for Robust and Explainable Analysis of Electromagnetic Unintended Radiated Emissions(http://arxiv.org/abs/2309.15386)</code></li>
<li>Summary: <p>We present a comprehensive evaluation of the robustness and explainability of
ResNet-like models in the context of Unintended Radiated Emission (URE)
classification and suggest a new approach leveraging Neural Stochastic
Differential Equations (SDEs) to address identified limitations. We provide an
empirical demonstration of the fragility of ResNet-like models to Gaussian
noise perturbations, where the model performance deteriorates sharply and its
F1-score drops to near insignificance at 0.008 with a Gaussian noise of only
0.5 standard deviation. We also highlight a concerning discrepancy where the
explanations provided by ResNet-like models do not reflect the inherent
periodicity in the input data, a crucial attribute in URE detection from stable
devices. In response to these findings, we propose a novel application of
Neural SDEs to build models for URE classification that are not only robust to
noise but also provide more meaningful and intuitive explanations. Neural SDE
models maintain a high F1-score of 0.93 even when exposed to Gaussian noise
with a standard deviation of 0.5, demonstrating superior resilience to ResNet
models. Neural SDE models successfully recover the time-invariant or periodic
horizontal bands from the input data, a feature that was conspicuously missing
in the explanations generated by ResNet-like models. This advancement presents
a small but significant step in the development of robust and interpretable
models for real-world URE applications where data is inherently noisy and
assurance arguments demand interpretable machine learning predictions.
</p></li>
</ul>

<h3>Title: Robust Internal Representations for Domain Generalization. (arXiv:2309.15522v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15522">http://arxiv.org/abs/2309.15522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15522]] Robust Internal Representations for Domain Generalization(http://arxiv.org/abs/2309.15522)</code></li>
<li>Summary: <p>This paper which is part of the New Faculty Highlights Invited Speaker
Program of AAAI'23, serves as a comprehensive survey of my research in transfer
learning by utilizing embedding spaces. The work reviewed in this paper
specifically revolves around the inherent challenges associated with continual
learning and limited availability of labeled data. By providing an overview of
my past and ongoing contributions, this paper aims to present a holistic
understanding of my research, paving the way for future explorations and
advancements in the field. My research delves into the various settings of
transfer learning, including, few-shot learning, zero-shot learning, continual
learning, domain adaptation, and distributed learning. I hope this survey
provides a forward-looking perspective for researchers who would like to focus
on similar research directions.
</p></li>
</ul>

<h3>Title: Startup success prediction and VC portfolio simulation using CrunchBase data. (arXiv:2309.15552v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15552">http://arxiv.org/abs/2309.15552</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15552]] Startup success prediction and VC portfolio simulation using CrunchBase data(http://arxiv.org/abs/2309.15552)</code></li>
<li>Summary: <p>Predicting startup success presents a formidable challenge due to the
inherently volatile landscape of the entrepreneurial ecosystem. The advent of
extensive databases like Crunchbase jointly with available open data enables
the application of machine learning and artificial intelligence for more
accurate predictive analytics. This paper focuses on startups at their Series B
and Series C investment stages, aiming to predict key success milestones such
as achieving an Initial Public Offering (IPO), attaining unicorn status, or
executing a successful Merger and Acquisition (M\&amp;A). We introduce novel deep
learning model for predicting startup success, integrating a variety of factors
such as funding metrics, founder features, industry category. A distinctive
feature of our research is the use of a comprehensive backtesting algorithm
designed to simulate the venture capital investment process. This simulation
allows for a robust evaluation of our model's performance against historical
data, providing actionable insights into its practical utility in real-world
investment contexts. Evaluating our model on Crunchbase's, we achieved a 14
times capital growth and successfully identified on B round high-potential
startups including Revolut, DigitalOcean, Klarna, Github and others. Our
empirical findings illuminate the importance of incorporating diverse feature
sets in enhancing the model's predictive accuracy. In summary, our work
demonstrates the considerable promise of deep learning models and alternative
unstructured data in predicting startup success and sets the stage for future
advancements in this research area.
</p></li>
</ul>

<h3>Title: Enhancing Sharpness-Aware Optimization Through Variance Suppression. (arXiv:2309.15639v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15639">http://arxiv.org/abs/2309.15639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15639]] Enhancing Sharpness-Aware Optimization Through Variance Suppression(http://arxiv.org/abs/2309.15639)</code></li>
<li>Summary: <p>Sharpness-aware minimization (SAM) has well documented merits in enhancing
generalization of deep neural networks, even without sizable data augmentation.
Embracing the geometry of the loss function, where neighborhoods of 'flat
minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing
the maximum loss caused by an adversary perturbing parameters within the
neighborhood. Although critical to account for sharpness of the loss function,
such an 'over-friendly adversary' can curtail the outmost level of
generalization. The novel approach of this contribution fosters stabilization
of adversaries through variance suppression (VaSSO) to avoid such friendliness.
VaSSO's provable stability safeguards its numerical improvement over SAM in
model-agnostic tasks, including image classification and machine translation.
In addition, experiments confirm that VaSSO endows SAM with robustness against
high levels of label noise.
</p></li>
</ul>

<h3>Title: On Computational Entanglement and Its Interpretation in Adversarial Machine Learning. (arXiv:2309.15669v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15669">http://arxiv.org/abs/2309.15669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15669]] On Computational Entanglement and Its Interpretation in Adversarial Machine Learning(http://arxiv.org/abs/2309.15669)</code></li>
<li>Summary: <p>Adversarial examples in machine learning has emerged as a focal point of
research due to their remarkable ability to deceive models with seemingly
inconspicuous input perturbations, potentially resulting in severe
consequences. In this study, we embark on a comprehensive exploration of
adversarial machine learning models, shedding light on their intrinsic
complexity and interpretability. Our investigation reveals intriguing links
between machine learning model complexity and Einstein's theory of special
relativity, through the concept of entanglement. More specific, we define
entanglement computationally and demonstrate that distant feature samples can
exhibit strong correlations, akin to entanglement in quantum realm. This
revelation challenges conventional perspectives in describing the phenomenon of
adversarial transferability observed in contemporary machine learning models.
By drawing parallels with the relativistic effects of time dilation and length
contraction during computation, we gain deeper insights into adversarial
machine learning, paving the way for more robust and interpretable models in
this rapidly evolving field.
</p></li>
</ul>

<h3>Title: Deep Model Fusion: A Survey. (arXiv:2309.15698v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15698">http://arxiv.org/abs/2309.15698</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15698]] Deep Model Fusion: A Survey(http://arxiv.org/abs/2309.15698)</code></li>
<li>Summary: <p>Deep model fusion/merging is an emerging technique that merges the parameters
or predictions of multiple deep learning models into a single one. It combines
the abilities of different models to make up for the biases and errors of a
single model to achieve better performance. However, deep model fusion on
large-scale deep learning models (e.g., LLMs and foundation models) faces
several challenges, including high computational cost, high-dimensional
parameter space, interference between different heterogeneous models, etc.
Although model fusion has attracted widespread attention due to its potential
to solve complex real-world tasks, there is still a lack of complete and
detailed survey research on this technique. Accordingly, in order to understand
the model fusion method better and promote its development, we present a
comprehensive survey to summarize the recent progress. Specifically, we
categorize existing deep model fusion methods as four-fold: (1) "Mode
connectivity", which connects the solutions in weight space via a path of
non-increasing loss, in order to obtain better initialization for model fusion;
(2) "Alignment" matches units between neural networks to create better
conditions for fusion; (3) "Weight average", a classical model fusion method,
averages the weights of multiple models to obtain more accurate results closer
to the optimal solution; (4) "Ensemble learning" combines the outputs of
diverse models, which is a foundational technique for improving the accuracy
and robustness of the final model. In addition, we analyze the challenges faced
by deep model fusion and propose possible research directions for model fusion
in the future. Our review is helpful in deeply understanding the correlation
between different model fusion methods and practical application methods, which
can enlighten the research in the field of deep model fusion.
</p></li>
</ul>

<h3>Title: Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15757">http://arxiv.org/abs/2309.15757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15757]] Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data(http://arxiv.org/abs/2309.15757)</code></li>
<li>Summary: <p>In the domain of semi-supervised learning, the current approaches
insufficiently exploit the potential of considering inter-instance
relationships among (un)labeled data. In this work, we address this limitation
by providing an approach for inferring latent graphs that capture the intrinsic
data relationships. By leveraging graph-based representations, our approach
facilitates the seamless propagation of information throughout the graph,
enabling the effective incorporation of global and local knowledge. Through
evaluations on biomedical tabular datasets, we compare the capabilities of our
approach to other contemporary methods. Our work demonstrates the significance
of inter-instance relationship discovery as practical means for constructing
robust latent graphs to enhance semi-supervised learning techniques. Our method
achieves state-of-the-art results on three biomedical datasets.
</p></li>
</ul>

<h3>Title: Importance-Weighted Offline Learning Done Right. (arXiv:2309.15771v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15771">http://arxiv.org/abs/2309.15771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15771]] Importance-Weighted Offline Learning Done Right(http://arxiv.org/abs/2309.15771)</code></li>
<li>Summary: <p>We study the problem of offline policy optimization in stochastic contextual
bandit problems, where the goal is to learn a near-optimal policy based on a
dataset of decision data collected by a suboptimal behavior policy. Rather than
making any structural assumptions on the reward function, we assume access to a
given policy class and aim to compete with the best comparator policy within
this class. In this setting, a standard approach is to compute
importance-weighted estimators of the value of each policy, and select a policy
that minimizes the estimated value up to a "pessimistic" adjustment subtracted
from the estimates to reduce their random fluctuations. In this paper, we show
that a simple alternative approach based on the "implicit exploration"
estimator of \citet{Neu2015} yields performance guarantees that are superior in
nearly all possible terms to all previous results. Most notably, we remove an
extremely restrictive "uniform coverage" assumption made in all previous works.
These improvements are made possible by the observation that the upper and
lower tails importance-weighted estimators behave very differently from each
other, and their careful control can massively improve on previous results that
were all based on symmetric two-sided concentration inequalities. We also
extend our results to infinite policy classes in a PAC-Bayesian fashion, and
showcase the robustness of our algorithm to the choice of hyper-parameters by
means of numerical simulations.
</p></li>
</ul>

<h3>Title: Learning the Efficient Frontier. (arXiv:2309.15775v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15775">http://arxiv.org/abs/2309.15775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15775]] Learning the Efficient Frontier(http://arxiv.org/abs/2309.15775)</code></li>
<li>Summary: <p>The efficient frontier (EF) is a fundamental resource allocation problem
where one has to find an optimal portfolio maximizing a reward at a given level
of risk. This optimal solution is traditionally found by solving a convex
optimization problem. In this paper, we introduce NeuralEF: a fast neural
approximation framework that robustly forecasts the result of the EF convex
optimization problem with respect to heterogeneous linear constraints and
variable number of optimization inputs. By reformulating an optimization
problem as a sequence to sequence problem, we show that NeuralEF is a viable
solution to accelerate large-scale simulation while handling discontinuous
behavior.
</p></li>
</ul>

<h3>Title: ANNCRIPS: Artificial Neural Networks for Cancer Research In Prediction & Survival. (arXiv:2309.15803v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15803">http://arxiv.org/abs/2309.15803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15803]] ANNCRIPS: Artificial Neural Networks for Cancer Research In Prediction & Survival(http://arxiv.org/abs/2309.15803)</code></li>
<li>Summary: <p>Prostate cancer is a prevalent malignancy among men aged 50 and older.
Current diagnostic methods primarily rely on blood tests, PSA:Prostate-Specific
Antigen levels, and Digital Rectal Examinations (DRE). However, these methods
suffer from a significant rate of false positive results. This study focuses on
the development and validation of an intelligent mathematical model utilizing
Artificial Neural Networks (ANNs) to enhance the early detection of prostate
cancer. The primary objective of this research paper is to present a novel
mathematical model designed to aid in the early detection of prostate cancer,
facilitating prompt intervention by healthcare professionals. The model's
implementation demonstrates promising potential in reducing the incidence of
false positives, thereby improving patient outcomes. Furthermore, we envision
that, with further refinement, extensive testing, and validation, this model
can evolve into a robust, marketable solution for prostate cancer detection.
The long-term goal is to make this solution readily available for deployment in
various screening centers, hospitals, and research institutions, ultimately
contributing to more effective cancer screening and patient care.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Semantics-Driven Cloud-Edge Collaborative Inference. (arXiv:2309.15435v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15435">http://arxiv.org/abs/2309.15435</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15435]] Semantics-Driven Cloud-Edge Collaborative Inference(http://arxiv.org/abs/2309.15435)</code></li>
<li>Summary: <p>With the proliferation of video data in smart city applications like
intelligent transportation, efficient video analytics has become crucial but
also challenging. This paper proposes a semantics-driven cloud-edge
collaborative approach for accelerating video inference, using license plate
recognition as a case study. The method separates semantics extraction and
recognition, allowing edge servers to only extract visual semantics (license
plate patches) from video frames and offload computation-intensive recognition
to the cloud or neighboring edges based on load. This segmented processing
coupled with a load-aware work distribution strategy aims to reduce end-to-end
latency and improve throughput. Experiments demonstrate significant
improvements in end-to-end inference speed (up to 5x faster), throughput (up to
9 FPS), and reduced traffic volumes (50% less) compared to cloud-only or
edge-only processing, validating the efficiency of the proposed approach. The
cloud-edge collaborative framework with semantics-driven work partitioning
provides a promising solution for scaling video analytics in smart cities.
</p></li>
</ul>

<h3>Title: Teaching Text-to-Image Models to Communicate. (arXiv:2309.15516v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15516">http://arxiv.org/abs/2309.15516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15516]] Teaching Text-to-Image Models to Communicate(http://arxiv.org/abs/2309.15516)</code></li>
<li>Summary: <p>Various works have been extensively studied in the research of text-to-image
generation. Although existing models perform well in text-to-image generation,
there are significant challenges when directly employing them to generate
images in dialogs. In this paper, we first highlight a new problem:
dialog-to-image generation, that is, given the dialog context, the model should
generate a realistic image which is consistent with the specified conversation
as response. To tackle the problem, we propose an efficient approach for
dialog-to-image generation without any intermediate translation, which
maximizes the extraction of the semantic information contained in the dialog.
Considering the characteristics of dialog structure, we put segment token
before each sentence in a turn of a dialog to differentiate different speakers.
Then, we fine-tune pre-trained text-to-image models to enable them to generate
images conditioning on processed dialog context. After fine-tuning, our
approach can consistently improve the performance of various models across
multiple metrics. Experimental results on public benchmark demonstrate the
effectiveness and practicability of our method.
</p></li>
</ul>

<h3>Title: From LAION-5B to LAION-EO: Filtering Billions of Images Using Anchor Datasets for Satellite Image Extraction. (arXiv:2309.15535v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15535">http://arxiv.org/abs/2309.15535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15535]] From LAION-5B to LAION-EO: Filtering Billions of Images Using Anchor Datasets for Satellite Image Extraction(http://arxiv.org/abs/2309.15535)</code></li>
<li>Summary: <p>Large datasets, such as LAION-5B, contain a diverse distribution of images
shared online. However, extraction of domain-specific subsets of large image
corpora is challenging. The extraction approach based on an anchor dataset,
combined with further filtering, is proposed here and demonstrated for the
domain of satellite imagery. This results in the release of LAION-EO, a dataset
sourced from the web containing pairs of text and satellite images in high
(pixel-wise) resolution. The paper outlines the acquisition procedure as well
as some of the features of the dataset.
</p></li>
</ul>

<h3>Title: Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization. (arXiv:2309.15556v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15556">http://arxiv.org/abs/2309.15556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15556]] Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization(http://arxiv.org/abs/2309.15556)</code></li>
<li>Summary: <p>This paper addresses the problem of estimating the 3-DoF camera pose for a
ground-level image with respect to a satellite image that encompasses the local
surroundings. We propose a novel end-to-end approach that leverages the
learning of dense pixel-wise flow fields in pairs of ground and satellite
images to calculate the camera pose. Our approach differs from existing methods
by constructing the feature metric at the pixel level, enabling full-image
supervision for learning distinctive geometric configurations and visual
appearances across views. Specifically, our method employs two distinct
convolution networks for ground and satellite feature extraction. Then, we
project the ground feature map to the bird's eye view (BEV) using a fixed
camera height assumption to achieve preliminary geometric alignment. To further
establish content association between the BEV and satellite features, we
introduce a residual convolution block to refine the projected BEV feature.
Optical flow estimation is performed on the refined BEV feature map and the
satellite feature map using flow decoder networks based on RAFT. After
obtaining dense flow correspondences, we apply the least square method to
filter matching inliers and regress the ground camera pose. Extensive
experiments demonstrate significant improvements compared to state-of-the-art
methods. Notably, our approach reduces the median localization error by 89%,
19%, 80% and 35% on the KITTI, Ford multi-AV, VIGOR and Oxford RobotCar
datasets, respectively.
</p></li>
</ul>

<h3>Title: Dynamic Multi-Scale Context Aggregation for Conversational Aspect-Based Sentiment Quadruple Analysis. (arXiv:2309.15476v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15476">http://arxiv.org/abs/2309.15476</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15476]] Dynamic Multi-Scale Context Aggregation for Conversational Aspect-Based Sentiment Quadruple Analysis(http://arxiv.org/abs/2309.15476)</code></li>
<li>Summary: <p>Conversational aspect-based sentiment quadruple analysis (DiaASQ) aims to
extract the quadruple of target-aspect-opinion-sentiment within a dialogue. In
DiaASQ, a quadruple's elements often cross multiple utterances. This situation
complicates the extraction process, emphasizing the need for an adequate
understanding of conversational context and interactions. However, existing
work independently encodes each utterance, thereby struggling to capture
long-range conversational context and overlooking the deep inter-utterance
dependencies. In this work, we propose a novel Dynamic Multi-scale Context
Aggregation network (DMCA) to address the challenges. Specifically, we first
utilize dialogue structure to generate multi-scale utterance windows for
capturing rich contextual information. After that, we design a Dynamic
Hierarchical Aggregation module (DHA) to integrate progressive cues between
them. In addition, we form a multi-stage loss strategy to improve model
performance and generalization ability. Extensive experimental results show
that the DMCA model outperforms baselines significantly and achieves
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection. (arXiv:2309.15670v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15670">http://arxiv.org/abs/2309.15670</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15670]] MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection(http://arxiv.org/abs/2309.15670)</code></li>
<li>Summary: <p>In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have
been increasingly popular in the Bangla language, which is the seventh most
spoken language throughout the entire world. However, the language is
structurally complicated, which makes this field arduous to extract emotions in
an accurate manner. Several distinct approaches such as the extraction of
positive and negative sentiments as well as multiclass emotions, have been
implemented in this field of study. Nevertheless, the extraction of multiple
sentiments is an almost untouched area in this language. Which involves
identifying several feelings based on a single piece of text. Therefore, this
study demonstrates a thorough method for constructing an annotated corpus based
on scrapped data from Facebook to bridge the gaps in this subject area to
overcome the challenges. To make this annotation more fruitful, the
context-based approach has been used. Bidirectional Encoder Representations
from Transformers (BERT), a well-known methodology of transformers, have been
shown the best results of all methods implemented. Finally, a web application
has been developed to demonstrate the performance of the pre-trained
top-performer model (BERT) for multi-label ER in Bangla.
</p></li>
</ul>

<h3>Title: OceanBench: The Sea Surface Height Edition. (arXiv:2309.15599v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15599">http://arxiv.org/abs/2309.15599</a></li>
<li>Code URL: https://github.com/jejjohnson/oceanbench</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15599]] OceanBench: The Sea Surface Height Edition(http://arxiv.org/abs/2309.15599)</code></li>
<li>Summary: <p>The ocean profoundly influences human activities and plays a critical role in
climate regulation. Our understanding has improved over the last decades with
the advent of satellite remote sensing data, allowing us to capture essential
quantities over the globe, e.g., sea surface height (SSH). However, ocean
satellite data presents challenges for information extraction due to their
sparsity and irregular sampling, signal complexity, and noise. Machine learning
(ML) techniques have demonstrated their capabilities in dealing with
large-scale, complex signals. Therefore we see an opportunity for ML models to
harness the information contained in ocean satellite data. However, data
representation and relevant evaluation metrics can be the defining factors when
determining the success of applied ML. The processing steps from the raw
observation data to a ML-ready state and from model outputs to interpretable
quantities require domain expertise, which can be a significant barrier to
entry for ML researchers. OceanBench is a unifying framework that provides
standardized processing steps that comply with domain-expert standards. It
provides plug-and-play data and pre-configured pipelines for ML researchers to
benchmark their models and a transparent configurable framework for researchers
to customize and extend the pipeline for their tasks. In this work, we
demonstrate the OceanBench framework through a first edition dedicated to SSH
interpolation challenges. We provide datasets and ML-ready benchmarking
pipelines for the long-standing problem of interpolating observations from
simulated ocean satellite data, multi-modal and multi-sensor fusion issues, and
transfer-learning to real ocean satellite observations. The OceanBench
framework is available at github.com/jejjohnson/oceanbench and the dataset
registry is available at github.com/quentinf00/oceanbench-data-registry.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations. (arXiv:2309.15499v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15499">http://arxiv.org/abs/2309.15499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15499]] Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations(http://arxiv.org/abs/2309.15499)</code></li>
<li>Summary: <p>Bayesian personalized federated learning (BPFL) addresses challenges in
existing personalized FL (PFL). BPFL aims to quantify the uncertainty and
heterogeneity within and across clients towards uncertainty representations by
addressing the statistical heterogeneity of client data. In PFL, some recent
preliminary work proposes to decompose hidden neural representations into
shared and local components and demonstrates interesting results. However, most
of them do not address client uncertainty and heterogeneity in FL systems,
while appropriately decoupling neural representations is challenging and often
ad hoc. In this paper, we make the first attempt to introduce a general BPFL
framework to decompose and jointly learn shared and personalized uncertainty
representations on statistically heterogeneous client data over time. A
Bayesian federated neural network BPFed instantiates BPFL by jointly learning
cross-client shared uncertainty and client-specific personalized uncertainty
over statistically heterogeneous and randomly participating clients. We further
involve continual updating of prior distribution in BPFed to speed up the
convergence and avoid catastrophic forgetting. Theoretical analysis and
guarantees are provided in addition to the experimental evaluation of BPFed
against the diversified baselines.
</p></li>
</ul>

<h3>Title: Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency. (arXiv:2309.15659v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15659">http://arxiv.org/abs/2309.15659</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15659]] Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency(http://arxiv.org/abs/2309.15659)</code></li>
<li>Summary: <p>Federated Learning (FL) is a prominent distributed learning paradigm
facilitating collaboration among nodes within an edge network to co-train a
global model without centralizing data. By shifting computation to the network
edge, FL offers robust and responsive edge-AI solutions and enhance
privacy-preservation. However, deploying deep FL models within edge
environments is often hindered by communication bottlenecks, data
heterogeneity, and memory limitations. To address these challenges jointly, we
introduce FeDEQ, a pioneering FL framework that effectively employs deep
equilibrium learning and consensus optimization to exploit a compact shared
data representation across edge nodes, allowing the derivation of personalized
models specific to each node. We delve into a unique model structure composed
of an equilibrium layer followed by traditional neural network layers. Here,
the equilibrium layer functions as a global feature representation that edge
nodes can adapt to personalize their local layers. Capitalizing on FeDEQ's
compactness and representation power, we present a novel distributed algorithm
rooted in the alternating direction method of multipliers (ADMM) consensus
optimization and theoretically establish its convergence for smooth objectives.
Experiments across various benchmarks demonstrate that FeDEQ achieves
performance comparable to state-of-the-art personalized methods while employing
models of up to 4 times smaller in communication size and 1.5 times lower
memory footprint during training.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15216">http://arxiv.org/abs/2309.15216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15216]] Auto-grading C programming assignments with CodeBERT and Random Forest Regressor(http://arxiv.org/abs/2309.15216)</code></li>
<li>Summary: <p>Grading coding assignments manually is challenging due to complexity and
subjectivity. However, auto-grading with deep learning simplifies the task. It
objectively assesses code quality, detects errors, and assigns marks
accurately, reducing the burden on instructors while ensuring efficient and
fair assessment. This study provides an analysis of auto-grading of the C
programming assignments using machine learning and deep learning approaches
like regression, convolutional neural networks (CNN) and long short-term memory
(LSTM). Using a code-based transformer word embedding model called CodeBERT,
the textual code inputs were transformed into vectors, and the vectors were
then fed into several models. The testing findings demonstrated the efficacy of
the suggested strategy with a root mean squared error (RMSE) of 1.89. The
contrast between statistical methods and deep learning techniques is discussed
in the study.
</p></li>
</ul>

<h3>Title: Fair Canonical Correlation Analysis. (arXiv:2309.15809v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15809">http://arxiv.org/abs/2309.15809</a></li>
<li>Code URL: https://github.com/pennshenlab/fair_cca</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15809]] Fair Canonical Correlation Analysis(http://arxiv.org/abs/2309.15809)</code></li>
<li>Summary: <p>This paper investigates fairness and bias in Canonical Correlation Analysis
(CCA), a widely used statistical technique for examining the relationship
between two sets of variables. We present a framework that alleviates
unfairness by minimizing the correlation disparity error associated with
protected attributes. Our approach enables CCA to learn global projection
matrices from all data points while ensuring that these matrices yield
comparable correlation levels to group-specific projection matrices.
Experimental evaluation on both synthetic and real-world datasets demonstrates
the efficacy of our method in reducing correlation disparity error without
compromising CCA accuracy.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: DPA-WNO: A gray box model for a class of stochastic mechanics problem. (arXiv:2309.15128v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15128">http://arxiv.org/abs/2309.15128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15128]] DPA-WNO: A gray box model for a class of stochastic mechanics problem(http://arxiv.org/abs/2309.15128)</code></li>
<li>Summary: <p>The well-known governing physics in science and engineering is often based on
certain assumptions and approximations. Therefore, analyses and designs carried
out based on these equations are also approximate. The emergence of data-driven
models has, to a certain degree, addressed this challenge; however, the purely
data-driven models often (a) lack interpretability, (b) are data-hungry, and
(c) do not generalize beyond the training window. Operator learning has
recently been proposed as a potential alternative to address the aforementioned
challenges; however, the challenges are still persistent. We here argue that
one of the possible solutions resides in data-physics fusion, where the
data-driven model is used to correct/identify the missing physics. To that end,
we propose a novel Differentiable Physics Augmented Wavelet Neural Operator
(DPA-WNO). The proposed DPA-WNO blends a differentiable physics solver with the
Wavelet Neural Operator (WNO), where the role of WNO is to model the missing
physics. This empowers the proposed framework to exploit the capability of WNO
to learn from data while retaining the interpretability and generalizability
associated with physics-based solvers. We illustrate the applicability of the
proposed approach in solving time-dependent uncertainty quantification problems
due to randomness in the initial condition. Four benchmark uncertainty
quantification and reliability analysis examples from various fields of science
and engineering are solved using the proposed approach. The results presented
</p></li>
</ul>

<h3>Title: From Asset Flow to Status, Action and Intention Discovery: Early Malice Detection in Cryptocurrency. (arXiv:2309.15133v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15133">http://arxiv.org/abs/2309.15133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15133]] From Asset Flow to Status, Action and Intention Discovery: Early Malice Detection in Cryptocurrency(http://arxiv.org/abs/2309.15133)</code></li>
<li>Summary: <p>Cryptocurrency has been subject to illicit activities probably more often
than traditional financial assets due to the pseudo-anonymous nature of its
transacting entities. An ideal detection model is expected to achieve all three
critical properties of (I) early detection, (II) good interpretability, and
(III) versatility for various illicit activities. However, existing solutions
cannot meet all these requirements, as most of them heavily rely on deep
learning without interpretability and are only available for retrospective
analysis of a specific illicit type. To tackle all these challenges, we propose
Intention-Monitor for early malice detection in Bitcoin (BTC), where the
on-chain record data for a certain address are much scarcer than other
cryptocurrency platforms. We first define asset transfer paths with the
Decision-Tree based feature Selection and Complement (DT-SC) to build different
feature sets for different malice types. Then, the Status/Action Proposal
Module (S/A-PM) and the Intention-VAE module generate the status, action,
intent-snippet, and hidden intent-snippet embedding. With all these modules,
our model is highly interpretable and can detect various illegal activities.
Moreover, well-designed loss functions further enhance the prediction speed and
model's interpretability. Extensive experiments on three real-world datasets
demonstrate that our proposed algorithm outperforms the state-of-the-art
methods. Furthermore, additional case studies justify our model can not only
explain existing illicit patterns but can also find new suspicious characters.
</p></li>
</ul>

<h3>Title: A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction. (arXiv:2309.15284v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15284">http://arxiv.org/abs/2309.15284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15284]] A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction(http://arxiv.org/abs/2309.15284)</code></li>
<li>Summary: <p>In vehicle trajectory prediction, physics models and data-driven models are
two predominant methodologies. However, each approach presents its own set of
challenges: physics models fall short in predictability, while data-driven
models lack interpretability. Addressing these identified shortcomings, this
paper proposes a novel framework, the Physics-Enhanced Residual Learning (PERL)
model. PERL integrates the strengths of physics-based and data-driven methods
for traffic state prediction. PERL contains a physics model and a residual
learning model. Its prediction is the sum of the physics model result and a
predicted residual as a correction to it. It preserves the interpretability
inherent to physics-based models and has reduced data requirements compared to
data-driven methods. Experiments were conducted using a real-world vehicle
trajectory dataset. We proposed a PERL model, with the Intelligent Driver Model
(IDM) as its physics car-following model and Long Short-Term Memory (LSTM) as
its residual learning model. We compare this PERL model with the physics
car-following model, data-driven model, and other physics-informed neural
network (PINN) models. The result reveals that PERL achieves better prediction
with a small dataset, compared to the physics model, data-driven model, and
PINN model. Second, the PERL model showed faster convergence during training,
offering comparable performance with fewer training samples than the
data-driven model and PINN model. Sensitivity analysis also proves comparable
performance of PERL using another residual learning model and a physics
car-following model.
</p></li>
</ul>

<h3>Title: Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution. (arXiv:2309.15559v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15559">http://arxiv.org/abs/2309.15559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15559]] Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution(http://arxiv.org/abs/2309.15559)</code></li>
<li>Summary: <p>Self-interpreting neural networks have garnered significant interest in
research. Existing works in this domain often (1) lack a solid theoretical
foundation ensuring genuine interpretability or (2) compromise model
expressiveness. In response, we formulate a generic Additive Self-Attribution
(ASA) framework. Observing the absence of Shapley value in Additive
Self-Attribution, we propose Shapley Additive Self-Attributing Neural Network
(SASANet), with theoretical guarantees for the self-attribution value equal to
the output's Shapley values. Specifically, SASANet uses a marginal
contribution-based sequential schema and internal distillation-based training
strategies to model meaningful outputs for any number of features, resulting in
un-approximated meaningful value function. Our experimental results indicate
SASANet surpasses existing self-attributing models in performance and rivals
black-box models. Moreover, SASANet is shown more precise and efficient than
post-hoc methods in interpreting its own predictions.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: DreamCom: Finetuning Text-guided Inpainting Model for Image Composition. (arXiv:2309.15508v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15508">http://arxiv.org/abs/2309.15508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15508]] DreamCom: Finetuning Text-guided Inpainting Model for Image Composition(http://arxiv.org/abs/2309.15508)</code></li>
<li>Summary: <p>The goal of image composition is merging a foreground object into a
background image to obtain a realistic composite image. Recently, generative
composition methods are built on large pretrained diffusion models, due to
their unprecedented image generation ability. They train a model on abundant
pairs of foregrounds and backgrounds, so that it can be directly applied to a
new pair of foreground and background at test time. However, the generated
results often lose the foreground details and exhibit noticeable artifacts. In
this work, we propose an embarrassingly simple approach named DreamCom inspired
by DreamBooth. Specifically, given a few reference images for a subject, we
finetune text-guided inpainting diffusion model to associate this subject with
a special token and inpaint this subject in the specified bounding box. We also
construct a new dataset named MureCom well-tailored for this task.
</p></li>
</ul>

<h3>Title: Uncertainty Quantification via Neural Posterior Principal Components. (arXiv:2309.15533v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15533">http://arxiv.org/abs/2309.15533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15533]] Uncertainty Quantification via Neural Posterior Principal Components(http://arxiv.org/abs/2309.15533)</code></li>
<li>Summary: <p>Uncertainty quantification is crucial for the deployment of image restoration
models in safety-critical domains, like autonomous driving and biological
imaging. To date, methods for uncertainty visualization have mainly focused on
per-pixel estimates. However, a heatmap of per-pixel variances is typically of
little practical use, as it does not capture the strong correlations between
pixels. A more natural measure of uncertainty corresponds to the variances
along the principal components (PCs) of the posterior distribution.
Theoretically, the PCs can be computed by applying PCA on samples generated
from a conditional generative model for the input image. However, this requires
generating a very large number of samples at test time, which is painfully slow
with the current state-of-the-art (diffusion) models. In this work, we present
a method for predicting the PCs of the posterior distribution for any input
image, in a single forward pass of a neural network. Our method can either wrap
around a pre-trained model that was trained to minimize the mean square error
(MSE), or can be trained from scratch to output both a predicted image and the
posterior PCs. We showcase our method on multiple inverse problems in imaging,
including denoising, inpainting, super-resolution, and biological
image-to-image translation. Our method reliably conveys instance-adaptive
uncertainty directions, achieving uncertainty quantification comparable with
posterior samplers while being orders of magnitude faster. Examples are
available at https://eliasnehme.github.io/NPPC/
</p></li>
</ul>

<h3>Title: Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing. (arXiv:2309.15664v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15664">http://arxiv.org/abs/2309.15664</a></li>
<li>Code URL: https://github.com/wangkai930418/DPL</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15664]] Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing(http://arxiv.org/abs/2309.15664)</code></li>
<li>Summary: <p>Large-scale text-to-image generative models have been a ground-breaking
development in generative AI, with diffusion models showing their astounding
ability to synthesize convincing images following an input text prompt. The
goal of image editing research is to give users control over the generated
images by modifying the text prompt. Current image editing techniques are
susceptible to unintended modifications of regions outside the targeted area,
such as on the background or on distractor objects which have some semantic or
visual relationship with the targeted object. According to our experimental
findings, inaccurate cross-attention maps are at the root of this problem.
Based on this observation, we propose Dynamic Prompt Learning (DPL) to force
cross-attention maps to focus on correct noun words in the text prompt. By
updating the dynamic tokens for nouns in the textual input with the proposed
leakage repairment losses, we achieve fine-grained image editing over
particular objects while preventing undesired changes to other image regions.
Our method DPL, based on the publicly available Stable Diffusion, is
extensively evaluated on a wide range of images, and consistently obtains
superior results both quantitatively (CLIP score, Structure-Dist) and
qualitatively (on user-evaluation). We show improved prompt editing results for
Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for
complex multi-object scenes.
</p></li>
</ul>

<h3>Title: Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation. (arXiv:2309.15726v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15726">http://arxiv.org/abs/2309.15726</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15726]] Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation(http://arxiv.org/abs/2309.15726)</code></li>
<li>Summary: <p>We develop a neural network architecture which, trained in an unsupervised
manner as a denoising diffusion model, simultaneously learns to both generate
and segment images. Learning is driven entirely by the denoising diffusion
objective, without any annotation or prior knowledge about regions during
training. A computational bottleneck, built into the neural architecture,
encourages the denoising network to partition an input into regions, denoise
them in parallel, and combine the results. Our trained model generates both
synthetic images and, by simple examination of its internal predicted
partitions, a semantic segmentation of those images. Without any finetuning, we
directly apply our unsupervised model to the downstream task of segmenting real
images via noising and subsequently denoising them. Experiments demonstrate
that our model achieves accurate unsupervised image segmentation and
high-quality synthetic image generation across multiple datasets.
</p></li>
</ul>

<h3>Title: Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack. (arXiv:2309.15807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15807">http://arxiv.org/abs/2309.15807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15807]] Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack(http://arxiv.org/abs/2309.15807)</code></li>
<li>Summary: <p>Training text-to-image models with web scale image-text pairs enables the
generation of a wide range of visual concepts from text. However, these
pre-trained models often face challenges when it comes to generating highly
aesthetic images. This creates the need for aesthetic alignment post
pre-training. In this paper, we propose quality-tuning to effectively guide a
pre-trained model to exclusively generate highly visually appealing images,
while maintaining generality across visual concepts. Our key insight is that
supervised fine-tuning with a set of surprisingly small but extremely visually
appealing images can significantly improve the generation quality. We pre-train
a latent diffusion model on $1.1$ billion image-text pairs and fine-tune it
with only a few thousand carefully selected high-quality images. The resulting
model, Emu, achieves a win rate of $82.9\%$ compared with its pre-trained only
counterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred
$68.4\%$ and $71.3\%$ of the time on visual appeal on the standard PartiPrompts
and our Open User Input benchmark based on the real-world usage of
text-to-image models. In addition, we show that quality-tuning is a generic
approach that is also effective for other architectures, including pixel
diffusion and masked generative transformer models.
</p></li>
</ul>

<h3>Title: Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation. (arXiv:2309.15818v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15818">http://arxiv.org/abs/2309.15818</a></li>
<li>Code URL: https://github.com/showlab/show-1</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15818]] Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation(http://arxiv.org/abs/2309.15818)</code></li>
<li>Summary: <p>Significant advancements have been achieved in the realm of large-scale
pre-trained text-to-video Diffusion Models (VDMs). However, previous methods
either rely solely on pixel-based VDMs, which come with high computational
costs, or on latent-based VDMs, which often struggle with precise text-video
alignment. In this paper, we are the first to propose a hybrid model, dubbed as
Show-1, which marries pixel-based and latent-based VDMs for text-to-video
generation. Our model first uses pixel-based VDMs to produce a low-resolution
video of strong text-video correlation. After that, we propose a novel expert
translation method that employs the latent-based VDMs to further upsample the
low-resolution video to high resolution. Compared to latent VDMs, Show-1 can
produce high-quality videos of precise text-video alignment; Compared to pixel
VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G
vs 72G). We also validate our model on standard video generation benchmarks.
Our code and model weights are publicly available at
\url{https://github.com/showlab/Show-1}.
</p></li>
</ul>

<h3>Title: Exploiting the Signal-Leak Bias in Diffusion Models. (arXiv:2309.15842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15842">http://arxiv.org/abs/2309.15842</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15842]] Exploiting the Signal-Leak Bias in Diffusion Models(http://arxiv.org/abs/2309.15842)</code></li>
<li>Summary: <p>There is a bias in the inference pipeline of most diffusion models. This bias
arises from a signal leak whose distribution deviates from the noise
distribution, creating a discrepancy between training and inference processes.
We demonstrate that this signal-leak bias is particularly significant when
models are tuned to a specific style, causing sub-optimal style matching.
Recent research tries to avoid the signal leakage during training. We instead
show how we can exploit this signal-leak bias in existing diffusion models to
allow more control over the generated images. This enables us to generate
images with more varied brightness, and images that better match a desired
style or color. By modeling the distribution of the signal leak in the spatial
frequency and pixel domains, and including a signal leak in the initial latent,
we generate images that better match expected results without any additional
training.
</p></li>
</ul>

<h3>Title: Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15238">http://arxiv.org/abs/2309.15238</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15238]] Learning Using Generated Privileged Information by Text-to-Image Diffusion Models(http://arxiv.org/abs/2309.15238)</code></li>
<li>Summary: <p>Learning Using Privileged Information is a particular type of knowledge
distillation where the teacher model benefits from an additional data
representation during training, called privileged information, improving the
student model, which does not see the extra representation. However, privileged
information is rarely available in practice. To this end, we propose a text
classification framework that harnesses text-to-image diffusion models to
generate artificial privileged information. The generated images and the
original text samples are further used to train multimodal teacher models based
on state-of-the-art transformer-based architectures. Finally, the knowledge
from multimodal teachers is distilled into a text-based (unimodal) student.
Hence, by employing a generative model to produce synthetic data as privileged
information, we guide the training of the student model. Our framework, called
Learning Using Generated Privileged Information (LUGPI), yields noticeable
performance gains on four text classification data sets, demonstrating its
potential in text classification without any additional cost during inference.
</p></li>
</ul>

<h3>Title: PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning. (arXiv:2309.15139v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15139">http://arxiv.org/abs/2309.15139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15139]] PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning(http://arxiv.org/abs/2309.15139)</code></li>
<li>Summary: <p>The normalization constraint on probability density poses a significant
challenge for solving the Fokker-Planck equation. Normalizing Flow, an
invertible generative model leverages the change of variables formula to ensure
probability density conservation and enable the learning of complex data
distributions. In this paper, we introduce Physics-Informed Normalizing Flows
(PINF), a novel extension of continuous normalizing flows, incorporating
diffusion through the method of characteristics. Our method, which is mesh-free
and causality-free, can efficiently solve high dimensional time-dependent and
steady-state Fokker-Planck equations.
</p></li>
</ul>

<h3>Title: Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15214">http://arxiv.org/abs/2309.15214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15214]] Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling(http://arxiv.org/abs/2309.15214)</code></li>
<li>Summary: <p>The state of the art for physical hazard prediction from weather and climate
requires expensive km-scale numerical simulations driven by coarser resolution
global inputs. Here, a km-scale downscaling diffusion model is presented as a
cost effective alternative. The model is trained from a regional
high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis
data. To address the downscaling uncertainties, large resolution ratios (25km
to 2km), different physics involved at different scales and predict channels
that are not in the input data, we employ a two-step approach
(\textit{ResDiff}) where a (UNet) regression predicts the mean in the first
step and a diffusion model predicts the residual in the second step.
\textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The
predicted spectra and distributions from ResDiff faithfully recover important
power law relationships regulating damaging wind and rain extremes. Case
studies of coherent weather phenomena reveal appropriate multivariate
relationships reminiscent of learnt physics. This includes the sharp wind and
temperature variations that co-locate with intense rainfall in a cold front,
and the extreme winds and rainfall bands that surround the eyewall of typhoons.
Some evidence of simultaneous bias correction is found. A first attempt at
downscaling directly from an operational global forecast model successfully
retains many of these benefits. The implication is that a new era of fully
end-to-end, global-to-regional machine learning weather prediction is likely
near at hand.
</p></li>
</ul>

<h3>Title: Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15293">http://arxiv.org/abs/2309.15293</a></li>
<li>Code URL: https://github.com/murpheylab/maxdiffrl</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15293]] Maximum Diffusion Reinforcement Learning(http://arxiv.org/abs/2309.15293)</code></li>
<li>Summary: <p>The assumption that data are independent and identically distributed
underpins all machine learning. When data are collected sequentially from agent
experiences this assumption does not generally hold, as in reinforcement
learning. Here, we derive a method that overcomes these limitations by
exploiting the statistical mechanics of ergodic processes, which we term
maximum diffusion reinforcement learning. By decorrelating agent experiences,
our approach provably enables agents to learn continually in single-shot
deployments regardless of how they are initialized. Moreover, we prove our
approach generalizes well-known maximum entropy techniques, and show that it
robustly exceeds state-of-the-art performance across popular benchmarks. Our
results at the nexus of physics, learning, and control pave the way towards
more transparent and reliable decision-making in reinforcement learning agents,
such as locomoting robots and self-driving cars.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Efficient Low-rank Backpropagation for Vision Transformer Adaptation. (arXiv:2309.15275v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15275">http://arxiv.org/abs/2309.15275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15275]] Efficient Low-rank Backpropagation for Vision Transformer Adaptation(http://arxiv.org/abs/2309.15275)</code></li>
<li>Summary: <p>The increasing scale of vision transformers (ViT) has made the efficient
fine-tuning of these large models for specific needs a significant challenge in
various applications. This issue originates from the computationally demanding
matrix multiplications required during the backpropagation process through
linear layers in ViT. In this paper, we tackle this problem by proposing a new
Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method.
Intuitively, LBP-WHT projects the gradient into a low-rank space and carries
out backpropagation. This approach substantially reduces the computation needed
for adapting ViT, as matrix multiplication in the low-rank space is far less
resource-intensive. We conduct extensive experiments with different models
(ViT, hybrid convolution-ViT model) on multiple datasets to demonstrate the
effectiveness of our method. For instance, when adapting an EfficientFormer-L1
model on CIFAR100, our LBP-WHT achieves 10.4% higher accuracy than the
state-of-the-art baseline, while requiring 9 MFLOPs less computation. As the
first work to accelerate ViT adaptation with low-rank backpropagation, our
LBP-WHT method is complementary to many prior efforts and can be combined with
them for better performance.
</p></li>
</ul>

<h3>Title: Boosting High Resolution Image Classification with Scaling-up Transformers. (arXiv:2309.15277v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15277">http://arxiv.org/abs/2309.15277</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15277]] Boosting High Resolution Image Classification with Scaling-up Transformers(http://arxiv.org/abs/2309.15277)</code></li>
<li>Summary: <p>We present a holistic approach for high resolution image classification that
won second place in the ICCV/CVPPA2023 Deep Nutrient Deficiency Challenge. The
approach consists of a full pipeline of: 1) data distribution analysis to check
potential domain shift, 2) backbone selection for a strong baseline model that
scales up for high resolution input, 3) transfer learning that utilizes
published pretrained models and continuous fine-tuning on small sub-datasets,
4) data augmentation for the diversity of training data and to prevent
overfitting, 5) test-time augmentation to improve the prediction's robustness,
and 6) "data soups" that conducts cross-fold model prediction average for
smoothened final test results.
</p></li>
</ul>

<h3>Title: Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15505">http://arxiv.org/abs/2309.15505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15505]] Finite Scalar Quantization: VQ-VAE Made Simple(http://arxiv.org/abs/2309.15505)</code></li>
<li>Summary: <p>We propose to replace vector quantization (VQ) in the latent representation
of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where
we project the VAE representation down to a few dimensions (typically less than
10). Each dimension is quantized to a small set of fixed values, leading to an
(implicit) codebook given by the product of these sets. By appropriately
choosing the number of dimensions and values each dimension can take, we obtain
the same codebook size as in VQ. On top of such discrete representations, we
can train the same models that have been trained on VQ-VAE representations. For
example, autoregressive and masked transformer models for image generation,
multimodal generation, and dense prediction computer vision tasks. Concretely,
we employ FSQ with MaskGIT for image generation, and with UViM for depth
estimation, colorization, and panoptic segmentation. Despite the much simpler
design of FSQ, we obtain competitive performance in all these tasks. We
emphasize that FSQ does not suffer from codebook collapse and does not need the
complex machinery employed in VQ (commitment losses, codebook reseeding, code
splitting, entropy penalties, etc.) to learn expressive discrete
representations.
</p></li>
</ul>

<h3>Title: Improving Facade Parsing with Vision Transformers and Line Integration. (arXiv:2309.15523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15523">http://arxiv.org/abs/2309.15523</a></li>
<li>Code URL: https://github.com/wbw520/rtfp</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15523]] Improving Facade Parsing with Vision Transformers and Line Integration(http://arxiv.org/abs/2309.15523)</code></li>
<li>Summary: <p>Facade parsing stands as a pivotal computer vision task with far-reaching
applications in areas like architecture, urban planning, and energy efficiency.
Despite the recent success of deep learning-based methods in yielding
impressive results on certain open-source datasets, their viability for
real-world applications remains uncertain. Real-world scenarios are
considerably more intricate, demanding greater computational efficiency.
Existing datasets often fall short in representing these settings, and previous
methods frequently rely on extra models to enhance accuracy, which requires
much computation cost. In this paper, we introduce Comprehensive Facade Parsing
(CFP), a dataset meticulously designed to encompass the intricacies of
real-world facade parsing tasks. Comprising a total of 602 high-resolution
street-view images, this dataset captures a diverse array of challenging
scenarios, including sloping angles and densely clustered buildings, with
painstakingly curated annotations for each image. We introduce a new pipeline
known as Revision-based Transformer Facade Parsing (RTFP). This marks the
pioneering utilization of Vision Transformers (ViT) in facade parsing, and our
experimental results definitively substantiate its merit. We also design Line
Acquisition, Filtering, and Revision (LAFR), an efficient yet accurate revision
algorithm that can improve the segment result solely from simple line detection
using prior knowledge of the facade. In ECP 2011, RueMonge 2014, and our CFP,
we evaluate the superiority of our method. The dataset and code are available
at https://github.com/wbw520/RTFP.
</p></li>
</ul>

<h3>Title: Neuromorphic Imaging and Classification with Graph Learning. (arXiv:2309.15627v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15627">http://arxiv.org/abs/2309.15627</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15627]] Neuromorphic Imaging and Classification with Graph Learning(http://arxiv.org/abs/2309.15627)</code></li>
<li>Summary: <p>Bio-inspired neuromorphic cameras asynchronously record pixel brightness
changes and generate sparse event streams. They can capture dynamic scenes with
little motion blur and more details in extreme illumination conditions. Due to
the multidimensional address-event structure, most existing vision algorithms
cannot properly handle asynchronous event streams. While several event
representations and processing methods have been developed to address such an
issue, they are typically driven by a large number of events, leading to
substantial overheads in runtime and memory. In this paper, we propose a new
graph representation of the event data and couple it with a Graph Transformer
to perform accurate neuromorphic classification. Extensive experiments show
that our approach leads to better results and excels at the challenging
realistic situations where only a small number of events and limited
computational resources are available, paving the way for neuromorphic
applications embedded into mobile facilities.
</p></li>
</ul>

<h3>Title: CAIT: Triple-Win Compression towards High Accuracy, Fast Inference, and Favorable Transferability For ViTs. (arXiv:2309.15755v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15755">http://arxiv.org/abs/2309.15755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15755]] CAIT: Triple-Win Compression towards High Accuracy, Fast Inference, and Favorable Transferability For ViTs(http://arxiv.org/abs/2309.15755)</code></li>
<li>Summary: <p>Vision Transformers (ViTs) have emerged as state-of-the-art models for
various vision tasks recently. However, their heavy computation costs remain
daunting for resource-limited devices. Consequently, researchers have dedicated
themselves to compressing redundant information in ViTs for acceleration.
However, they generally sparsely drop redundant image tokens by token pruning
or brutally remove channels by channel pruning, leading to a sub-optimal
balance between model performance and inference speed. They are also
disadvantageous in transferring compressed models to downstream vision tasks
that require the spatial structure of images, such as semantic segmentation. To
tackle these issues, we propose a joint compression method for ViTs that offers
both high accuracy and fast inference speed, while also maintaining favorable
transferability to downstream tasks (CAIT). Specifically, we introduce an
asymmetric token merging (ATME) strategy to effectively integrate neighboring
tokens. It can successfully compress redundant token information while
preserving the spatial structure of images. We further employ a consistent
dynamic channel pruning (CDCP) strategy to dynamically prune unimportant
channels in ViTs. Thanks to CDCP, insignificant channels in multi-head
self-attention modules of ViTs can be pruned uniformly, greatly enhancing the
model compression. Extensive experiments on benchmark datasets demonstrate that
our proposed method can achieve state-of-the-art performance across various
ViTs. For example, our pruned DeiT-Tiny and DeiT-Small achieve speedups of
1.7$\times$ and 1.9$\times$, respectively, without accuracy drops on ImageNet.
On the ADE20k segmentation dataset, our method can enjoy up to 1.31$\times$
speedups with comparable mIoU. Our code will be publicly available.
</p></li>
</ul>

<h3>Title: ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension. (arXiv:2309.15714v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15714">http://arxiv.org/abs/2309.15714</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15714]] ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension(http://arxiv.org/abs/2309.15714)</code></li>
<li>Summary: <p>With the recent explosion of large language models (LLMs), such as Generative
Pretrained Transformers (GPT), the need to understand the ability of humans and
machines to comprehend semantic language meaning has entered a new phase. This
requires interdisciplinary research that bridges the fields of cognitive
science and natural language processing (NLP). This pilot study aims to provide
insights into individuals' neural states during a semantic relation
reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and
electroencephalographic (EEG) data to study how the brain processes words with
varying degrees of relevance to a keyword during reading. We also use a feature
engineering approach to improve the fixation-related EEG data classification
while participants read words with high versus low relevance to the keyword.
The best validation accuracy in this word-level classification is over 60\%
across 12 subjects. Words of high relevance to the inference keyword had
significantly more eye fixations per word: 1.0584 compared to 0.6576 when
excluding no-fixation words, and 1.5126 compared to 1.4026 when including them.
This study represents the first attempt to classify brain states at a word
level using LLM knowledge. It provides valuable insights into human cognitive
abilities and the realm of Artificial General Intelligence (AGI), and offers
guidance for developing potential reading-assisted technologies.
</p></li>
</ul>

<h3>Title: Question answering using deep learning in low resource Indian language Marathi. (arXiv:2309.15779v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15779">http://arxiv.org/abs/2309.15779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15779]] Question answering using deep learning in low resource Indian language Marathi(http://arxiv.org/abs/2309.15779)</code></li>
<li>Summary: <p>Precise answers are extracted from a text for a given input question in a
question answering system. Marathi question answering system is created in
recent studies by using ontology, rule base and machine learning based
approaches. Recently transformer models and transfer learning approaches are
used to solve question answering challenges. In this paper we investigate
different transformer models for creating a reading comprehension-based Marathi
question answering system. We have experimented on different pretrained Marathi
language multilingual and monolingual models like Multilingual Representations
for Indian Languages (MuRIL), MahaBERT, Indic Bidirectional Encoder
Representations from Transformers (IndicBERT) and fine-tuned it on a Marathi
reading comprehension-based data set. We got the best accuracy in a MuRIL
multilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuning
the model on the Marathi dataset.
</p></li>
</ul>

<h3>Title: Balancing Computational Efficiency and Forecast Error in Machine Learning-based Time-Series Forecasting: Insights from Live Experiments on Meteorological Nowcasting. (arXiv:2309.15207v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15207">http://arxiv.org/abs/2309.15207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15207]] Balancing Computational Efficiency and Forecast Error in Machine Learning-based Time-Series Forecasting: Insights from Live Experiments on Meteorological Nowcasting(http://arxiv.org/abs/2309.15207)</code></li>
<li>Summary: <p>Machine learning for time-series forecasting remains a key area of research.
Despite successful application of many machine learning techniques, relating
computational efficiency to forecast error remains an under-explored domain.
This paper addresses this topic through a series of real-time experiments to
quantify the relationship between computational cost and forecast error using
meteorological nowcasting as an example use-case. We employ a variety of
popular regression techniques (XGBoost, FC-MLP, Transformer, and LSTM) for
multi-horizon, short-term forecasting of three variables (temperature, wind
speed, and cloud cover) for multiple locations. During a 5-day live experiment,
4000 data sources were streamed for training and inferencing 144 models per
hour. These models were parameterized to explore forecast error for two
computational cost minimization methods: a novel auto-adaptive data reduction
technique (Variance Horizon) and a performance-based concept drift-detection
mechanism. Forecast error of all model variations were benchmarked in real-time
against a state-of-the-art numerical weather prediction model. Performance was
assessed using classical and novel evaluation metrics. Results indicate that
using the Variance Horizon reduced computational usage by more than 50\%, while
increasing between 0-15\% in error. Meanwhile, performance-based retraining
reduced computational usage by up to 90\% while \emph{also} improving forecast
error by up to 10\%. Finally, the combination of both the Variance Horizon and
performance-based retraining outperformed other model configurations by up to
99.7\% when considering error normalized to computational usage.
</p></li>
</ul>

<h3>Title: Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15798">http://arxiv.org/abs/2309.15798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15798]] Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction(http://arxiv.org/abs/2309.15798)</code></li>
<li>Summary: <p>Single-step retrosynthesis is a crucial task in organic chemistry and drug
design, requiring the identification of required reactants to synthesize a
specific compound. with the advent of computer-aided synthesis planning, there
is growing interest in using machine-learning techniques to facilitate the
process. Existing template-free machine learning-based models typically utilize
transformer structures and represent molecules as ID sequences. However, these
methods often face challenges in fully leveraging the extensive topological
information of the molecule and aligning atoms between the production and
reactants, leading to results that are not as competitive as those of
semi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G),
also serves as a transformer-based template-free model but utilizes 2D
molecular graphs and 3D conformation information. Furthermore, our approach
simplifies the incorporation of production-reactant atom mapping alignment by
leveraging node alignment to determine a specific order for node generation and
generating molecular graphs in an auto-regressive manner node-by-node. This
method ensures that the node generation order coincides with the node order in
the input graph, overcoming the difficulty of determining a specific node
generation order in an auto-regressive manner. Our extensive benchmarking
results demonstrate that the proposed NAG2G can outperform the previous
state-of-the-art baselines in various metrics.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Subjective Face Transform using Human First Impressions. (arXiv:2309.15381v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15381">http://arxiv.org/abs/2309.15381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15381]] Subjective Face Transform using Human First Impressions(http://arxiv.org/abs/2309.15381)</code></li>
<li>Summary: <p>Humans tend to form quick subjective first impressions of non-physical
attributes when seeing someone's face, such as perceived trustworthiness or
attractiveness. To understand what variations in a face lead to different
subjective impressions, this work uses generative models to find semantically
meaningful edits to a face image that change perceived attributes. Unlike prior
work that relied on statistical manipulation in feature space, our end-to-end
framework considers trade-offs between preserving identity and changing
perceptual attributes. It maps identity-preserving latent space directions to
changes in attribute scores, enabling transformation of any input face along an
attribute axis according to a target change. We train on real and synthetic
faces, evaluate for in-domain and out-of-domain images using predictive models
and human ratings, demonstrating the generalizability of our approach.
Ultimately, such a framework can be used to understand and explain biases in
subjective interpretation of faces that are not dependent on the identity.
</p></li>
</ul>

<h3>Title: The Triad of Failure Modes and a Possible Way Out. (arXiv:2309.15420v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15420">http://arxiv.org/abs/2309.15420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15420]] The Triad of Failure Modes and a Possible Way Out(http://arxiv.org/abs/2309.15420)</code></li>
<li>Summary: <p>We present a novel objective function for cluster-based self-supervised
learning (SSL) that is designed to circumvent the triad of failure modes,
namely representation collapse, cluster collapse, and the problem of invariance
to permutations of cluster assignments. This objective consists of three key
components: (i) A generative term that penalizes representation collapse, (ii)
a term that promotes invariance to data augmentations, thereby addressing the
issue of label permutations and (ii) a uniformity term that penalizes cluster
collapse. Additionally, our proposed objective possesses two notable
advantages. Firstly, it can be interpreted from a Bayesian perspective as a
lower bound on the data log-likelihood. Secondly, it enables the training of a
standard backbone architecture without the need for asymmetric elements like
stop gradients, momentum encoders, or specialized clustering layers. Due to its
simplicity and theoretical foundation, our proposed objective is well-suited
for optimization. Experiments on both toy and real world data demonstrate its
effectiveness
</p></li>
</ul>

<h3>Title: P2I-NET: Mapping Camera Pose to Image via Adversarial Learning for New View Synthesis in Real Indoor Environments. (arXiv:2309.15526v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15526">http://arxiv.org/abs/2309.15526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15526]] P2I-NET: Mapping Camera Pose to Image via Adversarial Learning for New View Synthesis in Real Indoor Environments(http://arxiv.org/abs/2309.15526)</code></li>
<li>Summary: <p>Given a new $6DoF$ camera pose in an indoor environment, we study the
challenging problem of predicting the view from that pose based on a set of
reference RGBD views. Existing explicit or implicit 3D geometry construction
methods are computationally expensive while those based on learning have
predominantly focused on isolated views of object categories with regular
geometric structure. Differing from the traditional \textit{render-inpaint}
approach to new view synthesis in the real indoor environment, we propose a
conditional generative adversarial neural network (P2I-NET) to directly predict
the new view from the given pose. P2I-NET learns the conditional distribution
of the images of the environment for establishing the correspondence between
the camera pose and its view of the environment, and achieves this through a
number of innovative designs in its architecture and training lost function.
Two auxiliary discriminator constraints are introduced for enforcing the
consistency between the pose of the generated image and that of the
corresponding real world image in both the latent feature space and the real
world pose space. Additionally a deep convolutional neural network (CNN) is
introduced to further reinforce this consistency in the pixel space. We have
performed extensive new view synthesis experiments on real indoor datasets.
Results show that P2I-NET has superior performance against a number of NeRF
based strong baseline models. In particular, we show that P2I-NET is 40 to 100
times faster than these competitor techniques while synthesising similar
quality images. Furthermore, we contribute a new publicly available indoor
environment dataset containing 22 high resolution RGBD videos where each frame
also has accurate camera pose parameters.
</p></li>
</ul>

<h3>Title: Guided Frequency Loss for Image Restoration. (arXiv:2309.15563v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15563">http://arxiv.org/abs/2309.15563</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15563]] Guided Frequency Loss for Image Restoration(http://arxiv.org/abs/2309.15563)</code></li>
<li>Summary: <p>Image Restoration has seen remarkable progress in recent years. Many
generative models have been adapted to tackle the known restoration cases of
images. However, the interest in benefiting from the frequency domain is not
well explored despite its major factor in these particular cases of image
synthesis. In this study, we propose the Guided Frequency Loss (GFL), which
helps the model to learn in a balanced way the image's frequency content
alongside the spatial content. It aggregates three major components that work
in parallel to enhance learning efficiency; a Charbonnier component, a
Laplacian Pyramid component, and a Gradual Frequency component. We tested GFL
on the Super Resolution and the Denoising tasks. We used three different
datasets and three different architectures for each of them. We found that the
GFL loss improved the PSNR metric in most implemented experiments. Also, it
improved the training of the Super Resolution models in both SwinIR and SRGAN.
In addition, the utility of the GFL loss increased better on constrained data
due to the less stochasticity in the high frequencies' components among
samples.
</p></li>
</ul>

<h3>Title: A Unified View of Differentially Private Deep Generative Modeling. (arXiv:2309.15696v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15696">http://arxiv.org/abs/2309.15696</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15696]] A Unified View of Differentially Private Deep Generative Modeling(http://arxiv.org/abs/2309.15696)</code></li>
<li>Summary: <p>The availability of rich and vast data sources has greatly advanced machine
learning applications in various domains. However, data with privacy concerns
comes with stringent regulations that frequently prohibited data access and
data sharing. Overcoming these obstacles in compliance with privacy
considerations is key for technological progress in many real-world application
scenarios that involve privacy sensitive data. Differentially private (DP) data
publishing provides a compelling solution, where only a sanitized form of the
data is publicly released, enabling privacy-preserving downstream analysis and
reproducible research in sensitive domains. In recent years, various approaches
have been proposed for achieving privacy-preserving high-dimensional data
generation by private training on top of deep neural networks. In this paper,
we present a novel unified view that systematizes these approaches. Our view
provides a joint design space for systematically deriving methods that cater to
different use cases. We then discuss the strengths, limitations, and inherent
correlations between different approaches, aiming to shed light on crucial
aspects and inspire future research. We conclude by presenting potential paths
forward for the field of DP data generation, with the aim of steering the
community toward making the next important steps in advancing
privacy-preserving learning.
</p></li>
</ul>

<h3>Title: Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15649">http://arxiv.org/abs/2309.15649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15649]] Generative Speech Recognition Error Correction with Large Language Models(http://arxiv.org/abs/2309.15649)</code></li>
<li>Summary: <p>We explore the ability of large language models (LLMs) to act as ASR
post-processors that perform rescoring and error correction. Our focus is on
instruction prompting to let LLMs perform these task without fine-tuning, for
which we evaluate different prompting schemes, both zero- and few-shot
in-context learning, and a novel task-activating prompting (TAP) method that
combines instruction and demonstration. Using a pre-trained first-pass system
and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that
rescoring only by in-context learning with frozen LLMs achieves results that
are competitive with rescoring by domain-tuned LMs. By combining prompting
techniques with fine-tuning we achieve error rates below the N-best oracle
level, showcasing the generalization power of the LLMs.
</p></li>
</ul>

<h3>Title: HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15701">http://arxiv.org/abs/2309.15701</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15701]] HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models(http://arxiv.org/abs/2309.15701)</code></li>
<li>Summary: <p>Advancements in deep neural networks have allowed automatic speech
recognition (ASR) systems to attain human parity on several publicly available
clean speech datasets. However, even state-of-the-art ASR systems experience
performance degradation when confronted with adverse conditions, as a
well-trained acoustic model is sensitive to variations in the speech domain,
e.g., background noise. Intuitively, humans address this issue by relying on
their linguistic knowledge: the meaning of ambiguous spoken terms is usually
inferred from contextual cues thereby reducing the dependency on the auditory
system. Inspired by this observation, we introduce the first open-source
benchmark to utilize external large language models (LLMs) for ASR error
correction, where N-best decoding hypotheses provide informative elements for
true transcription prediction. This approach is a paradigm shift from the
traditional language model rescoring strategy that can only select one
candidate hypothesis as the output transcription. The proposed benchmark
contains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs
of N-best hypotheses and corresponding accurate transcriptions across prevalent
speech domains. Given this dataset, we examine three types of error correction
techniques based on LLMs with varying amounts of labeled
hypotheses-transcription pairs, which gains a significant word error rate (WER)
reduction. Experimental evidence demonstrates the proposed technique achieves a
breakthrough by surpassing the upper bound of traditional re-ranking based
methods. More surprisingly, LLM with reasonable prompt and its generative
capability can even correct those tokens that are missing in N-best list. We
make our results publicly accessible for reproducible pipelines with released
pre-trained models, thus providing a new evaluation paradigm for ASR error
correction with LLMs.
</p></li>
</ul>

<h3>Title: Deep Generative Methods for Producing Forecast Trajectories in Power Systems. (arXiv:2309.15137v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15137">http://arxiv.org/abs/2309.15137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15137]] Deep Generative Methods for Producing Forecast Trajectories in Power Systems(http://arxiv.org/abs/2309.15137)</code></li>
<li>Summary: <p>With the expansion of renewables in the electricity mix, power grid
variability will increase, hence a need to robustify the system to guarantee
its security. Therefore, Transport System Operators (TSOs) must conduct
analyses to simulate the future functioning of power systems. Then, these
simulations are used as inputs in decision-making processes. In this context,
we investigate using deep learning models to generate energy production and
load forecast trajectories. To capture the spatiotemporal correlations in these
multivariate time series, we adapt autoregressive networks and normalizing
flows, demonstrating their effectiveness against the current copula-based
statistical approach. We conduct extensive experiments on the French TSO RTE
wind forecast data and compare the different models with \textit{ad hoc}
evaluation metrics for time series generation.
</p></li>
</ul>

<h3>Title: Deep Learning in Deterministic Computational Mechanics. (arXiv:2309.15421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15421">http://arxiv.org/abs/2309.15421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15421]] Deep Learning in Deterministic Computational Mechanics(http://arxiv.org/abs/2309.15421)</code></li>
<li>Summary: <p>The rapid growth of deep learning research, including within the field of
computational mechanics, has resulted in an extensive and diverse body of
literature. To help researchers identify key concepts and promising
methodologies within this field, we provide an overview of deep learning in
deterministic computational mechanics. Five main categories are identified and
explored: simulation substitution, simulation enhancement, discretizations as
neural networks, generative approaches, and deep reinforcement learning. This
review focuses on deep learning methods rather than applications for
computational mechanics, thereby enabling researchers to explore this field
more effectively. As such, the review is not necessarily aimed at researchers
with extensive knowledge of deep learning -- instead, the primary audience is
researchers at the verge of entering this field or those who attempt to gain an
overview of deep learning in computational mechanics. The discussed concepts
are, therefore, explained as simple as possible.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15289">http://arxiv.org/abs/2309.15289</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15289]] SEPT: Towards Efficient Scene Representation Learning for Motion Prediction(http://arxiv.org/abs/2309.15289)</code></li>
<li>Summary: <p>Motion prediction is crucial for autonomous vehicles to operate safely in
complex traffic environments. Extracting effective spatiotemporal relationships
among traffic elements is key to accurate forecasting. Inspired by the
successful practice of pretrained large language models, this paper presents
SEPT, a modeling framework that leverages self-supervised learning to develop
powerful spatiotemporal understanding for complex traffic scenes. Specifically,
our approach involves three masking-reconstruction modeling tasks on scene
inputs including agents' trajectories and road network, pretraining the scene
encoder to capture kinematics within trajectory, spatial structure of road
network, and interactions among roads and agents. The pretrained encoder is
then finetuned on the downstream forecasting task. Extensive experiments
demonstrate that SEPT, without elaborate architectural design or manual feature
engineering, achieves state-of-the-art performance on the Argoverse 1 and
Argoverse 2 motion forecasting benchmarks, outperforming previous methods on
all main metrics by a large margin.
</p></li>
</ul>

<h3>Title: Tackling VQA with Pretrained Foundation Models without Further Training. (arXiv:2309.15487v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15487">http://arxiv.org/abs/2309.15487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15487]] Tackling VQA with Pretrained Foundation Models without Further Training(http://arxiv.org/abs/2309.15487)</code></li>
<li>Summary: <p>Large language models (LLMs) have achieved state-of-the-art results in many
natural language processing tasks. They have also demonstrated ability to adapt
well to different tasks through zero-shot or few-shot settings. With the
capability of these LLMs, researchers have looked into how to adopt them for
use with Visual Question Answering (VQA). Many methods require further training
to align the image and text embeddings. However, these methods are
computationally expensive and requires large scale image-text dataset for
training. In this paper, we explore a method of combining pretrained LLMs and
other foundation models without further training to solve the VQA problem. The
general idea is to use natural language to represent the images such that the
LLM can understand the images. We explore different decoding strategies for
generating textual representation of the image and evaluate their performance
on the VQAv2 dataset.
</p></li>
</ul>

<h3>Title: MindGPT: Interpreting What You See with Non-invasive Brain Recordings. (arXiv:2309.15729v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15729">http://arxiv.org/abs/2309.15729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15729]] MindGPT: Interpreting What You See with Non-invasive Brain Recordings(http://arxiv.org/abs/2309.15729)</code></li>
<li>Summary: <p>Decoding of seen visual contents with non-invasive brain recordings has
important scientific and practical values. Efforts have been made to recover
the seen images from brain signals. However, most existing approaches cannot
faithfully reflect the visual contents due to insufficient image quality or
semantic mismatches. Compared with reconstructing pixel-level visual images,
speaking is a more efficient and effective way to explain visual information.
Here we introduce a non-invasive neural decoder, termed as MindGPT, which
interprets perceived visual stimuli into natural languages from fMRI signals.
Specifically, our model builds upon a visually guided neural encoder with a
cross-attention mechanism, which permits us to guide latent neural
representations towards a desired language semantic direction in an end-to-end
manner by the collaborative use of the large language model GPT. By doing so,
we found that the neural representations of the MindGPT are explainable, which
can be used to evaluate the contributions of visual properties to language
semantics. Our experiments show that the generated word sequences truthfully
represented the visual information (with essential details) conveyed in the
seen stimuli. The results also suggested that with respect to language decoding
tasks, the higher visual cortex (HVC) is more semantically informative than the
lower visual cortex (LVC), and using only the HVC can recover most of the
semantic information. The code of the MindGPT model will be publicly available
at https://github.com/JxuanC/MindGPT.
</p></li>
</ul>

<h3>Title: One For All: Video Conversation is Feasible Without Video Instruction Tuning. (arXiv:2309.15785v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15785">http://arxiv.org/abs/2309.15785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15785]] One For All: Video Conversation is Feasible Without Video Instruction Tuning(http://arxiv.org/abs/2309.15785)</code></li>
<li>Summary: <p>The recent progress in Large Language Models (LLM) has spurred various
advancements in image-language conversation agents, while how to build a
proficient video-based dialogue system is still under exploration. Considering
the extensive scale of LLM and visual backbone, minimal GPU memory is left for
facilitating effective temporal modeling, which is crucial for comprehending
and providing feedback on videos. To this end, we propose Branching Temporal
Adapter (BT-Adapter), a novel method for extending image-language pretrained
models into the video domain. Specifically, BT-Adapter serves as a plug-and-use
temporal modeling branch alongside the pretrained visual encoder, which is
tuned while keeping the backbone frozen. Just pretrained once, BT-Adapter can
be seamlessly integrated into all image conversation models using this version
of CLIP, enabling video conversations without the need for video instructions.
Besides, we develop a unique asymmetric token masking strategy inside the
branch with tailor-made training tasks for BT-Adapter, facilitating faster
convergence and better results. Thanks to BT-Adapter, we are able to empower
existing multimodal dialogue models with strong video understanding
capabilities without incurring excessive GPU costs. Without bells and whistles,
BT-Adapter achieves (1) state-of-the-art zero-shot results on various video
tasks using thousands of fewer GPU hours. (2) better performance than current
video chatbots without any video instruction tuning. (3) state-of-the-art
results of video chatting using video instruction tuning, outperforming
previous SOTAs by a large margin.
</p></li>
</ul>

<h3>Title: Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15223">http://arxiv.org/abs/2309.15223</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15223]] Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition(http://arxiv.org/abs/2309.15223)</code></li>
<li>Summary: <p>We propose a neural language modeling system based on low-rank adaptation
(LoRA) for speech recognition output rescoring. Although pretrained language
models (LMs) like BERT have shown superior performance in second-pass
rescoring, the high computational cost of scaling up the pretraining stage and
adapting the pretrained models to specific domains limit their practical use in
rescoring. Here we present a method based on low-rank decomposition to train a
rescoring BERT model and adapt it to new domains using only a fraction (0.08%)
of the pretrained parameters. These inserted matrices are optimized through a
discriminative training objective along with a correlation-based regularization
loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is
evaluated on LibriSpeech and internal datasets with decreased training times by
factors between 5.4 and 3.6.
</p></li>
</ul>

<h3>Title: Beyond the Chat: Executable and Verifiable Text-Editing with LLMs. (arXiv:2309.15337v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15337">http://arxiv.org/abs/2309.15337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15337]] Beyond the Chat: Executable and Verifiable Text-Editing with LLMs(http://arxiv.org/abs/2309.15337)</code></li>
<li>Summary: <p>Conversational interfaces powered by Large Language Models (LLMs) have
recently become a popular way to obtain feedback during document editing.
However, standard chat-based conversational interfaces do not support
transparency and verifiability of the editing changes that they suggest. To
give the author more agency when editing with an LLM, we present InkSync, an
editing interface that suggests executable edits directly within the document
being edited. Because LLMs are known to introduce factual errors, Inksync also
supports a 3-stage approach to mitigate this risk: Warn authors when a
suggested edit introduces new information, help authors Verify the new
information's accuracy through external search, and allow an auditor to perform
an a-posteriori verification by Auditing the document via a trace of all
auto-generated content. Two usability studies confirm the effectiveness of
InkSync's components when compared to standard LLM-based chat interfaces,
leading to more accurate, more efficient editing, and improved user experience.
</p></li>
</ul>

<h3>Title: Graph Neural Prompting with Large Language Models. (arXiv:2309.15427v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15427">http://arxiv.org/abs/2309.15427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15427]] Graph Neural Prompting with Large Language Models(http://arxiv.org/abs/2309.15427)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have shown remarkable generalization capability
with exceptional performance in various language modeling tasks. However, they
still exhibit inherent limitations in precisely capturing and returning
grounded knowledge. While existing work has explored utilizing knowledge graphs
to enhance language modeling via joint training and customized model
architectures, applying this to LLMs is problematic owing to their large number
of parameters and high computational cost. In addition, how to leverage the
pre-trained LLMs and avoid training a customized model from scratch remains an
open question. In this work, we propose Graph Neural Prompting (GNP), a novel
plug-and-play method to assist pre-trained LLMs in learning beneficial
knowledge from KGs. GNP encompasses various designs, including a standard graph
neural network encoder, a cross-modality pooling module, a domain projector,
and a self-supervised link prediction objective. Extensive experiments on
multiple datasets demonstrate the superiority of GNP on both commonsense and
biomedical reasoning tasks across different LLM sizes and settings.
</p></li>
</ul>

<h3>Title: ChatCounselor: A Large Language Models for Mental Health Support. (arXiv:2309.15461v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15461">http://arxiv.org/abs/2309.15461</a></li>
<li>Code URL: https://github.com/emocareai/chatpsychiatrist</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15461]] ChatCounselor: A Large Language Models for Mental Health Support(http://arxiv.org/abs/2309.15461)</code></li>
<li>Summary: <p>This paper presents ChatCounselor, a large language model (LLM) solution
designed to provide mental health support. Unlike generic chatbots,
ChatCounselor is distinguished by its foundation in real conversations between
consulting clients and professional psychologists, enabling it to possess
specialized knowledge and counseling skills in the field of psychology. The
training dataset, Psych8k, was constructed from 260 in-depth interviews, each
spanning an hour. To assess the quality of counseling responses, the counseling
Bench was devised. Leveraging GPT-4 and meticulously crafted prompts based on
seven metrics of psychological counseling assessment, the model underwent
evaluation using a set of real-world counseling questions. Impressively,
ChatCounselor surpasses existing open-source models in the counseling Bench and
approaches the performance level of ChatGPT, showcasing the remarkable
enhancement in model capability attained through high-quality domain-specific
data.
</p></li>
</ul>

<h3>Title: NLPBench: Evaluating Large Language Models on Solving NLP Problems. (arXiv:2309.15630v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15630">http://arxiv.org/abs/2309.15630</a></li>
<li>Code URL: https://github.com/linxins97/nlpbench</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15630]] NLPBench: Evaluating Large Language Models on Solving NLP Problems(http://arxiv.org/abs/2309.15630)</code></li>
<li>Summary: <p>Recent developments in large language models (LLMs) have shown promise in
enhancing the capabilities of natural language processing (NLP). Despite these
successes, there remains a dearth of research dedicated to the NLP
problem-solving abilities of LLMs. To fill the gap in this area, we present a
unique benchmarking dataset, NLPBench, comprising 378 college-level NLP
questions spanning various NLP topics sourced from Yale University's prior
final exams. NLPBench includes questions with context, in which multiple
sub-questions share the same public information, and diverse question types,
including multiple choice, short answer, and math. Our evaluation, centered on
LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting
strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study
reveals that the effectiveness of the advanced prompting strategies can be
inconsistent, occasionally damaging LLM performance, especially in smaller
models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated
specific shortcomings in LLMs' scientific problem-solving skills, with
weaknesses in logical decomposition and reasoning notably affecting results.
</p></li>
</ul>

<h3>Title: Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis. (arXiv:2309.15656v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15656">http://arxiv.org/abs/2309.15656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15656]] Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis(http://arxiv.org/abs/2309.15656)</code></li>
<li>Summary: <p>Scripted dialogues such as movie and TV subtitles constitute a widespread
source of training data for conversational NLP models. However, the linguistic
characteristics of those dialogues are notably different from those observed in
corpora of spontaneous interactions. This difference is particularly marked for
communicative feedback and grounding phenomena such as backchannels,
acknowledgments, or clarification requests. Such signals are known to
constitute a key part of the conversation flow and are used by the dialogue
participants to provide feedback to one another on their perception of the
ongoing interaction. This paper presents a quantitative analysis of such
communicative feedback phenomena in both subtitles and spontaneous
conversations. Based on dialogue data in English, French, German, Hungarian,
Italian, Japanese, Norwegian and Chinese, we extract both lexical statistics
and classification outputs obtained with a neural dialogue act tagger. Two main
findings of this empirical study are that (1) conversational feedback is
markedly less frequent in subtitles than in spontaneous dialogues and (2)
subtitles contain a higher proportion of negative feedback. Furthermore, we
show that dialogue responses generated by large language models also follow the
same underlying trends and include comparatively few occurrences of
communicative feedback, except when those models are explicitly fine-tuned on
spontaneous dialogues.
</p></li>
</ul>

<h3>Title: Large Language Model Routing with Benchmark Datasets. (arXiv:2309.15789v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15789">http://arxiv.org/abs/2309.15789</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15789]] Large Language Model Routing with Benchmark Datasets(http://arxiv.org/abs/2309.15789)</code></li>
<li>Summary: <p>There is a rapidly growing number of open-source Large Language Models (LLMs)
and benchmark datasets to compare them. While some models dominate these
benchmarks, no single model typically achieves the best accuracy in all tasks
and use cases. In this work, we address the challenge of selecting the best LLM
out of a collection of models for new tasks. We propose a new formulation for
the problem, in which benchmark datasets are repurposed to learn a "router"
model for this LLM selection, and we show that this problem can be reduced to a
collection of binary classification tasks. We demonstrate the utility and
limitations of learning model routers from various benchmark datasets, where we
consistently improve performance upon using any single model for all tasks.
</p></li>
</ul>

<h3>Title: Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15806">http://arxiv.org/abs/2309.15806</a></li>
<li>Code URL: https://github.com/chuanyang-zheng/lyra-theorem-prover</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15806]] Lyra: Orchestrating Dual Correction in Automated Theorem Proving(http://arxiv.org/abs/2309.15806)</code></li>
<li>Summary: <p>Large Language Models (LLMs) present an intriguing avenue for exploration in
the field of formal theorem proving. Nevertheless, their full potential,
particularly concerning the mitigation of hallucinations and refinement through
prover error messages, remains an area that has yet to be thoroughly
investigated. To enhance the effectiveness of LLMs in the field, we introduce
the Lyra, a new framework that employs two distinct correction mechanisms: Tool
Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in
the post-processing of formal proofs, we leverage prior knowledge to utilize
predefined prover tools (e.g., Sledgehammer) for guiding the replacement of
incorrect tools. Tool Correction significantly contributes to mitigating
hallucinations, thereby improving the overall accuracy of the proof. In
addition, we introduce Conjecture Correction, an error feedback mechanism
designed to interact with prover to refine formal proof conjectures with prover
error messages. Compared to the previous refinement framework, the proposed
Conjecture Correction refines generation with instruction but does not collect
paired (generation, error &amp; refinement) prompts. Our method has achieved
state-of-the-art (SOTA) performance on both miniF2F validation (48.0% -&gt; 55.3%)
and test (45.5% -&gt; 51.2%). We also present 3 IMO problems solved by Lyra. We
believe Tool Correction (post-process for hallucination mitigation) and
Conjecture Correction (subgoal adjustment from interaction with environment)
could provide a promising avenue for future research in this field.
</p></li>
</ul>

<h3>Title: How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. (arXiv:2309.15840v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15840">http://arxiv.org/abs/2309.15840</a></li>
<li>Code URL: https://github.com/lorypack/llm-liedetector</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15840]] How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions(http://arxiv.org/abs/2309.15840)</code></li>
<li>Summary: <p>Large language models (LLMs) can "lie", which we define as outputting false
statements despite "knowing" the truth in a demonstrable sense. LLMs might
"lie", for example, when instructed to output misinformation. Here, we develop
a simple lie detector that requires neither access to the LLM's activations
(black-box) nor ground-truth knowledge of the fact in question. The detector
works by asking a predefined set of unrelated follow-up questions after a
suspected lie, and feeding the LLM's yes/no answers into a logistic regression
classifier. Despite its simplicity, this lie detector is highly accurate and
surprisingly general. When trained on examples from a single setting --
prompting GPT-3.5 to lie about factual questions -- the detector generalises
out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie,
(3) sycophantic lies, and (4) lies emerging in real-life scenarios such as
sales. These results indicate that LLMs have distinctive lie-related
behavioural patterns, consistent across architectures and contexts, which could
enable general-purpose lie detection.
</p></li>
</ul>

<h3>Title: Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15531">http://arxiv.org/abs/2309.15531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15531]] Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models(http://arxiv.org/abs/2309.15531)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have recently demonstrated a remarkable success
across various tasks. However, efficiently serving LLMs has been a challenge
due to its large memory bottleneck, specifically in small batch inference
settings (e.g. mobile devices). Weight-only quantization can be a promising
approach, but sub-4 bit quantization remains a challenge due to large-magnitude
activation outliers. To mitigate the undesirable outlier effect, we first
propose per-IC quantization, a simple yet effective method that creates
quantization groups within each input channel (IC) rather than the conventional
per-output channel (OC). Our method is motivated by the observation that
activation outliers affect the input dimension of the weight matrix, so
similarly grouping the weights in the IC direction can isolate outliers to be
within a group. We also find that activation outliers do not dictate
quantization difficulty, and inherent weight sensitivities also exist. With
per-IC quantization as a new outlier-friendly scheme, we then propose Adaptive
Dimensions (AdaDim), a versatile quantization framework that can adapt to
various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim
by augmenting prior methods such as Round-To-Nearest and GPTQ, showing
significant improvements across various language modeling benchmarks for both
base (up to +4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval)
LLMs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Memory-Efficient Continual Learning Object Segmentation for Long Video. (arXiv:2309.15274v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15274">http://arxiv.org/abs/2309.15274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15274]] Memory-Efficient Continual Learning Object Segmentation for Long Video(http://arxiv.org/abs/2309.15274)</code></li>
<li>Summary: <p>Recent state-of-the-art semi-supervised Video Object Segmentation (VOS)
methods have shown significant improvements in target object segmentation
accuracy when information from preceding frames is used in undertaking
segmentation on the current frame. In particular, such memory-based approaches
can help a model to more effectively handle appearance changes (representation
drift) or occlusions. Ideally, for maximum performance, online VOS methods
would need all or most of the preceding frames (or their extracted information)
to be stored in memory and be used for online learning in consecutive frames.
Such a solution is not feasible for long videos, as the required memory size
would grow without bound. On the other hand, these methods can fail when memory
is limited and a target object experiences repeated representation drifts
throughout a video.
</p>
<p>We propose two novel techniques to reduce the memory requirement of online
VOS methods while improving modeling accuracy and generalization on long
videos. Motivated by the success of continual learning techniques in preserving
previously-learned knowledge, here we propose Gated-Regularizer Continual
Learning (GRCL), which improves the performance of any online VOS subject to
limited memory, and a Reconstruction-based Memory Selection Continual Learning
(RMSCL) which empowers online VOS methods to efficiently benefit from stored
information in memory.
</p>
<p>Experimental results show that the proposed methods improve the performance
of online VOS models up to 10 %, and boosts their robustness on long-video
datasets while maintaining comparable performance on short-video datasets
DAVIS16 and DAVIS17.
</p></li>
</ul>

<h3>Title: M$^{3}$3D: Learning 3D priors using Multi-Modal Masked Autoencoders for 2D image and video understanding. (arXiv:2309.15313v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15313">http://arxiv.org/abs/2309.15313</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15313]] M$^{3}$3D: Learning 3D priors using Multi-Modal Masked Autoencoders for 2D image and video understanding(http://arxiv.org/abs/2309.15313)</code></li>
<li>Summary: <p>We present a new pre-training strategy called M$^{3}$3D
($\underline{M}$ulti-$\underline{M}$odal $\underline{M}$asked $\underline{3D}$)
built based on Multi-modal masked autoencoders that can leverage 3D priors and
learned cross-modal representations in RGB-D data. We integrate two major
self-supervised learning frameworks; Masked Image Modeling (MIM) and
contrastive learning; aiming to effectively embed masked 3D priors and modality
complementary features to enhance the correspondence between modalities. In
contrast to recent approaches which are either focusing on specific downstream
tasks or require multi-view correspondence, we show that our pre-training
strategy is ubiquitous, enabling improved representation learning that can
transfer into improved performance on various downstream tasks such as video
action recognition, video action detection, 2D semantic segmentation and depth
estimation. Experiments show that M$^{3}$3D outperforms the existing
state-of-the-art approaches on ScanNet, NYUv2, UCF-101 and OR-AR, particularly
with an improvement of +1.3\% mIoU against Mask3D on ScanNet semantic
segmentation. We further evaluate our method on low-data regime and demonstrate
its superior data efficiency compared to current state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: Towards Foundation Models Learned from Anatomy in Medical Imaging via Self-Supervision. (arXiv:2309.15358v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15358">http://arxiv.org/abs/2309.15358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15358]] Towards Foundation Models Learned from Anatomy in Medical Imaging via Self-Supervision(http://arxiv.org/abs/2309.15358)</code></li>
<li>Summary: <p>Human anatomy is the foundation of medical imaging and boasts one striking
characteristic: its hierarchy in nature, exhibiting two intrinsic properties:
(1) locality: each anatomical structure is morphologically distinct from the
others; and (2) compositionality: each anatomical structure is an integrated
part of a larger whole. We envision a foundation model for medical imaging that
is consciously and purposefully developed upon this foundation to gain the
capability of "understanding" human anatomy and to possess the fundamental
properties of medical imaging. As our first step in realizing this vision
towards foundation models in medical imaging, we devise a novel self-supervised
learning (SSL) strategy that exploits the hierarchical nature of human anatomy.
Our extensive experiments demonstrate that the SSL pretrained model, derived
from our training strategy, not only outperforms state-of-the-art (SOTA)
fully/self-supervised baselines but also enhances annotation efficiency,
offering potential few-shot segmentation capabilities with performance
improvements ranging from 9% to 30% for segmentation tasks compared to SSL
baselines. This performance is attributed to the significance of anatomy
comprehension via our learning strategy, which encapsulates the intrinsic
attributes of anatomical structures-locality and compositionality-within the
embedding space, yet overlooked in existing SSL methods. All code and
pretrained models are available at https://github.com/JLiangLab/Eden.
</p></li>
</ul>

<h3>Title: Seeing Beyond the Patch: Scale-Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery based on Reinforcement Learning. (arXiv:2309.15372v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15372">http://arxiv.org/abs/2309.15372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15372]] Seeing Beyond the Patch: Scale-Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery based on Reinforcement Learning(http://arxiv.org/abs/2309.15372)</code></li>
<li>Summary: <p>In remote sensing imagery analysis, patch-based methods have limitations in
capturing information beyond the sliding window. This shortcoming poses a
significant challenge in processing complex and variable geo-objects, which
results in semantic inconsistency in segmentation results. To address this
challenge, we propose a dynamic scale perception framework, named GeoAgent,
which adaptively captures appropriate scale context information outside the
image patch based on the different geo-objects. In GeoAgent, each image patch's
states are represented by a global thumbnail and a location mask. The global
thumbnail provides context beyond the patch, and the location mask guides the
perceived spatial relationships. The scale-selection actions are performed
through a Scale Control Agent (SCA). A feature indexing module is proposed to
enhance the ability of the agent to distinguish the current image patch's
location. The action switches the patch scale and context branch of a
dual-branch segmentation network that extracts and fuses the features of
multi-scale patches. The GeoAgent adjusts the network parameters to perform the
appropriate scale-selection action based on the reward received for the
selected scale. The experimental results, using two publicly available datasets
and our newly constructed dataset WUSU, demonstrate that GeoAgent outperforms
previous segmentation methods, particularly for large-scale mapping
applications.
</p></li>
</ul>

<h3>Title: Inherit with Distillation and Evolve with Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory. (arXiv:2309.15413v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15413">http://arxiv.org/abs/2309.15413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15413]] Inherit with Distillation and Evolve with Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory(http://arxiv.org/abs/2309.15413)</code></li>
<li>Summary: <p>As a front-burner problem in incremental learning, class incremental semantic
segmentation (CISS) is plagued by catastrophic forgetting and semantic drift.
Although recent methods have utilized knowledge distillation to transfer
knowledge from the old model, they are still unable to avoid pixel confusion,
which results in severe misclassification after incremental steps due to the
lack of annotations for past and future classes. Meanwhile data-replay-based
approaches suffer from storage burdens and privacy concerns. In this paper, we
propose to address CISS without exemplar memory and resolve catastrophic
forgetting as well as semantic drift synchronously. We present Inherit with
Distillation and Evolve with Contrast (IDEC), which consists of a Dense
Knowledge Distillation on all Aspects (DADA) manner and an Asymmetric
Region-wise Contrastive Learning (ARCL) module. Driven by the devised dynamic
class-specific pseudo-labelling strategy, DADA distils intermediate-layer
features and output-logits collaboratively with more emphasis on
semantic-invariant knowledge inheritance. ARCL implements region-wise
contrastive learning in the latent space to resolve semantic drift among known
classes, current classes, and unknown classes. We demonstrate the effectiveness
of our method on multiple CISS tasks by state-of-the-art performance, including
Pascal VOC 2012, ADE20K and ISPRS datasets. Our method also shows superior
anti-forgetting ability, particularly in multi-step CISS tasks.
</p></li>
</ul>

<h3>Title: Investigating the changes in BOLD responses during viewing of images with varied complexity: An fMRI time-series based analysis on human vision. (arXiv:2309.15495v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15495">http://arxiv.org/abs/2309.15495</a></li>
<li>Code URL: https://github.com/naveen7102/fmri-time-series-classification</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15495]] Investigating the changes in BOLD responses during viewing of images with varied complexity: An fMRI time-series based analysis on human vision(http://arxiv.org/abs/2309.15495)</code></li>
<li>Summary: <p>Functional MRI (fMRI) is widely used to examine brain functionality by
detecting alteration in oxygenated blood flow that arises with brain activity.
This work aims to investigate the neurological variation of human brain
responses during viewing of images with varied complexity using fMRI time
series (TS) analysis. Publicly available BOLD5000 dataset is used for this
purpose which contains fMRI scans while viewing 5254 distinct images of diverse
categories, drawn from three standard computer vision datasets: COCO, Imagenet
and SUN. To understand vision, it is important to study how brain functions
while looking at images of diverse complexities. Our first study employs
classical machine learning and deep learning strategies to classify image
complexity-specific fMRI TS, represents instances when images from COCO,
Imagenet and SUN datasets are seen. The implementation of this classification
across visual datasets holds great significance, as it provides valuable
insights into the fluctuations in BOLD signals when perceiving images of
varying complexities. Subsequently, temporal semantic segmentation is also
performed on whole fMRI TS to segment these time instances. The obtained result
of this analysis has established a baseline in studying how differently human
brain functions while looking into images of diverse complexities. Therefore,
accurate identification and distinguishing of variations in BOLD signals from
fMRI TS data serves as a critical initial step in vision studies, providing
insightful explanations for how static images with diverse complexities are
perceived.
</p></li>
</ul>

<h3>Title: Learning from SAM: Harnessing a Segmentation Foundation Model for Sim2Real Domain Adaptation through Regularization. (arXiv:2309.15562v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15562">http://arxiv.org/abs/2309.15562</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15562]] Learning from SAM: Harnessing a Segmentation Foundation Model for Sim2Real Domain Adaptation through Regularization(http://arxiv.org/abs/2309.15562)</code></li>
<li>Summary: <p>Domain adaptation is especially important for robotics applications, where
target domain training data is usually scarce and annotations are costly to
obtain. We present a method for self-supervised domain adaptation for the
scenario where annotated source domain data (e.g. from synthetic generation) is
available, but the target domain data is completely unannotated. Our method
targets the semantic segmentation task and leverages a segmentation foundation
model (Segment Anything Model) to obtain segment information on unannotated
data. We take inspiration from recent advances in unsupervised local feature
learning and propose an invariance-variance loss structure over the detected
segments for regularizing feature representations in the target domain.
Crucially, this loss structure and network architecture can handle overlapping
segments and oversegmentation as produced by Segment Anything. We demonstrate
the advantage of our method on the challenging YCB-Video and HomebrewedDB
datasets and show that it outperforms prior work and, on YCB-Video, even a
network trained with real annotations.
</p></li>
</ul>

<h3>Title: Leveraging Topology for Domain Adaptive Road Segmentation in Satellite and Aerial Imagery. (arXiv:2309.15625v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15625">http://arxiv.org/abs/2309.15625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15625]] Leveraging Topology for Domain Adaptive Road Segmentation in Satellite and Aerial Imagery(http://arxiv.org/abs/2309.15625)</code></li>
<li>Summary: <p>Getting precise aspects of road through segmentation from remote sensing
imagery is useful for many real-world applications such as autonomous vehicles,
urban development and planning, and achieving sustainable development goals.
Roads are only a small part of the image, and their appearance, type, width,
elevation, directions, etc. exhibit large variations across geographical areas.
Furthermore, due to differences in urbanization styles, planning, and the
natural environments; regions along the roads vary significantly. Due to these
variations among the train and test domains, the road segmentation algorithms
fail to generalize to new geographical locations. Unlike the generic domain
alignment scenarios, road segmentation has no scene structure, and generic
domain adaptation methods are unable to enforce topological properties like
continuity, connectivity, smoothness, etc., thus resulting in degraded domain
alignment. In this work, we propose a topology-aware unsupervised domain
adaptation approach for road segmentation in remote sensing imagery.
Specifically, we predict road skeleton, an auxiliary task to impose the
topological constraints. To enforce consistent predictions of road and
skeleton, especially in the unlabeled target domain, the conformity loss is
defined across the skeleton prediction head and the road-segmentation head.
Furthermore, for self-training, we filter out the noisy pseudo-labels by using
a connectivity-based pseudo-labels refinement strategy, on both road and
skeleton segmentation heads, thus avoiding holes and discontinuities. Extensive
experiments on the benchmark datasets show the effectiveness of the proposed
approach compared to existing state-of-the-art methods. Specifically, for
SpaceNet to DeepGlobe adaptation, the proposed approach outperforms the
competing methods by a minimum margin of 6.6%, 6.7%, and 9.8% in IoU, F1-score,
and APLS, respectively.
</p></li>
</ul>

<h3>Title: End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning. (arXiv:2309.15683v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15683">http://arxiv.org/abs/2309.15683</a></li>
<li>Code URL: https://github.com/Thinksky5124/SVTAS</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15683]] End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning(http://arxiv.org/abs/2309.15683)</code></li>
<li>Summary: <p>Temporal Action Segmentation (TAS) from video is a kind of frame recognition
task for long video with multiple action classes. As an video understanding
task for long videos, current methods typically combine multi-modality action
recognition models with temporal models to convert feature sequences to label
sequences. This approach can only be applied to offline scenarios, which
severely limits the TAS application. Therefore, this paper proposes an
end-to-end Streaming Video Temporal Action Segmentation with Reinforce Learning
(SVTAS-RL). The end-to-end SVTAS which regard TAS as an action segment
clustering task can expand the application scenarios of TAS; and RL is used to
alleviate the problem of inconsistent optimization objective and direction.
Through extensive experiments, the SVTAS-RL model achieves a competitive
performance to the state-of-the-art model of TAS on multiple datasets, and
shows greater advantages on the ultra-long video dataset EGTEA. This indicates
that our method can replace all current TAS models end-to-end and SVTAS-RL is
more suitable for long video TAS. Code is availabel at
https://github.com/Thinksky5124/SVTAS.
</p></li>
</ul>

<h3>Title: InfraParis: A multi-modal and multi-task autonomous driving dataset. (arXiv:2309.15751v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15751">http://arxiv.org/abs/2309.15751</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15751]] InfraParis: A multi-modal and multi-task autonomous driving dataset(http://arxiv.org/abs/2309.15751)</code></li>
<li>Summary: <p>Current deep neural networks (DNNs) for autonomous driving computer vision
are typically trained on specific datasets that only involve a single type of
data and urban scenes. Consequently, these models struggle to handle new
objects, noise, nighttime conditions, and diverse scenarios, which is essential
for safety-critical applications. Despite ongoing efforts to enhance the
resilience of computer vision DNNs, progress has been sluggish, partly due to
the absence of benchmarks featuring multiple modalities. We introduce a novel
and versatile dataset named InfraParis that supports multiple tasks across
three modalities: RGB, depth, and infrared. We assess various state-of-the-art
baseline techniques, encompassing models for the tasks of semantic
segmentation, object detection, and depth estimation.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
