<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection. (arXiv:2310.13573v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13573">http://arxiv.org/abs/2310.13573</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13573]] Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection(http://arxiv.org/abs/2310.13573)</code></li>
<li>Summary: <p>We introduce a high-performance fingerprint liveness feature extraction
technique that secured first place in LivDet 2023 Fingerprint Representation
Challenge. Additionally, we developed a practical fingerprint recognition
system with 94.68% accuracy, earning second place in LivDet 2023 Liveness
Detection in Action. By investigating various methods, particularly style
transfer, we demonstrate improvements in accuracy and generalization when faced
with limited training data. As a result, our approach achieved state-of-the-art
performance in LivDet 2023 Challenges.
</p></li>
</ul>

<h3>Title: Zero-Knowledge Proofs for Questionnaire Result Verification in Smart Contracts. (arXiv:2310.13618v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13618">http://arxiv.org/abs/2310.13618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13618]] Zero-Knowledge Proofs for Questionnaire Result Verification in Smart Contracts(http://arxiv.org/abs/2310.13618)</code></li>
<li>Summary: <p>We present an implementation of a Web3 platform that leverages the Groth16
Zero-Knowledge Proof schema to verify the validity of questionnaire results
within Smart Contracts. Our approach ensures that the answer key of the
questionnaire remains undisclosed throughout the verification process, while
ensuring that the evaluation is done fairly. To accomplish this, users respond
to a series of questions, and their answers are encoded and securely
transmitted to a hidden backend. The backend then performs an evaluation of the
user's answers, generating the overall result of the questionnaire.
Additionally, it generates a Zero-Knowledge Proof, attesting that the answers
were appropriately evaluated against a valid set of constraints. Next, the user
submits their result along with the proof to a Smart Contract, which verifies
their validity and issues a non-fungible token (NFT) as an attestation of the
user's test result. In this research, we implemented the Zero-Knowledge
functionality using Circom 2 and deployed the Smart Contract using Solidity,
thereby showcasing a practical and secure solution for questionnaire validity
verification in the context of Smart Contracts.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches. (arXiv:2310.13247v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13247">http://arxiv.org/abs/2310.13247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13247]] Anomaly Detection of Command Shell Sessions based on DistilBERT: Unsupervised and Supervised Approaches(http://arxiv.org/abs/2310.13247)</code></li>
<li>Summary: <p>Anomaly detection in command shell sessions is a critical aspect of computer
security. Recent advances in deep learning and natural language processing,
particularly transformer-based models, have shown great promise for addressing
complex security challenges. In this paper, we implement a comprehensive
approach to detect anomalies in Unix shell sessions using a pretrained
DistilBERT model, leveraging both unsupervised and supervised learning
techniques to identify anomalous activity while minimizing data labeling. The
unsupervised method captures the underlying structure and syntax of Unix shell
commands, enabling the detection of session deviations from normal behavior.
Experiments on a large-scale enterprise dataset collected from production
systems demonstrate the effectiveness of our approach in detecting anomalous
behavior in Unix shell sessions. This work highlights the potential of
leveraging recent advances in transformers to address important computer
security challenges.
</p></li>
</ul>

<h3>Title: Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models. (arXiv:2310.13315v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13315">http://arxiv.org/abs/2310.13315</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13315]] Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models(http://arxiv.org/abs/2310.13315)</code></li>
<li>Summary: <p>Quantization is a promising approach for reducing memory overhead and
accelerating inference, especially in large pre-trained language model (PLM)
scenarios. While having no access to original training data due to security and
privacy concerns has emerged the demand for zero-shot quantization. Most of the
cutting-edge zero-shot quantization methods primarily 1) apply to computer
vision tasks, and 2) neglect of overfitting problem in the generative
adversarial learning process, leading to sub-optimal performance. Motivated by
this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ)
framework for the zero-shot quantization of various PLMs. The key algorithm in
solving ZSAQ is the SAM-SGA optimization, which aims to improve the
quantization accuracy and model generalization via optimizing a minimax
problem. We theoretically prove the convergence rate for the minimax
optimization problem and this result can be applied to other nonconvex-PL
minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate
that our method brings consistent and significant performance gains on both
discriminative and generative PLMs, i.e., up to +6.98 average score.
Furthermore, we empirically validate that our method can effectively improve
the model generalization.
</p></li>
</ul>

<h3>Title: Reward Shaping for Happier Autonomous Cyber Security Agents. (arXiv:2310.13565v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13565">http://arxiv.org/abs/2310.13565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13565]] Reward Shaping for Happier Autonomous Cyber Security Agents(http://arxiv.org/abs/2310.13565)</code></li>
<li>Summary: <p>As machine learning models become more capable, they have exhibited increased
potential in solving complex tasks. One of the most promising directions uses
deep reinforcement learning to train autonomous agents in computer network
defense tasks. This work studies the impact of the reward signal that is
provided to the agents when training for this task. Due to the nature of
cybersecurity tasks, the reward signal is typically 1) in the form of penalties
(e.g., when a compromise occurs), and 2) distributed sparsely across each
defense episode. Such reward characteristics are atypical of classic
reinforcement learning tasks where the agent is regularly rewarded for progress
(cf. to getting occasionally penalized for failures). We investigate reward
shaping techniques that could bridge this gap so as to enable agents to train
more sample-efficiently and potentially converge to a better performance. We
first show that deep reinforcement learning algorithms are sensitive to the
magnitude of the penalties and their relative size. Then, we combine penalties
with positive external rewards and study their effect compared to penalty-only
training. Finally, we evaluate intrinsic curiosity as an internal positive
reward mechanism and discuss why it might not be as advantageous for high-level
network monitoring tasks.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks. (arXiv:2310.13291v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13291">http://arxiv.org/abs/2310.13291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13291]] Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks(http://arxiv.org/abs/2310.13291)</code></li>
<li>Summary: <p>Large language models have revolutionized the field of NLP by achieving
state-of-the-art performance on various tasks. However, there is a concern that
these models may disclose information in the training data. In this study, we
focus on the summarization task and investigate the membership inference (MI)
attack: given a sample and black-box access to a model's API, it is possible to
determine if the sample was part of the training data. We exploit text
similarity and the model's resistance to document modifications as potential MI
signals and evaluate their effectiveness on widely used datasets. Our results
demonstrate that summarization models are at risk of exposing data membership,
even in cases where the reference summary is not available. Furthermore, we
discuss several safeguards for training summarization models to protect against
MI attacks and discuss the inherent trade-off between privacy and utility.
</p></li>
</ul>

<h3>Title: Mean Estimation Under Heterogeneous Privacy Demands. (arXiv:2310.13137v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13137">http://arxiv.org/abs/2310.13137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13137]] Mean Estimation Under Heterogeneous Privacy Demands(http://arxiv.org/abs/2310.13137)</code></li>
<li>Summary: <p>Differential Privacy (DP) is a well-established framework to quantify privacy
loss incurred by any algorithm. Traditional formulations impose a uniform
privacy requirement for all users, which is often inconsistent with real-world
scenarios in which users dictate their privacy preferences individually. This
work considers the problem of mean estimation, where each user can impose their
own distinct privacy level. The algorithm we propose is shown to be minimax
optimal and has a near-linear run-time. Our results elicit an interesting
saturation phenomenon that occurs. Namely, the privacy requirements of the most
stringent users dictate the overall error rates. As a consequence, users with
less but differing privacy requirements are all given more privacy than they
require, in equal amounts. In other words, these privacy-indifferent users are
given a nontrivial degree of privacy for free, without any sacrifice in the
performance of the estimator.
</p></li>
</ul>

<h3>Title: Privacy Preserving Decision Tree Training and Prediction via Fully Homomorphic Encryption with No Decryption. (arXiv:2310.13140v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13140">http://arxiv.org/abs/2310.13140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13140]] Privacy Preserving Decision Tree Training and Prediction via Fully Homomorphic Encryption with No Decryption(http://arxiv.org/abs/2310.13140)</code></li>
<li>Summary: <p>With data-outsourcing becoming commonplace, there grows a need for secure
outsourcing of data and machine learning models. Namely, data and model owners
(client) often have a need for their information to remain private and secure
against the potentially untrusted computing resource (server) to whom they want
to outsource said data and models to. Various approaches to privacy-preserving
machine learning (PPML) have been devised with different techniques and
solutions introduced in the past. These solutions often involved one of two
compromises: (1) client-server interactions to allow intermediary rounds of
decryption and re-encryption of data or (2) complex architectures for
multi-party computation. This paper devises a paradigm using Fully Homomorphic
Encryption (FHE) that minimizes architectural complexity and removes
client-side involvement during the training and prediction lifecycle of machine
learning models. In addition, the paradigm proposed in this work achieves both
model security as well as data security. To remove client-side involvement, the
devised paradigm proposes a no decryption approach that allows the server to
handle PPML in its entirety without rounds of decryption and re-encryption. To
the best of our knowledge, this paradigm is the first to achieve
privacy-preserving decision tree training with no decryption while maintaining
a simple client-server architecture.
</p></li>
</ul>

<h3>Title: Watch Nearby! Privacy Analysis of the People Nearby Service of Telegram. (arXiv:2310.13528v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13528">http://arxiv.org/abs/2310.13528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13528]] Watch Nearby! Privacy Analysis of the People Nearby Service of Telegram(http://arxiv.org/abs/2310.13528)</code></li>
<li>Summary: <p>People Nearby is a service offered by Telegram that allows a user to discover
other Telegram users, based only on geographical proximity. Nearby users are
reported with a rough estimate of their distance from the position of the
reference user, allowing Telegram to claim location privacy In this paper, we
systematically analyze the location privacy provided by Telegram to users of
the People Nearby service. Through an extensive measurement campaign run by
spoofing the user's location all over the world, we reverse-engineer the
algorithm adopted by People Nearby to compute distances between users. Although
the service protects against precise user localization, we demonstrate that
location privacy is always lower than the one declared by Telegram of 500
meters. Specifically, we discover that location privacy is a function of the
geographical position of the user. Indeed, the radius of the location privacy
area (localization error) spans between 400 meters (close to the equator) and
128 meters (close to the poles), with a difference of up to 75% (worst case)
compared to what Telegram declares. After our responsible disclosure, Telegram
updated the FAQ associated with the service. Finally, we provide some solutions
and countermeasures that Telegram can implement to improve location privacy. In
general, the reported findings highlight the significant privacy risks
associated with using People Nearby service.
</p></li>
</ul>

<h3>Title: Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing. (arXiv:2310.13384v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13384">http://arxiv.org/abs/2310.13384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13384]] Salted Inference: Enhancing Privacy while Maintaining Efficiency of Split Inference in Mobile Computing(http://arxiv.org/abs/2310.13384)</code></li>
<li>Summary: <p>Split inference partitions a deep neural network (DNN) to run the early part
at the edge and the later part in the cloud. This meets two key requirements
for on-device machine learning: input privacy and compute efficiency. Still, an
open question in split inference is output privacy, given that the output of a
DNN is visible to the cloud. While encrypted computing can protect output
privacy, it mandates extensive computation and communication resources. In this
paper, we introduce "Salted DNNs": a novel method that lets clients control the
semantic interpretation of DNN output at inference time while maintaining
accuracy and efficiency very close to that of a standard DNN. Experimental
evaluations conducted on both image and sensor data show that Salted DNNs
achieve classification accuracy very close to standard DNNs, particularly when
the salted layer is positioned within the early part to meet the requirements
of split inference. Our method is general and can be applied to various DNNs.
We open-source our code and results, as a benchmark for future studies.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: PatchCURE: Improving Certifiable Robustness, Model Utility, and Computation Efficiency of Adversarial Patch Defenses. (arXiv:2310.13076v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13076">http://arxiv.org/abs/2310.13076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13076]] PatchCURE: Improving Certifiable Robustness, Model Utility, and Computation Efficiency of Adversarial Patch Defenses(http://arxiv.org/abs/2310.13076)</code></li>
<li>Summary: <p>State-of-the-art defenses against adversarial patch attacks can now achieve
strong certifiable robustness with a marginal drop in model utility. However,
this impressive performance typically comes at the cost of 10-100x more
inference-time computation compared to undefended models -- the research
community has witnessed an intense three-way trade-off between certifiable
robustness, model utility, and computation efficiency. In this paper, we
propose a defense framework named PatchCURE to approach this trade-off problem.
PatchCURE provides sufficient "knobs" for tuning defense performance and allows
us to build a family of defenses: the most robust PatchCURE instance can match
the performance of any existing state-of-the-art defense (without efficiency
considerations); the most efficient PatchCURE instance has similar inference
efficiency as undefended models. Notably, PatchCURE achieves state-of-the-art
robustness and utility performance across all different efficiency levels,
e.g., 16-23% absolute clean accuracy and certified robust accuracy advantages
over prior defenses when requiring computation efficiency to be close to
undefended models. The family of PatchCURE defenses enables us to flexibly
choose appropriate defenses to satisfy given computation and/or utility
constraints in practice.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13019">http://arxiv.org/abs/2310.13019</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13019]] Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm(http://arxiv.org/abs/2310.13019)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have significantly advanced various domains, but
their vulnerability to adversarial attacks poses serious concerns.
Understanding these vulnerabilities and developing effective defense mechanisms
is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016),
finds minimal perturbations to misclassify input images. However, DeepFool
lacks a targeted approach, making it less effective in specific attack
scenarios. Also, in previous related works, researchers primarily focus on
success, not considering how much an image is getting distorted; the integrity
of the image quality, and the confidence level to misclassifying. So, in this
paper, we propose Targeted DeepFool, an augmented version of DeepFool that
allows targeting specific classes for misclassification. We also introduce a
minimum confidence score requirement hyperparameter to enhance flexibility. Our
experiments demonstrate the effectiveness and efficiency of the proposed method
across different deep neural network architectures while preserving image
integrity as much as possible. Results show that one of the deep convolutional
neural network architectures, AlexNet, and one of the state-of-the-art model
Vision Transformer exhibit high robustness to getting fooled. Our code will be
made public when publishing the paper.
</p></li>
</ul>

<h3>Title: No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network. (arXiv:2310.13099v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13099">http://arxiv.org/abs/2310.13099</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13099]] No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network(http://arxiv.org/abs/2310.13099)</code></li>
<li>Summary: <p>We introduce a simple yet efficient sentence-level attack on black-box
toxicity detector models. By adding several positive words or sentences to the
end of a hateful message, we are able to change the prediction of a neural
network and pass the toxicity detection system check. This approach is shown to
be working on seven languages from three different language families. We also
describe the defence mechanism against the aforementioned attack and discuss
its limitations.
</p></li>
</ul>

<h3>Title: Critical Path Prioritization Dashboard for Alert-driven Attack Graphs. (arXiv:2310.13079v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13079">http://arxiv.org/abs/2310.13079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13079]] Critical Path Prioritization Dashboard for Alert-driven Attack Graphs(http://arxiv.org/abs/2310.13079)</code></li>
<li>Summary: <p>Although intrusion alerts can provide threat intelligence regarding attacker
strategies, extracting such intelligence via existing tools is expensive and
time-consuming. Earlier work has proposed SAGE, which generates attack graphs
from intrusion alerts using unsupervised sequential machine learning. This
paper proposes a querying and prioritization-enabled visual analytics dashboard
for SAGE. The dashboard has three main components: (i) a Graph Explorer that
presents a global view of all attacker strategies, (ii) a Timeline Viewer that
correlates attacker actions chronologically, and (iii) a Recommender Matrix
that highlights prevalent critical alerts via a MITRE ATT&amp;CK-inspired attack
stage matrix. We describe the utility of the proposed dashboard using intrusion
alerts collected from a distributed multi-stage team-based attack scenario. We
evaluate the utility of the dashboard through a user study. Based on the
responses of a small set of security practitioners, we find that the dashboard
is useful in depicting attacker strategies and attack progression, but can be
improved in terms of usability.
</p></li>
</ul>

<h3>Title: Adaptive Experimental Design for Intrusion Data Collection. (arXiv:2310.13224v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13224">http://arxiv.org/abs/2310.13224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13224]] Adaptive Experimental Design for Intrusion Data Collection(http://arxiv.org/abs/2310.13224)</code></li>
<li>Summary: <p>Intrusion research frequently collects data on attack techniques currently
employed and their potential symptoms. This includes deploying honeypots,
logging events from existing devices, employing a red team for a sample attack
campaign, or simulating system activity. However, these observational studies
do not clearly discern the cause-and-effect relationships between the design of
the environment and the data recorded. Neglecting such relationships increases
the chance of drawing biased conclusions due to unconsidered factors, such as
spurious correlations between features and errors in measurement or
classification. In this paper, we present the theory and empirical data on
methods that aim to discover such causal relationships efficiently. Our
adaptive design (AD) is inspired by the clinical trial community: a variant of
a randomized control trial (RCT) to measure how a particular ``treatment''
affects a population. To contrast our method with observational studies and
RCT, we run the first controlled and adaptive honeypot deployment study,
identifying the causal relationship between an ssh vulnerability and the rate
of server exploitation. We demonstrate that our AD method decreases the total
time needed to run the deployment by at least 33%, while still confidently
stating the impact of our change in the environment. Compared to an analogous
honeypot study with a control group, our AD requests 17% fewer honeypots while
collecting 19% more attack recordings than an analogous honeypot study with a
control group.
</p></li>
</ul>

<h3>Title: An LLM can Fool Itself: A Prompt-Based Adversarial Attack. (arXiv:2310.13345v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13345">http://arxiv.org/abs/2310.13345</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13345]] An LLM can Fool Itself: A Prompt-Based Adversarial Attack(http://arxiv.org/abs/2310.13345)</code></li>
<li>Summary: <p>The wide-ranging applications of large language models (LLMs), especially in
safety-critical domains, necessitate the proper evaluation of the LLM's
adversarial robustness. This paper proposes an efficient tool to audit the
LLM's adversarial robustness via a prompt-based adversarial attack
(PromptAttack). PromptAttack converts adversarial textual attacks into an
attack prompt that can cause the victim LLM to output the adversarial sample to
fool itself. The attack prompt is composed of three important components: (1)
original input (OI) including the original sample and its ground-truth label,
(2) attack objective (AO) illustrating a task description of generating a new
sample that can fool itself without changing the semantic meaning, and (3)
attack guidance (AG) containing the perturbation instructions to guide the LLM
on how to complete the task by perturbing the original sample at character,
word, and sentence levels, respectively. Besides, we use a fidelity filter to
ensure that PromptAttack maintains the original semantic meanings of the
adversarial examples. Further, we enhance the attack power of PromptAttack by
ensembling adversarial examples at different perturbation levels. Comprehensive
empirical results using Llama2 and GPT-3.5 validate that PromptAttack
consistently yields a much higher attack success rate compared to AdvGLUE and
AdvGLUE++. Interesting findings include that a simple emoji can easily mislead
GPT-3.5 to make wrong predictions.
</p></li>
</ul>

<h3>Title: On the Effect of Clock Frequency on Voltage and Electromagnetic Fault Injection. (arXiv:2310.13389v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13389">http://arxiv.org/abs/2310.13389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13389]] On the Effect of Clock Frequency on Voltage and Electromagnetic Fault Injection(http://arxiv.org/abs/2310.13389)</code></li>
<li>Summary: <p>We investigate the influence of clock frequency on the success rate of a
fault injection attack. In particular, we examine the success rate of voltage
and electromagnetic fault attacks for varying clock frequencies. Using three
different tests that cover different components of a System-on-Chip, we perform
fault injection while its CPU operates at different clock frequencies. Our
results show that the attack's success rate increases with an increase in clock
frequency for both voltage and EM fault injection attacks. As the technology
advances push the clock frequency further, these results can help assess the
impact of fault injection attacks more accurately and develop appropriate
countermeasures to address them.
</p></li>
</ul>

<h3>Title: FLTracer: Accurate Poisoning Attack Provenance in Federated Learning. (arXiv:2310.13424v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13424">http://arxiv.org/abs/2310.13424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13424]] FLTracer: Accurate Poisoning Attack Provenance in Federated Learning(http://arxiv.org/abs/2310.13424)</code></li>
<li>Summary: <p>Federated Learning (FL) is a promising distributed learning approach that
enables multiple clients to collaboratively train a shared global model.
However, recent studies show that FL is vulnerable to various poisoning
attacks, which can degrade the performance of global models or introduce
backdoors into them. In this paper, we first conduct a comprehensive study on
prior FL attacks and detection methods. The results show that all existing
detection methods are only effective against limited and specific attacks. Most
detection methods suffer from high false positives, which lead to significant
performance degradation, especially in not independent and identically
distributed (non-IID) settings. To address these issues, we propose FLTracer,
the first FL attack provenance framework to accurately detect various attacks
and trace the attack time, objective, type, and poisoned location of updates.
Different from existing methodologies that rely solely on cross-client anomaly
detection, we propose a Kalman filter-based cross-round detection to identify
adversaries by seeking the behavior changes before and after the attack. Thus,
this makes it resilient to data heterogeneity and is effective even in non-IID
settings. To further improve the accuracy of our detection method, we employ
four novel features and capture their anomalies with the joint decisions.
Extensive evaluations show that FLTracer achieves an average true positive rate
of over $96.88\%$ at an average false positive rate of less than $2.67\%$,
significantly outperforming SOTA detection methods. \footnote{Code is available
at \url{https://github.com/Eyr3/FLTracer}.}
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Robust multimodal models have outlier features and encode more concepts. (arXiv:2310.13040v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13040">http://arxiv.org/abs/2310.13040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13040]] Robust multimodal models have outlier features and encode more concepts(http://arxiv.org/abs/2310.13040)</code></li>
<li>Summary: <p>What distinguishes robust models from non-robust ones? This question has
gained traction with the appearance of large-scale multimodal models, such as
CLIP. These models have demonstrated unprecedented robustness with respect to
natural distribution shifts. While it has been shown that such differences in
robustness can be traced back to differences in training data, so far it is not
known what that translates to in terms of what the model has learned. In this
work, we bridge this gap by probing the representation spaces of 12 robust
multimodal models with various backbones (ResNets and ViTs) and pretraining
sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two
signatures of robustness in the representation spaces of these models: (1)
Robust models exhibit outlier features characterized by their activations, with
some being several orders of magnitude above average. These outlier features
induce privileged directions in the model's representation space. We
demonstrate that these privileged directions explain most of the predictive
power of the model by pruning up to $80 \%$ of the least important
representation space directions without negative impacts on model accuracy and
robustness; (2) Robust models encode substantially more concepts in their
representation space. While this superposition of concepts allows robust models
to store much information, it also results in highly polysemantic features,
which makes their interpretation challenging. We discuss how these insights
pave the way for future research in various fields, such as model pruning and
mechanistic interpretability.
</p></li>
</ul>

<h3>Title: Zone Evaluation: Revealing Spatial Bias in Object Detection. (arXiv:2310.13215v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13215">http://arxiv.org/abs/2310.13215</a></li>
<li>Code URL: https://github.com/zzh-tju/zoneeval</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13215]] Zone Evaluation: Revealing Spatial Bias in Object Detection(http://arxiv.org/abs/2310.13215)</code></li>
<li>Summary: <p>A fundamental limitation of object detectors is that they suffer from
"spatial bias", and in particular perform less satisfactorily when detecting
objects near image borders. For a long time, there has been a lack of effective
ways to measure and identify spatial bias, and little is known about where it
comes from and what degree it is. To this end, we present a new zone evaluation
protocol, extending from the traditional evaluation to a more generalized one,
which measures the detection performance over zones, yielding a series of Zone
Precisions (ZPs). For the first time, we provide numerical results, showing
that the object detectors perform quite unevenly across the zones.
Surprisingly, the detector's performance in the 96\% border zone of the image
does not reach the AP value (Average Precision, commonly regarded as the
average detection performance in the entire image zone). To better understand
spatial bias, a series of heuristic experiments are conducted. Our
investigation excludes two intuitive conjectures about spatial bias that the
object scale and the absolute positions of objects barely influence the spatial
bias. We find that the key lies in the human-imperceptible divergence in data
patterns between objects in different zones, thus eventually forming a visible
performance gap between the zones. With these findings, we finally discuss a
future direction for object detection, namely, spatial disequilibrium problem,
aiming at pursuing a balanced detection ability over the entire image zone. By
broadly evaluating 10 popular object detectors and 5 detection datasets, we
shed light on the spatial bias of object detectors. We hope this work could
raise a focus on detection robustness. The source codes, evaluation protocols,
and tutorials are publicly available at
\url{https://github.com/Zzh-tju/ZoneEval}.
</p></li>
</ul>

<h3>Title: InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution. (arXiv:2310.13276v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13276">http://arxiv.org/abs/2310.13276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13276]] InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution(http://arxiv.org/abs/2310.13276)</code></li>
<li>Summary: <p>Over recent decades, significant advancements in cross-modal retrieval are
mainly driven by breakthroughs in visual and linguistic modeling. However, a
recent study shows that multi-modal data representations tend to cluster within
a limited convex cone (as representation degeneration problem), which hinders
retrieval performance due to the inseparability of these representations. In
our study, we first empirically validate the presence of the representation
degeneration problem across multiple cross-modal benchmarks and methods. Next,
to address it, we introduce a novel method, called InvGC, a post-processing
technique inspired by graph convolution and average pooling. Specifically,
InvGC defines the graph topology within the datasets and then applies graph
convolution in a subtractive manner. This method effectively separates
representations by increasing the distances between data points. To improve the
efficiency and effectiveness of InvGC, we propose an advanced graph topology,
LocalAdj, which only aims to increase the distances between each data point and
its nearest neighbors. To understand why InvGC works, we present a detailed
theoretical analysis, proving that the lower bound of recall will be improved
after deploying InvGC. Extensive empirical results show that InvGC and InvGC
w/LocalAdj significantly mitigate the representation degeneration problem,
thereby enhancing retrieval performance.
</p>
<p>Our code is available at
https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval
</p></li>
</ul>

<h3>Title: RaceLens: A Machine Intelligence-Based Application for Racing Photo Analysis. (arXiv:2310.13515v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13515">http://arxiv.org/abs/2310.13515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13515]] RaceLens: A Machine Intelligence-Based Application for Racing Photo Analysis(http://arxiv.org/abs/2310.13515)</code></li>
<li>Summary: <p>This paper presents RaceLens, a novel application utilizing advanced deep
learning and computer vision models for comprehensive analysis of racing
photos. The developed models have demonstrated their efficiency in a wide array
of tasks, including detecting racing cars, recognizing car numbers, detecting
and quantifying car details, and recognizing car orientations. We discuss the
process of collecting a robust dataset necessary for training our models, and
describe an approach we have designed to augment and improve this dataset
continually. Our method leverages a feedback loop for continuous model
improvement, thus enhancing the performance and accuracy of RaceLens over time.
A significant part of our study is dedicated to illustrating the practical
application of RaceLens, focusing on its successful deployment by NASCAR teams
over four seasons. We provide a comprehensive evaluation of our system's
performance and its direct impact on the team's strategic decisions and
performance metrics. The results underscore the transformative potential of
machine intelligence in the competitive and dynamic world of car racing,
setting a precedent for future applications.
</p></li>
</ul>

<h3>Title: Longer-range Contextualized Masked Autoencoder. (arXiv:2310.13593v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13593">http://arxiv.org/abs/2310.13593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13593]] Longer-range Contextualized Masked Autoencoder(http://arxiv.org/abs/2310.13593)</code></li>
<li>Summary: <p>Masked image modeling (MIM) has emerged as a promising self-supervised
learning (SSL) strategy. The MIM pre-training facilitates learning powerful
representations using an encoder-decoder framework by randomly masking some
input pixels and reconstructing the masked pixels from the remaining ones.
However, as the encoder is trained with partial pixels, the MIM pre-training
can suffer from a low capability of understanding long-range dependency. This
limitation may hinder its capability to fully understand multiple-range
dependencies, resulting in narrow highlighted regions in the attention map that
may incur accuracy drops. To mitigate the limitation, We propose a
self-supervised learning framework, named Longer-range Contextualized Masked
Autoencoder (LC-MAE). LC-MAE effectively leverages a global context
understanding of visual representations while simultaneously reducing the
spatial redundancy of input at the same time. Our method steers the encoder to
learn from entire pixels in multiple views while also learning local
representation from sparse pixels. As a result, LC-MAE learns more
discriminative representations, leading to a performance improvement of
achieving 84.2% top-1 accuracy with ViT-B on ImageNet-1K with 0.6%p gain. We
attribute the success to the enhanced pre-training method, as evidenced by the
singular value spectrum and attention analyses. Finally, LC-MAE achieves
significant performance gains at the downstream semantic segmentation and
fine-grained visual classification tasks; and on diverse robust evaluation
metrics. Our code will be publicly available.
</p></li>
</ul>

<h3>Title: What you see is what you get: Experience ranking with deep neural dataset-to-dataset similarity for topological localisation. (arXiv:2310.13622v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13622">http://arxiv.org/abs/2310.13622</a></li>
<li>Code URL: https://github.com/mttgdd/vdna-experience-selection</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13622]] What you see is what you get: Experience ranking with deep neural dataset-to-dataset similarity for topological localisation(http://arxiv.org/abs/2310.13622)</code></li>
<li>Summary: <p>Recalling the most relevant visual memories for localisation or understanding
a priori the likely outcome of localisation effort against a particular visual
memory is useful for efficient and robust visual navigation. Solutions to this
problem should be divorced from performance appraisal against ground truth - as
this is not available at run-time - and should ideally be based on
generalisable environmental observations. For this, we propose applying the
recently developed Visual DNA as a highly scalable tool for comparing datasets
of images - in this work, sequences of map and live experiences. In the case of
localisation, important dataset differences impacting performance are modes of
appearance change, including weather, lighting, and season. Specifically, for
any deep architecture which is used for place recognition by matching feature
volumes at a particular layer, we use distribution measures to compare
neuron-wise activation statistics between live images and multiple previously
recorded past experiences, with a potentially large seasonal (winter/summer) or
time of day (day/night) shift. We find that differences in these statistics
correlate to performance when localising using a past experience with the same
appearance gap. We validate our approach over the Nordland cross-season dataset
as well as data from Oxford's University Parks with lighting and mild seasonal
change, showing excellent ability of our system to rank actual localisation
performance across candidate experiences.
</p></li>
</ul>

<h3>Title: Compositional preference models for aligning LMs. (arXiv:2310.13011v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13011">http://arxiv.org/abs/2310.13011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13011]] Compositional preference models for aligning LMs(http://arxiv.org/abs/2310.13011)</code></li>
<li>Summary: <p>As language models (LMs) become more capable, it is increasingly important to
align them with human preferences. However, the dominant paradigm for training
Preference Models (PMs) for that purpose suffers from fundamental limitations,
such as lack of transparency and scalability, along with susceptibility to
overfitting the preference dataset. We propose Compositional Preference Models
(CPMs), a novel PM framework that decomposes one global preference assessment
into several interpretable features, obtains scalar scores for these features
from a prompted LM, and aggregates these scores using a logistic regression
classifier. CPMs allow to control which properties of the preference data are
used to train the preference model and to build it based on features that are
believed to underlie the human preference judgment. Our experiments show that
CPMs not only improve generalization and are more robust to overoptimization
than standard PMs, but also that best-of-n samples obtained using CPMs tend to
be preferred over samples obtained using conventional PMs. Overall, our
approach demonstrates the benefits of endowing PMs with priors about which
features determine human preferences while relying on LM capabilities to
extract those features in a scalable and robust way.
</p></li>
</ul>

<h3>Title: Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding. (arXiv:2310.13022v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13022">http://arxiv.org/abs/2310.13022</a></li>
<li>Code URL: https://github.com/wjn1996/upet</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13022]] Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding(http://arxiv.org/abs/2310.13022)</code></li>
<li>Summary: <p>The recent success of large pre-trained language models (PLMs) heavily hinges
on massive labeled data, which typically produces inferior performance in
low-resource scenarios. To remedy this dilemma, we study self-training as one
of the predominant semi-supervised learning (SSL) approaches, which utilizes
large-scale unlabeled data to generate synthetic examples. However, too many
noisy labels will hurt the model performance, and the self-training procedure
requires multiple training iterations making it more expensive if all the model
parameters of the PLM are updated. This paper presents UPET, a novel
Uncertainty-aware Parameter-Efficient self-Training framework to effectively
and efficiently address the labeled data scarcity issue. Specifically, we
incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to
perform uncertainty estimation for the teacher model and then judiciously
select reliable pseudo-labeled examples based on confidence and certainty.
During the student training, we introduce multiple parameter-efficient learning
(PEL) paradigms that allow the optimization of only a small percentage of
parameters. We also propose a novel Easy-Hard Contrastive Tuning to enhance the
robustness and generalization. Extensive experiments over multiple downstream
tasks demonstrate that UPET achieves a substantial improvement in terms of
performance and efficiency. Our codes and data are released at https:
//github.com/wjn1996/UPET.
</p></li>
</ul>

<h3>Title: CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain. (arXiv:2310.13146v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13146">http://arxiv.org/abs/2310.13146</a></li>
<li>Code URL: https://github.com/openlifescience-ai/clift</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13146]] CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain(http://arxiv.org/abs/2310.13146)</code></li>
<li>Summary: <p>This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical
domain Question-answering task. The testbed includes 7.5k high-quality question
answering samples to provide a diverse and reliable benchmark. We performed a
comprehensive experimental study and evaluated several QA deep-learning models
under the proposed testbed. Despite impressive results on the original test
set, the performance degrades when applied to new test sets, which shows the
distribution shift. Our findings emphasize the need for and the potential for
increasing the robustness of clinical domain models under distributional
shifts. The testbed offers one way to track progress in that direction. It also
highlights the necessity of adopting evaluation metrics that consider
robustness to natural distribution shifts. We plan to expand the corpus by
adding more samples and model results. The full paper and the updated benchmark
are available at github.com/openlifescience-ai/clift
</p></li>
</ul>

<h3>Title: Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13191">http://arxiv.org/abs/2310.13191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13191]] Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models(http://arxiv.org/abs/2310.13191)</code></li>
<li>Summary: <p>The pruning objective has recently extended beyond accuracy and sparsity to
robustness in language models. Despite this, existing methods struggle to
enhance robustness against adversarial attacks when continually increasing
model sparsity and require a retraining process. As humans step into the era of
large language models, these issues become increasingly prominent. This paper
proposes that the robustness of language models is proportional to the extent
of pre-trained knowledge they encompass. Accordingly, we introduce a
post-training pruning strategy designed to faithfully replicate the embedding
space and feature space of dense language models, aiming to conserve more
pre-trained knowledge during the pruning process. In this setup, each layer's
reconstruction error not only originates from itself but also includes
cumulative error from preceding layers, followed by an adaptive rectification.
Compared to other state-of-art baselines, our approach demonstrates a superior
balance between accuracy, sparsity, robustness, and pruning cost with BERT on
datasets SST2, IMDB, and AGNews, marking a significant stride towards robust
pruning in language models.
</p></li>
</ul>

<h3>Title: Test-Time Self-Adaptive Small Language Models for Question Answering. (arXiv:2310.13307v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13307">http://arxiv.org/abs/2310.13307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13307]] Test-Time Self-Adaptive Small Language Models for Question Answering(http://arxiv.org/abs/2310.13307)</code></li>
<li>Summary: <p>Recent instruction-finetuned large language models (LMs) have achieved
notable performances in various tasks, such as question-answering (QA).
However, despite their ability to memorize a vast amount of general knowledge
across diverse tasks, they might be suboptimal on specific tasks due to their
limited capacity to transfer and adapt knowledge to target tasks. Moreover,
further finetuning LMs with labeled datasets is often infeasible due to their
absence, but it is also questionable if we can transfer smaller LMs having
limited knowledge only with unlabeled test data. In this work, we show and
investigate the capabilities of smaller self-adaptive LMs, only with unlabeled
test data. In particular, we first stochastically generate multiple answers,
and then ensemble them while filtering out low-quality samples to mitigate
noise from inaccurate labels. Our proposed self-adaption strategy demonstrates
significant performance improvements on benchmark QA datasets with higher
robustness across diverse prompts, enabling LMs to stay stable. Code is
available at: https://github.com/starsuzi/T-SAS.
</p></li>
</ul>

<h3>Title: Beyond Hard Samples: Robust and Effective Grammatical Error Correction with Cycle Self-Augmenting. (arXiv:2310.13321v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13321">http://arxiv.org/abs/2310.13321</a></li>
<li>Code URL: https://github.com/zetangforward/csa-gec</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13321]] Beyond Hard Samples: Robust and Effective Grammatical Error Correction with Cycle Self-Augmenting(http://arxiv.org/abs/2310.13321)</code></li>
<li>Summary: <p>Recent studies have revealed that grammatical error correction methods in the
sequence-to-sequence paradigm are vulnerable to adversarial attack, and simply
utilizing adversarial examples in the pre-training or post-training process can
significantly enhance the robustness of GEC models to certain types of attack
without suffering too much performance loss on clean data. In this paper, we
further conduct a thorough robustness evaluation of cutting-edge GEC methods
for four different types of adversarial attacks and propose a simple yet very
effective Cycle Self-Augmenting (CSA) method accordingly. By leveraging the
augmenting data from the GEC models themselves in the post-training process and
introducing regularization data for cycle training, our proposed method can
effectively improve the model robustness of well-trained GEC models with only a
few more training epochs as an extra cost. More concretely, further training on
the regularization data can prevent the GEC models from over-fitting on
easy-to-learn samples and thus can improve the generalization capability and
robustness towards unseen data (adversarial noise/samples). Meanwhile, the
self-augmented data can provide more high-quality pseudo pairs to improve model
performance on the original testing data. Experiments on four benchmark
datasets and seven strong models indicate that our proposed training method can
significantly enhance the robustness of four types of attacks without using
purposely built adversarial examples in training. Evaluation results on clean
data further confirm that our proposed CSA method significantly improves the
performance of four baselines and yields nearly comparable results with other
state-of-the-art models. Our code is available at
https://github.com/ZetangForward/CSA-GEC.
</p></li>
</ul>

<h3>Title: Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13469">http://arxiv.org/abs/2310.13469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13469]] Ask Language Model to Clean Your Noisy Translation Data(http://arxiv.org/abs/2310.13469)</code></li>
<li>Summary: <p>Transformer models have demonstrated remarkable performance in neural machine
translation (NMT). However, their vulnerability to noisy input poses a
significant challenge in practical implementation, where generating clean
output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used
as a benchmark for evaluating the robustness of NMT models against noisy input.
Nevertheless, its utility is limited due to the presence of noise in both the
source and target sentences. To address this limitation, we focus on cleaning
the noise from the target sentences in MTNT, making it more suitable as a
benchmark for noise evaluation. Leveraging the capabilities of large language
models (LLMs), we observe their impressive abilities in noise removal. For
example, they can remove emojis while considering their semantic meaning.
Additionally, we show that LLM can effectively rephrase slang, jargon, and
profanities. The resulting datasets, called C-MTNT, exhibit significantly less
noise in the target sentences while preserving the semantic integrity of the
original sentences. Our human and GPT-4 evaluations also lead to a consistent
conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT
showcased its effectiveness in evaluating the robustness of NMT models,
highlighting the potential of advanced language models for data cleaning and
emphasizing C-MTNT as a valuable resource.
</p></li>
</ul>

<h3>Title: Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning. (arXiv:2310.13486v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13486">http://arxiv.org/abs/2310.13486</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13486]] Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning(http://arxiv.org/abs/2310.13486)</code></li>
<li>Summary: <p>Finding the best way of adapting pre-trained language models to a task is a
big challenge in current NLP. Just like the previous generation of task-tuned
models (TT), models that are adapted to tasks via in-context-learning (ICL) are
robust in some setups but not in others. Here, we present a detailed analysis
of which design choices cause instabilities and inconsistencies in LLM
predictions. First, we show how spurious correlations between input
distributions and labels -- a known issue in TT models -- form only a minor
problem for prompted models. Then, we engage in a systematic, holistic
evaluation of different factors that have been found to influence predictions
in a prompting setup. We test all possible combinations of a range of factors
on both vanilla and instruction-tuned (IT) LLMs of different scale and
statistically analyse the results to show which factors are the most
influential, interactive or stable. Our results show which factors can be used
without precautions and which should be avoided or handled with care in most
settings.
</p></li>
</ul>

<h3>Title: Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13505">http://arxiv.org/abs/2310.13505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13505]] Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation(http://arxiv.org/abs/2310.13505)</code></li>
<li>Summary: <p>Models for conversational question answering (ConvQA) over knowledge graphs
(KGs) are usually trained and tested on benchmarks of gold QA pairs. This
implies that training is limited to surface forms seen in the respective
datasets, and evaluation is on a small set of held-out questions. Through our
proposed framework REIGN, we take several steps to remedy this restricted
learning setup. First, we systematically generate reformulations of training
questions to increase robustness of models to surface form variations. This is
a particularly challenging problem, given the incomplete nature of such
questions. Second, we guide ConvQA models towards higher performance by feeding
it only those reformulations that help improve their answering quality, using
deep reinforcement learning. Third, we demonstrate the viability of training
major model components on one benchmark and applying them zero-shot to another.
Finally, for a rigorous evaluation of robustness for trained models, we use and
release large numbers of diverse reformulations generated by prompting GPT for
benchmark test sets (resulting in 20x increase in sizes). Our findings show
that ConvQA models with robust training via reformulations, significantly
outperform those with standard training from gold QA pairs only.
</p></li>
</ul>

<h3>Title: Progressively Efficient Learning. (arXiv:2310.13004v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13004">http://arxiv.org/abs/2310.13004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13004]] Progressively Efficient Learning(http://arxiv.org/abs/2310.13004)</code></li>
<li>Summary: <p>Assistant AI agents should be capable of rapidly acquiring novel skills and
adapting to new user preferences. Traditional frameworks like imitation
learning and reinforcement learning do not facilitate this capability because
they support only low-level, inefficient forms of communication. In contrast,
humans communicate with progressive efficiency by defining and sharing abstract
intentions. Reproducing similar capability in AI agents, we develop a novel
learning framework named Communication-Efficient Interactive Learning (CEIL).
By equipping a learning agent with an abstract, dynamic language and an
intrinsic motivation to learn with minimal communication effort, CEIL leads to
emergence of a human-like pattern where the learner and the teacher communicate
progressively efficiently by exchanging increasingly more abstract intentions.
CEIL demonstrates impressive performance and communication efficiency on a 2D
MineCraft domain featuring long-horizon decision-making tasks. Agents trained
with CEIL quickly master new tasks, outperforming non-hierarchical and
hierarchical imitation learning by up to 50% and 20% in absolute success rate,
respectively, given the same number of interactions with the teacher.
Especially, the framework performs robustly with teachers modeled after human
pragmatic communication behavior.
</p></li>
</ul>

<h3>Title: To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. (arXiv:2310.13061v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13061">http://arxiv.org/abs/2310.13061</a></li>
<li>Code URL: https://github.com/d-doshi/Grokking</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13061]] To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets(http://arxiv.org/abs/2310.13061)</code></li>
<li>Summary: <p>Robust generalization is a major challenge in deep learning, particularly
when the number of trainable parameters is very large. In general, it is very
difficult to know if the network has memorized a particular set of examples or
understood the underlying rule (or both). Motivated by this challenge, we study
an interpretable model where generalizing representations are understood
analytically, and are easily distinguishable from the memorizing ones. Namely,
we consider two-layer neural networks trained on modular arithmetic tasks where
($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the
modular operations in the training set are incorrect). We show that (i) it is
possible for the network to memorize the corrupted labels \emph{and} achieve
$100\%$ generalization at the same time; (ii) the memorizing neurons can be
identified and pruned, lowering the accuracy on corrupted data and improving
the accuracy on uncorrupted data; (iii) regularization methods such as weight
decay, dropout and BatchNorm force the network to ignore the corrupted data
during optimization, and achieve $100\%$ accuracy on the uncorrupted dataset;
and (iv) the effect of these regularization methods is (``mechanistically'')
interpretable: weight decay and dropout force all the neurons to learn
generalizing representations, while BatchNorm de-amplifies the output of
memorizing neurons and amplifies the output of the generalizing ones. Finally,
we show that in the presence of regularization, the training dynamics involves
two consecutive stages: first, the network undergoes the \emph{grokking}
dynamics reaching high train \emph{and} test accuracy; second, it unlearns the
memorizing representations, where train accuracy suddenly jumps from $100\%$ to
$100 (1-\xi)\%$.
</p></li>
</ul>

<h3>Title: BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model. (arXiv:2310.13403v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13403">http://arxiv.org/abs/2310.13403</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13403]] BRFL: A Blockchain-based Byzantine-Robust Federated Learning Model(http://arxiv.org/abs/2310.13403)</code></li>
<li>Summary: <p>With the increasing importance of machine learning, the privacy and security
of training data have become critical. Federated learning, which stores data in
distributed nodes and shares only model parameters, has gained significant
attention for addressing this concern. However, a challenge arises in federated
learning due to the Byzantine Attack Problem, where malicious local models can
compromise the global model's performance during aggregation. This article
proposes the Blockchain-based Byzantine-Robust Federated Learning (BRLF) model
that combines federated learning with blockchain technology. This integration
enables traceability of malicious models and provides incentives for locally
trained clients. Our approach involves selecting the aggregation node based on
Pearson's correlation coefficient, and we perform spectral clustering and
calculate the average gradient within each cluster, validating its accuracy
using local dataset of the aggregation nodes. Experimental results on public
datasets demonstrate the superior byzantine robustness of our secure
aggregation algorithm compared to other baseline byzantine robust aggregation
methods, and proved our proposed model effectiveness in addressing the resource
consumption problem.
</p></li>
</ul>

<h3>Title: ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction. (arXiv:2310.13590v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13590">http://arxiv.org/abs/2310.13590</a></li>
<li>Code URL: https://github.com/syr-cn/relm</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13590]] ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction(http://arxiv.org/abs/2310.13590)</code></li>
<li>Summary: <p>Predicting chemical reactions, a fundamental challenge in chemistry, involves
forecasting the resulting products from a given reaction process. Conventional
techniques, notably those employing Graph Neural Networks (GNNs), are often
limited by insufficient training data and their inability to utilize textual
information, undermining their applicability in real-world applications. In
this work, we propose ReLM, a novel framework that leverages the chemical
knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing
the accuracy of real-world chemical reaction predictions. To further enhance
the model's robustness and interpretability, we incorporate the confidence
score strategy, enabling the LMs to self-assess the reliability of their
predictions. Our experimental results demonstrate that ReLM improves the
performance of state-of-the-art GNN-based methods across various chemical
reaction datasets, especially in out-of-distribution settings. Codes are
available at https://github.com/syr-cn/ReLM.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering. (arXiv:2310.13120v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13120">http://arxiv.org/abs/2310.13120</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13120]] RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering(http://arxiv.org/abs/2310.13120)</code></li>
<li>Summary: <p>In recent years, with the rapid advancement of transformer models,
transformer-based multimodal architectures have found wide application in
various downstream tasks, including but not limited to Image Captioning, Visual
Question Answering (VQA), and Image-Text Generation. However, contemporary
approaches to Remote Sensing (RS) VQA often involve resource-intensive
techniques, such as full fine-tuning of large models or the extraction of
image-text features from pre-trained multimodal models, followed by modality
fusion using decoders. These approaches demand significant computational
resources and time, and a considerable number of trainable parameters are
introduced. To address these challenges, we introduce a novel method known as
RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter
comprises two key components: the Parallel Adapter and an additional linear
transformation layer inserted after each fully connected (FC) layer within the
Adapter. This approach not only improves adaptation to pre-trained multimodal
models but also allows the parameters of the linear transformation layer to be
integrated into the preceding FC layers during inference, reducing inference
costs. To demonstrate the effectiveness of RSAdapter, we conduct an extensive
series of experiments using three distinct RS-VQA datasets and achieve
state-of-the-art results on all three datasets. The code for RSAdapter will be
available online at https://github.com/Y-D-Wang/RSAdapter.
</p></li>
</ul>

<h3>Title: A Car Model Identification System for Streamlining the Automobile Sales Process. (arXiv:2310.13198v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13198">http://arxiv.org/abs/2310.13198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13198]] A Car Model Identification System for Streamlining the Automobile Sales Process(http://arxiv.org/abs/2310.13198)</code></li>
<li>Summary: <p>This project presents an automated solution for the efficient identification
of car models and makes from images, aimed at streamlining the vehicle listing
process on online car-selling platforms. Through a thorough exploration
encompassing various efficient network architectures including Convolutional
Neural Networks (CNNs), Vision Transformers (ViTs), and hybrid models, we
achieved a notable accuracy of 81.97% employing the EfficientNet (V2 b2)
architecture. To refine performance, a combination of strategies, including
data augmentation, fine-tuning pretrained models, and extensive hyperparameter
tuning, were applied. The trained model offers the potential for automating
information extraction, promising enhanced user experiences across car-selling
websites.
</p></li>
</ul>

<h3>Title: Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model. (arXiv:2310.13106v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13106">http://arxiv.org/abs/2310.13106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13106]] Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model(http://arxiv.org/abs/2310.13106)</code></li>
<li>Summary: <p>Question generation is a widely used data augmentation approach with
extensive applications, and extracting qualified candidate answers from context
passages is a critical step for most question generation systems. However,
existing methods for candidate answer extraction are reliant on linguistic
rules or annotated data that face the partial annotation issue and challenges
in generalization. To overcome these limitations, we propose a novel
unsupervised candidate answer extraction approach that leverages the inherent
structure of context passages through a Differentiable Masker-Reconstructor
(DMR) Model with the enforcement of self-consistency for picking up salient
information tokens. We curated two datasets with exhaustively-annotated answers
and benchmark a comprehensive set of supervised and unsupervised candidate
answer extraction methods. We demonstrate the effectiveness of the DMR model by
showing its performance is superior among unsupervised methods and comparable
to supervised methods.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs. (arXiv:2310.13161v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13161">http://arxiv.org/abs/2310.13161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13161]] A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs(http://arxiv.org/abs/2310.13161)</code></li>
<li>Summary: <p>The classification of weather data involves categorizing meteorological
phenomena into classes, thereby facilitating nuanced analyses and precise
predictions for various sectors such as agriculture, aviation, and disaster
management. This involves utilizing machine learning models to analyze large,
multidimensional weather datasets for patterns and trends. These datasets may
include variables such as temperature, humidity, wind speed, and pressure,
contributing to meteorological conditions. Furthermore, it's imperative that
classification algorithms proficiently navigate challenges such as data
imbalances, where certain weather events (e.g., storms or extreme temperatures)
might be underrepresented. This empirical study explores data augmentation
methods to address imbalanced classes in tabular weather data in centralized
and federated settings. Employing data augmentation techniques such as the
Synthetic Minority Over-sampling Technique or Generative Adversarial Networks
can improve the model's accuracy in classifying rare but critical weather
events. Moreover, with advancements in federated learning, machine learning
models can be trained across decentralized databases, ensuring privacy and data
integrity while mitigating the need for centralized data storage and
processing. Thus, the classification of weather data stands as a critical
bridge, linking raw meteorological data to actionable insights, enhancing our
capacity to anticipate and prepare for diverse weather conditions.
</p></li>
</ul>

<h3>Title: Training A Semantic Communication System with Federated Learning. (arXiv:2310.13236v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13236">http://arxiv.org/abs/2310.13236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13236]] Training A Semantic Communication System with Federated Learning(http://arxiv.org/abs/2310.13236)</code></li>
<li>Summary: <p>Semantic communication has emerged as a pillar for the next generation of
communication systems due to its capabilities in alleviating data redundancy.
Most semantic communication systems are built using advanced deep learning
models whose performance heavily depends on data availability. These studies
assume that an abundance of training data is available, which is unrealistic.
In practice, data is mainly created on the user side. Due to privacy and
security concerns, the transmission of data is restricted, which is necessary
for conventional centralized training schemes. To address this challenge, we
explore semantic communication in federated learning (FL) setting that utilizes
user data without leaking privacy. Additionally, we design our system to tackle
the communication overhead by reducing the quantity of information delivered in
each global round. In this way, we can save significant bandwidth for
resource-limited devices and reduce overall network traffic. Finally, we
propose a mechanism to aggregate the global model from the clients, called
FedLol. Extensive simulation results demonstrate the efficacy of our proposed
technique compared to baseline methods.
</p></li>
</ul>

<h3>Title: FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows. (arXiv:2310.13248v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13248">http://arxiv.org/abs/2310.13248</a></li>
<li>Code URL: https://github.com/geods/flee-gnn</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13248]] FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows(http://arxiv.org/abs/2310.13248)</code></li>
<li>Summary: <p>Understanding and measuring the resilience of food supply networks is a
global imperative to tackle increasing food insecurity. However, the complexity
of these networks, with their multidimensional interactions and decisions,
presents significant challenges. This paper proposes FLEE-GNN, a novel
Federated Learning System for Edge-Enhanced Graph Neural Network, designed to
overcome these challenges and enhance the analysis of geospatial resilience of
multicommodity food flow network, which is one type of spatial networks.
FLEE-GNN addresses the limitations of current methodologies, such as
entropy-based methods, in terms of generalizability, scalability, and data
privacy. It combines the robustness and adaptability of graph neural networks
with the privacy-conscious and decentralized aspects of federated learning on
food supply network resilience analysis across geographical regions. This paper
also discusses FLEE-GNN's innovative data generation techniques, experimental
designs, and future directions for improvement. The results show the
advancements of this approach to quantifying the resilience of multicommodity
food flow networks, contributing to efforts towards ensuring global food
security using AI methods. The developed FLEE-GNN has the potential to be
applied in other spatial networks with spatially heterogeneous sub-network
distributions.
</p></li>
</ul>

<h3>Title: FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning. (arXiv:2310.13283v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13283">http://arxiv.org/abs/2310.13283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13283]] FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning(http://arxiv.org/abs/2310.13283)</code></li>
<li>Summary: <p>Federated learning (FL) is an emerging machine learning paradigm in which a
central server coordinates multiple participants (a.k.a. FL clients) to train a
model collaboratively on decentralized data with privacy protection. This
paradigm constrains that all clients have to train models with the same
structures (homogeneous). In practice, FL often faces statistical
heterogeneity, system heterogeneity and model heterogeneity challenges. These
challenging issues inspire the field of Model-Heterogeneous Personalized
Federated Learning (MHPFL) which aims to train a personalized and heterogeneous
local model for each FL client. Existing MHPFL approaches cannot achieve
satisfactory model performance, acceptable computational overhead and efficient
communication simultaneously. To bridge this gap, we propose a novel
computation- and communication-efficient model-heterogeneous personalized
Federated learning framework based on LoRA tuning (FedLoRA). It is designed to
incorporate a homogeneous small adapter for each client's heterogeneous local
model. Both models are trained following the proposed iterative training for
global-local knowledge exchange. The homogeneous small local adapters are sent
to the FL server to be aggregated into a global adapter. In this way, FL
clients can train heterogeneous local models without incurring high computation
and communication costs. We theoretically prove the non-convex convergence rate
of FedLoRA. Extensive experiments on two real-world datasets demonstrate that
FedLoRA outperforms six state-of-the-art baselines, beating the best approach
by 1.35% in terms of test accuracy, 11.81 times computation overhead reduction
and 7.41 times communication cost saving.
</p></li>
</ul>

<h3>Title: VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13367">http://arxiv.org/abs/2310.13367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13367]] VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models(http://arxiv.org/abs/2310.13367)</code></li>
<li>Summary: <p>Vertical Federated Learning (VFL) has gained increasing attention as a novel
training paradigm that integrates sample alignment and feature union. However,
existing VFL methods face challenges when dealing with heterogeneous local
models among participants, which affects optimization convergence and
generalization. To address this issue, this paper proposes a novel approach
called Vertical Federated learning for training Multi-parties Heterogeneous
models (VFedMH). VFedMH focuses on aggregating the embeddings of each
participant's knowledge instead of intermediate results during forward
propagation. The active party, who possesses labels and features of the sample,
in VFedMH securely aggregates local embeddings to obtain global knowledge
embeddings, and sends them to passive parties. The passive parties, who own
only features of the sample, then utilize the global embeddings to propagate
forward on their local heterogeneous networks. However, the passive party does
not own the labels, so the local model gradient cannot be calculated locally.
To overcome this limitation, the active party assists the passive party in
computing its local heterogeneous model gradients. Then, each participant
trains their local model using the heterogeneous model gradients. The objective
is to minimize the loss value of their respective local heterogeneous models.
Additionally, the paper provides a theoretical analysis of VFedMH's convergence
performance. Extensive experiments are conducted to demonstrate that VFedMH can
simultaneously train multiple heterogeneous models with heterogeneous
optimization and outperform some recent methods in model performance.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability. (arXiv:2310.13240v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13240">http://arxiv.org/abs/2310.13240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13240]] Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability(http://arxiv.org/abs/2310.13240)</code></li>
<li>Summary: <p>Causal machine learning tools are beginning to see use in real-world policy
evaluation tasks to flexibly estimate treatment effects. One issue with these
methods is that the machine learning models used are generally black boxes,
i.e., there is no globally interpretable way to understand how a model makes
estimates. This is a clear problem in policy evaluation applications,
particularly in government, because it is difficult to understand whether such
models are functioning in ways that are fair, based on the correct
interpretation of evidence and transparent enough to allow for accountability
if things go wrong. However, there has been little discussion of transparency
problems in the causal machine learning literature and how these might be
overcome. This paper explores why transparency issues are a problem for causal
machine learning in public policy evaluation applications and considers ways
these problems might be addressed through explainable AI tools and by
simplifying models in line with interpretable AI principles. It then applies
these ideas to a case-study using a causal forest model to estimate conditional
average treatment effects for a hypothetical change in the school leaving age
in Australia. It shows that existing tools for understanding black-box
predictive models are poorly suited to causal machine learning and that
simplifying the model to make it interpretable leads to an unacceptable
increase in error (in this application). It concludes that new tools are needed
to properly understand causal machine learning models and the algorithms that
fit them.
</p></li>
</ul>

<h3>Title: Dissecting Causal Biases. (arXiv:2310.13364v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13364">http://arxiv.org/abs/2310.13364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13364]] Dissecting Causal Biases(http://arxiv.org/abs/2310.13364)</code></li>
<li>Summary: <p>Accurately measuring discrimination in machine learning-based automated
decision systems is required to address the vital issue of fairness between
subpopulations and/or individuals. Any bias in measuring discrimination can
lead to either amplification or underestimation of the true value of
discrimination. This paper focuses on a class of bias originating in the way
training data is generated and/or collected. We call such class causal biases
and use tools from the field of causality to formally define and analyze such
biases. Four sources of bias are considered, namely, confounding, selection,
measurement, and interaction. The main contribution of this paper is to
provide, for each source of bias, a closed-form expression in terms of the
model parameters. This makes it possible to analyze the behavior of each source
of bias, in particular, in which cases they are absent and in which other cases
they are maximized. We hope that the provided characterizations help the
community better understand the sources of bias in machine learning
applications.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Using Logic Programming and Kernel-Grouping for Improving Interpretability of Convolutional Neural Networks. (arXiv:2310.13073v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13073">http://arxiv.org/abs/2310.13073</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13073]] Using Logic Programming and Kernel-Grouping for Improving Interpretability of Convolutional Neural Networks(http://arxiv.org/abs/2310.13073)</code></li>
<li>Summary: <p>Within the realm of deep learning, the interpretability of Convolutional
Neural Networks (CNNs), particularly in the context of image classification
tasks, remains a formidable challenge. To this end we present a neurosymbolic
framework, NeSyFOLD-G that generates a symbolic rule-set using the last layer
kernels of the CNN to make its underlying knowledge interpretable. What makes
NeSyFOLD-G different from other similar frameworks is that we first find groups
of similar kernels in the CNN (kernel-grouping) using the cosine-similarity
between the feature maps generated by various kernels. Once such kernel groups
are found, we binarize each kernel group's output in the CNN and use it to
generate a binarization table which serves as input data to FOLD-SE-M which is
a Rule Based Machine Learning (RBML) algorithm. FOLD-SE-M then generates a
rule-set that can be used to make predictions. We present a novel kernel
grouping algorithm and show that grouping similar kernels leads to a
significant reduction in the size of the rule-set generated by FOLD-SE-M,
consequently, improving the interpretability. This rule-set symbolically
encapsulates the connectionist knowledge of the trained CNN. The rule-set can
be viewed as a normal logic program wherein each predicate's truth value
depends on a kernel group in the CNN. Each predicate in the rule-set is mapped
to a concept using a few semantic segmentation masks of the images used for
training, to make it human-understandable. The last layers of the CNN can then
be replaced by this rule-set to obtain the NeSy-G model which can then be used
for the image classification task. The goal directed ASP system s(CASP) can be
used to obtain the justification of any prediction made using the NeSy-G model.
We also propose a novel algorithm for labeling each predicate in the rule-set
with the semantic concept(s) that its corresponding kernel group represents.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture Propagation. (arXiv:2310.13119v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13119">http://arxiv.org/abs/2310.13119</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13119]] DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture Propagation(http://arxiv.org/abs/2310.13119)</code></li>
<li>Summary: <p>Diffusion-based methods have achieved prominent success in generating 2D
media. However, accomplishing similar proficiencies for scene-level mesh
texturing in 3D spatial applications, e.g., XR/VR, remains constrained,
primarily due to the intricate nature of 3D geometry and the necessity for
immersive free-viewpoint rendering. In this paper, we propose a novel indoor
scene texturing framework, which delivers text-driven texture generation with
enchanting details and authentic spatial coherence. The key insight is to first
imagine a stylized 360{\deg} panoramic texture from the central viewpoint of
the scene, and then propagate it to the rest areas with inpainting and
imitating techniques. To ensure meaningful and aligned textures to the scene,
we develop a novel coarse-to-fine panoramic texture generation approach with
dual texture alignment, which both considers the geometry and texture cues of
the captured scenes. To survive from cluttered geometries during texture
propagation, we design a separated strategy, which conducts texture inpainting
in confidential regions and then learns an implicit imitating network to
synthesize textures in occluded and tiny structural areas. Extensive
experiments and the immersive VR application on real-world indoor scenes
demonstrate the high quality of the generated textures and the engaging
experience on VR headsets. Project webpage:
https://ybbbbt.com/publication/dreamspace
</p></li>
</ul>

<h3>Title: CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation. (arXiv:2310.13165v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13165">http://arxiv.org/abs/2310.13165</a></li>
<li>Code URL: https://github.com/sled-group/cyclenet</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13165]] CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation(http://arxiv.org/abs/2310.13165)</code></li>
<li>Summary: <p>Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks
but lack an intuitive interface for consistent image-to-image (I2I)
translation. Various methods have been explored to address this issue,
including mask-based methods, attention-based methods, and image-conditioning.
However, it remains a critical challenge to enable unpaired I2I translation
with pre-trained DMs while maintaining satisfying consistency. This paper
introduces Cyclenet, a novel but simple method that incorporates cycle
consistency into DMs to regularize image manipulation. We validate Cyclenet on
unpaired I2I tasks of different granularities. Besides the scene and object
level translation, we additionally contribute a multi-domain I2I translation
dataset to study the physical state changes of objects. Our empirical studies
show that Cyclenet is superior in translation consistency and quality, and can
generate high-quality images for out-of-domain distributions with a simple
change of the textual prompt. Cyclenet is a practical framework, which is
robust even with very limited training data (around 2k) and requires minimal
computational resources (1 GPU) to train. Project homepage:
https://cyclenetweb.github.io/
</p></li>
</ul>

<h3>Title: DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics. (arXiv:2310.13268v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13268">http://arxiv.org/abs/2310.13268</a></li>
<li>Code URL: https://github.com/thu-ml/dpm-solver-v3</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13268]] DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics(http://arxiv.org/abs/2310.13268)</code></li>
<li>Summary: <p>Diffusion probabilistic models (DPMs) have exhibited excellent performance
for high-fidelity image generation while suffering from inefficient sampling.
Recent works accelerate the sampling procedure by proposing fast ODE solvers
that leverage the specific ODE form of DPMs. However, they highly rely on
specific parameterization during inference (such as noise/data prediction),
which might not be the optimal choice. In this work, we propose a novel
formulation towards the optimal parameterization during sampling that minimizes
the first-order discretization error of the ODE solution. Based on such
formulation, we propose \textit{DPM-Solver-v3}, a new fast ODE solver for DPMs
by introducing several coefficients efficiently computed on the pretrained
model, which we call \textit{empirical model statistics}. We further
incorporate multistep methods and a predictor-corrector framework, and propose
some techniques for improving sample quality at small numbers of function
evaluations (NFE) or large guidance scales. Experiments show that DPM-Solver-v3
achieves consistently better or comparable performance in both unconditional
and conditional sampling with both pixel-space and latent-space DPMs,
especially in 5$\sim$10 NFEs. We achieve FIDs of 12.21 (5 NFE), 2.51 (10 NFE)
on unconditional CIFAR10, and MSE of 0.55 (5 NFE, 7.5 guidance scale) on Stable
Diffusion, bringing a speed-up of 15\%$\sim$30\% compared to previous
state-of-the-art training-free methods. Code is available at
\url{https://github.com/thu-ml/DPM-Solver-v3}.
</p></li>
</ul>

<h3>Title: ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection. (arXiv:2310.13545v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13545">http://arxiv.org/abs/2310.13545</a></li>
<li>Code URL: https://github.com/sail-sg/scalelong</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13545]] ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection(http://arxiv.org/abs/2310.13545)</code></li>
<li>Summary: <p>In diffusion models, UNet is the most popular network backbone, since its
long skip connects (LSCs) to connect distant network blocks can aggregate
long-distant information and alleviate vanishing gradient. Unfortunately, UNet
often suffers from unstable training in diffusion models which can be
alleviated by scaling its LSC coefficients smaller. However, theoretical
understandings of the instability of UNet in diffusion models and also the
performance improvement of LSC scaling remain absent yet. To solve this issue,
we theoretically show that the coefficients of LSCs in UNet have big effects on
the stableness of the forward and backward propagation and robustness of UNet.
Specifically, the hidden feature and gradient of UNet at any layer can
oscillate and their oscillation ranges are actually large which explains the
instability of UNet training. Moreover, UNet is also provably sensitive to
perturbed input, and predicts an output distant from the desired output,
yielding oscillatory loss and thus oscillatory gradient. Besides, we also
observe the theoretical benefits of the LSC coefficient scaling of UNet in the
stableness of hidden features and gradient and also robustness. Finally,
inspired by our theory, we propose an effective coefficient scaling framework
ScaleLong that scales the coefficients of LSC in UNet and better improves the
training stability of UNet. Experimental results on four famous datasets show
that our methods are superior to stabilize training and yield about 1.5x
training acceleration on different diffusion models with UNet or UViT
backbones. Code: https://github.com/sail-sg/ScaleLong
</p></li>
</ul>

<h3>Title: Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13102">http://arxiv.org/abs/2310.13102</a></li>
<li>Code URL: https://github.com/gcorso/particle-guidance</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13102]] Particle Guidance: non-I(http://arxiv.org/abs/2310.13102)</code></li>
<li>Summary: <p>In light of the widespread success of generative models, a significant amount
of research has gone into speeding up their sampling time. However, generative
models are often sampled multiple times to obtain a diverse set incurring a
cost that is orthogonal to sampling time. We tackle the question of how to
improve diversity and sample efficiency by moving beyond the common assumption
of independent samples. We propose particle guidance, an extension of
diffusion-based generative sampling where a joint-particle time-evolving
potential enforces diversity. We analyze theoretically the joint distribution
that particle guidance generates, its implications on the choice of potential,
and the connections with methods in other disciplines. Empirically, we test the
framework both in the setting of conditional image generation, where we are
able to increase diversity without affecting quality, and molecular conformer
generation, where we reduce the state-of-the-art median error by 13% on
average.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection. (arXiv:2310.13103v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13103">http://arxiv.org/abs/2310.13103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13103]] AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection(http://arxiv.org/abs/2310.13103)</code></li>
<li>Summary: <p>Forged content shared widely on social media platforms is a major social
problem that requires increased regulation and poses new challenges to the
research community. The recent proliferation of hyper-realistic deepfake videos
has drawn attention to the threat of audio and visual forgeries. Most previous
work on detecting AI-generated fake videos only utilizes visual modality or
audio modality. While there are some methods in the literature that exploit
audio and visual modalities to detect forged videos, they have not been
comprehensively evaluated on multi-modal datasets of deepfake videos involving
acoustic and visual manipulations. Moreover, these existing methods are mostly
based on CNN and suffer from low detection accuracy. Inspired by the recent
success of Transformer in various fields, to address the challenges posed by
deepfake technology, in this paper, we propose an Audio-Visual
Transformer-based Ensemble Network (AVTENet) framework that considers both
acoustic manipulation and visual manipulation to achieve effective video
forgery detection. Specifically, the proposed model integrates several purely
transformer-based variants that capture video, audio, and audio-visual salient
cues to reach a consensus in prediction. For evaluation, we use the recently
released benchmark multi-modal audio-video FakeAVCeleb dataset. For a detailed
analysis, we evaluate AVTENet, its variants, and several existing methods on
multiple test sets of the FakeAVCeleb dataset. Experimental results show that
our best model outperforms all existing methods and achieves state-of-the-art
performance on Testset-I and Testset-II of the FakeAVCeleb dataset.
</p></li>
</ul>

<h3>Title: LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for Autonomous Driving with Multi-Task Learning. (arXiv:2310.13135v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13135">http://arxiv.org/abs/2310.13135</a></li>
<li>Code URL: https://github.com/pagand/e2etransfuser</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13135]] LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for Autonomous Driving with Multi-Task Learning(http://arxiv.org/abs/2310.13135)</code></li>
<li>Summary: <p>In end-to-end autonomous driving, the utilization of existing sensor fusion
techniques for imitation learning proves inadequate in challenging situations
that involve numerous dynamic agents. To address this issue, we introduce
LeTFuser, a transformer-based algorithm for fusing multiple RGB-D camera
representations. To perform perception and control tasks simultaneously, we
utilize multi-task learning. Our model comprises of two modules, the first
being the perception module that is responsible for encoding the observation
data obtained from the RGB-D cameras. It carries out tasks such as semantic
segmentation, semantic depth cloud mapping (SDC), and traffic light state
recognition. Our approach employs the Convolutional vision Transformer (CvT)
\cite{wu2021cvt} to better extract and fuse features from multiple RGB cameras
due to local and global feature extraction capability of convolution and
transformer modules, respectively. Following this, the control module
undertakes the decoding of the encoded characteristics together with
supplementary data, comprising a rough simulator for static and dynamic
environments, as well as various measurements, in order to anticipate the
waypoints associated with a latent feature space. We use two methods to process
these outputs and generate the vehicular controls (e.g. steering, throttle, and
brake) levels. The first method uses a PID algorithm to follow the waypoints on
the fly, whereas the second one directly predicts the control policy using the
measurement features and environmental state. We evaluate the model and conduct
a comparative analysis with recent models on the CARLA simulator using various
scenarios, ranging from normal to adversarial conditions, to simulate
real-world scenarios. Our code is available at
\url{https://github.com/pagand/e2etransfuser/tree/cvpr-w} to facilitate future
studies.
</p></li>
</ul>

<h3>Title: Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation. (arXiv:2310.13361v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13361">http://arxiv.org/abs/2310.13361</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13361]] Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation(http://arxiv.org/abs/2310.13361)</code></li>
<li>Summary: <p>Multimodal machine translation (MMT) simultaneously takes the source sentence
and a relevant image as input for translation. Since there is no paired image
available for the input sentence in most cases, recent studies suggest
utilizing powerful text-to-image generation models to provide image inputs.
Nevertheless, synthetic images generated by these models often follow different
distributions compared to authentic images. Consequently, using authentic
images for training and synthetic images for inference can introduce a
distribution shift, resulting in performance degradation during inference. To
tackle this challenge, in this paper, we feed synthetic and authentic images to
the MMT model, respectively. Then we minimize the gap between the synthetic and
authentic images by drawing close the input image representations of the
Transformer Encoder and the output distributions of the Transformer Decoder.
Therefore, we mitigate the distribution disparity introduced by the synthetic
images during inference, thereby freeing the authentic images from the
inference process.Experimental results show that our approach achieves
state-of-the-art performance on the Multi30K En-De and En-Fr datasets, while
remaining independent of authentic images during inference.
</p></li>
</ul>

<h3>Title: POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization. (arXiv:2310.13585v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13585">http://arxiv.org/abs/2310.13585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13585]] POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization(http://arxiv.org/abs/2310.13585)</code></li>
<li>Summary: <p>This paper tackles the challenge of point-supervised temporal action
detection, wherein only a single frame is annotated for each action instance in
the training set. Most of the current methods, hindered by the sparse nature of
annotated points, struggle to effectively represent the continuous structure of
actions or the inherent temporal and semantic dependencies within action
instances. Consequently, these methods frequently learn merely the most
distinctive segments of actions, leading to the creation of incomplete action
proposals. This paper proposes POTLoc, a Pseudo-label Oriented Transformer for
weakly-supervised Action Localization utilizing only point-level annotation.
POTLoc is designed to identify and track continuous action structures via a
self-training strategy. The base model begins by generating action proposals
solely with point-level supervision. These proposals undergo refinement and
regression to enhance the precision of the estimated action boundaries, which
subsequently results in the production of `pseudo-labels' to serve as
supplementary supervisory signals. The architecture of the model integrates a
transformer with a temporal feature pyramid to capture video snippet
dependencies and model actions of varying duration. The pseudo-labels,
providing information about the coarse locations and boundaries of actions,
assist in guiding the transformer for enhanced learning of action dynamics.
POTLoc outperforms the state-of-the-art point-supervised methods on THUMOS'14
and ActivityNet-v1.2 datasets, showing a significant improvement of 5% average
mAP on the former.
</p></li>
</ul>

<h3>Title: FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer. (arXiv:2310.13605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13605">http://arxiv.org/abs/2310.13605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13605]] FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer(http://arxiv.org/abs/2310.13605)</code></li>
<li>Summary: <p>Local Feature Matching, an essential component of several computer vision
tasks (e.g., structure from motion and visual localization), has been
effectively settled by Transformer-based methods. However, these methods only
integrate long-range context information among keypoints with a fixed receptive
field, which constrains the network from reconciling the importance of features
with different receptive fields to realize complete image perception, hence
limiting the matching accuracy. In addition, these methods utilize a
conventional handcrafted encoding approach to integrate the positional
information of keypoints into the visual descriptors, which limits the
capability of the network to extract reliable positional encoding message. In
this study, we propose Feature Matching with Reconciliatory Transformer (FMRT),
a novel Transformer-based detector-free method that reconciles different
features with multiple receptive fields adaptively and utilizes parallel
networks to realize reliable positional encoding. Specifically, FMRT proposes a
dedicated Reconciliatory Transformer (RecFormer) that consists of a Global
Perception Attention Layer (GPAL) to extract visual descriptors with different
receptive fields and integrate global context information under various scales,
Perception Weight Layer (PWL) to measure the importance of various receptive
fields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract
deep aggregated multi-scale local feature representation. Extensive experiments
demonstrate that FMRT yields extraordinary performance on multiple benchmarks,
including pose estimation, visual localization, homography estimation, and
image matching.
</p></li>
</ul>

<h3>Title: Improving Question Generation with Multi-level Content Planning. (arXiv:2310.13512v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13512">http://arxiv.org/abs/2310.13512</a></li>
<li>Code URL: https://github.com/zeaver/multifactor</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13512]] Improving Question Generation with Multi-level Content Planning(http://arxiv.org/abs/2310.13512)</code></li>
<li>Summary: <p>This paper addresses the problem of generating questions from a given context
and an answer, specifically focusing on questions that require multi-hop
reasoning across an extended context. Previous studies have suggested that key
phrase selection is essential for question generation (QG), yet it is still
challenging to connect such disjointed phrases into meaningful questions,
particularly for long context. To mitigate this issue, we propose MultiFactor,
a novel QG framework based on multi-level content planning. Specifically,
MultiFactor includes two components: FA-model, which simultaneously selects key
phrases and generates full answers, and Q-model which takes the generated full
answer as an additional input to generate questions. Here, full answer
generation is introduced to connect the short answer with the selected key
phrases, thus forming an answer-aware summary to facilitate QG. Both FA-model
and Q-model are formalized as simple-yet-effective Phrase-Enhanced
Transformers, our joint model for phrase selection and text generation.
Experimental results show that our method outperforms strong baselines on two
popular QG datasets. Our code is available at
https://github.com/zeaver/MultiFactor.
</p></li>
</ul>

<h3>Title: Controlled Randomness Improves the Performance of Transformer Models. (arXiv:2310.13526v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13526">http://arxiv.org/abs/2310.13526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13526]] Controlled Randomness Improves the Performance of Transformer Models(http://arxiv.org/abs/2310.13526)</code></li>
<li>Summary: <p>During the pre-training step of natural language models, the main objective
is to learn a general representation of the pre-training dataset, usually
requiring large amounts of textual data to capture the complexity and diversity
of natural language. Contrasting this, in most cases, the size of the data
available to solve the specific downstream task is often dwarfed by the
aforementioned pre-training dataset, especially in domains where data is
scarce. We introduce controlled randomness, i.e. noise, into the training
process to improve fine-tuning language models and explore the performance of
targeted noise in addition to the parameters of these models. We find that
adding such noise can improve the performance in our two downstream tasks of
joint named entity recognition and relation extraction and text summarization.
</p></li>
</ul>

<h3>Title: Hunayn: Elevating Translation Beyond the Literal. (arXiv:2310.13613v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13613">http://arxiv.org/abs/2310.13613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13613]] Hunayn: Elevating Translation Beyond the Literal(http://arxiv.org/abs/2310.13613)</code></li>
<li>Summary: <p>This project introduces an advanced English-to-Arabic translator surpassing
conventional tools. Leveraging the Helsinki transformer (MarianMT), our
approach involves fine-tuning on a self-scraped, purely literary Arabic
dataset. Evaluations against Google Translate show consistent outperformance in
qualitative assessments. Notably, it excels in cultural sensitivity and context
accuracy. This research underscores the Helsinki transformer's superiority for
English-to-Arabic translation using a Fusha dataset.
</p></li>
</ul>

<h3>Title: Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13121">http://arxiv.org/abs/2310.13121</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13121]] Understanding Addition in Transformers(http://arxiv.org/abs/2310.13121)</code></li>
<li>Summary: <p>Understanding the inner workings of machine learning models like Transformers
is vital for their safe and ethical use. This paper presents an in-depth
analysis of a one-layer Transformer model trained for integer addition. We
reveal that the model divides the task into parallel, digit-specific streams
and employs distinct algorithms for different digit positions. Our study also
finds that the model starts calculations late but executes them rapidly. A rare
use case with high loss is identified and explained. Overall, the model's
algorithm is explained in detail. These findings are validated through rigorous
testing and mathematical modeling, contributing to the broader works in
Mechanistic Interpretability, AI safety, and alignment. Our approach opens the
door for analyzing more complex tasks and multi-layer Transformer models.
</p></li>
</ul>

<h3>Title: In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern. (arXiv:2310.13220v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13220">http://arxiv.org/abs/2310.13220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13220]] In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern(http://arxiv.org/abs/2310.13220)</code></li>
<li>Summary: <p>Pre-trained large language models based on Transformers have demonstrated
amazing in-context learning (ICL) abilities. Given several demonstration
examples, the models can implement new tasks without any parameter updates.
However, it is still an open question to understand the mechanism of ICL. In
this paper, we interpret the inference process of ICL as a gradient descent
process in a contrastive learning pattern. Firstly, leveraging kernel methods,
we establish the relationship between gradient descent and self-attention
mechanism under generally used softmax attention setting instead of linear
attention setting. Then, we analyze the corresponding gradient descent process
of ICL from the perspective of contrastive learning without negative samples
and discuss possible improvements of this contrastive learning pattern, based
on which the self-attention layer can be further modified. Finally, we design
experiments to support our opinions. To the best of our knowledge, our work is
the first to provide the understanding of ICL from the perspective of
contrastive learning and has the potential to facilitate future model design by
referring to related works on contrastive learning.
</p></li>
</ul>

<h3>Title: Scalable Neural Network Kernels. (arXiv:2310.13225v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13225">http://arxiv.org/abs/2310.13225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13225]] Scalable Neural Network Kernels(http://arxiv.org/abs/2310.13225)</code></li>
<li>Summary: <p>We introduce the concept of scalable neural network kernels (SNNKs), the
replacements of regular feedforward layers (FFLs), capable of approximating the
latter, but with favorable computational properties. SNNKs effectively
disentangle the inputs from the parameters of the neural network in the FFL,
only to connect them in the final computation via the dot-product kernel. They
are also strictly more expressive, as allowing to model complicated
relationships beyond the functions of the dot-products of parameter-input
vectors. We also introduce the neural network bundling process that applies
SNNKs to compactify deep neural network architectures, resulting in additional
compression gains. In its extreme version, it leads to the fully bundled
network whose optimal parameters can be expressed via explicit formulae for
several loss functions (e.g. mean squared error), opening a possibility to
bypass backpropagation. As a by-product of our analysis, we introduce the
mechanism of the universal random features (or URFs), applied to instantiate
several SNNK variants, and interesting on its own in the context of scalable
kernel methods. We provide rigorous theoretical analysis of all these concepts
as well as an extensive empirical evaluation, ranging from point-wise kernel
estimation to Transformers' fine-tuning with novel adapter layers inspired by
SNNKs. Our mechanism provides up to 5x reduction in the number of trainable
parameters, while maintaining competitive accuracy.
</p></li>
</ul>

<h3>Title: SigFormer: Signature Transformers for Deep Hedging. (arXiv:2310.13369v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13369">http://arxiv.org/abs/2310.13369</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13369]] SigFormer: Signature Transformers for Deep Hedging(http://arxiv.org/abs/2310.13369)</code></li>
<li>Summary: <p>Deep hedging is a promising direction in quantitative finance, incorporating
models and techniques from deep learning research. While giving excellent
hedging strategies, models inherently requires careful treatment in designing
architectures for neural networks. To mitigate such difficulties, we introduce
SigFormer, a novel deep learning model that combines the power of path
signatures and transformers to handle sequential data, particularly in cases
with irregularities. Path signatures effectively capture complex data patterns,
while transformers provide superior sequential attention. Our proposed model is
empirically compared to existing methods on synthetic data, showcasing faster
learning and enhanced robustness, especially in the presence of irregular
underlying price data. Additionally, we validate our model performance through
a real-world backtest on hedging the SP 500 index, demonstrating positive
outcomes.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Conditional Generative Modeling for Images, 3D Animations, and Video. (arXiv:2310.13157v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13157">http://arxiv.org/abs/2310.13157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13157]] Conditional Generative Modeling for Images, 3D Animations, and Video(http://arxiv.org/abs/2310.13157)</code></li>
<li>Summary: <p>This dissertation attempts to drive innovation in the field of generative
modeling for computer vision, by exploring novel formulations of conditional
generative models, and innovative applications in images, 3D animations, and
video. Our research focuses on architectures that offer reversible
transformations of noise and visual data, and the application of
encoder-decoder architectures for generative tasks and 3D content manipulation.
In all instances, we incorporate conditional information to enhance the
synthesis of visual data, improving the efficiency of the generation process as
well as the generated content.
</p>
<p>We introduce the use of Neural ODEs to model video dynamics using an
encoder-decoder architecture, demonstrating their ability to predict future
video frames despite being trained solely to reconstruct current frames. Next,
we propose a conditional variant of continuous normalizing flows that enables
higher-resolution image generation based on lower-resolution input, achieving
comparable image quality while reducing parameters and training time. Our next
contribution presents a pipeline that takes human images as input,
automatically aligns a user-specified 3D character with the pose of the human,
and facilitates pose editing based on partial inputs. Next, we derive the
relevant mathematical details for denoising diffusion models that use
non-isotropic Gaussian processes, and show comparable generation quality.
Finally, we devise a novel denoising diffusion framework capable of solving all
three video tasks of prediction, generation, and interpolation. We perform
ablation studies, and show SOTA results on multiple datasets.
</p>
<p>Our contributions are published articles at peer-reviewed venues. Overall,
our research aims to make a meaningful contribution to the pursuit of more
efficient and flexible generative models, with the potential to shape the
future of computer vision.
</p></li>
</ul>

<h3>Title: Generative error correction for code-switching speech recognition using large language models. (arXiv:2310.13013v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13013">http://arxiv.org/abs/2310.13013</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13013]] Generative error correction for code-switching speech recognition using large language models(http://arxiv.org/abs/2310.13013)</code></li>
<li>Summary: <p>Code-switching (CS) speech refers to the phenomenon of mixing two or more
languages within the same sentence. Despite the recent advances in automatic
speech recognition (ASR), CS-ASR is still a challenging task ought to the
grammatical structure complexity of the phenomenon and the data scarcity of
specific training corpus. In this work, we propose to leverage large language
models (LLMs) and lists of hypotheses generated by an ASR to address the CS
problem. Specifically, we first employ multiple well-trained ASR models for
N-best hypotheses generation, with the aim of increasing the diverse and
informative elements in the set of hypotheses. Next, we utilize the LLMs to
learn the hypotheses-to-transcription (H2T) mapping by adding a trainable
low-rank adapter. Such a generative error correction (GER) method directly
predicts the accurate transcription according to its expert linguistic
knowledge and N-best hypotheses, resulting in a paradigm shift from the
traditional language model rescoring or error correction techniques.
Experimental evidence demonstrates that GER significantly enhances CS-ASR
accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show
remarkable data efficiency for H2T learning, providing a potential solution to
the data scarcity problem of CS-ASR in low-resource languages.
</p></li>
</ul>

<h3>Title: Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models. (arXiv:2310.13127v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13127">http://arxiv.org/abs/2310.13127</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13127]] Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models(http://arxiv.org/abs/2310.13127)</code></li>
<li>Summary: <p>Large language models (LLMs) can perform a wide range of tasks by following
natural language instructions, without the necessity of task-specific
fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by
the quality of these instructions, and manually writing effective instructions
for each task is a laborious and subjective process. In this paper, we
introduce Auto-Instruct, a novel method to automatically improve the quality of
instructions provided to LLMs. Our method leverages the inherent generative
ability of LLMs to produce diverse candidate instructions for a given task, and
then ranks them using a scoring model trained on a variety of 575 existing NLP
tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both
human-written instructions and existing baselines of LLM-generated
instructions. Furthermore, our method exhibits notable generalizability even
with other LLMs that are not incorporated into its training process.
</p></li>
</ul>

<h3>Title: Fast and Accurate Factual Inconsistency Detection Over Long Documents. (arXiv:2310.13189v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13189">http://arxiv.org/abs/2310.13189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13189]] Fast and Accurate Factual Inconsistency Detection Over Long Documents(http://arxiv.org/abs/2310.13189)</code></li>
<li>Summary: <p>Generative AI models exhibit remarkable potential; however, hallucinations
across various tasks present a significant challenge, particularly for longer
inputs that current approaches struggle to address effectively. We introduce
SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a
task-agnostic model for detecting factual inconsistencies using a novel
chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI)
based model that uses large text chunks to condition over long texts. This
approach achieves state-of-the-art performance in factual inconsistency
detection for diverse tasks and long inputs. Additionally, we leverage the
chunking mechanism and employ a novel algorithm to explain SCALE's decisions
through relevant source sentence retrieval. Our evaluations reveal that SCALE
outperforms existing methods on both standard benchmarks and a new long-form
dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses
competitive systems in efficiency and model explanation evaluations.
</p></li>
</ul>

<h3>Title: DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee. (arXiv:2310.13261v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13261">http://arxiv.org/abs/2310.13261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13261]] DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee(http://arxiv.org/abs/2310.13261)</code></li>
<li>Summary: <p>Mixed-integer linear programming (MILP) stands as a notable NP-hard problem
pivotal to numerous crucial industrial applications. The development of
effective algorithms, the tuning of solvers, and the training of machine
learning models for MILP resolution all hinge on access to extensive, diverse,
and representative data. Yet compared to the abundant naturally occurring data
in image and text realms, MILP is markedly data deficient, underscoring the
vital role of synthetic MILP generation. We present DIG-MILP, a deep generative
framework based on variational auto-encoder (VAE), adept at extracting
deep-level structural features from highly limited MILP data and producing
instances that closely mirror the target data. Notably, by leveraging the MILP
duality, DIG-MILP guarantees a correct and complete generation space as well as
ensures the boundedness and feasibility of the generated instances. Our
empirical study highlights the novelty and quality of the instances generated
by DIG-MILP through two distinct downstream tasks: (S1) Data sharing, where
solver solution times correlate highly positive between original and
DIG-MILP-generated instances, allowing data sharing for solver tuning without
publishing the original data; (S2) Data Augmentation, wherein the
DIG-MILP-generated instances bolster the generalization performance of machine
learning models tasked with resolving MILP problems.
</p></li>
</ul>

<h3>Title: Learning Recurrent Models with Temporally Local Rules. (arXiv:2310.13284v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13284">http://arxiv.org/abs/2310.13284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13284]] Learning Recurrent Models with Temporally Local Rules(http://arxiv.org/abs/2310.13284)</code></li>
<li>Summary: <p>Fitting generative models to sequential data typically involves two recursive
computations through time, one forward and one backward. The latter could be a
computation of the loss gradient (as in backpropagation through time), or an
inference algorithm (as in the RTS/Kalman smoother). The backward pass in
particular is computationally expensive (since it is inherently serial and
cannot exploit GPUs), and difficult to map onto biological processes.
Work-arounds have been proposed; here we explore a very different one:
requiring the generative model to learn the joint distribution over current and
previous states, rather than merely the transition probabilities. We show on
toy datasets that different architectures employing this principle can learn
aspects of the data typically requiring the backward pass.
</p></li>
</ul>

<h3>Title: Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances. (arXiv:2310.13433v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13433">http://arxiv.org/abs/2310.13433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13433]] Y-Diagonal Couplings: Approximating Posteriors with Conditional Wasserstein Distances(http://arxiv.org/abs/2310.13433)</code></li>
<li>Summary: <p>In inverse problems, many conditional generative models approximate the
posterior measure by minimizing a distance between the joint measure and its
learned approximation. While this approach also controls the distance between
the posterior measures in the case of the Kullback Leibler divergence, it does
not hold true for the Wasserstein distance. We will introduce a conditional
Wasserstein distance with a set of restricted couplings that equals the
expected Wasserstein distance of the posteriors. By deriving its dual, we find
a rigorous way to motivate the loss of conditional Wasserstein GANs. We outline
conditions under which the vanilla and the conditional Wasserstein distance
coincide. Furthermore, we will show numerical examples where training with the
conditional Wasserstein distance yields favorable properties for posterior
sampling.
</p></li>
</ul>

<h3>Title: Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13459">http://arxiv.org/abs/2310.13459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13459]] Stable Nonconvex-Nonconcave Training via Linear Interpolation(http://arxiv.org/abs/2310.13459)</code></li>
<li>Summary: <p>This paper presents a theoretical analysis of linear interpolation as a
principled method for stabilizing (large-scale) neural network training. We
argue that instabilities in the optimization process are often caused by the
nonmonotonicity of the loss landscape and show how linear interpolation can
help by leveraging the theory of nonexpansive operators. We construct a new
optimization scheme called relaxed approximate proximal point (RAPP), which is
the first explicit method to achieve last iterate convergence rates for the
full range of cohypomonotone problems. The construction extends to constrained
and regularized settings. By replacing the inner optimizer in RAPP we
rediscover the family of Lookahead algorithms for which we establish
convergence in cohypomonotone problems even when the base optimizer is taken to
be gradient descent ascent. The range of cohypomonotone problems in which
Lookahead converges is further expanded by exploiting that Lookahead inherits
the properties of the base optimizer. We corroborate the results with
experiments on generative adversarial networks which demonstrates the benefits
of the linear interpolation present in both RAPP and Lookahead.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds. (arXiv:2310.13255v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13255">http://arxiv.org/abs/2310.13255</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13255]] Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds(http://arxiv.org/abs/2310.13255)</code></li>
<li>Summary: <p>Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact
with the world, which marks an initial step toward versatile robotics. However,
these efforts tend to overlook the visual richness of open worlds, rendering
the entire interactive process akin to "a blindfolded text-based game."
Consequently, LLM-based agents frequently encounter challenges in intuitively
comprehending their surroundings and producing responses that are easy to
understand. In this paper, we propose Steve-Eye, an end-to-end trained large
multimodal model designed to address this limitation. Steve-Eye integrates the
LLM with a visual encoder which enables it to process visual-text inputs and
generate multimodal feedback. In addition, we use a semi-automatic strategy to
collect an extensive dataset comprising 850K open-world instruction pairs,
empowering our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks, then carry
out extensive experiments from a wide range of perspectives to validate our
model's capability to strategically act and plan. Codes and datasets will be
released.
</p></li>
</ul>

<h3>Title: OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data. (arXiv:2310.13398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13398">http://arxiv.org/abs/2310.13398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13398]] OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data(http://arxiv.org/abs/2310.13398)</code></li>
<li>Summary: <p>In the era of big data and large models, automatic annotating functions for
multi-modal data are of great significance for real-world AI-driven
applications, such as autonomous driving and embodied AI. Unlike traditional
closed-set annotation, open-vocabulary annotation is essential to achieve
human-level cognition capability. However, there are few open-vocabulary
auto-labeling systems for multi-modal 3D data. In this paper, we introduce
OpenAnnotate3D, an open-source open-vocabulary auto-labeling system that can
automatically generate 2D masks, 3D masks, and 3D bounding box annotations for
vision and point cloud data. Our system integrates the chain-of-thought
capabilities of Large Language Models (LLMs) and the cross-modality
capabilities of vision-language models (VLMs). To the best of our knowledge,
OpenAnnotate3D is one of the pioneering works for open-vocabulary multi-modal
3D auto-labeling. We conduct comprehensive evaluations on both public and
in-house real-world datasets, which demonstrate that the system significantly
improves annotation efficiency compared to manual annotation while providing
accurate open-vocabulary auto-annotating results.
</p></li>
</ul>

<h3>Title: Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models. (arXiv:2310.13473v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13473">http://arxiv.org/abs/2310.13473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13473]] Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models(http://arxiv.org/abs/2310.13473)</code></li>
<li>Summary: <p>Multimodal large language models (MLLMs) have shown great potential in
perception and interpretation tasks, but their capabilities in predictive
reasoning remain under-explored. To address this gap, we introduce a novel
benchmark that assesses the predictive reasoning capabilities of MLLMs across
diverse scenarios. Our benchmark targets three important domains: abstract
pattern reasoning, human activity prediction, and physical interaction
prediction. We further develop three evaluation methods powered by large
language model to robustly quantify a model's performance in predicting and
reasoning the future based on multi-visual context. Empirical experiments
confirm the soundness of the proposed benchmark and evaluation methods via
rigorous testing and reveal pros and cons of current popular MLLMs in the task
of predictive reasoning. Lastly, our proposed benchmark provides a standardized
evaluation framework for MLLMs and can facilitate the development of more
advanced models that can reason and predict over complex long sequence of
multimodal input.
</p></li>
</ul>

<h3>Title: Enhancing Health Data Interoperability with Large Language Models: A FHIR Study. (arXiv:2310.12989v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.12989">http://arxiv.org/abs/2310.12989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.12989]] Enhancing Health Data Interoperability with Large Language Models: A FHIR Study(http://arxiv.org/abs/2310.12989)</code></li>
<li>Summary: <p>In this study, we investigated the ability of the large language model (LLM)
to enhance healthcare data interoperability. We leveraged the LLM to convert
clinical texts into their corresponding FHIR resources. Our experiments,
conducted on 3,671 snippets of clinical text, demonstrated that the LLM not
only streamlines the multi-step natural language processing and human
calibration processes but also achieves an exceptional accuracy rate of over
90% in exact matches when compared to human annotations.
</p></li>
</ul>

<h3>Title: Are Large Language Models Geospatially Knowledgeable?. (arXiv:2310.13002v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13002">http://arxiv.org/abs/2310.13002</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13002]] Are Large Language Models Geospatially Knowledgeable?(http://arxiv.org/abs/2310.13002)</code></li>
<li>Summary: <p>Despite the impressive performance of Large Language Models (LLM) for various
natural language processing tasks, little is known about their comprehension of
geographic data and related ability to facilitate informed geospatial
decision-making. This paper investigates the extent of geospatial knowledge,
awareness, and reasoning abilities encoded within such pretrained LLMs. With a
focus on autoregressive language models, we devise experimental approaches
related to (i) probing LLMs for geo-coordinates to assess geospatial knowledge,
(ii) using geospatial and non-geospatial prepositions to gauge their geospatial
awareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to
assess the models' geospatial reasoning capabilities and to determine locations
of cities based on prompting. Our results confirm that it does not only take
larger, but also more sophisticated LLMs to synthesize geospatial knowledge
from textual information. As such, this research contributes to understanding
the potential and limitations of LLMs in dealing with geospatial information.
</p></li>
</ul>

<h3>Title: H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13012">http://arxiv.org/abs/2310.13012</a></li>
<li>Code URL: https://github.com/h2oai/h2o-llmstudio</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13012]] H2O Open Ecosystem for State-of-the-art Large Language Models(http://arxiv.org/abs/2310.13012)</code></li>
<li>Summary: <p>Large Language Models (LLMs) represent a revolution in AI. However, they also
pose many significant risks, such as the presence of biased, private,
copyrighted or harmful text. For this reason we need open, transparent and safe
solutions. We introduce a complete open-source ecosystem for developing and
testing LLMs. The goal of this project is to boost open alternatives to
closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7
to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and
no-code GUI designed for efficient fine-tuning, evaluation, and deployment of
LLMs using the most recent state-of-the-art techniques. Our code and models are
licensed under fully permissive Apache 2.0 licenses. We believe open-source
language models help to boost AI development and make it more accessible and
trustworthy. The demo is available at: https://gpt.h2o.ai/
</p></li>
</ul>

<h3>Title: GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13023">http://arxiv.org/abs/2310.13023</a></li>
<li>Code URL: https://github.com/HKUDS/GraphGPT</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13023]] GraphGPT: Graph Instruction Tuning for Large Language Models(http://arxiv.org/abs/2310.13023)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have advanced graph structure understanding via
recursive information exchange and aggregation among graph nodes. To improve
model robustness, self-supervised learning (SSL) has emerged as a promising
approach for data augmentation. However, existing methods for generating
pre-trained graph embeddings often rely on fine-tuning with specific downstream
task labels, which limits their usability in scenarios where labeled data is
scarce or unavailable. To address this, our research focuses on advancing the
generalization capabilities of graph models in challenging zero-shot learning
scenarios. Inspired by the success of large language models (LLMs), we aim to
develop a graph-oriented LLM that can achieve high generalization across
diverse downstream datasets and tasks, even without any information available
from the downstream graph data. In this work, we present the GraphGPT framework
that aligns LLMs with graph structural knowledge with a graph instruction
tuning paradigm. Our framework incorporates a text-graph grounding component to
establish a connection between textual information and graph structures.
Additionally, we propose a dual-stage instruction tuning paradigm, accompanied
by a lightweight graph-text alignment projector. This paradigm explores
self-supervised graph structural signals and task-specific graph instructions,
to guide LLMs in understanding complex graph structures and improving their
adaptability across different downstream tasks. Our framework is evaluated on
supervised and zero-shot graph learning tasks, demonstrating superior
generalization and outperforming state-of-the-art baselines.
</p></li>
</ul>

<h3>Title: Reliable Academic Conference Question Answering: A Study Based on Large Language Model. (arXiv:2310.13028v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13028">http://arxiv.org/abs/2310.13028</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13028]] Reliable Academic Conference Question Answering: A Study Based on Large Language Model(http://arxiv.org/abs/2310.13028)</code></li>
<li>Summary: <p>The rapid growth of computer science has led to a proliferation of research
presented at academic conferences, fostering global scholarly communication.
Researchers consistently seek accurate, current information about these events
at all stages. This data surge necessitates an intelligent question-answering
system to efficiently address researchers' queries and ensure awareness of the
latest advancements. The information of conferences is usually published on
their official website, organized in a semi-structured way with a lot of text.
To address this need, we have developed the ConferenceQA dataset for 7 diverse
academic conferences with human annotations. Firstly, we employ a combination
of manual and automated methods to organize academic conference data in a
semi-structured JSON format. Subsequently, we annotate nearly 100
question-answer pairs for each conference. Each pair is classified into four
different dimensions. To ensure the reliability of the data, we manually
annotate the source of each answer. In light of recent advancements, Large
Language Models (LLMs) have demonstrated impressive performance in various NLP
tasks. They have demonstrated impressive capabilities in information-seeking
question answering after instruction fine-tuning, and as such, we present our
conference QA study based on LLM. Due to hallucination and outdated knowledge
of LLMs, we adopt retrieval based methods to enhance LLMs' question-answering
abilities. We have proposed a structure-aware retrieval method, specifically
designed to leverage inherent structural information during the retrieval
process. Empirical validation on the ConferenceQA dataset has demonstrated the
effectiveness of this method. The dataset and code are readily accessible on
https://github.com/zjukg/ConferenceQA.
</p></li>
</ul>

<h3>Title: Ask Me in English Instead: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries. (arXiv:2310.13132v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13132">http://arxiv.org/abs/2310.13132</a></li>
<li>Code URL: https://github.com/claws-lab/XLingEval</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13132]] Ask Me in English Instead: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries(http://arxiv.org/abs/2310.13132)</code></li>
<li>Summary: <p>Large language models (LLMs) are transforming the ways the general public
accesses and consumes information. Their influence is particularly pronounced
in pivotal sectors like healthcare, where lay individuals are increasingly
appropriating LLMs as conversational agents for everyday queries. While LLMs
demonstrate impressive language understanding and generation proficiencies,
concerns regarding their safety remain paramount in these high-stake domains.
Moreover, the development of LLMs is disproportionately focused on English. It
remains unclear how these LLMs perform in the context of non-English languages,
a gap that is critical for ensuring equity in the real-world use of these
systems.This paper provides a framework to investigate the effectiveness of
LLMs as multi-lingual dialogue systems for healthcare queries. Our
empirically-derived framework XlingEval focuses on three fundamental criteria
for evaluating LLM responses to naturalistic human-authored health-related
questions: correctness, consistency, and verifiability. Through extensive
experiments on four major global languages, including English, Spanish,
Chinese, and Hindi, spanning three expert-annotated large health Q&amp;A datasets,
and through an amalgamation of algorithmic and human-evaluation strategies, we
found a pronounced disparity in LLM responses across these languages,
indicating a need for enhanced cross-lingual capabilities. We further propose
XlingHealth, a cross-lingual benchmark for examining the multilingual
capabilities of LLMs in the healthcare context. Our findings underscore the
pressing need to bolster the cross-lingual capacities of these models, and to
provide an equitable information ecosystem accessible to all.
</p></li>
</ul>

<h3>Title: NameGuess: Column Name Expansion for Tabular Data. (arXiv:2310.13196v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13196">http://arxiv.org/abs/2310.13196</a></li>
<li>Code URL: https://github.com/amazon-science/nameguess</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13196]] NameGuess: Column Name Expansion for Tabular Data(http://arxiv.org/abs/2310.13196)</code></li>
<li>Summary: <p>Recent advances in large language models have revolutionized many sectors,
including the database industry. One common challenge when dealing with large
volumes of tabular data is the pervasive use of abbreviated column names, which
can negatively impact performance on various data search, access, and
understanding tasks. To address this issue, we introduce a new task, called
NameGuess, to expand column names (used in database schema) as a natural
language generation problem. We create a training dataset of 384K
abbreviated-expanded column pairs using a new data fabrication method and a
human-annotated evaluation benchmark that includes 9.2K examples from
real-world tables. To tackle the complexities associated with polysemy and
ambiguity in NameGuess, we enhance auto-regressive language models by
conditioning on table content and column header names -- yielding a fine-tuned
model (with 2.7B parameters) that matches human performance. Furthermore, we
conduct a comprehensive analysis (on multiple LLMs) to validate the
effectiveness of table content in NameGuess and identify promising future
opportunities. Code has been made available at
https://github.com/amazon-science/nameguess.
</p></li>
</ul>

<h3>Title: Primacy Effect of ChatGPT. (arXiv:2310.13206v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13206">http://arxiv.org/abs/2310.13206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13206]] Primacy Effect of ChatGPT(http://arxiv.org/abs/2310.13206)</code></li>
<li>Summary: <p>Instruction-tuned large language models (LLMs), such as ChatGPT, have led to
promising zero-shot performance in discriminative natural language
understanding (NLU) tasks. This involves querying the LLM using a prompt
containing the question, and the candidate labels to choose from. The
question-answering capabilities of ChatGPT arise from its pre-training on large
amounts of human-written text, as well as its subsequent fine-tuning on human
preferences, which motivates us to ask: Does ChatGPT also inherits humans'
cognitive biases? In this paper, we study the primacy effect of ChatGPT: the
tendency of selecting the labels at earlier positions as the answer. We have
two main findings: i) ChatGPT's decision is sensitive to the order of labels in
the prompt; ii) ChatGPT has a clearly higher chance to select the labels at
earlier positions as the answer. We hope that our experiments and analyses
provide additional insights into building more reliable ChatGPT-based
solutions. We release the source code at
https://github.com/wangywUST/PrimacyEffectGPT.
</p></li>
</ul>

<h3>Title: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering. (arXiv:2310.13226v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13226">http://arxiv.org/abs/2310.13226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13226]] Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering(http://arxiv.org/abs/2310.13226)</code></li>
<li>Summary: <p>Blockchain technology has revolutionized the financial landscape, with
cryptocurrencies gaining widespread adoption for their decentralized and
transparent nature. As the sentiment expressed on social media platforms can
significantly influence cryptocurrency discussions and market movements,
sentiment analysis has emerged as a crucial tool for understanding public
opinion and predicting market trends. Motivated by the aim to enhance sentiment
analysis accuracy in the cryptocurrency domain, this paper investigates
fine-tuning techniques on large language models. This paper also investigates
the efficacy of supervised fine-tuning and instruction-based fine-tuning on
large language models for unseen tasks. Experimental results demonstrate a
significant average zero-shot performance gain of 40% after fine-tuning,
highlighting the potential of this technique in optimizing pre-trained language
model efficiency. Additionally, the impact of instruction tuning on models of
varying scales is examined, revealing that larger models benefit from
instruction tuning, achieving the highest average accuracy score of 75.16%. In
contrast, smaller-scale models may experience reduced generalization due to the
complete utilization of model capacity. To gain deeper insight about how
instruction works with these language models, this paper presents an
experimental investigation into the response of an instruction-based model
under different instruction tuning setups. The investigation demonstrates that
the model achieves an average accuracy score of 72.38% for short and simple
instructions. This performance significantly outperforms its accuracy under
long and complex instructions by over 12%, thereby effectively highlighting the
profound significance of instruction characteristics in maximizing model
performance.
</p></li>
</ul>

<h3>Title: ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search. (arXiv:2310.13227v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13227">http://arxiv.org/abs/2310.13227</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13227]] ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search(http://arxiv.org/abs/2310.13227)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated powerful decision-making and
planning capabilities in solving complicated real-world problems. LLM-based
autonomous agents can interact with diverse tools (e.g., functional APIs) and
generate solution plans that execute a series of API function calls in a
step-by-step manner. The multitude of candidate API function calls
significantly expands the action space, amplifying the critical need for
efficient action space navigation. However, existing methods either struggle
with unidirectional exploration in expansive action spaces, trapped into a
locally optimal solution, or suffer from exhaustively traversing all potential
actions, causing inefficient navigation. To address these issues, we propose
ToolChain*, an efficient tree search-based planning algorithm for LLM-based
agents. It formulates the entire action space as a decision tree, where each
node represents a possible API function call involved in a solution plan. By
incorporating the A* search algorithm with task-specific cost function design,
it efficiently prunes high-cost branches that may involve incorrect actions,
identifying the most low-cost valid path as the solution. Extensive experiments
on multiple tool-use and reasoning tasks demonstrate that ToolChain*
efficiently balances exploration and exploitation within an expansive action
space. It outperforms state-of-the-art baselines on planning and reasoning
tasks by 3.1% and 3.5% on average while requiring 7.35x and 2.31x less time,
respectively.
</p></li>
</ul>

<h3>Title: MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model. (arXiv:2310.13265v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13265">http://arxiv.org/abs/2310.13265</a></li>
<li>Code URL: https://github.com/lezhang7/moqagpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13265]] MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model(http://arxiv.org/abs/2310.13265)</code></li>
<li>Summary: <p>Multi-modal open-domain question answering typically requires evidence
retrieval from databases across diverse modalities, such as images, tables,
passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this
task. To enable LLMs to tackle the task in a zero-shot manner, we introduce
MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer
strategy that bypasses intricate multi-modality ranking, our framework can
accommodate new modalities and seamlessly transition to new models for the
task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each
modality separately, then fuses this multi-modal information using LLMs to
produce a final answer. Our methodology boosts performance on the MMCoQA
dataset, improving F1 by +37.91 points and EM by +34.07 points over the
supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the
zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and
significantly closes the gap with supervised methods. Our codebase is available
at https://github.com/lezhang7/MOQAGPT.
</p></li>
</ul>

<h3>Title: Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting. (arXiv:2310.13297v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13297">http://arxiv.org/abs/2310.13297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13297]] Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting(http://arxiv.org/abs/2310.13297)</code></li>
<li>Summary: <p>Automatic response forecasting for news media plays a crucial role in
enabling content producers to efficiently predict the impact of news releases
and prevent unexpected negative outcomes such as social conflict and moral
injury. To effectively forecast responses, it is essential to develop measures
that leverage the social dynamics and contextual information surrounding
individuals, especially in cases where explicit profiles or historical actions
of the users are limited (referred to as lurkers). As shown in a previous
study, 97% of all tweets are produced by only the most active 25% of users.
However, existing approaches have limited exploration of how to best process
and utilize these important features. To address this gap, we propose a novel
framework, named SocialSense, that leverages a large language model to induce a
belief-centered graph on top of an existent social network, along with
graph-based propagation to capture social dynamics. We hypothesize that the
induced graph that bridges the gap between distant users who share similar
beliefs allows the model to effectively capture the response patterns. Our
method surpasses existing state-of-the-art in experimental evaluations for both
zero-shot and supervised settings, demonstrating its effectiveness in response
forecasting. Moreover, the analysis reveals the framework's capability to
effectively handle unseen user and lurker scenarios, further highlighting its
robustness and practical applicability.
</p></li>
</ul>

<h3>Title: Democratizing Reasoning Ability: Tailored Learning from Large Language Model. (arXiv:2310.13332v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13332">http://arxiv.org/abs/2310.13332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13332]] Democratizing Reasoning Ability: Tailored Learning from Large Language Model(http://arxiv.org/abs/2310.13332)</code></li>
<li>Summary: <p>Large language models (LLMs) exhibit impressive emergent abilities in natural
language processing, but their democratization is hindered due to huge
computation requirements and closed-source nature. Recent research on advancing
open-source smaller LMs by distilling knowledge from black-box LLMs has
obtained promising results in the instruction-following ability. However, the
reasoning ability which is more challenging to foster, is relatively rarely
explored. In this paper, we propose a tailored learning approach to distill
such reasoning ability to smaller LMs to facilitate the democratization of the
exclusive reasoning ability. In contrast to merely employing LLM as a data
annotator, we exploit the potential of LLM as a reasoning teacher by building
an interactive multi-round learning paradigm. This paradigm enables the student
to expose its deficiencies to the black-box teacher who then can provide
customized training data in return. Further, to exploit the reasoning potential
of the smaller LM, we propose self-reflection learning to motivate the student
to learn from self-made mistakes. The learning from self-reflection and LLM are
all tailored to the student's learning status, thanks to the seamless
integration with the multi-round learning paradigm. Comprehensive experiments
and analysis on mathematical and commonsense reasoning tasks demonstrate the
effectiveness of our method. The code will be available at
https://github.com/Raibows/Learn-to-Reason.
</p></li>
</ul>

<h3>Title: Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs). (arXiv:2310.13343v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13343">http://arxiv.org/abs/2310.13343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13343]] Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs)(http://arxiv.org/abs/2310.13343)</code></li>
<li>Summary: <p>With the development of large language models (LLMs) like the GPT series,
their widespread use across various application scenarios presents a myriad of
challenges. This review initially explores the issue of domain specificity,
where LLMs may struggle to provide precise answers to specialized questions
within niche fields. The problem of knowledge forgetting arises as these LLMs
might find it hard to balance old and new information. The knowledge repetition
phenomenon reveals that sometimes LLMs might deliver overly mechanized
responses, lacking depth and originality. Furthermore, knowledge illusion
describes situations where LLMs might provide answers that seem insightful but
are actually superficial, while knowledge toxicity focuses on harmful or biased
information outputs. These challenges underscore problems in the training data
and algorithmic design of LLMs. To address these issues, it's suggested to
diversify training data, fine-tune models, enhance transparency and
interpretability, and incorporate ethics and fairness training. Future
technological trends might lean towards iterative methodologies, multimodal
learning, model personalization and customization, and real-time learning and
feedback mechanisms. In conclusion, future LLMs should prioritize fairness,
transparency, and ethics, ensuring they uphold high moral and ethical standards
when serving humanity.
</p></li>
</ul>

<h3>Title: Tuna: Instruction Tuning using Feedback from Large Language Models. (arXiv:2310.13385v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13385">http://arxiv.org/abs/2310.13385</a></li>
<li>Code URL: https://github.com/microsoft/lmops</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13385]] Tuna: Instruction Tuning using Feedback from Large Language Models(http://arxiv.org/abs/2310.13385)</code></li>
<li>Summary: <p>Instruction tuning of open-source large language models (LLMs) like LLaMA,
using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4,
has proven to be a cost-effective way to align model behaviors with human
preferences. However, the instruction-tuned model has only seen one response
per instruction, lacking the knowledge of potentially better responses. In this
paper, we propose finetuning an instruction-tuned LLM using our novel
\textit{probabilistic ranking} and \textit{contextual ranking} approaches to
increase the likelihood of generating better responses. Probabilistic ranking
enables the instruction-tuned model to inherit the relative rankings of
high-quality and low-quality responses from the teacher LLM. On the other hand,
learning with contextual ranking allows the model to refine its own response
distribution using the contextual understanding ability of stronger LLMs.
Furthermore, we apply probabilistic ranking and contextual ranking sequentially
to the instruction-tuned LLM. The resulting model, which we call \textbf{Tuna},
consistently improves the performance on Super Natural Instructions (119 test
tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results
than several strong reinforcement learning baselines. Our code and data are
available at \url{ https://github.com/microsoft/LMOps}.
</p></li>
</ul>

<h3>Title: POSQA: Probe the World Models of LLMs with Size Comparisons. (arXiv:2310.13394v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13394">http://arxiv.org/abs/2310.13394</a></li>
<li>Code URL: https://github.com/cambridgeltl/posqa</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13394]] POSQA: Probe the World Models of LLMs with Size Comparisons(http://arxiv.org/abs/2310.13394)</code></li>
<li>Summary: <p>Embodied language comprehension emphasizes that language understanding is not
solely a matter of mental processing in the brain but also involves
interactions with the physical and social environment. With the explosive
growth of Large Language Models (LLMs) and their already ubiquitous presence in
our daily lives, it is becoming increasingly necessary to verify their
real-world understanding. Inspired by cognitive theories, we propose POSQA: a
Physical Object Size Question Answering dataset with simple size comparison
questions to examine the extremity and analyze the potential mechanisms of the
embodied comprehension of the latest LLMs.
</p>
<p>We show that even the largest LLMs today perform poorly under the zero-shot
setting. We then push their limits with advanced prompting techniques and
external knowledge augmentation. Furthermore, we investigate whether their
real-world comprehension primarily derives from contextual information or
internal weights and analyse the impact of prompt formats and report bias of
different objects. Our results show that real-world understanding that LLMs
shaped from textual data can be vulnerable to deception and confusion by the
surface form of prompts, which makes it less aligned with human behaviours.
</p></li>
</ul>

<h3>Title: Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models. (arXiv:2310.13395v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13395">http://arxiv.org/abs/2310.13395</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13395]] Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models(http://arxiv.org/abs/2310.13395)</code></li>
<li>Summary: <p>Prompting Large Language Models (LLMs) performs impressively in zero- and
few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot
afford the cost of creating large task-specific training datasets, but also the
cost of pretraining their own LLMs, are increasingly turning to third-party
services that allow them to prompt LLMs. However, such services currently
require a payment per call, which becomes a significant operating expense
(OpEx). Furthermore, customer inputs are often very similar over time, hence
SMEs end-up prompting LLMs with very similar instances. We propose a framework
that allows reducing the calls to LLMs by caching previous LLM responses and
using them to train a local inexpensive model on the SME side. The framework
includes criteria for deciding when to trust the local model or call the LLM,
and a methodology to tune the criteria and measure the tradeoff between
performance and cost. For experimental purposes, we instantiate our framework
with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN
classifier or a Multi-Layer Perceptron, using two common business tasks, intent
recognition and sentiment analysis. Experimental results indicate that
significant OpEx savings can be obtained with only slightly lower performance.
</p></li>
</ul>

<h3>Title: Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations. (arXiv:2310.13420v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13420">http://arxiv.org/abs/2310.13420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13420]] Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations(http://arxiv.org/abs/2310.13420)</code></li>
<li>Summary: <p>In the field of natural language processing, open-domain chatbots have
emerged as an important research topic. However, a major limitation of existing
open-domain chatbot research is its singular focus on short single-session
dialogue, neglecting the potential need for understanding contextual
information in multiple consecutive sessions that precede an ongoing dialogue.
Among the elements that compose the context in multi-session conversation
settings, the time intervals between sessions and the relationships between
speakers would be particularly important. Despite their importance, current
research efforts have not sufficiently addressed these dialogical components.
In this paper, we introduce a new 1M multi-session dialogue dataset, called
Conversation Chronicles, for implementing a long-term conversation setup in
which time intervals and fine-grained speaker relationships are incorporated.
Following recent works, we exploit a large language model to produce the data.
The extensive human evaluation shows that dialogue episodes in Conversation
Chronicles reflect those properties while maintaining coherent and consistent
interactions across all the sessions. We also propose a dialogue model, called
ReBot, which consists of chronological summarization and dialogue generation
modules using only around 630M parameters. When trained on Conversation
Chronicles, ReBot demonstrates long-term context understanding with a high
human engagement score.
</p></li>
</ul>

<h3>Title: Self-Consistency of Large Language Models under Ambiguity. (arXiv:2310.13439v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13439">http://arxiv.org/abs/2310.13439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13439]] Self-Consistency of Large Language Models under Ambiguity(http://arxiv.org/abs/2310.13439)</code></li>
<li>Summary: <p>Large language models (LLMs) that do not give consistent answers across
contexts are problematic when used for tasks with expectations of consistency,
e.g., question-answering, explanations, etc. Our work presents an evaluation
benchmark for self-consistency in cases of under-specification where two or
more answers can be correct. We conduct a series of behavioral experiments on
the OpenAI model suite using an ambiguous integer sequence completion task. We
find that average consistency ranges from 67\% to 82\%, far higher than would
be predicted if a model's consistency was random, and increases as model
capability improves. Furthermore, we show that models tend to maintain
self-consistency across a series of robustness checks, including prompting
speaker changes and sequence length changes. These results suggest that
self-consistency arises as an emergent capability without specifically training
for it. Despite this, we find that models are uncalibrated when judging their
own consistency, with models displaying both over- and under-confidence. We
also propose a nonparametric test for determining from token output
distribution whether a model assigns non-trivial probability to alternative
answers. Using this test, we find that despite increases in self-consistency,
models usually place significant weight on alternative, inconsistent answers.
This distribution of probability mass provides evidence that even highly
self-consistent models internally compute multiple possible responses.
</p></li>
</ul>

<h3>Title: Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning. (arXiv:2310.13448v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13448">http://arxiv.org/abs/2310.13448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13448]] Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning(http://arxiv.org/abs/2310.13448)</code></li>
<li>Summary: <p>Large language models (LLMs) are a promising avenue for machine translation
(MT). However, current LLM-based MT systems are brittle: their effectiveness
highly depends on the choice of few-shot examples and they often require extra
post-processing due to overgeneration. Alternatives such as finetuning on
translation instructions are computationally expensive and may weaken
in-context learning capabilities, due to overspecialization. In this paper, we
provide a closer look at this problem. We start by showing that adapter-based
finetuning with LoRA matches the performance of traditional finetuning while
reducing the number of training parameters by a factor of 50. This method also
outperforms few-shot prompting and eliminates the need for post-processing or
in-context examples. However, we show that finetuning generally degrades
few-shot performance, hindering adaptation capabilities. Finally, to obtain the
best of both worlds, we propose a simple approach that incorporates few-shot
examples during finetuning. Experiments on 10 language pairs show that our
proposed approach recovers the original few-shot capabilities while keeping the
added benefits of finetuning.
</p></li>
</ul>

<h3>Title: Explaining Interactions Between Text Spans. (arXiv:2310.13506v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13506">http://arxiv.org/abs/2310.13506</a></li>
<li>Code URL: https://github.com/copenlu/spanex</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13506]] Explaining Interactions Between Text Spans(http://arxiv.org/abs/2310.13506)</code></li>
<li>Summary: <p>Reasoning over spans of tokens from different parts of the input is essential
for natural language understanding (NLU) tasks such as fact-checking (FC),
machine reading comprehension (MRC) or natural language inference (NLI).
However, existing highlight-based explanations primarily focus on identifying
individual important tokens or interactions only between adjacent tokens or
tuples of tokens. Most notably, there is a lack of annotations capturing the
human decision-making process w.r.t. the necessary interactions for informed
decision-making in such tasks. To bridge this gap, we introduce SpanEx, a
multi-annotator dataset of human span interaction explanations for two NLU
tasks: NLI and FC. We then investigate the decision-making processes of
multiple fine-tuned large language models in terms of the employed connections
between spans in separate parts of the input and compare them to the human
reasoning processes. Finally, we present a novel community detection based
unsupervised method to extract such interaction explanations from a model's
inner workings.
</p></li>
</ul>

<h3>Title: Teaching Language Models to Self-Improve through Interactive Demonstrations. (arXiv:2310.13522v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13522">http://arxiv.org/abs/2310.13522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13522]] Teaching Language Models to Self-Improve through Interactive Demonstrations(http://arxiv.org/abs/2310.13522)</code></li>
<li>Summary: <p>The self-improving ability of large language models (LLMs), enabled by
prompting them to analyze and revise their own outputs, has garnered
significant interest in recent research. However, this ability has been shown
to be absent and difficult to learn for smaller models, thus widening the
performance gap between state-of-the-art LLMs and more cost-effective and
faster ones. To reduce this gap, we introduce TriPosT, a training algorithm
that endows smaller models with such self-improvement ability, and show that
our approach can improve a LLaMA-7b's performance on math and reasoning tasks
by up to 7.13%. In contrast to prior work, we achieve this by using the smaller
model to interact with LLMs to collect feedback and improvements on its own
generations. We then replay this experience to train the small model. Our
experiments on four math and reasoning datasets show that the interactive
experience of learning from and correcting its own mistakes is crucial for
small models to improve their performance.
</p></li>
</ul>

<h3>Title: The Perils & Promises of Fact-checking with Large Language Models. (arXiv:2310.13549v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13549">http://arxiv.org/abs/2310.13549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13549]] The Perils & Promises of Fact-checking with Large Language Models(http://arxiv.org/abs/2310.13549)</code></li>
<li>Summary: <p>Autonomous fact-checking, using machine learning to verify claims, has grown
vital as misinformation spreads beyond human fact-checking capacity. Large
Language Models (LLMs) like GPT-4 are increasingly trusted to verify
information and write academic papers, lawsuits, and news articles, emphasizing
their role in discerning truth from falsehood and the importance of being able
to verify their outputs. Here, we evaluate the use of LLM agents in
fact-checking by having them phrase queries, retrieve contextual data, and make
decisions. Importantly, in our framework, agents explain their reasoning and
cite the relevant sources from the retrieved context. Our results show the
enhanced prowess of LLMs when equipped with contextual information. GPT-4
outperforms GPT-3, but accuracy varies based on query language and claim
veracity. While LLMs show promise in fact-checking, caution is essential due to
inconsistent accuracy. Our investigation calls for further research, fostering
a deeper comprehension of when agents succeed and when they fail.
</p></li>
</ul>

<h3>Title: Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning. (arXiv:2310.13552v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13552">http://arxiv.org/abs/2310.13552</a></li>
<li>Code URL: https://github.com/noewangjy/sp-cot</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13552]] Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning(http://arxiv.org/abs/2310.13552)</code></li>
<li>Summary: <p>In open-domain question-answering (ODQA), most existing questions require
single-hop reasoning on commonsense. To further extend this task, we officially
introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop
questions with explicit reasoning steps in open-domain setting. Recently, large
language models (LLMs) have found significant utility in facilitating ODQA
without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts
the reasoning capability of LLMs to a greater extent with manual or automated
paradigms. However, existing automated methods lack of quality assurance, while
manual approaches suffer from limited scalability and poor diversity, hindering
the capabilities of LLMs. In this paper, we propose Self-prompted
Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality
CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation
pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT
selection and self-prompted inference via in-context learning. Extensive
experiments on four multi-hop question-answering benchmarks show that our
proposed SP-CoT not only significantly surpasses the previous SOTA methods on
large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of
small-scale (13B) LLMs. Further analysis reveals the remarkable capability of
SP-CoT to elicit direct and concise intermediate reasoning steps by recalling
$\sim$50\% of intermediate answers on MuSiQue-Ans dataset.
</p></li>
</ul>

<h3>Title: Cache & Distil: Optimising API Calls to Large Language Models. (arXiv:2310.13561v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13561">http://arxiv.org/abs/2310.13561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13561]] Cache & Distil: Optimising API Calls to Large Language Models(http://arxiv.org/abs/2310.13561)</code></li>
<li>Summary: <p>Large-scale deployment of generative AI tools often depends on costly API
calls to a Large Language Model (LLM) to fulfil user queries. To curtail the
frequency of these calls, one can employ a smaller language model -- a student
-- which is continuously trained on the responses of the LLM. This student
gradually gains proficiency in independently handling an increasing number of
user requests, a process we term neural caching. The crucial element in neural
caching is a policy that decides which requests should be processed by the
student alone and which should be redirected to the LLM, subsequently aiding
the student's learning. In this study, we focus on classification tasks, and we
consider a range of classic active learning-based selection criteria as the
policy. Our experiments suggest that Margin Sampling and Query by Committee
bring consistent benefits across tasks and budgets.
</p></li>
</ul>

<h3>Title: Why Can Large Language Models Generate Correct Chain-of-Thoughts?. (arXiv:2310.13571v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13571">http://arxiv.org/abs/2310.13571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13571]] Why Can Large Language Models Generate Correct Chain-of-Thoughts?(http://arxiv.org/abs/2310.13571)</code></li>
<li>Summary: <p>This paper delves into the capabilities of large language models (LLMs),
specifically focusing on advancing the theoretical comprehension of
chain-of-thought prompting. We investigate how LLMs can be effectively induced
to generate a coherent chain of thoughts. To achieve this, we introduce a
two-level hierarchical graphical model tailored for natural language
generation. Within this framework, we establish a compelling geometrical
convergence rate that gauges the likelihood of an LLM-generated chain of
thoughts compared to those originating from the true language. Our findings
provide a theoretical justification for the ability of LLMs to produce the
correct sequence of thoughts (potentially) explaining performance gains in
tasks demanding reasoning skills.
</p></li>
</ul>

<h3>Title: MarineGPT: Unlocking Secrets of Ocean to the Public. (arXiv:2310.13596v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13596">http://arxiv.org/abs/2310.13596</a></li>
<li>Code URL: https://github.com/hkust-vgd/marinegpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13596]] MarineGPT: Unlocking Secrets of Ocean to the Public(http://arxiv.org/abs/2310.13596)</code></li>
<li>Summary: <p>Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be
powerful tools in promoting the user experience as an AI assistant. The
continuous works are proposing multi-modal large language models (MLLM),
empowering LLMs with the ability to sense multiple modality inputs through
constructing a joint semantic space (e.g. visual-text space). Though
significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in
domain-specific applications that required domain-specific knowledge and
expertise has been less conducted, especially for \textbf{marine domain}.
Different from general-purpose MLLMs, the marine-specific MLLM is required to
yield much more \textbf{sensitive}, \textbf{informative}, and
\textbf{scientific} responses. In this work, we demonstrate that the existing
MLLMs optimized on huge amounts of readily available general-purpose training
data show a minimal ability to understand domain-specific intents and then
generate informative and satisfactory responses. To address these issues, we
propose \textbf{MarineGPT}, the first vision-language model specially designed
for the marine domain, unlocking the secrets of the ocean to the public. We
present our \textbf{Marine-5M} dataset with more than 5 million marine
image-text pairs to inject domain-specific marine knowledge into our model and
achieve better marine vision and language alignment. Our MarineGPT not only
pushes the boundaries of marine understanding to the general public but also
offers a standard protocol for adapting a general-purpose assistant to
downstream domain-specific experts. We pave the way for a wide range of marine
applications while setting valuable data and pre-trained models for future
research in both academic and industrial communities.
</p></li>
</ul>

<h3>Title: Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning. (arXiv:2310.13615v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13615">http://arxiv.org/abs/2310.13615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13615]] Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning(http://arxiv.org/abs/2310.13615)</code></li>
<li>Summary: <p>Due to the remarkable language understanding and generation abilities of
large language models (LLMs), their use in educational applications has been
explored. However, little work has been done on investigating the pedagogical
ability of LLMs in helping students to learn mathematics. In this position
paper, we discuss the challenges associated with employing LLMs to enhance
students' mathematical problem-solving skills by providing adaptive feedback.
Apart from generating the wrong reasoning processes, LLMs can misinterpret the
meaning of the question, and also exhibit difficulty in understanding the given
questions' rationales when attempting to correct students' answers. Three
research questions are formulated.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models. (arXiv:2310.12995v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.12995">http://arxiv.org/abs/2310.12995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.12995]] Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models(http://arxiv.org/abs/2310.12995)</code></li>
<li>Summary: <p>This paper introduces a comprehensive approach for segmenting regions of
interest (ROI) in diverse medical imaging datasets, encompassing ultrasound, CT
scans, and X-ray images. The proposed method harnesses the capabilities of the
YOLOv8 model for approximate boundary box detection across modalities,
alongside the Segment Anything Model (SAM) and High Quality (HQ) SAM for fully
automatic and precise segmentation. To generate boundary boxes, the YOLOv8
model was trained using a limited set of 100 images and masks from each
modality. The results obtained from our approach are extensively computed and
analyzed, demonstrating its effectiveness and potential in medical image
analysis. Various evaluation metrics, including precision, recall, F1 score,
and Dice Score, were employed to quantify the accuracy of the segmentation
results. A comparative analysis was conducted to assess the individual and
combined performance of the YOLOv8, YOLOv8+SAM, and YOLOv8+HQ-SAM models. The
results indicate that the SAM model performs better than the other two models,
exhibiting higher segmentation accuracy and overall performance. While HQ-SAM
offers potential advantages, its incremental gains over the standard SAM model
may not justify the additional computational cost. The YOLOv8+SAM model shows
promise for enhancing medical image segmentation and its clinical implications.
</p></li>
</ul>

<h3>Title: Weakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models. (arXiv:2310.13026v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13026">http://arxiv.org/abs/2310.13026</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13026]] Weakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models(http://arxiv.org/abs/2310.13026)</code></li>
<li>Summary: <p>The rapid development of deep learning has driven significant progress in the
field of image semantic segmentation - a fundamental task in computer vision.
Semantic segmentation algorithms often depend on the availability of
pixel-level labels (i.e., masks of objects), which are expensive,
time-consuming, and labor-intensive. Weakly-supervised semantic segmentation
(WSSS) is an effective solution to avoid such labeling. It utilizes only
partial or incomplete annotations and provides a cost-effective alternative to
fully-supervised semantic segmentation. In this paper, we focus on the WSSS
with image-level labels, which is the most challenging form of WSSS. Our work
has two parts. First, we conduct a comprehensive survey on traditional methods,
primarily focusing on those presented at premier research conferences. We
categorize them into four groups based on where their methods operate:
pixel-wise, image-wise, cross-image, and external data. Second, we investigate
the applicability of visual foundation models, such as the Segment Anything
Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing
scenarios: text prompting and zero-shot learning. We provide insights into the
potential and challenges associated with deploying visual foundational models
for WSSS, facilitating future developments in this exciting research area.
</p></li>
</ul>

<h3>Title: FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery. (arXiv:2310.13336v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13336">http://arxiv.org/abs/2310.13336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13336]] FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery(http://arxiv.org/abs/2310.13336)</code></li>
<li>Summary: <p>We introduce the French Land cover from Aerospace ImageRy (FLAIR), an
extensive dataset from the French National Institute of Geographical and Forest
Information (IGN) that provides a unique and rich resource for large-scale
geospatial analysis. FLAIR contains high-resolution aerial imagery with a
ground sample distance of 20 cm and over 20 billion individually labeled pixels
for precise land-cover classification. The dataset also integrates temporal and
spectral data from optical satellite time series. FLAIR thus combines data with
varying spatial, spectral, and temporal resolutions across over 817 km2 of
acquisitions representing the full landscape diversity of France. This
diversity makes FLAIR a valuable resource for the development and evaluation of
novel methods for large-scale land-cover semantic segmentation and raises
significant challenges in terms of computer vision, data fusion, and geospatial
analysis. We also provide powerful uni- and multi-sensor baseline models that
can be employed to assess algorithm's performance and for downstream
applications. Through its extent and the quality of its annotation, FLAIR aims
to spur improvements in monitoring and understanding key anthropogenic
development indicators such as urban growth, deforestation, and soil
artificialization. Dataset and codes can be accessed at
https://ignf.github.io/FLAIR/
</p></li>
</ul>

<h3>Title: SILC: Improving Vision Language Pretraining with Self-Distillation. (arXiv:2310.13355v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13355">http://arxiv.org/abs/2310.13355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13355]] SILC: Improving Vision Language Pretraining with Self-Distillation(http://arxiv.org/abs/2310.13355)</code></li>
<li>Summary: <p>Image-Text pretraining on web-scale image caption dataset has become the
default recipe for open vocabulary classification and retrieval models thanks
to the success of CLIP and its variants. Several works have also used CLIP
features for dense prediction tasks and have shown the emergence of open-set
abilities. However, the contrastive objective only focuses on image-text
alignment and does not incentivise image feature learning for dense prediction
tasks. In this work, we propose the simple addition of local-to-global
correspondence learning by self-distillation as an additional objective for
contrastive pre-training to propose SILC. We show that distilling local image
features from an exponential moving average (EMA) teacher model significantly
improves model performance on several computer vision tasks including
classification, retrieval, and especially segmentation. We further show that
SILC scales better with the same training duration compared to the baselines.
Our model SILC sets a new state of the art for zero-shot classification, few
shot classification, image and text retrieval, zero-shot segmentation, and open
vocabulary segmentation.
</p></li>
</ul>

<h3>Title: Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation. (arXiv:2310.13479v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13479">http://arxiv.org/abs/2310.13479</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13479]] Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation(http://arxiv.org/abs/2310.13479)</code></li>
<li>Summary: <p>Referring Image Segmentation (RIS) - the problem of identifying objects in
images through natural language sentences - is a challenging task currently
mostly solved through supervised learning. However, while collecting referred
annotation masks is a time-consuming process, the few existing
weakly-supervised and zero-shot approaches fall significantly short in
performance compared to fully-supervised learning ones. To bridge the
performance gap without mask annotations, we propose a novel weakly-supervised
framework that tackles RIS by decomposing it into three steps: obtaining
instance masks for the object mentioned in the referencing instruction
(segment), using zero-shot learning to select a potentially correct mask for
the given instruction (select), and bootstrapping a model which allows for
fixing the mistakes of zero-shot selection (correct). In our experiments, using
only the first two steps (zero-shot segment and select) outperforms other
zero-shot baselines by as much as 19%, while our full method improves upon this
much stronger baseline and sets the new state-of-the-art for weakly-supervised
RIS, reducing the gap between the weakly-supervised and fully-supervised
methods in some cases from around 33% to as little as 14%. Code is available at
https://github.com/fgirbal/segment-select-correct.
</p></li>
</ul>

<h3>Title: A review of individual tree crown detection and delineation from optical remote sensing images. (arXiv:2310.13481v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13481">http://arxiv.org/abs/2310.13481</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13481]] A review of individual tree crown detection and delineation from optical remote sensing images(http://arxiv.org/abs/2310.13481)</code></li>
<li>Summary: <p>Powered by the advances of optical remote sensing sensors, the production of
very high spatial resolution multispectral images provides great potential for
achieving cost-efficient and high-accuracy forest inventory and analysis in an
automated way. Lots of studies that aim at providing an inventory to the level
of each individual tree have generated a variety of methods for Individual Tree
Crown Detection and Delineation (ITCD). This review covers ITCD methods for
detecting and delineating individual tree crowns, and systematically reviews
the past and present of ITCD-related researches applied to the optical remote
sensing images. With the goal to provide a clear knowledge map of existing ITCD
efforts, we conduct a comprehensive review of recent ITCD papers to build a
meta-data analysis, including the algorithm, the study site, the tree species,
the sensor type, the evaluation method, etc. We categorize the reviewed methods
into three classes: (1) traditional image processing methods (such as local
maximum filtering, image segmentation, etc.); (2) traditional machine learning
methods (such as random forest, decision tree, etc.); and (3) deep learning
based methods. With the deep learning-oriented approaches contributing a
majority of the papers, we further discuss the deep learning-based methods as
semantic segmentation and object detection methods. In addition, we discuss
four ITCD-related issues to further comprehend the ITCD domain using optical
remote sensing data, such as comparisons between multi-sensor based data and
optical data in ITCD domain, comparisons among different algorithms and
different ITCD tasks, etc. Finally, this review proposes some ITCD-related
applications and a few exciting prospects and potential hot topics in future
ITCD research.
</p></li>
</ul>

<h3>Title: Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation. (arXiv:2310.13533v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13533">http://arxiv.org/abs/2310.13533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13533]] Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation(http://arxiv.org/abs/2310.13533)</code></li>
<li>Summary: <p>The goal of the challenge is to develop a test-time adaptation (TTA) method,
which could adapt the model to gradually changing domains in video sequences
for semantic segmentation task. It is based on a synthetic driving video
dataset - SHIFT. The source model is trained on images taken during daytime in
clear weather. Domain changes at test-time are mainly caused by varying weather
conditions and times of day. The TTA methods are evaluated in each image
sequence (video) separately, meaning the model is reset to the source model
state before the next sequence. Images come one by one and a prediction has to
be made at the arrival of each frame. Each sequence is composed of 401 images
and starts with the source domain, then gradually drifts to a different one
(changing weather or time of day) until the middle of the sequence. In the
second half of the sequence, the domain gradually shifts back to the source
one. Ground truth data is available only for the validation split of the SHIFT
dataset, in which there are only six sequences that start and end with the
source domain. We conduct an analysis specifically on those sequences. Ground
truth data for test split, on which the developed TTA methods are evaluated for
leader board ranking, are not publicly available.
</p>
<p>The proposed solution secured a 3rd place in a challenge and received an
innovation award. Contrary to the solutions that scored better, we did not use
any external pretrained models or specialized data augmentations, to keep the
solutions as general as possible. We have focused on analyzing the
distributional shift and developing a method that could adapt to changing data
dynamics and generalize across different scenarios.
</p></li>
</ul>

<h3>Title: ROSS: Radar Off-road Semantic Segmentation. (arXiv:2310.13551v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13551">http://arxiv.org/abs/2310.13551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13551]] ROSS: Radar Off-road Semantic Segmentation(http://arxiv.org/abs/2310.13551)</code></li>
<li>Summary: <p>As the demand for autonomous navigation in off-road environments increases,
the need for effective solutions to understand these surroundings becomes
essential. In this study, we confront the inherent complexities of semantic
segmentation in RADAR data for off-road scenarios. We present a novel pipeline
that utilizes LIDAR data and an existing annotated off-road LIDAR dataset for
generating RADAR labels, in which the RADAR data are represented as images.
Validated with real-world datasets, our pragmatic approach underscores the
potential of RADAR technology for navigation applications in off-road
environments.
</p></li>
</ul>

<h3>Title: Accelerated sparse Kernel Spectral Clustering for large scale data clustering problems. (arXiv:2310.13381v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.13381">http://arxiv.org/abs/2310.13381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.13381]] Accelerated sparse Kernel Spectral Clustering for large scale data clustering problems(http://arxiv.org/abs/2310.13381)</code></li>
<li>Summary: <p>An improved version of the sparse multiway kernel spectral clustering (KSC)
is presented in this brief. The original algorithm is derived from weighted
kernel principal component (KPCA) analysis formulated within the primal-dual
least-squares support vector machine (LS-SVM) framework. Sparsity is achieved
then by the combination of the incomplete Cholesky decomposition (ICD) based
low rank approximation of the kernel matrix with the so called reduced set
method. The original ICD based sparse KSC algorithm was reported to be
computationally far too demanding, especially when applied on large scale data
clustering problems that actually it was designed for, which has prevented to
gain more than simply theoretical relevance so far. This is altered by the
modifications reported in this brief that drastically improve the computational
characteristics. Solving the alternative, symmetrized version of the
computationally most demanding core eigenvalue problem eliminates the necessity
of forming and SVD of large matrices during the model construction. This
results in solving clustering problems now within seconds that were reported to
require hours without altering the results. Furthermore, sparsity is also
improved significantly, leading to more compact model representation,
increasing further not only the computational efficiency but also the
descriptive power. These transform the original, only theoretically relevant
ICD based sparse KSC algorithm applicable for large scale practical clustering
problems. Theoretical results and improvements are demonstrated by
computational experiments on carefully selected synthetic data as well as on
real life problems such as image segmentation.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
