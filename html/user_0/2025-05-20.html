<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-20</h1>
<h3>Title: Hybrid Privacy Policy-Code Consistency Check using Knowledge Graphs and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Mao, Xinxin Fan, Yifei Wang, Jacky Keung, Jialong Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11502">https://arxiv.org/abs/2505.11502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11502">https://arxiv.org/pdf/2505.11502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11502]] Hybrid Privacy Policy-Code Consistency Check using Knowledge Graphs and LLMs(https://arxiv.org/abs/2505.11502)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The increasing concern in user privacy misuse has accelerated research into checking consistencies between smartphone apps' declared privacy policies and their actual behaviors. Recent advances in Large Language Models (LLMs) have introduced promising techniques for semantic comparison, but these methods often suffer from low accuracies and expensive computational costs. To address this problem, this paper proposes a novel hybrid approach that integrates 1) knowledge graph-based deterministic checking to ensure higher accuracy, and 2) LLMs exclusively used for preliminary semantic analysis to save computational costs. Preliminary evaluation indicates this hybrid approach not only achieves 37.63% increase in precision and 23.13% increase F1-score but also consumes 93.5% less tokens and 87.3% shorter time.</li>
</ul>

<h3>Title: Improving Open-Set Semantic Segmentation in 3D Point Clouds by Conditional Channel Capacity Maximization: Preliminary Results</h3>
<ul>
<li><strong>Authors: </strong>Wang Fang, Shirin Rahimi, Olivia Bennett, Sophie Carter, Mitra Hassani, Xu Lan, Omid Javadi, Lucas Mitchell</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11521">https://arxiv.org/abs/2505.11521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11521">https://arxiv.org/pdf/2505.11521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11521]] Improving Open-Set Semantic Segmentation in 3D Point Clouds by Conditional Channel Capacity Maximization: Preliminary Results(https://arxiv.org/abs/2505.11521)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Point-cloud semantic segmentation underpins a wide range of critical applications. Although recent deep architectures and large-scale datasets have driven impressive closed-set performance, these models struggle to recognize or properly segment objects outside their training classes. This gap has sparked interest in Open-Set Semantic Segmentation (O3S), where models must both correctly label known categories and detect novel, unseen classes. In this paper, we propose a plug and play framework for O3S. By modeling the segmentation pipeline as a conditional Markov chain, we derive a novel regularizer term dubbed Conditional Channel Capacity Maximization (3CM), that maximizes the mutual information between features and predictions conditioned on each class. When incorporated into standard loss functions, 3CM encourages the encoder to retain richer, label-dependent features, thereby enhancing the network's ability to distinguish and segment previously unseen categories. Experimental results demonstrate effectiveness of proposed method on detecting unseen objects. We further outline future directions for dynamic open-world adaptation and efficient information-theoretic estimation.</li>
</ul>

<h3>Title: A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism</h3>
<ul>
<li><strong>Authors: </strong>Jinqiang Wang, Huansheng Ning, Tao Zhu, Jianguo Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11533">https://arxiv.org/abs/2505.11533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11533">https://arxiv.org/pdf/2505.11533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11533]] A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism(https://arxiv.org/abs/2505.11533)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the tourism domain, Large Language Models (LLMs) often struggle to mine implicit user intentions from tourists' ambiguous inquiries and lack the capacity to proactively guide users toward clarifying their needs. A critical bottleneck is the scarcity of high-quality training datasets that facilitate proactive questioning and implicit intention mining. While recent advances leverage LLM-driven data synthesis to generate such datasets and transfer specialized knowledge to downstream models, existing approaches suffer from several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed distributions of detail levels in initial inquiries, (3) contextual redundancy in the implicit intention mining module, and (4) lack of explicit thinking about tourists' emotions and intention values. Therefore, we propose SynPT (A Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User Intentions in the Tourism), which constructs an LLM-driven user agent and assistant agent to simulate dialogues based on seed data collected from Chinese tourism websites. This approach addresses the aforementioned limitations and generates SynPT-Dialog, a training dataset containing explicit reasoning. The dataset is utilized to fine-tune a general LLM, enabling it to proactively mine implicit user intentions. Experimental evaluations, conducted from both human and LLM perspectives, demonstrate the superiority of SynPT compared to existing methods. Furthermore, we analyze key hyperparameters and present case studies to illustrate the practical applicability of our method, including discussions on its adaptability to English-language scenarios. All code and data are publicly available.</li>
</ul>

<h3>Title: MorphMark: Flexible Adaptive Watermarking for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zongqi Wang, Tianle Gu, Baoyuan Wu, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11541">https://arxiv.org/abs/2505.11541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11541">https://arxiv.org/pdf/2505.11541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11541]] MorphMark: Flexible Adaptive Watermarking for Large Language Models(https://arxiv.org/abs/2505.11541)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>Watermarking by altering token sampling probabilities based on red-green list is a promising method for tracing the origin of text generated by large language models (LLMs). However, existing watermark methods often struggle with a fundamental dilemma: improving watermark effectiveness (the detectability of the watermark) often comes at the cost of reduced text quality. This trade-off limits their practical application. To address this challenge, we first formalize the problem within a multi-objective trade-off analysis framework. Within this framework, we identify a key factor that influences the dilemma. Unlike existing methods, where watermark strength is typically treated as a fixed hyperparameter, our theoretical insights lead to the development of MorphMarka method that adaptively adjusts the watermark strength in response to changes in the identified factor, thereby achieving an effective resolution of the dilemma. In addition, MorphMark also prioritizes flexibility since it is a model-agnostic and model-free watermark method, thereby offering a practical solution for real-world deployment, particularly in light of the rapid evolution of AI models. Extensive experiments demonstrate that MorphMark achieves a superior resolution of the effectiveness-quality dilemma, while also offering greater flexibility and time and space efficiency.</li>
</ul>

<h3>Title: Cybersecurity threat detection based on a UEBA framework using Deep Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Jose Fuentes, Ines Ortega-Fernandez, Nora M. Villanueva, Marta Sestelo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11542">https://arxiv.org/abs/2505.11542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11542">https://arxiv.org/pdf/2505.11542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11542]] Cybersecurity threat detection based on a UEBA framework using Deep Autoencoders(https://arxiv.org/abs/2505.11542)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>User and Entity Behaviour Analytics (UEBA) is a broad branch of data analytics that attempts to build a normal behavioural profile in order to detect anomalous events. Among the techniques used to detect anomalies, Deep Autoencoders constitute one of the most promising deep learning models on UEBA tasks, allowing explainable detection of security incidents that could lead to the leak of personal data, hijacking of systems, or access to sensitive business information. In this study, we introduce the first implementation of an explainable UEBA-based anomaly detection framework that leverages Deep Autoencoders in combination with Doc2Vec to process both numerical and textual features. Additionally, based on the theoretical foundations of neural networks, we offer a novel proof demonstrating the equivalence of two widely used definitions for fully-connected neural networks. The experimental results demonstrate the proposed framework capability to detect real and synthetic anomalies effectively generated from real attack data, showing that the models provide not only correct identification of anomalies but also explainable results that enable the reconstruction of the possible origin of the anomaly. Our findings suggest that the proposed UEBA framework can be seamlessly integrated into enterprise environments, complementing existing security systems for explainable threat detection.</li>
</ul>

<h3>Title: On Technique Identification and Threat-Actor Attribution using LLMs and Embedding Models</h3>
<ul>
<li><strong>Authors: </strong>Kyla Guru, Robert J. Moss, Mykel J. Kochenderfer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11547">https://arxiv.org/abs/2505.11547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11547">https://arxiv.org/pdf/2505.11547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11547]] On Technique Identification and Threat-Actor Attribution using LLMs and Embedding Models(https://arxiv.org/abs/2505.11547)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Attribution of cyber-attacks remains a complex but critical challenge for cyber defenders. Currently, manual extraction of behavioral indicators from dense forensic documentation causes significant attribution delays, especially following major incidents at the international scale. This research evaluates large language models (LLMs) for cyber-attack attribution based on behavioral indicators extracted from forensic documentation. We test OpenAI's GPT-4 and text-embedding-3-large for identifying threat actors' tactics, techniques, and procedures (TTPs) by comparing LLM-generated TTPs against human-generated data from MITRE ATT&CK Groups. Our framework then identifies TTPs from text using vector embedding search and builds profiles to attribute new attacks for a machine learning model to learn. Key contributions include: (1) assessing off-the-shelf LLMs for TTP extraction and attribution, and (2) developing an end-to-end pipeline from raw CTI documents to threat-actor prediction. This research finds that standard LLMs generate TTP datasets with noise, resulting in a low similarity to human-generated datasets. However, the TTPs generated are similar in frequency to those within the existing MITRE datasets. Additionally, although these TTPs are different than human-generated datasets, our work demonstrates that they still prove useful for training a model that performs above baseline on attribution. Project code and files are contained here: this https URL.</li>
</ul>

<h3>Title: One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Chang, Xiaojun Jia, Mingyang Li, Junjie Wang, Yuekai Huang, Qing Wang, Ziyou Jiang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11548">https://arxiv.org/abs/2505.11548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11548">https://arxiv.org/pdf/2505.11548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11548]] One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2505.11548)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) have shown improved performance in generating accurate responses. However, the dependence on external knowledge bases introduces potential security vulnerabilities, particularly when these knowledge bases are publicly accessible and modifiable. Poisoning attacks on knowledge bases for RAG systems face two fundamental challenges: the injected malicious content must compete with multiple authentic documents retrieved by the retriever, and LLMs tend to trust retrieved information that aligns with their internal memorized knowledge. Previous works attempt to address these challenges by injecting multiple malicious documents, but such saturation attacks are easily detectable and impractical in real-world scenarios. To enable the effective single document poisoning attack, we propose AuthChain, a novel knowledge poisoning attack method that leverages Chain-of-Evidence theory and authority effect to craft more convincing poisoned documents. AuthChain generates poisoned content that establishes strong evidence chains and incorporates authoritative statements, effectively overcoming the interference from both authentic documents and LLMs' internal knowledge. Extensive experiments across six popular LLMs demonstrate that AuthChain achieves significantly higher attack success rates while maintaining superior stealthiness against RAG defense mechanisms compared to state-of-the-art baselines.</li>
</ul>

<h3>Title: Managerial Insights on Investment Strategy in Cybersecurity: Findings from Multi-Country Research</h3>
<ul>
<li><strong>Authors: </strong>Silvia Tedeschi, Giacomo Marzi, Marco Balzano, Gabriele Costa</a></li>
<li><strong>Subjects: </strong>cs.CR, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11549">https://arxiv.org/abs/2505.11549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11549">https://arxiv.org/pdf/2505.11549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11549]] Managerial Insights on Investment Strategy in Cybersecurity: Findings from Multi-Country Research(https://arxiv.org/abs/2505.11549)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This study examines the strategic role of cybersecurity based on survey data from 1,083 managers across Europe, the UK, and the United States. The findings indicate growing recognition of cybersecurity as a source of competitive advantage, although firms continue to face barriers such as limited resources, talent shortages, and cultural resistance. Larger and high-tech firms tend to adopt more proactive strategies, while SMEs and low-tech sectors display greater variability. Key managerial tensions emerge in balancing security with innovation and agility. Notable country-level differences are observed across Europe, the UK, and the United States. Across all contexts, leadership and employee engagement appear central to closing the gap between strategic intent and operational practice.</li>
</ul>

<h3>Title: AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification</h3>
<ul>
<li><strong>Authors: </strong>Harika Abburi, Sanmitra Bhattacharya, Edward Bowen, Nirmala Pudota</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11550">https://arxiv.org/abs/2505.11550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11550">https://arxiv.org/pdf/2505.11550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11550]] AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification(https://arxiv.org/abs/2505.11550)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across a wide range of styles and genres. However, such capabilities are prone to potential misuse, such as fake news generation, spam email creation, and misuse in academic assignments. As a result, accurate detection of AI-generated text and identification of the model that generated it are crucial for maintaining the responsible use of LLMs. In this work, we addressed two sub-tasks put forward by the Defactify workshop under AI-Generated Text Detection shared task at the Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A involved distinguishing between human-authored or AI-generated text, while Task B focused on attributing text to its originating language model. For each task, we proposed two neural architectures: an optimized model and a simpler variant. For Task A, the optimized neural architecture achieved fifth place with $F1$ score of 0.994, and for Task B, the simpler neural architecture also ranked fifth place with $F1$ score of 0.627.</li>
</ul>

<h3>Title: A Survey of Learning-Based Intrusion Detection Systems for In-Vehicle Network</h3>
<ul>
<li><strong>Authors: </strong>Muzun Althunayyan, Amir Javed, Omer Rana</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11551">https://arxiv.org/abs/2505.11551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11551">https://arxiv.org/pdf/2505.11551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11551]] A Survey of Learning-Based Intrusion Detection Systems for In-Vehicle Network(https://arxiv.org/abs/2505.11551)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Connected and Autonomous Vehicles (CAVs) enhance mobility but face cybersecurity threats, particularly through the insecure Controller Area Network (CAN) bus. Cyberattacks can have devastating consequences in connected vehicles, including the loss of control over critical systems, necessitating robust security solutions. In-vehicle Intrusion Detection Systems (IDSs) offer a promising approach by detecting malicious activities in real time. This survey provides a comprehensive review of state-of-the-art research on learning-based in-vehicle IDSs, focusing on Machine Learning (ML), Deep Learning (DL), and Federated Learning (FL) approaches. Based on the reviewed studies, we critically examine existing IDS approaches, categorising them by the types of attacks they detect - known, unknown, and combined known-unknown attacks - while identifying their limitations. We also review the evaluation metrics used in research, emphasising the need to consider multiple criteria to meet the requirements of safety-critical systems. Additionally, we analyse FL-based IDSs and highlight their limitations. By doing so, this survey helps identify effective security measures, address existing limitations, and guide future research toward more resilient and adaptive protection mechanisms, ensuring the safety and reliability of CAVs.</li>
</ul>

<h3>Title: Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Li, Aoi Naito, Hirokazu Shirado</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11556">https://arxiv.org/abs/2505.11556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11556">https://arxiv.org/pdf/2505.11556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11556]] Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks(https://arxiv.org/abs/2505.11556)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving through distributed information integration, but also risk replicating collective reasoning failures observed in human groups. Yet, no theory-grounded benchmark exists to systematically evaluate such failures. In this paper, we introduce the Hidden Profile paradigm from social psychology as a diagnostic testbed for multi-agent LLM systems. By distributing critical information asymmetrically across agents, the paradigm reveals how inter-agent dynamics support or hinder collective reasoning. We first formalize the paradigm for multi-agent decision-making under distributed knowledge and instantiate it as a benchmark with nine tasks spanning diverse scenarios, including adaptations from prior human studies. We then conduct experiments with GPT-4.1 and five other leading LLMs, including reasoning-enhanced variants, showing that multi-agent systems across all models fail to match the accuracy of single agents given complete information. While agents' collective performance is broadly comparable to that of human groups, nuanced behavioral differences emerge, such as increased sensitivity to social desirability. Finally, we demonstrate the paradigm's diagnostic utility by exploring a cooperation-contradiction trade-off in multi-agent LLM systems. We find that while cooperative agents are prone to over-coordination in collective settings, increased contradiction impairs group convergence. This work contributes a reproducible framework for evaluating multi-agent LLM systems and motivates future research on artificial collective intelligence and human-AI interaction.</li>
</ul>

<h3>Title: HessFormer: Hessians at Foundation Scale</h3>
<ul>
<li><strong>Authors: </strong>Diego Granziol</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11564">https://arxiv.org/abs/2505.11564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11564">https://arxiv.org/pdf/2505.11564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11564]] HessFormer: Hessians at Foundation Scale(https://arxiv.org/abs/2505.11564)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Whilst there have been major advancements in the field of first order optimisation of deep learning models, where state of the art open source mixture of expert models go into the hundreds of billions of parameters, methods that rely on Hessian vector products, are still limited to run on a single GPU and thus cannot even work for models in the billion parameter range. We release a software package \textbf{HessFormer}, which integrates nicely with the well known Transformers package and allows for distributed hessian vector computation across a single node with multiple GPUs. Underpinning our implementation is a distributed stochastic lanczos quadrature algorithm, which we release for public consumption. Using this package we investigate the Hessian spectral density of the recent Deepseek $70$bn parameter model.</li>
</ul>

<h3>Title: ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?</h3>
<ul>
<li><strong>Authors: </strong>Sarthak Munshi, Swapnil Pathak, Sonam Ghatode, Thenuga Priyadarshini, Dhivya Chandramouleeswaran, Ashutosh Rana</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11565">https://arxiv.org/abs/2505.11565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11565">https://arxiv.org/pdf/2505.11565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11565]] ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?(https://arxiv.org/abs/2505.11565)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models have shown promise in cybersecurity applications, their effectiveness in identifying security threats within cloud deployments remains unexplored. This paper introduces AWS Cloud Security Engineering Eval, a novel dataset for evaluating LLMs cloud security threat modeling capabilities. ACSE-Eval contains 100 production grade AWS deployment scenarios, each featuring detailed architectural specifications, Infrastructure as Code implementations, documented security vulnerabilities, and associated threat modeling parameters. Our dataset enables systemic assessment of LLMs abilities to identify security risks, analyze attack vectors, and propose mitigation strategies in cloud environments. Our evaluations on ACSE-Eval demonstrate that GPT 4.1 and Gemini 2.5 Pro excel at threat identification, with Gemini 2.5 Pro performing optimally in 0-shot scenarios and GPT 4.1 showing superior results in few-shot settings. While GPT 4.1 maintains a slight overall performance advantage, Claude 3.7 Sonnet generates the most semantically sophisticated threat models but struggles with threat categorization and generalization. To promote reproducibility and advance research in automated cybersecurity threat analysis, we open-source our dataset, evaluation metrics, and methodologies.</li>
</ul>

<h3>Title: Towards Adaptive Deep Learning: Model Elasticity via Prune-and-Grow CNN Architectures</h3>
<ul>
<li><strong>Authors: </strong>Pooja Mangal, Sudaksh Kalra, Dolly Sapra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11569">https://arxiv.org/abs/2505.11569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11569">https://arxiv.org/pdf/2505.11569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11569]] Towards Adaptive Deep Learning: Model Elasticity via Prune-and-Grow CNN Architectures(https://arxiv.org/abs/2505.11569)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deploying deep convolutional neural networks (CNNs) on resource-constrained devices presents significant challenges due to their high computational demands and rigid, static architectures. To overcome these limitations, this thesis explores methods for enabling CNNs to dynamically adjust their computational complexity based on available hardware resources. We introduce adaptive CNN architectures capable of scaling their capacity at runtime, thus efficiently balancing performance and resource utilization. To achieve this adaptability, we propose a structured pruning and dynamic re-construction approach that creates nested subnetworks within a single CNN model. This approach allows the network to dynamically switch between compact and full-sized configurations without retraining, making it suitable for deployment across varying hardware platforms. Experiments conducted across multiple CNN architectures including VGG-16, AlexNet, ResNet-20, and ResNet-56 on CIFAR-10 and Imagenette datasets demonstrate that adaptive models effectively maintain or even enhance performance under varying computational constraints. Our results highlight that embedding adaptability directly into CNN architectures significantly improves their robustness and flexibility, paving the way for efficient real-world deployment in diverse computational environments.</li>
</ul>

<h3>Title: Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Chongyang Tan, Ruoqi Wen, Rongpeng Li, Zhifeng Zhao, Ekram Hossain, Honggang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11570">https://arxiv.org/abs/2505.11570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11570">https://arxiv.org/pdf/2505.11570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11570]] Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning(https://arxiv.org/abs/2505.11570)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, generative, large language model</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables distributed model training across edge devices in a privacy-friendly manner. However, its efficiency heavily depends on effective device selection and high-dimensional resource allocation in dynamic and heterogeneous wireless environments. Conventional methods demand a confluence of domain-specific expertise, extensive hyperparameter tuning, and/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary Large Language Model (T-ELLM) framework to generate a qualified policy for device selection in a wireless FL environment. Unlike conventional optimization methods, T-ELLM leverages natural language-based scenario prompts to enhance generalization across varying network conditions. The framework decouples the joint optimization problem mathematically, enabling tractable learning of device selection policies while delegating resource allocation to convex optimization tools. To improve adaptability, T-ELLM integrates a sample-efficient, model-based virtual learning environment that captures the relationship between device selection and learning performance, facilitating subsequent group relative policy optimization. This concerted approach reduces reliance on real-world interactions, minimizing communication overhead while maintaining high-fidelity decision-making. Theoretical analysis proves that the discrepancy between virtual and real environments is bounded, ensuring the advantage function learned in the virtual environment maintains a provably small deviation from real-world conditions. Experimental results demonstrate that T-ELLM outperforms benchmark methods in energy efficiency and exhibits robust adaptability to environmental changes.</li>
</ul>

<h3>Title: InfiJanice: Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhen Li, Yupeng Su, Songmiao Wang, Runming Yang, Congkai Xie, Aofan Liu, Ming Li, Jiannong Cao, Yuan Xie, Ngai Wong, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11574">https://arxiv.org/abs/2505.11574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11574">https://arxiv.org/pdf/2505.11574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11574]] InfiJanice: Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models(https://arxiv.org/abs/2505.11574)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive performance on complex reasoning benchmarks such as GSM8K, MATH, and AIME. However, the substantial computational demands of these tasks pose significant challenges for real-world deployment. Model quantization has emerged as a promising approach to reduce memory footprint and inference latency by representing weights and activations with lower bit-widths. In this work, we conduct a comprehensive study of mainstream quantization methods(e.g., AWQ, GPTQ, SmoothQuant) on the most popular open-sourced models (e.g., Qwen2.5, LLaMA3 series), and reveal that quantization can degrade mathematical reasoning accuracy by up to 69.81%. To better understand this degradation, we develop an automated assignment and judgment pipeline that qualitatively categorizes failures into four error types and quantitatively identifies the most impacted reasoning capabilities. Building on these findings, we employ an automated data-curation pipeline to construct a compact "Silver Bullet" datasets. Training a quantized model on as few as 332 carefully selected examples for just 3-5 minutes on a single GPU is enough to restore its reasoning accuracy to match that of the full-precision baseline.</li>
</ul>

<h3>Title: Concept-Guided Interpretability via Neural Chunking</h3>
<ul>
<li><strong>Authors: </strong>Shuchen Wu, Stephan Alaniz, Shyamgopal Karthik, Peter Dayan, Eric Schulz, Zeynep Akata</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11576">https://arxiv.org/abs/2505.11576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11576">https://arxiv.org/pdf/2505.11576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11576]] Concept-Guided Interpretability via Neural Chunking(https://arxiv.org/abs/2505.11576)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Neural networks are often black boxes, reflecting the significant challenge of understanding their internal workings. We propose a different perspective that challenges the prevailing view: rather than being inscrutable, neural networks exhibit patterns in their raw population activity that mirror regularities in the training data. We refer to this as the Reflection Hypothesis and provide evidence for this phenomenon in both simple recurrent neural networks (RNNs) and complex large language models (LLMs). Building on this insight, we propose to leverage cognitively-inspired methods of chunking to segment high-dimensional neural population dynamics into interpretable units that reflect underlying concepts. We propose three methods to extract these emerging entities, complementing each other based on label availability and dimensionality. Discrete sequence chunking (DSC) creates a dictionary of entities; population averaging (PA) extracts recurring entities that correspond to known labels; and unsupervised chunk discovery (UCD) can be used when labels are absent. We demonstrate the effectiveness of these methods in extracting entities across varying model sizes, ranging from inducing compositionality in RNNs to uncovering recurring neural population states in large models with diverse architectures, and illustrate their advantage over other methods. Throughout, we observe a robust correspondence between the extracted entities and concrete or abstract concepts. Artificially inducing the extracted entities in neural populations effectively alters the network's generation of associated concepts. Our work points to a new direction for interpretability, one that harnesses both cognitive principles and the structure of naturalistic data to reveal the hidden computations of complex learning systems, gradually transforming them from black boxes into systems we can begin to understand.</li>
</ul>

<h3>Title: Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Peimian Du, Jiabin Liu, Xiaowei Jin, Mengwang Zuo, Hui Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11578">https://arxiv.org/abs/2505.11578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11578">https://arxiv.org/pdf/2505.11578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11578]] Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning(https://arxiv.org/abs/2505.11578)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This research confronts the challenge of substantial physical equation discrepancies encountered in the generation of spatiotemporal physical fields through data-driven trained models. A spatiotemporal physical field generation model, named HMT-PF, is developed based on the hybrid Mamba-Transformer architecture, incorporating unstructured grid information as input. A fine-tuning block, enhanced with physical information, is introduced to effectively reduce the physical equation discrepancies. The physical equation residuals are computed through a point query mechanism for efficient gradient evaluation, then encoded into latent space for refinement. The fine-tuning process employs a self-supervised learning approach to achieve physical consistency while maintaining essential field characteristics. Results show that the hybrid Mamba-Transformer model achieves good performance in generating spatiotemporal fields, while the physics-informed fine-tuning mechanism further reduces significant physical errors effectively. A MSE-R evaluation method is developed to assess the accuracy and realism of physical field generation.</li>
</ul>

<h3>Title: Flash Invariant Point Attention</h3>
<ul>
<li><strong>Authors: </strong>Andrew Liu, Axel Elaldi, Nicholas T Franklin, Nathan Russell, Gurinder S Atwal, Yih-En A Ban, Olivia Viessmann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11580">https://arxiv.org/abs/2505.11580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11580">https://arxiv.org/pdf/2505.11580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11580]] Flash Invariant Point Attention(https://arxiv.org/abs/2505.11580)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at this https URL.</li>
</ul>

<h3>Title: Scaling an ISO Compliance Practice: Strategic Insights from Building a \$1m+ Cybersecurity Certification Line</h3>
<ul>
<li><strong>Authors: </strong>Nishant Sonkar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11583">https://arxiv.org/abs/2505.11583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11583">https://arxiv.org/pdf/2505.11583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11583]] Scaling an ISO Compliance Practice: Strategic Insights from Building a \$1m+ Cybersecurity Certification Line(https://arxiv.org/abs/2505.11583)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>The rapid exponential growth in cloud-first business models and tightened global data protection regulations have led to the exponential increase in the level of importance of ISO certifications, especially ISO/IEC 27001, 27017, and 27018, as strategic imperative propositions for organizations wanting to build trust, ensure compliance, and achieve a competitive advantage. This article describes a case study of a successful design, implementation, and scaling of a cybersecurity certification practice in Armanino LLP, a pioneering US accounting and consulting firm. In reaction to increasing client desires for formalized information security frameworks, I founded an industry practice from conception through implementation to aid mid-market and high-growth technology firms. During one year, the initiative brought in over \$1 million in new service revenue, expanded our portfolio of cybersecurity clients by 150%, and produced more than 20 successful ISO certifications on various verticals such as SaaS, healthcare, and fintech. Based on the strategic wisdom and operational strategy, this paper outlines the technical architecture of the ISO service line from modular audit templates to certification readiness kits, from stakeholder enablement to integration with SOC 2 and CIS controls. The approach gave value to repeatability, speed, and assurance, thus making Armanino a reputable certification body. The lessons drawn out provide us with a flexible template that can be utilized by firms wishing to build strong compliance programs that can be tailored to address changing digital risk terrains. This work adds to the increasing knowledge about audit scalability, cybersecurity compliance, and ISO standardisation.</li>
</ul>

<h3>Title: The Ripple Effect: On Unforeseen Complications of Backdoor Attacks</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhang, Yun Shen, Hongwei Li, Wenbo Jiang, Hanxiao Chen, Yuan Zhang, Guowen Xu, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11586">https://arxiv.org/abs/2505.11586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11586">https://arxiv.org/pdf/2505.11586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11586]] The Ripple Effect: On Unforeseen Complications of Backdoor Attacks(https://arxiv.org/abs/2505.11586)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>Recent research highlights concerns about the trustworthiness of third-party Pre-Trained Language Models (PTLMs) due to potential backdoor attacks. These backdoored PTLMs, however, are effective only for specific pre-defined downstream tasks. In reality, these PTLMs can be adapted to many other unrelated downstream tasks. Such adaptation may lead to unforeseen consequences in downstream model outputs, consequently raising user suspicion and compromising attack stealthiness. We refer to this phenomenon as backdoor complications. In this paper, we undertake the first comprehensive quantification of backdoor complications. Through extensive experiments using 4 prominent PTLMs and 16 text classification benchmark datasets, we demonstrate the widespread presence of backdoor complications in downstream models fine-tuned from backdoored PTLMs. The output distribution of triggered samples significantly deviates from that of clean samples. Consequently, we propose a backdoor complication reduction method leveraging multi-task learning to mitigate complications without prior knowledge of downstream tasks. The experimental results demonstrate that our proposed method can effectively reduce complications while maintaining the efficacy and consistency of backdoor attacks. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Training Framework for Optimal and Stable Training of Polynomial Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Forsad Al Hossain, Tauhidur Rahman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11589">https://arxiv.org/abs/2505.11589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11589">https://arxiv.org/pdf/2505.11589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11589]] A Training Framework for Optimal and Stable Training of Polynomial Neural Networks(https://arxiv.org/abs/2505.11589)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust</a></li>
<li><strong>Abstract: </strong>By replacing standard non-linearities with polynomial activations, Polynomial Neural Networks (PNNs) are pivotal for applications such as privacy-preserving inference via Homomorphic Encryption (HE). However, training PNNs effectively presents a significant challenge: low-degree polynomials can limit model expressivity, while higher-degree polynomials, crucial for capturing complex functions, often suffer from numerical instability and gradient explosion. We introduce a robust and versatile training framework featuring two synergistic innovations: 1) a novel Boundary Loss that exponentially penalizes activation inputs outside a predefined stable range, and 2) Selective Gradient Clipping that effectively tames gradient magnitudes while preserving essential Batch Normalization statistics. We demonstrate our framework's broad efficacy by training PNNs within deep architectures composed of HE-compatible layers (e.g., linear layers, average pooling, batch normalization, as used in ResNet variants) across diverse image, audio, and human activity recognition datasets. These models consistently achieve high accuracy with low-degree polynomial activations (such as degree 2) and, critically, exhibit stable training and strong performance with polynomial degrees up to 22, where standard methods typically fail or suffer severe degradation. Furthermore, the performance of these PNNs achieves a remarkable parity, closely approaching that of their original ReLU-based counterparts. Extensive ablation studies validate the contributions of our techniques and guide hyperparameter selection. We confirm the HE-compatibility of the trained models, advancing the practical deployment of accurate, stable, and secure deep learning inference.</li>
</ul>

<h3>Title: Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO</h3>
<ul>
<li><strong>Authors: </strong>Peter Chen, Xiaopeng Li, Ziniu Li, Xi Chen, Tianyi Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11595">https://arxiv.org/abs/2505.11595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11595">https://arxiv.org/pdf/2505.11595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11595]] Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO(https://arxiv.org/abs/2505.11595)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has demonstrated significant success in enhancing reasoning capabilities in large language models (LLMs). One of the most widely used RL methods is Group Relative Policy Optimization (GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls when all sampled responses in a group are incorrect -- referred to as an \emph{all-negative-sample} group -- as it fails to update the policy, hindering learning progress. The contributions of this paper are two-fold. First, we propose a simple yet effective framework that introduces response diversity within all-negative-sample groups in GRPO using AI feedback. We also provide a theoretical analysis, via a stylized model, showing how this diversification improves learning dynamics. Second, we empirically validate our approach, showing the improved performance across various model sizes (7B, 14B, 32B) in both offline and online learning settings with 10 benchmarks, including base and distilled variants. Our findings highlight that learning from all-negative-sample groups is not only feasible but beneficial, advancing recent insights from \citet{Xiong-2025-Minimalist}.</li>
</ul>

<h3>Title: Continuous Optimization for Feature Selection with Permutation-Invariant Embedding and Policy-Guided Search</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Rui Xie, Zijun Yao, Yanjie Fu, Dongjie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11601">https://arxiv.org/abs/2505.11601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11601">https://arxiv.org/pdf/2505.11601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11601]] Continuous Optimization for Feature Selection with Permutation-Invariant Embedding and Policy-Guided Search(https://arxiv.org/abs/2505.11601)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Feature selection removes redundant features to enhanc performance and computational efficiency in downstream tasks. Existing works often struggle to capture complex feature interactions and adapt to diverse scenarios. Recent advances in this domain have incorporated generative intelligence to address these drawbacks by uncovering intricate relationships between features. However, two key limitations remain: 1) embedding feature subsets in a continuous space is challenging due to permutation sensitivity, as changes in feature order can introduce biases and weaken the embedding learning process; 2) gradient-based search in the embedding space assumes convexity, which is rarely guaranteed, leading to reduced search effectiveness and suboptimal subsets. To address these limitations, we propose a new framework that can: 1) preserve feature subset knowledge in a continuous embedding space while ensuring permutation invariance; 2) effectively explore the embedding space without relying on strong convex assumptions. For the first objective, we develop an encoder-decoder paradigm to preserve feature selection knowledge into a continuous embedding space. This paradigm captures feature interactions through pairwise relationships within the subset, removing the influence of feature order on the embedding. Moreover, an inducing point mechanism is introduced to accelerate pairwise relationship computations. For the second objective, we employ a policy-based reinforcement learning (RL) approach to guide the exploration of the embedding space. The RL agent effectively navigates the space by balancing multiple objectives. By prioritizing high-potential regions adaptively and eliminating the reliance on convexity assumptions, the RL agent effectively reduces the risk of converging to local optima. Extensive experiments demonstrate the effectiveness, efficiency, robustness and explicitness of our model.</li>
</ul>

<h3>Title: Regularity and Stability Properties of Selective SSMs with Discontinuous Gating</h3>
<ul>
<li><strong>Authors: </strong>Nikola Zubi, Davide Scaramuzza</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11602">https://arxiv.org/abs/2505.11602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11602">https://arxiv.org/pdf/2505.11602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11602]] Regularity and Stability Properties of Selective SSMs with Discontinuous Gating(https://arxiv.org/abs/2505.11602)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep Selective State-Space Models (SSMs), characterized by input-dependent, time-varying parameters, offer significant expressive power but pose challenges for stability analysis, especially with discontinuous gating signals. In this paper, we investigate the stability and regularity properties of continuous-time selective SSMs through the lens of passivity and Input-to-State Stability (ISS). We establish that intrinsic energy dissipation guarantees exponential forgetting of past states. Crucially, we prove that the unforced system dynamics possess an underlying minimal quadratic energy function whose defining matrix exhibits robust $\text{AUC}_{\text{loc}}$ regularity, accommodating discontinuous gating. Furthermore, assuming a universal quadratic storage function ensures passivity across all inputs, we derive parametric LMI conditions and kernel constraints that limit gating mechanisms, formalizing "irreversible forgetting" of recurrent models. Finally, we provide sufficient conditions for global ISS, linking uniform local dissipativity to overall system robustness. Our findings offer a rigorous framework for understanding and designing stable and reliable deep selective SSMs.</li>
</ul>

<h3>Title: Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kyudan Jung, Hojun Cho, Jooyeol Yun, Jaehyeok Jang, Jagul Choo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11604">https://arxiv.org/abs/2505.11604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11604">https://arxiv.org/pdf/2505.11604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11604]] Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models(https://arxiv.org/abs/2505.11604)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing research on large language models (LLMs) for PowerPoint predominantly focuses on slide generation, overlooking the common yet tedious task of editing existing slides. We introduce Talk-to-Your-Slides, an LLM-powered agent that directly edits slides within active PowerPoint sessions through COM communication. Our system employs a two-level approach: (1) high-level processing where an LLM agent interprets instructions and formulates editing plans, and (2) low-level execution where Python scripts directly manipulate PowerPoint objects. Unlike previous methods relying on predefined operations, our approach enables more flexible and contextually-aware editing. To facilitate evaluation, we present TSBench, a human-annotated dataset of 379 diverse editing instructions with corresponding slide variations. Experimental results demonstrate that Talk-to-Your-Slides significantly outperforms baseline methods in execution success rate, instruction fidelity, and editing efficiency. Our code and benchmark are available at this https URL</li>
</ul>

<h3>Title: MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaomin Li, Mingye Gao, Yuexing Hao, Taoran Li, Guangya Wan, Zihan Wang, Yijun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11613">https://arxiv.org/abs/2505.11613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11613">https://arxiv.org/pdf/2505.11613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11613]] MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models(https://arxiv.org/abs/2505.11613)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Clinical guidelines, typically structured as decision trees, are central to evidence-based medical practice and critical for ensuring safe and accurate diagnostic decision-making. However, it remains unclear whether Large Language Models (LLMs) can reliably follow such structured protocols. In this work, we introduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to make guideline-consistent clinical decisions. MedGUIDE is constructed from 55 curated NCCN decision trees across 17 cancer types and uses clinical scenarios generated by LLMs to create a large pool of multiple-choice diagnostic questions. We apply a two-stage quality selection process, combining expert-labeled reward models and LLM-as-a-judge ensembles across ten clinical and linguistic criteria, to select 7,747 high-quality samples. We evaluate 25 LLMs spanning general-purpose, open-source, and medically specialized models, and find that even domain-specific LLMs often underperform on tasks requiring structured guideline adherence. We also test whether performance can be improved via in-context guideline inclusion or continued pretraining. Our findings underscore the importance of MedGUIDE in assessing whether LLMs can operate safely within the procedural frameworks expected in real-world clinical settings.</li>
</ul>

<h3>Title: Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations</h3>
<ul>
<li><strong>Authors: </strong>Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11615">https://arxiv.org/abs/2505.11615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11615">https://arxiv.org/pdf/2505.11615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11615]] Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations(https://arxiv.org/abs/2505.11615)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Changing the behavior of large language models (LLMs) can be as straightforward as editing the Transformer's residual streams using appropriately constructed "steering vectors." These modifications to internal neural activations, a form of representation engineering, offer an effective and targeted means of influencing model behavior without retraining or fine-tuning the model. But how can such steering vectors be systematically identified? We propose a principled approach for uncovering steering vectors by aligning latent representations elicited through behavioral methods (specifically, Markov chain Monte Carlo with LLMs) with their neural counterparts. To evaluate this approach, we focus on extracting latent risk preferences from LLMs and steering their risk-related outputs using the aligned representations as steering vectors. We show that the resulting steering vectors successfully and reliably modulate LLM outputs in line with the targeted behavior.</li>
</ul>

<h3>Title: Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization</h3>
<ul>
<li><strong>Authors: </strong>Aaron Wilhelm, Nils Napp</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11620">https://arxiv.org/abs/2505.11620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11620">https://arxiv.org/pdf/2505.11620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11620]] Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization(https://arxiv.org/abs/2505.11620)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ground texture localization using a downward-facing camera offers a low-cost, high-precision localization solution that is robust to dynamic environments and requires no environmental modification. We present a significantly improved bag-of-words (BoW) image retrieval system for ground texture localization, achieving substantially higher accuracy for global localization and higher precision and recall for loop closure detection in SLAM. Our approach leverages an approximate $k$-means (AKM) vocabulary with soft assignment, and exploits the consistent orientation and constant scale constraints inherent to ground texture localization. Identifying the different needs of global localization vs. loop closure detection for SLAM, we present both high-accuracy and high-speed versions of our algorithm. We test the effect of each of our proposed improvements through an ablation study and demonstrate our method's effectiveness for both global localization and loop closure detection. With numerous ground texture localization systems already using BoW, our method can readily replace other generic BoW systems in their pipeline and immediately improve their results.</li>
</ul>

<h3>Title: Nearest Neighbor Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Huiliang Zhang, Ping Nie, Lijun Sun, Benoit Boulet</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11625">https://arxiv.org/abs/2505.11625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11625">https://arxiv.org/pdf/2505.11625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11625]] Nearest Neighbor Multivariate Time Series Forecasting(https://arxiv.org/abs/2505.11625)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Multivariate time series (MTS) forecasting has a wide range of applications in both industry and academia. Recently, spatial-temporal graph neural networks (STGNNs) have gained popularity as MTS forecasting methods. However, current STGNNs can only use the finite length of MTS input data due to the computational complexity. Moreover, they lack the ability to identify similar patterns throughout the entire dataset and struggle with data that exhibit sparsely and discontinuously distributed correlations among variables over an extensive historical period, resulting in only marginal improvements. In this article, we introduce a simple yet effective k-nearest neighbor MTS forecasting ( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval mechanism over a large datastore of cached series, using representations from the MTS model for similarity search. This approach requires no additional training and scales to give the MTS model direct access to the whole dataset at test time, resulting in a highly expressive model that consistently improves performance, and has the ability to extract sparse distributed but similar patterns spanning over multivariables from the entire dataset. Furthermore, a hybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can capture both long-term temporal and short-term spatial-temporal dependencies and is shown to provide accurate representation for kNN-MTSfor better forecasting. Experimental results on several real-world datasets show a significant improvement in the forecasting performance of kNN-MTS. The quantitative analysis also illustrates the interpretability and efficiency of kNN-MTS, showing better application prospects and opening up a new path for efficiently using the large dataset in MTS models.</li>
</ul>

<h3>Title: THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Udita Patel, Rutu Mulkar, Jay Roberts, Cibi Chakravarthy Senthilkumar, Sujay Gandhi, Xiaofei Zheng, Naumaan Nayyar, Rafael Castrillo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11626">https://arxiv.org/abs/2505.11626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11626">https://arxiv.org/pdf/2505.11626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11626]] THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering(https://arxiv.org/abs/2505.11626)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose THELMA (Task Based Holistic Evaluation of Large Language Model Applications), a reference free framework for RAG (Retrieval Augmented generation) based question answering (QA) applications. THELMA consist of six interdependent metrics specifically designed for holistic, fine grained evaluation of RAG QA applications. THELMA framework helps developers and application owners evaluate, monitor and improve end to end RAG QA pipelines without requiring labelled sources or reference this http URL also present our findings on the interplay of the proposed THELMA metrics, which can be interpreted to identify the specific RAG component needing improvement in QA applications.</li>
</ul>

<h3>Title: Adaptive Robust Optimization with Data-Driven Uncertainty for Enhancing Distribution System Resilience</h3>
<ul>
<li><strong>Authors: </strong>Shuyi Chen, Shixiang Zhu, Ramteen Sioshansi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11627">https://arxiv.org/abs/2505.11627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11627">https://arxiv.org/pdf/2505.11627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11627]] Adaptive Robust Optimization with Data-Driven Uncertainty for Enhancing Distribution System Resilience(https://arxiv.org/abs/2505.11627)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Extreme weather events are placing growing strain on electric power systems, exposing the limitations of purely reactive responses and prompting the need for proactive resilience planning. However, existing approaches often rely on simplified uncertainty models and decouple proactive and reactive decisions, overlooking their critical interdependence. This paper proposes a novel tri-level optimization framework that integrates proactive infrastructure investment, adversarial modeling of spatio-temporal disruptions, and adaptive reactive response. We construct high-probability, distribution-free uncertainty sets using conformal prediction to capture complex and data-scarce outage patterns. To solve the resulting nested decision problem, we derive a bi-level reformulation via strong duality and develop a scalable Benders decomposition algorithm. Experiments on both real and synthetic data demonstrate that our approach consistently outperforms conventional robust and two-stage methods, achieving lower worst-case losses and more efficient resource allocation, especially under tight operational constraints and large-scale uncertainty.</li>
</ul>

<h3>Title: Enhancing Network Anomaly Detection with Quantum GANs and Successive Data Injection for Multivariate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Wajdi Hammami, Soumaya Cherkaoui, Shengrui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11631">https://arxiv.org/abs/2505.11631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11631">https://arxiv.org/pdf/2505.11631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11631]] Enhancing Network Anomaly Detection with Quantum GANs and Successive Data Injection for Multivariate Time Series(https://arxiv.org/abs/2505.11631)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantum computing may offer new approaches for advancing machine learning, including in complex tasks such as anomaly detection in network traffic. In this paper, we introduce a quantum generative adversarial network (QGAN) architecture for multivariate time-series anomaly detection that leverages variational quantum circuits (VQCs) in combination with a time-window shifting technique, data re-uploading, and successive data injection (SuDaI). The method encodes multivariate time series data as rotation angles. By integrating both data re-uploading and SuDaI, the approach maps classical data into quantum states efficiently, helping to address hardware limitations such as the restricted number of available qubits. In addition, the approach employs an anomaly scoring technique that utilizes both the generator and the discriminator output to enhance the accuracy of anomaly detection. The QGAN was trained using the parameter shift rule and benchmarked against a classical GAN. Experimental results indicate that the quantum model achieves a accuracy high along with high recall and F1-scores in anomaly detection, and attains a lower MSE compared to the classical model. Notably, the QGAN accomplishes this performance with only 80 parameters, demonstrating competitive results with a compact architecture. Tests using a noisy simulator suggest that the approach remains effective under realistic noise-prone conditions.</li>
</ul>

<h3>Title: The Gaussian-Multinoulli Restricted Boltzmann Machine: A Potts Model Extension of the GRBM</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Kapasi, William Whitehead, Luke Theogarajan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11635">https://arxiv.org/abs/2505.11635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11635">https://arxiv.org/pdf/2505.11635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11635]] The Gaussian-Multinoulli Restricted Boltzmann Machine: A Potts Model Extension of the GRBM(https://arxiv.org/abs/2505.11635)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many real-world tasks, from associative memory to symbolic reasoning, demand discrete, structured representations that standard continuous latent models struggle to express naturally. We introduce the Gaussian-Multinoulli Restricted Boltzmann Machine (GM-RBM), a generative energy-based model that extends the Gaussian-Bernoulli RBM (GB-RBM) by replacing binary hidden units with $q$-state Potts variables. This modification enables a combinatorially richer latent space and supports learning over multivalued, interpretable latent concepts. We formally derive GM-RBM's energy function, learning dynamics, and conditional distributions, showing that it preserves tractable inference and training through contrastive divergence. Empirically, we demonstrate that GM-RBMs model complex multimodal distributions more effectively than binary RBMs, outperforming them on tasks involving analogical recall and structured memory. Our results highlight GM-RBMs as a scalable framework for discrete latent inference with enhanced expressiveness and interoperability.</li>
</ul>

<h3>Title: BandRC: Band Shifted Raised Cosine Activated Implicit Neural Representations</h3>
<ul>
<li><strong>Authors: </strong>Pandula Thennakoon, Avishka Ranasinghe, Mario De Silva, Buwaneka Epakanda, Roshan Godaliyadda, Parakrama Ekanayake, Vijitha Herath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11640">https://arxiv.org/abs/2505.11640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11640">https://arxiv.org/pdf/2505.11640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11640]] BandRC: Band Shifted Raised Cosine Activated Implicit Neural Representations(https://arxiv.org/abs/2505.11640)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, implicit neural representations(INRs) have gained popularity in the computer vision community. This is mainly due to the strong performance of INRs in many computer vision tasks. These networks can extract a continuous signal representation given a discrete signal representation. In previous studies, it has been repeatedly shown that INR performance has a strong correlation with the activation functions used in its multilayer perceptrons. Although numerous activation functions have been proposed that are competitive with one another, they share some common set of challenges such as spectral bias(Lack of sensitivity to high-frequency content in signals), limited robustness to signal noise and difficulties in simultaneous capturing both local and global features. and furthermore, the requirement for manual parameter tuning. To address these issues, we introduce a novel activation function, Band Shifted Raised Cosine Activated Implicit Neural Networks \textbf{(BandRC)} tailored to enhance signal representation capacity further. We also incorporate deep prior knowledge extracted from the signal to adjust the activation functions through a task-specific model. Through a mathematical analysis and a series of experiments which include image reconstruction (with a +8.93 dB PSNR improvement over the nearest counterpart), denoising (with a +0.46 dB increase in PSNR), super-resolution (with a +1.03 dB improvement over the nearest State-Of-The-Art (SOTA) method for 6X super-resolution), inpainting, and 3D shape reconstruction we demonstrate the dominance of BandRC over existing state of the art activation functions.</li>
</ul>

<h3>Title: Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Jinzhou Cao, Xiangxu Wang, Jiashi Chen, Wei Tu, Zhenhui Li, Xindong Yang, Tianhong Zhao, Qingquan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11645">https://arxiv.org/abs/2505.11645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11645">https://arxiv.org/pdf/2505.11645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11645]] Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach(https://arxiv.org/abs/2505.11645)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Fine-grained economic mapping through urban representation learning has emerged as a crucial tool for evidence-based economic decisions. While existing methods primarily rely on supervised or unsupervised approaches, they often overlook semi-supervised learning in data-scarce scenarios and lack unified multi-task frameworks for comprehensive sectoral economic analysis. To address these gaps, we propose SemiGTX, an explainable semi-supervised graph learning framework for sectoral economic mapping. The framework is designed with dedicated fusion encoding modules for various geospatial data modalities, seamlessly integrating them into a cohesive graph structure. It introduces a semi-information loss function that combines spatial self-supervision with locally masked supervised regression, enabling more informative and effective region representations. Through multi-task learning, SemiGTX concurrently maps GDP across primary, secondary, and tertiary sectors within a unified model. Extensive experiments conducted in the Pearl River Delta region of China demonstrate the model's superior performance compared to existing methods, achieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary and tertiary sectors, respectively. Cross-regional experiments in Beijing and Chengdu further illustrate its generality. Systematic analysis reveals how different data modalities influence model predictions, enhancing explainability while providing valuable insights for regional development planning. This representation learning framework advances regional economic monitoring through diverse urban data integration, providing a robust foundation for precise economic forecasting.</li>
</ul>

<h3>Title: Joint Graph Estimation and Signal Restoration for Robust Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Tsutahiro Fukuhara, Junya Hara, Hiroshi Higashi, Yuichi Tanaka</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11648">https://arxiv.org/abs/2505.11648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11648">https://arxiv.org/pdf/2505.11648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11648]] Joint Graph Estimation and Signal Restoration for Robust Federated Learning(https://arxiv.org/abs/2505.11648)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>We propose a robust aggregation method for model parameters in federated learning (FL) under noisy communications. FL is a distributed machine learning paradigm in which a central server aggregates local model parameters from multiple clients. These parameters are often noisy and/or have missing values during data collection, training, and communication between the clients and server. This may cause a considerable drop in model accuracy. To address this issue, we learn a graph that represents pairwise relationships between model parameters of the clients during aggregation. We realize it with a joint problem of graph learning and signal (i.e., model parameters) restoration. The problem is formulated as a difference-of-convex (DC) optimization, which is efficiently solved via a proximal DC algorithm. Experimental results on MNIST and CIFAR-10 datasets show that the proposed method outperforms existing approaches by up to $2$--$5\%$ in classification accuracy under biased data distributions and noisy conditions.</li>
</ul>

<h3>Title: UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Liu, Yingxue Zhang, Xin Zhang, Ling Tian, Xu Zheng, Yanhua Li, Jun Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11654">https://arxiv.org/abs/2505.11654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11654">https://arxiv.org/pdf/2505.11654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11654]] UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models(https://arxiv.org/abs/2505.11654)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Understanding and predicting urban dynamics is crucial for managing transportation systems, optimizing urban planning, and enhancing public services. While neural network-based approaches have achieved success, they often rely on task-specific architectures and large volumes of data, limiting their ability to generalize across diverse urban scenarios. Meanwhile, Large Language Models (LLMs) offer strong reasoning and generalization capabilities, yet their application to spatial-temporal urban dynamics remains underexplored. Existing LLM-based methods struggle to effectively integrate multifaceted spatial-temporal data and fail to address distributional shifts between training and testing data, limiting their predictive reliability in real-world applications. To bridge this gap, we propose UrbanMind, a novel spatial-temporal LLM framework for multifaceted urban dynamics prediction that ensures both accurate forecasting and robust generalization. At its core, UrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with specialized masking strategies that capture intricate spatial-temporal dependencies and intercorrelations among multifaceted urban dynamics. Additionally, we design a semantic-aware prompting and fine-tuning strategy that encodes spatial-temporal contextual details into prompts, enhancing LLMs' ability to reason over spatial-temporal patterns. To further improve generalization, we introduce a test time adaptation mechanism with a test data reconstructor, enabling UrbanMind to dynamically adjust to unseen test data by reconstructing LLM-generated embeddings. Extensive experiments on real-world urban datasets across multiple cities demonstrate that UrbanMind consistently outperforms state-of-the-art baselines, achieving high accuracy and robust generalization, even in zero-shot settings.</li>
</ul>

<h3>Title: Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks</h3>
<ul>
<li><strong>Authors: </strong>Shubham Vatsal, Harsh Dubey, Aditi Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11665">https://arxiv.org/abs/2505.11665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11665">https://arxiv.org/pdf/2505.11665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11665]] Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks(https://arxiv.org/abs/2505.11665)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks. However, ensuring their effectiveness across multiple languages presents unique challenges. Multilingual prompt engineering has emerged as a key approach to enhance LLMs' capabilities in diverse linguistic settings without requiring extensive parameter re-training or fine-tuning. With growing interest in multilingual prompt engineering over the past two to three years, researchers have explored various strategies to improve LLMs' performance across languages and NLP tasks. By crafting structured natural language prompts, researchers have successfully extracted knowledge from LLMs across different languages, making these techniques an accessible pathway for a broader audience, including those without deep expertise in machine learning, to harness the capabilities of LLMs. In this paper, we survey and categorize different multilingual prompting techniques based on the NLP tasks they address across a diverse set of datasets that collectively span around 250 languages. We further highlight the LLMs employed, present a taxonomy of approaches and discuss potential state-of-the-art (SoTA) methods for specific multilingual datasets. Additionally, we derive a range of insights across language families and resource levels (high-resource vs. low-resource), including analyses such as the distribution of NLP tasks by language resource type and the frequency of prompting methods across different language families. Our survey reviews 36 research papers covering 39 prompting techniques applied to 30 multilingual NLP tasks, with the majority of these studies published in the last two years.</li>
</ul>

<h3>Title: DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Zhao, Xiaoguang Li, Linjia Shi, Nasrin Imanpour, Song Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11676">https://arxiv.org/abs/2505.11676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11676">https://arxiv.org/pdf/2505.11676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11676]] DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation(https://arxiv.org/abs/2505.11676)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary semantic segmentation aims to segment images into distinct semantic regions for both seen and unseen categories at the pixel level. Current methods utilize text embeddings from pre-trained vision-language models like CLIP but struggle with the inherent domain gap between image and text embeddings, even after extensive alignment during training. Additionally, relying solely on deep text-aligned features limits shallow-level feature guidance, which is crucial for detecting small objects and fine details, ultimately reducing segmentation accuracy. To address these limitations, we propose a dual prompting framework, DPSeg, for this task. Our approach combines dual-prompt cost volume generation, a cost volume-guided decoder, and a semantic-guided prompt refinement strategy that leverages our dual prompting scheme to mitigate alignment issues in visual prompt generation. By incorporating visual embeddings from a visual prompt encoder, our approach reduces the domain gap between text and image embeddings while providing multi-level guidance through shallow features. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches on multiple public datasets.</li>
</ul>

<h3>Title: Ambiguity Resolution in Text-to-Structured Data Mapping</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-Young Paik, Liming Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11679">https://arxiv.org/abs/2505.11679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11679">https://arxiv.org/pdf/2505.11679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11679]] Ambiguity Resolution in Text-to-Structured Data Mapping(https://arxiv.org/abs/2505.11679)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods of ambiguity handling either exploit ReACT framework to produce the correct mapping through trial and error, or supervised fine tuning to guide models to produce a biased mapping to improve certain tasks. In this paper, we adopt a different approach that characterizes the representation difference of ambiguous text in the latent space and leverage the difference to identify ambiguity before mapping them to structured data. To detect ambiguity of a sentence, we focused on the relationship between ambiguous questions and their interpretations and what cause the LLM ignore multiple interpretations. Different to the distance calculated by dense embedding vectors, we utilize the observation that ambiguity is caused by concept missing in latent space of LLM to design a new distance measurement, computed through the path kernel by the integral of gradient values for each concepts from sparse-autoencoder (SAE) under each state. We identify patterns to distinguish ambiguous questions with this measurement. Based on our observation, We propose a new framework to improve the performance of LLMs on ambiguous agentic tool calling through missing concepts prediction.</li>
</ul>

<h3>Title: Mollifier Layers: Enabling Efficient High-Order Derivatives in Inverse PDE Learning</h3>
<ul>
<li><strong>Authors: </strong>Ananyae Kumar Bhartari, Vinayak Vinayak, Vivek B Shenoy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11682">https://arxiv.org/abs/2505.11682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11682">https://arxiv.org/pdf/2505.11682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11682]] Mollifier Layers: Enabling Efficient High-Order Derivatives in Inverse PDE Learning(https://arxiv.org/abs/2505.11682)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Parameter estimation in inverse problems involving partial differential equations (PDEs) underpins modeling across scientific disciplines, especially when parameters vary in space or time. Physics-informed Machine Learning (PhiML) integrates PDE constraints into deep learning, but prevailing approaches depend on recursive automatic differentiation (autodiff), which produces inaccurate high-order derivatives, inflates memory usage, and underperforms in noisy settings. We propose Mollifier Layers, a lightweight, architecture-agnostic module that replaces autodiff with convolutional operations using analytically defined mollifiers. This reframing of derivative computation as smoothing integration enables efficient, noise-robust estimation of high-order derivatives directly from network outputs. Mollifier Layers attach at the output layer and require no architectural modifications. We compare them with three distinct architectures and benchmark performance across first-, second-, and fourth-order PDEs -- including Langevin dynamics, heat diffusion, and reaction-diffusion systems -- observing significant improvements in memory efficiency, training time and accuracy for parameter recovery across tasks. To demonstrate practical relevance, we apply Mollifier Layers to infer spatially varying epigenetic reaction rates from super-resolution chromatin imaging data -- a real-world inverse problem with biomedical significance. Our results establish Mollifier Layers as an efficient and scalable tool for physics-constrained learning.</li>
</ul>

<h3>Title: Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Sukairaj Hafiz Imam, Babangida Sani, Dawit Ketema Gete, Bedru Yimam Ahamed, Ibrahim Said Ahmad, Idris Abdulmumin, Seid Muhie Yimam, Muhammad Yahuza Bello, Shamsuddeen Hassan Muhammad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11690">https://arxiv.org/abs/2505.11690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11690">https://arxiv.org/pdf/2505.11690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11690]] Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions(https://arxiv.org/abs/2505.11690)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Automatic Speech Recognition (ASR) technologies have transformed human-computer interaction; however, low-resource languages in Africa remain significantly underrepresented in both research and practical applications. This study investigates the major challenges hindering the development of ASR systems for these languages, which include data scarcity, linguistic complexity, limited computational resources, acoustic variability, and ethical concerns surrounding bias and privacy. The primary goal is to critically analyze these barriers and identify practical, inclusive strategies to advance ASR technologies within the African context. Recent advances and case studies emphasize promising strategies such as community-driven data collection, self-supervised and multilingual learning, lightweight model architectures, and techniques that prioritize privacy. Evidence from pilot projects involving various African languages showcases the feasibility and impact of customized solutions, which encompass morpheme-based modeling and domain-specific ASR applications in sectors like healthcare and education. The findings highlight the importance of interdisciplinary collaboration and sustained investment to tackle the distinct linguistic and infrastructural challenges faced by the continent. This study offers a progressive roadmap for creating ethical, efficient, and inclusive ASR systems that not only safeguard linguistic diversity but also improve digital accessibility and promote socioeconomic participation for speakers of African languages.</li>
</ul>

<h3>Title: The Geometry of ReLU Networks through the ReLU Transition Graph</h3>
<ul>
<li><strong>Authors: </strong>Sahil Rajesh Dhayalkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11692">https://arxiv.org/abs/2505.11692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11692">https://arxiv.org/pdf/2505.11692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11692]] The Geometry of ReLU Networks through the ReLU Transition Graph(https://arxiv.org/abs/2505.11692)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We develop a novel theoretical framework for analyzing ReLU neural networks through the lens of a combinatorial object we term the ReLU Transition Graph (RTG). In this graph, each node corresponds to a linear region induced by the network's activation patterns, and edges connect regions that differ by a single neuron flip. Building on this structure, we derive a suite of new theoretical results connecting RTG geometry to expressivity, generalization, and robustness. Our contributions include tight combinatorial bounds on RTG size and diameter, a proof of RTG connectivity, and graph-theoretic interpretations of VC-dimension. We also relate entropy and average degree of the RTG to generalization error. Each theoretical result is rigorously validated via carefully controlled experiments across varied network depths, widths, and data regimes. This work provides the first unified treatment of ReLU network structure via graph theory and opens new avenues for compression, regularization, and complexity control rooted in RTG analysis.</li>
</ul>

<h3>Title: Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization</h3>
<ul>
<li><strong>Authors: </strong>Shihao Zhang, Haoyu Zhang, Ian Colbert, Rayan Saab</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11695">https://arxiv.org/abs/2505.11695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11695">https://arxiv.org/pdf/2505.11695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11695]] Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization(https://arxiv.org/abs/2505.11695)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches.</li>
</ul>

<h3>Title: Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Haipeng Fang, Sheng Tang, Juan Cao, Enshuo Zhang, Fan Tang, Tong-Yee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11707">https://arxiv.org/abs/2505.11707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11707">https://arxiv.org/pdf/2505.11707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11707]] Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration(https://arxiv.org/abs/2505.11707)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion transformers have shown exceptional performance in visual generation but incur high computational costs. Token reduction techniques that compress models by sharing the denoising process among similar tokens have been introduced. However, existing approaches neglect the denoising priors of the diffusion models, leading to suboptimal acceleration and diminished image quality. This study proposes a novel concept: attend to prune feature redundancies in areas not attended by the diffusion process. We analyze the location and degree of feature redundancies based on the structure-then-detail denoising priors. Subsequently, we introduce SDTM, a structure-then-detail token merging approach that dynamically compresses feature redundancies. Specifically, we design dynamic visual token merging, compression ratio adjusting, and prompt reweighting for different stages. Served in a post-training way, the proposed method can be integrated seamlessly into any DiT architecture. Extensive experiments across various backbones, schedulers, and datasets showcase the superiority of our method, for example, it achieves 1.55 times acceleration with negligible impact on image quality. Project page: this https URL.</li>
</ul>

<h3>Title: Unveiling the Black Box: A Multi-Layer Framework for Explaining Reinforcement Learning-Based Cyber Agents</h3>
<ul>
<li><strong>Authors: </strong>Diksha Goel, Kristen Moore, Jeff Wang, Minjune Kim, Thanh Thi Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11708">https://arxiv.org/abs/2505.11708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11708">https://arxiv.org/pdf/2505.11708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11708]] Unveiling the Black Box: A Multi-Layer Framework for Explaining Reinforcement Learning-Based Cyber Agents(https://arxiv.org/abs/2505.11708)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, explainability</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) agents are increasingly used to simulate sophisticated cyberattacks, but their decision-making processes remain opaque, hindering trust, debugging, and defensive preparedness. In high-stakes cybersecurity contexts, explainability is essential for understanding how adversarial strategies are formed and evolve over time. In this paper, we propose a unified, multi-layer explainability framework for RL-based attacker agents that reveals both strategic (MDP-level) and tactical (policy-level) reasoning. At the MDP level, we model cyberattacks as a Partially Observable Markov Decision Processes (POMDPs) to expose exploration-exploitation dynamics and phase-aware behavioural shifts. At the policy level, we analyse the temporal evolution of Q-values and use Prioritised Experience Replay (PER) to surface critical learning transitions and evolving action preferences. Evaluated across CyberBattleSim environments of increasing complexity, our framework offers interpretable insights into agent behaviour at scale. Unlike previous explainable RL methods, which are often post-hoc, domain-specific, or limited in depth, our approach is both agent- and environment-agnostic, supporting use cases ranging from red-team simulation to RL policy debugging. By transforming black-box learning into actionable behavioural intelligence, our framework enables both defenders and developers to better anticipate, analyse, and respond to autonomous cyber threats.</li>
</ul>

<h3>Title: Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming</h3>
<ul>
<li><strong>Authors: </strong>Diksha Goel, Hussain Ahmad, Kristen Moore, Mingyu Guo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11710">https://arxiv.org/abs/2505.11710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11710">https://arxiv.org/pdf/2505.11710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11710]] Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming(https://arxiv.org/abs/2505.11710)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Modern enterprise networks increasingly rely on Active Directory (AD) for identity and access management. However, this centralization exposes a single point of failure, allowing adversaries to compromise high-value assets. Existing AD defense approaches often assume static attacker behavior, but real-world adversaries adapt dynamically, rendering such methods brittle. To address this, we model attacker-defender interactions in AD as a Stackelberg game between an adaptive attacker and a proactive defender. We propose a co-evolutionary defense framework that combines Graph Neural Network Approximated Dynamic Programming (GNNDP) to model attacker strategies, with Evolutionary Diversity Optimization (EDO) to generate resilient blocking strategies. To ensure scalability, we introduce a Fixed-Parameter Tractable (FPT) graph reduction method that reduces complexity while preserving strategic structure. Our framework jointly refines attacker and defender policies to improve generalization and prevent premature convergence. Experiments on synthetic AD graphs show near-optimal results (within 0.1 percent of optimality on r500) and improved performance on larger graphs (r1000 and r2000), demonstrating the framework's scalability and effectiveness.</li>
</ul>

<h3>Title: Reinforcement Learning Finetunes Small Subnetworks in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tur, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11711">https://arxiv.org/abs/2505.11711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11711">https://arxiv.org/pdf/2505.11711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11711]] Reinforcement Learning Finetunes Small Subnetworks in Large Language Models(https://arxiv.org/abs/2505.11711)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) yields substantial improvements in large language models (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5 percent to 30 percent of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments. This sparsity is intrinsic and occurs without any explicit sparsity promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning. The subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance. Our analysis suggests that this sparsity is not due to updating only a subset of layers, instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank, suggesting RL updates a small subset of parameters that nevertheless span almost the full subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution, techniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.</li>
</ul>

<h3>Title: EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, Neil Zhenqiang Gong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11717">https://arxiv.org/abs/2505.11717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11717">https://arxiv.org/pdf/2505.11717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11717]] EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents(https://arxiv.org/abs/2505.11717)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. Environmental prompt injection attacks manipulate the environment to induce the web agent to perform a specific, attacker-chosen action--referred to as the target action. However, existing attacks suffer from limited effectiveness or stealthiness, or are impractical in real-world settings. In this work, we propose EnvInjection, a new attack that addresses these limitations. Our attack adds a perturbation to the raw pixel values of the rendered webpage, which can be implemented by modifying the webpage's source code. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the target action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple webpage datasets shows that EnvInjection is highly effective and significantly outperforms existing baselines.</li>
</ul>

<h3>Title: UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights</h3>
<ul>
<li><strong>Authors: </strong>Shijun Liang, Ismail R. Alkhouri, Siddhant Gautam, Qing Qu, Saiprasad Ravishankar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11720">https://arxiv.org/abs/2505.11720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11720">https://arxiv.org/pdf/2505.11720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11720]] UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights(https://arxiv.org/abs/2505.11720)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, data-free, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in data-centric deep generative models have led to significant progress in solving inverse imaging problems. However, these models (e.g., diffusion models (DMs)) typically require large amounts of fully sampled (clean) training data, which is often impractical in medical and scientific settings such as dynamic imaging. On the other hand, training-data-free approaches like the Deep Image Prior (DIP) do not require clean ground-truth images but suffer from noise overfitting and can be computationally expensive as the network parameters need to be optimized for each measurement set independently. Moreover, DIP-based methods often overlook the potential of learning a prior using a small number of sub-sampled measurements (or degraded images) available during training. In this paper, we propose UGoDIT, an Unsupervised Group DIP via Transferable weights, designed for the low-data regime where only a very small number, M, of sub-sampled measurement vectors are available during training. Our method learns a set of transferable weights by optimizing a shared encoder and M disentangled decoders. At test time, we reconstruct the unseen degraded image using a DIP network, where part of the parameters are fixed to the learned weights, while the remaining are optimized to enforce measurement consistency. We evaluate UGoDIT on both medical (multi-coil MRI) and natural (super resolution and non-linear deblurring) image recovery tasks under various settings. Compared to recent standalone DIP methods, UGoDIT provides accelerated convergence and notable improvement in reconstruction quality. Furthermore, our method achieves performance competitive with SOTA DM-based and supervised approaches, despite not requiring large amounts of clean training data.</li>
</ul>

<h3>Title: Semantically-Aware Game Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Kai Zhu, Vignesh Edithal, Le Zhang, Ilia Blank, Imran Junejo</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11724">https://arxiv.org/abs/2505.11724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11724">https://arxiv.org/pdf/2505.11724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11724]] Semantically-Aware Game Image Quality Assessment(https://arxiv.org/abs/2505.11724)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Assessing the visual quality of video game graphics presents unique challenges due to the absence of reference images and the distinct types of distortions, such as aliasing, texture blur, and geometry level of detail (LOD) issues, which differ from those in natural images or user-generated content. Existing no-reference image and video quality assessment (NR-IQA/VQA) methods fail to generalize to gaming environments as they are primarily designed for distortions like compression artifacts. This study introduces a semantically-aware NR-IQA model tailored to gaming. The model employs a knowledge-distilled Game distortion feature extractor (GDFE) to detect and quantify game-specific distortions, while integrating semantic gating via CLIP embeddings to dynamically weight feature importance based on scene content. Training on gameplay data recorded across graphical quality presets enables the model to produce quality scores that align with human perception. Our results demonstrate that the GDFE, trained through knowledge distillation from binary classifiers, generalizes effectively to intermediate distortion levels unseen during training. Semantic gating further improves contextual relevance and reduces prediction variance. In the absence of in-domain NR-IQA baselines, our model outperforms out-of-domain methods and exhibits robust, monotonic quality trends across unseen games in the same genre. This work establishes a foundation for automated graphical quality assessment in gaming, advancing NR-IQA methods in this domain.</li>
</ul>

<h3>Title: CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median</h3>
<ul>
<li><strong>Authors: </strong>Imon Banerjee, Sayak Chakrabarty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, math.ST, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11725">https://arxiv.org/abs/2505.11725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11725">https://arxiv.org/pdf/2505.11725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11725]] CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median(https://arxiv.org/abs/2505.11725)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet (1992), approximates the distribution of a statistic by repeatedly drawing m subsamples (with m much smaller than n) without replacement from an original sample of size n. It is now routinely used for robust inference with heavy-tailed data, bandwidth selection, and other large-sample applications. Despite its broad applicability across econometrics, biostatistics, and machine learning, rigorous parameter-free guarantees for the soundness of the m-out-of-n bootstrap when estimating sample quantiles have remained elusive. This paper establishes such guarantees by analyzing the estimator of sample quantiles obtained from m-out-of-n resampling of a dataset of size n. We first prove a central limit theorem for a fully data-driven version of the estimator that holds under a mild moment condition and involves no unknown nuisance parameters. We then show that the moment assumption is essentially tight by constructing a counter-example in which the CLT fails. Strengthening the assumptions slightly, we derive an Edgeworth expansion that provides exact convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap approximation error. Finally, we illustrate the scope of our results by deriving parameter-free asymptotic distributions for practical statistics, including the quantiles for random walk Metropolis-Hastings and the rewards of ergodic Markov decision processes, thereby demonstrating the usefulness of our theory in modern estimation and learning tasks.</li>
</ul>

<h3>Title: Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Harshil Vejendla, Haizhou Shi, Yibin Wang, Tunyu Zhang, Huan Zhang, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11731">https://arxiv.org/abs/2505.11731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11731">https://arxiv.org/pdf/2505.11731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11731]] Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models(https://arxiv.org/abs/2505.11731)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in uncertainty estimation for Large Language Models (LLMs) during downstream adaptation have addressed key challenges of reliability and simplicity. However, existing Bayesian methods typically require multiple sampling iterations during inference, creating significant efficiency issues that limit practical deployment. In this paper, we investigate the possibility of eliminating the need for test-time sampling for LLM uncertainty estimation. Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned confidence into a non-Bayesian student LLM by minimizing the divergence between their predictive distributions. Unlike typical calibration methods, our distillation is carried out solely on the training dataset without the need of an additional validation dataset. This simple yet effective approach achieves N-times more efficient uncertainty estimation during testing, where N is the number of samples traditionally required by Bayesian LLMs. Our extensive experiments demonstrate that uncertainty estimation capabilities on training data can successfully generalize to unseen test data through our distillation technique, consistently producing results comparable to (or even better than) state-of-the-art Bayesian LLMs.</li>
</ul>

<h3>Title: MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports</h3>
<ul>
<li><strong>Authors: </strong>Kevin Wu, Eric Wu, Rahul Thapa, Kevin Wei, Angela Zhang, Arvind Suresh, Jacqueline J. Tao, Min Woo Sun, Alejandro Lozano, James Zou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11733">https://arxiv.org/abs/2505.11733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11733">https://arxiv.org/pdf/2505.11733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11733]] MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports(https://arxiv.org/abs/2505.11733)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at this https URL.</li>
</ul>

<h3>Title: Token-Level Uncertainty Estimation for Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Tunyu Zhang, Haizhou Shi, Yibin Wang, Hengyi Wang, Xiaoxiao He, Zhuowei Li, Haoxian Chen, Ligong Han, Kai Xu, Huan Zhang, Dimitris Metaxas, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11737">https://arxiv.org/abs/2505.11737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11737">https://arxiv.org/pdf/2505.11737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11737]] Token-Level Uncertainty Estimation for Large Language Model Reasoning(https://arxiv.org/abs/2505.11737)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a token-level uncertainty estimation framework to enable LLMs to self-assess and self-improve their generation quality in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation to LLM decoding, generating predictive distributions that we use to estimate token-level uncertainties. We then aggregate these uncertainties to reflect semantic uncertainty of the generated sequences. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that our token-level uncertainty metrics strongly correlate with answer correctness and model robustness. Additionally, we explore using uncertainty to directly enhance the model's reasoning performance through multiple generations and the particle filtering algorithm. Our approach consistently outperforms existing uncertainty estimation methods, establishing effective uncertainty estimation as a valuable tool for both evaluating and improving reasoning generation in LLMs.</li>
</ul>

<h3>Title: ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training</h3>
<ul>
<li><strong>Authors: </strong>Feijiang Han, Xiaodong Yu, Jianheng Tang, Lyle Ungar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11739">https://arxiv.org/abs/2505.11739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11739">https://arxiv.org/pdf/2505.11739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11739]] ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training(https://arxiv.org/abs/2505.11739)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recently, training-free methods for improving large language models (LLMs) have attracted growing interest, with token-level attention tuning emerging as a promising and interpretable direction. However, existing methods typically rely on auxiliary mechanisms to identify important or irrelevant task-specific tokens, introducing potential bias and limiting applicability. In this paper, we uncover a surprising and elegant alternative: the semantically empty initial token is a powerful and underexplored control point for optimizing model behavior. Through theoretical analysis, we show that tuning the initial token's attention sharpens or flattens the attention distribution over subsequent tokens, and its role as an attention sink amplifies this effect. Empirically, we find that: (1) tuning its attention improves LLM performance more effectively than tuning other task-specific tokens; (2) the effect follows a consistent trend across layers, with earlier layers having greater impact, but varies across attention heads, with different heads showing distinct preferences in how they attend to this token. Based on these findings, we propose ZeroTuning, a training-free approach that improves LLM performance by applying head-specific attention adjustments to this special token. Despite tuning only one token, ZeroTuning achieves higher performance on text classification, multiple-choice, and multi-turn conversation tasks across models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its multi-turn score from 7.804 to 7.966. The method is also robust to limited resources, few-shot settings, long contexts, quantization, decoding strategies, and prompt variations. Our work sheds light on a previously overlooked control point in LLMs, offering new insights into both inference-time tuning and model interpretability.</li>
</ul>

<h3>Title: Simple and Effective Specialized Representations for Fair Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Alberto Sinigaglia, Davide Sartor, Marina Ceccon, Gian Antonio Susto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11740">https://arxiv.org/abs/2505.11740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11740">https://arxiv.org/pdf/2505.11740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11740]] Simple and Effective Specialized Representations for Fair Classifiers(https://arxiv.org/abs/2505.11740)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Fair classification is a critical challenge that has gained increasing importance due to international regulations and its growing use in high-stakes decision-making settings. Existing methods often rely on adversarial learning or distribution matching across sensitive groups; however, adversarial learning can be unstable, and distribution matching can be computationally intensive. To address these limitations, we propose a novel approach based on the characteristic function distance. Our method ensures that the learned representation contains minimal sensitive information while maintaining high effectiveness for downstream tasks. By utilizing characteristic functions, we achieve a more stable and efficient solution compared to traditional methods. Additionally, we introduce a simple relaxation of the objective function that guarantees fairness in common classification models with no performance degradation. Experimental results on benchmark datasets demonstrate that our approach consistently matches or achieves better fairness and predictive accuracy than existing methods. Moreover, our method maintains robustness and computational efficiency, making it a practical solution for real-world applications.</li>
</ul>

<h3>Title: Decentralized Multi-Authority Attribute-Based Inner-Product Functional Encryption: Noisy and Evasive Constructions from Lattices</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Liu, Yan Wang, Fang-Wei Fu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11744">https://arxiv.org/abs/2505.11744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11744">https://arxiv.org/pdf/2505.11744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11744]] Decentralized Multi-Authority Attribute-Based Inner-Product Functional Encryption: Noisy and Evasive Constructions from Lattices(https://arxiv.org/abs/2505.11744)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>We study multi-authority attribute-based functional encryption for noisy inner-product functionality, and propose two new primitives: (1) multi-authority attribute-based (noisy) inner-product functional encryption (MA-AB(N)IPFE), which generalizes existing multi-authority attribute-based IPFE schemes by Agrawal et al. (TCC'21), by enabling approximate inner-product computation; and (2) multi-authority attribute-based evasive inner-product functional encryption (MA-evIPFE), a relaxed variant inspired by the evasive IPFE framework by Hsieh et al. (EUROCRYPT'24), shifting focus from ciphertext indistinguishability to a more relaxed pseudorandomness-based security notion. To support the above notions, we introduce two variants of lattice-based computational assumptions: evasive IPFE assumption and indistinguishability-based evasive IPFE assumption (IND-evIPFE). We present lattice-based constructions of both primitives for subset policies, building upon the framework of Waters et al.( TCC'22). Our schemes are proven to be statically secure in the random oracle model under the standard LWE assumption and the newly introduced assumptions. Additionally, we show our MA-AB(N)IPFE scheme can be transformed via modulus switching into a noiseless MA-IPFE scheme that supports exact inner-product functionality. This yields the first lattice-based construction of such a primitive. All our schemes support arbitrary polynomial-size attribute policies and are secure in the random oracle model under lattice assumptions with a sub-exponential modulus-to-noise ratio, making them practical candidates for noise-tolerant, fine-grained access control in multi-authority settings.</li>
</ul>

<h3>Title: POCAII: Parameter Optimization with Conscious Allocation using Iterative Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Joshua Inman, Tanmay Khandait, Lalitha Sankar, Giulia Pedrielli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11745">https://arxiv.org/abs/2505.11745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11745">https://arxiv.org/pdf/2505.11745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11745]] POCAII: Parameter Optimization with Conscious Allocation using Iterative Intelligence(https://arxiv.org/abs/2505.11745)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper we propose for the first time the hyperparameter optimization (HPO) algorithm POCAII. POCAII differs from the Hyperband and Successive Halving literature by explicitly separating the search and evaluation phases and utilizing principled approaches to exploration and exploitation principles during both phases. Such distinction results in a highly flexible scheme for managing a hyperparameter optimization budget by focusing on search (i.e., generating competing configurations) towards the start of the HPO process while increasing the evaluation effort as the HPO comes to an end. POCAII was compared to state of the art approaches SMAC, BOHB and DEHB. Our algorithm shows superior performance in low-budget hyperparameter optimization regimes. Since many practitioners do not have exhaustive resources to assign to HPO, it has wide applications to real-world problems. Moreover, the empirical evidence showed how POCAII demonstrates higher robustness and lower variance in the results. This is again very important when considering realistic scenarios with extremely expensive models to train.</li>
</ul>

<h3>Title: Token Masking Improves Transformer-Based Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Xianglong Xu, John Bowen, Rojin Taheri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11746">https://arxiv.org/abs/2505.11746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11746">https://arxiv.org/pdf/2505.11746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11746]] Token Masking Improves Transformer-Based Text Classification(https://arxiv.org/abs/2505.11746)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While transformer-based models achieve strong performance on text classification, we explore whether masking input tokens can further enhance their effectiveness. We propose token masking regularization, a simple yet theoretically motivated method that randomly replaces input tokens with a special [MASK] token at probability p. This introduces stochastic perturbations during training, leading to implicit gradient averaging that encourages the model to capture deeper inter-token dependencies. Experiments on language identification and sentiment analysis -- across diverse models (mBERT, Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard regularization techniques. We identify task-specific optimal masking rates, with p = 0.1 as a strong general default. We attribute the gains to two key effects: (1) input perturbation reduces overfitting, and (2) gradient-level smoothing acts as implicit ensembling.</li>
</ul>

<h3>Title: X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Valentina Bazyleva, Nicolo Bonettini, Gaurav Bharaj</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11753">https://arxiv.org/abs/2505.11753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11753">https://arxiv.org/pdf/2505.11753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11753]] X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models(https://arxiv.org/abs/2505.11753)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Text-guided diffusion models have significantly advanced image editing, enabling highly realistic and local modifications based on textual prompts. While these developments expand creative possibilities, their malicious use poses substantial challenges for detection of such subtle deepfake edits. To this end, we introduce Explain Edit (X-Edit), a novel method for localizing diffusion-based edits in images. To localize the edits for an image, we invert the image using a pretrained diffusion model, then use these inverted features as input to a segmentation network that explicitly predicts the edited masked regions via channel and spatial attention. Further, we finetune the model using a combined segmentation and relevance loss. The segmentation loss ensures accurate mask prediction by balancing pixel-wise errors and perceptual similarity, while the relevance loss guides the model to focus on low-frequency regions and mitigate high-frequency artifacts, enhancing the localization of subtle edits. To the best of our knowledge, we are the first to address and model the problem of localizing diffusion-based modified regions in images. We additionally contribute a new dataset of paired original and edited images addressing the current lack of resources for this task. Experimental results demonstrate that X-Edit accurately localizes edits in images altered by text-guided diffusion models, outperforming baselines in PSNR and SSIM metrics. This highlights X-Edit's potential as a robust forensic tool for detecting and pinpointing manipulations introduced by advanced image editing techniques.</li>
</ul>

<h3>Title: Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning</h3>
<ul>
<li><strong>Authors: </strong>Sriram Mandalika</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11758">https://arxiv.org/abs/2505.11758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11758">https://arxiv.org/pdf/2505.11758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11758]] Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning(https://arxiv.org/abs/2505.11758)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Few-shot adaptation remains a core challenge for vision-language models (VLMs), especially under limited supervision and noisy support samples. We propose PromptFuseNL, a unified framework that enhances few-shot generalization by combining predictive prompt tuning with dual-branch positive and negative learning. The method refines class prototypes through task-conditioned residuals, multi-stage cross-modal coordination, and semantic hard negative mining. To address label noise, we introduce an unsupervised instance reweighting strategy that downweights unreliable support examples without requiring additional labels or structural changes. PromptFuseNL fuses visual and textual cues through lightweight modules for efficient and discriminative prediction. Evaluated across 15 benchmarks, it consistently surpasses existing prompt- and adapter-based methods in all shot settings while remaining highly efficient, achieving up to 300x faster training and 1000x lower FLOPs compared to full prompt tuning, achieving a new state-of-the-art for robust and scalable few-shot vision-language adaptation.</li>
</ul>

<h3>Title: Towards Universal Semantics With Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Raymond Baartmans, Matthew Raffel, Rahul Vikram, Aiden Deringer, Lizhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11764">https://arxiv.org/abs/2505.11764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11764">https://arxiv.org/pdf/2505.11764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11764]] Towards Universal Semantics With Large Language Models(https://arxiv.org/abs/2505.11764)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a universal set of semantic primes: simple, primitive word-meanings that have been shown to exist in most, if not all, languages of the world. According to this framework, any word, regardless of complexity, can be paraphrased using these primes, revealing a clear and universally translatable meaning. These paraphrases, known as explications, can offer valuable applications for many natural language processing (NLP) tasks, but producing them has traditionally been a slow, manual process. In this work, we present the first study of using large language models (LLMs) to generate NSM explications. We introduce automatic evaluation methods, a tailored dataset for training and evaluation, and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in producing accurate, cross-translatable explications, marking a significant step toward universal semantic representation with LLMs and opening up new possibilities for applications in semantic analysis, translation, and beyond.</li>
</ul>

<h3>Title: Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Boosting Off-Road Segmentation via Photometric Distortion and Exponential Moving Average</h3>
<ul>
<li><strong>Authors: </strong>Wonjune Kim, Lae-kyoung Lee, Su-Yong An</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11769">https://arxiv.org/abs/2505.11769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11769">https://arxiv.org/pdf/2505.11769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11769]] Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Boosting Off-Road Segmentation via Photometric Distortion and Exponential Moving Average(https://arxiv.org/abs/2505.11769)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We report on the application of a high-capacity semantic segmentation pipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured off-road environments. Using a FlashInternImage-B backbone together with a UPerNet decoder, we adapt established techniques, rather than designing new ones, to the distinctive conditions of off-road scenes. Our training recipe couples strong photometric distortion augmentation (to emulate the wide lighting variations of outdoor terrain) with an Exponential Moving Average (EMA) of weights for better generalization. Using only the GOOSE training dataset, we achieve 88.8\% mIoU on the validation set.</li>
</ul>

<h3>Title: Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</h3>
<ul>
<li><strong>Authors: </strong>Jing Huang, Junyi Tao, Thomas Icard, Diyi Yang, Christopher Potts</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11770">https://arxiv.org/abs/2505.11770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11770">https://arxiv.org/pdf/2505.11770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11770]] Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors(https://arxiv.org/abs/2505.11770)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.</li>
</ul>

<h3>Title: Residual Feature Integration is Sufficient to Prevent Negative Transfer</h3>
<ul>
<li><strong>Authors: </strong>Yichen Xu, Ryumei Nakada, Linjun Zhang, Lexin Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11771">https://arxiv.org/abs/2505.11771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11771">https://arxiv.org/pdf/2505.11771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11771]] Residual Feature Integration is Sufficient to Prevent Negative Transfer(https://arxiv.org/abs/2505.11771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Transfer learning typically leverages representations learned from a source domain to improve performance on a target task. A common approach is to extract features from a pre-trained model and directly apply them for target prediction. However, this strategy is prone to negative transfer where the source representation fails to align with the target distribution. In this article, we propose Residual Feature Integration (REFINE), a simple yet effective method designed to mitigate negative transfer. Our approach combines a fixed source-side representation with a trainable target-side encoder and fits a shallow neural network on the resulting joint representation, which adapts to the target domain while preserving transferable knowledge from the source domain. Theoretically, we prove that REFINE is sufficient to prevent negative transfer under mild conditions, and derive the generalization bound demonstrating its theoretical benefit. Empirically, we show that REFINE consistently enhances performance across diverse application and data modalities including vision, text, and tabular data, and outperforms numerous alternative solutions. Our method is lightweight, architecture-agnostic, and robust, making it a valuable addition to the existing transfer learning toolbox.</li>
</ul>

<h3>Title: HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class</h3>
<ul>
<li><strong>Authors: </strong>James V. Roggeveen, Erik Y. Wang, Will Flintoft, Peter Donets, Lucy S. Nathwani, Nickholas Gutierrez, David Ettel, Anton Marius Graf, Siddharth Dandavate, Arjun Nageswaran, Raglan Ward, Ava Williamson, Anne Mykland, Kacper K. Migacz, Yijun Wang, Egemen Bostan, Duy Thuc Nguyen, Zhe He, Marc L. Descoteaux, Felix Yeung, Shida Liu, Jorge Garca Ponce, Luke Zhu, Yuyang Chen, Ekaterina S. Ivshina, Miguel Fernandez, Minjae Kim, Kennan Gumbs, Matthew Scott Tan, Russell Yang, Mai Hoang, David Brown, Isabella A. Silveira, Lavon Sykes, Ahmed Roman, William Fredenberg, Yiming Chen, Lucas Martin, Yixing Tang, Kelly Werker Smith, Hongyu Liao, Logan G. Wilson, Alexander Dazhen Cai, Andrea Elizabeth Biju, Michael P. Brenner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11774">https://arxiv.org/abs/2505.11774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11774">https://arxiv.org/pdf/2505.11774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11774]] HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class(https://arxiv.org/abs/2505.11774)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable progress in mathematical problem-solving, but evaluation has largely focused on problems that have exact analytical solutions or involve formal proofs, often overlooking approximation-based problems ubiquitous in applied science and engineering. To fill this gap, we build on prior work and present HARDMath2, a dataset of 211 original problems covering the core topics in an introductory graduate applied math class, including boundary-layer analysis, WKB methods, asymptotic solutions of nonlinear partial differential equations, and the asymptotics of oscillatory integrals. This dataset was designed and verified by the students and instructors of a core graduate applied mathematics course at Harvard. We build the dataset through a novel collaborative environment that challenges students to write and refine difficult problems consistent with the class syllabus, peer-validate solutions, test different models, and automatically check LLM-generated solutions against their own answers and numerical ground truths. Evaluation results show that leading frontier models still struggle with many of the problems in the dataset, highlighting a gap in the mathematical reasoning skills of current LLMs. Importantly, students identified strategies to create increasingly difficult problems by interacting with the models and exploiting common failure modes. This back-and-forth with the models not only resulted in a richer and more challenging benchmark but also led to qualitative improvements in the students' understanding of the course material, which is increasingly important as we enter an age where state-of-the-art language models can solve many challenging problems across a wide domain of fields.</li>
</ul>

<h3>Title: Generative and Contrastive Graph Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiali Chen, Avijit Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11776">https://arxiv.org/abs/2505.11776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11776">https://arxiv.org/pdf/2505.11776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11776]] Generative and Contrastive Graph Representation Learning(https://arxiv.org/abs/2505.11776)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) on graphs generates node and graph representations (i.e., embeddings) that can be used for downstream tasks such as node classification, node clustering, and link prediction. Graph SSL is particularly useful in scenarios with limited or no labeled data. Existing SSL methods predominantly follow contrastive or generative paradigms, each excelling in different tasks: contrastive methods typically perform well on classification tasks, while generative methods often excel in link prediction. In this paper, we present a novel architecture for graph SSL that integrates the strengths of both approaches. Our framework introduces community-aware node-level contrastive learning, providing more robust and effective positive and negative node pairs generation, alongside graph-level contrastive learning to capture global semantic information. Additionally, we employ a comprehensive augmentation strategy that combines feature masking, node perturbation, and edge perturbation, enabling robust and diverse representation learning. By incorporating these enhancements, our model achieves superior performance across multiple tasks, including node classification, clustering, and link prediction. Evaluations on open benchmark datasets demonstrate that our model outperforms state-of-the-art methods, achieving a performance lift of 0.23%-2.01% depending on the task and dataset.</li>
</ul>

<h3>Title: Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations</h3>
<ul>
<li><strong>Authors: </strong>Fu-Yun Wang, Keqiang Sun, Yao Teng, Xihui Liu, Jiaming Song, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11777">https://arxiv.org/abs/2505.11777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11777">https://arxiv.org/pdf/2505.11777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11777]] Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations(https://arxiv.org/abs/2505.11777)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable success in various visual generation tasks, including image, video, and 3D content generation. Preference optimization (PO) is a prominent and growing area of research that aims to align these models with human preferences. While existing PO methods primarily concentrate on producing favorable outputs, they often overlook the significance of classifier-free guidance (CFG) in mitigating undesirable results. Diffusion-NPO addresses this gap by introducing negative preference optimization (NPO), training models to generate outputs opposite to human preferences and thereby steering them away from unfavorable outcomes. However, prior NPO approaches, including Diffusion-NPO, rely on costly and fragile procedures for obtaining explicit preference annotations (e.g., manual pairwise labeling or reward model training), limiting their practicality in domains where such data are scarce or difficult to acquire. In this work, we introduce Self-NPO, a Negative Preference Optimization approach that learns exclusively from the model itself, thereby eliminating the need for manual data labeling or reward model training. Moreover, our method is highly efficient and does not require exhaustive data sampling. We demonstrate that Self-NPO integrates seamlessly into widely used diffusion models, including SD1.5, SDXL, and CogVideoX, as well as models already optimized for human preferences, consistently enhancing both their generation quality and alignment with human preferences.</li>
</ul>

<h3>Title: Multi-Order Wavelet Derivative Transform for Deep Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Zhou, Jiaxi Hu, Qingsong Wen, James T. Kwok, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11781">https://arxiv.org/abs/2505.11781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11781">https://arxiv.org/pdf/2505.11781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11781]] Multi-Order Wavelet Derivative Transform for Deep Time Series Forecasting(https://arxiv.org/abs/2505.11781)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In deep time series forecasting, the Fourier Transform (FT) is extensively employed for frequency representation learning. However, it often struggles in capturing multi-scale, time-sensitive patterns. Although the Wavelet Transform (WT) can capture these patterns through frequency decomposition, its coefficients are insensitive to change points in time series, leading to suboptimal modeling. To mitigate these limitations, we introduce the multi-order Wavelet Derivative Transform (WDT) grounded in the WT, enabling the extraction of time-aware patterns spanning both the overall trend and subtle fluctuations. Compared with the standard FT and WT, which model the raw series, the WDT operates on the derivative of the series, selectively magnifying rate-of-change cues and exposing abrupt regime shifts that are particularly informative for time series modeling. Practically, we embed the WDT into a multi-branch framework named WaveTS, which decomposes the input series into multi-scale time-frequency coefficients, refines them via linear layers, and reconstructs them into the time domain via the inverse WDT. Extensive experiments on ten benchmark datasets demonstrate that WaveTS achieves state-of-the-art forecasting accuracy while retaining high computational efficiency.</li>
</ul>

<h3>Title: JULI: Jailbreak Large Language Models by Self-Introspection</h3>
<ul>
<li><strong>Authors: </strong>Jesson Wang, Zhanhao Hu, David Wagner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11790">https://arxiv.org/abs/2505.11790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11790">https://arxiv.org/pdf/2505.11790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11790]] JULI: Jailbreak Large Language Models by Self-Introspection(https://arxiv.org/abs/2505.11790)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained with safety alignment to prevent generating malicious content. Although some attacks have highlighted vulnerabilities in these safety-aligned LLMs, they typically have limitations, such as necessitating access to the model weights or the generation process. Since proprietary models through API-calling do not grant users such permissions, these attacks find it challenging to compromise them. In this paper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks LLMs by manipulating the token log probabilities, using a tiny plug-in block, BiasNet. JULI relies solely on the knowledge of the target LLM's predicted token log probabilities. It can effectively jailbreak API-calling LLMs under a black-box setting and knowing only top-$5$ token log probabilities. Our approach demonstrates superior effectiveness, outperforming existing state-of-the-art (SOTA) approaches across multiple metrics.</li>
</ul>

<h3>Title: CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection</h3>
<ul>
<li><strong>Authors: </strong>Jianing Wang, Siying Guo, Zheng Hua, Runhu Huang, Jinyu Hu, Maoguo Gong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11793">https://arxiv.org/abs/2505.11793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11793">https://arxiv.org/pdf/2505.11793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11793]] CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection(https://arxiv.org/abs/2505.11793)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) has attracted remarkable attention in hyperspectral image (HSI) processing fields, and most existing deep learning (DL)-based algorithms indicate dramatic potential for detecting anomaly samples through specific training process under current scenario. However, the limited prior information and the catastrophic forgetting problem indicate crucial challenges for existing DL structure in open scenarios cross-domain detection. In order to improve the detection performance, a novel continual learning-based capsule differential generative adversarial network (CL-CaGAN) is proposed to elevate the cross-scenario learning performance for facilitating the real application of DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule structure with adversarial learning network is constructed to estimate the background distribution for surmounting the deficiency of prior information. To mitigate the catastrophic forgetting phenomenon, clustering-based sample replay strategy and a designed extra self-distillation regularization are integrated for merging the history and future knowledge in continual AD task, while the discriminative learning ability from previous detection scenario to current scenario is retained by the elaborately designed structure with continual learning (CL) strategy. In addition, the differentiable enhancement is enforced to augment the generation performance of the training data. This further stabilizes the training process with better convergence and efficiently consolidates the reconstruction ability of background samples. To verify the effectiveness of our proposed CL-CaGAN, we conduct experiments on several real HSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher detection performance and continuous learning capacity for mitigating the catastrophic forgetting under cross-domain scenarios.</li>
</ul>

<h3>Title: CL-BioGAN: Biologically-Inspired Cross-Domain Continual Learning for Hyperspectral Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jianing Wang, Zheng Hua, Wan Zhang, Shengjia Hao, Yuqiong Yao, Maoguo Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11796">https://arxiv.org/abs/2505.11796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11796">https://arxiv.org/pdf/2505.11796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11796]] CL-BioGAN: Biologically-Inspired Cross-Domain Continual Learning for Hyperspectral Anomaly Detection(https://arxiv.org/abs/2505.11796)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Memory stability and learning flexibility in continual learning (CL) is a core challenge for cross-scene Hyperspectral Anomaly Detection (HAD) task. Biological neural networks can actively forget history knowledge that conflicts with the learning of new experiences by regulating learning-triggered synaptic expansion and synaptic convergence. Inspired by this phenomenon, we propose a novel Biologically-Inspired Continual Learning Generative Adversarial Network (CL-BioGAN) for augmenting continuous distribution fitting ability for cross-domain HAD task, where Continual Learning Bio-inspired Loss (CL-Bio Loss) and self-attention Generative Adversarial Network (BioGAN) are incorporated to realize forgetting history knowledge as well as involving replay strategy in the proposed BioGAN. Specifically, a novel Bio-Inspired Loss composed with an Active Forgetting Loss (AF Loss) and a CL loss is designed to realize parameters releasing and enhancing between new task and history tasks from a Bayesian perspective. Meanwhile, BioGAN loss with L2-Norm enhances self-attention (SA) to further balance the stability and flexibility for better fitting background distribution for open scenario HAD (OHAD) tasks. Experiment results underscore that the proposed CL-BioGAN can achieve more robust and satisfying accuracy for cross-domain HAD with fewer parameters and computation cost. This dual contribution not only elevates CL performance but also offers new insights into neural adaptation mechanisms in OHAD task.</li>
</ul>

<h3>Title: Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jian Zhu, He Wang, Yang Xu, Zebin Wu, Zhihui Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11800">https://arxiv.org/abs/2505.11800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11800">https://arxiv.org/pdf/2505.11800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11800]] Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model(https://arxiv.org/abs/2505.11800)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Hyperspectral and multispectral image (HSI-MSI) fusion involves combining a low-resolution hyperspectral image (LR-HSI) with a high-resolution multispectral image (HR-MSI) to generate a high-resolution hyperspectral image (HR-HSI). Most deep learning-based methods for HSI-MSI fusion rely on large amounts of hyperspectral data for supervised training, which is often scarce in practical applications. In this paper, we propose a self-learning Adaptive Residual Guided Subspace Diffusion Model (ARGS-Diff), which only utilizes the observed images without any extra training data. Specifically, as the LR-HSI contains spectral information and the HR-MSI contains spatial information, we design two lightweight spectral and spatial diffusion models to separately learn the spectral and spatial distributions from them. Then, we use these two models to reconstruct HR-HSI from two low-dimensional components, i.e, the spectral basis and the reduced coefficient, during the reverse diffusion process. Furthermore, we introduce an Adaptive Residual Guided Module (ARGM), which refines the two components through a residual guided function at each sampling step, thereby stabilizing the sampling process. Extensive experimental results demonstrate that ARGS-Diff outperforms existing state-of-the-art methods in terms of both performance and computational efficiency in the field of HSI-MSI fusion. Code is available at this https URL.</li>
</ul>

<h3>Title: Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness</h3>
<ul>
<li><strong>Authors: </strong>Chuang Zhao, Hui Tang, Hongke Zhao, Xiaomeng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11802">https://arxiv.org/abs/2505.11802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11802">https://arxiv.org/pdf/2505.11802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11802]] Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness(https://arxiv.org/abs/2505.11802)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Advanced healthcare predictions offer significant improvements in patient outcomes by leveraging predictive analytics. Existing works primarily utilize various views of Electronic Health Record (EHR) data, such as diagnoses, lab tests, or clinical notes, for model training. These methods typically assume the availability of complete EHR views and that the designed model could fully leverage the potential of each view. However, in practice, random missing views and view laziness present two significant challenges that hinder further improvements in multi-view utilization. To address these challenges, we introduce Diffmv, an innovative diffusion-based generative framework designed to advance the exploitation of multiple views of EHR data. Specifically, to address random missing views, we integrate various views of EHR data into a unified diffusion-denoising framework, enriched with diverse contextual conditions to facilitate progressive alignment and view transformation. To mitigate view laziness, we propose a novel reweighting strategy that assesses the relative advantages of each view, promoting a balanced utilization of various data views within the model. Our proposed strategy achieves superior performance across multiple health prediction tasks derived from three popular datasets, including multi-view and multi-modality scenarios.</li>
</ul>

<h3>Title: Are vision language models robust to uncertain inputs?</h3>
<ul>
<li><strong>Authors: </strong>Xi Wang, Eric Nalisnick</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11804">https://arxiv.org/abs/2505.11804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11804">https://arxiv.org/pdf/2505.11804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11804]] Are vision language models robust to uncertain inputs?(https://arxiv.org/abs/2505.11804)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robustness against uncertain and ambiguous inputs is a critical challenge for deep learning models. While recent advancements in large scale vision language models (VLMs, e.g. GPT4o) might suggest that increasing model and training dataset size would mitigate this issue, our empirical evaluation shows a more complicated picture. Testing models using two classic uncertainty quantification tasks, anomaly detection and classification under inherently ambiguous conditions, we find that newer and larger VLMs indeed exhibit improved robustness compared to earlier models, but still suffer from a tendency to strictly follow instructions, often causing them to hallucinate confident responses even when faced with unclear or anomalous inputs. Remarkably, for natural images such as ImageNet, this limitation can be overcome without pipeline modifications: simply prompting models to abstain from uncertain predictions enables significant reliability gains, achieving near-perfect robustness in several settings. However, for domain-specific tasks such as galaxy morphology classification, a lack of specialized knowledge prevents reliable uncertainty estimation. Finally, we propose a novel mechanism based on caption diversity to reveal a model's internal uncertainty, enabling practitioners to predict when models will successfully abstain without relying on labeled data.</li>
</ul>

<h3>Title: Retrospex: Language Agent Meets Offline Reinforcement Learning Critic</h3>
<ul>
<li><strong>Authors: </strong>Yufei Xiang, Yiqun Shen, Yeqin Zhang, Cam-Tu Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11807">https://arxiv.org/abs/2505.11807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11807">https://arxiv.org/pdf/2505.11807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11807]] Retrospex: Language Agent Meets Offline Reinforcement Learning Critic(https://arxiv.org/abs/2505.11807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM's context. Instead, it combines the LLM's action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline ''retrospection'' process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong, contemporary baselines.</li>
</ul>

<h3>Title: Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Shen Li, Renfen Hu, Lijun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11810">https://arxiv.org/abs/2505.11810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11810">https://arxiv.org/pdf/2505.11810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11810]] Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model(https://arxiv.org/abs/2505.11810)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>General-purpose large language models demonstrate notable capabilities in language comprehension and generation, achieving results that are comparable to, or even surpass, human performance in many language information processing tasks. Nevertheless, when general models are applied to some specific domains, e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and fine-tuning open-source foundational models similarly struggles to adequately incorporate domain-specific knowledge. To address this challenge, this study developed a large language model, AI Taiyan, specifically designed for understanding and generating Classical Chinese. Experiments show that with a reasonable model design, data processing, foundational training, and fine-tuning, satisfactory results can be achieved with only 1.8 billion parameters. In key tasks related to Classical Chinese information processing such as punctuation, identification of allusions, explanation of word meanings, and translation between ancient and modern Chinese, this model exhibits a clear advantage over both general-purpose large models and domain-specific traditional models, achieving levels close to or surpassing human baselines. This research provides a reference for the efficient construction of specialized domain-specific large language models. Furthermore, the paper discusses the application of this model in fields such as the collation of ancient texts, dictionary editing, and language research, combined with case studies.</li>
</ul>

<h3>Title: BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Xiaofeng He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11811">https://arxiv.org/abs/2505.11811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11811">https://arxiv.org/pdf/2505.11811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11811]] BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering(https://arxiv.org/abs/2505.11811)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-hop question answering (QA) involves finding multiple relevant passages and performing step-by-step reasoning to answer complex questions. Previous works on multi-hop QA employ specific methods from different modeling perspectives based on large language models (LLMs), regardless of the question types. In this paper, we first conduct an in-depth analysis of public multi-hop QA benchmarks, dividing the questions into four types and evaluating five types of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step, Iterative-step, Sub-step, and Adaptive-step. We find that different types of multi-hop questions have varying degrees of sensitivity to different types of methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to address multi-hop QA by specifically focusing on the correspondence between question types and methods, where each type of method is regarded as an ''operator'' by prompting LLMs differently. The first level of BELLE includes multiple agents that debate to obtain an executive plan of combined ''operators'' to address the multi-hop QA task comprehensively. During the debate, in addition to the basic roles of affirmative debater, negative debater, and judge, at the second level, we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable. Extensive experiments demonstrate that BELLE significantly outperforms strong baselines in various datasets. Additionally, the model consumption of BELLE is higher cost-effectiveness than that of single models in more complex multi-hop QA scenarios.</li>
</ul>

<h3>Title: SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Dong, Fang-Yi Su, Jung-Hsien Chiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11813">https://arxiv.org/abs/2505.11813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11813">https://arxiv.org/pdf/2505.11813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11813]] SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation(https://arxiv.org/abs/2505.11813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data augmentation for domain-specific image classification tasks often struggles to simultaneously address diversity, faithfulness, and label clarity of generated data, leading to suboptimal performance in downstream tasks. While existing generative diffusion model-based methods aim to enhance augmentation, they fail to cohesively tackle these three critical aspects and often overlook intrinsic challenges of diffusion models, such as sensitivity to model characteristics and stochasticity under strong transformations. In this paper, we propose a novel framework that explicitly integrates diversity, faithfulness, and label clarity into the augmentation process. Our approach employs saliency-guided mixing and a fine-tuned diffusion model to preserve foreground semantics, enrich background diversity, and ensure label consistency, while mitigating diffusion model limitations. Extensive experiments across fine-grained, long-tail, few-shot, and background robustness tasks demonstrate our method's superior performance over state-of-the-art approaches.</li>
</ul>

<h3>Title: UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Qin, Yuan Pu, Zhuolun He, Seunggeun Kim, David Z. Pan, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11815">https://arxiv.org/abs/2505.11815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11815">https://arxiv.org/pdf/2505.11815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11815]] UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings(https://arxiv.org/abs/2505.11815)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current research has explored vision-language models for multi-modal embedding tasks, such as information retrieval, visual grounding, and classification. However, real-world scenarios often involve diverse modality combinations between queries and targets, such as text and image to text, text and image to text and image, and text to text and image. These diverse combinations pose significant challenges for existing models, as they struggle to align all modality combinations within a unified embedding space during training, which degrades performance at inference. To address this limitation, we propose UniMoCo, a novel vision-language model architecture designed for multi-modal embedding tasks. UniMoCo introduces a modality-completion module that generates visual features from textual inputs, ensuring modality completeness for both queries and targets. Additionally, we develop a specialized training strategy to align embeddings from both original and modality-completed inputs, ensuring consistency within the embedding space. This enables the model to robustly handle a wide range of modality combinations across embedding tasks. Experiments show that UniMoCo outperforms previous methods while demonstrating consistent robustness across diverse settings. More importantly, we identify and quantify the inherent bias in conventional approaches caused by imbalance of modality combinations in training data, which can be mitigated through our modality-completion paradigm. The code is available at this https URL.</li>
</ul>

<h3>Title: Chain-of-Model Learning for Language Model</h3>
<ul>
<li><strong>Authors: </strong>Kaitao Song, Xiaohua Wang, Xu Tan, Huiqiang Jiang, Chengruidong Zhang, Yongliang Shen, Cen LU, Zihao Li, Zifan Song, Caihua Shan, Yansen Wang, Kan Ren, Xiaoqing Zheng, Tao Qin, Yuqing Yang, Dongsheng Li, Lili Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11820">https://arxiv.org/abs/2505.11820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11820">https://arxiv.org/pdf/2505.11820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11820]] Chain-of-Model Learning for Language Model(https://arxiv.org/abs/2505.11820)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: this https URL.</li>
</ul>

<h3>Title: Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment</h3>
<ul>
<li><strong>Authors: </strong>Siliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, Mingyi Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11821">https://arxiv.org/abs/2505.11821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11821">https://arxiv.org/pdf/2505.11821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11821]] Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment(https://arxiv.org/abs/2505.11821)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates approaches to enhance the reasoning capabilities of Large Language Model (LLM) agents using Reinforcement Learning (RL). Specifically, we focus on multi-turn tool-use scenarios, which can be naturally modeled as Markov Decision Processes (MDPs). While existing approaches often train multi-turn LLM agents with trajectory-level advantage estimation in bandit settings, they struggle with turn-level credit assignment across multiple decision steps, limiting their performance on multi-turn reasoning tasks. To address this, we introduce a fine-grained turn-level advantage estimation strategy to enable more precise credit assignment in multi-turn agent interactions. The strategy is general and can be incorporated into various RL algorithms such as Group Relative Preference Optimization (GRPO). Our experimental evaluation on multi-turn reasoning and search-based tool-use tasks with GRPO implementations highlights the effectiveness of the MDP framework and the turn-level credit assignment in advancing the multi-turn reasoning capabilities of LLM agents in complex decision-making settings. Our method achieves 100% success in tool execution and 50% accuracy in exact answer matching, significantly outperforming baselines, which fail to invoke tools and achieve only 20-30% exact match accuracy.</li>
</ul>

<h3>Title: Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Ke Li, Di Wang, Xiaowei Wang, Zhihong Wu, Yiming Zhang, Yifeng Wang, Quan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11822">https://arxiv.org/abs/2505.11822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11822">https://arxiv.org/pdf/2505.11822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11822]] Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement(https://arxiv.org/abs/2505.11822)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cross-view geo-localization (CVGL) aims to match images of the same geographic location captured from different perspectives, such as drones and satellites. Despite recent advances, CVGL remains highly challenging due to significant appearance changes and spatial distortions caused by viewpoint variations. Existing methods typically assume that cross-view images can be directly aligned within a shared feature space by maximizing feature similarity through contrastive learning. Nonetheless, this assumption overlooks the inherent conflicts induced by viewpoint discrepancies, resulting in extracted features containing inconsistent information that hinders precise localization. In this study, we take a manifold learning perspective and model the feature space of cross-view images as a composite manifold jointly governed by content and viewpoint information. Building upon this insight, we propose $\textbf{CVD}$, a new CVGL framework that explicitly disentangles $\textit{content}$ and $\textit{viewpoint}$ factors. To promote effective disentanglement, we introduce two constraints: $\textit{(i)}$ An intra-view independence constraint, which encourages statistical independence between the two factors by minimizing their mutual information. $\textit{(ii)}$ An inter-view reconstruction constraint that reconstructs each view by cross-combining $\textit{content}$ and $\textit{viewpoint}$ from paired images, ensuring factor-specific semantics are preserved. As a plug-and-play module, CVD can be seamlessly integrated into existing geo-localization pipelines. Extensive experiments on four benchmarks, i.e., University-1652, SUES-200, CVUSA, and CVACT, demonstrate that CVD consistently improves both localization accuracy and generalization across multiple baselines.</li>
</ul>

<h3>Title: Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data</h3>
<ul>
<li><strong>Authors: </strong>Xudong Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11825">https://arxiv.org/abs/2505.11825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11825">https://arxiv.org/pdf/2505.11825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11825]] Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data(https://arxiv.org/abs/2505.11825)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, diffusion</a></li>
<li><strong>Abstract: </strong>Training diffusion models requires large datasets. However, acquiring large volumes of high-quality data can be challenging, for example, collecting large numbers of high-resolution images and long videos. On the other hand, there are many complementary data that are usually considered corrupted or partial, such as low-resolution images and short videos. Other examples of corrupted data include videos that contain subtitles, watermarks, and logos. In this study, we investigate the theoretical problem of whether the above partial data can be utilized to train conventional diffusion models. Motivated by our theoretical analysis in this study, we propose a straightforward approach of training diffusion models utilizing partial data views, where we consider each form of complementary data as a view of conventional data. Our proposed approach first trains one separate diffusion model for each individual view, and then trains a model for predicting the residual score function. We prove generalization error bounds, which show that the proposed diffusion model training approach can achieve lower generalization errors if proper regularizations are adopted in the residual score function training. In particular, we prove that the difficulty in training the residual score function scales proportionally with the signal correlations not captured by partial data views. Consequently, the proposed approach achieves near first-order optimal data efficiency.</li>
</ul>

<h3>Title: Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yansong Ning, Wei Li, Jun Fang, Naiqiang Tan, Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11827">https://arxiv.org/abs/2505.11827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11827">https://arxiv.org/pdf/2505.11827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11827]] Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning(https://arxiv.org/abs/2505.11827)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Compressing long chain-of-thought (CoT) from large language models (LLMs) is an emerging strategy to improve the reasoning efficiency of LLMs. Despite its promising benefits, existing studies equally compress all thoughts within a long CoT, hindering more concise and effective reasoning. To this end, we first investigate the importance of different thoughts by examining their effectiveness and efficiency in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. Building upon the insights, we propose a theoretically bounded metric to jointly measure the effectiveness and efficiency of different thoughts. We then propose Long$\otimes$Short, an efficient reasoning framework that enables two LLMs to collaboratively solve the problem: a long-thought LLM for more effectively generating important thoughts, while a short-thought LLM for efficiently generating remaining thoughts. Specifically, we begin by synthesizing a small amount of cold-start data to fine-tune LLMs for long-thought and short-thought reasoning styles, respectively. Furthermore, we propose a synergizing-oriented multi-turn reinforcement learning, focusing on the model self-evolution and collaboration between long-thought and short-thought LLMs. Experimental results show that our method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and GPQA Diamond benchmarks. Our data and code are available at this https URL.</li>
</ul>

<h3>Title: Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks</h3>
<ul>
<li><strong>Authors: </strong>Chenlu Wang, Weimin Lyu, Ritwik Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11829">https://arxiv.org/abs/2505.11829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11829">https://arxiv.org/pdf/2505.11829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11829]] Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks(https://arxiv.org/abs/2505.11829)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Detecting deviant language such as sexism, or nuanced language such as metaphors or sarcasm, is crucial for enhancing the safety, clarity, and interpretation of online social discourse. While existing classifiers deliver strong results on these tasks, they often come with significant computational cost and high data demands. In this work, we propose \textbf{Cla}ss \textbf{D}istillation (ClaD), a novel training paradigm that targets the core challenge: distilling a small, well-defined target class from a highly diverse and heterogeneous background. ClaD integrates two key innovations: (i) a loss function informed by the structural properties of class distributions, based on Mahalanobis distance, and (ii) an interpretable decision algorithm optimized for class separation. Across three benchmark detection tasks -- sexism, metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with smaller language models and orders of magnitude fewer parameters, achieves performance comparable to several large language models (LLMs). These results demonstrate ClaD as an efficient tool for pragmatic language understanding tasks that require gleaning a small target class from a larger heterogeneous background.</li>
</ul>

<h3>Title: Multilingual Collaborative Defense for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, Kaiyu Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11835">https://arxiv.org/abs/2505.11835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11835">https://arxiv.org/pdf/2505.11835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11835]] Multilingual Collaborative Defense for Large Language Models(https://arxiv.org/abs/2505.11835)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The robustness and security of large language models (LLMs) has become a prominent research area. One notable vulnerability is the ability to bypass LLM safeguards by translating harmful queries into rare or underrepresented languages, a simple yet effective method of "jailbreaking" these models. Despite the growing concern, there has been limited research addressing the safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to enhance multilingual safety. In this work, we investigate the correlation between various attack features across different languages and propose Multilingual Collaborative Defense (MCD), a novel learning method that optimizes a continuous, soft safety prompt automatically to facilitate multilingual safeguarding of LLMs. The MCD approach offers three advantages: First, it effectively improves safeguarding performance across multiple languages. Second, MCD maintains strong generalization capabilities while minimizing false refusal rates. Third, MCD mitigates the language safety misalignment caused by imbalances in LLM training corpora. To evaluate the effectiveness of MCD, we manually construct multilingual versions of commonly used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess various safeguarding methods. Additionally, we introduce these datasets in underrepresented (zero-shot) languages to verify the language transferability of MCD. The results demonstrate that MCD outperforms existing approaches in safeguarding against multilingual jailbreak attempts while also exhibiting strong language transfer capabilities. Our code is available at this https URL.</li>
</ul>

<h3>Title: SplInterp: Improving our Understanding and Training of Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Budd, Javier Ideami, Benjamin Macdowall Rynne, Keith Duggar, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11836">https://arxiv.org/abs/2505.11836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11836">https://arxiv.org/pdf/2505.11836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11836]] SplInterp: Improving our Understanding and Training of Sparse Autoencoders(https://arxiv.org/abs/2505.11836)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) have received considerable recent attention as tools for mechanistic interpretability, showing success at extracting interpretable features even from very large LLMs. However, this research has been largely empirical, and there have been recent doubts about the true utility of SAEs. In this work, we seek to enhance the theoretical understanding of SAEs, using the spline theory of deep learning. By situating SAEs in this framework: we discover that SAEs generalise ``$k$-means autoencoders'' to be piecewise affine, but sacrifice accuracy for interpretability vs. the optimal ``$k$-means-esque plus local principal component analysis (PCA)'' piecewise affine autoencoder. We characterise the underlying geometry of (TopK) SAEs using power diagrams. And we develop a novel proximal alternating method SGD (PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations and promising empirical results in MNIST and LLM experiments, particularly in sample efficiency and (in the LLM setting) improved sparsity of codes. All code is available at: this https URL</li>
</ul>

<h3>Title: On Membership Inference Attacks in Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Ziyao Cui, Minxing Zhang, Jian Pei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11837">https://arxiv.org/abs/2505.11837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11837">https://arxiv.org/pdf/2505.11837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11837]] On Membership Inference Attacks in Knowledge Distillation(https://arxiv.org/abs/2505.11837)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, attack, robust, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Nowadays, Large Language Models (LLMs) are trained on huge datasets, some including sensitive information. This poses a serious privacy concern because privacy attacks such as Membership Inference Attacks (MIAs) may detect this sensitive information. While knowledge distillation compresses LLMs into efficient, smaller student models, its impact on privacy remains underexplored. In this paper, we investigate how knowledge distillation affects model robustness against MIA. We focus on two questions. First, how is private data protected in teacher and student models? Second, how can we strengthen privacy preservation against MIAs in knowledge distillation? Through comprehensive experiments, we show that while teacher and student models achieve similar overall MIA accuracy, teacher models better protect member data, the primary target of MIA, whereas student models better protect non-member data. To address this vulnerability in student models, we propose 5 privacy-preserving distillation methods and demonstrate that they successfully reduce student models' vulnerability to MIA, with ensembling further stabilizing the robustness, offering a reliable approach for distilling more secure and efficient student models. Our implementation source code is available at this https URL.</li>
</ul>

<h3>Title: RVTBench: A Benchmark for Visual Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yiqing Shen, Chenjia Li, Chenxiao Fan, Mathias Unberath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11838">https://arxiv.org/abs/2505.11838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11838">https://arxiv.org/pdf/2505.11838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11838]] RVTBench: A Benchmark for Visual Reasoning Tasks(https://arxiv.org/abs/2505.11838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Visual reasoning, the capability to interpret visual input in response to implicit text query through multi-step reasoning, remains a challenge for deep learning models due to the lack of relevant benchmarks. Previous work in visual reasoning has primarily focused on reasoning segmentation, where models aim to segment objects based on implicit text queries. This paper introduces reasoning visual tasks (RVTs), a unified formulation that extends beyond traditional video reasoning segmentation to a diverse family of visual language reasoning problems, which can therefore accommodate multiple output formats including bounding boxes, natural language descriptions, and question-answer pairs. Correspondingly, we identify the limitations in current benchmark construction methods that rely solely on large language models (LLMs), which inadequately capture complex spatial-temporal relationships and multi-step reasoning chains in video due to their reliance on token representation, resulting in benchmarks with artificially limited reasoning complexity. To address this limitation, we propose a novel automated RVT benchmark construction pipeline that leverages digital twin (DT) representations as structured intermediaries between perception and the generation of implicit text queries. Based on this method, we construct RVTBench, a RVT benchmark containing 3,896 queries of over 1.2 million tokens across four types of RVT (segmentation, grounding, VQA and summary), three reasoning categories (semantic, spatial, and temporal), and four increasing difficulty levels, derived from 200 video sequences. Finally, we propose RVTagent, an agent framework for RVT that allows for zero-shot generalization across various types of RVT without task-specific fine-tuning.</li>
</ul>

<h3>Title: On the $O(\frac{\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\ell_1$ Norm</h3>
<ul>
<li><strong>Authors: </strong>Huan Li, Yiming Dong, Zhouchen Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11840">https://arxiv.org/abs/2505.11840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11840">https://arxiv.org/pdf/2505.11840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11840]] On the $O(\frac{\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\ell_1$ Norm(https://arxiv.org/abs/2505.11840)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the default optimizer for training large language models, AdamW has achieved remarkable success in deep learning. However, its convergence behavior is not theoretically well-understood. This paper establishes the convergence rate $\frac{1}{K}\sum_{k=1}^KE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}C}{K^{1/4}})$ for AdamW measured by $\ell_1$ norm, where $K$ represents the iteration number, $d$ denotes the model dimension, and $C$ matches the constant in the optimal convergence rate of SGD. Theoretically, we have $E\left[\|\nabla f(x)\|_1\right]\geq\sqrt{\frac{2d}{\pi}}E\left[\|\nabla f(x)\|_2\right]$ when each element of $\nabla f(x)$ is generated from Gaussian distribution $\mathcal N(0,1)$. Empirically, our experimental results on real-world deep learning tasks reveal $\|\nabla f(x)\|_1=\varTheta(\sqrt{d})\|\nabla f(x)\|_2$. Both support that our convergence rate can be considered to be analogous to the optimal $\frac{1}{K}\sum_{k=1}^KE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{C}{K^{1/4}})$ convergence rate of SGD.</li>
</ul>

<h3>Title: Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuannan Liu, Zekun Li, Zheqi He, Peipei Li, Shuhan Xia, Xing Cui, Huaibo Huang, Xi Yang, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11842">https://arxiv.org/abs/2505.11842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11842">https://arxiv.org/pdf/2505.11842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11842]] Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs(https://arxiv.org/abs/2505.11842)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>The increasing deployment of Large Vision-Language Models (LVLMs) raises safety concerns under potential malicious inputs. However, existing multimodal safety evaluations primarily focus on model vulnerabilities exposed by static image inputs, ignoring the temporal dynamics of video that may induce distinct safety risks. To bridge this gap, we introduce Video-SafetyBench, the first comprehensive benchmark designed to evaluate the safety of LVLMs under video-text attacks. It comprises 2,264 video-text pairs spanning 48 fine-grained unsafe categories, each pairing a synthesized video with either a harmful query, which contains explicit malice, or a benign query, which appears harmless but triggers harmful behavior when interpreted alongside the video. To generate semantically accurate videos for safety evaluation, we design a controllable pipeline that decomposes video semantics into subject images (what is shown) and motion text (how it moves), which jointly guide the synthesis of query-relevant videos. To effectively evaluate uncertain or borderline harmful outputs, we propose RJScore, a novel LLM-based metric that incorporates the confidence of judge models and human-aligned decision threshold calibration. Extensive experiments show that benign-query video composition achieves average attack success rates of 67.2%, revealing consistent vulnerabilities to video-induced attacks. We believe Video-SafetyBench will catalyze future research into video-based safety evaluation and defense strategies.</li>
</ul>

<h3>Title: MedSG-Bench: A Benchmark for Medical Image Sequences Grounding</h3>
<ul>
<li><strong>Authors: </strong>Jingkun Yue, Siqi Zhang, Zinan Jia, Huihuan Xu, Zongbo Han, Xiaohong Liu, Guangyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11852">https://arxiv.org/abs/2505.11852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11852">https://arxiv.org/pdf/2505.11852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11852]] MedSG-Bench: A Benchmark for Medical Image Sequences Grounding(https://arxiv.org/abs/2505.11852)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual grounding is essential for precise perception and reasoning in multimodal large language models (MLLMs), especially in medical imaging domains. While existing medical visual grounding benchmarks primarily focus on single-image scenarios, real-world clinical applications often involve sequential images, where accurate lesion localization across different modalities and temporal tracking of disease progression (e.g., pre- vs. post-treatment comparison) require fine-grained cross-image semantic alignment and context-aware reasoning. To remedy the underrepresentation of image sequences in existing medical visual grounding benchmarks, we propose MedSG-Bench, the first benchmark tailored for Medical Image Sequences Grounding. It comprises eight VQA-style tasks, formulated into two paradigms of the grounding tasks, including 1) Image Difference Grounding, which focuses on detecting change regions across images, and 2) Image Consistency Grounding, which emphasizes detection of consistent or shared semantics across sequential images. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities, and a wide spectrum of anatomical structures and diseases, totaling 9,630 question-answer pairs. We benchmark both general-purpose MLLMs (e.g., Qwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision), observing that even the advanced models exhibit substantial limitations in medical sequential grounding tasks. To advance this field, we construct MedSG-188K, a large-scale instruction-tuning dataset tailored for sequential visual grounding, and further develop MedSeq-Grounder, an MLLM designed to facilitate future research on fine-grained understanding across medical sequential images. The benchmark, dataset, and model are available at this https URL</li>
</ul>

<h3>Title: When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research</h3>
<ul>
<li><strong>Authors: </strong>Guijin Son, Jiwoo Hong, Honglu Fan, Heejeong Nam, Hyunwoo Ko, Seungwon Lim, Jinyeop Song, Jinha Choi, Gonalo Paulo, Youngjae Yu, Stella Biderman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11855">https://arxiv.org/abs/2505.11855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11855">https://arxiv.org/pdf/2505.11855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11855]] When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research(https://arxiv.org/abs/2505.11855)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the \textbf{academic verification of scientific manuscripts}. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.</li>
</ul>

<h3>Title: Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Kalyan Cherukuri, Aarav Lala</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11864">https://arxiv.org/abs/2505.11864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11864">https://arxiv.org/pdf/2505.11864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11864]] Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning(https://arxiv.org/abs/2505.11864)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As generative agents become increasingly capable, alignment of their behavior with complex human values remains a fundamental challenge. Existing approaches often simplify human intent through reduction to a scalar reward, overlooking the multi-faceted nature of human feedback. In this work, we introduce a theoretical framework for preference-based Multi-Objective Inverse Reinforcement Learning (MO-IRL), where human preferences are modeled as latent vector-valued reward functions. We formalize the problem of recovering a Pareto-optimal reward representation from noisy preference queries and establish conditions for identifying the underlying multi-objective structure. We derive tight sample complexity bounds for recovering $\epsilon$-approximations of the Pareto front and introduce a regret formulation to quantify suboptimality in this multi-objective setting. Furthermore, we propose a provably convergent algorithm for policy optimization using preference-inferred reward cones. Our results bridge the gap between practical alignment techniques and theoretical guarantees, providing a principled foundation for learning aligned behaviors in a high-dimension and value-pluralistic environment.</li>
</ul>

<h3>Title: MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Zhou, Xiaogang Wang, Yulan Guo, Kai Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11868">https://arxiv.org/abs/2505.11868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11868">https://arxiv.org/pdf/2505.11868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11868]] MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos(https://arxiv.org/abs/2505.11868)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurately analyzing the motion parts and their motion attributes in dynamic environments is crucial for advancing key areas such as embodied intelligence. Addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations, we propose an innovative framework that can analyze 3D mobility from monocular videos in a zero-shot manner. This framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. Specifically, our method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, we introduce an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. To validate the robustness and wide applicability of our method, we created a comprehensive dataset comprising both simulated and real-world scenarios. Experimental results show that our framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications.</li>
</ul>

<h3>Title: PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Quoc-Huy Trinh, Minh-Van Nguyen, Jung Peng, Ulas Bagci, Debesh Jha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11872">https://arxiv.org/abs/2505.11872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11872">https://arxiv.org/pdf/2505.11872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11872]] PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging(https://arxiv.org/abs/2505.11872)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements in prompt-based medical image segmentation have enabled clinicians to identify tumors using simple input like bounding boxes or text prompts. However, existing methods face challenges when doctors need to interact through natural language or when position reasoning is required - understanding spatial relationships between anatomical structures and pathologies. We present PRS-Med, a framework that integrates vision-language models with segmentation capabilities to generate both accurate segmentation masks and corresponding spatial reasoning outputs. Additionally, we introduce the MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation), which provides diverse, spatially-grounded question-answer pairs to address the lack of position reasoning data in medical imaging. PRS-Med demonstrates superior performance across six imaging modalities (CT, MRI, X-ray, ultrasound, endoscopy, RGB), significantly outperforming state-of-the-art methods in both segmentation accuracy and position reasoning. Our approach enables intuitive doctor-system interaction through natural language, facilitating more efficient diagnoses. Our dataset pipeline, model, and codebase will be released to foster further research in spatially-aware multimodal reasoning for medical applications.</li>
</ul>

<h3>Title: J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Chi-Min Chan, Chunpu Xu, Jiaming Ji, Zhen Ye, Pengcheng Wen, Chunyang Jiang, Yaodong Yang, Wei Xue, Sirui Han, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11875">https://arxiv.org/abs/2505.11875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11875">https://arxiv.org/pdf/2505.11875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11875]] J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge(https://arxiv.org/abs/2505.11875)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The current focus of AI research is shifting from emphasizing model training towards enhancing evaluation quality, a transition that is crucial for driving further advancements in AI systems. Traditional evaluation methods typically rely on reward models assigning scalar preference scores to outputs. Although effective, such approaches lack interpretability, leaving users often uncertain about why a reward model rates a particular response as high or low. The advent of LLM-as-a-Judge provides a more scalable and interpretable method of supervision, offering insights into the decision-making process. Moreover, with the emergence of large reasoning models, which consume more tokens for deeper thinking and answer refinement, scaling test-time computation in the LLM-as-a-Judge paradigm presents an avenue for further boosting performance and providing more interpretability through reasoning traces. In this paper, we introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on reflection-enhanced datasets collected via rejection-sampling and subsequently trained using Reinforcement Learning (RL) with verifiable rewards. At inference time, we apply Simple Test-Time Scaling (STTS) strategies for additional performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$ surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally, we present three key findings: (1) Existing LLM-as-a-Judge does not inherently exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced datasets continues to demonstrate similarly weak scaling behavior. (3) Significant scaling trend emerges primarily during the RL phase, suggesting that effective STTS capability is acquired predominantly through RL training.</li>
</ul>

<h3>Title: NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yanbo Dai, Zhenlan Ji, Zongjie Li, Shuai Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11876">https://arxiv.org/abs/2505.11876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11876">https://arxiv.org/pdf/2505.11876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11876]] NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization(https://arxiv.org/abs/2505.11876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Model editing techniques are essential for efficiently updating knowledge in large language models (LLMs). However, the effectiveness of existing approaches degrades in massive editing scenarios, particularly when evaluated with practical metrics or in context-rich settings. We attribute these failures to embedding collisions among knowledge items, which undermine editing reliability at scale. To address this, we propose NAMET (Noise-aware Model Editing in Transformers), a simple yet effective method that introduces noise during memory extraction via a one-line modification to MEMIT. Extensive experiments across six LLMs and three datasets demonstrate that NAMET consistently outperforms existing methods when editing thousands of facts.</li>
</ul>

<h3>Title: Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks</h3>
<ul>
<li><strong>Authors: </strong>Giyeong Oh, Woohyun Cho, Siyeol Kim, Suhwan Choi, Younjae Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11881">https://arxiv.org/abs/2505.11881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11881">https://arxiv.org/pdf/2505.11881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11881]] Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks(https://arxiv.org/abs/2505.11881)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\%p top-1 accuracy gain for ViT-B on ImageNet-1k.</li>
</ul>

<h3>Title: GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Shiming Chen, Dingjie Fu, Salman Khan, Fahad Shahbaz Khan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11882">https://arxiv.org/abs/2505.11882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11882">https://arxiv.org/pdf/2505.11882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11882]] GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder(https://arxiv.org/abs/2505.11882)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Remarkable progress in zero-shot learning (ZSL) has been achieved using generative models. However, existing generative ZSL methods merely generate (imagine) the visual features from scratch guided by the strong class semantic vectors annotated by experts, resulting in suboptimal generative performance and limited scene generalization. To address these and advance ZSL, we propose an inductive variational autoencoder for generative zero-shot learning, dubbed GenZSL. Mimicking human-level concept learning, GenZSL operates by inducting new class samples from similar seen classes using weak class semantic vectors derived from target class names (i.e., CLIP text embedding). To ensure the generation of informative samples for training an effective ZSL classifier, our GenZSL incorporates two key strategies. Firstly, it employs class diversity promotion to enhance the diversity of class semantic vectors. Secondly, it utilizes target class-guided information boosting criteria to optimize the model. Extensive experiments conducted on three popular benchmark datasets showcase the superiority and potential of our GenZSL with significant efficacy and efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than $60\times$ faster training speed on AWA2. Codes are available at this https URL.</li>
</ul>

<h3>Title: MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Zihuan Qiu, Yi Xu, Chiyuan He, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11883">https://arxiv.org/abs/2505.11883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11883">https://arxiv.org/pdf/2505.11883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11883]] MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging(https://arxiv.org/abs/2505.11883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Continual model merging integrates independently fine-tuned models sequentially without access to original training data, providing a scalable and efficient solution to continual learning. However, current methods still face critical challenges, notably parameter interference among tasks and limited adaptability to evolving test distributions. The former causes catastrophic forgetting of integrated tasks, while the latter hinders effective adaptation to new tasks. To address these, we propose MINGLE, a novel framework for test-time continual model merging, which leverages test-time adaptation using a small set of unlabeled test samples from the current task to dynamically guide the merging process. MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation. Extensive experiments on standard continual merging benchmarks demonstrate that MINGLE achieves robust generalization, reduces forgetting significantly, and consistently surpasses previous state-of-the-art methods by 7-9\% on average across diverse task orders.</li>
</ul>

<h3>Title: Facial Recognition Leveraging Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Zhongwen Li, Zongwei Li, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11884">https://arxiv.org/abs/2505.11884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11884">https://arxiv.org/pdf/2505.11884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11884]] Facial Recognition Leveraging Generative Adversarial Networks(https://arxiv.org/abs/2505.11884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face recognition performance based on deep learning heavily relies on large-scale training data, which is often difficult to acquire in practical applications. To address this challenge, this paper proposes a GAN-based data augmentation method with three key contributions: (1) a residual-embedded generator to alleviate gradient vanishing/exploding problems, (2) an Inception ResNet-V1 based FaceNet discriminator for improved adversarial training, and (3) an end-to-end framework that jointly optimizes data generation and recognition performance. Experimental results demonstrate that our approach achieves stable training dynamics and significantly improves face recognition accuracy by 12.7% on the LFW benchmark compared to baseline methods, while maintaining good generalization capability with limited training samples.</li>
</ul>

<h3>Title: AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xiechi Zhang, Zetian Ouyang, Linlin Wang, Gerard de Melo, Zhu Cao, Xiaoling Wang, Ya Zhang, Yanfeng Wang, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11887">https://arxiv.org/abs/2505.11887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11887">https://arxiv.org/pdf/2505.11887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11887]] AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation(https://arxiv.org/abs/2505.11887)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to measure quality, significantly overlook the importance of medical terminology. While human evaluation tends to be more reliable, it can be very costly and may as well suffer from inaccuracies due to limits in human expertise and motivation. Although there are some evaluation methods based on LLMs, their usability in the medical field is limited due to their proprietary nature or lack of expertise. To tackle these challenges, we present AutoMedEval, an open-sourced automatic evaluation model with 13B parameters specifically engineered to measure the question-answering proficiency of medical LLMs. The overarching objective of AutoMedEval is to assess the quality of responses produced by diverse models, aspiring to significantly reduce the dependence on human evaluation. Specifically, we propose a hierarchical training method involving curriculum instruction tuning and an iterative knowledge introspection mechanism, enabling AutoMedEval to acquire professional medical assessment capabilities with limited instructional data. Human evaluations indicate that AutoMedEval surpasses other baselines in terms of correlation with human judgments.</li>
</ul>

<h3>Title: Fast RoPE Attention: Combining the Polynomial Method and Fast Fourier Transform</h3>
<ul>
<li><strong>Authors: </strong>Josh Alman, Zhao Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11892">https://arxiv.org/abs/2505.11892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11892">https://arxiv.org/pdf/2505.11892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11892]] Fast RoPE Attention: Combining the Polynomial Method and Fast Fourier Transform(https://arxiv.org/abs/2505.11892)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The transformer architecture has been widely applied to many machine learning tasks. A main bottleneck in the time to perform transformer computations is a task called attention computation. [Alman and Song, NeurIPS 2023] have shown that in the bounded entry regime, there is an almost linear time algorithm to approximate the attention computation. They also proved that the bounded entry assumption is necessary for a fast algorithm assuming the popular Strong Exponential Time Hypothesis. A new version of transformer which uses position embeddings has recently been very successful. At a high level, position embedding enables the model to capture the correlations between tokens while taking into account their position in the sequence. Perhaps the most popular and effective version is Rotary Position Embedding (RoPE), which was proposed by [Su, Lu, Pan, Murtadha, Wen, and Liu, Neurocomputing 2024]. A main downside of RoPE is that it complicates the attention computation problem, so that previous techniques for designing almost linear time algorithms no longer seem to work. In this paper, we show how to overcome this issue, and give a new algorithm to compute the RoPE attention in almost linear time in the bounded entry regime. (Again, known lower bounds imply that bounded entries are necessary.) Our new algorithm combines two techniques in a novel way: the polynomial method, which was used in prior fast attention algorithms, and the Fast Fourier Transform.</li>
</ul>

<h3>Title: RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving</h3>
<ul>
<li><strong>Authors: </strong>Zepeng Ding, Dixuan Wang, Ziqin Luo, Guochao Jiang, Deqing Yang, Jiaqing Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11893">https://arxiv.org/abs/2505.11893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11893">https://arxiv.org/pdf/2505.11893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11893]] RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving(https://arxiv.org/abs/2505.11893)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multi-step planning has been widely employed to enhance the performance of large language models (LLMs) on downstream natural language processing (NLP) tasks, which decomposes the original task into multiple subtasks and guide LLMs to solve them sequentially without additional training. When addressing task instances, existing methods either preset the order of steps or attempt multiple paths at each step. However, these methods overlook instances' linguistic features and rely on the intrinsic planning capabilities of LLMs to evaluate intermediate feedback and then select subtasks, resulting in suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this paper we propose a Reinforcement Learning enhanced Adaptive Planning framework (RLAP). In our framework, we model an NLP task as a Markov decision process (MDP) and employ an LLM directly into the environment. In particular, a lightweight Actor model is trained to estimate Q-values for natural language sequences consisting of states and actions through reinforcement learning. Therefore, during sequential planning, the linguistic features of each sequence in the MDP can be taken into account, and the Actor model interacts with the LLM to determine the optimal order of subtasks for each task instance. We apply RLAP on three different types of NLP tasks and conduct extensive experiments on multiple datasets to verify RLAP's effectiveness and robustness.</li>
</ul>

<h3>Title: Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration</h3>
<ul>
<li><strong>Authors: </strong>Chih-Ting Liao, Bin Ren, Guofeng Mei, Xu Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11895">https://arxiv.org/abs/2505.11895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11895">https://arxiv.org/pdf/2505.11895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11895]] Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration(https://arxiv.org/abs/2505.11895)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Recent unified multi-modal encoders align a wide range of modalities into a shared representation space, enabling diverse cross-modal tasks. Despite their impressive capabilities, the robustness of these models under adversarial perturbations remains underexplored, which is a critical concern for safety-sensitive applications. In this work, we present the first comprehensive study of adversarial vulnerability in unified multi-modal encoders. We find that even mild adversarial perturbations lead to substantial performance drops across all modalities. Non-visual inputs, such as audio and point clouds, are especially fragile, while visual inputs like images and videos also degrade significantly. To address this, we propose an efficient adversarial calibration framework that improves robustness across modalities without modifying pretrained encoders or semantic centers, ensuring compatibility with existing foundation models. Our method introduces modality-specific projection heads trained solely on adversarial examples, while keeping the backbone and embeddings frozen. We explore three training objectives: fixed-center cross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial InfoNCE, and we introduce a regularization strategy to ensure modality-consistent alignment under attack. Experiments on six modalities and three Bind-style models show that our method improves adversarial robustness by up to 47.3 percent at epsilon = 4/255, while preserving or even improving clean zero-shot and retrieval performance with less than 1 percent trainable parameters.</li>
</ul>

<h3>Title: AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, Shuangzhi Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11896">https://arxiv.org/abs/2505.11896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11896">https://arxiv.org/pdf/2505.11896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11896]] AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning(https://arxiv.org/abs/2505.11896)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.</li>
</ul>

<h3>Title: Benchmarking LLMs in an Embodied Environment for Blue Team Threat Hlunting</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqun Liu, Feiyang Yu, Xi Li, Guanhua Yan, Ping Yang, Zhaohan Xi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11901">https://arxiv.org/abs/2505.11901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11901">https://arxiv.org/pdf/2505.11901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11901]] Benchmarking LLMs in an Embodied Environment for Blue Team Threat Hlunting(https://arxiv.org/abs/2505.11901)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>As cyber threats continue to grow in scale and sophistication, blue team defenders increasingly require advanced tools to proactively detect and mitigate risks. Large Language Models (LLMs) offer promising capabilities for enhancing threat analysis. However, their effectiveness in real-world blue team threat-hunting scenarios remains insufficiently explored. In this paper, we present CYBERTEAM, a benchmark designed to guide LLMs in blue teaming practice. CYBERTEAM constructs an embodied environment in two stages. First, it models realistic threat-hunting workflows by capturing the dependencies among analytical tasks from threat attribution to incident response. Next, each task is addressed through a set of embodied functions tailored to its specific analytical requirements. This transforms the overall threat-hunting process into a structured sequence of function-driven operations, where each node represents a discrete function and edges define the execution order. Guided by this framework, LLMs are directed to perform threat-hunting tasks through modular steps. Overall, CYBERTEAM integrates 30 tasks and 9 embodied functions, guiding LLMs through pipelined threat analysis. We evaluate leading LLMs and state-of-the-art cybersecurity agents, comparing CYBERTEAM's embodied function-calling against fundamental elicitation strategies. Our results offer valuable insights into the current capabilities and limitations of LLMs in threat hunting, laying the foundation for the practical adoption in real-world cybersecurity applications.</li>
</ul>

<h3>Title: GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity</h3>
<ul>
<li><strong>Authors: </strong>Takuya Ikeda, Sergey Zakharov, Muhammad Zubair Irshad, Istvan Balazs Opra, Shun Iwase, Dian Chen, Mark Tjersland, Robert Lee, Alexandre Dilly, Rares Ambrus, Koichi Nishiwaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11905">https://arxiv.org/abs/2505.11905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11905">https://arxiv.org/pdf/2505.11905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11905]] GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity(https://arxiv.org/abs/2505.11905)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a novel method for 6-DoF object tracking and high-quality 3D reconstruction from monocular RGBD video. Existing methods, while achieving impressive results, often struggle with complex objects, particularly those exhibiting symmetry, intricate geometry or complex appearance. To bridge these gaps, we introduce an adaptive method that combines 3D Gaussian Splatting, hybrid geometry/appearance tracking, and key frame selection to achieve robust tracking and accurate reconstructions across a diverse range of objects. Additionally, we present a benchmark covering these challenging object classes, providing high-quality annotations for evaluating both tracking and reconstruction performance. Our approach demonstrates strong capabilities in recovering high-fidelity object meshes, setting a new standard for single-sensor 3D reconstruction in open-world environments.</li>
</ul>

<h3>Title: Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?</h3>
<ul>
<li><strong>Authors: </strong>Zihao Dongfang, Xu Zheng, Ziqiao Weng, Yuanhuiyi Lyu, Danda Pani Paudel, Luc Van Gool, Kailun Yang, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11907">https://arxiv.org/abs/2505.11907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11907">https://arxiv.org/pdf/2505.11907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11907]] Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?(https://arxiv.org/abs/2505.11907)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The 180x360 omnidirectional field of view captured by 360-degree cameras enables their use in a wide range of applications such as embodied AI and virtual reality. Although recent advances in multimodal large language models (MLLMs) have shown promise in visual-spatial reasoning, most studies focus on standard pinhole-view images, leaving omnidirectional perception largely unexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial reasoning? To investigate this, we introduce OSR-Bench, the first benchmark specifically designed for this setting. OSR-Bench includes over 153,000 diverse question-answer pairs grounded in high-fidelity panoramic indoor scene maps. It covers key reasoning types including object counting, relative distance, and direction. We also propose a negative sampling strategy that inserts non-existent objects into prompts to evaluate hallucination and grounding robustness. For fine-grained analysis, we design a two-stage evaluation framework assessing both cognitive map generation and QA accuracy using rotation-invariant matching and a combination of rule-based and LLM-based metrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5 Pro, and leading open-source models under zero-shot settings. Results show that current models struggle with spatial reasoning in panoramic contexts, highlighting the need for more perceptually grounded MLLMs. OSR-Bench and code will be released at: this https URL</li>
</ul>

<h3>Title: ELITE: Embedding-Less retrieval with Iterative Text Exploration</h3>
<ul>
<li><strong>Authors: </strong>Zhangyu Wang, Siyuan Gao, Rong Zhou, Hao Wang, Li Ning</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11908">https://arxiv.org/abs/2505.11908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11908">https://arxiv.org/pdf/2505.11908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11908]] ELITE: Embedding-Less retrieval with Iterative Text Exploration(https://arxiv.org/abs/2505.11908)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive progress in natural language processing, but their limited ability to retain long-term context constrains performance on document-level or multi-turn tasks. Retrieval-Augmented Generation (RAG) mitigates this by retrieving relevant information from an external corpus. However, existing RAG systems often rely on embedding-based retrieval trained on corpus-level semantic similarity, which can lead to retrieving content that is semantically similar in form but misaligned with the question's true intent. Furthermore, recent RAG variants construct graph- or hierarchy-based structures to improve retrieval accuracy, resulting in significant computation and storage overhead. In this paper, we propose an embedding-free retrieval framework. Our method leverages the logical inferencing ability of LLMs in retrieval using iterative search space refinement guided by our novel importance measure and extend our retrieval results with logically related information without explicit graph construction. Experiments on long-context QA benchmarks, including NovelQA and Marathon, show that our approach outperforms strong baselines while reducing storage and runtime by over an order of magnitude.</li>
</ul>

<h3>Title: Modles de Substitution pour les Modles  base d'Agents : Enjeux, Mthodes et Applications</h3>
<ul>
<li><strong>Authors: </strong>Paul Saves, Nicolas Verstaevel, Benot Gaudou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11912">https://arxiv.org/abs/2505.11912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11912">https://arxiv.org/pdf/2505.11912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11912]] Modles de Substitution pour les Modles  base d'Agents : Enjeux, Mthodes et Applications(https://arxiv.org/abs/2505.11912)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Multi-agent simulations enables the modeling and analyses of the dynamic behaviors and interactions of autonomous entities evolving in complex environments. Agent-based models (ABM) are widely used to study emergent phenomena arising from local interactions. However, their high computational cost poses a significant challenge, particularly for large-scale simulations requiring extensive parameter exploration, optimization, or uncertainty quantification. The increasing complexity of ABM limits their feasibility for real-time decision-making and large-scale scenario analysis. To address these limitations, surrogate models offer an efficient alternative by learning approximations from sparse simulation data. These models provide cheap-to-evaluate predictions, significantly reducing computational costs while maintaining accuracy. Various machine learning techniques, including regression models, neural networks, random forests and Gaussian processes, have been applied to construct robust surrogates. Moreover, uncertainty quantification and sensitivity analysis play a crucial role in enhancing model reliability and interpretability. This article explores the motivations, methods, and applications of surrogate modeling for ABM, emphasizing the trade-offs between accuracy, computational efficiency, and interpretability. Through a case study on a segregation model, we highlight the challenges associated with building and validating surrogate models, comparing different approaches and evaluating their performance. Finally, we discuss future perspectives on integrating surrogate models within ABM to improve scalability, explainability, and real-time decision support across various fields such as ecology, urban planning and economics.</li>
</ul>

<h3>Title: Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Chen, Ruofan Wu, Guanhua Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11918">https://arxiv.org/abs/2505.11918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11918">https://arxiv.org/pdf/2505.11918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11918]] Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures(https://arxiv.org/abs/2505.11918)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the under standing of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.</li>
</ul>

<h3>Title: DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities</h3>
<ul>
<li><strong>Authors: </strong>Haitao Li, Ziyu Li, Yiheng Mao, Zhengyao Ding, Zhengxing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11921">https://arxiv.org/abs/2505.11921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11921">https://arxiv.org/pdf/2505.11921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11921]] DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities(https://arxiv.org/abs/2505.11921)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of brain images typically requires the integration of complementary information from multiple image modalities. However, clinical data for all modalities may not be available for every patient, creating a significant challenge. To address this, previous studies encode multiple modalities into a shared latent space. While somewhat effective, it remains suboptimal, as each modality contains distinct and valuable information. In this study, we propose DC-Seg (Disentangled Contrastive Learning for Segmentation), a new method that explicitly disentangles images into modality-invariant anatomical representation and modality-specific representation, by using anatomical contrastive learning and modality contrastive learning respectively. This solution improves the separation of anatomical and modality-specific features by considering the modality gaps, leading to more robust representations. Furthermore, we introduce a segmentation-based regularizer that enhances the model's robustness to missing modalities. Extensive experiments on the BraTS 2020 and a private white matter hyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms state-of-the-art methods in handling incomplete multimodal brain tumor segmentation tasks with varying missing modalities, while also demonstrate strong generalizability in WMH segmentation. The code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Lu, ZiMeng Bai, Caixia Yuan, Huixing Jiang, Xiaojie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11922">https://arxiv.org/abs/2505.11922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11922">https://arxiv.org/pdf/2505.11922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11922]] Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning(https://arxiv.org/abs/2505.11922)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable capabilities in handling natural language tasks; however, they may struggle to consistently follow complex instructions including those involve multiple constraints. Post-training LLMs using supervised fine-tuning (SFT) is a standard approach to improve their ability to follow instructions. In addressing complex instruction following, existing efforts primarily focus on data-driven methods that synthesize complex instruction-output pairs for SFT. However, insufficient attention allocated to crucial sub-contexts may reduce the effectiveness of SFT. In this work, we propose transforming sequentially structured input instruction into multiple parallel instructions containing subcontexts. To support processing this multi-input, we propose MISO (Multi-Input Single-Output), an extension to currently dominant decoder-only transformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that jointly considers the overall instruction-output alignment and the influence of individual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning to complex instructionfollowing datasets and evaluate it with standard LLM inference. Empirical results demonstrate the superiority of MISO as a fine-tuning method for LLMs, both in terms of effectiveness in complex instruction-following scenarios and its potential for training efficiency.</li>
</ul>

<h3>Title: PyScrew: A Comprehensive Dataset Collection from Industrial Screw Driving Experiments</h3>
<ul>
<li><strong>Authors: </strong>Nikolai West, Jochen Deuse</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11925">https://arxiv.org/abs/2505.11925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11925">https://arxiv.org/pdf/2505.11925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11925]] PyScrew: A Comprehensive Dataset Collection from Industrial Screw Driving Experiments(https://arxiv.org/abs/2505.11925)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive collection of industrial screw driving datasets designed to advance research in manufacturing process monitoring and quality control. The collection comprises six distinct datasets with over 34,000 individual screw driving operations conducted under controlled experimental conditions, capturing the multifaceted nature of screw driving processes in plastic components. Each dataset systematically investigates specific aspects: natural thread degradation patterns through repeated use (s01), variations in surface friction conditions including contamination and surface treatments (s02), diverse assembly faults with up to 27 error types (s03-s04), and fabrication parameter variations in both upper and lower workpieces through modified injection molding settings (s05-s06). We detail the standardized experimental setup used across all datasets, including hardware specifications, process phases, and data acquisition methods. The hierarchical data model preserves the temporal and operational structure of screw driving processes, facilitating both exploratory analysis and the development of machine learning models. To maximize accessibility, we provide dual access pathways: raw data through Zenodo with a persistent DOI, and a purpose-built Python library (PyScrew) that offers consistent interfaces for data loading, preprocessing, and integration with common analysis workflows. These datasets serve diverse research applications including anomaly detection, predictive maintenance, quality control system development, feature extraction methodology evaluation, and classification of specific error conditions. By addressing the scarcity of standardized, comprehensive datasets in industrial manufacturing, this collection enables reproducible research and fair comparison of analytical approaches in an area of growing importance for industrial automation.</li>
</ul>

<h3>Title: SafeVid: Toward Safety Aligned Video Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Yixu Wang, Jiaxin Song, Yifeng Gao, Xin Wang, Yang Yao, Yan Teng, Xingjun Ma, Yingchun Wang, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11926">https://arxiv.org/abs/2505.11926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11926">https://arxiv.org/pdf/2505.11926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11926]] SafeVid: Toward Safety Aligned Video Large Multimodal Models(https://arxiv.org/abs/2505.11926)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent complexity introduces significant safety challenges, particularly the issue of mismatched generalization where static safety alignments fail to transfer to dynamic video contexts. We introduce SafeVid, a framework designed to instill video-specific safety principles in VLMMs. SafeVid uniquely transfers robust textual safety alignment capabilities to the video domain by employing detailed textual video descriptions as an interpretive bridge, facilitating LLM-based rule-driven safety reasoning. This is achieved through a closed-loop system comprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific safety preference dataset; 2) targeted alignment of VLMMs using Direct Preference Optimization (DPO); and 3) comprehensive evaluation via our new SafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM safety, with models like LLaVA-NeXT-Video demonstrating substantial improvements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical resources and a structured approach, demonstrating that leveraging textual descriptions as a conduit for safety reasoning markedly improves the safety alignment of VLMMs. We have made SafeVid-350K dataset (this https URL) publicly available.</li>
</ul>

<h3>Title: The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics</h3>
<ul>
<li><strong>Authors: </strong>Marco Slzer, Przemysaw Andrzej Waga, Martin Lange</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11930">https://arxiv.org/abs/2505.11930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11930">https://arxiv.org/pdf/2505.11930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11930]] The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics(https://arxiv.org/abs/2505.11930)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In recent years, the expressive power of various neural architectures -- including graph neural networks (GNNs), transformers, and recurrent neural networks -- has been characterised using tools from logic and formal language theory. As the capabilities of basic architectures are becoming well understood, increasing attention is turning to models that combine multiple architectural paradigms. Among them particularly important, and challenging to analyse, are temporal extensions of GNNs, which integrate both spatial (graph-structure) and temporal (evolution over time) dimensions. In this paper, we initiate the study of logical characterisation of temporal GNNs by connecting them to two-dimensional product logics. We show that the expressive power of temporal GNNs depends on how graph and temporal components are combined. In particular, temporal GNNs that apply static GNNs recursively over time can capture all properties definable in the product logic of (past) propositional temporal logic PTL and the modal logic K. In contrast, architectures such as graph-and-time TGNNs and global TGNNs can only express restricted fragments of this logic, where the interaction between temporal and spatial operators is syntactically constrained. These results yield the first logical characterisations of temporal GNNs and establish new relative expressiveness results for temporal GNNs.</li>
</ul>

<h3>Title: iSegMan: Interactive Segment-and-Manipulate 3D Gaussians</h3>
<ul>
<li><strong>Authors: </strong>Yian Zhao, Wanshi Xu, Ruochong Zheng, Pengchong Qiao, Chang Liu, Jie Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11934">https://arxiv.org/abs/2505.11934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11934">https://arxiv.org/pdf/2505.11934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11934]] iSegMan: Interactive Segment-and-Manipulate 3D Gaussians(https://arxiv.org/abs/2505.11934)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>The efficient rendering and explicit nature of 3DGS promote the advancement of 3D scene manipulation. However, existing methods typically encounter challenges in controlling the manipulation region and are unable to furnish the user with interactive feedback, which inevitably leads to unexpected results. Intuitively, incorporating interactive 3D segmentation tools can compensate for this deficiency. Nevertheless, existing segmentation frameworks impose a pre-processing step of scene-specific parameter training, which limits the efficiency and flexibility of scene manipulation. To deliver a 3D region control module that is well-suited for scene manipulation with reliable efficiency, we propose interactive Segment-and-Manipulate 3D Gaussians (iSegMan), an interactive segmentation and manipulation framework that only requires simple 2D user interactions in any view. To propagate user interactions to other views, we propose Epipolar-guided Interaction Propagation (EIP), which innovatively exploits epipolar constraint for efficient and robust interaction matching. To avoid scene-specific training to maintain efficiency, we further propose the novel Visibility-based Gaussian Voting (VGV), which obtains 2D segmentations from SAM and models the region extraction as a voting game between 2D Pixels and 3D Gaussians based on Gaussian visibility. Taking advantage of the efficient and precise region control of EIP and VGV, we put forth a Manipulation Toolbox to implement various functions on selected regions, enhancing the controllability, flexibility and practicality of scene manipulation. Extensive results on 3D scene manipulation and segmentation tasks fully demonstrate the significant advantages of iSegMan. Project page is available at this https URL.</li>
</ul>

<h3>Title: ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing</h3>
<ul>
<li><strong>Authors: </strong>Xuanle Zhao, Xuexin Liu, Haoyue Yang, Xianzhen Luo, Fanhu Zeng, Jianling Li, Qi Shi, Chi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11935">https://arxiv.org/abs/2505.11935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11935">https://arxiv.org/pdf/2505.11935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11935]] ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing(https://arxiv.org/abs/2505.11935)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Although multimodal large language models (MLLMs) show promise in generating chart rendering code, chart editing presents a greater challenge. This difficulty stems from its nature as a labor-intensive task for humans that also demands MLLMs to integrate chart understanding, complex reasoning, and precise intent interpretation. While many MLLMs claim such editing capabilities, current assessments typically rely on limited case studies rather than robust evaluation methodologies, highlighting the urgent need for a comprehensive evaluation framework. In this work, we propose ChartEdit, a new high-quality benchmark designed for chart editing tasks. This benchmark comprises $1,405$ diverse editing instructions applied to $233$ real-world charts, with each instruction-chart instance having been manually annotated and validated for accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream MLLMs across two types of experiments, assessing them at both the code and chart levels. The results suggest that large-scale models can generate code to produce images that partially match the reference images. However, their ability to generate accurate edits according to the instructions remains limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$, highlighting significant challenges in precise modification. In contrast, small-scale models, including chart-domain models, struggle both with following editing instructions and generating overall chart images, underscoring the need for further development in this area. Code is available at this https URL.</li>
</ul>

<h3>Title: How can Diffusion Models Evolve into Continual Generators?</h3>
<ul>
<li><strong>Authors: </strong>Jingren Liu, Zhong Ji, Xiangyu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11936">https://arxiv.org/abs/2505.11936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11936">https://arxiv.org/pdf/2505.11936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11936]] How can Diffusion Models Evolve into Continual Generators?(https://arxiv.org/abs/2505.11936)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>While diffusion models have achieved remarkable success in static data generation, their deployment in streaming or continual learning (CL) scenarios faces a major challenge: catastrophic forgetting (CF), where newly acquired generative capabilities overwrite previously learned ones. To systematically address this, we introduce a formal Continual Diffusion Generation (CDG) paradigm that characterizes and redefines CL in the context of generative diffusion models. Prior efforts often adapt heuristic strategies from continual classification tasks but lack alignment with the underlying diffusion process. In this work, we develop the first theoretical framework for CDG by analyzing cross-task dynamics in diffusion-based generative modeling. Our analysis reveals that the retention and stability of generative knowledge across tasks are governed by three key consistency criteria: inter-task knowledge consistency (IKC), unconditional knowledge consistency (UKC), and label knowledge consistency (LKC). Building on these insights, we propose Continual Consistency Diffusion (CCD), a principled framework that integrates these consistency objectives into training via hierarchical loss terms $\mathcal{L}_{IKC}$, $\mathcal{L}_{UKC}$, and $\mathcal{L}_{LKC}$. This promotes effective knowledge retention while enabling the assimilation of new generative capabilities. Extensive experiments on four benchmark datasets demonstrate that CCD achieves state-of-the-art performance under continual settings, with substantial gains in Mean Fidelity (MF) and Incremental Mean Fidelity (IMF), particularly in tasks with rich cross-task knowledge overlap.</li>
</ul>

<h3>Title: Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Bonan li, Zicheng Zhang, Songhua Liu, Weihao Yu, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11945">https://arxiv.org/abs/2505.11945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11945">https://arxiv.org/pdf/2505.11945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11945]] Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning(https://arxiv.org/abs/2505.11945)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual instruction tuning aims to enable large language models to comprehend the visual world, with a pivotal challenge lying in establishing an effective vision-to-language projection. However, existing methods often grapple with the intractable trade-off between accuracy and efficiency. In this paper, we present LLaVA-Meteor, a novel approach designed to break this deadlock, equipped with a novel Top-Down Compression paradigm that strategically compresses visual tokens without compromising core information. Specifically, we construct a trainable Flash Global Fusion module based on efficient selective state space operators, which aligns the feature space while enabling each token to perceive holistic visual context and instruction preference at low cost. Furthermore, a local-to-single scanning manner is employed to effectively capture local dependencies, thereby enhancing the model's capability in vision modeling. To alleviate computational overhead, we explore a Visual-Native Selection mechanism that independently assesses token significance by both the visual and native experts, followed by aggregation to retain the most critical subset. Extensive experiments show that our approach reduces visual tokens by 75--95% while achieving comparable or superior performance across 12 benchmarks, significantly improving efficiency.</li>
</ul>

<h3>Title: Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Puning Yang, Qizhou Wang, Zhuo Huang, Tongliang Liu, Chengqi Zhang, Bo Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11953">https://arxiv.org/abs/2505.11953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11953">https://arxiv.org/pdf/2505.11953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11953]] Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning(https://arxiv.org/abs/2505.11953)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Loss reweighting has shown significant benefits for machine unlearning with large language models (LLMs). However, their exact functionalities are left unclear and the optimal strategy remains an open question, thus impeding the understanding and improvement of existing methodologies. In this paper, we identify two distinct goals of loss reweighting, namely, Saturation and Importance -- the former indicates that those insufficiently optimized data should be emphasized, while the latter stresses some critical data that are most influential for loss minimization. To study their usefulness, we design specific reweighting strategies for each goal and evaluate their respective effects on unlearning. We conduct extensive empirical analyses on well-established benchmarks, and summarize some important observations as follows: (i) Saturation enhances efficacy more than importance-based reweighting, and their combination can yield additional improvements. (ii) Saturation typically allocates lower weights to data with lower likelihoods, whereas importance-based reweighting does the opposite. (iii) The efficacy of unlearning is also largely influenced by the smoothness and granularity of the weight distributions. Based on these findings, we propose SatImp, a simple reweighting method that combines the advantages of both saturation and importance. Empirical results on extensive datasets validate the efficacy of our method, potentially bridging existing research gaps and indicating directions for future research. Our code is available at this https URL.</li>
</ul>

<h3>Title: MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Luca Collini, Baleegh Ahmad, Joey Ah-kiow, Ramesh Karri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11963">https://arxiv.org/abs/2505.11963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11963">https://arxiv.org/pdf/2505.11963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11963]] MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models(https://arxiv.org/abs/2505.11963)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Hardware security verification is a challenging and time-consuming task. For this purpose, design engineers may utilize tools such as formal verification, linters, and functional simulation tests, coupled with analysis and a deep understanding of the hardware design being inspected. Large Language Models (LLMs) have been used to assist during this task, either directly or in conjunction with existing tools. We improve the state of the art by proposing MARVEL, a multi-agent LLM framework for a unified approach to decision-making, tool use, and reasoning. MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code. It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation. It delegates tasks to validate the security policy to individual executor agents. Each executor agent carries out its assigned task using a particular strategy. Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation. MARVEL includes executor agents that leverage formal tools, linters, simulation tests, LLM-based detection schemes, and static analysis-based checks. We test our approach on a known buggy SoC based on OpenTitan from the Hack@DATE competition. We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.</li>
</ul>

<h3>Title: CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation</h3>
<ul>
<li><strong>Authors: </strong>Xu Liu, Guanyi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11965">https://arxiv.org/abs/2505.11965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11965">https://arxiv.org/pdf/2505.11965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11965]] CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation(https://arxiv.org/abs/2505.11965)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, large language model</a></li>
<li><strong>Abstract: </strong>We present the system developed by the Central China Normal University (CCNU) team for the Mu-SHROOM shared task, which focuses on identifying hallucinations in question-answering systems across 14 different languages. Our approach leverages multiple Large Language Models (LLMs) with distinct areas of expertise, employing them in parallel to annotate hallucinations, effectively simulating a crowdsourcing annotation process. Furthermore, each LLM-based annotator integrates both internal and external knowledge related to the input during the annotation process. Using the open-source LLM DeepSeek-V3, our system achieves the top ranking (\#1) for Hindi data and secures a Top-5 position in seven other languages. In this paper, we also discuss unsuccessful approaches explored during our development process and share key insights gained from participating in this shared task.</li>
</ul>

<h3>Title: An Annotated Corpus of Arabic Tweets for Hate Speech Analysis</h3>
<ul>
<li><strong>Authors: </strong>Md. Rafiul Biswas, Wajdi Zaghouani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11969">https://arxiv.org/abs/2505.11969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11969">https://arxiv.org/pdf/2505.11969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11969]] An Annotated Corpus of Arabic Tweets for Hate Speech Analysis(https://arxiv.org/abs/2505.11969)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Identifying hate speech content in the Arabic language is challenging due to the rich quality of dialectal variations. This study introduces a multilabel hate speech dataset in the Arabic language. We have collected 10000 Arabic tweets and annotated each tweet, whether it contains offensive content or not. If a text contains offensive content, we further classify it into different hate speech targets such as religion, gender, politics, ethnicity, origin, and others. A text can contain either single or multiple targets. Multiple annotators are involved in the data annotation task. We calculated the inter-annotator agreement, which was reported to be 0.86 for offensive content and 0.71 for multiple hate speech targets. Finally, we evaluated the data annotation task by employing a different transformers-based model in which AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of 0.786.</li>
</ul>

<h3>Title: AoP-SAM: Automation of Prompts for Efficient Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yi Chen, Mu-Young Son, Chuanbo Hua, Joo-Young Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11980">https://arxiv.org/abs/2505.11980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11980">https://arxiv.org/pdf/2505.11980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11980]] AoP-SAM: Automation of Prompts for Efficient Segmentation(https://arxiv.org/abs/2505.11980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) is a powerful foundation model for image segmentation, showing robust zero-shot generalization through prompt engineering. However, relying on manual prompts is impractical for real-world applications, particularly in scenarios where rapid prompt provision and resource efficiency are crucial. In this paper, we propose the Automation of Prompts for SAM (AoP-SAM), a novel approach that learns to generate essential prompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency and usability by eliminating manual input, making it better suited for real-world tasks. Our approach employs a lightweight yet efficient Prompt Predictor model that detects key entities across images and identifies the optimal regions for placing prompt candidates. This method leverages SAM's image embeddings, preserving its zero-shot generalization capabilities without requiring fine-tuning. Additionally, we introduce a test-time instance-level Adaptive Sampling and Filtering mechanism that generates prompts in a coarse-to-fine manner. This notably enhances both prompt and mask generation efficiency by reducing computational overhead and minimizing redundant mask refinements. Evaluations of three datasets demonstrate that AoP-SAM substantially improves both prompt generation efficiency and mask generation accuracy, making SAM more effective for automated segmentation tasks.</li>
</ul>

<h3>Title: FedHQ: Hybrid Runtime Quantization for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zheng, Ziyao Wang, Xiuping Cui, Maoliang Li, Jiayu Chen, Yun (Eric)Liang, Ang Li, Xiang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11982">https://arxiv.org/abs/2505.11982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11982">https://arxiv.org/pdf/2505.11982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11982]] FedHQ: Hybrid Runtime Quantization for Federated Learning(https://arxiv.org/abs/2505.11982)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a decentralized model training approach that preserves data privacy but struggles with low efficiency. Quantization, a powerful training optimization technique, has been widely explored for integration into FL. However, many studies fail to consider the distinct performance attribution between particular quantization strategies, such as post-training quantization (PTQ) or quantization-aware training (QAT). As a result, existing FL quantization methods rely solely on either PTQ or QAT, optimizing for speed or accuracy while compromising the other. To efficiently accelerate FL and maintain distributed convergence accuracy across various FL settings, this paper proposes a hybrid quantitation approach combining PTQ and QAT for FL systems. We conduct case studies to validate the effectiveness of using hybrid quantization in FL. To solve the difficulty of modeling speed and accuracy caused by device and data heterogeneity, we propose a hardware-related analysis and data-distribution-related analysis to help identify the trade-off boundaries for strategy selection. Based on these, we proposed a novel framework named FedHQ to automatically adopt optimal hybrid strategy allocation for FL systems. Specifically, FedHQ develops a coarse-grained global initialization and fine-grained ML-based adjustment to ensure efficiency and robustness. Experiments show that FedHQ achieves up to 2.47x times training acceleration and up to 11.15% accuracy improvement and negligible extra overhead.</li>
</ul>

<h3>Title: TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Lekssays, Utsav Shukla, Husrev Taha Sencar, Md Rizwan Parvez</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11988">https://arxiv.org/abs/2505.11988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11988">https://arxiv.org/pdf/2505.11988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11988]] TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text(https://arxiv.org/abs/2505.11988)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains. We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques. Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.</li>
</ul>

<h3>Title: SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations</h3>
<ul>
<li><strong>Authors: </strong>Songchun Zhang, Huiyao Xu, Sitong Guo, Zhongwei Xie, Pengwei Liu, Hujun Bao, Weiwei Xu, Changqing Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11992">https://arxiv.org/abs/2505.11992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11992">https://arxiv.org/pdf/2505.11992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11992]] SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations(https://arxiv.org/abs/2505.11992)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Novel view synthesis (NVS) boosts immersive experiences in computer vision and graphics. Existing techniques, though progressed, rely on dense multi-view observations, restricting their application. This work takes on the challenge of reconstructing photorealistic 3D scenes from sparse or single-view inputs. We introduce SpatialCrafter, a framework that leverages the rich knowledge in video diffusion models to generate plausible additional observations, thereby alleviating reconstruction ambiguity. Through a trainable camera encoder and an epipolar attention mechanism for explicit geometric constraints, we achieve precise camera control and 3D consistency, further reinforced by a unified scale estimation strategy to handle scale discrepancies across datasets. Furthermore, by integrating monocular depth priors with semantic features in the video latent space, our framework directly regresses 3D Gaussian primitives and efficiently processes long-sequence features using a hybrid network structure. Extensive experiments show our method enhances sparse view reconstruction and restores the realistic appearance of 3D scenes.</li>
</ul>

<h3>Title: Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11995">https://arxiv.org/abs/2505.11995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11995">https://arxiv.org/pdf/2505.11995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11995]] Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation(https://arxiv.org/abs/2505.11995)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Considering the inherent limitations of parametric knowledge in large language models (LLMs), retrieval-augmented generation (RAG) is widely employed to expand their knowledge scope. Since RAG has shown promise in knowledge-intensive tasks like open-domain question answering, its broader application to complex tasks and intelligent assistants has further advanced its utility. Despite this progress, the underlying knowledge utilization mechanisms of LLM-based RAG remain underexplored. In this paper, we present a systematic investigation of the intrinsic mechanisms by which LLMs integrate internal (parametric) and external (retrieved) knowledge in RAG scenarios. Specially, we employ knowledge stream analysis at the macroscopic level, and investigate the function of individual modules at the microscopic level. Drawing on knowledge streaming analyses, we decompose the knowledge utilization process into four distinct stages within LLM layers: knowledge refinement, knowledge elicitation, knowledge expression, and knowledge contestation. We further demonstrate that the relevance of passages guides the streaming of knowledge through these stages. At the module level, we introduce a new method, knowledge activation probability entropy (KAPE) for neuron identification associated with either internal or external knowledge. By selectively deactivating these neurons, we achieve targeted shifts in the LLM's reliance on one knowledge source over the other. Moreover, we discern complementary roles for multi-head attention and multi-layer perceptron layers during knowledge formation. These insights offer a foundation for improving interpretability and reliability in retrieval-augmented LLMs, paving the way for more robust and transparent generative solutions in knowledge-intensive domains.</li>
</ul>

<h3>Title: Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Prashant Shivaram Bhat, Shakib Yazdani, Elahe Arani, Bahram Zonooz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.11998">https://arxiv.org/abs/2505.11998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.11998">https://arxiv.org/pdf/2505.11998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.11998]] Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation(https://arxiv.org/abs/2505.11998)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting has remained a critical challenge for deep neural networks in Continual Learning (CL) as it undermines consolidated knowledge when learning new tasks. Parameter efficient fine tuning CL techniques are gaining traction for their effectiveness in addressing catastrophic forgetting with a lightweight training schedule while avoiding degradation of consolidated knowledge in pre-trained models. However, low rank adapters (LoRA) in these approaches are highly sensitive to rank selection which can lead to sub-optimal resource allocation and performance. To this end, we introduce PEARL, a rehearsal-free CL framework that entails dynamic rank allocation for LoRA components during CL training. Specifically, PEARL leverages reference task weights and adaptively determines the rank of task-specific LoRA components based on the current tasks' proximity to reference task weights in parameter space. To demonstrate the versatility of PEARL, we evaluate it across three vision architectures (ResNet, Separable Convolutional Network and Vision Transformer) and a multitude of CL scenarios, and show that PEARL outperforms all considered baselines by a large margin.</li>
</ul>

<h3>Title: Approximation theory for 1-Lipschitz ResNets</h3>
<ul>
<li><strong>Authors: </strong>Davide Murari, Takashi Furuya, Carola-Bibiane Schnlieb</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12003">https://arxiv.org/abs/2505.12003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12003">https://arxiv.org/pdf/2505.12003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12003]] Approximation theory for 1-Lipschitz ResNets(https://arxiv.org/abs/2505.12003)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>1-Lipschitz neural networks are fundamental for generative modelling, inverse problems, and robust classifiers. In this paper, we focus on 1-Lipschitz residual networks (ResNets) based on explicit Euler steps of negative gradient flows and study their approximation capabilities. Leveraging the Restricted Stone-Weierstrass Theorem, we first show that these 1-Lipschitz ResNets are dense in the set of scalar 1-Lipschitz functions on any compact domain when width and depth are allowed to grow. We also show that these networks can exactly represent scalar piecewise affine 1-Lipschitz functions. We then prove a stronger statement: by inserting norm-constrained linear maps between the residual blocks, the same density holds when the hidden width is fixed. Because every layer obeys simple norm constraints, the resulting models can be trained with off-the-shelf optimisers. This paper provides the first universal approximation guarantees for 1-Lipschitz ResNets, laying a rigorous foundation for their practical use.</li>
</ul>

<h3>Title: Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Runduo Han, Xiuping Liu, Shangxuan Yi, Yi Zhang, Hongchen Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12007">https://arxiv.org/abs/2505.12007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12007">https://arxiv.org/pdf/2505.12007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12007]] Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition(https://arxiv.org/abs/2505.12007)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this paper, we proposed a Multi-modal Collaborative Optimization and Expansion Network (MCO-E Net), to use event modalities to resist challenges such as low light, high exposure, and high dynamic range in single-eye expression recognition tasks. The MCO-E Net introduces two innovative designs: Multi-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous Collaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building upon Mamba, leverages dual-modal information to jointly optimize the model, facilitating collaborative interaction and fusion of modal semantics. This approach encourages the model to balance the learning of both modalities and harness their respective strengths. HCE-MoE, on the other hand, employs a dynamic routing mechanism to distribute structurally varied experts (deep, attention, and focal), fostering collaborative learning of complementary semantics. This heterogeneous architecture systematically integrates diverse feature extraction paradigms to comprehensively capture expression semantics. Extensive experiments demonstrate that our proposed network achieves competitive performance in the task of single-eye expression recognition, especially under poor lighting conditions.</li>
</ul>

<h3>Title: Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhiying Li, Guanggang Geng, Yeying Jin, Zhizhi Guo, Bruce Gu, Jidong Huo, Zhaoxin Fan, Wenjun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12009">https://arxiv.org/abs/2505.12009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12009">https://arxiv.org/pdf/2505.12009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12009]] Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation(https://arxiv.org/abs/2505.12009)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>Expressive human pose and shape (EHPS) estimation is vital for digital human generation, particularly in live-streaming applications. However, most existing EHPS models focus primarily on minimizing estimation errors, with limited attention on potential security vulnerabilities. Current adversarial attacks on EHPS models often require white-box access (e.g., model details or gradients) or generate visually conspicuous perturbations, limiting their practicality and ability to expose real-world security threats. To address these limitations, we propose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA leverages the latent-space representations of natural images to generate an optimal adversarial noise pattern and iteratively refine its attack potency along an optimized direction in digital space. Crucially, this process relies solely on querying the model's output, requiring no internal knowledge of the EHPS architecture, while guiding the noise optimization toward greater stealth and effectiveness. Extensive experiments and visual analyses demonstrate the superiority of UBA. Notably, UBA increases the pose estimation errors of EHPS models by 17.27%-58.21% on average, revealing critical vulnerabilities. These findings underscore the urgent need to address and mitigate security risks associated with digital human generation systems.</li>
</ul>

<h3>Title: A Human Study of Cognitive Biases in CTF Challenges</h3>
<ul>
<li><strong>Authors: </strong>Yuwei Yang, Skyler Grandel, Daniel Balasubramanian, Yu Huang, Kevin Leach</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12018">https://arxiv.org/abs/2505.12018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12018">https://arxiv.org/pdf/2505.12018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12018]] A Human Study of Cognitive Biases in CTF Challenges(https://arxiv.org/abs/2505.12018)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Cybersecurity training has become a crucial part of computer science education and industrial onboarding. Capture the Flag (CTF) competitions have emerged as a valuable, gamified approach for developing and refining the skills of cybersecurity and software engineering professionals. However, while CTFs provide a controlled environment for tackling real world challenges, the participants' decision making and problem solving processes remain under explored. Recognizing that psychology may play a role in a cyber attacker's behavior, we investigate how cognitive biases could be used to improve CTF education and security. In this paper, we present an approach to control cognitive biases, specifically Satisfaction of Search and Loss Aversion, to influence and potentially hinder attackers' effectiveness against web application vulnerabilities in a CTF style challenge. We employ a rigorous quantitative and qualitative analysis through a controlled human study of CTF tasks. CTF exercises are widely used in cybersecurity education and research to simulate real world attack scenarios and help participants develop critical skills by solving security challenges in controlled environments. In our study, participants interact with a web application containing deliberately embedded vulnerabilities while being subjected to tasks designed to trigger cognitive biases. Our study reveals that many participants exhibit the Satisfaction of Search bias and that this bias has a significant effect on their success. On average, participants found 25% fewer flags compared to those who did not exhibit this bias. Our findings provide valuable insights into how cognitive biases can be strategically employed to enhance cybersecurity outcomes, education, and measurements through the lens of CTF challenges.</li>
</ul>

<h3>Title: FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients</h3>
<ul>
<li><strong>Authors: </strong>Jianyi Zhang, Ziyin Zhou, Yilong Li, Qichao Jin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12019">https://arxiv.org/abs/2505.12019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12019">https://arxiv.org/pdf/2505.12019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12019]] FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients(https://arxiv.org/abs/2505.12019)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, steal, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is gaining increasing attention as an emerging collaborative machine learning approach, particularly in the context of large-scale computing and data systems. However, the fundamental algorithm of FL, Federated Averaging (FedAvg), is susceptible to backdoor attacks. Although researchers have proposed numerous defense algorithms, two significant challenges remain. The attack is becoming more stealthy and harder to detect, and current defense methods are unable to handle 50\% or more malicious users or assume an auxiliary server dataset. To address these challenges, we propose a novel defense algorithm, FL-PLAS, \textbf{F}ederated \textbf{L}earning based on \textbf{P}artial\textbf{ L}ayer \textbf{A}ggregation \textbf{S}trategy. In particular, we divide the local model into a feature extractor and a classifier. In each iteration, the clients only upload the parameters of a feature extractor after local training. The server then aggregates these local parameters and returns the results to the clients. Each client retains its own classifier layer, ensuring that the backdoor labels do not impact other clients. We assess the effectiveness of FL-PLAS against state-of-the-art (SOTA) backdoor attacks on three image datasets and compare our approach to six defense strategies. The results of the experiment demonstrate that our methods can effectively protect local models from backdoor attacks. Without requiring any auxiliary dataset for the server, our method achieves a high main-task accuracy with a lower backdoor accuracy even under the condition of 90\% malicious users with the attacks of trigger, semantic and edge-case.</li>
</ul>

<h3>Title: GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Xi Han, Jingwei Zhang, Dimitris Samaras, Fei Hou, Hong Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12020">https://arxiv.org/abs/2505.12020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12020">https://arxiv.org/pdf/2505.12020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12020]] GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations(https://arxiv.org/abs/2505.12020)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The neural operator (NO) framework has emerged as a powerful tool for solving partial differential equations (PDEs). Recent NOs are dominated by the Transformer architecture, which offers NOs the capability to capture long-range dependencies in PDE dynamics. However, existing Transformer-based NOs suffer from quadratic complexity, lack geometric rigor, and thus suffer from sub-optimal performance on regular grids. As a remedy, we propose the Geometric Mamba Neural Operator (GeoMaNO) framework, which empowers NOs with Mamba's modeling capability, linear complexity, plus geometric rigor. We evaluate GeoMaNO's performance on multiple standard and popularly employed PDE benchmarks, spanning from Darcy flow problems to Navier-Stokes problems. GeoMaNO improves existing baselines in solution operator approximation by as much as 58.9%.</li>
</ul>

<h3>Title: Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment</h3>
<ul>
<li><strong>Authors: </strong>Kazuhiko Kawamoto, Atsuhiro Endo, Hiroshi Kera</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12021">https://arxiv.org/abs/2505.12021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12021">https://arxiv.org/pdf/2505.12021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12021]] Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment(https://arxiv.org/abs/2505.12021)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Task arithmetic enables efficient model editing by representing task-specific changes as vectors in parameter space. Task arithmetic typically assumes that the source and target models are initialized from the same pre-trained parameters. This assumption limits its applicability in cross-model transfer settings, where models are independently pre-trained on different datasets. To address this challenge, we propose a method based on few-shot orthogonal alignment, which aligns task vectors to the parameter space of a differently pre-trained target model. These transformations preserve key properties of task vectors, such as norm and rank, and are learned using only a small number of labeled examples. We evaluate the method using two Vision Transformers pre-trained on YFCC100M and LAION400M, and test on eight classification datasets. Experimental results show that our method improves transfer accuracy over direct task vector application and achieves performance comparable to few-shot fine-tuning, while maintaining the modularity and reusability of task vectors. Our code is available at this https URL.</li>
</ul>

<h3>Title: Spotlight Your Instructions: Instruction-following with Dynamic Attention Steering</h3>
<ul>
<li><strong>Authors: </strong>Praveen Venkateswaran, Danish Contractor</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12025">https://arxiv.org/abs/2505.12025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12025">https://arxiv.org/pdf/2505.12025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12025]] Spotlight Your Instructions: Instruction-following with Dynamic Attention Steering(https://arxiv.org/abs/2505.12025)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In many real-world applications, users rely on natural language instructions to guide large language models (LLMs) across a wide range of tasks. These instructions are often complex, diverse, and subject to frequent change. However, LLMs do not always attend to these instructions reliably, and users lack simple mechanisms to emphasize their importance beyond modifying prompt wording or structure. To address this, we present an inference-time method that enables users to emphasize specific parts of their prompt by steering the model's attention toward them, aligning the model's perceived importance of different prompt tokens with user intent. Unlike prior approaches that are limited to static instructions, require significant offline profiling, or rely on fixed biases, we dynamically update the proportion of model attention given to the user-specified parts--ensuring improved instruction following without performance degradation. We demonstrate that our approach improves instruction following across a variety of tasks involving multiple instructions and generalizes across models of varying scales.</li>
</ul>

<h3>Title: Relation-Aware Graph Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Jianxiang Yu, Jiapeng Zhu, Hao Qian, Ziqi Liu, Zhiqiang Zhang, Xiang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12027">https://arxiv.org/abs/2505.12027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12027">https://arxiv.org/pdf/2505.12027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12027]] Relation-Aware Graph Foundation Model(https://arxiv.org/abs/2505.12027)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have demonstrated remarkable generalization capabilities across various natural language processing (NLP) tasks. Similarly, graph foundation models (GFMs) have emerged as a promising direction in graph learning, aiming to generalize across diverse datasets through large-scale pre-training. However, unlike language models that rely on explicit token representations, graphs lack a well-defined unit for generalization, making it challenging to design effective pre-training strategies. In this work, we propose REEF, a novel framework that leverages relation tokens as the basic units for GFMs. Inspired by the token vocabulary in LLMs, we construct a relation vocabulary of relation tokens to store relational information within graphs. To accommodate diverse relations, we introduce two hypernetworks that adaptively generate the parameters of aggregators and classifiers in graph neural networks based on relation tokens. In addition, we design another hypernetwork to construct dataset-specific projectors and incorporate a dataset-level feature bias into the initial node representations, enhancing flexibility across different datasets with the same relation. Further, we adopt graph data augmentation and a mixed-dataset pre-training strategy, allowing REEF to capture relational diversity more effectively and exhibit strong generalization capabilities. Extensive experiments show that REEF significantly outperforms existing methods on both pre-training and transfer learning tasks, underscoring its potential as a powerful foundation model for graph-based applications.</li>
</ul>

<h3>Title: Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method</h3>
<ul>
<li><strong>Authors: </strong>Yupei Ren, Xinyi Zhou, Ning Zhang, Shangqing Zhao, Man Lan, Xiaopeng Bai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12028">https://arxiv.org/abs/2505.12028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12028">https://arxiv.org/pdf/2505.12028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12028]] Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method(https://arxiv.org/abs/2505.12028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Argument mining has garnered increasing attention over the years, with the recent advancement of Large Language Models (LLMs) further propelling this trend. However, current argument relations remain relatively simplistic and foundational, struggling to capture the full scope of argument information, particularly when it comes to representing complex argument structures in real-world scenarios. To address this limitation, we propose 14 fine-grained relation types from both vertical and horizontal dimensions, thereby capturing the intricate interplay between argument components for a thorough understanding of argument structure. On this basis, we conducted extensive experiments on three tasks: argument component detection, relation prediction, and automated essay grading. Additionally, we explored the impact of writing quality on argument component detection and relation prediction, as well as the connections between discourse relations and argumentative features. The findings highlight the importance of fine-grained argumentative annotations for argumentative writing quality assessment and encourage multi-dimensional argument analysis.</li>
</ul>

<h3>Title: Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets</h3>
<ul>
<li><strong>Authors: </strong>Ning Lu, Shengcai Liu, Jiahao Wu, Weiyu Chen, Zhirui Zhang, Yew-Soon Ong, Qi Wang, Ke Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12038">https://arxiv.org/abs/2505.12038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12038">https://arxiv.org/pdf/2505.12038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12038]] Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets(https://arxiv.org/abs/2505.12038)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown great potential as general-purpose AI assistants across various domains. To fully leverage this potential in specific applications, many companies provide fine-tuning API services, enabling users to upload their own data for LLM customization. However, fine-tuning services introduce a new safety threat: user-uploaded data, whether harmful or benign, can break the model's alignment, leading to unsafe outputs. Moreover, existing defense methods struggle to address the diversity of fine-tuning datasets (e.g., varying sizes, tasks), often sacrificing utility for safety or vice versa. To address this issue, we propose Safe Delta, a safety-aware post-training defense method that adjusts the delta parameters (i.e., the parameter change before and after fine-tuning). Specifically, Safe Delta estimates the safety degradation, selects delta parameters to maximize utility while limiting overall safety loss, and applies a safety compensation vector to mitigate residual safety loss. Through extensive experiments on four diverse datasets with varying settings, our approach consistently preserves safety while ensuring that the utility gain from benign datasets remains unaffected.</li>
</ul>

<h3>Title: FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shuai Yuan, Guowen Xu, Hongwei Li, Rui Zhang, Xinyuan Qian, Wenbo Jiang, Hangcheng Cao, Qingchuan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12045">https://arxiv.org/abs/2505.12045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12045">https://arxiv.org/pdf/2505.12045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12045]] FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition(https://arxiv.org/abs/2505.12045)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Traffic sign recognition (TSR) systems are crucial for autonomous driving but are vulnerable to backdoor attacks. Existing physical backdoor attacks either lack stealth, provide inflexible attack control, or ignore emerging Vision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the first physical-world backdoor attack leveraging fluorescent ink as triggers. Fluorescent triggers are invisible under normal conditions and activated stealthily by ultraviolet light, providing superior stealthiness, flexibility, and untraceability. Inspired by real-world graffiti, we derive realistic trigger shapes and enhance their robustness via an interpolation-based fluorescence simulation algorithm. Furthermore, we develop an automated backdoor sample generation method to support three attack objectives. Extensive evaluations in the physical world demonstrate FIGhost's effectiveness against state-of-the-art detectors and VLMs, maintaining robustness under environmental variations and effectively evading existing defenses.</li>
</ul>

<h3>Title: Unsupervised Port Berth Identification from Automatic Identification System Data</h3>
<ul>
<li><strong>Authors: </strong>Andreas Hadjipieris, Neofytos Dimitriou, Ognjen Arandjelovi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12046">https://arxiv.org/abs/2505.12046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12046">https://arxiv.org/pdf/2505.12046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12046]] Unsupervised Port Berth Identification from Automatic Identification System Data(https://arxiv.org/abs/2505.12046)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Port berthing sites are regions of high interest for monitoring and optimizing port operations. Data sourced from the Automatic Identification System (AIS) can be superimposed on berths enabling their real-time monitoring and revealing long-term utilization patterns. Ultimately, insights from multiple berths can uncover bottlenecks, and lead to the optimization of the underlying supply chain of the port and beyond. However, publicly available documentation of port berths, even when available, is frequently incomplete - e.g. there may be missing berths or inaccuracies such as incorrect boundary boxes - necessitating a more robust, data-driven approach to port berth localization. In this context, we propose an unsupervised spatial modeling method that leverages AIS data clustering and hyperparameter optimization to identify berthing sites. Trained on one month of freely available AIS data and evaluated across ports of varying sizes, our models significantly outperform competing methods, achieving a mean Bhattacharyya distance of 0.85 when comparing Gaussian Mixture Models (GMMs) trained on separate data splits, compared to 13.56 for the best existing method. Qualitative comparison with satellite images and existing berth labels further supports the superiority of our method, revealing more precise berth boundaries and improved spatial resolution across diverse port environments.</li>
</ul>

<h3>Title: Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling</h3>
<ul>
<li><strong>Authors: </strong>Rui Qin, Qijie Wang, Ming Sun, Haowei Zhu, Chao Zhou, Bin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12048">https://arxiv.org/abs/2505.12048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12048">https://arxiv.org/pdf/2505.12048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12048]] Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling(https://arxiv.org/abs/2505.12048)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have gained attention for their success in modeling complex distributions, achieving impressive perceptual quality in SR tasks. However, existing diffusion-based SR methods often suffer from high computational costs, requiring numerous iterative steps for training and inference. Existing acceleration techniques, such as distillation and solver optimization, are generally task-agnostic and do not fully leverage the specific characteristics of low-level tasks like super-resolution (SR). In this study, we analyze the frequency- and spatial-domain properties of diffusion-based SR methods, revealing key insights into the temporal and spatial dependencies of high-frequency signal recovery. Specifically, high-frequency details benefit from concentrated optimization during early and late diffusion iterations, while spatially textured regions demand adaptive denoising strategies. Building on these observations, we propose the Time-Spatial-aware Sampling strategy (TSS) for the acceleration of Diffusion SR without any extra training cost. TSS combines Time Dynamic Sampling (TDS), which allocates more iterations to refining textures, and Spatial Dynamic Sampling (SDS), which dynamically adjusts strategies based on image content. Extensive evaluations across multiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA) performance with significantly fewer iterations, improving MUSIQ scores by 0.2 - 3.0 and outperforming the current acceleration methods with only half the number of steps.</li>
</ul>

<h3>Title: VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption</h3>
<ul>
<li><strong>Authors: </strong>Tianxiong Zhong, Xingye Tian, Boyuan Jiang, Xuebo Wang, Xin Tao, Pengfei Wan, Zhiwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12053">https://arxiv.org/abs/2505.12053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12053">https://arxiv.org/pdf/2505.12053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12053]] VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption(https://arxiv.org/abs/2505.12053)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers.</li>
</ul>

<h3>Title: GenderBench: Evaluation Suite for Gender Biases in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mat Pikuliak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12054">https://arxiv.org/abs/2505.12054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12054">https://arxiv.org/pdf/2505.12054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12054]] GenderBench: Evaluation Suite for Gender Biases in LLMs(https://arxiv.org/abs/2505.12054)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present GenderBench -- a comprehensive evaluation suite designed to measure gender biases in LLMs. GenderBench includes 14 probes that quantify 19 gender-related harmful behaviors exhibited by LLMs. We release GenderBench as an open-source and extensible library to improve the reproducibility and robustness of benchmarking across the field. We also publish our evaluation of 12 LLMs. Our measurements reveal consistent patterns in their behavior. We show that LLMs struggle with stereotypical reasoning, equitable gender representation in generated texts, and occasionally also with discriminatory behavior in high-stakes scenarios, such as hiring.</li>
</ul>

<h3>Title: Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Peng Ding, Jun Kuang, Zongyu Wang, Xuezhi Cao, Xunliang Cai, Jiajun Chen, Shujian Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12060">https://arxiv.org/abs/2505.12060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12060">https://arxiv.org/pdf/2505.12060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12060]] Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement(https://arxiv.org/abs/2505.12060)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive capabilities across various tasks but remain vulnerable to meticulously crafted jailbreak attacks. In this paper, we identify a critical safety gap: while LLMs are adept at detecting jailbreak prompts, they often produce unsafe responses when directly processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware Guard Enhancement), a training-free defense strategy designed to align LLMs' strong safety discrimination performance with their relatively weaker safety generation ability. SAGE consists of two core components: a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against sophisticated jailbreak attempts through flexible safety discrimination instructions. Extensive experiments demonstrate SAGE's effectiveness and robustness across various open-source and closed-source LLMs of different sizes and architectures, achieving an average 99% defense success rate against numerous complex and covert jailbreak methods while maintaining helpfulness on general benchmarks. We further conduct mechanistic interpretability analysis through hidden states and attention distributions, revealing the underlying mechanisms of this detection-generation discrepancy. Our work thus contributes to developing future LLMs with coherent safety awareness and generation behavior. Our code and datasets are publicly available at this https URL.</li>
</ul>

<h3>Title: Beluga Whale Detection from Satellite Imagery with Point Labels</h3>
<ul>
<li><strong>Authors: </strong>Yijie Zheng, Jinxuan Yang, Yu Chen, Yaxuan Wang, Yihang Lu, Guoqing Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12066">https://arxiv.org/abs/2505.12066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12066">https://arxiv.org/pdf/2505.12066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12066]] Beluga Whale Detection from Satellite Imagery with Point Labels(https://arxiv.org/abs/2505.12066)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Very high-resolution (VHR) satellite imagery has emerged as a powerful tool for monitoring marine animals on a large scale. However, existing deep learning-based whale detection methods usually require manually created, high-quality bounding box annotations, which are labor-intensive to produce. Moreover, existing studies often exclude ``uncertain whales'', individuals that have ambiguous appearances in satellite imagery, limiting the applicability of these models in real-world scenarios. To address these limitations, this study introduces an automated pipeline for detecting beluga whales and harp seals in VHR satellite imagery. The pipeline leverages point annotations and the Segment Anything Model (SAM) to generate precise bounding box annotations, which are used to train YOLOv8 for multiclass detection of certain whales, uncertain whales, and harp seals. Experimental results demonstrated that SAM-generated annotations significantly improved detection performance, achieving higher $\text{F}_\text{1}$-scores compared to traditional buffer-based annotations. YOLOv8 trained on SAM-labeled boxes achieved an overall $\text{F}_\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals, with superior performance in dense scenes. The proposed approach not only reduces the manual effort required for annotation but also enhances the detection of uncertain whales, offering a more comprehensive solution for marine animal monitoring. This method holds great potential for extending to other species, habitats, and remote sensing platforms, as well as for estimating whale biometrics, thereby advancing ecological monitoring and conservation efforts. The codes for our label and detection pipeline are publicly available at this http URL .</li>
</ul>

<h3>Title: Do different prompting methods yield a common task representation in language models?</h3>
<ul>
<li><strong>Authors: </strong>Guy Davidson, Todd M. Gureckis, Brenden M. Lake, Adina Williams</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12075">https://arxiv.org/abs/2505.12075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12075">https://arxiv.org/pdf/2505.12075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12075]] Do different prompting methods yield a common task representation in language models?(https://arxiv.org/abs/2505.12075)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through function vectors, recently proposed as a mechanism to extract few-shot ICL task representations. We generalize function vectors to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task presentations do not induce a common task representation but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining textual instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms.</li>
</ul>

<h3>Title: VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12081">https://arxiv.org/abs/2505.12081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12081">https://arxiv.org/pdf/2505.12081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12081]] VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning(https://arxiv.org/abs/2505.12081)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in a unified framework. The model generates a structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting).</li>
</ul>

<h3>Title: Model Merging in Pre-training of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, Jin Ma, Xunhao Lai, Yao Luo, Xingyan Bin, Hongbin Ren, Mingji Han, Wenhao Hao, Bairen Yi, LingJun Liu, Bole Ma, Xiaoying Jia, Zhou Xun, Liang Xiang, Yonghui Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12082">https://arxiv.org/abs/2505.12082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12082">https://arxiv.org/pdf/2505.12082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12082]] Model Merging in Pre-training of Large Language Models(https://arxiv.org/abs/2505.12082)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.</li>
</ul>

<h3>Title: Discovering Symbolic Differential Equations with Symmetry Invariants</h3>
<ul>
<li><strong>Authors: </strong>Jianke Yang, Manu Bhat, Bryan Hu, Yadi Cao, Nima Dehmamy, Robin Walters, Rose Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12083">https://arxiv.org/abs/2505.12083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12083">https://arxiv.org/pdf/2505.12083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12083]] Discovering Symbolic Differential Equations with Symmetry Invariants(https://arxiv.org/abs/2505.12083)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discovering symbolic differential equations from data uncovers fundamental dynamical laws underlying complex systems. However, existing methods often struggle with the vast search space of equations and may produce equations that violate known physical laws. In this work, we address these problems by introducing the concept of \textit{symmetry invariants} in equation discovery. We leverage the fact that differential equations admitting a symmetry group can be expressed in terms of differential invariants of symmetry transformations. Thus, we propose to use these invariants as atomic entities in equation discovery, ensuring the discovered equations satisfy the specified symmetry. Our approach integrates seamlessly with existing equation discovery methods such as sparse regression and genetic programming, improving their accuracy and efficiency. We validate the proposed method through applications to various physical systems, such as fluid and reaction-diffusion, demonstrating its ability to recover parsimonious and interpretable equations that respect the laws of physics.</li>
</ul>

<h3>Title: Personalized Author Obfuscation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Shokri, Sarah Ita Levitan, Rivka Levitan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12090">https://arxiv.org/abs/2505.12090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12090">https://arxiv.org/pdf/2505.12090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12090]] Personalized Author Obfuscation with Large Language Models(https://arxiv.org/abs/2505.12090)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the efficacy of large language models (LLMs) in obfuscating authorship by paraphrasing and altering writing styles. Rather than adopting a holistic approach that evaluates performance across the entire dataset, we focus on user-wise performance to analyze how obfuscation effectiveness varies across individual authors. While LLMs are generally effective, we observe a bimodal distribution of efficacy, with performance varying significantly across users. To address this, we propose a personalized prompting method that outperforms standard prompting techniques and partially mitigates the bimodality issue.</li>
</ul>

<h3>Title: Attribution Projection Calculus: A Novel Framework for Causal Inference in Bayesian Networks</h3>
<ul>
<li><strong>Authors: </strong>M Ruhul Amin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12094">https://arxiv.org/abs/2505.12094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12094">https://arxiv.org/pdf/2505.12094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12094]] Attribution Projection Calculus: A Novel Framework for Causal Inference in Bayesian Networks(https://arxiv.org/abs/2505.12094)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces Attribution Projection Calculus (AP-Calculus), a novel mathematical framework for determining causal relationships in structured Bayesian networks. We investigate a specific network architecture with source nodes connected to destination nodes through intermediate nodes, where each input maps to a single label with maximum marginal probability. We prove that for each label, exactly one intermediate node acts as a deconfounder while others serve as confounders, enabling optimal attribution of features to their corresponding labels. The framework formalizes the dual nature of intermediate nodes as both confounders and deconfounders depending on the context, and establishes separation functions that maximize distinctions between intermediate representations. We demonstrate that the proposed network architecture is optimal for causal inference compared to alternative structures, including those based on Pearl's causal framework. AP-Calculus provides a comprehensive mathematical foundation for analyzing feature-label attributions, managing spurious correlations, quantifying information gain, ensuring fairness, and evaluating uncertainty in prediction models, including large language models. Theoretical verification shows that AP-Calculus not only extends but can also subsume traditional do-calculus for many practical applications, offering a more direct approach to causal inference in supervised learning contexts.</li>
</ul>

<h3>Title: Improving Fairness in LLMs Through Testing-Time Adversaries</h3>
<ul>
<li><strong>Authors: </strong>Isabela Pereira Gregio, Ian Pons, Anna Helena Reali Costa, Artur Jordo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12100">https://arxiv.org/abs/2505.12100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12100">https://arxiv.org/pdf/2505.12100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12100]] Improving Fairness in LLMs Through Testing-Time Adversaries(https://arxiv.org/abs/2505.12100)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) push the bound-aries in natural language processing and generative AI, driving progress across various aspects of modern society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e., predictions) poses a significant and open challenge, hindering their application in tasks involving ethical sensitivity and responsible decision-making. In this work, we propose a straightforward, user-friendly and practical method to mitigate such biases, enhancing the reliability and trustworthiness of LLMs. Our method creates multiple variations of a given sentence by modifying specific attributes and evaluates the corresponding prediction behavior compared to the original, unaltered, prediction/sentence. The idea behind this process is that critical ethical predictions often exhibit notable inconsistencies, indicating the presence of bias. Unlike previous approaches, our method relies solely on forward passes (i.e., testing-time adversaries), eliminating the need for training, fine-tuning, or prior knowledge of the training data distribution. Through extensive experiments on the popular Llama family, we demonstrate the effectiveness of our method in improving various fairness metrics, focusing on the reduction of disparities in how the model treats individuals from different racial groups. Specifically, using standard metrics, we improve the fairness in Llama3 in up to 27 percentage points. Overall, our approach significantly enhances fairness, equity, and reliability in LLM-generated results without parameter tuning or training data modifications, confirming its effectiveness in practical scenarios. We believe our work establishes an important step toward enabling the use of LLMs in tasks that require ethical considerations and responsible decision-making.</li>
</ul>

<h3>Title: The Impact of Emerging Phishing Threats: Assessing Quishing and LLM-generated Phishing Emails against Organizations</h3>
<ul>
<li><strong>Authors: </strong>Marie Weinz, Nicola Zannone, Luca Allodi, Giovanni Apruzzese</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12104">https://arxiv.org/abs/2505.12104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12104">https://arxiv.org/pdf/2505.12104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12104]] The Impact of Emerging Phishing Threats: Assessing Quishing and LLM-generated Phishing Emails against Organizations(https://arxiv.org/abs/2505.12104)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Modern organizations are persistently targeted by phishing emails. Despite advances in detection systems and widespread employee training, attackers continue to innovate, posing ongoing threats. Two emerging vectors stand out in the current landscape: QR-code baits and LLM-enabled pretexting. Yet, little is known about the effectiveness of current defenses against these attacks, particularly when it comes to real-world impact on employees. This gap leaves uncertainty around to what extent related countermeasures are justified or needed. Our work addresses this issue. We conduct three phishing simulations across organizations of varying sizes -- from small-medium businesses to a multinational enterprise. In total, we send over 71k emails targeting employees, including: a "traditional" phishing email with a click-through button; a nearly-identical "quishing" email with a QR code instead; and a phishing email written with the assistance of an LLM and open-source intelligence. Our results show that quishing emails have the same effectiveness as traditional phishing emails at luring users to the landing webpage -- which is worrying, given that quishing emails are much harder to identify even by operational detectors. We also find that LLMs can be very good "social engineers": in one company, over 30% of the emails opened led to visiting the landing webpage -- a rate exceeding some prior benchmarks. Finally, we complement our study by conducting a survey across the organizations' employees, measuring their "perceived" phishing awareness. Our findings suggest a correlation between higher self-reported awareness and organizational resilience to phishing attempts.</li>
</ul>

<h3>Title: MalVis: A Large-Scale Image-Based Framework and Dataset for Advancing Android Malware Classification</h3>
<ul>
<li><strong>Authors: </strong>Saleh J. Makkawy, Michael J. De Lucia, Kenneth E. Barner</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12106">https://arxiv.org/abs/2505.12106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12106">https://arxiv.org/pdf/2505.12106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12106]] MalVis: A Large-Scale Image-Based Framework and Dataset for Advancing Android Malware Classification(https://arxiv.org/abs/2505.12106)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability</a></li>
<li><strong>Abstract: </strong>As technology advances, Android malware continues to pose significant threats to devices and sensitive data. The open-source nature of the Android OS and the availability of its SDK contribute to this rapid growth. Traditional malware detection techniques, such as signature-based, static, and dynamic analysis, struggle to detect obfuscated threats that use encryption, packing, or compression. While deep learning (DL)-based visualization methods have been proposed, they often fail to highlight the critical malicious features effectively. This research introduces MalVis, a unified visualization framework that integrates entropy and N-gram analysis to emphasize structural and anomalous patterns in malware bytecode. MalVis addresses key limitations of prior methods, including insufficient feature representation, poor interpretability, and limited data accessibility. The framework leverages a newly introduced large-scale dataset, the MalVis dataset, containing over 1.3 million visual samples across nine malware classes and one benign class. We evaluate MalVis against state-of-the-art visualization techniques using leading CNN models: MobileNet-V2, DenseNet201, ResNet50, and Inception-V3. To enhance performance and reduce overfitting, we implement eight ensemble learning strategies. Additionally, an undersampling technique mitigates class imbalance in the multiclass setting. MalVis achieves strong results: 95.19% accuracy, 90.81% F1-score, 92.58% precision, 89.10% recall, 87.58% MCC, and 98.06% ROC-AUC. These findings demonstrate the effectiveness of MalVis in enabling accurate, interpretable malware detection and providing a valuable resource for security research and applications.</li>
</ul>

<h3>Title: EarthSynth: Generating Informative Earth Observation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiancheng Pan, Shiye Lei, Yuqian Fu, Jiahao Li, Yanxing Liu, Yuze Sun, Xiao He, Long Peng, Xiaomeng Huang, Bo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12108">https://arxiv.org/abs/2505.12108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12108">https://arxiv.org/pdf/2505.12108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12108]] EarthSynth: Generating Informative Earth Observation with Diffusion Models(https://arxiv.org/abs/2505.12108)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Remote sensing image (RSI) interpretation typically faces challenges due to the scarcity of labeled data, which limits the performance of RSI interpretation tasks. To tackle this challenge, we propose EarthSynth, a diffusion-based generative foundation model that enables synthesizing multi-category, cross-satellite labeled Earth observation for downstream RSI interpretation tasks. To the best of our knowledge, EarthSynth is the first to explore multi-task generation for remote sensing. EarthSynth, trained on the EarthSynth-180K dataset, employs the Counterfactual Composition training strategy to improve training data diversity and enhance category control. Furthermore, a rule-based method of R-Filter is proposed to filter more informative synthetic data for downstream tasks. We evaluate our EarthSynth on scene classification, object detection, and semantic segmentation in open-world scenarios, offering a practical solution for advancing RSI interpretation.</li>
</ul>

<h3>Title: SAINT: Attention-Based Modeling of Sub-Action Dependencies in Multi-Action Policies</h3>
<ul>
<li><strong>Authors: </strong>Matthew Landers, Taylor W. Killian, Thomas Hartvigsen, Afsaneh Doryab</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12109">https://arxiv.org/abs/2505.12109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12109">https://arxiv.org/pdf/2505.12109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12109]] SAINT: Attention-Based Modeling of Sub-Action Dependencies in Multi-Action Policies(https://arxiv.org/abs/2505.12109)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The combinatorial structure of many real-world action spaces leads to exponential growth in the number of possible actions, limiting the effectiveness of conventional reinforcement learning algorithms. Recent approaches for combinatorial action spaces impose factorized or sequential structures over sub-actions, failing to capture complex joint behavior. We introduce the Sub-Action Interaction Network using Transformers (SAINT), a novel policy architecture that represents multi-component actions as unordered sets and models their dependencies via self-attention conditioned on the global state. SAINT is permutation-invariant, sample-efficient, and compatible with standard policy optimization algorithms. In 15 distinct combinatorial environments across three task domains, including environments with nearly 17 million joint actions, SAINT consistently outperforms strong baselines.</li>
</ul>

<h3>Title: Back to Square Roots: An Optimal Bound on the Matrix Factorization Error for Multi-Epoch Differentially Private SGD</h3>
<ul>
<li><strong>Authors: </strong>Nikita P. Kalinin, Ryan McKenna, Jalaj Upadhyay, Christoph H. Lampert</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12128">https://arxiv.org/abs/2505.12128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12128">https://arxiv.org/pdf/2505.12128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12128]] Back to Square Roots: An Optimal Bound on the Matrix Factorization Error for Multi-Epoch Differentially Private SGD(https://arxiv.org/abs/2505.12128)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Matrix factorization mechanisms for differentially private training have emerged as a promising approach to improve model utility under privacy constraints. In practical settings, models are typically trained over multiple epochs, requiring matrix factorizations that account for repeated participation. Existing theoretical upper and lower bounds on multi-epoch factorization error leave a significant gap. In this work, we introduce a new explicit factorization method, Banded Inverse Square Root (BISR), which imposes a banded structure on the inverse correlation matrix. This factorization enables us to derive an explicit and tight characterization of the multi-epoch error. We further prove that BISR achieves asymptotically optimal error by matching the upper and lower bounds. Empirically, BISR performs on par with state-of-the-art factorization methods, while being simpler to implement, computationally efficient, and easier to analyze.</li>
</ul>

<h3>Title: Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Niaz Ahmad, Jawad Khan, Kang G. Shin, Youngmoon Lee, Guanghui Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12130">https://arxiv.org/abs/2505.12130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12130">https://arxiv.org/pdf/2505.12130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12130]] Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation(https://arxiv.org/abs/2505.12130)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The dynamic movement of the human body presents a fundamental challenge for human pose estimation and body segmentation. State-of-the-art approaches primarily rely on combining keypoint heatmaps with segmentation masks but often struggle in scenarios involving overlapping joints or rapidly changing poses during instance-level segmentation. To address these limitations, we propose Keypoints as Dynamic Centroid (KDC), a new centroid-based representation for unified human pose estimation and instance-level segmentation. KDC adopts a bottom-up paradigm to generate keypoint heatmaps for both easily distinguishable and complex keypoints and improves keypoint detection and confidence scores by introducing KeyCentroids using a keypoint disk. It leverages high-confidence keypoints as dynamic centroids in the embedding space to generate MaskCentroids, allowing for swift clustering of pixels to specific human instances during rapid body movements in live environments. Our experimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks demonstrate KDC's effectiveness and generalizability in challenging scenarios in terms of both accuracy and runtime performance. The implementation is available at: this https URL.</li>
</ul>

<h3>Title: Transformer learns the cross-task prior and regularization for in-context learning</h3>
<ul>
<li><strong>Authors: </strong>Fei Lu, Yue Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12138">https://arxiv.org/abs/2505.12138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12138">https://arxiv.org/pdf/2505.12138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12138]] Transformer learns the cross-task prior and regularization for in-context learning(https://arxiv.org/abs/2505.12138)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Transformers have shown a remarkable ability for in-context learning (ICL), making predictions based on contextual examples. However, while theoretical analyses have explored this prediction capability, the nature of the inferred context and its utility for downstream predictions remain open questions. This paper aims to address these questions by examining ICL for inverse linear regression (ILR), where context inference can be characterized by unsupervised learning of underlying weight vectors. Focusing on the challenging scenario of rank-deficient inverse problems, where context length is smaller than the number of unknowns in the weight vectors and regularization is necessary, we introduce a linear transformer to learn the inverse mapping from contextual examples to the underlying weight vector. Our findings reveal that the transformer implicitly learns both a prior distribution and an effective regularization strategy, outperforming traditional ridge regression and regularization methods. A key insight is the necessity of low task dimensionality relative to the context length for successful learning. Furthermore, we numerically verify that the error of the transformer estimator scales linearly with the noise level, the ratio of task dimension to context length, and the condition number of the input data. These results not only demonstrate the potential of transformers for solving ill-posed inverse problems, but also provide a new perspective towards understanding the knowledge extraction mechanism within transformers.</li>
</ul>

<h3>Title: Proof-of-Social-Capital: Privacy-Preserving Consensus Protocol Replacing Stake for Social Capital</h3>
<ul>
<li><strong>Authors: </strong>Juraj Mariani, Ivan Homoliak</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12144">https://arxiv.org/abs/2505.12144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12144">https://arxiv.org/pdf/2505.12144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12144]] Proof-of-Social-Capital: Privacy-Preserving Consensus Protocol Replacing Stake for Social Capital(https://arxiv.org/abs/2505.12144)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, fair</a></li>
<li><strong>Abstract: </strong>Consensus protocols used today in blockchains often rely on computational power or financial stakes - scarce resources. We propose a novel protocol using social capital - trust and influence from social interactions - as a non-transferable staking mechanism to ensure fairness and decentralization. The methodology integrates zero-knowledge proofs, verifiable credentials, a Whisk-like leader election, and an incentive scheme to prevent Sybil attacks and encourage engagement. The theoretical framework would enhance privacy and equity, though unresolved issues like off-chain bribery require further research. This work offers a new model aligned with modern social media behavior and lifestyle, with applications in finance, providing a practical insight for decentralized system development.</li>
</ul>

<h3>Title: Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features</h3>
<ul>
<li><strong>Authors: </strong>Alex Heyman, Joel Zylberberg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12151">https://arxiv.org/abs/2505.12151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12151">https://arxiv.org/pdf/2505.12151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12151]] Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features(https://arxiv.org/abs/2505.12151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have recently made great strides in reasoning task performance through chain-of-thought (CoT) strategies trained via reinforcement learning; however, these "reasoning large language models" (RLLMs) remain imperfect reasoners, and understanding the frequencies and causes of their failure modes is important for both users and developers. We test o1-mini, o3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3 Mini Beta on graph coloring as a variable-complexity constraint-satisfaction logic problem, and find evidence from both error rate comparisons and CoT/explanation text analysis that RLLMs are prone to hallucinate edges not specified in the prompt's description of the graph. This phenomenon persists across multiple problem complexity levels and semantic frames, and it appears to account for a significant fraction of the incorrect answers from every tested model, and the vast majority of them for some models. Our results indicate that RLLMs may possess broader issues with misrepresentation of problem specifics, and we offer suggestions for design choices to mitigate this weakness.</li>
</ul>

<h3>Title: Learning to Highlight Audio by Watching Movies</h3>
<ul>
<li><strong>Authors: </strong>Chao Huang, Ruohan Gao, J. M. F. Tsang, Jan Kurcius, Cagdas Bilen, Chenliang Xu, Anurag Kumar, Sanjeel Parekh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12154">https://arxiv.org/abs/2505.12154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12154">https://arxiv.org/pdf/2505.12154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12154]] Learning to Highlight Audio by Watching Movies(https://arxiv.org/abs/2505.12154)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent years have seen a significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in a disconnect between visual and acoustic saliency. To bridge this gap, we introduce a novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating a more harmonious audio-visual experience. We propose a flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce a new dataset -- the muddy mix dataset, leveraging the meticulous audio and video crafting found in movies, which provides a form of free supervision. We develop a pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through a three-step process -- separation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: this https URL.</li>
</ul>

<h3>Title: SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds</h3>
<ul>
<li><strong>Authors: </strong>Ranit Karmakar, Simon F. Nrrelykke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12155">https://arxiv.org/abs/2505.12155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12155">https://arxiv.org/pdf/2505.12155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12155]] SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds(https://arxiv.org/abs/2505.12155)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Segmentation evaluation metrics traditionally rely on binary decision logic: predictions are either correct or incorrect, based on rigid IoU thresholds. Detection--based metrics such as F1 and mAP determine correctness at the object level using fixed overlap cutoffs, while overlap--based metrics like Intersection over Union (IoU) and Dice operate at the pixel level, often overlooking instance--level structure. Panoptic Quality (PQ) attempts to unify detection and segmentation assessment, but it remains dependent on hard-threshold matching--treating predictions below the threshold as entirely incorrect. This binary framing obscures important distinctions between qualitatively different errors and fails to reward gradual model improvements. We propose SoftPQ, a flexible and interpretable instance segmentation metric that redefines evaluation as a graded continuum rather than a binary classification. SoftPQ introduces tunable upper and lower IoU thresholds to define a partial matching region and applies a sublinear penalty function to ambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit smoother score behavior, greater robustness to structural segmentation errors, and more informative feedback for model development and evaluation. Through controlled perturbation experiments, we show that SoftPQ captures meaningful differences in segmentation quality that existing metrics overlook, making it a practical and principled alternative for both benchmarking and iterative model refinement.</li>
</ul>

<h3>Title: The AI Gap: How Socioeconomic Status Affects Language Technology Interactions</h3>
<ul>
<li><strong>Authors: </strong>Elisa Bassignana, Amanda Cercas Curry, Dirk Hovy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12158">https://arxiv.org/abs/2505.12158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12158">https://arxiv.org/pdf/2505.12158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12158]] The AI Gap: How Socioeconomic Status Affects Language Technology Interactions(https://arxiv.org/abs/2505.12158)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Socioeconomic status (SES) fundamentally influences how people interact with each other and more recently, with digital technologies like Large Language Models (LLMs). While previous research has highlighted the interaction between SES and language technology, it was limited by reliance on proxy metrics and synthetic data. We survey 1,000 individuals from diverse socioeconomic backgrounds about their use of language technologies and generative AI, and collect 6,482 prompts from their previous interactions with LLMs. We find systematic differences across SES groups in language technology usage (i.e., frequency, performed tasks), interaction styles, and topics. Higher SES entails a higher level of abstraction, convey requests more concisely, and topics like 'inclusivity' and 'travel'. Lower SES correlates with higher anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more concrete language. Our findings suggest that while generative language technologies are becoming more accessible to everyone, socioeconomic linguistic differences still stratify their use to exacerbate the digital divide. These differences underscore the importance of considering SES in developing language technologies to accommodate varying linguistic needs rooted in socioeconomic factors and limit the AI Gap across SES groups.</li>
</ul>

<h3>Title: FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting Models</h3>
<ul>
<li><strong>Authors: </strong>Yue Deng, Asadullah Hill Galib, Xin Lan, Pang-Ning Tan, Lifeng Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12167">https://arxiv.org/abs/2505.12167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12167">https://arxiv.org/pdf/2505.12167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12167]] FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting Models(https://arxiv.org/abs/2505.12167)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Deep learning-based weather forecasting models have recently demonstrated significant performance improvements over gold-standard physics-based simulation tools. However, these models are vulnerable to adversarial attacks, which raises concerns about their trustworthiness. In this paper, we first investigate the feasibility of applying existing adversarial attack methods to weather forecasting models. We argue that a successful attack should (1) not modify significantly its original inputs, (2) be faithful, i.e., achieve the desired forecast at targeted locations with minimal changes to non-targeted locations, and (3) be geospatio-temporally realistic. However, balancing these criteria is a challenge as existing methods are not designed to preserve the geospatio-temporal dependencies of the original samples. To address this challenge, we propose a novel framework called FABLE (Forecast Alteration By Localized targeted advErsarial attack), which employs a 3D discrete wavelet decomposition to extract the varying components of the geospatio-temporal data. By regulating the magnitude of adversarial perturbations across different components, FABLE can generate adversarial inputs that maintain geospatio-temporal coherence while remaining faithful and closely aligned with the original inputs. Experimental results on multiple real-world datasets demonstrate the effectiveness of our framework over baseline methods across various metrics.</li>
</ul>

<h3>Title: Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases</h3>
<ul>
<li><strong>Authors: </strong>Manari Hirose, Masato Uchida</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12183">https://arxiv.org/abs/2505.12183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12183">https://arxiv.org/pdf/2505.12183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12183]] Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases(https://arxiv.org/abs/2505.12183)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.</li>
</ul>

<h3>Title: Self-Destructive Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Wang, Rongyi Zhu, Ting Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12186">https://arxiv.org/abs/2505.12186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12186">https://arxiv.org/pdf/2505.12186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12186]] Self-Destructive Language Model(https://arxiv.org/abs/2505.12186)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Harmful fine-tuning attacks pose a major threat to the security of large language models (LLMs), allowing adversaries to compromise safety guardrails with minimal harmful data. While existing defenses attempt to reinforce LLM alignment, they fail to address models' inherent "trainability" on harmful data, leaving them vulnerable to stronger attacks with increased learning rates or larger harmful datasets. To overcome this critical limitation, we introduce SEAM, a novel alignment-enhancing defense that transforms LLMs into self-destructive models with intrinsic resilience to misalignment attempts. Specifically, these models retain their capabilities for legitimate tasks while exhibiting substantial performance degradation when fine-tuned on harmful data. The protection is achieved through a novel loss function that couples the optimization trajectories of benign and harmful data, enhanced with adversarial gradient ascent to amplify the self-destructive effect. To enable practical training, we develop an efficient Hessian-free gradient estimate with theoretical error bounds. Extensive evaluation across LLMs and datasets demonstrates that SEAM creates a no-win situation for adversaries: the self-destructive models achieve state-of-the-art robustness against low-intensity attacks and undergo catastrophic performance collapse under high-intensity attacks, rendering them effectively unusable. (warning: this paper contains potentially harmful content generated by LLMs.)</li>
</ul>

<h3>Title: Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum</h3>
<ul>
<li><strong>Authors: </strong>Wenquan Lu, Jiaqi Zhang, Hugues Van Assel, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12191">https://arxiv.org/abs/2505.12191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12191">https://arxiv.org/pdf/2505.12191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12191]] Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum(https://arxiv.org/abs/2505.12191)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Self-Supervised Learning (SSL) has become a powerful solution to extract rich representations from unlabeled data. Yet, SSL research is mostly focused on clean, curated and high-quality datasets. As a result, applying SSL on noisy data remains a challenge, despite being crucial to applications such as astrophysics, medical imaging, geophysics or finance. In this work, we present a fully self-supervised framework that enables noise-robust representation learning without requiring a denoiser at inference or downstream fine-tuning. Our method first trains an SSL denoiser on noisy data, then uses it to construct a denoised-to-noisy data curriculum (i.e., training first on denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2), combined with a teacher-guided regularization that anchors noisy embeddings to their denoised counterparts. This process encourages the model to internalize noise robustness. Notably, the denoiser can be discarded after pretraining, simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise ($\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by 4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from noise-aware pretraining. The code is available at this https URL.</li>
</ul>

<h3>Title: BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech</h3>
<ul>
<li><strong>Authors: </strong>Riad Hossain, Muhammad Ashad Kabir, Arat Ibne Golam Mowla, Animesh Chandra Roy, Ranjit Kumar Ghosh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12192">https://arxiv.org/abs/2505.12192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12192">https://arxiv.org/pdf/2505.12192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12192]] BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech(https://arxiv.org/abs/2505.12192)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) poses a growing global health challenge, with Bangladesh experiencing a notable rise in PD-related mortality. Early detection of PD remains particularly challenging in resource-constrained settings, where voice-based analysis has emerged as a promising non-invasive and cost-effective alternative. However, existing studies predominantly focus on English or other major languages; notably, no voice dataset for PD exists for Bengali - posing a significant barrier to culturally inclusive and accessible healthcare solutions. Moreover, most prior studies employed only a narrow set of acoustic features, with limited or no hyperparameter tuning and feature selection strategies, and little attention to model explainability. This restricts the development of a robust and generalizable machine learning model. To address this gap, we present BenSparX, the first Bengali conversational speech dataset for PD detection, along with a robust and explainable machine learning framework tailored for early diagnosis. The proposed framework incorporates diverse acoustic feature categories, systematic feature selection methods, and state-of-the-art machine learning algorithms with extensive hyperparameter optimization. Furthermore, to enhance interpretability and trust in model predictions, the framework incorporates SHAP (SHapley Additive exPlanations) analysis to quantify the contribution of individual acoustic features toward PD detection. Our framework achieves state-of-the-art performance, yielding an accuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further externally validated our approach by applying the framework to existing PD datasets in other languages, where it consistently outperforms state-of-the-art approaches. To facilitate further research and reproducibility, the dataset has been made publicly available at this https URL.</li>
</ul>

<h3>Title: Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled</h3>
<ul>
<li><strong>Authors: </strong>Yi-Chien Lin, Hongao Zhu, William Schuler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12196">https://arxiv.org/abs/2505.12196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12196">https://arxiv.org/pdf/2505.12196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12196]] Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled(https://arxiv.org/abs/2505.12196)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The impressive linguistic abilities of large language models (LLMs) have recommended them as models of human sentence processing, with some conjecturing a positive 'quality-power' relationship (Wilcox et al., 2023), in which language models' (LMs') fit to psychometric data continues to improve as their ability to predict words in context increases. This is important because it suggests that elements of LLM architecture, such as veridical attention to context and a unique objective of predicting upcoming words, reflect the architecture of the human sentence processing faculty, and that any inadequacies in predicting human reading time and brain imaging data may be attributed to insufficient model complexity, which recedes as larger models become available. Recent studies (Oh and Schuler, 2023) have shown this scaling inverts after a point, as LMs become excessively large and accurate, when word prediction probability (as information-theoretic surprisal) is used as a predictor. Other studies propose the use of entire vectors from differently sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting doubt on the value of surprisal as a predictor, but do not control for the larger number of predictors in vectors from larger LMs. This study evaluates LLM scaling using entire LLM vectors, while controlling for the larger number of predictors in vectors from larger LLMs. Results show that inverse scaling obtains, suggesting that inadequacies in predicting human reading time and brain imaging data may be due to substantial misalignment between LLMs and human sentence processing, which worsens as larger models are used.</li>
</ul>

<h3>Title: Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather</h3>
<ul>
<li><strong>Authors: </strong>Kui Jiang, Jing Cao, Zhaocheng Yu, Junjun Jiang, Jingchun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12199">https://arxiv.org/abs/2505.12199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12199">https://arxiv.org/pdf/2505.12199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12199]] Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather(https://arxiv.org/abs/2505.12199)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation is critical for applications such as autonomous driving and scene reconstruction. While existing methods perform well under normal scenarios, their performance declines in adverse weather, due to challenging domain shifts and difficulties in extracting scene information. To address this issue, we present a robust monocular depth estimation method called \textbf{ACDepth} from the perspective of high-quality training data generation and domain adaptation. Specifically, we introduce a one-step diffusion model for generating samples that simulate adverse weather conditions, constructing a multi-tuple degradation dataset during training. To ensure the quality of the generated degradation samples, we employ LoRA adapters to fine-tune the generation weights of diffusion model. Additionally, we integrate circular consistency loss and adversarial training to guarantee the fidelity and naturalness of the scene contents. Furthermore, we elaborate on a multi-granularity knowledge distillation strategy (MKD) that encourages the student network to absorb knowledge from both the teacher model and pretrained Depth Anything V2. This strategy guides the student model in learning degradation-agnostic scene information from various degradation inputs. In particular, we introduce an ordinal guidance distillation mechanism (OGD) that encourages the network to focus on uncertain regions through differential ranking, leading to a more precise depth estimation. Experimental results demonstrate that our ACDepth surpasses md4all-DD by 2.50\% for night scene and 2.61\% for rainy scene on the nuScenes dataset in terms of the absRel metric.</li>
</ul>

<h3>Title: How Reliable is Multilingual LLM-as-a-Judge?</h3>
<ul>
<li><strong>Authors: </strong>Xiyan Fu, Wei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12201">https://arxiv.org/abs/2505.12201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12201">https://arxiv.org/pdf/2505.12201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12201]] How Reliable is Multilingual LLM-as-a-Judge?(https://arxiv.org/abs/2505.12201)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced large language models assess generation results in alignment with human instructions. While these models serve as a promising alternative to human annotators, their reliability in multilingual evaluation remains uncertain. To bridge this gap, we conduct a comprehensive analysis of multilingual LLM-as-a-Judge. Specifically, we evaluate five models from different model families across five diverse tasks involving 25 languages. Our findings reveal that LLMs struggle to achieve consistent judgment results across languages, with an average Fleiss' Kappa of approximately 0.3, and some models performing even worse. To investigate the cause of inconsistency, we analyze various influencing factors. We observe that consistency varies significantly across languages, with particularly poor performance in low-resource languages. Additionally, we find that neither training on multilingual data nor increasing model scale directly improves judgment consistency. These findings suggest that LLMs are not yet reliable for evaluating multilingual predictions. We finally propose an ensemble strategy which improves the consistency of the multilingual judge in real-world applications.</li>
</ul>

<h3>Title: Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhenghao Li, Shengbo Wang, Nian Si</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12202">https://arxiv.org/abs/2505.12202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12202">https://arxiv.org/pdf/2505.12202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12202]] Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning(https://arxiv.org/abs/2505.12202)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Distributionally robust reinforcement learning (DR-RL) has recently gained significant attention as a principled approach that addresses discrepancies between training and testing environments. To balance robustness, conservatism, and computational traceability, the literature has introduced DR-RL models with SA-rectangular and S-rectangular adversaries. While most existing statistical analyses focus on SA-rectangular models, owing to their algorithmic simplicity and the optimality of deterministic policies, S-rectangular models more accurately capture distributional discrepancies in many real-world applications and often yield more effective robust randomized policies. In this paper, we study the empirical value iteration algorithm for divergence-based S-rectangular DR-RL and establish near-optimal sample complexity bounds of $\widetilde{O}(|\mathcal{S}||\mathcal{A}|(1-\gamma)^{-4}\varepsilon^{-2})$, where $\varepsilon$ is the target accuracy, $|\mathcal{S}|$ and $|\mathcal{A}|$ denote the cardinalities of the state and action spaces, and $\gamma$ is the discount factor. To the best of our knowledge, these are the first sample complexity results for divergence-based S-rectangular models that achieve optimal dependence on $|\mathcal{S}|$, $|\mathcal{A}|$, and $\varepsilon$ simultaneously. We further validate this theoretical dependence through numerical experiments on a robust inventory control problem and a theoretical worst-case example, demonstrating the fast learning performance of our proposed algorithm.</li>
</ul>

<h3>Title: Road Segmentation for ADAS/AD Applications</h3>
<ul>
<li><strong>Authors: </strong>Mathanesh Vellingiri Ramasamy, Dimas Rizky Kurniasalim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12206">https://arxiv.org/abs/2505.12206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12206">https://arxiv.org/pdf/2505.12206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12206]] Road Segmentation for ADAS/AD Applications(https://arxiv.org/abs/2505.12206)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate road segmentation is essential for autonomous driving and ADAS, enabling effective navigation in complex environments. This study examines how model architecture and dataset choice affect segmentation by training a modified VGG-16 on the Comma10k dataset and a modified U-Net on the KITTI Road dataset. Both models achieved high accuracy, with cross-dataset testing showing VGG-16 outperforming U-Net despite U-Net being trained for more epochs. We analyze model performance using metrics such as F1-score, mean intersection over union, and precision, discussing how architecture and dataset impact results.</li>
</ul>

<h3>Title: Nonmalleable Progress Leakage</h3>
<ul>
<li><strong>Authors: </strong>Ethan Cecchetti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12210">https://arxiv.org/abs/2505.12210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12210">https://arxiv.org/pdf/2505.12210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12210]] Nonmalleable Progress Leakage(https://arxiv.org/abs/2505.12210)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Information-flow control systems often enforce progress-insensitive noninterference, as it is simple to understand and enforce. Unfortunately, real programs need to declassify results and endorse inputs, which noninterference disallows, while preventing attackers from controlling leakage, including through progress channels, which progress-insensitivity ignores. This work combines ideas for progress-sensitive security with secure downgrading (declassification and endorsement) to identify a notion of securely downgrading progress information. We use hyperproperties to distill the separation between progress-sensitive and progress-insensitive noninterference and combine it with nonmalleable information flow, an existing (progress-insensitive) definition of secure downgrading, to define nonmalleable progress leakage (NMPL). We present the first information-flow type system to allow some progress leakage while enforcing NMPL, and we show how to infer the location of secure progress downgrades. All theorems are verified in Rocq.</li>
</ul>

<h3>Title: Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Shaobo Wang, Ziming Wang, Xiangqi Jin, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12212">https://arxiv.org/abs/2505.12212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12212">https://arxiv.org/pdf/2505.12212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12212]] Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning(https://arxiv.org/abs/2505.12212)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model's predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup.</li>
</ul>

<h3>Title: GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiwei Tang, Zhicheng Zhang, Shunlong Wu, Jingheng Ye, Lichen Bai, Zitai Wang, Tingwei Lu, Jiaqi Chen, Lin Hai, Hai-Tao Zheng, Hong-Gee Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12215">https://arxiv.org/abs/2505.12215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12215">https://arxiv.org/pdf/2505.12215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12215]] GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment(https://arxiv.org/abs/2505.12215)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive performance in a variety of natural language processing (NLP) tasks. However, when applied to long-context scenarios, they face two challenges, i.e., low computational efficiency and much redundant information. This paper introduces GMSA, a context compression framework based on the encoder-decoder architecture, which addresses these challenges by reducing input sequence length and redundant information. Structurally, GMSA has two key components: Group Merging and Layer Semantic Alignment (LSA). Group merging is used to effectively and efficiently extract summary vectors from the original context. Layer semantic alignment, on the other hand, aligns the high-level summary vectors with the low-level primary input semantics, thus bridging the semantic gap between different layers. In the training process, GMSA first learns soft tokens that contain complete semantics through autoencoder training. To furtherly adapt GMSA to downstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract knowledge from the soft tokens for downstream tasks. We train GMSA by randomly sampling the compression rate for each sample in the dataset. Under this condition, GMSA not only significantly outperforms the traditional compression paradigm in context restoration but also achieves stable and significantly faster convergence with only a few encoder layers. In downstream question-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in end-to-end inference while outperforming both the original input prompts and various state-of-the-art (SOTA) methods by a large margin.</li>
</ul>

<h3>Title: One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rongguang Ye, Ming Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12216">https://arxiv.org/abs/2505.12216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12216">https://arxiv.org/pdf/2505.12216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12216]] One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models(https://arxiv.org/abs/2505.12216)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing pruning methods for large language models (LLMs) focus on achieving high compression rates while maintaining model performance. Although these methods have demonstrated satisfactory performance in handling a single user's compression request, their processing time increases linearly with the number of requests, making them inefficient for real-world scenarios with multiple simultaneous requests. To address this limitation, we propose a Univeral Model for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that learns to map arbitrary requests to their optimal pruning strategy. The challenge in training StratNet lies in the high computational cost of evaluating pruning strategies and the non-differentiable nature of the pruning process, which hinders gradient backpropagation for StratNet updates. To overcome these challenges, we leverage a Gaussian process to approximate the evaluation process. Since the gradient of the Gaussian process is computable, we can use it to approximate the gradient of the non-differentiable pruning process, thereby enabling StratNet updates. Experimental results show that UniCuCo is 28 times faster than baselines in processing 64 requests, while maintaining comparable accuracy to baselines.</li>
</ul>

<h3>Title: Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aryan Das, Tanishq Rachamalla, Pravendra Singh, Koushik Biswas, Vinay Kumar Verma, Swalpa Kumar Roy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12217">https://arxiv.org/abs/2505.12217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12217">https://arxiv.org/pdf/2505.12217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12217]] Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models(https://arxiv.org/abs/2505.12217)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce HyperCap, the first large-scale hyperspectral captioning dataset designed to enhance model performance and effectiveness in remote sensing applications. Unlike traditional hyperspectral imaging (HSI) datasets that focus solely on classification tasks, HyperCap integrates spectral data with pixel-wise textual annotations, enabling deeper semantic understanding of hyperspectral imagery. This dataset enhances model performance in tasks like classification and feature extraction, providing a valuable resource for advanced remote sensing applications. HyperCap is constructed from four benchmark datasets and annotated through a hybrid approach combining automated and manual methods to ensure accuracy and consistency. Empirical evaluations using state-of-the-art encoders and diverse fusion techniques demonstrate significant improvements in classification performance. These results underscore the potential of vision-language learning in HSI and position HyperCap as a foundational dataset for future research in the field.</li>
</ul>

<h3>Title: Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers</h3>
<ul>
<li><strong>Authors: </strong>Tong Bao, Yi Zhao, Jin Mao, Chengzhi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12218">https://arxiv.org/abs/2505.12218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12218">https://arxiv.org/pdf/2505.12218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12218]] Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers(https://arxiv.org/abs/2505.12218)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), such as ChatGPT, have prompted academic concerns about their impact on academic writing. Existing studies have primarily examined LLM usage in academic writing through quantitative approaches, such as word frequency statistics and probability-based analyses. However, few have systematically examined the potential impact of LLMs on the linguistic characteristics of academic writing. To address this gap, we conducted a large-scale analysis across 823,798 abstracts published in last decade from arXiv dataset. Through the linguistic analysis of features such as the frequency of LLM-preferred words, lexical complexity, syntactic complexity, cohesion, readability and sentiment, the results indicate a significant increase in the proportion of LLM-preferred words in abstracts, revealing the widespread influence of LLMs on academic writing. Additionally, we observed an increase in lexical complexity and sentiment in the abstracts, but a decrease in syntactic complexity, suggesting that LLMs introduce more new vocabulary and simplify sentence structure. However, the significant decrease in cohesion and readability indicates that abstracts have fewer connecting words and are becoming more difficult to read. Moreover, our analysis reveals that scholars with weaker English proficiency were more likely to use the LLMs for academic writing, and focused on improving the overall logic and fluency of the abstracts. Finally, at discipline level, we found that scholars in Computer Science showed more pronounced changes in writing style, while the changes in Mathematics were minimal.</li>
</ul>

<h3>Title: Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling</h3>
<ul>
<li><strong>Authors: </strong>Jizhou Guo, Zhaomin Wu, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12225">https://arxiv.org/abs/2505.12225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12225">https://arxiv.org/pdf/2505.12225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12225]] Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling(https://arxiv.org/abs/2505.12225)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>High-quality reward models are crucial for unlocking the reasoning potential of large language models (LLMs), with best-of-N voting demonstrating significant performance gains. However, current reward models, which typically operate on the textual output of LLMs, are computationally expensive and parameter-heavy, limiting their real-world applications. We introduce the Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly parameter-efficient approach that leverages the rich information embedded in LLM hidden states to address these issues. ELHSR systematically outperform baselines with less than 0.005% of the parameters of baselines, requiring only a few samples for training. ELHSR also achieves orders-of-magnitude efficiency improvement with significantly less time and fewer FLOPs per sample than baseline reward models. Moreover, ELHSR exhibits robust performance even when trained only on logits, extending its applicability to some closed-source LLMs. In addition, ELHSR can also be combined with traditional reward models to achieve additional performance gains.</li>
</ul>

<h3>Title: From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI</h3>
<ul>
<li><strong>Authors: </strong>Karthik Gopinath, Annabel Sorby-Adams, Jonathan W. Ramirez, Dina Zemlyanker, Jennifer Guo, David Hunt, Christine L. Mac Donald, C. Dirk Keene, Timothy Coalson, Matthew F. Glasser, David Van Essen, Matthew S. Rosen, Oula Puonti, W. Taylor Kimberly, Juan Eugenio Iglesias</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12228">https://arxiv.org/abs/2505.12228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12228">https://arxiv.org/pdf/2505.12228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12228]] From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI(https://arxiv.org/abs/2505.12228)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Three-dimensional reconstruction of cortical surfaces from MRI for morphometric analysis is fundamental for understanding brain structure. While high-field MRI (HF-MRI) is standard in research and clinical settings, its limited availability hinders widespread use. Low-field MRI (LF-MRI), particularly portable systems, offers a cost-effective and accessible alternative. However, existing cortical surface analysis tools are optimized for high-resolution HF-MRI and struggle with the lower signal-to-noise ratio and resolution of LF-MRI. In this work, we present a machine learning method for 3D reconstruction and analysis of portable LF-MRI across a range of contrasts and resolutions. Our method works "out of the box" without retraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed distance functions of cortical surfaces, followed by geometric processing to ensure topological accuracy. We evaluate our method using paired HF/LF-MRI scans of the same subjects, showing that LF-MRI surface reconstruction accuracy depends on acquisition parameters, including contrast type (T1 vs T2), orientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted scan acquired in under 4 minutes, yields strong agreement with HF-derived surfaces: surface area correlates at r=0.96, cortical parcellations reach Dice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains more challenging with correlations up to r=0.70, reflecting the difficulty of sub-mm precision with 3mm voxels. We further validate our method on challenging postmortem LF-MRI, demonstrating its robustness. Our method represents a step toward enabling cortical surface analysis on portable LF-MRI. Code is available at this https URL</li>
</ul>

<h3>Title: NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation</h3>
<ul>
<li><strong>Authors: </strong>Jia Li, Nan Gao, Huaibo Huang, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12235">https://arxiv.org/abs/2505.12235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12235">https://arxiv.org/pdf/2505.12235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12235]] NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation(https://arxiv.org/abs/2505.12235)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The diffusion model has provided a strong tool for implementing text-to-image (T2I) and image-to-image (I2I) generation. Recently, topology and texture control are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and DSG. These methods explicitly consider high-fidelity controllable editing based on external signals or diffusion feature manipulations. As for diversity, they directly choose different noise latents. However, the diffused noise is capable of implicitly representing the topological and textural manifold of the corresponding image. Moreover, it's an effective workbench to conduct the trade-off between content preservation and controllable variations. Previous T2I and I2I diffusion works do not explore the information within the compressed contextual latent. In this paper, we first propose a plug-and-play noise finetune NOFT module employed by Stable Diffusion to generate highly correlated and diverse images. We fine-tune seed noise or inverse noise through an optimal-transported (OT) information bottleneck (IB) with around only 14K trainable parameters and 10 minutes of training. Our test-time NOFT is good at producing high-fidelity image variations considering topology and texture alignments. Comprehensive experiments demonstrate that NOFT is a powerful general reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with text or image guidance.</li>
</ul>

<h3>Title: Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Quanjiang Guo, Jinchuan Zhang, Sijie Wang, Ling Tian, Zhao Kang, Bin Yan, Weidong Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12236">https://arxiv.org/abs/2505.12236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12236">https://arxiv.org/pdf/2505.12236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12236]] Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training(https://arxiv.org/abs/2505.12236)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Few-Shot Relation Extraction (FSRE) remains a challenging task due to the scarcity of annotated data and the limited generalization capabilities of existing models. Although large language models (LLMs) have demonstrated potential in FSRE through in-context learning (ICL), their general-purpose training objectives often result in suboptimal performance for task-specific relation extraction. To overcome these challenges, we propose TKRE (Two-Stage Knowledge-Guided Pre-training for Relation Extraction), a novel framework that synergistically integrates LLMs with traditional relation extraction models, bridging generative and discriminative learning paradigms. TKRE introduces two key innovations: (1) leveraging LLMs to generate explanation-driven knowledge and schema-constrained synthetic data, addressing the issue of data scarcity; and (2) a two-stage pre-training strategy combining Masked Span Language Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational reasoning and generalization. Together, these components enable TKRE to effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in FSRE and underscoring its potential for broader application in low-resource scenarios. \footnote{The code and data are released on this https URL.</li>
</ul>

<h3>Title: From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations</h3>
<ul>
<li><strong>Authors: </strong>Yuzhi Li, Haojun Xu, Fang Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12237">https://arxiv.org/abs/2505.12237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12237">https://arxiv.org/pdf/2505.12237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12237]] From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations(https://arxiv.org/abs/2505.12237)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable reasoning and generalization capabilities in video understanding; however, their application in video editing remains largely underexplored. This paper presents the first systematic study of LLMs in the context of video editing. To bridge the gap between visual information and language-based reasoning, we introduce L-Storyboard, an intermediate representation that transforms discrete video shots into structured language descriptions suitable for LLM processing. We categorize video editing tasks into Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot Attributes Classification, Next Shot Selection, and Shot Sequence Ordering. To address the inherent instability of divergent task outputs, we propose the StoryFlow strategy, which converts the divergent multi-path reasoning process into a convergent selection mechanism, effectively enhancing task accuracy and logical coherence. Experimental results demonstrate that L-Storyboard facilitates a more robust mapping between visual information and language descriptions, significantly improving the interpretability and privacy protection of video editing tasks. Furthermore, StoryFlow enhances the logical consistency and output stability in Shot Sequence Ordering, underscoring the substantial potential of LLMs in intelligent video editing.</li>
</ul>

<h3>Title: PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sriram Selvam, Anneswa Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12238">https://arxiv.org/abs/2505.12238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12238">https://arxiv.org/pdf/2505.12238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12238]] PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs(https://arxiv.org/abs/2505.12238)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The memorization of sensitive and personally identifiable information (PII) by large language models (LLMs) poses growing privacy risks as models scale and are increasingly deployed in real-world applications. Existing efforts to study sensitive and PII data memorization and develop mitigation strategies are hampered by the absence of comprehensive, realistic, and ethically sourced datasets reflecting the diversity of sensitive information found on the web. We introduce PANORAMA - Profile-based Assemblage for Naturalistic Online Representation and Attribute Memorization Analysis, a large-scale synthetic corpus of 384,789 samples derived from 9,674 synthetic profiles designed to closely emulate the distribution, variety, and context of PII and sensitive data as it naturally occurs in online environments. Our data generation pipeline begins with the construction of internally consistent, multi-attribute human profiles using constrained selection to reflect real-world demographics such as education, health attributes, financial status, etc. Using a combination of zero-shot prompting and OpenAI o3-mini, we generate diverse content types - including wiki-style articles, social media posts, forum discussions, online reviews, comments, and marketplace listings - each embedding realistic, contextually appropriate PII and other sensitive information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and measure PII memorization rates - revealing not only consistent increases with repetition but also variation across content types, highlighting PANORAMA's ability to model how memorization risks differ by context. Our dataset and code are publicly available, providing a much-needed resource for privacy risk assessment, model auditing, and the development of privacy-preserving LLMs.</li>
</ul>

<h3>Title: ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation</h3>
<ul>
<li><strong>Authors: </strong>Jianheng Tang, Huiping Zhuang, Di Fang, Jiaxu Li, Feijiang Han, Yajiang Huang, Kejia Fan, Leye Wang, Zhanxing Zhu, Shanghang Zhang, Houbing Herbert Song, Yunhuai Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12239">https://arxiv.org/abs/2505.12239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12239">https://arxiv.org/pdf/2505.12239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12239]] ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation(https://arxiv.org/abs/2505.12239)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The development of artificial intelligence demands that models incrementally update knowledge by Continual Learning (CL) to adapt to open-world environments. To meet privacy and security requirements, Continual Unlearning (CU) emerges as an important problem, aiming to sequentially forget particular knowledge acquired during the CL phase. However, existing unlearning methods primarily focus on single-shot joint forgetting and face significant limitations when applied to CU. First, most existing methods require access to the retained dataset for re-training or fine-tuning, violating the inherent constraint in CL that historical data cannot be revisited. Second, these methods often suffer from a poor trade-off between system efficiency and model fidelity, making them vulnerable to being overwhelmed or degraded by adversaries through deliberately frequent requests. In this paper, we identify that the limitations of existing unlearning methods stem fundamentally from their reliance on gradient-based updates. To bridge the research gap at its root, we propose a novel gradient-free method for CU, named Analytic Continual Unlearning (ACU), for efficient and exact forgetting with historical data privacy preservation. In response to each unlearning request, our ACU recursively derives an analytical (i.e., closed-form) solution in an interpretable manner using the least squares method. Theoretical and experimental evaluations validate the superiority of our ACU on unlearning effectiveness, model fidelity, and system efficiency.</li>
</ul>

<h3>Title: AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Jianheng Tang, Huiping Zhuang, Jingyu He, Run He, Jingchao Wang, Kejia Fan, Anfeng Liu, Tian Wang, Leye Wang, Zhanxing Zhu, Shanghang Zhang, Houbing Herbert Song, Yunhuai Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12245">https://arxiv.org/abs/2505.12245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12245">https://arxiv.org/pdf/2505.12245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12245]] AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data(https://arxiv.org/abs/2505.12245)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Continual Learning (FCL) enables distributed clients to collaboratively train a global model from online task streams in dynamic real-world scenarios. However, existing FCL methods face challenges of both spatial data heterogeneity among distributed clients and temporal data heterogeneity across online tasks. Such data heterogeneity significantly degrades the model performance with severe spatial-temporal catastrophic forgetting of local and past knowledge. In this paper, we identify that the root cause of this issue lies in the inherent vulnerability and sensitivity of gradients to non-IID data. To fundamentally address this issue, we propose a gradient-free method, named Analytic Federated Continual Learning (AFCL), by deriving analytical (i.e., closed-form) solutions from frozen extracted features. In local training, our AFCL enables single-epoch learning with only a lightweight forward-propagation process for each client. In global aggregation, the server can recursively and efficiently update the global model with single-round aggregation. Theoretical analyses validate that our AFCL achieves spatio-temporal invariance of non-IID data. This ideal property implies that, regardless of how heterogeneous the data are distributed across local clients and online tasks, the aggregated model of our AFCL remains invariant and identical to that of centralized joint learning. Extensive experiments show the consistent superiority of our AFCL over state-of-the-art baselines across various benchmark datasets and settings.</li>
</ul>

<h3>Title: Not All Documents Are What You Need for Extracting Instruction Tuning Data</h3>
<ul>
<li><strong>Authors: </strong>Chi Zhang, Huaping Zhong, Hongtao Li, Chengliang Chai, Jiawei Hong, Yuhao Deng, Jiacheng Wang, Tian Tan, Yizhou Yan, Jiantao Qiu, Ye Yuan, Guoren Wang, Conghui He, Lei Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12250">https://arxiv.org/abs/2505.12250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12250">https://arxiv.org/pdf/2505.12250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12250]] Not All Documents Are What You Need for Extracting Instruction Tuning Data(https://arxiv.org/abs/2505.12250)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Instruction tuning improves the performance of large language models (LLMs), but it heavily relies on high-quality training data. Recently, LLMs have been used to synthesize instruction data using seed question-answer (QA) pairs. However, these synthesized instructions often lack diversity and tend to be similar to the input seeds, limiting their applicability in real-world scenarios. To address this, we propose extracting instruction tuning data from web corpora that contain rich and diverse knowledge. A naive solution is to retrieve domain-specific documents and extract all QA pairs from them, but this faces two key challenges: (1) extracting all QA pairs using LLMs is prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to the downstream tasks, potentially degrading model performance. To tackle these issues, we introduce EQUAL, an effective and scalable data extraction framework that iteratively alternates between document selection and high-quality QA pair extraction to enhance instruction tuning. EQUAL first clusters the document corpus based on embeddings derived from contrastive learning, then uses a multi-armed bandit strategy to efficiently identify clusters that are likely to contain valuable QA pairs. This iterative approach significantly reduces computational cost while boosting model performance. Experiments on AutoMathText and StackOverflow across four downstream tasks show that EQUAL reduces computational costs by 5-10x and improves accuracy by 2.5 percent on LLaMA-3.1-8B and Mistral-7B</li>
</ul>

<h3>Title: SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Xiang, Han Zhang, Yu Cheng, Xiongwen Quan, Wanwan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12251">https://arxiv.org/abs/2505.12251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12251">https://arxiv.org/pdf/2505.12251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12251]] SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis(https://arxiv.org/abs/2505.12251)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Multimodal medical image fusion plays a crucial role in medical diagnosis by integrating complementary information from different modalities to enhance image readability and clinical applicability. However, existing methods mainly follow computer vision standards for feature extraction and fusion strategy formulation, overlooking the rich semantic information inherent in medical images. To address this limitation, we propose a novel semantic-guided medical image fusion approach that, for the first time, incorporates medical prior knowledge into the fusion process. Specifically, we construct a publicly available multimodal medical image-text dataset, upon which text descriptions generated by BiomedGPT are encoded and semantically aligned with image features in a high-dimensional space via a semantic interaction alignment module. During this process, a cross attention based linear transformation automatically maps the relationship between textual and visual features to facilitate comprehensive learning. The aligned features are then embedded into a text-injection module for further feature-level fusion. Unlike traditional methods, we further generate diagnostic reports from the fused images to assess the preservation of medical information. Additionally, we design a medical semantic loss function to enhance the retention of textual cues from the source images. Experimental results on test datasets demonstrate that the proposed method achieves superior performance in both qualitative and quantitative evaluations while preserving more critical medical information.</li>
</ul>

<h3>Title: TPM2.0-Supported Runtime Customizable TEE on FPGA-SoC with User-Controllable vTPM</h3>
<ul>
<li><strong>Authors: </strong>Jingkai Mao, Xiaolin Chang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12256">https://arxiv.org/abs/2505.12256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12256">https://arxiv.org/pdf/2505.12256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12256]] TPM2.0-Supported Runtime Customizable TEE on FPGA-SoC with User-Controllable vTPM(https://arxiv.org/abs/2505.12256)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>Constructing a Trusted Execution Environment (TEE) on Field Programmable Gate Array System on Chip (FPGA-SoC) in Cloud can effectively protect users' private intel-lectual Property (IP) cores. In order to facilitate the wide-spread deployment of FPGA-SoC TEE, this paper proposes an approach for constructing a TPM 2.0-compatible runtime customizable TEE on FPGA-SoC. This approach leverages a user-controllable virtual Trusted Platform Module (vTPM) that integrates sensitive operations specific to FPGA-SoC TEE. It provides TPM 2.0 support for a customizable FPGA-SoC TEE to dynamically measure, deploy, and invoke IP during runtime. Our main contributions include: (i) Propose an FPGA-vTPM architecture that enables the TPM 2.0 specification support for FPGA-SoC TEE; (ii) Explore the utilization of FPGA-vTPM to dynamically measure, deploy, and invoke users' IPs on FPGA-SoC TEE; (iii) Extend the TPM command set to accommodate the sensitive operations of FPGA-SoC TEE, enabling users to perform sensitive tasks in a secure and verifiable manner according to the TPM 2.0 specification. We implement a prototype of TRCTEE on the Xilinx Zynq UltraScale+ MPSoC platform and conducted security analysis and performance evaluations to prove the practicality and enhanced security features of this approach.</li>
</ul>

<h3>Title: Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Zhou, Xutian Chen, Yixin Cao, Yuchen Ni, Yu He, Siyu Tian, Xiang Liu, Jian Zhang, Chuanjun Ji, Guangnan Ye, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12259">https://arxiv.org/abs/2505.12259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12259">https://arxiv.org/pdf/2505.12259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12259]] Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches(https://arxiv.org/abs/2505.12259)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in large language models (LLMs) has outpaced the development of effective evaluation methods. Traditional benchmarks rely on task-specific metrics and static datasets, which often suffer from fairness issues, limited scalability, and contamination risks. In this paper, we introduce Teach2Eval, an indirect evaluation framework inspired by the Feynman Technique. Instead of directly testing LLMs on predefined tasks, our method evaluates a model's multiple abilities to teach weaker student models to perform tasks effectively. By converting open-ended tasks into standardized multiple-choice questions (MCQs) through teacher-generated feedback, Teach2Eval enables scalable, automated, and multi-dimensional assessment. Our approach not only avoids data leakage and memorization but also captures a broad range of cognitive abilities that are orthogonal to current benchmarks. Experimental results across 26 leading LLMs show strong alignment with existing human and model-based dynamic rankings, while offering additional interpretability for training guidance.</li>
</ul>

<h3>Title: Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation</h3>
<ul>
<li><strong>Authors: </strong>Chengwei Qin, Wenxuan Zhou, Karthik Abinav Sankararaman, Nanshu Wang, Tengyu Xu, Alexander Radovic, Eryk Helenowski, Arya Talebzadeh, Aditya Tayade, Sinong Wang, Shafiq Joty, Han Fang, Hao Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12265">https://arxiv.org/abs/2505.12265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12265">https://arxiv.org/pdf/2505.12265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12265]] Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation(https://arxiv.org/abs/2505.12265)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucination, the generation of factually incorrect information, remains a significant challenge for large language models (LLMs), especially in open-domain long-form generation. Existing approaches for detecting hallucination in long-form tasks either focus on limited domains or rely heavily on external fact-checking tools, which may not always be available. In this work, we systematically investigate reference-free hallucination detection in open-domain long-form responses. Our findings reveal that internal states (e.g., model's output probability and entropy) alone are insufficient for reliably (i.e., better than random guessing) distinguishing between factual and hallucinated content. To enhance detection, we explore various existing approaches, including prompting-based methods, probing, and fine-tuning, with fine-tuning proving the most effective. To further improve the accuracy, we introduce a new paradigm, named RATE-FT, that augments fine-tuning with an auxiliary task for the model to jointly learn with the main task of hallucination detection. With extensive experiments and analysis using a variety of model families & datasets, we demonstrate the effectiveness and generalizability of our method, e.g., +3% over general fine-tuning methods on LongFact.</li>
</ul>

<h3>Title: PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement</h3>
<ul>
<li><strong>Authors: </strong>ZhanFeng Feng, Long Peng, Xin Di, Yong Guo, Wenbo Li, Yulun Zhang, Renjing Pei, Yang Wang, Yang Cao, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12266">https://arxiv.org/abs/2505.12266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12266">https://arxiv.org/pdf/2505.12266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12266]] PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement(https://arxiv.org/abs/2505.12266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Multi-frame video enhancement tasks aim to improve the spatial and temporal resolution and quality of video sequences by leveraging temporal information from multiple frames, which are widely used in streaming video processing, surveillance, and generation. Although numerous Transformer-based enhancement methods have achieved impressive performance, their computational and memory demands hinder deployment on edge devices. Quantization offers a practical solution by reducing the bit-width of weights and activations to improve efficiency. However, directly applying existing quantization methods to video enhancement tasks often leads to significant performance degradation and loss of fine details. This stems from two limitations: (a) inability to allocate varying representational capacity across frames, which results in suboptimal dynamic range adaptation; (b) over-reliance on full-precision teachers, which limits the learning of low-bit student models. To tackle these challenges, we propose a novel quantization method for video enhancement: Progressive Multi-Frame Quantization for Video Enhancement (PMQ-VE). This framework features a coarse-to-fine two-stage process: Backtracking-based Multi-Frame Quantization (BMFQ) and Progressive Multi-Teacher Distillation (PMTD). BMFQ utilizes a percentile-based initialization and iterative search with pruning and backtracking for robust clipping bounds. PMTD employs a progressive distillation strategy with both full-precision and multiple high-bit (INT) teachers to enhance low-bit models' capacity and quality. Extensive experiments demonstrate that our method outperforms existing approaches, achieving state-of-the-art performance across multiple tasks and this http URL code will be made publicly available at: this https URL.</li>
</ul>

<h3>Title: $K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks</h3>
<ul>
<li><strong>Authors: </strong>Pratim Chowdhary</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12268">https://arxiv.org/abs/2505.12268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12268">https://arxiv.org/pdf/2505.12268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12268]] $K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks(https://arxiv.org/abs/2505.12268)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding which neural components drive specific capabilities in mid-sized language models ($\leq$10B parameters) remains a key challenge. We introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC), a methodology to identify minimal sets of attention heads crucial for classification tasks as well as Search-K-MSHC, an efficient algorithm for discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B, we analyze three syntactic task families: grammar acceptability, arithmetic verification, and arithmetic word problems. Our findings reveal distinct task-specific head circuits, with grammar tasks predominantly utilizing early layers, word problems showing pronounced activity in both shallow and deep regions, and arithmetic verification demonstrating a more distributed pattern across the network. We discover non-linear circuit overlap patterns, where different task pairs share computational components at varying levels of importance. While grammar and arithmetic share many "weak" heads, arithmetic and word problems share more consistently critical "strong" heads. Importantly, we find that each task maintains dedicated "super-heads" with minimal cross-task overlap, suggesting that syntactic and numerical competencies emerge from specialized yet partially reusable head circuits.</li>
</ul>

<h3>Title: LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Md. Atiqur Rahman, Sabrina Islam, Mushfiqul Haque Omi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12273">https://arxiv.org/abs/2505.12273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12273">https://arxiv.org/pdf/2505.12273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12273]] LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark(https://arxiv.org/abs/2505.12273)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating machine translation (MT) for low-resource languages poses a persistent challenge, primarily due to the limited availability of high quality reference translations. This issue is further exacerbated in languages with multiple dialects, where linguistic diversity and data scarcity hinder robust evaluation. Large Language Models (LLMs) present a promising solution through reference-free evaluation techniques; however, their effectiveness diminishes in the absence of dialect-specific context and tailored guidance. In this work, we propose a comprehensive framework that enhances LLM-based MT evaluation using a dialect guided approach. We extend the ONUBAD dataset by incorporating Sylheti-English sentence pairs, corresponding machine translations, and Direct Assessment (DA) scores annotated by native speakers. To address the vocabulary gap, we augment the tokenizer vocabulary with dialect-specific terms. We further introduce a regression head to enable scalar score prediction and design a dialect-guided (DG) prompting strategy. Our evaluation across multiple LLMs shows that the proposed pipeline consistently outperforms existing methods, achieving the highest gain of +0.1083 in Spearman correlation, along with improvements across other evaluation settings. The dataset and the code are available at this https URL.</li>
</ul>

<h3>Title: Context-Aware Autoregressive Models for Multi-Conditional Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yixiao Chen, Zhiyuan Ma, Guoli Jia, Che Jiang, Jianjun Li, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12274">https://arxiv.org/abs/2505.12274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12274">https://arxiv.org/pdf/2505.12274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12274]] Context-Aware Autoregressive Models for Multi-Conditional Image Generation(https://arxiv.org/abs/2505.12274)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Autoregressive transformers have recently shown impressive image generation quality and efficiency on par with state-of-the-art diffusion models. Unlike diffusion architectures, autoregressive models can naturally incorporate arbitrary modalities into a single, unified token sequence--offering a concise solution for multi-conditional image generation tasks. In this work, we propose $\textbf{ContextAR}$, a flexible and effective framework for multi-conditional image generation. ContextAR embeds diverse conditions (e.g., canny edges, depth maps, poses) directly into the token sequence, preserving modality-specific semantics. To maintain spatial alignment while enhancing discrimination among different condition types, we introduce hybrid positional encodings that fuse Rotary Position Embedding with Learnable Positional Embedding. We design Conditional Context-aware Attention to reduces computational complexity while preserving effective intra-condition perception. Without any fine-tuning, ContextAR supports arbitrary combinations of conditions during inference time. Experimental results demonstrate the powerful controllability and versatility of our approach, and show that the competitive perpormance than diffusion-based multi-conditional control approaches the existing autoregressive baseline across diverse multi-condition driven scenarios. Project page: $\href{this https URL}{this https URL.}$</li>
</ul>

<h3>Title: Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction</h3>
<ul>
<li><strong>Authors: </strong>Sijie Zhao, Feng Liu, Xueliang Zhang, Hao Chen, Pengfeng Xiao, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12280">https://arxiv.org/abs/2505.12280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12280">https://arxiv.org/pdf/2505.12280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12280]] Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction(https://arxiv.org/abs/2505.12280)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>The proliferation of diverse remote sensing data has spurred advancements in dense prediction tasks, yet significant challenges remain in handling data heterogeneity. Remote sensing imagery exhibits substantial variability across temporal, spectral, and spatial (TSS) dimensions, complicating unified data processing. Current deep learning models for dense prediction tasks, such as semantic segmentation and change detection, are typically tailored to specific input-output configurations. Consequently, variations in data dimensionality or task requirements often lead to significant performance degradation or model incompatibility, necessitating costly retraining or fine-tuning efforts for different application scenarios. This paper introduces the Temporal-Spectral-Spatial Unified Network (TSSUN), a novel architecture designed for unified representation and modeling of remote sensing data across diverse TSS characteristics and task types. TSSUN employs a Temporal-Spectral-Spatial Unified Strategy that leverages meta-information to decouple and standardize input representations from varied temporal, spectral, and spatial configurations, and similarly unifies output structures for different dense prediction tasks and class numbers. Furthermore, a Local-Global Window Attention mechanism is proposed to efficiently capture both local contextual details and global dependencies, enhancing the model's adaptability and feature extraction capabilities. Extensive experiments on multiple datasets demonstrate that a single TSSUN model effectively adapts to heterogeneous inputs and unifies various dense prediction tasks. The proposed approach consistently achieves or surpasses state-of-the-art performance, highlighting its robustness and generalizability for complex remote sensing applications without requiring task-specific modifications.</li>
</ul>

<h3>Title: The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Linghan Huang, Haolin Jin, Zhaoge Bi, Pengyue Yang, Peizhou Zhao, Taozhao Chen, Xiongfei Wu, Lei Ma, Huaming Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12287">https://arxiv.org/abs/2505.12287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12287">https://arxiv.org/pdf/2505.12287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12287]] The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models(https://arxiv.org/abs/2505.12287)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have seen widespread applications across various domains, yet remain vulnerable to adversarial prompt injections. While most existing research on jailbreak attacks and hallucination phenomena has focused primarily on open-source models, we investigate the frontier of closed-source LLMs under multilingual attack scenarios. We present a first-of-its-kind integrated adversarial framework that leverages diverse attack techniques to systematically evaluate frontier proprietary solutions, including GPT-4o, DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories of security contents in both English and Chinese, generating 38,400 responses across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as the quantitative metric to assess performance from three dimensions: prompt design, model architecture, and language environment. Our findings suggest that Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense. Notably, prompts in Chinese consistently yield higher ASRs than their English counterparts, and our novel Two-Sides attack technique proves to be the most effective across all models. This work highlights a dire need for language-aware alignment and robust cross-lingual defenses in LLMs, and we hope it will inspire researchers, developers, and policymakers toward more robust and inclusive AI systems.</li>
</ul>

<h3>Title: PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Haiyu Deng, Yanna Jiang, Guangsheng Yu, Qin Wang, Xu Wang, Baihe Ma, Wei Ni, Ren Ping Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12296">https://arxiv.org/abs/2505.12296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12296">https://arxiv.org/pdf/2505.12296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12296]] PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking(https://arxiv.org/abs/2505.12296)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, watermark</a></li>
<li><strong>Abstract: </strong>Machine learning models are increasingly shared and outsourced, raising requirements of verifying training effort (Proof-of-Learning, PoL) to ensure claimed performance and establishing ownership (Proof-of-Ownership, PoO) for transactions. When models are trained by untrusted parties, PoL and PoO must be enforced together to enable protection, attribution, and compensation. However, existing studies typically address them separately, which not only weakens protection against forgery and privacy breaches but also leads to high verification overhead. We propose PoLO, a unified framework that simultaneously achieves PoL and PoO using chained watermarks. PoLO splits the training process into fine-grained training shards and embeds a dedicated watermark in each shard. Each watermark is generated using the hash of the preceding shard, certifying the training process of the preceding shard. The chained structure makes it computationally difficult to forge any individual part of the whole training process. The complete set of watermarks serves as the PoL, while the final watermark provides the PoO. PoLO offers more efficient and privacy-preserving verification compared to the vanilla PoL solutions that rely on gradient-based trajectory tracing and inadvertently expose training data during verification, while maintaining the same level of ownership assurance of watermark-based PoO schemes. Our evaluation shows that PoLO achieves 99% watermark detection accuracy for ownership verification, while preserving data privacy and cutting verification costs to just 1.5-10% of traditional methods. Forging PoLO demands 1.1-4x more resources than honest proof generation, with the original proof retaining over 90% detection accuracy even after attacks.</li>
</ul>

<h3>Title: HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12300">https://arxiv.org/abs/2505.12300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12300">https://arxiv.org/pdf/2505.12300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12300]] HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models(https://arxiv.org/abs/2505.12300)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) on a mixture of diverse datasets poses challenges due to data imbalance and heterogeneity. Existing methods often address these issues across datasets (globally) but overlook the imbalance and heterogeneity within individual datasets (locally), which limits their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a novel method that enables LLMs to autonomously adjust data allocation during fine-tuning both across datasets (globally) and within each individual dataset (locally). HBO employs a bilevel optimization strategy with two types of actors: a Global Actor, which balances data sampling across different subsets of the training mixture, and several Local Actors, which optimizes data usage within each subset based on difficulty levels. These actors are guided by reward functions derived from the LLM's training state, which measure learning progress and relative performance improvement. We evaluate HBO on three LLM backbones across nine diverse tasks in multilingual and multitask setups. Results show that HBO consistently outperforms existing baselines, achieving significant accuracy gains. Our in-depth analysis further demonstrates that both the global actor and local actors of HBO effectively adjust data usage during fine-tuning. HBO provides a comprehensive solution to the challenges of data imbalance and heterogeneity in LLM fine-tuning, enabling more effective training across diverse datasets.</li>
</ul>

<h3>Title: SenseFlow: A Physics-Informed and Self-Ensembling Iterative Framework for Power Flow Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhen Zhao, Wenqi Huang, Zicheng Wang, Jiaxuan Hou, Peng Li, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12302">https://arxiv.org/abs/2505.12302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12302">https://arxiv.org/pdf/2505.12302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12302]] SenseFlow: A Physics-Informed and Self-Ensembling Iterative Framework for Power Flow Estimation(https://arxiv.org/abs/2505.12302)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Power flow estimation plays a vital role in ensuring the stability and reliability of electrical power systems, particularly in the context of growing network complexities and renewable energy integration. However, existing studies often fail to adequately address the unique characteristics of power systems, such as the sparsity of network connections and the critical importance of the unique Slack node, which poses significant challenges in achieving high-accuracy estimations. In this paper, we present SenseFlow, a novel physics-informed and self-ensembling iterative framework that integrates two main designs, the Physics-Informed Power Flow Network (FlowNet) and Self-Ensembling Iterative Estimation (SeIter), to carefully address the unique properties of the power system and thereby enhance the power flow estimation. Specifically, SenseFlow enforces the FlowNet to gradually predict high-precision voltage magnitudes and phase angles through the iterative SeIter process. On the one hand, FlowNet employs the Virtual Node Attention and Slack-Gated Feed-Forward modules to facilitate efficient global-local communication in the face of network sparsity and amplify the influence of the Slack node on angle predictions, respectively. On the other hand, SeIter maintains an exponential moving average of FlowNet's parameters to create a robust ensemble model that refines power state predictions throughout the iterative fitting process. Experimental results demonstrate that SenseFlow outperforms existing methods, providing a promising solution for high-accuracy power flow estimation across diverse grid configurations.</li>
</ul>

<h3>Title: Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection</h3>
<ul>
<li><strong>Authors: </strong>Yuwei Zhang, Wenhao Yu, Shangbin Feng, Yifan Zhu, Letian Peng, Jayanth Srinivasa, Gaowen Liu, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12306">https://arxiv.org/abs/2505.12306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12306">https://arxiv.org/pdf/2505.12306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12306]] Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection(https://arxiv.org/abs/2505.12306)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's "Did You Know..." entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.</li>
</ul>

<h3>Title: Visuospatial Cognitive Assistant</h3>
<ul>
<li><strong>Authors: </strong>Qi Feng (1), Hidetoshi Shimodaira (1 and 2) ((1) Kyoto University, (2) RIKEN)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12312">https://arxiv.org/abs/2505.12312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12312">https://arxiv.org/pdf/2505.12312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12312]] Visuospatial Cognitive Assistant(https://arxiv.org/abs/2505.12312)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.</li>
</ul>

<h3>Title: ExpertSteer: Intervening in LLMs through Expert Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12313">https://arxiv.org/abs/2505.12313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12313">https://arxiv.org/pdf/2505.12313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12313]] ExpertSteer: Intervening in LLMs through Expert Knowledge(https://arxiv.org/abs/2505.12313)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit remarkable capabilities across various tasks, yet guiding them to follow desired behaviours during inference remains a significant challenge. Activation steering offers a promising method to control the generation process of LLMs by modifying their internal activations. However, existing methods commonly intervene in the model's behaviour using steering vectors generated by the model itself, which constrains their effectiveness to that specific model and excludes the possibility of leveraging powerful external expert models for steering. To address these limitations, we propose ExpertSteer, a novel approach that leverages arbitrary specialized expert models to generate steering vectors, enabling intervention in any LLMs. ExpertSteer transfers the knowledge from an expert model to a target LLM through a cohesive four-step process: first aligning representation dimensions with auto-encoders to enable cross-model transfer, then identifying intervention layer pairs based on mutual information analysis, next generating steering vectors from the expert model using Recursive Feature Machines, and finally applying these vectors on the identified layers during inference to selectively guide the target LLM without updating model parameters. We conduct comprehensive experiments using three LLMs on 15 popular benchmarks across four distinct domains. Experiments demonstrate that ExpertSteer significantly outperforms established baselines across diverse tasks at minimal cost.</li>
</ul>

<h3>Title: Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces</h3>
<ul>
<li><strong>Authors: </strong>Ruoqi Wang, Haitao Wang, Shaojie Guo, Qiong Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12317">https://arxiv.org/abs/2505.12317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12317">https://arxiv.org/pdf/2505.12317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12317]] Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces(https://arxiv.org/abs/2505.12317)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Out-of-domain (OOD) robustness under domain adaptation settings, where labeled source data and unlabeled target data come from different distributions, is a key challenge in real-world applications. A common approach to improving OOD robustness is through data augmentations. However, in real-world scenarios, models trained with generic augmentations can only improve marginally when generalized under distribution shifts toward unlabeled target domains. While dataset-specific targeted augmentations can address this issue, they typically require expert knowledge and extensive prior data analysis to identify the nature of the datasets and domain shift. To address these challenges, we propose Frequency-Pixel Connect, a domain-adaptation framework that enhances OOD robustness by introducing a targeted augmentation in both the frequency space and pixel space. Specifically, we mix the amplitude spectrum and pixel content of a source image and a target image to generate augmented samples that introduce domain diversity while preserving the semantic structure of the source image. Unlike previous targeted augmentation methods that are both dataset-specific and limited to the pixel space, Frequency-Pixel Connect is dataset-agnostic, enabling broader and more flexible applicability beyond natural image datasets. We further analyze the effectiveness of Frequency-Pixel Connect by evaluating the performance of our method connecting same-class cross-domain samples while separating different-class examples. We demonstrate that Frequency-Pixel Connect significantly improves cross-domain connectivity and outperforms previous generic methods on four diverse real-world benchmarks across vision, medical, audio, and astronomical domains, and it also outperforms other dataset-specific targeted augmentation methods.</li>
</ul>

<h3>Title: Efficient Federated Class-Incremental Learning of Pre-Trained Models via Task-agnostic Low-rank Residual Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Feng Yu, Jia Hu, Geyong Min</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12318">https://arxiv.org/abs/2505.12318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12318">https://arxiv.org/pdf/2505.12318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12318]] Efficient Federated Class-Incremental Learning of Pre-Trained Models via Task-agnostic Low-rank Residual Adaptation(https://arxiv.org/abs/2505.12318)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Parameter-Efficient Fine-Tuning (FedPEFT) reduces communication and computation costs in federated fine-tuning of pre-trained models by updating only a small subset of model parameters. However, existing approaches assume static data distributions, failing to adequately address real-world scenarios where new classes continually emerge, particularly in Federated Class Incremental Learning (FCIL). FCIL faces two key challenges: catastrophic forgetting and performance degradation caused by non-IID data across clients. Unlike current methods that maintain separate task-specific components or suffer from aggregation noise during parameter aggregation, we propose Federated Task-agnostic Low-rank Residual Adaptation (Fed-TaLoRA), a novel parameter-efficient approach for fine-tuning in resource-constrained FCIL scenarios. Specifically, we fine-tune only shared task-agnostic LoRA parameters across sequential tasks, effectively mitigating catastrophic forgetting while enabling efficient knowledge transfer among clients. Based on a theoretical analysis of aggregation, we develop a novel residual weight update mechanism that ensures accurate knowledge consolidation with minimal overhead. Our methodological innovations are attributed to three key strategies: task-agnostic adaptation, post-aggregation model calibration, and strategic placement of LoRA modules. Extensive experiments on multiple benchmark datasets demonstrate that Fed-TaLoRA consistently outperforms state-of-the-art methods in diverse data heterogeneity scenarios while substantially reducing resource requirements.</li>
</ul>

<h3>Title: GraphFLEx: Structure Learning Framework for Large Expanding Graphs</h3>
<ul>
<li><strong>Authors: </strong>Mohit Kataria, Nikita Malik, Sandeep Kumar, Jayadeva</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12323">https://arxiv.org/abs/2505.12323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12323">https://arxiv.org/pdf/2505.12323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12323]] GraphFLEx: Structure Learning Framework for Large Expanding Graphs(https://arxiv.org/abs/2505.12323)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Graph structure learning is a core problem in graph-based machine learning, essential for uncovering latent relationships and ensuring model interpretability. However, most existing approaches are ill-suited for large-scale and dynamically evolving graphs, as they often require complete re-learning of the structure upon the arrival of new nodes and incur substantial computational and memory costs. In this work, we propose GraphFLEx: a unified and scalable framework for Graph Structure Learning in Large and Expanding Graphs. GraphFLEx mitigates the scalability bottlenecks by restricting edge formation to structurally relevant subsets of nodes identified through a combination of clustering and coarsening techniques. This dramatically reduces the search space and enables efficient, incremental graph updates. The framework supports 48 flexible configurations by integrating diverse choices of learning paradigms, coarsening strategies, and clustering methods, making it adaptable to a wide range of graph settings and learning objectives. Extensive experiments across 26 diverse datasets and Graph Neural Network architectures demonstrate that GraphFLEx achieves state-of-the-art performance with significantly improved scalability.</li>
</ul>

<h3>Title: LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xinye Li, Mingqi Wan, Dianbo Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12328">https://arxiv.org/abs/2505.12328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12328">https://arxiv.org/pdf/2505.12328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12328]] LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning(https://arxiv.org/abs/2505.12328)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which evaluates large language models on producing fine-grained, controllable, and interpretable reasoning processes. Systems must extract all problem conditions, decompose a chain of thought into statement-evidence pairs, and verify the logical validity of each pair. Leveraging only the off-the-shelf Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that first enumerates all conditions and then guides the model to label, cite, and adjudicate every reasoning step. A lightweight post-processor based on regular expressions normalises spans and enforces the official JSON schema. Without fine-tuning, external retrieval, or ensembling, our method ranks 5th overall, achieving macro F1 scores on par with substantially more complex and resource-consuming pipelines. We conclude by analysing the strengths and limitations of our approach and outlining directions for future research in structural reasoning with LLMs. Our code is available at this https URL.</li>
</ul>

<h3>Title: Is Artificial Intelligence Generated Image Detection a Solved Problem?</h3>
<ul>
<li><strong>Authors: </strong>Ziqiang Li, Jiazhen Yan, Ziwen He, Kai Zeng, Weiwei Jiang, Lizhi Xiong, Zhangjie Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12335">https://arxiv.org/abs/2505.12335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12335">https://arxiv.org/pdf/2505.12335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12335]] Is Artificial Intelligence Generated Image Detection a Solved Problem?(https://arxiv.org/abs/2505.12335)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative models, such as GANs and Diffusion models, has enabled the creation of highly realistic synthetic images, raising serious concerns about misinformation, deepfakes, and copyright infringement. Although numerous Artificial Intelligence Generated Image (AIGI) detectors have been proposed, often reporting high accuracy, their effectiveness in real-world scenarios remains questionable. To bridge this gap, we introduce AIGIBench, a comprehensive benchmark designed to rigorously evaluate the robustness and generalization capabilities of state-of-the-art AIGI detectors. AIGIBench simulates real-world challenges through four core tasks: multi-source generalization, robustness to image degradation, sensitivity to data augmentation, and impact of test-time pre-processing. It includes 23 diverse fake image subsets that span both advanced and widely adopted image generation techniques, along with real-world samples collected from social media and AI art platforms. Extensive experiments on 11 advanced detectors demonstrate that, despite their high reported accuracy in controlled settings, these detectors suffer significant performance drops on real-world data, limited benefits from common augmentations, and nuanced effects of pre-processing, highlighting the need for more robust detection strategies. By providing a unified and realistic evaluation framework, AIGIBench offers valuable insights to guide future research toward dependable and generalizable AIGI detection.</li>
</ul>

<h3>Title: Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Midou Guo, Qilin Yin, Wei Lu, Xiangyang Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12339">https://arxiv.org/abs/2505.12339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12339">https://arxiv.org/pdf/2505.12339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12339]] Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation(https://arxiv.org/abs/2505.12339)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>With the development of generative artificial intelligence, new forgery methods are rapidly emerging. Social platforms are flooded with vast amounts of unlabeled synthetic data and authentic data, making it increasingly challenging to distinguish real from fake. Due to the lack of labels, existing supervised detection methods struggle to effectively address the detection of unknown deepfake methods. Moreover, in open world scenarios, the amount of unlabeled data greatly exceeds that of labeled data. Therefore, we define a new deepfake detection generalization task which focuses on how to achieve efficient detection of large amounts of unlabeled data based on limited labeled data to simulate a open world scenario. To solve the above mentioned task, we propose a novel Open-World Deepfake Detection Generalization Enhancement Training Strategy (OWG-DS) to improve the generalization ability of existing methods. Our approach aims to transfer deepfake detection knowledge from a small amount of labeled source domain data to large-scale unlabeled target domain data. Specifically, we introduce the Domain Distance Optimization (DDO) module to align different domain features by optimizing both inter-domain and intra-domain distances. Additionally, the Similarity-based Class Boundary Separation (SCBS) module is used to enhance the aggregation of similar samples to ensure clearer class boundaries, while an adversarial training mechanism is adopted to learn the domain-invariant features. Extensive experiments show that the proposed deepfake detection generalization enhancement training strategy excels in cross-method and cross-dataset scenarios, improving the model's generalization.</li>
</ul>

<h3>Title: Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kai Tang, Jinhao You, Xiuqi Ge, Hanze Li, Yichen Guo, Xiande Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12343">https://arxiv.org/abs/2505.12343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12343">https://arxiv.org/pdf/2505.12343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12343]] Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models(https://arxiv.org/abs/2505.12343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite the impressive capabilities of Large Vision-Language Models (LVLMs), they remain susceptible to hallucinations-generating content that is inconsistent with the input image. Existing training-free hallucination mitigation methods often suffer from unstable performance and high sensitivity to hyperparameter settings, limiting their practicality and broader adoption. In this paper, we propose a novel decoding mechanism, Decoding with Inter-layer Consistency via Layer Aggregation (DCLA), which requires no retraining, fine-tuning, or access to external knowledge bases. Specifically, our approach constructs a dynamic semantic reference by aggregating representations from previous layers, and corrects semantically deviated layers to enforce inter-layer consistency. The method allows DCLA to robustly mitigate hallucinations across multiple LVLMs. Experiments on hallucination benchmarks such as MME and POPE demonstrate that DCLA effectively reduces hallucinations while enhancing the reliability and performance of LVLMs.</li>
</ul>

<h3>Title: UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qizhou Chen, Dakan Wang, Taolin Zhang, Zaoming Yan, Chengsong You, Chengyu Wang, Xiaofeng He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12345">https://arxiv.org/abs/2505.12345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12345">https://arxiv.org/pdf/2505.12345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12345]] UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models(https://arxiv.org/abs/2505.12345)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model editing aims to enhance the accuracy and reliability of large language models (LLMs) by efficiently adjusting their internal parameters. Currently, most LLM editing datasets are confined to narrow knowledge domains and cover a limited range of editing evaluation. They often overlook the broad scope of editing demands and the diversity of ripple effects resulting from edits. In this context, we introduce UniEdit, a unified benchmark for LLM editing grounded in open-domain knowledge. First, we construct editing samples by selecting entities from 25 common domains across five major categories, utilizing the extensive triple knowledge available in open-domain knowledge graphs to ensure comprehensive coverage of the knowledge domains. To address the issues of generality and locality in editing, we design an Neighborhood Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we employ proprietary LLMs to convert the sampled knowledge subgraphs into natural language text, guaranteeing grammatical accuracy and syntactical diversity. Extensive statistical analysis confirms the scale, comprehensiveness, and diversity of our UniEdit benchmark. We conduct comprehensive experiments across multiple LLMs and editors, analyzing their performance to highlight strengths and weaknesses in editing across open knowledge domains and various evaluation criteria, thereby offering valuable insights for future research endeavors.</li>
</ul>

<h3>Title: Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</h3>
<ul>
<li><strong>Authors: </strong>Axel Abels, Tom Lenaerts</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12349">https://arxiv.org/abs/2505.12349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12349">https://arxiv.org/pdf/2505.12349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12349]] Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds(https://arxiv.org/abs/2505.12349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the "wisdom of the crowd", can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.</li>
</ul>

<h3>Title: Importance Sampling for Nonlinear Models</h3>
<ul>
<li><strong>Authors: </strong>Prakash Palanivelu Rajmohan, Fred Roosta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12353">https://arxiv.org/abs/2505.12353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12353">https://arxiv.org/pdf/2505.12353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12353]] Importance Sampling for Nonlinear Models(https://arxiv.org/abs/2505.12353)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>While norm-based and leverage-score-based methods have been extensively studied for identifying "important" data points in linear models, analogous tools for nonlinear models remain significantly underdeveloped. By introducing the concept of the adjoint operator of a nonlinear map, we address this gap and generalize norm-based and leverage-score-based importance sampling to nonlinear settings. We demonstrate that sampling based on these generalized notions of norm and leverage scores provides approximation guarantees for the underlying nonlinear mapping, similar to linear subspace embeddings. As direct applications, these nonlinear scores not only reduce the computational complexity of training nonlinear models by enabling efficient sampling over large datasets but also offer a novel mechanism for model explainability and outlier detection. Our contributions are supported by both theoretical analyses and experimental results across a variety of supervised learning scenarios.</li>
</ul>

<h3>Title: AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion</h3>
<ul>
<li><strong>Authors: </strong>Abrar Rahman Abir, Haz Sameen Shahgir, Md Rownok Zahan Ratul, Md Toki Tahmid, Greg Ver Steeg, Yue Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12358">https://arxiv.org/abs/2505.12358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12358">https://arxiv.org/pdf/2505.12358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12358]] AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion(https://arxiv.org/abs/2505.12358)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Complementarity Determining Regions (CDRs) are critical segments of an antibody that facilitate binding to specific antigens. Current computational methods for CDR design utilize reconstruction losses and do not jointly optimize binding energy, a crucial metric for antibody efficacy. Rather, binding energy optimization is done through computationally expensive Online Reinforcement Learning (RL) pipelines rely heavily on unreliable binding energy estimators. In this paper, we propose AbFlowNet, a novel generative framework that integrates GFlowNet with Diffusion models. By framing each diffusion step as a state in the GFlowNet framework, AbFlowNet jointly optimizes standard diffusion losses and binding energy by directly incorporating energy signals into the training process, thereby unifying diffusion and reward optimization in a single procedure. Experimental results show that AbFlowNet outperforms the base diffusion model by 3.06% in amino acid recovery, 20.40% in geometric reconstruction (RMSD), and 3.60% in binding energy improvement ratio. ABFlowNet also decreases Top-1 total energy and binding energy errors by 24.8% and 38.1% without pseudo-labeling the test dataset or using computationally expensive online RL regimes.</li>
</ul>

<h3>Title: Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</h3>
<ul>
<li><strong>Authors: </strong>Qi Feng (1), Hidetoshi Shimodaira (1 and 2) ((1) Kyoto University, (2) RIKEN)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12363">https://arxiv.org/abs/2505.12363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12363">https://arxiv.org/pdf/2505.12363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12363]] Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts(https://arxiv.org/abs/2505.12363)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.</li>
</ul>

<h3>Title: CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Gauri Kholkar, Ratinder Ahuja</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12368">https://arxiv.org/abs/2505.12368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12368">https://arxiv.org/pdf/2505.12368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12368]] CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement(https://arxiv.org/abs/2505.12368)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations.</li>
</ul>

<h3>Title: Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward</h3>
<ul>
<li><strong>Authors: </strong>Han Weng, Boyi Liu, Yuanfeng Song, Dun Zeng, Yingxiang Yang, Yi Zhan, Longjie Cui, Xiaoming Yin, Yang Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12380">https://arxiv.org/abs/2505.12380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12380">https://arxiv.org/pdf/2505.12380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12380]] Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward(https://arxiv.org/abs/2505.12380)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has been widely adopted to enhance the performance of large language models (LLMs) on Text-to-SQL tasks. However, existing methods often rely on execution-based or LLM-based Bradley-Terry reward models. The former suffers from high execution latency caused by repeated database calls, whereas the latter imposes substantial GPU memory overhead, both of which significantly hinder the efficiency and scalability of RL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning framework named Graph-Reward-SQL, which employs the GMNScore outcome reward model. We leverage SQL graph representations to provide accurate reward signals while significantly reducing inference time and GPU memory usage. Building on this foundation, we further introduce StepRTM, a stepwise reward model that provides intermediate supervision over Common Table Expression (CTE) subqueries. This encourages both functional correctness and structural clarity of SQL. Extensive comparative and ablation experiments on standard benchmarks, including Spider and BIRD, demonstrate that our method consistently outperforms existing reward models.</li>
</ul>

<h3>Title: From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Mohsinul Kabir, Tasfia Tahsin, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12381">https://arxiv.org/abs/2505.12381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12381">https://arxiv.org/pdf/2505.12381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12381]] From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling(https://arxiv.org/abs/2505.12381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Current research on bias in language models (LMs) predominantly focuses on data quality, with significantly less attention paid to model architecture and temporal influences of data. Even more critically, few studies systematically investigate the origins of bias. We propose a methodology grounded in comparative behavioral theory to interpret the complex interaction between training data and model architecture in bias propagation during language modeling. Building on recent work that relates transformers to n-gram LMs, we evaluate how data, model design choices, and temporal dynamics affect bias propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to context window size in bias propagation, while transformers demonstrate architectural robustness; (2) the temporal provenance of training data significantly affects bias; and (3) different model architectures respond differentially to controlled bias injection, with certain biases (e.g. sexual orientation) being disproportionately amplified. As language models become ubiquitous, our findings highlight the need for a holistic approach -- tracing bias to its origins across both data and model dimensions, not just symptoms, to mitigate harm.</li>
</ul>

<h3>Title: Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Liu Ziyin, Yizhou Xu, Isaac Chuang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, cond-mat.stat-mech, math-ph, q-bio.NC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12387">https://arxiv.org/abs/2505.12387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12387">https://arxiv.org/pdf/2505.12387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12387]] Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning(https://arxiv.org/abs/2505.12387)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid discovery of emergent phenomena in deep learning and large language models, explaining and understanding their cause has become an urgent need. Here, we propose a rigorous entropic-force theory for understanding the learning dynamics of neural networks trained with stochastic gradient descent (SGD) and its variants. Building on the theory of parameter symmetries and an entropic loss landscape, we show that representation learning is crucially governed by emergent entropic forces arising from stochasticity and discrete-time updates. These forces systematically break continuous parameter symmetries and preserve discrete ones, leading to a series of gradient balance phenomena that resemble the equipartition property of thermal systems. These phenomena, in turn, (a) explain the universal alignment of neural representations between AI models and lead to a proof of the Platonic Representation Hypothesis, and (b) reconcile the seemingly contradictory observations of sharpness- and flatness-seeking behavior of deep learning optimization. Our theory and experiments demonstrate that a combination of entropic forces and symmetry breaking is key to understanding emergent phenomena in deep learning.</li>
</ul>

<h3>Title: Engineering application of physics-informed neural networks for Saint-Venant torsion</h3>
<ul>
<li><strong>Authors: </strong>Su Yeong Jo, Sanghyeon Park, Seungchan Ko, Jongcheon Park, Hosung Kim, Sangseung Lee, Joongoo Jeon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12389">https://arxiv.org/abs/2505.12389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12389">https://arxiv.org/pdf/2505.12389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12389]] Engineering application of physics-informed neural networks for Saint-Venant torsion(https://arxiv.org/abs/2505.12389)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Saint-Venant torsion theory is a classical theory for analyzing the torsional behavior of structural components, and it remains critically important in modern computational design workflows. Conventional numerical methods, including the finite element method (FEM), typically rely on mesh-based approaches to obtain approximate solutions. However, these methods often require complex and computationally intensive techniques to overcome the limitations of approximation, leading to significant increases in computational cost. The objective of this study is to develop a series of novel numerical methods based on physics-informed neural networks (PINN) for solving the Saint-Venant torsion equations. Utilizing the expressive power and the automatic differentiation capability of neural networks, the PINN can solve partial differential equations (PDEs) along with boundary conditions without the need for intricate computational techniques. First, a PINN solver was developed to compute the torsional constant for bars with arbitrary cross-sectional geometries. This was followed by the development of a solver capable of handling cases with sharp geometric transitions; variable-scaling PINN (VS-PINN). Finally, a parametric PINN was constructed to address the limitations of conventional single-instance PINN. The results from all three solvers showed good agreement with reference solutions, demonstrating their accuracy and robustness. Each solver can be selectively utilized depending on the specific requirements of torsional behavior analysis.</li>
</ul>

<h3>Title: SLOT: Sample-specific Language Model Optimization at Test-time</h3>
<ul>
<li><strong>Authors: </strong>Yang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, Guojun Qi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12392">https://arxiv.org/abs/2505.12392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12392">https://arxiv.org/pdf/2505.12392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12392]] SLOT: Sample-specific Language Model Optimization at Test-time(https://arxiv.org/abs/2505.12392)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at this https URL.</li>
</ul>

<h3>Title: Few-Shot Concept Unlearning with Low Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Udaya Shreyas, L.N. Aadarsh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12395">https://arxiv.org/abs/2505.12395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12395">https://arxiv.org/pdf/2505.12395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12395]] Few-Shot Concept Unlearning with Low Rank Adaptation(https://arxiv.org/abs/2505.12395)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Image Generation models are a trending topic nowadays, with many people utilizing Artificial Intelligence models in order to generate images. There are many such models which, given a prompt of a text, will generate an image which depicts said prompt. There are many image generation models, such as Latent Diffusion Models, Denoising Diffusion Probabilistic Models, Generative Adversarial Networks and many more. When generating images, these models can generate sensitive image data, which can be threatening to privacy or may violate copyright laws of private entities. Machine unlearning aims at removing the influence of specific data subsets from the trained models and in the case of image generation models, remove the influence of a concept such that the model is unable to generate said images of the concept when prompted. Conventional retraining of the model can take upto days, hence fast algorithms are the need of the hour. In this paper we propose an algorithm that aims to remove the influence of concepts in diffusion models through updating the gradients of the final layers of the text encoders. Using a weighted loss function, we utilize backpropagation in order to update the weights of the final layers of the Text Encoder componet of the Stable Diffusion Model, removing influence of the concept from the text-image embedding space, such that when prompted, the result is an image not containing the concept. The weighted loss function makes use of Textual Inversion and Low-Rank this http URL perform our experiments on Latent Diffusion Models, namely the Stable Diffusion v2 model, with an average concept unlearning runtime of 50 seconds using 4-5 images.</li>
</ul>

<h3>Title: Traversal Verification for Speculative Tree Decoding</h3>
<ul>
<li><strong>Authors: </strong>Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12398">https://arxiv.org/abs/2505.12398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12398">https://arxiv.org/pdf/2505.12398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12398]] Traversal Verification for Speculative Tree Decoding(https://arxiv.org/abs/2505.12398)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods</li>
</ul>

<h3>Title: Automated Profile Inference with Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Yuntao Du, Zitao Li, Bolin Ding, Yaliang Li, Hanshen Xiao, Jingren Zhou, Ninghui Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12402">https://arxiv.org/abs/2505.12402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12402">https://arxiv.org/pdf/2505.12402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12402]] Automated Profile Inference with Language Model Agents(https://arxiv.org/abs/2505.12402)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Impressive progress has been made in automated problem-solving by the collaboration of large language models (LLMs) based agents. However, these automated capabilities also open avenues for malicious applications. In this paper, we study a new threat that LLMs pose to online pseudonymity, called automated profile inference, where an adversary can instruct LLMs to automatically scrape and extract sensitive personal attributes from publicly visible user activities on pseudonymous platforms. We also introduce an automated profiling framework called AutoProfiler to assess the feasibility of such threats in real-world scenarios. AutoProfiler consists of four specialized LLM agents, who work collaboratively to collect and process user online activities and generate a profile with extracted personal information. Experimental results on two real-world datasets and one synthetic dataset demonstrate that AutoProfiler is highly effective and efficient, and can be easily deployed on a web scale. We demonstrate that the inferred attributes are both sensitive and identifiable, posing significant risks of privacy breaches, such as de-anonymization and sensitive information leakage. Additionally, we explore mitigation strategies from different perspectives and advocate for increased public awareness of this emerging privacy threat to online pseudonymity.</li>
</ul>

<h3>Title: The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Xylogiannopoulos, Petros Xanthopoulos, Panagiotis Karampelas, Georgios Bakamitsos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12405">https://arxiv.org/abs/2505.12405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12405">https://arxiv.org/pdf/2505.12405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12405]] The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT(https://arxiv.org/abs/2505.12405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI paraphrased text can be used for copyright infringement and the AI paraphrased content can deprive substantial revenue from original content creators. Despite this recent surge of malicious use of generative AI, there are few academic publications that research this threat. In this article, we demonstrate the ability of pattern-based similarity detection for AI paraphrased news recognition. We propose an algorithmic scheme, which is not limited to detect whether an article is an AI paraphrase, but, more importantly, to identify that the source of infringement is the ChatGPT. The proposed method is tested with a benchmark dataset specifically created for this task that incorporates real articles from BBC, incorporating a total of 2,224 articles across five different news categories, as well as 2,224 paraphrased articles created with ChatGPT. Results show that our pattern similarity-based method, that makes no use of deep learning, can detect ChatGPT assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1 score.</li>
</ul>

<h3>Title: It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference Graph</h3>
<ul>
<li><strong>Authors: </strong>Harel Mendelman, Haggai Maron, Ronen Talmon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12411">https://arxiv.org/abs/2505.12411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12411">https://arxiv.org/pdf/2505.12411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12411]] It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference Graph(https://arxiv.org/abs/2505.12411)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) excel at analyzing graph-structured data but struggle on heterophilic graphs, where connected nodes often belong to different classes. While this challenge is commonly addressed with specialized GNN architectures, graph rewiring remains an underexplored strategy in this context. We provide theoretical foundations linking edge homophily, GNN embedding smoothness, and node classification performance, motivating the need to enhance homophily. Building on this insight, we introduce a rewiring framework that increases graph homophily using a reference graph, with theoretical guarantees on the homophily of the rewired graph. To broaden applicability, we propose a label-driven diffusion approach for constructing a homophilic reference graph from node features and training labels. Through extensive simulations, we analyze how the homophily of both the original and reference graphs influences the rewired graph homophily and downstream GNN performance. We evaluate our method on 11 real-world heterophilic datasets and show that it outperforms existing rewiring techniques and specialized GNNs for heterophilic graphs, achieving improved node classification accuracy while remaining efficient and scalable to large graphs.</li>
</ul>

<h3>Title: Table-R1: Region-based Reinforcement Learning for Table Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhenhe Wu, Jian Yang, Jiaheng Liu, Xianjie Wu, Changzai Pan, Jie Zhang, Yu Zhao, Shuangyong Song, Yongxiang Li, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12415">https://arxiv.org/abs/2505.12415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12415">https://arxiv.org/pdf/2505.12415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12415]] Table-R1: Region-based Reinforcement Learning for Table Understanding(https://arxiv.org/abs/2505.12415)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tables present unique challenges for language models due to their structured row-column interactions, necessitating specialized approaches for effective comprehension. While large language models (LLMs) have demonstrated potential in table reasoning through prompting and techniques like chain-of-thought (CoT) and program-of-thought (PoT), optimizing their performance for table question answering remains underexplored. In this paper, we introduce region-based Table-R1, a novel reinforcement learning approach that enhances LLM table understanding by integrating region evidence into reasoning steps. Our method employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in identifying relevant table regions before generating answers, incorporating textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group Relative Policy Optimization (TARPO) introduces a mixed reward system to dynamically balance region accuracy and answer correctness, with decaying region rewards and consistency penalties to align reasoning steps. Experiments show that Table-R1 achieves an average performance improvement of 14.36 points across multiple base models on three benchmark datasets, even outperforming baseline models with ten times the parameters, while TARPO reduces response token consumption by 67.5% compared to GRPO, significantly advancing LLM capabilities in efficient tabular reasoning.</li>
</ul>

<h3>Title: Fixed Point Explainability</h3>
<ul>
<li><strong>Authors: </strong>Emanuele La Malfa, Jon Vadillo, Marco Molinari, Michael Wooldridge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12421">https://arxiv.org/abs/2505.12421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12421">https://arxiv.org/pdf/2505.12421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12421]] Fixed Point Explainability(https://arxiv.org/abs/2505.12421)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>This paper introduces a formal notion of fixed point explanations, inspired by the "why regress" principle, to assess, through recursive applications, the stability of the interplay between a model and its explainer. Fixed point explanations satisfy properties like minimality, stability, and faithfulness, revealing hidden model behaviours and explanatory weaknesses. We define convergence conditions for several classes of explainers, from feature-based to mechanistic tools like Sparse AutoEncoders, and we report quantitative and qualitative results.</li>
</ul>

<h3>Title: PSC: Extending Context Window of Large Language Models via Phase Shift Calibration</h3>
<ul>
<li><strong>Authors: </strong>Wenqiao Zhu, Chao Xu, Lulu Wang, Jun Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12423">https://arxiv.org/abs/2505.12423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12423">https://arxiv.org/pdf/2505.12423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12423]] PSC: Extending Context Window of Large Language Models via Phase Shift Calibration(https://arxiv.org/abs/2505.12423)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks. The code can be found at this https URL.</li>
</ul>

<h3>Title: DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Siwei Xia, Li Sun, Tiantian Sun, Qingli Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12427">https://arxiv.org/abs/2505.12427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12427">https://arxiv.org/pdf/2505.12427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12427]] DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model(https://arxiv.org/abs/2505.12427)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Drag-based editing within pretrained diffusion model provides a precise and flexible way to manipulate foreground objects. Traditional methods optimize the input feature obtained from DDIM inversion directly, adjusting them iteratively to guide handle points towards target locations. However, these approaches often suffer from limited accuracy due to the low representation ability of the feature in motion supervision, as well as inefficiencies caused by the large search space required for point tracking. To address these limitations, we present DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation) adapters into the drag-based editing pipeline. To enhance the training of LoRA adapters, we introduce an additional denoising score distillation loss which regularizes the online model by aligning its output with that of the original model. Additionally, we improve the consistency of motion supervision by adapting the input features using the updated LoRA, giving a more stable and accurate input feature for subsequent operations. Building on this, we design an adaptive optimization scheme that dynamically toggles between two modes, prioritizing efficiency without compromising precision. Extensive experiments demonstrate that DragLoRA significantly enhances the control precision and computational efficiency for drag-based image editing. The Codes of DragLoRA are available at: this https URL.</li>
</ul>

<h3>Title: Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zirun Guo, Minjie Hong, Tao Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12432">https://arxiv.org/abs/2505.12432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12432">https://arxiv.org/pdf/2505.12432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12432]] Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning(https://arxiv.org/abs/2505.12432)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has shown promise in improving the reasoning abilities of Large Language Models (LLMs). However, the specific challenges of adapting RL to multimodal data and formats remain relatively unexplored. In this work, we present Observe-R1, a novel framework aimed at enhancing the reasoning capabilities of multimodal large language models (MLLMs). We draw inspirations from human learning progression--from simple to complex and easy to difficult, and propose a gradual learning paradigm for MLLMs. To this end, we construct the NeuraLadder dataset, which is organized and sampled according to the difficulty and complexity of data samples for RL training. To tackle multimodal tasks, we introduce a multimodal format constraint that encourages careful observation of images, resulting in enhanced visual abilities and clearer and more structured responses. Additionally, we implement a bonus reward system that favors concise, correct answers within a length constraint, alongside a dynamic weighting mechanism that prioritizes uncertain and medium-difficulty problems, ensuring that more informative samples have a greater impact on training. Our experiments with the Qwen2.5-VL-3B and Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that Observe-R1 outperforms a series of larger reasoning models on both reasoning and general benchmarks, achieving superior clarity and conciseness in reasoning chains. Ablation studies validate the effectiveness of our strategies, highlighting the robustness and generalization of our approach. The dataset and code will be released at this https URL.</li>
</ul>

<h3>Title: VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, Tianfei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12434">https://arxiv.org/abs/2505.12434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12434">https://arxiv.org/pdf/2505.12434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12434]] VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning(https://arxiv.org/abs/2505.12434)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement fine-tuning (RFT) has shown great promise in achieving humanlevel reasoning capabilities of Large Language Models (LLMs), and has recently been extended to MLLMs. Nevertheless, reasoning about videos, which is a fundamental aspect of human intelligence, remains a persistent challenge due to the complex logic, temporal and causal structures inherent in video data. To fill this gap, we propose VIDEORFT, a novel approach that extends the RFT paradigm to cultivate human-like video reasoning capabilities in MLLMs. VIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning (SFT) with chain-of-thought (CoT) annotations, followed by reinforcement learning (RL) to improve generalization. A central challenge to achieve this in the video domain lies in the scarcity of large-scale, high-quality video CoT datasets. We address this by building a fully automatic CoT curation pipeline. First, we devise a cognitioninspired prompting strategy to elicit a reasoning LLM to generate preliminary CoTs based solely on rich, structured, and literal representations of video content. Subsequently, these CoTs are revised by a visual-language model conditioned on the actual video, ensuring visual consistency and reducing visual hallucinations. This pipeline results in two new datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To further strength the RL phase, we introduce a novel semantic-consistency reward that explicitly promotes the alignment between textual reasoning with visual evidence. This reward encourages the model to produce coherent, context-aware reasoning outputs grounded in visual input. Extensive experiments show that VIDEORFT achieves state-of-the-art performance on six video reasoning benchmarks.</li>
</ul>

<h3>Title: SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Wenqiao Zhu, Ji Liu, Lulu Wang, Jun Wu, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12435">https://arxiv.org/abs/2505.12435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12435">https://arxiv.org/pdf/2505.12435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12435]] SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment(https://arxiv.org/abs/2505.12435)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) is broadly utilized for aligning Large Language Models (LLMs) with human values because of its flexibility. Despite its effectiveness, it has been observed that the capability of DPO to generate human-preferred response is limited and the results of DPO are far from resilient. To address these limitations, in this paper we propose a novel Self-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which incorporates a pilot term to steer the gradient flow during the optimization process, allowing for fine-grained control over the updates of chosen and rejected rewards. We provide a detailed theoretical analysis of our proposed method and elucidate its operational mechanism. Furthermore, we conduct comprehensive experiments on various models and benchmarks. The extensive experimental results demonstrate the consistency between the empirical results and our theoretical analysis and confirm the effectiveness of our proposed approach (up to 9.19% higher score).</li>
</ul>

<h3>Title: Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games</h3>
<ul>
<li><strong>Authors: </strong>Jinming Zhang, Yunfei Long</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12439">https://arxiv.org/abs/2505.12439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12439">https://arxiv.org/pdf/2505.12439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12439]] Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games(https://arxiv.org/abs/2505.12439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Interactive Fiction games (IF games) are where players interact through natural language commands. While recent advances in Artificial Intelligence agents have reignited interest in IF games as a domain for studying decision-making, existing approaches prioritize task-specific performance metrics over human-like comprehension of narrative context and gameplay logic. This work presents a cognitively inspired framework that guides Large Language Models (LLMs) to learn and play IF games systematically. Our proposed **L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three key components: (1) structured map building to capture spatial and narrative relationships, (2) action learning to identify context-appropriate commands, and (3) feedback-driven experience analysis to refine decision-making over time. By aligning LLMs-based agents' behavior with narrative intent and commonsense constraints, LPLH moves beyond purely exploratory strategies to deliver more interpretable, human-like performance. Crucially, this approach draws on cognitive science principles to more closely simulate how human players read, interpret, and respond within narrative worlds. As a result, LPLH reframes the IF games challenge as a learning problem for LLMs-based agents, offering a new path toward robust, context-aware gameplay in complex text-based environments.</li>
</ul>

<h3>Title: IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, Shing-Chi Cheung</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12442">https://arxiv.org/abs/2505.12442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12442">https://arxiv.org/pdf/2505.12442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12442]] IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems(https://arxiv.org/abs/2505.12442)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.</li>
</ul>

<h3>Title: Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment</h3>
<ul>
<li><strong>Authors: </strong>Siyang Wu, Honglin Bao, Nadav Kunievsky, James A. Evans</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.DL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12452">https://arxiv.org/abs/2505.12452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12452">https://arxiv.org/pdf/2505.12452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12452]] Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment(https://arxiv.org/abs/2505.12452)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly demonstrate signs of conceptual understanding, yet much of their internal knowledge remains latent, loosely structured, and difficult to access or evaluate. We propose self-questioning as a lightweight and scalable strategy to improve LLMs' understanding, particularly in domains where success depends on fine-grained semantic distinctions. To evaluate this approach, we introduce a challenging new benchmark of 1.3 million post-2015 computer science patent pairs, characterized by dense technical jargon and strategically complex writing. The benchmark centers on a pairwise differentiation task: can a model distinguish between closely related but substantively different inventions? We show that prompting LLMs to generate and answer their own questions - targeting the background knowledge required for the task - significantly improves performance. These self-generated questions and answers activate otherwise underutilized internal knowledge. Allowing LLMs to retrieve answers from external scientific texts further enhances performance, suggesting that model knowledge is compressed and lacks the full richness of the training data. We also find that chain-of-thought prompting and self-questioning converge, though self-questioning remains more effective for improving understanding of technical concepts. Notably, we uncover an asymmetry in prompting: smaller models often generate more fundamental, more open-ended, better-aligned questions for mid-sized models than large models with better understanding do, revealing a new strategy for cross-model collaboration. Altogether, our findings establish self-questioning as both a practical mechanism for automatically improving LLM comprehension, especially in domains with sparse and underrepresented knowledge, and a diagnostic probe of how internal and external knowledge are organized.</li>
</ul>

<h3>Title: SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding</h3>
<ul>
<li><strong>Authors: </strong>Peihua Mai, Youlong Ding, Ziyan Lyu, Minxin Du, Yan Pang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12453">https://arxiv.org/abs/2505.12453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12453">https://arxiv.org/pdf/2505.12453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12453]] SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding(https://arxiv.org/abs/2505.12453)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated recommender system (FedRec) has emerged as a solution to protect user data through collaborative training techniques. A typical FedRec involves transmitting the full model and entire weight updates between edge devices and the server, causing significant burdens to devices with limited bandwidth and computational power. While the sparsity of embedding updates provides opportunity for payload optimization, existing sparsity-aware federated protocols generally sacrifice privacy for efficiency. A key challenge in designing a secure sparsity-aware efficient protocol is to protect the rated item indices from the server. In this paper, we propose a lossless secure recommender systems on sparse embedding updates (SecEmb). SecEmb reduces user payload while ensuring that the server learns no information about both rated item indices and individual updates except the aggregated model. The protocol consists of two correlated modules: (1) a privacy-preserving embedding retrieval module that allows users to download relevant embeddings from the server, and (2) an update aggregation module that securely aggregates updates at the server. Empirical analysis demonstrates that SecEmb reduces both download and upload communication costs by up to 90x and decreases user-side computation time by up to 70x compared with secure FedRec protocols. Additionally, it offers non-negligible utility advantages compared with lossy message compression methods.</li>
</ul>

<h3>Title: Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations</h3>
<ul>
<li><strong>Authors: </strong>Yuyang Ding, Dan Qiao, Juntao Li, Jiajie Xu, Pingfu Chao, Xiaofang Zhou, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12454">https://arxiv.org/abs/2505.12454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12454">https://arxiv.org/pdf/2505.12454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12454]] Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations(https://arxiv.org/abs/2505.12454)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Distantly supervised named entity recognition (DS-NER) has emerged as a cheap and convenient alternative to traditional human annotation methods, enabling the automatic generation of training data by aligning text with external resources. Despite the many efforts in noise measurement methods, few works focus on the latent noise distribution between different distant annotation methods. In this work, we explore the effectiveness and robustness of DS-NER by two aspects: (1) distant annotation techniques, which encompasses both traditional rule-based methods and the innovative large language model supervision approach, and (2) noise assessment, for which we introduce a novel framework. This framework addresses the challenges by distinctly categorizing them into the unlabeled-entity problem (UEP) and the noisy-entity problem (NEP), subsequently providing specialized solutions for each. Our proposed method achieves significant improvements on eight real-world distant supervision datasets originating from three different data sources and involving four distinct annotation techniques, confirming its superiority over current state-of-the-art methods.</li>
</ul>

<h3>Title: AltLoRA: Towards Better Gradient Approximation in Low-Rank Adaptation with Alternating Projections</h3>
<ul>
<li><strong>Authors: </strong>Xin Yu, Yujia Wang, Jinghui Chen, Lingzhou Xue</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12455">https://arxiv.org/abs/2505.12455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12455">https://arxiv.org/pdf/2505.12455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12455]] AltLoRA: Towards Better Gradient Approximation in Low-Rank Adaptation with Alternating Projections(https://arxiv.org/abs/2505.12455)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has emerged as an effective technique for reducing memory overhead in fine-tuning large language models. However, it often suffers from sub-optimal performance compared with full fine-tuning since the update is constrained in the low-rank space. Recent variants such as LoRA-Pro attempt to mitigate this by adjusting the gradients of the low-rank matrices to approximate the full gradient. However, LoRA-Pro's solution is not unique, and different solutions can lead to significantly varying performance in ablation studies. Besides, to incorporate momentum or adaptive optimization design, approaches like LoRA-Pro must first compute the equivalent gradient, causing a higher memory cost close to full fine-tuning. A key challenge remains in integrating momentum properly into the low-rank space with lower memory cost. In this work, we propose AltLoRA, an alternating projection method that avoids the difficulties in gradient approximation brought by the joint update design, meanwhile integrating momentum without higher memory complexity. Our theoretical analysis provides convergence guarantees and further shows that AltLoRA enables stable feature learning and robustness to transformation invariance. Extensive experiments across multiple tasks demonstrate that AltLoRA outperforms LoRA and its variants, narrowing the gap toward full fine-tuning while preserving superior memory efficiency.</li>
</ul>

<h3>Title: A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zachary Roch, Chi Zhang, George Atia, Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12462">https://arxiv.org/abs/2505.12462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12462">https://arxiv.org/pdf/2505.12462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12462]] A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning(https://arxiv.org/abs/2505.12462)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust reinforcement learning (RL) under the average-reward criterion is crucial for long-term decision making under potential environment mismatches, yet its finite-sample complexity study remains largely unexplored. Existing works offer algorithms with asymptotic guarantees, but the absence of finite-sample analysis hinders its principled understanding and practical deployment, especially in data-limited settings. We close this gap by proposing Robust Halpern Iteration (RHI), the first algorithm with provable finite-sample complexity guarantee. Under standard uncertainty sets -- including contamination sets and $\ell_p$-norm balls -- RHI attains an $\epsilon$-optimal policy with near-optimal sample complexity of $\tilde{\mathcal O}\left(\frac{SA\mathcal H^{2}}{\epsilon^{2}}\right)$, where $S$ and $A$ denote the numbers of states and actions, and $\mathcal H$ is the robust optimal bias span. This result gives the first polynomial sample complexity guarantee for robust average-reward RL. Moreover, our RHI's independence from prior knowledge distinguishes it from many previous average-reward RL studies. Our work thus constitutes a significant advancement in enhancing the practical applicability of robust average-reward methods to complex, real-world problems.</li>
</ul>

<h3>Title: What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization</h3>
<ul>
<li><strong>Authors: </strong>Weixiao Zhou, Junnan Zhu, Gengyao Li, Xianfu Cheng, Xinnian Liang, Feifei Zhai, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12474">https://arxiv.org/abs/2505.12474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12474">https://arxiv.org/pdf/2505.12474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12474]] What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization(https://arxiv.org/abs/2505.12474)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we investigate the performance of LLMs on a new task that requires combining discussion with background knowledge for summarization. This aims to address the limitation of outside observer confusion in existing dialogue summarization systems due to their reliance solely on discussion information. To achieve this, we model the task output as background and opinion summaries and define two standardized summarization patterns. To support assessment, we introduce the first benchmark comprising high-quality samples consistently annotated by human experts and propose a novel hierarchical evaluation framework with fine-grained, interpretable metrics. We evaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our findings reveal: (1) LLMs struggle with background summary retrieval, generation, and opinion summary integration. (2) Even top LLMs achieve less than 69% average performance across both patterns. (3) Current LLMs lack adequate self-evaluation and self-correction capabilities for this task.</li>
</ul>

<h3>Title: Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering</h3>
<ul>
<li><strong>Authors: </strong>Xiao Long, Liansheng Zhuang, Chen Shen, Shaotian Yan, Yifei Li, Shafei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12476">https://arxiv.org/abs/2505.12476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12476">https://arxiv.org/pdf/2505.12476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12476]] Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering(https://arxiv.org/abs/2505.12476)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have demonstrated impressive performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to find answers based on knowledge graphs (KGs) for natural language questions. Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the large KGs, and then generates the answers based on them. However, these methods emphasize the exploration of new optimal reasoning paths in KGs while ignoring the exploitation of historical reasoning paths, which may lead to sub-optimal reasoning paths. Additionally, the complex semantics contained in questions may lead to the retrieval of inaccurate reasoning paths. To address these issues, this paper proposes a novel and training-free framework for KGQA tasks called Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original question into a series of simpler and well-defined sub-questions to handle the complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided by a reward model is introduced to iteratively retrieve weighted reasoning paths as contextual knowledge. Finally, it stacks the weighted reasoning paths according to their weights to generate the final answers. Extensive experiments on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves 8.7\% and 7.0\% performance improvement over the state-of-the-art method on the GrailQA and the WebQSP respectively.</li>
</ul>

<h3>Title: $$-FedHT: Stepsize-Aware Hard-Threshold Gradient Compression in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Rongwei Lu, Yutong Jiang, Jinrui Zhang, Chunyang Li, Yifei Zhu, Bin Chen, Zhi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12479">https://arxiv.org/abs/2505.12479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12479">https://arxiv.org/pdf/2505.12479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12479]] $$-FedHT: Stepsize-Aware Hard-Threshold Gradient Compression in Federated Learning(https://arxiv.org/abs/2505.12479)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Gradient compression can effectively alleviate communication bottlenecks in Federated Learning (FL). Contemporary state-of-the-art sparse compressors, such as Top-$k$, exhibit high computational complexity, up to $\mathcal{O}(d\log_2{k})$, where $d$ is the number of model parameters. The hard-threshold compressor, which simply transmits elements with absolute values higher than a fixed threshold, is thus proposed to reduce the complexity to $\mathcal{O}(d)$. However, the hard-threshold compression causes accuracy degradation in FL, where the datasets are non-IID and the stepsize $\gamma$ is decreasing for model convergence. The decaying stepsize reduces the updates and causes the compression ratio of the hard-threshold compression to drop rapidly to an aggressive ratio. At or below this ratio, the model accuracy has been observed to degrade severely. To address this, we propose $\gamma$-FedHT, a stepsize-aware low-cost compressor with Error-Feedback to guarantee convergence. Given that the traditional theoretical framework of FL does not consider Error-Feedback, we introduce the fundamental conversation of Error-Feedback. We prove that $\gamma$-FedHT has the convergence rate of $\mathcal{O}(\frac{1}{T})$ ($T$ representing total training iterations) under $\mu$-strongly convex cases and $\mathcal{O}(\frac{1}{\sqrt{T}})$ under non-convex cases, \textit{same as FedAVG}. Extensive experiments demonstrate that $\gamma$-FedHT improves accuracy by up to $7.42\%$ over Top-$k$ under equal communication traffic on various non-IID image datasets.</li>
</ul>

<h3>Title: Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation</h3>
<ul>
<li><strong>Authors: </strong>Sangmin Jung, Utkarsh Nath, Yezhou Yang, Giulia Pedrielli, Joydeep Biswas, Amy Zhang, Hassan Ghasemzadeh, Pavan Turaga</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12486">https://arxiv.org/abs/2505.12486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12486">https://arxiv.org/pdf/2505.12486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12486]] Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation(https://arxiv.org/abs/2505.12486)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Text-to-image generation models have achieved remarkable capabilities in synthesizing images, but often struggle to provide fine-grained control over the output. Existing guidance approaches, such as segmentation maps and depth maps, introduce spatial rigidity that restricts the inherent diversity of diffusion models. In this work, we introduce Deep Geometric Moments (DGM) as a novel form of guidance that encapsulates the subject's visual features and nuances through a learned geometric prior. DGMs focus specifically on the subject itself compared to DINO or CLIP features, which suffer from overemphasis on global image features or semantics. Unlike ResNets, which are sensitive to pixel-wise perturbations, DGMs rely on robust geometric moments. Our experiments demonstrate that DGM effectively balance control and diversity in diffusion-based image generation, allowing a flexible control mechanism for steering the diffusion process.</li>
</ul>

<h3>Title: Video-GPT via Next Clip Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Shaobin Zhuang, Zhipeng Huang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Binxin Yang, Chong Sun, Chen Li, Yali Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12489">https://arxiv.org/abs/2505.12489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12489">https://arxiv.org/pdf/2505.12489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12489]] Video-GPT via Next Clip Diffusion(https://arxiv.org/abs/2505.12489)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>GPT has shown its remarkable success in natural language processing. However, the language sequence is not sufficient to describe spatial-temporal details in the visual world. Alternatively, the video sequence is good at capturing such details. Motivated by this fact, we propose a concise Video-GPT in this paper by treating video as new language for visual world modeling. By analogy to next token prediction in GPT, we introduce a novel next clip diffusion paradigm for pretraining Video-GPT. Different from the previous works, this distinct paradigm allows Video-GPT to tackle both short-term generation and long-term prediction, by autoregressively denoising the noisy clip according to the clean clips in the history. Extensive experiments show our Video-GPT achieves the state-of-the-art performance on video prediction, which is the key factor towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in both video generation and understanding, showing its great generalization capacity in downstream. The project page is at this https URL.</li>
</ul>

<h3>Title: Proposal for Improving Google A2A Protocol: Safeguarding Sensitive Data in Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Yedidel Louck, Ariel Stulman, Amit Dvir</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12490">https://arxiv.org/abs/2505.12490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12490">https://arxiv.org/pdf/2505.12490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12490]] Proposal for Improving Google A2A Protocol: Safeguarding Sensitive Data in Multi-Agent Systems(https://arxiv.org/abs/2505.12490)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust</a></li>
<li><strong>Abstract: </strong>A2A, a protocol for AI agent communication, offers a robust foundation for secure AI agent communication. However, it has several critical issues in handling sensitive data, such as payment details, identification documents, and personal information. This paper reviews the existing protocol, identifies its limitations, and proposes specific enhancements to improve security, privacy, and trust. It includes a concrete example to illustrate the problem and solution, research-backed rationales, and implementation considerations, drawing on prior studies to strengthen the arguments and proposed solutions. This proposal includes seven enhancements: short-lived tokens, customer authentication (SCA), granular scopes, explicit consent, direct data transfer, multi-transaction approval, and payment standard compliance. The vacation booking example illustrates how these enhancements reduce risks and enhance user experience.</li>
</ul>

<h3>Title: Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12499">https://arxiv.org/abs/2505.12499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12499">https://arxiv.org/pdf/2505.12499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12499]] Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval(https://arxiv.org/abs/2505.12499)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Recent advances in text-video retrieval have been largely driven by contrastive learning frameworks. However, existing methods overlook a key source of optimization tension: the separation between text and video distributions in the representation space (referred to as the modality gap), and the prevalence of false negatives in batch sampling. These factors lead to conflicting gradients under the InfoNCE loss, impeding stable alignment. To mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces a learnable, pair-specific increment Delta_ij between text t_i and video v_j to offload the tension from the global anchor representation. We first derive the ideal form of Delta_ij via a coupled multivariate first-order Taylor approximation of the InfoNCE loss under a trust-region constraint, revealing it as a mechanism for resolving gradient conflicts by guiding updates along a locally optimal descent direction. Due to the high cost of directly computing Delta_ij, we introduce a lightweight neural module conditioned on the semantic gap between each video-text pair, enabling structure-aware correction guided by gradient supervision. To further stabilize learning and promote interpretability, we regularize Delta using three components: a trust-region constraint to prevent oscillation, a directional diversity term to promote semantic coverage, and an information bottleneck to limit redundancy. Experiments across four retrieval benchmarks show that GARE consistently improves alignment accuracy and robustness to noisy supervision, confirming the effectiveness of gap-aware tension mitigation.</li>
</ul>

<h3>Title: CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zongkai Liu, Fanqing Meng, Lingxiao Du, Zhixiang Zhou, Chao Yu, Wenqi Shao, Qiaosheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12504">https://arxiv.org/abs/2505.12504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12504">https://arxiv.org/pdf/2505.12504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12504]] CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models(https://arxiv.org/abs/2505.12504)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at this https URL.</li>
</ul>

<h3>Title: Unsupervised Invariant Risk Minimization</h3>
<ul>
<li><strong>Authors: </strong>Yotam Norman, Ron Meir</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12506">https://arxiv.org/abs/2505.12506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12506">https://arxiv.org/pdf/2505.12506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12506]] Unsupervised Invariant Risk Minimization(https://arxiv.org/abs/2505.12506)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>We propose a novel unsupervised framework for \emph{Invariant Risk Minimization} (IRM), extending the concept of invariance to settings where labels are unavailable. Traditional IRM methods rely on labeled data to learn representations that are robust to distributional shifts across environments. In contrast, our approach redefines invariance through feature distribution alignment, enabling robust representation learning from unlabeled data. We introduce two methods within this framework: Principal Invariant Component Analysis (PICA), a linear method that extracts invariant directions under Gaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep generative model that disentangles environment-invariant and environment-dependent latent factors. Our approach is based on a novel ``unsupervised'' structural causal model and supports environment-conditioned sample-generation and intervention. Empirical evaluations on synthetic dataset and modified versions of MNIST demonstrate the effectiveness of our methods in capturing invariant structure, preserving relevant information, and generalizing across environments without access to labels.</li>
</ul>

<h3>Title: LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection</h3>
<ul>
<li><strong>Authors: </strong>Xu Zheng, Zhuomin Chen, Esteban Schafir, Sipeng Chen, Hojat Allah Salehi, Haifeng Chen, Farhad Shirani, Wei Cheng, Dongsheng Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12507">https://arxiv.org/abs/2505.12507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12507">https://arxiv.org/pdf/2505.12507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12507]] LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection(https://arxiv.org/abs/2505.12507)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>The impressive ability of large language models to generate natural text across various tasks has led to critical challenges in authorship authentication. Although numerous detection methods have been developed to differentiate between machine-generated texts (MGT) and human-generated texts (HGT), the explainability of these methods remains a significant gap. Traditional explainability techniques often fall short in capturing the complex word relationships that distinguish HGT from MGT. To address this limitation, we present LM$^2$otifs, a novel explainable framework for MGT detection. Inspired by probabilistic graphical models, we provide a theoretical rationale for the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks to achieve both accurate detection and interpretability. The LM$^2$otifs pipeline operates in three key stages: first, it transforms text into graphs based on word co-occurrence to represent lexical dependencies; second, graph neural networks are used for prediction; and third, a post-hoc explainability method extracts interpretable motifs, offering multi-level explanations from individual words to sentence structures. Extensive experiments on multiple benchmark datasets demonstrate the comparable performance of LM$^2$otifs. The empirical evaluation of the extracted explainable motifs confirms their effectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis reveals distinct and visible linguistic fingerprints characteristic of MGT.</li>
</ul>

<h3>Title: Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junhao Liu, Haonan Yu, Xin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12509">https://arxiv.org/abs/2505.12509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12509">https://arxiv.org/pdf/2505.12509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12509]] Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models(https://arxiv.org/abs/2505.12509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With Large language models (LLMs) becoming increasingly prevalent in various applications, the need for interpreting their predictions has become a critical challenge. As LLMs vary in architecture and some are closed-sourced, model-agnostic techniques show great promise without requiring access to the model's internal parameters. However, existing model-agnostic techniques need to invoke LLMs many times to gain sufficient samples for generating faithful explanations, which leads to high economic costs. In this paper, we show that it is practical to generate faithful explanations for large-scale LLMs by sampling from some budget-friendly models through a series of empirical studies. Moreover, we show that such proxy explanations also perform well on downstream tasks. Our analysis provides a new paradigm of model-agnostic explanation methods for LLMs, by including information from budget-friendly models.</li>
</ul>

<h3>Title: DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design</h3>
<ul>
<li><strong>Authors: </strong>Yanting Li, Jiyue Jiang, Zikang Wang, Ziqian Lin, Dongchen He, Yuheng Shan, Yanruisheng Shao, Jiayi Li, Xiangyu Shi, Jiuming Wang, Yanyu Chen, Yimin Fan, Han Li, Yu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12511">https://arxiv.org/abs/2505.12511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12511">https://arxiv.org/pdf/2505.12511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12511]] DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design(https://arxiv.org/abs/2505.12511)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Inverse Protein Folding (IPF) is a critical subtask in the field of protein design, aiming to engineer amino acid sequences capable of folding correctly into a specified three-dimensional (3D) conformation. Although substantial progress has been achieved in recent years, existing methods generally rely on either backbone coordinates or molecular surface features alone, which restricts their ability to fully capture the complex chemical and geometric constraints necessary for precise sequence prediction. To address this limitation, we present DS-ProGen, a dual-structure deep language model for functional protein design, which integrates both backbone geometry and surface-level representations. By incorporating backbone coordinates as well as surface chemical and geometric descriptors into a next-amino-acid prediction paradigm, DS-ProGen is able to generate functionally relevant and structurally stable sequences while satisfying both global and local conformational constraints. On the PRIDE dataset, DS-ProGen attains the current state-of-the-art recovery rate of 61.47%, demonstrating the synergistic advantage of multi-modal structural encoding in protein design. Furthermore, DS-ProGen excels in predicting interactions with a variety of biological partners, including ligands, ions, and RNA, confirming its robust functional retention capabilities.</li>
</ul>

<h3>Title: Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought</h3>
<ul>
<li><strong>Authors: </strong>Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12514">https://arxiv.org/abs/2505.12514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12514">https://arxiv.org/pdf/2505.12514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12514]] Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought(https://arxiv.org/abs/2505.12514)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thoughts (CoTs) techniques that generate ``thinking tokens'' before answering the questions. While existing theoretical works demonstrate that CoTs with discrete tokens boost the capability of LLMs, recent work on continuous CoTs lacks a theoretical understanding of why it outperforms discrete counterparts in various reasoning tasks such as directed graph reachability, a fundamental graph reasoning problem that includes many practical domain applications as special cases. In this paper, we prove that a two-layer transformer with $D$ steps of continuous CoTs can solve the directed graph reachability problem, where $D$ is the diameter of the graph, while the best known result of constant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps where $n$ is the number of vertices ($D<n$). In our construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoTs must choose a single path sampled from the superposition state, which leads to sequential search that requires many more steps and may be trapped into local solutions. We also performed extensive experiments to verify that our theoretical construction aligns well with the empirical solution obtained via training dynamics. Notably, encoding of multiple search frontiers as a superposition state automatically emerges in training continuous CoTs, without explicit supervision to guide the model to explore multiple paths simultaneously.</li>
</ul>

<h3>Title: Enforcing Fairness Where It Matters: An Approach Based on Difference-of-Convex Constraints</h3>
<ul>
<li><strong>Authors: </strong>Yutian He, Yankun Huang, Yao Yao, Qihang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12530">https://arxiv.org/abs/2505.12530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12530">https://arxiv.org/pdf/2505.12530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12530]] Enforcing Fairness Where It Matters: An Approach Based on Difference-of-Convex Constraints(https://arxiv.org/abs/2505.12530)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness in machine learning has become a critical concern, particularly in high-stakes applications. Existing approaches often focus on achieving full fairness across all score ranges generated by predictive models, ensuring fairness in both high and low-scoring populations. However, this stringent requirement can compromise predictive performance and may not align with the practical fairness concerns of stakeholders. In this work, we propose a novel framework for building partially fair machine learning models, which enforce fairness within a specific score range of interest, such as the middle range where decisions are most contested, while maintaining flexibility in other regions. We introduce two statistical metrics to rigorously evaluate partial fairness within a given score range, such as the top 20%-40% of scores. To achieve partial fairness, we propose an in-processing method by formulating the model training problem as constrained optimization with difference-of-convex constraints, which can be solved by an inexact difference-of-convex algorithm (IDCA). We provide the complexity analysis of IDCA for finding a nearly KKT point. Through numerical experiments on real-world datasets, we demonstrate that our framework achieves high predictive performance while enforcing partial fairness where it matters most.</li>
</ul>

<h3>Title: ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Navid Madani, Rohini Srihari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12531">https://arxiv.org/abs/2505.12531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12531">https://arxiv.org/pdf/2505.12531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12531]] ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents(https://arxiv.org/abs/2505.12531)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly power mental-health chatbots, yet the field still lacks a scalable, theory-grounded way to decide which model is most effective to deploy. We present ESC-Judge, the first end-to-end evaluation framework that (i) grounds head-to-head comparisons of emotional-support LLMs in Clara Hill's established Exploration-Insight-Action counseling model, providing a structured and interpretable view of performance, and (ii) fully automates the evaluation pipeline at scale. ESC-Judge operates in three stages: first, it synthesizes realistic help-seeker roles by sampling empirically salient attributes such as stressors, personality, and life history; second, it has two candidate support agents conduct separate sessions with the same role, isolating model-specific strategies; and third, it asks a specialized judge LLM to express pairwise preferences across rubric-anchored skills that span the Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and 86 percent of Action decisions, demonstrating human-level reliability at a fraction of the cost. All code, prompts, synthetic roles, transcripts, and judgment scripts are released to promote transparent progress in emotionally supportive AI.</li>
</ul>

<h3>Title: Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets</h3>
<ul>
<li><strong>Authors: </strong>Ahmet Bilican, M. Akn Ylmaz, A. Murat Tekalp, R. Gkberk Cinbi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12532">https://arxiv.org/abs/2505.12532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12532">https://arxiv.org/pdf/2505.12532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12532]] Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets(https://arxiv.org/abs/2505.12532)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Efficiently adapting large foundation models is critical, especially with tight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA offer limited granularity and effectiveness in few-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT method that learns highly sparse updates in the wavelet domain of residual matrices. WaveFT allows precise control of trainable parameters, offering fine-grained capacity adjustment and excelling with remarkably low parameter count, potentially far fewer than LoRA's minimum -- ideal for extreme parameter-efficient scenarios. In order to demonstrate the effect of the wavelet transform, we compare WaveFT with a special case, called SHiRA, that entails applying sparse updates directly in the weight domain. Evaluated on personalized text-to-image generation using Stable Diffusion XL as baseline, WaveFT significantly outperforms LoRA and other PEFT methods, especially at low parameter counts; achieving superior subject fidelity, prompt alignment, and image diversity.</li>
</ul>

<h3>Title: Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE</h3>
<ul>
<li><strong>Authors: </strong>Varvara Arzt, Allan Hanbury, Michael Wiegand, Gbor Recski, Terra Blevins</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12533">https://arxiv.org/abs/2505.12533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12533">https://arxiv.org/pdf/2505.12533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12533]] Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE(https://arxiv.org/abs/2505.12533)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Analysing the generalisation capabilities of relation extraction (RE) models is crucial for assessing whether they learn robust relational patterns or rely on spurious correlations. Our cross-dataset experiments find that RE models struggle with unseen data, even within similar domains. Notably, higher intra-dataset performance does not indicate better transferability, instead often signaling overfitting to dataset-specific artefacts. Our results also show that data quality, rather than lexical similarity, is key to robust transfer, and the choice of optimal adaptation strategy depends on the quality of data available: while fine-tuning yields the best cross-dataset performance with high-quality data, few-shot in-context learning (ICL) is more effective with noisier data. However, even in these cases, zero-shot baselines occasionally outperform all cross-dataset results. Structural issues in RE benchmarks, such as single-relation per sample constraints and non-standardised negative class definitions, further hinder model transferability.</li>
</ul>

<h3>Title: ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Adrian Mirza, Nawaf Alampara, Martio Ros-Garca, Mohamed Abdelalim, Jack Butler, Bethany Connolly, Tunca Dogan, Marianna Nezhurina, Bnyamin en, Santosh Tirunagari, Mark Worrall, Adamo Young, Philippe Schwaller, Michael Pieler, Kevin Maik Jablonka</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12534">https://arxiv.org/abs/2505.12534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12534">https://arxiv.org/pdf/2505.12534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12534]] ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models(https://arxiv.org/abs/2505.12534)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Foundation models have shown remarkable success across scientific domains, yet their impact in chemistry remains limited due to the absence of diverse, large-scale, high-quality datasets that reflect the field's multifaceted nature. We present the ChemPile, an open dataset containing over 75 billion tokens of curated chemical data, specifically built for training and evaluating general-purpose models in the chemical sciences. The dataset mirrors the human learning journey through chemistry -- from educational foundations to specialized expertise -- spanning multiple modalities and content types including structured data in diverse chemical representations (SMILES, SELFIES, IUPAC names, InChI, molecular renderings), scientific and educational text, executable code, and chemical images. ChemPile integrates foundational knowledge (textbooks, lecture notes), specialized expertise (scientific articles and language-interfaced data), visual understanding (molecular structures, diagrams), and advanced reasoning (problem-solving traces and code) -- mirroring how human chemists develop expertise through diverse learning materials and experiences. Constructed through hundreds of hours of expert curation, the ChemPile captures both foundational concepts and domain-specific complexity. We provide standardized training, validation, and test splits, enabling robust benchmarking. ChemPile is openly released via HuggingFace with a consistent API, permissive license, and detailed documentation. We hope the ChemPile will serve as a catalyst for chemical AI, enabling the development of the next generation of chemical foundation models.</li>
</ul>

<h3>Title: Harnessing the Universal Geometry of Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Rishi Jha, Collin Zhang, Vitaly Shmatikov, John X. Morris</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12540">https://arxiv.org/abs/2505.12540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12540">https://arxiv.org/pdf/2505.12540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12540]] Harnessing the Universal Geometry of Embeddings(https://arxiv.org/abs/2505.12540)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We introduce the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. Our unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). Our translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets. The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference.</li>
</ul>

<h3>Title: Disambiguation in Conversational Question Answering in the Era of LLM: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Md Mehrab Tanjim, Yeonjun In, Xiang Chen, Victor S. Bursztyn, Ryan A. Rossi, Sungchul Kim, Guang-Jie Ren, Vaishnavi Muppala, Shun Jiang, Yongsung Kim, Chanyoung Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12543">https://arxiv.org/abs/2505.12543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12543">https://arxiv.org/pdf/2505.12543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12543]] Disambiguation in Conversational Question Answering in the Era of LLM: A Survey(https://arxiv.org/abs/2505.12543)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Ambiguity remains a fundamental challenge in Natural Language Processing (NLP) due to the inherent complexity and flexibility of human language. With the advent of Large Language Models (LLMs), addressing ambiguity has become even more critical due to their expanded capabilities and applications. In the context of Conversational Question Answering (CQA), this paper explores the definition, forms, and implications of ambiguity for language driven systems, particularly in the context of LLMs. We define key terms and concepts, categorize various disambiguation approaches enabled by LLMs, and provide a comparative analysis of their advantages and disadvantages. We also explore publicly available datasets for benchmarking ambiguity detection and resolution techniques and highlight their relevance for ongoing research. Finally, we identify open problems and future research directions, proposing areas for further investigation. By offering a comprehensive review of current research on ambiguities and disambiguation with LLMs, we aim to contribute to the development of more robust and reliable language systems.</li>
</ul>

<h3>Title: Alternators With Noise Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad R. Rezaei, Adji Bousso Dieng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12544">https://arxiv.org/abs/2505.12544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12544">https://arxiv.org/pdf/2505.12544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12544]] Alternators With Noise Models(https://arxiv.org/abs/2505.12544)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Alternators have recently been introduced as a framework for modeling time-dependent data. They often outperform other popular frameworks, such as state-space models and diffusion models, on challenging time-series tasks. This paper introduces a new Alternator model, called Alternator++, which enhances the flexibility of traditional Alternators by explicitly modeling the noise terms used to sample the latent and observed trajectories, drawing on the idea of noise models from the diffusion modeling literature. Alternator++ optimizes the sum of the Alternator loss and a noise-matching loss. The latter forces the noise trajectories generated by the two noise models to approximate the noise trajectories that produce the observed and latent trajectories. We demonstrate the effectiveness of Alternator++ in tasks such as density estimation, time series imputation, and forecasting, showing that it outperforms several strong baselines, including Mambas, ScoreGrad, and Dyffusion.</li>
</ul>

<h3>Title: Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhao, Pu Wang, Yibo Zhao, Hongru Du, Hao (Frank)Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12545">https://arxiv.org/abs/2505.12545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12545">https://arxiv.org/pdf/2505.12545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12545]] Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models(https://arxiv.org/abs/2505.12545)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Predicting crash events is crucial for understanding crash distributions and their contributing factors, thereby enabling the design of proactive traffic safety policy interventions. However, existing methods struggle to interpret the complex interplay among various sources of traffic crash data, including numeric characteristics, textual reports, crash imagery, environmental conditions, and driver behavior records. As a result, they often fail to capture the rich semantic information and intricate interrelationships embedded in these diverse data sources, limiting their ability to identify critical crash risk factors. In this research, we propose TrafficSafe, a framework that adapts LLMs to reframe crash prediction and feature attribution as text-based reasoning. A multi-modal crash dataset including 58,903 real-world reports together with belonged infrastructure, environmental, driver, and vehicle information is collected and textualized into TrafficSafe Event Dataset. By customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves a 42% average improvement in F1-score over baselines. To interpret these predictions and uncover contributing factors, we introduce TrafficSafe Attribution, a sentence-level feature attribution framework enabling conditional risk analysis. Findings show that alcohol-impaired driving is the leading factor in severe crashes, with aggressive and impairment-related behaviors having nearly twice the contribution for severe crashes compared to other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal features during model training, guiding strategic crash data collection for iterative performance improvements. The proposed TrafficSafe offers a transformative leap in traffic safety research, providing a blueprint for translating advanced AI technologies into responsible, actionable, and life-saving outcomes.</li>
</ul>

<h3>Title: Extracting memorized pieces of (copyrighted) books from open-weight language models</h3>
<ul>
<li><strong>Authors: </strong>A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho, Percy Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12546">https://arxiv.org/abs/2505.12546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12546">https://arxiv.org/pdf/2505.12546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12546]] Extracting memorized pieces of (copyrighted) books from open-weight language models(https://arxiv.org/abs/2505.12546)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Plaintiffs and defendants in copyright lawsuits over generative AI often make sweeping, opposing claims about the extent to which large language models (LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial ML and copyright law, we show that these polarized positions dramatically oversimplify the relationship between memorization and copyright. To do so, we leverage a recent probabilistic extraction technique to extract pieces of the Books3 dataset from 13 open-weight LLMs. Through numerous experiments, we show that it's possible to extract substantial parts of at least some books from different LLMs. This is evidence that the LLMs have memorized the extracted text; this memorized content is copied inside the model parameters. But the results are complicated: the extent of memorization varies both by model and by book. With our specific experiments, we find that the largest LLMs don't memorize most books -- either in whole or in part. However, we also find that Llama 3.1 70B memorizes some books, like Harry Potter and 1984, almost entirely. We discuss why our results have significant implications for copyright cases, though not ones that unambiguously favor either side.</li>
</ul>

<h3>Title: ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations</h3>
<ul>
<li><strong>Authors: </strong>Florent Chiaroni, Ali Ayub, Ola Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12547">https://arxiv.org/abs/2505.12547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12547">https://arxiv.org/pdf/2505.12547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12547]] ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations(https://arxiv.org/abs/2505.12547)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In robotics applications, few-shot segmentation is crucial because it allows robots to perform complex tasks with minimal training data, facilitating their adaptation to diverse, real-world environments. However, pixel-level annotations of even small amount of images is highly time-consuming and costly. In this paper, we present a novel few-shot binary segmentation method based on bounding-box annotations instead of pixel-level labels. We introduce, ProMi, an efficient prototype-mixture-based method that treats the background class as a mixture of distributions. Our approach is simple, training-free, and effective, accommodating coarse annotations with ease. Compared to existing baselines, ProMi achieves the best results across different datasets with significant gains, demonstrating its effectiveness. Furthermore, we present qualitative experiments tailored to real-world mobile robot tasks, demonstrating the applicability of our approach in such scenarios. Our code: this https URL.</li>
</ul>

<h3>Title: Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers</h3>
<ul>
<li><strong>Authors: </strong>Taniya Kapoor, Abhishek Chandra, Anastasios Stamou, Stephen J Roberts</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12556">https://arxiv.org/abs/2505.12556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12556">https://arxiv.org/pdf/2505.12556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12556]] Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers(https://arxiv.org/abs/2505.12556)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-world systems, from aerospace to railway engineering, are modeled with partial differential equations (PDEs) describing the physics of the system. Estimating robust solutions for such problems is essential. Deep learning-based architectures, such as neural PDE solvers, have recently gained traction as a reliable solution method. The current state of development of these approaches, however, primarily focuses on improving accuracy. The environmental impact of excessive computation, leading to increased carbon emissions, has largely been overlooked. This paper introduces a carbon emission measure for a range of PDE solvers. Our proposed metric, EcoL2, balances model accuracy with emissions across data collection, model training, and deployment. Experiments across both physics-informed machine learning and operator learning architectures demonstrate that the proposed metric presents a holistic assessment of model performance and emission cost. As such solvers grow in scale and deployment, EcoL2 represents a step toward building performant scientific machine learning systems with lower long-term environmental impact.</li>
</ul>

<h3>Title: HybridServe: Efficient Serving of Large AI Models with Confidence-Based Cascade Routing</h3>
<ul>
<li><strong>Authors: </strong>Leyang Xue, Yao Fu, Luo Mai, Mahesh K. Marina</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12566">https://arxiv.org/abs/2505.12566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12566">https://arxiv.org/pdf/2505.12566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12566]] HybridServe: Efficient Serving of Large AI Models with Confidence-Based Cascade Routing(https://arxiv.org/abs/2505.12566)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Giant Deep Neural Networks (DNNs), have become indispensable for accurate and robust support of large-scale cloud based AI services. However, serving giant DNNs is prohibitively expensive from an energy consumption viewpoint easily exceeding that of training, due to the enormous scale of GPU clusters needed to hold giant DNN model partitions and replicas. Existing approaches can either optimize energy efficiency or inference accuracy but not both. To overcome this status quo, we propose HybridServe, a novel hybrid DNN model serving system that leverages multiple sized versions (small to giant) of the model to be served in tandem. Through a confidence based hybrid model serving dataflow, HybridServe prefers to serve inference requests with energy-efficient smaller models so long as accuracy is not compromised, thereby reducing the number of replicas needed for giant DNNs. HybridServe also features a dataflow planner for efficient partitioning and replication of candidate models to maximize serving system throughput. Experimental results using a prototype implementation of HybridServe show that it reduces energy footprint by up to 19.8x compared to the state-of-the-art DNN model serving systems while matching the accuracy of serving solely with giant DNNs.</li>
</ul>

<h3>Title: A Survey of Attacks on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenrui Xu, Keshab K. Parhi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12567">https://arxiv.org/abs/2505.12567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12567">https://arxiv.org/pdf/2505.12567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12567]] A Survey of Attacks on Large Language Models(https://arxiv.org/abs/2505.12567)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) and LLM-based agents have been widely deployed in a wide range of applications in the real world, including healthcare diagnostics, financial analysis, customer support, robotics, and autonomous driving, expanding their powerful capability of understanding, reasoning, and generating natural languages. However, the wide deployment of LLM-based applications exposes critical security and reliability risks, such as the potential for malicious misuse, privacy leakage, and service disruption that weaken user trust and undermine societal safety. This paper provides a systematic overview of the details of adversarial attacks targeting both LLMs and LLM-based agents. These attacks are organized into three phases in LLMs: Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity Attacks. For each phase, we analyze the details of representative and recently introduced attack methods along with their corresponding defenses. We hope our survey will provide a good tutorial and a comprehensive understanding of LLM security, especially for attacks on LLMs. We desire to raise attention to the risks inherent in widely deployed LLM-based applications and highlight the urgent need for robust mitigation strategies for evolving threats.</li>
</ul>

<h3>Title: Enriching Patent Claim Generation with European Patent Dataset</h3>
<ul>
<li><strong>Authors: </strong>Lekang Jiang, Chengzu Li, Stephan Goetz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12568">https://arxiv.org/abs/2505.12568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12568">https://arxiv.org/pdf/2505.12568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12568]] Enriching Patent Claim Generation with European Patent Dataset(https://arxiv.org/abs/2505.12568)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Drafting patent claims is time-intensive, costly, and requires professional skill. Therefore, researchers have investigated large language models (LLMs) to assist inventors in writing claims. However, existing work has largely relied on datasets from the United States Patent and Trademark Office (USPTO). To enlarge research scope regarding various jurisdictions, drafting conventions, and legal standards, we introduce EPD, a European patent dataset. EPD presents rich textual data and structured metadata to support multiple patent-related tasks, including claim generation. This dataset enriches the field in three critical aspects: (1) Jurisdictional diversity: Patents from different offices vary in legal and drafting conventions. EPD fills a critical gap by providing a benchmark for European patents to enable more comprehensive evaluation. (2) Quality improvement: EPD offers high-quality granted patents with finalized and legally approved texts, whereas others consist of patent applications that are unexamined or provisional. Experiments show that LLMs fine-tuned on EPD significantly outperform those trained on previous datasets and even GPT-4o in claim quality and cross-domain generalization. (3) Real-world simulation: We propose a difficult subset of EPD to better reflect real-world challenges of claim generation. Results reveal that all tested LLMs perform substantially worse on these challenging samples, which highlights the need for future research.</li>
</ul>

<h3>Title: Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio</h3>
<ul>
<li><strong>Authors: </strong>Hanwen Shen, Ting Ying</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12572">https://arxiv.org/abs/2505.12572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12572">https://arxiv.org/pdf/2505.12572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12572]] Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio(https://arxiv.org/abs/2505.12572)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Writing novels with Large Language Models (LLMs) raises a critical question: how much human-authored outline is necessary to generate high-quality million-word novels? While frameworks such as DOME, Plan&Write, and Long Writer have improved stylistic coherence and logical consistency, they primarily target shorter novels (10k--100k words), leaving ultra-long generation largely unexplored. Drawing on insights from recent text compression methods like LLMZip and LLM2Vec, we conduct an information-theoretic analysis that quantifies distortion occurring when LLMs compress and reconstruct ultra-long novels under varying compression-expansion ratios. We introduce a hierarchical two-stage generation pipeline (outline -> detailed outline -> manuscript) and find an optimal outline length that balances information preservation with human effort. Through extensive experimentation with Chinese novels, we establish that a two-stage hierarchical outline approach significantly reduces semantic distortion compared to single-stage methods. Our findings provide empirically-grounded guidance for authors and researchers collaborating with LLMs to create million-word novels.</li>
</ul>

<h3>Title: Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID</h3>
<ul>
<li><strong>Authors: </strong>Priyank Pathak, Yogesh S Rawat</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12580">https://arxiv.org/abs/2505.12580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12580">https://arxiv.org/pdf/2505.12580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12580]] Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID(https://arxiv.org/abs/2505.12580)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric</a></li>
<li><strong>Abstract: </strong>This work focuses on Clothes Changing Re-IDentification (CC-ReID) for the real world. Existing works perform well with high-quality (HQ) images, but struggle with low-quality (LQ) where we can have artifacts like pixelation, out-of-focus blur, and motion blur. These artifacts introduce noise to not only external biometric attributes (e.g. pose, body shape, etc.) but also corrupt the model's internal feature representation. Models usually cluster LQ image features together, making it difficult to distinguish between them, leading to incorrect matches. We propose a novel framework Robustness against Low-Quality (RLQ) to improve CC-ReID model on real-world data. RLQ relies on Coarse Attributes Prediction (CAP) and Task Agnostic Distillation (TAD) operating in alternate steps in a novel training mechanism. CAP enriches the model with external fine-grained attributes via coarse predictions, thereby reducing the effect of noisy inputs. On the other hand, TAD enhances the model's internal feature representation by bridging the gap between HQ and LQ features, via an external dataset through task-agnostic self-supervision and distillation. RLQ outperforms the existing approaches by 1.6%-2.9% Top-1 on real-world datasets like LaST, and DeepChange, while showing consistent improvement of 5.3%-6% Top-1 on PRCC with competitive performance on LTCC. *The code will be made public soon.*</li>
</ul>

<h3>Title: An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification</h3>
<ul>
<li><strong>Authors: </strong>Lucas M. Dorneles, Luan Fonseca Garcia, Joel Lus Carbonera</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12581">https://arxiv.org/abs/2505.12581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12581">https://arxiv.org/pdf/2505.12581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12581]] An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification(https://arxiv.org/abs/2505.12581)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural networks have become increasingly popular in the last few years as an effective tool for the task of image classification due to the impressive performance they have achieved on this task. In image classification tasks, it is common to use data augmentation strategies to increase the robustness of trained networks to changes in the input images and to avoid overfitting. Although data augmentation is a widely adopted technique, the literature lacks a body of research analyzing the effects data augmentation methods have on the patterns learned by neural network models working on complex datasets. The primary objective of this work is to propose a methodology and set of metrics that may allow a quantitative approach to analyzing the effects of data augmentation in convolutional networks applied to image classification. An important tool used in the proposed approach lies in the concept of class activation maps for said models, which allow us to identify and measure the importance these models assign to each individual pixel in an image when executing the classification task. From these maps, we may then extract metrics over the similarities and differences between maps generated by these models trained on a given dataset with different data augmentation strategies. Experiments made using this methodology suggest that the effects of these data augmentation techniques not only can be analyzed in this way but also allow us to identify different impact profiles over the trained models.</li>
</ul>

<h3>Title: Compile-Time Fully Homomorphic Encryption: Eliminating Online Encryption via Algebraic Basis Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Dongfang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12582">https://arxiv.org/abs/2505.12582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12582">https://arxiv.org/pdf/2505.12582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12582]] Compile-Time Fully Homomorphic Encryption: Eliminating Online Encryption via Algebraic Basis Synthesis(https://arxiv.org/abs/2505.12582)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We propose a new framework for compile-time ciphertext synthesis in fully homomorphic encryption (FHE) systems. Instead of invoking encryption algorithms at runtime, our method synthesizes ciphertexts from precomputed encrypted basis vectors using only homomorphic additions, scalar multiplications, and randomized encryptions of zero. This decouples ciphertext generation from encryption, and enables efficient batch encoding through algebraic reuse. We formalize this technique as a randomized module morphism and prove that it satisfies IND-CPA security. Our proof uses a hybrid game framework that interpolates between encrypted vector instances and reduces adversarial advantage to the indistinguishability of the underlying FHE scheme. This reduction structure captures the security implications of ciphertext basis reuse and structured noise injection. The proposed synthesis primitive supports fast, encryption-free ingestion in outsourced database systems and other high-throughput FHE pipelines. It is compatible with standard FHE APIs and preserves layout semantics for downstream homomorphic operations.</li>
</ul>

<h3>Title: Improving Multilingual Language Models by Aligning Representations through Steering</h3>
<ul>
<li><strong>Authors: </strong>Omar Mahmoud, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12584">https://arxiv.org/abs/2505.12584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12584">https://arxiv.org/pdf/2505.12584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12584]] Improving Multilingual Language Models by Aligning Representations through Steering(https://arxiv.org/abs/2505.12584)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate how large language models (LLMS) process non-English tokens within their layer representations, an open question despite significant advancements in the field. Using representation steering, specifically by adding a learned vector to a single model layer's activations, we demonstrate that steering a single model layer can notably enhance performance. Our analysis shows that this approach achieves results comparable to translation baselines and surpasses state of the art prompt optimization methods. Additionally, we highlight how advanced techniques like supervised fine tuning (\textsc{sft}) and reinforcement learning from human feedback (\textsc{rlhf}) improve multilingual capabilities by altering representation spaces. We further illustrate how these methods align with our approach to reshaping LLMS layer representations.</li>
</ul>

<h3>Title: Learning Robust Spectral Dynamics for Temporal Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>En Yu, Jie Lu, Xiaoyu Yang, Guangquan Zhang, Zhen Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12585">https://arxiv.org/abs/2505.12585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12585">https://arxiv.org/pdf/2505.12585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12585]] Learning Robust Spectral Dynamics for Temporal Domain Generalization(https://arxiv.org/abs/2505.12585)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modern machine learning models struggle to maintain performance in dynamic environments where temporal distribution shifts, \emph{i.e., concept drift}, are prevalent. Temporal Domain Generalization (TDG) seeks to enable model generalization across evolving domains, yet existing approaches typically assume smooth incremental changes, struggling with complex real-world drifts involving long-term structure (incremental evolution/periodicity) and local uncertainties. To overcome these limitations, we introduce FreKoo, which tackles these challenges via a novel frequency-domain analysis of parameter trajectories. It leverages the Fourier transform to disentangle parameter evolution into distinct spectral bands. Specifically, low-frequency component with dominant dynamics are learned and extrapolated using the Koopman operator, robustly capturing diverse drift patterns including both incremental and periodicity. Simultaneously, potentially disruptive high-frequency variations are smoothed via targeted temporal regularization, preventing overfitting to transient noise and domain uncertainties. In addition, this dual spectral strategy is rigorously grounded through theoretical analysis, providing stability guarantees for the Koopman prediction, a principled Bayesian justification for the high-frequency regularization, and culminating in a multiscale generalization bound connecting spectral dynamics to improved generalization. Extensive experiments demonstrate FreKoo's significant superiority over SOTA TDG approaches, particularly excelling in real-world streaming scenarios with complex drifts and uncertainties.</li>
</ul>

<h3>Title: A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection</h3>
<ul>
<li><strong>Authors: </strong>Sanggeon Yun, Ryozo Masukawa, Hyunwoo Oh, Nathaniel D. Bastian, Mohsen Imani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12586">https://arxiv.org/abs/2505.12586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12586">https://arxiv.org/pdf/2505.12586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12586]] A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection(https://arxiv.org/abs/2505.12586)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) are highly susceptible to adversarial examples--subtle, imperceptible perturbations that can lead to incorrect predictions. While detection-based defenses offer a practical alternative to adversarial training, many existing methods depend on external models, complex architectures, heavy augmentations, or adversarial data, limiting their efficiency and generalizability. We introduce a lightweight, plug-in detection framework that leverages internal layer-wise inconsistencies within the target model itself, requiring only benign data for calibration. Our approach is grounded in the A Few Large Shifts Assumption, which posits that adversarial perturbations typically induce large representation shifts in a small subset of layers. Building on this, we propose two complementary strategies--Recovery Testing (RT) and Logit-layer Testing (LT)--to expose internal disruptions caused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under both standard and adaptive threat models, our method achieves state-of-the-art detection performance with negligible computational overhead and no compromise to clean accuracy.</li>
</ul>

<h3>Title: CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Aditeya Baral, Allen George Ajith, Roshan Nayak, Mrityunjay Abhijeet Bhanja</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12587">https://arxiv.org/abs/2505.12587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12587">https://arxiv.org/pdf/2505.12587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12587]] CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling(https://arxiv.org/abs/2505.12587)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Code-mixed languages, characterized by frequent within-sentence language transitions, present structural challenges that standard language models fail to address. In this work, we propose CMLFormer, an enhanced multi-layer dual-decoder Transformer with a shared encoder and synchronized decoder cross-attention, designed to model the linguistic and semantic dynamics of code-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with switching point and translation annotations with multiple new objectives specifically aimed at capturing switching behavior, cross-lingual structure, and code-mixing complexity. Our experiments show that CMLFormer improves F1 score, precision, and accuracy over other approaches on the HASOC-2021 benchmark under select pre-training setups. Attention analyses further show that it can identify and attend to switching points, validating its sensitivity to code-mixed structure. These results demonstrate the effectiveness of CMLFormer's architecture and multi-task pre-training strategy for modeling code-mixed languages.</li>
</ul>

<h3>Title: PromptPrism: A Linguistically-Inspired Taxonomy for Prompts</h3>
<ul>
<li><strong>Authors: </strong>Sullam Jeoung, Yueyan Chen, Yi Zhang, Shuai Wang, Haibo Ding, Lin Lee Cheong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12592">https://arxiv.org/abs/2505.12592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12592">https://arxiv.org/pdf/2505.12592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12592]] PromptPrism: A Linguistically-Inspired Taxonomy for Prompts(https://arxiv.org/abs/2505.12592)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompts are the interface for eliciting the capabilities of large language models (LLMs). Understanding their structure and components is critical for analyzing LLM behavior and optimizing performance. However, the field lacks a comprehensive framework for systematic prompt analysis and understanding. We introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt analysis across three hierarchical levels: functional structure, semantic component, and syntactic pattern. We show the practical utility of PromptPrism by applying it to three applications: (1) a taxonomy-guided prompt refinement approach that automatically improves prompt quality and enhances model performance across a range of tasks; (2) a multi-dimensional dataset profiling method that extracts and aggregates structural, semantic, and syntactic characteristics from prompt datasets, enabling comprehensive analysis of prompt distributions and patterns; (3) a controlled experimental framework for prompt sensitivity analysis by quantifying the impact of semantic reordering and delimiter modifications on LLM performance. Our experimental results validate the effectiveness of our taxonomy across these applications, demonstrating that PromptPrism provides a foundation for refining, profiling, and analyzing prompts.</li>
</ul>

<h3>Title: Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex Learned Routers</h3>
<ul>
<li><strong>Authors: </strong>Yang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12601">https://arxiv.org/abs/2505.12601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12601">https://arxiv.org/pdf/2505.12601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12601]] Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex Learned Routers(https://arxiv.org/abs/2505.12601)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) grow in scale and specialization, routing--selecting the best model for a given input--has become essential for efficient and effective deployment. While recent methods rely on complex learned routing strategies, their dependence on disparate training data and evaluation setups makes comparison and generalization difficult. In this work, we revisit LLM routing through the lens of simplicity. We show that a well-tuned k-Nearest Neighbors (kNN) approach not only matches but often outperforms state-of-the-art learned routers across diverse tasks. To support systematic evaluation, we introduce a suite of standardized routing benchmarks spanning instruction-following, question-answering, and reasoning tasks, as well as the first multi-modal routing dataset involving visual inputs. Our findings reveal that the locality properties of model performance in embedding space enable simple non-parametric methods to achieve strong routing decisions with lower sample complexity than parametric approaches. This challenges the prevailing trend toward sophisticated architectures and highlights the importance of thoroughly evaluating simple baselines before investing in complex solutions. To support reproducibility and further exploration, we will release all benchmarks and code upon publication.</li>
</ul>

<h3>Title: Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Thong Nguyen, Zhiyuan Hu, Xu Lin, Cong-Duy Nguyen, See-Kiong Ng, Luu Anh Tuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12605">https://arxiv.org/abs/2505.12605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12605">https://arxiv.org/pdf/2505.12605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12605]] Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding(https://arxiv.org/abs/2505.12605)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed outstanding advances of large vision-language models (LVLMs). In order to tackle video understanding, most of them depend upon their implicit temporal understanding capacity. As such, they have not deciphered important components that contribute to temporal understanding ability, which might limit the potential of these LVLMs for video understanding. In this work, we conduct a thorough empirical study to demystify crucial components that influence the temporal understanding of LVLMs. Our empirical study reveals that significant impacts are centered around the intermediate interface between the visual encoder and the large language model. Building on these insights, we propose a temporal-oriented recipe that encompasses temporal-oriented training schemes and an upscaled interface. Our final model developed using our recipe significantly enhances previous LVLMs on standard video understanding tasks.</li>
</ul>

<h3>Title: Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Xuan, Zechao Li, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12606">https://arxiv.org/abs/2505.12606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12606">https://arxiv.org/pdf/2505.12606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12606]] Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking(https://arxiv.org/abs/2505.12606)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Multi-modal object tracking integrates auxiliary modalities such as depth, thermal infrared, event flow, and language to provide additional information beyond RGB images, showing great potential in improving tracking stabilization in complex scenarios. Existing methods typically start from an RGB-based tracker and learn to understand auxiliary modalities only from training data. Constrained by the limited multi-modal training data, the performance of these methods is unsatisfactory. To alleviate this limitation, this work proposes a unified multi-modal tracker Diff-MM by exploiting the multi-modal understanding capability of the pre-trained text-to-image generation model. Diff-MM leverages the UNet of pre-trained Stable Diffusion as a tracking feature extractor through the proposed parallel feature extraction pipeline, which enables pairwise image inputs for object tracking. We further introduce a multi-modal sub-module tuning method that learns to gain complementary information between different modalities. By harnessing the extensive prior knowledge in the generation model, we achieve a unified tracker with uniform parameters for RGB-N/D/T/E tracking. Experimental results demonstrate the promising performance of our method compared with recently proposed trackers, e.g., its AUC outperforms OneTracker by 8.3% on TNL2K.</li>
</ul>

<h3>Title: hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced Security and Privacy</h3>
<ul>
<li><strong>Authors: </strong>Musharraf Alruwaill, Saraju Mohanty, Elias Kougianos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12610">https://arxiv.org/abs/2505.12610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12610">https://arxiv.org/pdf/2505.12610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12610]] hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced Security and Privacy(https://arxiv.org/abs/2505.12610)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Concerns regarding privacy and data security in conventional healthcare prompted alternative technologies. In smart healthcare, blockchain technology addresses existing concerns with security, privacy, and electronic healthcare transmission. Integration of Blockchain Technology with the Internet of Medical Things (IoMT) allows real-time monitoring of protected healthcare data. Utilizing edge devices with IoMT devices is very advantageous for addressing security, computing, and storage challenges. Encryption using symmetric and asymmetric keys is used to conceal sensitive information from unauthorized parties. SHA256 is an algorithm for one-way hashing. It is used to verify that the data has not been altered, since if it had, the hash value would have changed. This article offers a blockchain-based smart healthcare system using IoMT devices for continuous patient monitoring. In addition, it employs edge resources in addition to IoMT devices to have extra computing power and storage to hash and encrypt incoming data before sending it to the blockchain. Symmetric key is utilized to keep the data private even in the blockchain, allowing the patient to safely communicate the data through smart contracts while preventing unauthorized physicians from seeing the data. Through the use of a verification node and blockchain, an asymmetric key is used for the signing and validation of patient data in the healthcare provider system. In addition to other security measures, location-based authentication is recommended to guarantee that data originates from the patient area. Through the edge device, SHA256 is utilized to secure the data's integrity and a secret key is used to maintain its secrecy. The hChain architecture improves the computing power of IoMT environments, the security of EHR sharing through smart contracts, and the privacy and authentication procedures.</li>
</ul>

<h3>Title: EPSpatial: Achieving Efficient and Private Statistical Analytics of Geospatial Data</h3>
<ul>
<li><strong>Authors: </strong>Chuan Zhang, Xuhao Ren, Zhangcheng Huang, Jinwen Liang, Jianzong Wang, Liehuang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12612">https://arxiv.org/abs/2505.12612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12612">https://arxiv.org/pdf/2505.12612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12612]] EPSpatial: Achieving Efficient and Private Statistical Analytics of Geospatial Data(https://arxiv.org/abs/2505.12612)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Geospatial data statistics involve the aggregation and analysis of location data to derive the distribution of clients within geospatial. The need for privacy protection in geospatial data analysis has become paramount due to concerns over the misuse or unauthorized access of client location information. However, existing private geospatial data statistics mainly rely on privacy computing techniques such as cryptographic tools and differential privacy, which leads to significant overhead and inaccurate results. In practical applications, geospatial data is frequently generated by mobile devices such as smartphones and IoT sensors. The continuous mobility of clients and the need for real-time updates introduce additional complexity. To address these issues, we first design \textit{spatially distributed point functions (SDPF)}, which combines a quad-tree structure with distributed point functions, allowing clients to succinctly secret-share values on the nodes of an exponentially large quad-tree. Then, we use Gray code to partition the region and combine SDPF with it to propose $\mathtt{EPSpatial}$, a scheme for accurate, efficient, and private statistical analytics of geospatial data. Moreover, considering clients' frequent movement requires continuous location updates, we leverage the region encoding property to present an efficient update this http URL analysis shows that $\mathtt{EPSpatial}$ effectively protects client location privacy. Theoretical analysis and experimental results on real datasets demonstrate that $\mathtt{EPSpatial}$ reduces computational and communication overhead by at least $50\%$ compared to existing statistical schemes.</li>
</ul>

<h3>Title: Towards Centralized Orchestration of Cyber Protection Condition (CPCON)</h3>
<ul>
<li><strong>Authors: </strong>Mark Timmons, Daniel Lukaszewski, Geoffrey Xie, Thomas Mayo, Donald McCanless</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12613">https://arxiv.org/abs/2505.12613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12613">https://arxiv.org/pdf/2505.12613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12613]] Towards Centralized Orchestration of Cyber Protection Condition (CPCON)(https://arxiv.org/abs/2505.12613)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>The United States Cyber Command (USCYBERCOM) Cyber Protection Condition (CPCON) framework mandates graduated security postures across Department of Defense (DoD) networks, but current implementation remains largely manual, inconsistent, and error-prone. This paper presents a prototype system for centralized orchestration of CPCON directives, enabling automated policy enforcement and real-time threat response across heterogeneous network environments. Building on prior work in host-based intrusion response, our system leverages a policy-driven orchestrator to standardize security actions, isolate compromised subnets, and verify enforcement status. We validate the system through emulated attack scenarios, demonstrating improved speed, accuracy, and verifiability in CPCON transitions with human-in-the-loop oversight.</li>
</ul>

<h3>Title: Adaptive Graph Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Ding, Yan Wang, Guanfeng Liu, Jiajie Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12614">https://arxiv.org/abs/2505.12614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12614">https://arxiv.org/pdf/2505.12614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12614]] Adaptive Graph Unlearning(https://arxiv.org/abs/2505.12614)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Graph unlearning, which deletes graph elements such as nodes and edges from trained graph neural networks (GNNs), is crucial for real-world applications where graph data may contain outdated, inaccurate, or privacy-sensitive information. However, existing methods often suffer from (1) incomplete or over unlearning due to neglecting the distinct objectives of different unlearning tasks, and (2) inaccurate identification of neighbors affected by deleted elements across various GNN architectures. To address these limitations, we propose AGU, a novel Adaptive Graph Unlearning framework that flexibly adapts to diverse unlearning tasks and GNN architectures. AGU ensures the complete forgetting of deleted elements while preserving the integrity of the remaining graph. It also accurately identifies affected neighbors for each GNN architecture and prioritizes important ones to enhance unlearning performance. Extensive experiments on seven real-world graphs demonstrate that AGU outperforms existing methods in terms of effectiveness, efficiency, and unlearning capability.</li>
</ul>

<h3>Title: BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation</h3>
<ul>
<li><strong>Authors: </strong>Haiquan Wen, Yiwei He, Zhenglin Huang, Tianxiao Li, Zihan YU, Xingru Huang, Lu Qi, Baoyuan Wu, Xiangtai Li, Guangliang Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12620">https://arxiv.org/abs/2505.12620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12620">https://arxiv.org/pdf/2505.12620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12620]] BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation(https://arxiv.org/abs/2505.12620)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Advances in AI generative models facilitate super-realistic video synthesis, amplifying misinformation risks via social media and eroding trust in digital content. Several research works have explored new deepfake detection methods on AI-generated images to alleviate these risks. However, with the fast development of video generation models, such as Sora and WanX, there is currently a lack of large-scale, high-quality AI-generated video datasets for forgery detection. In addition, existing detection approaches predominantly treat the task as binary classification, lacking explainability in model decision-making and failing to provide actionable insights or guidance for the public. To address these challenges, we propose \textbf{GenBuster-200K}, a large-scale AI-generated video dataset featuring 200K high-resolution video clips, diverse latest generative techniques, and real-world scenes. We further introduce \textbf{BusterX}, a novel AI-generated video detection and explanation framework leveraging multimodal large language model (MLLM) and reinforcement learning for authenticity determination and explainable rationale. To our knowledge, GenBuster-200K is the {\it \textbf{first}} large-scale, high-quality AI-generated video dataset that incorporates the latest generative techniques for real-world scenarios. BusterX is the {\it \textbf{first}} framework to integrate MLLM with reinforcement learning for explainable AI-generated video detection. Extensive comparisons with state-of-the-art methods and ablation studies validate the effectiveness and generalizability of BusterX. The code, models, and datasets will be released.</li>
</ul>

<h3>Title: Think Before You Attribute: Improving the Performance of LLMs Attribution Systems</h3>
<ul>
<li><strong>Authors: </strong>Joo Eduardo Batista, Emil Vatai, Mohamed Wahib</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12621">https://arxiv.org/abs/2505.12621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12621">https://arxiv.org/pdf/2505.12621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12621]] Think Before You Attribute: Improving the Performance of LLMs Attribution Systems(https://arxiv.org/abs/2505.12621)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly applied in various science domains, yet their broader adoption remains constrained by a critical challenge: the lack of trustworthy, verifiable outputs. Current LLMs often generate answers without reliable source attribution, or worse, with incorrect attributions, posing a barrier to their use in scientific and high-stakes settings, where traceability and accountability are non-negotiable. To be reliable, attribution systems need high accuracy and retrieve data with short lengths, i.e., attribute to a sentence within a document rather than a whole document. We propose a sentence-level pre-attribution step for Retrieve-Augmented Generation (RAG) systems that classify sentences into three categories: not attributable, attributable to a single quote, and attributable to multiple quotes. By separating sentences before attribution, a proper attribution method can be selected for the type of sentence, or the attribution can be skipped altogether. Our results indicate that classifiers are well-suited for this task. In this work, we propose a pre-attribution step to reduce the computational complexity of attribution, provide a clean version of the HAGRID dataset, and provide an end-to-end attribution system that works out of the box.</li>
</ul>

<h3>Title: R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model</h3>
<ul>
<li><strong>Authors: </strong>Ali Naseh, Harsh Chaudhari, Jaechul Roh, Mingshi Wu, Alina Oprea, Amir Houmansadr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12625">https://arxiv.org/abs/2505.12625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12625">https://arxiv.org/pdf/2505.12625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12625]] R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model(https://arxiv.org/abs/2505.12625)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>DeepSeek recently released R1, a high-performing large language model (LLM) optimized for reasoning tasks. Despite its efficient training pipeline, R1 achieves competitive performance, even surpassing leading reasoning models like OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1 refuses to answer certain prompts related to politically sensitive topics in China. While existing LLMs often implement safeguards to avoid generating harmful or offensive outputs, R1 represents a notable shift - exhibiting censorship-like behavior on politically charged queries. In this paper, we investigate this phenomenon by first introducing a large-scale set of heavily curated prompts that get censored by R1, covering a range of politically sensitive topics, but are not censored by other models. We then conduct a comprehensive analysis of R1's censorship patterns, examining their consistency, triggers, and variations across topics, prompt phrasing, and context. Beyond English-language queries, we explore censorship behavior in other languages. We also investigate the transferability of censorship to models distilled from the R1 language model. Finally, we propose techniques for bypassing or removing this censorship. Our findings reveal possible additional censorship integration likely shaped by design choices during training or alignment, raising concerns about transparency, bias, and governance in language model deployment.</li>
</ul>

<h3>Title: Dual-Agent Reinforcement Learning for Automated Feature Generation</h3>
<ul>
<li><strong>Authors: </strong>Wanfu Gao, Zengyao Man, Hanlin Pan, Kunpeng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12628">https://arxiv.org/abs/2505.12628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12628">https://arxiv.org/pdf/2505.12628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12628]] Dual-Agent Reinforcement Learning for Automated Feature Generation(https://arxiv.org/abs/2505.12628)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Feature generation involves creating new features from raw data to capture complex relationships among the original features, improving model robustness and machine learning performance. Current methods using reinforcement learning for feature generation have made feature exploration more flexible and efficient. However, several challenges remain: first, during feature expansion, a large number of redundant features are generated. When removing them, current methods only retain the best features each round, neglecting those that perform poorly initially but could improve later. Second, the state representation used by current methods fails to fully capture complex feature relationships. Third, there are significant differences between discrete and continuous features in tabular data, requiring different operations for each type. To address these challenges, we propose a novel dual-agent reinforcement learning method for feature generation. Two agents are designed: the first generates new features, and the second determines whether they should be preserved. A self-attention mechanism enhances state representation, and diverse operations distinguish interactions between discrete and continuous features. The experimental results on multiple datasets demonstrate that the proposed method is effective. The code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Latent Computation in Transformers with Latent Tokens</h3>
<ul>
<li><strong>Authors: </strong>Yuchang Sun, Yanxi Chen, Yaliang Li, Bolin Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12629">https://arxiv.org/abs/2505.12629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12629">https://arxiv.org/pdf/2505.12629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12629]] Enhancing Latent Computation in Transformers with Latent Tokens(https://arxiv.org/abs/2505.12629)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Augmenting large language models (LLMs) with auxiliary tokens has emerged as a promising strategy for enhancing model performance. In this work, we introduce a lightweight method termed latent tokens; these are dummy tokens that may be non-interpretable in natural language but steer the autoregressive decoding process of a Transformer-based LLM via the attention mechanism. The proposed latent tokens can be seamlessly integrated with a pre-trained Transformer, trained in a parameter-efficient manner, and applied flexibly at inference time, while adding minimal complexity overhead to the existing infrastructure of standard Transformers. We propose several hypotheses about the underlying mechanisms of latent tokens and design synthetic tasks accordingly to verify them. Numerical results confirm that the proposed method noticeably outperforms the baselines, particularly in the out-of-distribution generalization scenarios, highlighting its potential in improving the adaptability of LLMs.</li>
</ul>

<h3>Title: Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents</h3>
<ul>
<li><strong>Authors: </strong>Yunseok Jang, Yeda Song, Sungryull Sohn, Lajanugen Logeswaran, Tiange Luo, Dong-Ki Kim, Kyunghoon Bae, Honglak Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12632">https://arxiv.org/abs/2505.12632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12632">https://arxiv.org/pdf/2505.12632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12632]] Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents(https://arxiv.org/abs/2505.12632)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have sparked significant interest in developing GUI visual agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from YouTube), a large-scale dataset of 313K annotated frames from 20K instructional videos capturing diverse real-world mobile OS navigation across multiple platforms. Models that include MONDAY in their pre-training phases demonstrate robust cross-platform generalization capabilities, consistently outperforming models trained on existing single OS datasets while achieving an average performance gain of 18.11%p on an unseen mobile OS platform. To enable continuous dataset expansion as mobile platforms evolve, we present an automated framework that leverages publicly available video content to create comprehensive task datasets without manual annotation. Our framework comprises robust OCR-based scene detection (95.04% F1score), near-perfect UI element detection (99.87% hit ratio), and novel multi-step action identification to extract reliable action sequences across diverse interface configurations. We contribute both the MONDAY dataset and our automated collection framework to facilitate future research in mobile OS navigation.</li>
</ul>

<h3>Title: MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control</h3>
<ul>
<li><strong>Authors: </strong>Mingqi Shao, Feng Xiong, Zhaoxu Sun, Mu Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12635">https://arxiv.org/abs/2505.12635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12635">https://arxiv.org/pdf/2505.12635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12635]] MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control(https://arxiv.org/abs/2505.12635)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, significant advances have been made in 3D object generation. Building upon the generated geometry, current pipelines typically employ image diffusion models to generate multi-view RGB images, followed by UV texture reconstruction through texture baking. While 3D geometry generation has improved significantly, supported by multiple open-source frameworks, 3D texture generation remains underexplored. In this work, we systematically investigate 3D texture generation through the lens of three core dimensions: reference-texture alignment, geometry-texture consistency, and local texture quality. To tackle these issues, we propose MVPainter, which employs data filtering and augmentation strategies to enhance texture fidelity and detail, and introduces ControlNet-based geometric conditioning to improve texture-geometry alignment. Furthermore, we extract physically-based rendering (PBR) attributes from the generated views to produce PBR meshes suitable for real-world rendering applications. MVPainter achieves state-of-the-art results across all three dimensions, as demonstrated by human-aligned evaluations. To facilitate further research and reproducibility, we also release our full pipeline as an open-source system, including data construction, model architecture, and evaluation tools.</li>
</ul>

<h3>Title: Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing</h3>
<ul>
<li><strong>Authors: </strong>Jiakuan Xie, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12636">https://arxiv.org/abs/2505.12636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12636">https://arxiv.org/pdf/2505.12636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12636]] Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing(https://arxiv.org/abs/2505.12636)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Knowledge editing, which aims to update the knowledge encoded in language models, can be deceptive. Despite the fact that many existing knowledge editing algorithms achieve near-perfect performance on conventional metrics, the models edited by them are still prone to generating original knowledge. This paper introduces the concept of "superficial editing" to describe this phenomenon. Our comprehensive evaluation reveals that this issue presents a significant challenge to existing algorithms. Through systematic investigation, we identify and validate two key factors contributing to this issue: (1) the residual stream at the last subject position in earlier layers and (2) specific attention modules in later layers. Notably, certain attention heads in later layers, along with specific left singular vectors in their output matrices, encapsulate the original knowledge and exhibit a causal relationship with superficial editing. Furthermore, we extend our analysis to the task of superficial unlearning, where we observe consistent patterns in the behavior of specific attention heads and their corresponding left singular vectors, thereby demonstrating the robustness and broader applicability of our methodology and conclusions. Our code is available here.</li>
</ul>

<h3>Title: GDPRShield: AI-Powered GDPR Support for Software Developers in Small and Medium-Sized Enterprises</h3>
<ul>
<li><strong>Authors: </strong>Tharaka Wijesundara, Mathew Warren, Nalin Arachchilage</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12640">https://arxiv.org/abs/2505.12640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12640">https://arxiv.org/pdf/2505.12640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12640]] GDPRShield: AI-Powered GDPR Support for Software Developers in Small and Medium-Sized Enterprises(https://arxiv.org/abs/2505.12640)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>With the rapid increase in privacy violations in modern software development, regulatory frameworks such as the General Data Protection Regulation (GDPR) have been established to enforce strict data protection practices. However, insufficient privacy awareness among SME software developers contributes to failure in GDPR compliance. For instance, a developer unfamiliar with data minimization may build a system that collects excessive data, violating GDPR and risking fines. One reason for this lack of awareness is that developers in SMEs often take on multidisciplinary roles (e.g., front-end, back-end, database management, and privacy compliance), which limits specialization in privacy. This lack of awareness may lead to poor privacy attitudes, ultimately hindering the development of a strong organizational privacy culture. However, SMEs that achieve GDPR compliance may gain competitive advantages, such as increased user trust and marketing value, compared to others that do not. Therefore, in this paper, we introduce a novel AI-powered framework called "GDPRShield," specifically designed to enhance the GDPR awareness of SME software developers and, through this, improve their privacy attitudes. Simultaneously, GDPRShield boosts developers' motivation to comply with GDPR from the early stages of software development. It leverages functional requirements written as user stories to provide comprehensive GDPR-based privacy descriptions tailored to each requirement. Alongside improving awareness, GDPRShield strengthens motivation by presenting real-world consequences of noncompliance, such as heavy fines, reputational damage, and loss of user trust, aligned with each requirement. This dual focus on awareness and motivation leads developers to engage with GDPRShield, improving their GDPR compliance and privacy attitudes, which will help SMEs build a stronger privacy culture over time.</li>
</ul>

<h3>Title: Two out of Three (ToT): using self-consistency to make robust predictions</h3>
<ul>
<li><strong>Authors: </strong>Jung Hoon Lee, Sujith Vijayan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12642">https://arxiv.org/abs/2505.12642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12642">https://arxiv.org/pdf/2505.12642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12642]] Two out of Three (ToT): using self-consistency to make robust predictions(https://arxiv.org/abs/2505.12642)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning (DL) can automatically construct intelligent agents, deep neural networks (alternatively, DL models), that can outperform humans in certain tasks. However, the operating principles of DL remain poorly understood, making its decisions incomprehensible. As a result, it poses a great risk to deploy DL in high-stakes domains in which mistakes or errors may lead to critical consequences. Here, we aim to develop an algorithm that can help DL models make more robust decisions by allowing them to abstain from answering when they are uncertain. Our algorithm, named `Two out of Three (ToT)', is inspired by the sensitivity of the human brain to conflicting information. ToT creates two alternative predictions in addition to the original model prediction and uses the alternative predictions to decide whether it should provide an answer or not.</li>
</ul>

<h3>Title: Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Bo Yang, Hengwei Zhang, Jindong Wang, Yuchen Ren, Chenhao Lin, Chao Shen, Zhengyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12644">https://arxiv.org/abs/2505.12644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12644">https://arxiv.org/pdf/2505.12644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12644]] Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency(https://arxiv.org/abs/2505.12644)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In surrogate ensemble attacks, using more surrogate models yields higher transferability but lower resource efficiency. This practical trade-off between transferability and efficiency has largely limited existing attacks despite many pre-trained models are easily accessible online. In this paper, we argue that such a trade-off is caused by an unnecessary common assumption, i.e., all models should be identical across iterations. By lifting this assumption, we can use as many surrogates as we want to unleash transferability without sacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA), which dynamically selects diverse models (from easily accessible pre-trained models) across iterations based on our new interpretation of decoupling within-iteration and cross-iteration model this http URL this way, the number of within-iteration models is fixed for maintaining efficiency, while only cross-iteration model diversity is increased for higher transferability. Experiments on ImageNet demonstrate the superiority of SEA in various scenarios. For example, when dynamically selecting 4 from 20 accessible models, SEA yields 8.5% higher transferability than existing attacks under the same efficiency. The superiority of SEA also generalizes to real-world systems, such as commercial vision APIs and large vision-language models. Overall, SEA opens up the possibility of adaptively balancing transferability and efficiency according to specific resource requirements.</li>
</ul>

<h3>Title: Spiking Neural Network: a low power solution for physical layer authentication</h3>
<ul>
<li><strong>Authors: </strong>Jung Hoon Lee, Sujith Vijayan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12647">https://arxiv.org/abs/2505.12647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12647">https://arxiv.org/pdf/2505.12647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12647]] Spiking Neural Network: a low power solution for physical layer authentication(https://arxiv.org/abs/2505.12647)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Deep learning (DL) is a powerful tool that can solve complex problems, and thus, it seems natural to assume that DL can be used to enhance the security of wireless communication. However, deploying DL models to edge devices in wireless networks is challenging, as they require significant amounts of computing and power resources. Notably, Spiking Neural Networks (SNNs) are known to be efficient in terms of power consumption, meaning they can be an alternative platform for DL models for edge devices. In this study, we ask if SNNs can be used in physical layer authentication. Our evaluation suggests that SNNs can learn unique physical properties (i.e., `fingerprints') of RF transmitters and use them to identify individual devices. Furthermore, we find that SNNs are also vulnerable to adversarial attacks and that an autoencoder can be used clean out adversarial perturbations to harden SNNs against them.</li>
</ul>

<h3>Title: AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use</h3>
<ul>
<li><strong>Authors: </strong>Yaotian Yang, Yiwen Tang, Yizhe Chen, Xiao Chen, Jiangjie Qiu, Hao Xiong, Haoyu Yin, Zhiyao Luo, Yifei Zhang, Sijia Tao, Wentao Li, Qinghua Zhang, Yuqiang Li, Wanli Ouyang, Bin Zhao, Xiaonan Wang, Fei Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12650">https://arxiv.org/abs/2505.12650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12650">https://arxiv.org/pdf/2505.12650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12650]] AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use(https://arxiv.org/abs/2505.12650)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine learning-based interatomic potentials and force fields depend critically on accurate atomic structures, yet such data are scarce due to the limited availability of experimentally resolved crystals. Although atomic-resolution electron microscopy offers a potential source of structural data, converting these images into simulation-ready formats remains labor-intensive and error-prone, creating a bottleneck for model training and validation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that automatically transforms scanning transmission electron microscopy (STEM) images into atomic crystal structures and predicts their physical properties. AutoMat combines pattern-adaptive denoising, physics-guided template retrieval, symmetry-aware atomic reconstruction, fast relaxation and property prediction via MatterSim, and coordinated orchestration across all stages. We propose the first dedicated STEM2Mat-Bench for this task and evaluate performance using lattice RMSD, formation energy MAE, and structure-matching success rate. By orchestrating external tool calls, AutoMat enables a text-only LLM to outperform vision-language models in this domain, achieving closed-loop reasoning throughout the pipeline. In large-scale experiments over 450 structure samples, AutoMat substantially outperforms existing multimodal large language models and tools. These results validate both AutoMat and STEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic simulation in materials this http URL code and dataset are publicly available at this https URL and this https URL.</li>
</ul>

<h3>Title: Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yisheng Zhong, Yizhu Wen, Junfeng Guo, Mehran Kafai, Heng Huang, Hanqing Guo, Zhuangdi Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12655">https://arxiv.org/abs/2505.12655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12655">https://arxiv.org/pdf/2505.12655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12655]] Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models(https://arxiv.org/abs/2505.12655)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Protecting cyber Intellectual Property (IP) such as web content is an increasingly critical concern. The rise of large language models (LLMs) with online retrieval capabilities presents a double-edged sword that enables convenient access to information but often undermines the rights of original content creators. As users increasingly rely on LLM-generated responses, they gradually diminish direct engagement with original information sources, significantly reducing the incentives for IP creators to contribute, and leading to a saturating cyberspace with more AI-generated content. In response, we propose a novel defense framework that empowers web content creators to safeguard their web-based IP from unauthorized LLM real-time extraction by leveraging the semantic understanding capability of LLMs themselves. Our method follows principled motivations and effectively addresses an intractable black-box optimization problem. Real-world experiments demonstrated that our methods improve defense success rates from 2.5% to 88.6% on different LLMs, outperforming traditional defenses such as configuration-based restrictions.</li>
</ul>

<h3>Title: Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering</h3>
<ul>
<li><strong>Authors: </strong>Xukai Liu, Ye Liu, Shiwen Wu, Yanghai Zhang, Yihao Yuan, Kai Zhang, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12662">https://arxiv.org/abs/2505.12662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12662">https://arxiv.org/pdf/2505.12662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12662]] Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering(https://arxiv.org/abs/2505.12662)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have led to impressive progress in natural language generation, yet their tendency to produce hallucinated or unsubstantiated content remains a critical concern. To improve factual reliability, Retrieval-Augmented Generation (RAG) integrates external knowledge during inference. However, existing RAG systems face two major limitations: (1) unreliable adaptive control due to limited external knowledge supervision, and (2) hallucinations caused by inaccurate or irrelevant references. To address these issues, we propose Know3-RAG, a knowledge-aware RAG framework that leverages structured knowledge from knowledge graphs (KGs) to guide three core stages of the RAG process, including retrieval, generation, and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval module that employs KG embedding to assess the confidence of the generated answer and determine retrieval necessity, a knowledge-enhanced reference generation strategy that enriches queries with KG-derived entities to improve generated reference relevance, and a knowledge-driven reference filtering mechanism that ensures semantic alignment and factual accuracy of references. Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG consistently outperforms strong baselines, significantly reducing hallucinations and enhancing answer reliability.</li>
</ul>

<h3>Title: Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Zihan Su, Xuerui Qiu, Hongbin Xu, Tangyu Jiang, Junhao Zhuang, Chun Yuan, Ming Li, Shengfeng He, Fei Richard Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12667">https://arxiv.org/abs/2505.12667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12667">https://arxiv.org/pdf/2505.12667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12667]] Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking(https://arxiv.org/abs/2505.12667)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>The explosive growth of generative video models has amplified the demand for reliable copyright preservation of AI-generated content. Despite its popularity in image synthesis, invisible generative watermarking remains largely underexplored in video generation. To address this gap, we propose Safe-Sora, the first framework to embed graphical watermarks directly into the video generation process. Motivated by the observation that watermarking performance is closely tied to the visual similarity between the watermark and cover content, we introduce a hierarchical coarse-to-fine adaptive matching mechanism. Specifically, the watermark image is divided into patches, each assigned to the most visually similar video frame, and further localized to the optimal spatial region for seamless embedding. To enable spatiotemporal fusion of watermark patches across video frames, we develop a 3D wavelet transform-enhanced Mamba architecture with a novel spatiotemporal local scanning strategy, effectively modeling long-range dependencies during watermark embedding and retrieval. To the best of our knowledge, this is the first attempt to apply state space models to watermarking, opening new avenues for efficient and robust watermark protection. Extensive experiments demonstrate that Safe-Sora achieves state-of-the-art performance in terms of video quality, watermark fidelity, and robustness, which is largely attributed to our proposals. We will release our code upon publication.</li>
</ul>

<h3>Title: Few-Step Diffusion via Score identity Distillation</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Zhou, Yi Gu, Zhendong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12674">https://arxiv.org/abs/2505.12674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12674">https://arxiv.org/pdf/2505.12674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12674]] Few-Step Diffusion via Score identity Distillation(https://arxiv.org/abs/2505.12674)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, data-free</a></li>
<li><strong>Abstract: </strong>Diffusion distillation has emerged as a promising strategy for accelerating text-to-image (T2I) diffusion models by distilling a pretrained score network into a one- or few-step generator. While existing methods have made notable progress, they often rely on real or teacher-synthesized images to perform well when distilling high-resolution T2I diffusion models such as Stable Diffusion XL (SDXL), and their use of classifier-free guidance (CFG) introduces a persistent trade-off between text-image alignment and generation diversity. We address these challenges by optimizing Score identity Distillation (SiD) -- a data-free, one-step distillation framework -- for few-step generation. Backed by theoretical analysis that justifies matching a uniform mixture of outputs from all generation steps to the data distribution, our few-step distillation algorithm avoids step-specific networks and integrates seamlessly into existing pipelines, achieving state-of-the-art performance on SDXL at 1024x1024 resolution. To mitigate the alignment-diversity trade-off when real text-image pairs are available, we introduce a Diffusion GAN-based adversarial loss applied to the uniform mixture and propose two new guidance strategies: Zero-CFG, which disables CFG in the teacher and removes text conditioning in the fake score network, and Anti-CFG, which applies negative CFG in the fake score network. This flexible setup improves diversity without sacrificing alignment. Comprehensive experiments on SD1.5 and SDXL demonstrate state-of-the-art performance in both one-step and few-step generation settings, along with robustness to the absence of real images. Our efficient PyTorch implementation, along with the resulting one- and few-step distilled generators, will be released publicly as a separate branch at this https URL.</li>
</ul>

<h3>Title: CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shristi Das Biswas, Arani Roy, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12677">https://arxiv.org/abs/2505.12677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12677">https://arxiv.org/pdf/2505.12677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12677]] CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models(https://arxiv.org/abs/2505.12677)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, diffusion</a></li>
<li><strong>Abstract: </strong>As Text-to-Image models continue to evolve, so does the risk of generating unsafe, copyrighted, or privacy-violating content. Existing safety interventions - ranging from training data curation and model fine-tuning to inference-time filtering and guidance - often suffer from incomplete concept removal, susceptibility to jail-breaking, computational inefficiency, or collateral damage to unrelated capabilities. In this paper, we introduce CURE, a training-free concept unlearning framework that operates directly in the weight space of pre-trained diffusion models, enabling fast, interpretable, and highly specific suppression of undesired concepts. At the core of our method is the Spectral Eraser, a closed-form, orthogonal projection module that identifies discriminative subspaces using Singular Value Decomposition over token embeddings associated with the concepts to forget and retain. Intuitively, the Spectral Eraser identifies and isolates features unique to the undesired concept while preserving safe attributes. This operator is then applied in a single step update to yield an edited model in which the target concept is effectively unlearned - without retraining, supervision, or iterative optimization. To balance the trade-off between filtering toxicity and preserving unrelated concepts, we further introduce an Expansion Mechanism for spectral regularization which selectively modulates singular vectors based on their relative significance to control the strength of forgetting. All the processes above are in closed-form, guaranteeing extremely efficient erasure in only $2$ seconds. Benchmarking against prior approaches, CURE achieves a more efficient and thorough removal for targeted artistic styles, objects, identities, or explicit content, with minor damage to original generation ability and demonstrates enhanced robustness against red-teaming.</li>
</ul>

<h3>Title: On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Hana Satou, Alan Mitkiy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12681">https://arxiv.org/abs/2505.12681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12681">https://arxiv.org/pdf/2505.12681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12681]] On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning(https://arxiv.org/abs/2505.12681)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Transfer learning across domains with distribution shift remains a fundamental challenge in building robust and adaptable machine learning systems. While adversarial perturbations are traditionally viewed as threats that expose model vulnerabilities, recent studies suggest that they can also serve as constructive tools for data augmentation. In this work, we systematically investigate the role of adversarial data augmentation (ADA) in enhancing both robustness and adaptivity in transfer learning settings. We analyze how adversarial examples, when used strategically during training, improve domain generalization by enriching decision boundaries and reducing overfitting to source-domain-specific features. We further propose a unified framework that integrates ADA with consistency regularization and domain-invariant representation learning. Extensive experiments across multiple benchmark datasets -- including VisDA, DomainNet, and Office-Home -- demonstrate that our method consistently improves target-domain performance under both unsupervised and few-shot domain adaptation settings. Our results highlight a constructive perspective of adversarial learning, transforming perturbation from a destructive attack into a regularizing force for cross-domain transferability.</li>
</ul>

<h3>Title: RoFL: Robust Fingerprinting of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yun-Yun Tsai, Chuan Guo, Junfeng Yang, Laurens van der Maaten</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12682">https://arxiv.org/abs/2505.12682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12682">https://arxiv.org/pdf/2505.12682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12682]] RoFL: Robust Fingerprinting of Language Models(https://arxiv.org/abs/2505.12682)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>AI developers are releasing large language models (LLMs) under a variety of different licenses. Many of these licenses restrict the ways in which the models or their outputs may be used. This raises the question how license violations may be recognized. In particular, how can we identify that an API or product uses (an adapted version of) a particular LLM? We present a new method that enable model developers to perform such identification via fingerprints: statistical patterns that are unique to the developer's model and robust to common alterations of that model. Our method permits model identification in a black-box setting using a limited number of queries, enabling identification of models that can only be accessed via an API or product. The fingerprints are non-invasive: our method does not require any changes to the model during training, hence by design, it does not impact model quality. Empirically, we find our method provides a high degree of robustness to common changes in the model or inference settings. In our experiments, it substantially outperforms prior art, including invasive methods that explicitly train watermarks into the model.</li>
</ul>

<h3>Title: Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</h3>
<ul>
<li><strong>Authors: </strong>Yinlin Zhu, Xunkai Li, Jishuo Jia, Miao Hu, Di Wu, Meikang Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12684">https://arxiv.org/abs/2505.12684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12684">https://arxiv.org/pdf/2505.12684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12684]] Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement(https://arxiv.org/abs/2505.12684)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources. These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation. To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation. FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.</li>
</ul>

<h3>Title: RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Seungmin Kim, Sohee Park, Donghyun Kim, Jisu Lee, Daeseon Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12686">https://arxiv.org/abs/2505.12686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12686">https://arxiv.org/pdf/2505.12686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12686]] RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations(https://arxiv.org/abs/2505.12686)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>With the advancement of AI-based speech synthesis technologies such as Deep Voice, there is an increasing risk of voice spoofing attacks, including voice phishing and fake news, through unauthorized use of others' voices. Existing defenses that inject adversarial perturbations directly into audio signals have limited effectiveness, as these perturbations can easily be neutralized by speech enhancement methods. To overcome this limitation, we propose RoVo (Robust Voice), a novel proactive defense technique that injects adversarial perturbations into high-dimensional embedding vectors of audio signals, reconstructing them into protected speech. This approach effectively defends against speech synthesis attacks and also provides strong resistance to speech enhancement models, which represent a secondary attack threat. In extensive experiments, RoVo increased the Defense Success Rate (DSR) by over 70% compared to unprotected speech, across four state-of-the-art speech synthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial speaker-verification API, effectively neutralizing speech synthesis attack. Moreover, RoVo's perturbations remained robust even under strong speech enhancement conditions, outperforming traditional methods. A user study confirmed that RoVo preserves both naturalness and usability of protected speech, highlighting its effectiveness in complex and evolving threat scenarios.</li>
</ul>

<h3>Title: Shielding Latent Face Representations From Privacy Attacks</h3>
<ul>
<li><strong>Authors: </strong>Arjun Ramesh Kaushik, Bharat Chandra Yalavarthi, Arun Ross, Vishnu Boddeti, Nalini Ratha</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12688">https://arxiv.org/abs/2505.12688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12688">https://arxiv.org/pdf/2505.12688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12688]] Shielding Latent Face Representations From Privacy Attacks(https://arxiv.org/abs/2505.12688)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, biometric</a></li>
<li><strong>Abstract: </strong>In today's data-driven analytics landscape, deep learning has become a powerful tool, with latent representations, known as embeddings, playing a central role in several applications. In the face analytics domain, such embeddings are commonly used for biometric recognition (e.g., face identification). However, these embeddings, or templates, can inadvertently expose sensitive attributes such as age, gender, and ethnicity. Leaking such information can compromise personal privacy and affect civil liberty and human rights. To address these concerns, we introduce a multi-layer protection framework for embeddings. It consists of a sequence of operations: (a) encrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing them using irreversible feature manifold hashing. Unlike conventional encryption methods, FHE enables computations directly on encrypted data, allowing downstream analytics while maintaining strong privacy guarantees. To reduce the overhead of encrypted processing, we employ embedding compression. Our proposed method shields latent representations of sensitive data from leaking private attributes (such as age and gender) while retaining essential functional capabilities (such as face identification). Extensive experiments on two datasets using two face encoders demonstrate that our approach outperforms several state-of-the-art privacy protection methods.</li>
</ul>

<h3>Title: An Automated Blackbox Noncompliance Checker for QUIC Server Implementations</h3>
<ul>
<li><strong>Authors: </strong>Kian Kai Ang, Guy Farrelly, Cheryl Pope, Damith C. Ranasinghe</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12690">https://arxiv.org/abs/2505.12690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12690">https://arxiv.org/pdf/2505.12690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12690]] An Automated Blackbox Noncompliance Checker for QUIC Server Implementations(https://arxiv.org/abs/2505.12690)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We develop QUICtester, an automated approach for uncovering non-compliant behaviors in the ratified QUIC protocol implementations (RFC 9000/9001). QUICtester leverages active automata learning to abstract the behavior of a QUIC implementation into a finite state machine (FSM) representation. Unlike prior noncompliance checking methods, to help uncover state dependencies on event timing, QUICtester introduces the idea of state learning with event timing variations, adopting both valid and invalid input configurations, and combinations of security and transport layer parameters during learning. We use pairwise differential analysis of learned behaviour models of tested QUIC implementations to identify non-compliance instances as behaviour deviations in a property-agnostic way. This exploits the existence of the many different QUIC implementations, removing the need for validated, formal models. The diverse implementations act as cross-checking test oracles to discover non-compliance. We used QUICtester to analyze analyze 186 learned models from 19 QUIC implementations under the five security settings and discovered 55 implementation errors. Significantly, the tool uncovered a QUIC specification ambiguity resulting in an easily exploitable DoS vulnerability, led to 5 CVE assignments from developers, and two bug bounties thus far.</li>
</ul>

<h3>Title: Writing a Good Security Paper for ISSCC (2025)</h3>
<ul>
<li><strong>Authors: </strong>Utsav Banerjee, Chiraag Juvekar, Yong Ki Lee, Leibo Liu, Sanu Mathew, Thomas Poeppelmann, Shreyas Sen, Takeshi Sugawara, Ingrid Verbauwhede, Rabia Tugce Yazicigil</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12700">https://arxiv.org/abs/2505.12700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12700">https://arxiv.org/pdf/2505.12700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12700]] Writing a Good Security Paper for ISSCC (2025)(https://arxiv.org/abs/2505.12700)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Security is increasingly more important in designing chips and systems based on them, and the International Solid-State Circuits Conference (ISSCC), the leading conference for presenting advances in solid-state circuits and semiconductor technology, is committed to hardware security by establishing the security subcommittee since 2024. In the past two years, the authors of this paper reviewed submissions as members of the Security Subcommittee, a part of International Technical Program Committee (ITPC). This paper aims to encourage high-quality submissions to grow this field in the overall scope of the ISSCC.</li>
</ul>

<h3>Title: Counterfactual Explanations for Continuous Action Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shuyang Dong, Shangtong Zhang, Lu Feng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12701">https://arxiv.org/abs/2505.12701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12701">https://arxiv.org/pdf/2505.12701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12701]] Counterfactual Explanations for Continuous Action Reinforcement Learning(https://arxiv.org/abs/2505.12701)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has shown great promise in domains like healthcare and robotics but often struggles with adoption due to its lack of interpretability. Counterfactual explanations, which address "what if" scenarios, provide a promising avenue for understanding RL decisions but remain underexplored for continuous action spaces. We propose a novel approach for generating counterfactual explanations in continuous action RL by computing alternative action sequences that improve outcomes while minimizing deviations from the original sequence. Our approach leverages a distance metric for continuous actions and accounts for constraints such as adhering to predefined policies in specific states. Evaluations in two RL domains, Diabetes Control and Lunar Lander, demonstrate the effectiveness, efficiency, and generalization of our approach, enabling more interpretable and trustworthy RL applications.</li>
</ul>

<h3>Title: Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tianming Liang, Haichao Jiang, Yuting Yang, Chaolei Tan, Shuai Li, Wei-Shi Zheng, Jian-Fang Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12702">https://arxiv.org/abs/2505.12702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12702">https://arxiv.org/pdf/2505.12702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12702]] Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation(https://arxiv.org/abs/2505.12702)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Referring video object segmentation (RVOS) aims to identify, track and segment the objects in a video based on language descriptions, which has received great attention in recent years. However, existing datasets remain focus on short video clips within several seconds, with salient objects visible in most frames. To advance the task towards more practical scenarios, we introduce \textbf{Long-RVOS}, a large-scale benchmark for long-term referring video object segmentation. Long-RVOS contains 2,000+ videos of an average duration exceeding 60 seconds, covering a variety of objects that undergo occlusion, disappearance-reappearance and shot changing. The objects are manually annotated with three different types of descriptions to individually evaluate the understanding of static attributes, motion patterns and spatiotemporal relationships. Moreover, unlike previous benchmarks that rely solely on the per-frame spatial evaluation, we introduce two new metrics to assess the temporal and spatiotemporal consistency. We benchmark 6 state-of-the-art methods on Long-RVOS. The results show that current approaches struggle severely with the long-video challenges. To address this, we further propose ReferMo, a promising baseline method that integrates motion information to expand the temporal receptive field, and employs a local-to-global architecture to capture both short-term dynamics and long-term dependencies. Despite simplicity, ReferMo achieves significant improvements over current methods in long-term scenarios. We hope that Long-RVOS and our baseline can drive future RVOS research towards tackling more realistic and long-form videos.</li>
</ul>

<h3>Title: PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI</h3>
<ul>
<li><strong>Authors: </strong>Yingchen He, Christian D. Weilbach, Martyna E. Wojciechowska, Yuxuan Zhang, Frank Wood</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12707">https://arxiv.org/abs/2505.12707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12707">https://arxiv.org/pdf/2505.12707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12707]] PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI(https://arxiv.org/abs/2505.12707)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>Advances in deep generative modelling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environments. To address this, we present PLAICraft, a novel data collection platform and dataset capturing multiplayer Minecraft interactions across five time-aligned modalities: video, game output audio, microphone input audio, mouse, and keyboard actions. Each modality is logged with millisecond time precision, enabling the study of synchronous, embodied behaviour in a rich, open-ended world. The dataset comprises over 10,000 hours of gameplay from more than 10,000 global participants.\footnote{We have done a privacy review for the public release of an initial 200-hour subset of the dataset, with plans to release most of the dataset over time.} Alongside the dataset, we provide an evaluation suite for benchmarking model capabilities in object recognition, spatial awareness, language grounding, and long-term memory. PLAICraft opens a path toward training and evaluating agents that act fluently and purposefully in real time, paving the way for truly embodied artificial intelligence.</li>
</ul>

<h3>Title: Confidence-Regulated Generative Diffusion Models for Reliable AI Agent Migration in Vehicular Metaverses</h3>
<ul>
<li><strong>Authors: </strong>Yingkai Kang, Jiawen Kang, Jinbo Wen, Tao Zhang, Zhaohui Yang, Dusit Niyato, Yan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12710">https://arxiv.org/abs/2505.12710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12710">https://arxiv.org/pdf/2505.12710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12710]] Confidence-Regulated Generative Diffusion Models for Reliable AI Agent Migration in Vehicular Metaverses(https://arxiv.org/abs/2505.12710)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Vehicular metaverses are an emerging paradigm that merges intelligent transportation systems with virtual spaces, leveraging advanced digital twin and Artificial Intelligence (AI) technologies to seamlessly integrate vehicles, users, and digital environments. In this paradigm, vehicular AI agents are endowed with environment perception, decision-making, and action execution capabilities, enabling real-time processing and analysis of multi-modal data to provide users with customized interactive services. Since vehicular AI agents require substantial resources for real-time decision-making, given vehicle mobility and network dynamics conditions, the AI agents are deployed in RoadSide Units (RSUs) with sufficient resources and dynamically migrated among them. However, AI agent migration requires frequent data exchanges, which may expose vehicular metaverses to potential cyber attacks. To this end, we propose a reliable vehicular AI agent migration framework, achieving reliable dynamic migration and efficient resource scheduling through cooperation between vehicles and RSUs. Additionally, we design a trust evaluation model based on the theory of planned behavior to dynamically quantify the reputation of RSUs, thereby better accommodating the personalized trust preferences of users. We then model the vehicular AI agent migration process as a partially observable markov decision process and develop a Confidence-regulated Generative Diffusion Model (CGDM) to efficiently generate AI agent migration decisions. Numerical results demonstrate that the CGDM algorithm significantly outperforms baseline methods in reducing system latency and enhancing robustness against cyber attacks.</li>
</ul>

<h3>Title: Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Qichen Sun, Zhengrui Guo, Rui Peng, Hao Chen, Jinzhuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12711">https://arxiv.org/abs/2505.12711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12711">https://arxiv.org/pdf/2505.12711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12711]] Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining(https://arxiv.org/abs/2505.12711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in computational pathology and artificial intelligence have significantly enhanced the utilization of gigapixel whole-slide images and and additional modalities (e.g., genomics) for pathological diagnosis. Although deep learning has demonstrated strong potential in pathology, several key challenges persist: (1) fusing heterogeneous data types requires sophisticated strategies beyond simple concatenation due to high computational costs; (2) common scenarios of missing modalities necessitate flexible strategies that allow the model to learn robustly in the absence of certain modalities; (3) the downstream tasks in CPath are diverse, ranging from unimodal to multimodal, cnecessitating a unified model capable of handling all modalities. To address these challenges, we propose ALTER, an any-to-any tri-modal pretraining framework that integrates WSIs, genomics, and pathology reports. The term "any" emphasizes ALTER's modality-adaptive design, enabling flexible pretraining with any subset of modalities, and its capacity to learn robust, cross-modal representations beyond WSI-centric approaches. We evaluate ALTER across extensive clinical tasks including survival prediction, cancer subtyping, gene mutation prediction, and report generation, achieving superior or comparable performance to state-of-the-art baselines.</li>
</ul>

<h3>Title: IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo</h3>
<ul>
<li><strong>Authors: </strong>Yinzhe Wang, Yiwen Xiao, Hu Wang, Yiping Xu, Yan Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12714">https://arxiv.org/abs/2505.12714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12714">https://arxiv.org/pdf/2505.12714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12714]] IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo(https://arxiv.org/abs/2505.12714)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-view stereo (MVS) models based on progressive depth hypothesis narrowing have made remarkable advancements. However, existing methods haven't fully utilized the potential that the depth coverage of individual instances is smaller than that of the entire scene, which restricts further improvements in depth estimation precision. Moreover, inevitable deviations in the initial stage accumulate as the process advances. In this paper, we propose Instance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimation by narrowing the depth hypothesis range and conducting refinement on each instance. Additionally, a filtering mechanism based on intra-instance depth continuity priors is incorporated to boost robustness. Furthermore, recognizing that existing confidence estimation can degrade IA-MVS performance on point clouds. We have developed a detailed mathematical model for confidence estimation based on conditional probability. The proposed method can be widely applied in models based on MVSNet without imposing extra training burdens. Our method achieves state-of-the-art performance on the DTU benchmark. The source code is available at this https URL.</li>
</ul>

<h3>Title: VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Aditya Taparia, Noel Ngu, Mario Leiva, Joshua Shay Kricheli, John Corcoran, Nathaniel D. Bastian, Gerardo Simari, Paulo Shakarian, Ransalu Senanayake</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12715">https://arxiv.org/abs/2505.12715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12715">https://arxiv.org/pdf/2505.12715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12715]] VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection(https://arxiv.org/abs/2505.12715)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Although fusing multiple sensor modalities can enhance object detection performance, existing fusion approaches often overlook subtle variations in environmental conditions and sensor inputs. As a result, they struggle to adaptively weight each modality under such variations. To address this challenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a novel fusion framework that leverages a Vision-Language Model (VLM) to condition the fusion process on nuanced environmental cues. By capturing high-level environmental context such as as darkness, rain, and camera blurring, the VLM guides the model to dynamically adjust modality weights based on the current scene. We evaluate VLC Fusion on real-world autonomous driving and military target detection datasets that include image, LIDAR, and mid-wave infrared modalities. Our experiments show that VLC Fusion consistently outperforms conventional fusion baselines, achieving improved detection accuracy in both seen and unseen scenarios.</li>
</ul>

<h3>Title: Shadow-FT: Tuning Instruct via Base</h3>
<ul>
<li><strong>Authors: </strong>Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12716">https://arxiv.org/abs/2505.12716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12716">https://arxiv.org/pdf/2505.12716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12716]] Shadow-FT: Tuning Instruct via Base(https://arxiv.org/abs/2505.12716)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{this https URL}{Github}.</li>
</ul>

<h3>Title: ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Wu, Xueyi Chen, Rui Ming, Jilong Gao, Shoubo Hu, Zhuolun He, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12717">https://arxiv.org/abs/2505.12717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12717">https://arxiv.org/pdf/2505.12717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12717]] ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving(https://arxiv.org/abs/2505.12717)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes, which can be elicited by reinforcement learning (RL). However, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection. The reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction. In contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure. This reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification, assessment, and pruning of unproductive paths. This process can potentially lead to improved performance and reduced token costs. Building upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward. ToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs as players in a puzzle game during the ToTRL training process. Solving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints, which requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL, achieves significant improvement in performance and reasoning efficiency on complex reasoning tasks.</li>
</ul>

<h3>Title: Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework</h3>
<ul>
<li><strong>Authors: </strong>Jingyang Peng, Wenyuan Shen, Jiarui Rao, Jionghao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12718">https://arxiv.org/abs/2505.12718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12718">https://arxiv.org/pdf/2505.12718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12718]] Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework(https://arxiv.org/abs/2505.12718)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in Generative Artificial Intelligence (GenAI) have transformed educational content creation, particularly in developing tutor training materials. However, biases embedded in AI-generated content--such as gender, racial, or national stereotypes--raise significant ethical and educational concerns. Despite the growing use of GenAI, systematic methods for detecting and evaluating such biases in educational materials remain limited. This study proposes an automated bias assessment approach that integrates the Contextualized Embedding Association Test with a prompt-engineered word extraction method within a Retrieval-Augmented Generation framework. We applied this method to AI-generated texts used in tutor training lessons. Results show a high alignment between the automated and manually curated word sets, with a Pearson correlation coefficient of r = 0.993, indicating reliable and consistent bias assessment. Our method reduces human subjectivity and enhances fairness, scalability, and reproducibility in auditing GenAI-produced educational content.</li>
</ul>

<h3>Title: On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Wu, Rui Ming, Jilong Gao, Hangyu Zhao, Xueyi Chen, Yikai Yang, Haisheng Zheng, Zhuolun He, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12723">https://arxiv.org/abs/2505.12723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12723">https://arxiv.org/pdf/2505.12723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12723]] On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding(https://arxiv.org/abs/2505.12723)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve remarkable performance in code generation tasks. However, a significant performance disparity persists between popular programming languages (e.g., Python, C++) and others. To address this capability gap, we leverage the code translation task to train LLMs, thereby facilitating the transfer of coding proficiency across diverse programming languages. Moreover, we introduce OORL for training, a novel reinforcement learning (RL) framework that integrates on-policy and off-policy strategies. Within OORL, on-policy RL is applied during code translation, guided by a rule-based reward signal derived from unit tests. Complementing this coarse-grained rule-based reward, we propose Group Equivalent Preference Optimization (GEPO), a novel preference optimization method. Specifically, GEPO trains the LLM using intermediate representations (IRs) groups. LLMs can be guided to discern IRs equivalent to the source code from inequivalent ones, while also utilizing signals about the mutual equivalence between IRs within the group. This process allows LLMs to capture nuanced aspects of code functionality. By employing OORL for training with code translation tasks, LLMs improve their recognition of code functionality and their understanding of the relationships between code implemented in different languages. Extensive experiments demonstrate that our OORL for LLMs training with code translation tasks achieves significant performance improvements on code benchmarks across multiple programming languages.</li>
</ul>

<h3>Title: EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chenghua Gong, Rui Sun, Yuhao Zheng, Juyuan Zhang, Tianjun Gu, Liming Pan, Linyuan Lv</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12738">https://arxiv.org/abs/2505.12738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12738">https://arxiv.org/pdf/2505.12738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12738]] EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting(https://arxiv.org/abs/2505.12738)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Advanced epidemic forecasting is critical for enabling precision containment strategies, highlighting its strategic importance for public health security. While recent advances in Large Language Models (LLMs) have demonstrated effectiveness as foundation models for domain-specific tasks, their potential for epidemic forecasting remains largely unexplored. In this paper, we introduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal epidemic forecasting. Considering the key factors in real-world epidemic transmission: infection cases and human mobility, we introduce a dual-branch architecture to achieve fine-grained token-level alignment between such complex epidemic patterns and language tokens for LLM adaptation. To unleash the multi-step forecasting and generalization potential of LLM architectures, we propose an autoregressive modeling paradigm that reformulates the epidemic forecasting task into next-token prediction. To further enhance LLM perception of epidemics, we introduce spatio-temporal prompt learning techniques, which strengthen forecasting capabilities from a data-driven perspective. Extensive experiments show that EpiLLM significantly outperforms existing baselines on real-world COVID-19 datasets and exhibits scaling behavior characteristic of LLMs.</li>
</ul>

<h3>Title: Malware families discovery via Open-Set Recognition on Android manifest permissions</h3>
<ul>
<li><strong>Authors: </strong>Filippo Leveni, Matteo Mistura, Francesco Iubatti, Carmine Giangregorio, Nicol Pastore, Cesare Alippi, Giacomo Boracchi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12750">https://arxiv.org/abs/2505.12750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12750">https://arxiv.org/pdf/2505.12750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12750]] Malware families discovery via Open-Set Recognition on Android manifest permissions(https://arxiv.org/abs/2505.12750)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>Malware are malicious programs that are grouped into families based on their penetration technique, source code, and other characteristics. Classifying malware programs into their respective families is essential for building effective defenses against cyber threats. Machine learning models have a huge potential in malware detection on mobile devices, as malware families can be recognized by classifying permission data extracted from Android manifest files. Still, the malware classification task is challenging due to the high-dimensional nature of permission data and the limited availability of training samples. In particular, the steady emergence of new malware families makes it impossible to acquire a comprehensive training set covering all the malware classes. In this work, we present a malware classification system that, on top of classifying known malware, detects new ones. In particular, we combine an open-set recognition technique developed within the computer vision community, namely MaxLogit, with a tree-based Gradient Boosting classifier, which is particularly effective in classifying high-dimensional data. Our solution turns out to be very practical, as it can be seamlessly employed in a standard classification workflow, and efficient, as it adds minimal computational overhead. Experiments on public and proprietary datasets demonstrate the potential of our solution, which has been deployed in a business environment.</li>
</ul>

<h3>Title: Structure-based Anomaly Detection and Clustering</h3>
<ul>
<li><strong>Authors: </strong>Filippo Leveni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12751">https://arxiv.org/abs/2505.12751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12751">https://arxiv.org/pdf/2505.12751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12751]] Structure-based Anomaly Detection and Clustering(https://arxiv.org/abs/2505.12751)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Anomaly detection is a fundamental problem in domains such as healthcare, manufacturing, and cybersecurity. This thesis proposes new unsupervised methods for anomaly detection in both structured and streaming data settings. In the first part, we focus on structure-based anomaly detection, where normal data follows low-dimensional manifolds while anomalies deviate from them. We introduce Preference Isolation Forest (PIF), which embeds data into a high-dimensional preference space via manifold fitting, and isolates outliers using two variants: Voronoi-iForest, based on geometric distances, and RuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also propose Sliding-PIF, which captures local manifold information for streaming scenarios. Our methods outperform existing techniques on synthetic and real datasets. We extend this to structure-based clustering with MultiLink, a novel method for recovering multiple geometric model families in noisy data. MultiLink merges clusters via a model-aware linkage strategy, enabling robust multi-class structure recovery. It offers key advantages over existing approaches, such as speed, reduced sensitivity to thresholds, and improved robustness to poor initial sampling. The second part of the thesis addresses online anomaly detection in evolving data streams. We propose Online Isolation Forest (Online-iForest), which uses adaptive, multi-resolution histograms and dynamically updates tree structures to track changes over time. It avoids retraining while achieving accuracy comparable to offline models, with superior efficiency for real-time applications. Finally, we tackle anomaly detection in cybersecurity via open-set recognition for malware classification. We enhance a Gradient Boosting classifier with MaxLogit to detect unseen malware families, a method now integrated into Cleafy's production system.</li>
</ul>

<h3>Title: LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Martha Teiko Teye, Ori Maoz, Matthias Rottmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12753">https://arxiv.org/abs/2505.12753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12753">https://arxiv.org/pdf/2505.12753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12753]] LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking(https://arxiv.org/abs/2505.12753)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-object tracking from LiDAR point clouds presents unique challenges due to the sparse and irregular nature of the data, compounded by the need for temporal coherence across frames. Traditional tracking systems often rely on hand-crafted features and motion models, which can struggle to maintain consistent object identities in crowded or fast-moving scenes. We present a lidar-based two-staged DETR inspired transformer; a smoother and tracker. The smoother stage refines lidar object detections, from any off-the-shelf detector, across a moving temporal window. The tracker stage uses a DETR-based attention block to maintain tracks across time by associating tracked objects with the refined detections using the point cloud as context. The model is trained on the datasets nuScenes and KITTI in both online and offline (forward peeking) modes demonstrating strong performance across metrics such as ID-switch and multiple object tracking accuracy (MOTA). The numerical results indicate that the online mode outperforms the lidar-only baseline and SOTA models on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475, while the offline mode provides an additional 3 pp aMOTP</li>
</ul>

<h3>Title: Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding</h3>
<ul>
<li><strong>Authors: </strong>Donghwa Shin, Edwin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12761">https://arxiv.org/abs/2505.12761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12761">https://arxiv.org/pdf/2505.12761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12761]] Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding(https://arxiv.org/abs/2505.12761)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have recently gained popularity in time series forecasting due to their ability to capture long-term dependencies. However, many existing models focus only on capturing temporal dependencies while omitting intricate relationships between variables. Recent models have tried tackling this by explicitly modeling both cross-time and cross-variate dependencies through a sequential or unified attention mechanism, but they are entirely channel dependent (CD) across all layers, making them potentially susceptible to overfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE), a lightweight CD module that injects cross-variate context into channel-independent (CI) models by simply modifying the patch embedding process. We achieve this by adding a learnable positional encoding and a lightweight router-attention block to the vanilla patch embedding layer. We then integrate CVPE into Time-LLM, a multimodal CI forecasting model, to demonstrate its effectiveness in capturing cross-variate dependencies and enhance the CI model's performance. Extensive experimental results on seven real-world datasets show that our enhanced Time-LLM outperforms the original baseline model simply by incorporating the CVPE module, with no other changes.</li>
</ul>

<h3>Title: ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Yaxun Dai (1), Wenxuan Xie (3), Xialie Zhuang (4), Tianyu Yang (5), Yiying Yang (2), Haiqin Yang (6), Yuhang Zhao (2), Pingfu Chao (1), Wenhao Jiang (2) ((1) Soochow University, (2) Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), (3) South China University of Technology, (4) University of Chinese Academy of Sciences, (5) Alibaba DAMO Academy, (6) International Digital Economy Academy (IDEA))</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12768">https://arxiv.org/abs/2505.12768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12768">https://arxiv.org/pdf/2505.12768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12768]] ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL(https://arxiv.org/abs/2505.12768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In Text-to-SQL, execution feedback is essential for guiding large language models (LLMs) to reason accurately and generate reliable SQL queries. However, existing methods treat execution feedback solely as a post-hoc signal for correction or selection, failing to integrate it into the generation process. This limitation hinders their ability to address reasoning errors as they occur, ultimately reducing query accuracy and robustness. To address this issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement Learning), a framework for Text-to-SQL that enables models to interact with the database during decoding and dynamically adjust their reasoning based on execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm that interleaves intermediate SQL execution into reasoning paths, facilitating context-sensitive revisions. It achieves this through structured prompts with markup tags and a stepwise rollout strategy that integrates execution feedback into each stage of generation. To supervise policy learning, we develop a composite reward function that includes an exploration reward, explicitly encouraging effective database interaction. Additionally, ReEx-SQL adopts a tree-based decoding strategy to support exploratory reasoning, enabling dynamic expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving 85.2% on Spider-Realistic with leading performance. In addition, its tree-structured decoding improves efficiency and performance over linear decoding, reducing inference time by 51.9% on the BIRD development set.</li>
</ul>

<h3>Title: Testing Access-Control Configuration Changes for Web Applications</h3>
<ul>
<li><strong>Authors: </strong>Chengcheng Xiang, Li Zhong, Eric Mugnier, Nathaniel Nguyen, Yuanyuan Zhou, Tianyin Xu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.OS, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12770">https://arxiv.org/abs/2505.12770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12770">https://arxiv.org/pdf/2505.12770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12770]] Testing Access-Control Configuration Changes for Web Applications(https://arxiv.org/abs/2505.12770)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Access-control misconfigurations are among the main causes of today's data breaches in web applications. However, few techniques are available to support automatic and systematic testing for access-control changes and detecting risky changes to prevent severe consequences. As a result, those critical security configurations often lack testing, or are tested manually in an ad hoc way. This paper advocates that tests should be made available for users to test access-control configuration changes. The key challenges are such tests need to be run with production environments (to reason end-to-end behavior) and need to be performance-efficient. We present a new approach to create such tests, as a mini test environment incorporating production program and data, called ACtests. ACtests report the impacts of access-control changes, namely the requests that were denied but would be allowed after a change, and vice versa. Users can validate if the changed requests are intended or not and identify potential security vulnerabilities. We evaluate ACtests with 193 public configurations of widely-used web applications on Dockerhub. ACtests detect 168 new vulnerabilities from 72 configuration images. We report them to the image maintainers: 54 of them have been confirmed and 44 have been fixed. We also conduct in-depth experiments with five real-world deployed systems, including Wikipedia and a commercial company's web proxy. Our results show that ACtests effectively and efficiently detect all the change impacts.</li>
</ul>

<h3>Title: Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection</h3>
<ul>
<li><strong>Authors: </strong>Junyi Hu, Tian Bai, Fengyi Wu, Zhengming Peng, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12772">https://arxiv.org/abs/2505.12772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12772">https://arxiv.org/pdf/2505.12772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12772]] Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection(https://arxiv.org/abs/2505.12772)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Feature fusion is critical for high-performance vision models but often incurs prohibitive complexity. However, prevailing attention-based fusion methods often involve significant computational complexity and implementation challenges, limiting their efficiency in resource-constrained environments. To address these issues, we introduce the Pyramid Sparse Transformer (PST), a lightweight, plug-and-play module that integrates coarse-to-fine token selection and shared attention parameters to reduce computation while preserving spatial detail. PST can be trained using only coarse attention and seamlessly activated at inference for further accuracy gains without retraining. When added to state-of-the-art real-time detection models, such as YOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO with minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as backbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%, respectively. These results demonstrate PST's effectiveness as a simple, hardware-friendly enhancement for both detection and classification tasks.</li>
</ul>

<h3>Title: Enhancing Transformers Through Conditioned Embedded Tokens</h3>
<ul>
<li><strong>Authors: </strong>Hemanth Saratchandran, Simon Lucey</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12789">https://arxiv.org/abs/2505.12789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12789">https://arxiv.org/pdf/2505.12789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12789]] Enhancing Transformers Through Conditioned Embedded Tokens(https://arxiv.org/abs/2505.12789)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Transformers have transformed modern machine learning, driving breakthroughs in computer vision, natural language processing, and robotics. At the core of their success lies the attention mechanism, which enables the modeling of global dependencies among input tokens. However, we reveal that the attention block in transformers suffers from inherent ill-conditioning, which hampers gradient-based optimization and leads to inefficient training. To address this, we develop a theoretical framework that establishes a direct relationship between the conditioning of the attention block and that of the embedded tokenized data. Building on this insight, we introduce conditioned embedded tokens, a method that systematically modifies the embedded tokens to improve the conditioning of the attention mechanism. Our analysis demonstrates that this approach significantly mitigates ill-conditioning, leading to more stable and efficient training. We validate our methodology across various transformer architectures, achieving consistent improvements in image classification, object detection, instance segmentation, and natural language processing, highlighting its broad applicability and effectiveness.</li>
</ul>

<h3>Title: EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Zhu, Yuhang Xie, Guojie Song, Xin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12792">https://arxiv.org/abs/2505.12792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12792">https://arxiv.org/pdf/2505.12792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12792]] EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs(https://arxiv.org/abs/2505.12792)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of large language models (LLMs) has revolutionized various fields, including the identification and discovery of human values within text data. While traditional NLP models, such as BERT, have been employed for this task, their ability to represent textual data is significantly outperformed by emerging LLMs like GPTs. However, the performance of online LLMs often degrades when handling long contexts required for value identification, which also incurs substantial computational costs. To address these challenges, we propose EAVIT, an efficient and accurate framework for human value identification that combines the strengths of both locally fine-tunable and online black-box LLMs. Our framework employs a value detector - a small, local language model - to generate initial value estimations. These estimations are then used to construct concise input prompts for online LLMs, enabling accurate final value identification. To train the value detector, we introduce explanation-based training and data generation techniques specifically tailored for value identification, alongside sampling strategies to optimize the brevity of LLM input prompts. Our approach effectively reduces the number of input tokens by up to 1/6 compared to directly querying online LLMs, while consistently outperforming traditional NLP methods and other LLM-based strategies.</li>
</ul>

<h3>Title: Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiawen Xu, Odej Kao, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12803">https://arxiv.org/abs/2505.12803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12803">https://arxiv.org/pdf/2505.12803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12803]] Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation(https://arxiv.org/abs/2505.12803)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Open set recognition (OSR) is devised to address the problem of detecting novel classes during model inference. Even in recent vision models, this remains an open issue which is receiving increasing attention. Thereby, a crucial challenge is to learn features that are relevant for unseen categories from given data, for which these features might not be discriminative. To facilitate this process and "optimize to learn" more diverse features, we propose GradMix, a data augmentation method that dynamically leverages gradient-based attribution maps of the model during training to mask out already learned concepts. Thus GradMix encourages the model to learn a more complete set of representative features from the same data source. Extensive experiments on open set recognition, close set classification, and out-of-distribution detection reveal that our method can often outperform the state-of-the-art. GradMix can further increase model robustness to corruptions as well as downstream classification performance for self-supervised learning, indicating its benefit for model generalization.</li>
</ul>

<h3>Title: FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA</h3>
<ul>
<li><strong>Authors: </strong>Seanie Lee, Sangwoo Park, Dong Bok Lee, Dominik Wagner, Haebin Seong, Tobias Bocklet, Juho Lee, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12805">https://arxiv.org/abs/2505.12805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12805">https://arxiv.org/pdf/2505.12805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12805]] FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA(https://arxiv.org/abs/2505.12805)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update ($BA$) intensifies this effect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose FedSVD, a simple yet effective method that introduces a global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the $B$ matrix and transmits it to the server. The server aggregates the $B$ matrices, computes the product $BA$ using the previous $A$, and refactorizes the result via SVD. This yields a new adaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an updated $B$ containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing $A$ to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of $A$ bounds the gradient norms of $B$ and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD consistently improves stability and performance across a variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes.</li>
</ul>

<h3>Title: Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yanbin Yin, Kun Zhou, Zhen Wang, Xiangdong Zhang, Yifei Shao, Shibo Hao, Yi Gu, Jieyuan Liu, Somanshu Singla, Tianyang Liu, Eric P. Xing, Zhengzhong Liu, Haojian Jin, Zhiting Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12808">https://arxiv.org/abs/2505.12808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12808">https://arxiv.org/pdf/2505.12808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12808]] Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models(https://arxiv.org/abs/2505.12808)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent explosion of large language models (LLMs), each with its own general or specialized strengths, makes scalable, reliable benchmarking more urgent than ever. Standard practices nowadays face fundamental trade-offs: closed-ended question-based benchmarks (eg MMLU) struggle with saturation as newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely on costly and slow human judges. Recently, automated methods (eg LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one or a few "authority" models. To tackle these issues, we propose Decentralized Arena (dearena), a fully automated framework leveraging collective intelligence from all LLMs to evaluate each other. It mitigates single-model judge bias by democratic, pairwise evaluation, and remains efficient at scale through two key components: (1) a coarse-to-fine ranking algorithm for fast incremental insertion of new models with sub-quadratic complexity, and (2) an automatic question selection strategy for the construction of new evaluation dimensions. Across extensive experiments across 66 LLMs, dearena attains up to 97% correlation with human judgements, while significantly reducing the cost. Our code and data will be publicly released on this https URL.</li>
</ul>

<h3>Title: SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Han Sun, Zhen Sun, Zongmin Zhang, Linzhao Jia, Wei Shao, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12821">https://arxiv.org/abs/2505.12821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12821">https://arxiv.org/pdf/2505.12821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12821]] SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models(https://arxiv.org/abs/2505.12821)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are emerging as dominant forces for textual style transfer. However, for arbitrary style transfer, LLMs face two key challenges: (1) considerable reliance on manually-constructed prompts and (2) rigid stylistic biases inherent in LLMs. In this paper, we propose a novel Synthesize-then-Decode (SynDec) approach, which automatically synthesizes high-quality prompts and amplifies their roles during decoding process. Specifically, our approach synthesizes prompts by selecting representative few-shot samples, conducting a four-dimensional style analysis, and reranking the candidates. At LLM decoding stage, the TST effect is amplified by maximizing the contrast in output probabilities between scenarios with and without the synthesized prompt, as well as between prompts and negative samples. We conduct extensive experiments and the results show that SynDec outperforms existing state-of-the-art LLM-based methods on five out of six benchmarks (e.g., achieving up to a 9\% increase in accuracy for modern-to-Elizabethan English transfer). Detailed ablation studies further validate the effectiveness of SynDec.</li>
</ul>

<h3>Title: Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering</h3>
<ul>
<li><strong>Authors: </strong>Jianfeng Cai, Wengang Zhou, Zongmeng Zhang, Jiale Hong, Nianji Zhan, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12826">https://arxiv.org/abs/2505.12826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12826">https://arxiv.org/pdf/2505.12826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12826]] Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering(https://arxiv.org/abs/2505.12826)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have achieved remarkable progress in video this http URL, hallucination, where the model generates plausible yet incorrect outputs, persists as a significant and under-addressed challenge in the video domain. Among existing solutions, activation engineering has proven successful in mitigating hallucinations in LLMs and ImageLLMs, yet its applicability to VideoLLMs remains largely unexplored. In this work, we are the first to systematically investigate the effectiveness and underlying mechanisms of activation engineering for mitigating hallucinations in VideoLLMs. We initially conduct an investigation of the key factors affecting the performance of activation engineering and find that a model's sensitivity to hallucination depends on $\textbf{temporal variation}$ rather than task type. Moreover, selecting appropriate internal modules and dataset for activation engineering is critical for reducing hallucination. Guided by these findings, we propose a temporal-aware activation engineering framework for VideoLLMs, which adaptively identifies and manipulates hallucination-sensitive modules based on the temporal variation characteristic, substantially mitigating hallucinations without additional LLM fine-tuning. Experiments across multiple models and benchmarks demonstrate that our method markedly reduces hallucination in VideoLLMs, thereby validating the robustness of our findings.</li>
</ul>

<h3>Title: Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering</h3>
<ul>
<li><strong>Authors: </strong>Zifeng Cheng, Zhonghui Wang, Yuchen Fu, Zhiwei Jiang, Yafeng Yin, Cong Wang, Qing Gu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12831">https://arxiv.org/abs/2505.12831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12831">https://arxiv.org/pdf/2505.12831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12831]] Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering(https://arxiv.org/abs/2505.12831)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Extracting sentence embeddings from large language models (LLMs) is a practical direction, as it requires neither additional data nor fine-tuning. Previous studies usually focus on prompt engineering to guide LLMs to encode the core semantic information of the sentence into the embedding of the last token. However, the last token in these methods still encodes an excess of non-essential information, such as stop words, limiting its encoding capacity. To this end, we propose a Contrastive Prompting (CP) method that introduces an extra auxiliary prompt to elicit better sentence embedding. By contrasting with the auxiliary prompt, CP can steer existing prompts to encode the core semantics of the sentence, rather than non-essential information. CP is a plug-and-play inference-time intervention method that can be combined with various prompt-based methods. Extensive experiments on Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our method can improve the performance of existing prompt-based methods across different LLMs. Our code will be released at this https URL.</li>
</ul>

<h3>Title: A Study on the Refining Handwritten Font by Mixing Font Styles</h3>
<ul>
<li><strong>Authors: </strong>Avinash Kumar, Kyeolhee Kang, Ammar ul Hassan, Jaeyoung Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12834">https://arxiv.org/abs/2505.12834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12834">https://arxiv.org/pdf/2505.12834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12834]] A Study on the Refining Handwritten Font by Mixing Font Styles(https://arxiv.org/abs/2505.12834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Handwritten fonts have a distinct expressive character, but they are often difficult to read due to unclear or inconsistent handwriting. FontFusionGAN (FFGAN) is a novel method for improving handwritten fonts by combining them with printed fonts. Our method implements generative adversarial network (GAN) to generate font that mix the desirable features of handwritten and printed fonts. By training the GAN on a dataset of handwritten and printed fonts, it can generate legible and visually appealing font images. We apply our method to a dataset of handwritten fonts and demonstrate that it significantly enhances the readability of the original fonts while preserving their unique aesthetic. Our method has the potential to improve the readability of handwritten fonts, which would be helpful for a variety of applications including document creation, letter writing, and assisting individuals with reading and writing difficulties. In addition to addressing the difficulties of font creation for languages with complex character sets, our method is applicable to other text-image-related tasks, such as font attribute control and multilingual font style transfer.</li>
</ul>

<h3>Title: FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hengxing Cai, Jinhan Dong, Jingjun Tan, Jingcheng Deng, Sihang Li, Zhifeng Gao, Haidong Wang, Zicheng Su, Agachai Sumalee, Renxin Zhong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12835">https://arxiv.org/abs/2505.12835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12835">https://arxiv.org/pdf/2505.12835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12835]] FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models(https://arxiv.org/abs/2505.12835)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available.</li>
</ul>

<h3>Title: The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting</h3>
<ul>
<li><strong>Authors: </strong>Christian Braun, Alexander Lilienbeck, Daniel Mentjukov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12837">https://arxiv.org/abs/2505.12837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12837">https://arxiv.org/pdf/2505.12837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12837]] The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting(https://arxiv.org/abs/2505.12837)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Legal contracts possess an inherent, semantically vital structure (e.g., sections, clauses) that is crucial for human comprehension but whose impact on LLM processing remains under-explored. This paper investigates the effects of explicit input text structure and prompt engineering on the performance of GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the CUAD. We compare model exact-match accuracy across various input formats: well-structured plain-text (human-generated from CUAD), plain-text cleaned of line breaks, extracted plain-text from Azure OCR, plain-text extracted by GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o Vision. To give an indication of the impact of possible prompt engineering, we assess the impact of shifting task instructions to the system prompt and explicitly informing the model about the structured nature of the input. Our findings reveal that GPT-4o demonstrates considerable robustness to variations in input structure, but lacks in overall performance. Conversely, GPT-4.1's performance is markedly sensitive; poorly structured inputs yield suboptimal results (but identical with GPT-4o), while well-structured formats (original CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by ~20 percentage points. Optimizing the system prompt to include task details and an advisory about structured input further elevates GPT-4.1's accuracy by an additional ~10-13 percentage points, with Markdown ultimately achieving the highest performance under these conditions (79 percentage points overall exact-match accuracy). This research empirically demonstrates that while newer models exhibit greater resilience, careful input structuring and strategic prompt design remain critical for optimizing the performance of LLMs, and can significantly affect outcomes in high-stakes legal applications.</li>
</ul>

<h3>Title: GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents</h3>
<ul>
<li><strong>Authors: </strong>Zheng Wu, Pengzhou Cheng, Zongru Wu, Lingzhong Dong, Zhuosheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12842">https://arxiv.org/abs/2505.12842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12842">https://arxiv.org/pdf/2505.12842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12842]] GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents(https://arxiv.org/abs/2505.12842)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI Agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\% over the best-performing baseline. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at this https URL.</li>
</ul>

<h3>Title: Bias Fitting to Mitigate Length Bias of Reward Model in RLHF</h3>
<ul>
<li><strong>Authors: </strong>Kangwen Zhao, Jianfeng Cai, Jinhua Zhu, Ruopei Sun, Dongyun Xue, Wengang Zhou, Li Li, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12843">https://arxiv.org/abs/2505.12843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12843">https://arxiv.org/pdf/2505.12843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12843]] Bias Fitting to Mitigate Length Bias of Reward Model in RLHF(https://arxiv.org/abs/2505.12843)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback relies on reward models to align large language models with human preferences. However, RLHF often suffers from reward hacking, wherein policy learning exploits flaws in the trained reward model to maximize reward scores without genuinely aligning with human preferences. A significant example of such reward hacking is length bias, where reward models usually favor longer responses irrespective of actual response quality. Previous works on length bias have notable limitations, these approaches either mitigate bias without characterizing the bias form, or simply assume a linear length-reward relation. To accurately model the intricate nature of length bias and facilitate more effective bias mitigation, we propose FiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a framework that autonomously learns and corrects underlying bias patterns. Our approach consists of three stages: First, we train a standard reward model which inherently contains length bias. Next, we deploy a lightweight fitting model to explicitly capture the non-linear relation between length and reward. Finally, we incorporate this learned relation into the reward model to debias. Experimental results demonstrate that FiMi-RM achieves a more balanced length-reward distribution. Furthermore, when applied to alignment algorithms, our debiased reward model improves length-controlled win rate and reduces verbosity without compromising its performance.</li>
</ul>

<h3>Title: Accelerate TarFlow Sampling with GS-Jacobi Iteration</h3>
<ul>
<li><strong>Authors: </strong>Ben Liu, Zhen Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12849">https://arxiv.org/abs/2505.12849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12849">https://arxiv.org/pdf/2505.12849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12849]] Accelerate TarFlow Sampling with GS-Jacobi Iteration(https://arxiv.org/abs/2505.12849)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computation, TarFlow's sampling process is extremely slow. In this paper, we demonstrate that through a series of optimization strategies, TarFlow sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow model have varying importance: a small number of blocks play a major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether a TarFlow block is "simple" (converges in few iterations) or "tough" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. Experiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample quality. Code and checkpoints are accessible on this https URL</li>
</ul>

<h3>Title: FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting</h3>
<ul>
<li><strong>Authors: </strong>Yanhua Wen, Lu Ai, Gang Liu, Chuang Li, Jianhao Wei</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12851">https://arxiv.org/abs/2505.12851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12851">https://arxiv.org/pdf/2505.12851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12851]] FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting(https://arxiv.org/abs/2505.12851)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Byzantine attacks during model aggregation in Federated Learning (FL) threaten training integrity by manipulating malicious clients' updates. Existing methods struggle with limited robustness under high malicious client ratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To address this, we propose FLTG, a novel aggregation algorithm integrating angle-based defense and dynamic reference selection. FLTG first filters clients via ReLU-clipped cosine similarity, leveraging a server-side clean dataset to exclude misaligned updates. It then dynamically selects a reference client based on the prior global model to mitigate non-i.i.d. bias, assigns aggregation weights inversely proportional to angular deviations, and normalizes update magnitudes to suppress malicious scaling. Evaluations across datasets of varying complexity under five classic attacks demonstrate FLTG's superiority over state-of-the-art methods under extreme bias scenarios and sustains robustness with a higher proportion(over 50%) of malicious clients.</li>
</ul>

<h3>Title: Re-identification of De-identified Documents with Autoregressive Infilling</h3>
<ul>
<li><strong>Authors: </strong>Lucas Georges Gabriel Charpentier, Pierre Lison</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12859">https://arxiv.org/abs/2505.12859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12859">https://arxiv.org/pdf/2505.12859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12859]] Re-identification of De-identified Documents with Autoregressive Infilling(https://arxiv.org/abs/2505.12859)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Documents revealing sensitive information about individuals must typically be de-identified. This de-identification is often done by masking all mentions of personally identifiable information (PII), thereby making it more difficult to uncover the identity of the person(s) in question. To investigate the robustness of de-identification methods, we present a novel, RAG-inspired approach that attempts the reverse process of re-identification based on a database of documents representing background knowledge. Given a text in which personal identifiers have been masked, the re-identification proceeds in two steps. A retriever first selects from the background knowledge passages deemed relevant for the re-identification. Those passages are then provided to an infilling model which seeks to infer the original content of each text span. This process is repeated until all masked spans are replaced. We evaluate the re-identification on three datasets (Wikipedia biographies, court rulings and clinical notes). Results show that (1) as many as 80% of de-identified text spans can be successfully recovered and (2) the re-identification accuracy increases along with the level of background knowledge.</li>
</ul>

<h3>Title: Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Tan, Xu Zheng, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12861">https://arxiv.org/abs/2505.12861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12861">https://arxiv.org/pdf/2505.12861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12861]] Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation(https://arxiv.org/abs/2505.12861)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-modal semantic segmentation (MMSS) faces significant challenges in real-world scenarios due to dynamic environments, sensor failures, and noise interference, creating a gap between theoretical models and practical performance. To address this, we propose a two-stage framework called RobustSeg, which enhances multi-modal robustness through two key components: the Hybrid Prototype Distillation Module (HPDM) and the Representation Regularization Module (RRM). In the first stage, RobustSeg pre-trains a multi-modal teacher model using complete modalities. In the second stage, a student model is trained with random modality dropout while learning from the teacher via HPDM and RRM. HPDM transforms features into compact prototypes, enabling cross-modal hybrid knowledge distillation and mitigating bias from missing modalities. RRM reduces representation discrepancies between the teacher and student by optimizing functional entropy through the log-Sobolev inequality. Extensive experiments on three public benchmarks demonstrate that RobustSeg outperforms previous state-of-the-art methods, achieving improvements of +2.76%, +4.56%, and +0.98%, respectively. Code is available at: this https URL.</li>
</ul>

<h3>Title: LEXam: Benchmarking Legal Reasoning on 340 Law Exams</h3>
<ul>
<li><strong>Authors: </strong>Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstrwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12864">https://arxiv.org/abs/2505.12864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12864">https://arxiv.org/pdf/2505.12864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12864]] LEXam: Benchmarking Legal Reasoning on 340 Law Exams(https://arxiv.org/abs/2505.12864)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: this https URL</li>
</ul>

<h3>Title: Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Koki Wakiyama, Tomohiro I, Hiroshi Sakamoto</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12869">https://arxiv.org/abs/2505.12869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12869">https://arxiv.org/pdf/2505.12869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12869]] Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption(https://arxiv.org/abs/2505.12869)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, interpretability</a></li>
<li><strong>Abstract: </strong>Feature selection is a technique that extracts a meaningful subset from a set of features in training data. When the training data is large-scale, appropriate feature selection enables the removal of redundant features, which can improve generalization performance, accelerate the training process, and enhance the interpretability of the model. This study proposes a privacy-preserving computation model for feature selection. Generally, when the data owner and analyst are the same, there is no need to conceal the private information. However, when they are different parties or when multiple owners exist, an appropriate privacy-preserving framework is required. Although various private feature selection algorithms, they all require two or more computing parties and do not guarantee security in environments where no external party can be fully trusted. To address this issue, we propose the first outsourcing algorithm for feature selection using fully homomorphic encryption. Compared to a prior two-party algorithm, our result improves the time and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n denote the number of features and data samples, respectively. We also implemented the proposed algorithm and conducted comparative experiments with the naive one. The experimental result shows the efficiency of our method even with small datasets.</li>
</ul>

<h3>Title: Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?</h3>
<ul>
<li><strong>Authors: </strong>Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Ronghua Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12871">https://arxiv.org/abs/2505.12871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12871">https://arxiv.org/pdf/2505.12871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12871]] Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?(https://arxiv.org/abs/2505.12871)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Low rank adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) thanks to its superb efficiency gains over previous methods. While extensive studies have examined the performance and structural properties of LoRA, its behavior upon training-time attacks remain underexplored, posing significant security risks. In this paper, we theoretically investigate the security implications of LoRA's low-rank structure during fine-tuning, in the context of its robustness against data poisoning and backdoor attacks. We propose an analytical framework that models LoRA's training dynamics, employs the neural tangent kernel to simplify the analysis of the training process, and applies information theory to establish connections between LoRA's low rank structure and its vulnerability against training-time attacks. Our analysis indicates that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning due to its over-simplified information geometry. Extensive experimental evaluations have corroborated our theoretical findings.</li>
</ul>

<h3>Title: PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Jindong Han, Wei Fan, Weijia Zhang, Hao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12882">https://arxiv.org/abs/2505.12882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12882">https://arxiv.org/pdf/2505.12882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12882]] PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems(https://arxiv.org/abs/2505.12882)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data Assimilation (DA) plays a critical role in atmospheric science by reconstructing spatially continous estimates of the system state, which serves as initial conditions for scientific analysis. While recent advances in diffusion models have shown great potential for DA tasks, most existing approaches remain purely data-driven and often overlook the physical laws that govern complex atmospheric dynamics. As a result, they may yield physically inconsistent reconstructions that impair downstream applications. To overcome this limitation, we propose PhyDA, a physics-guided diffusion framework designed to ensure physical coherence in atmospheric data assimilation. PhyDA introduces two key components: (1) a Physically Regularized Diffusion Objective that integrates physical constraints into the training process by penalizing deviations from known physical laws expressed as partial differential equations, and (2) a Virtual Reconstruction Encoder that bridges observational sparsity for structured latent representations, further enhancing the model's ability to infer complete and physically coherent states. Experiments on the ERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and better physical plausibility compared to state-of-the-art baselines. Our results emphasize the importance of combining generative modeling with domain-specific physical knowledge and show that PhyDA offers a promising direction for improving real-world data assimilation systems.</li>
</ul>

<h3>Title: GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Jialun Zhong, Yanzeng Li, Sen Hu, Yang Zhang, Teng Xu, Lei Zou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12888">https://arxiv.org/abs/2505.12888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12888">https://arxiv.org/pdf/2505.12888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12888]] GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation(https://arxiv.org/abs/2505.12888)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medication recommendations have become an important task in the healthcare domain, especially in measuring the accuracy and safety of medical dialogue systems (MDS). Different from the recommendation task based on electronic health records (EHRs), dialogue-based medication recommendations require research on the interaction details between patients and doctors, which is crucial but may not exist in EHRs. Recent advancements in large language models (LLM) have extended the medical dialogue domain. These LLMs can interpret patients' intent and provide medical suggestions including medication recommendations, but some challenges are still worth attention. During a multi-turn dialogue, LLMs may ignore the fine-grained medical information or connections across the dialogue turns, which is vital for providing accurate suggestions. Besides, LLMs may generate non-factual responses when there is a lack of domain-specific knowledge, which is more risky in the medical domain. To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted \textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication recommendation. It extracts medical concepts and corresponding states from dialogue to construct an explicitly patient-centric graph, which can describe the neglected but important information. Further, combined with external medical knowledge graphs, GAP can generate abundant queries and prompts, thus retrieving information from multiple sources to reduce the non-factual responses. We evaluate GAP on a dialogue-based medication recommendation dataset and further explore its potential in a more difficult scenario, dynamically diagnostic interviewing. Extensive experiments demonstrate its competitive performance when compared with strong baselines.</li>
</ul>

<h3>Title: ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ege zsoy, Chantal Pellegrini, David Bani-Harouni, Kun Yuan, Matthias Keicher, Nassir Navab</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12890">https://arxiv.org/abs/2505.12890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12890">https://arxiv.org/pdf/2505.12890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12890]] ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling(https://arxiv.org/abs/2505.12890)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The real-world complexity of surgeries necessitates surgeons to have deep and holistic comprehension to ensure precision, safety, and effective interventions. Computational systems are required to have a similar level of comprehension within the operating room. Prior works, limited to single-task efforts like phase recognition or scene graph generation, lack scope and generalizability. In this work, we introduce ORQA, a novel OR question answering benchmark and foundational multimodal model to advance OR intelligence. By unifying all four public OR datasets into a comprehensive benchmark, we enable our approach to concurrently address a diverse range of OR challenges. The proposed multimodal large language model fuses diverse OR signals such as visual, auditory, and structured data, for a holistic modeling of the OR. Finally, we propose a novel, progressive knowledge distillation paradigm, to generate a family of models optimized for different speed and memory requirements. We show the strong performance of ORQA on our proposed benchmark, and its zero-shot generalization, paving the way for scalable, unified OR modeling and significantly advancing multimodal surgical intelligence. We will release our code and data upon acceptance.</li>
</ul>

<h3>Title: On the Thinking-Language Modeling Gap in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Liu, Yongqiang Chen, Tongliang Liu, James Cheng, Bo Han, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12896">https://arxiv.org/abs/2505.12896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12896">https://arxiv.org/pdf/2505.12896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12896]] On the Thinking-Language Modeling Gap in Large Language Models(https://arxiv.org/abs/2505.12896)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>System 2 reasoning is one of the defining characteristics of intelligence, which requires slow and logical thinking. Human conducts System 2 reasoning via the language of thoughts that organizes the reasoning process as a causal sequence of mental language, or thoughts. Recently, it has been observed that System 2 reasoning can be elicited from Large Language Models (LLMs) pre-trained on large-scale natural languages. However, in this work, we show that there is a significant gap between the modeling of languages and thoughts. As language is primarily a tool for humans to share knowledge and thinking, modeling human language can easily absorb language biases into LLMs deviated from the chain of thoughts in minds. Furthermore, we show that the biases will mislead the eliciting of "thoughts" in LLMs to focus only on a biased part of the premise. To this end, we propose a new prompt technique termed Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of directly eliciting the chain of thoughts from partial information, LoT instructs LLMs to adjust the order and token used for the expressions of all the relevant information. We show that the simple strategy significantly reduces the language modeling biases in LLMs and improves the performance of LLMs across a variety of reasoning tasks.</li>
</ul>

<h3>Title: Efficient training for large-scale optical neural network using an evolutionary strategy and attention pruning</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Yang, Zeyang Fan, Yihang Lai, Qi Chen, Tian Zhang, Jian Dai, Kun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12906">https://arxiv.org/abs/2505.12906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12906">https://arxiv.org/pdf/2505.12906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12906]] Efficient training for large-scale optical neural network using an evolutionary strategy and attention pruning(https://arxiv.org/abs/2505.12906)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>MZI-based block optical neural networks (BONNs), which can achieve large-scale network models, have increasingly drawn attentions. However, the robustness of the current training algorithm is not high enough. Moreover, large-scale BONNs usually contain numerous trainable parameters, resulting in expensive computation and power consumption. In this article, by pruning matrix blocks and directly optimizing the individuals in population, we propose an on-chip covariance matrix adaptation evolution strategy and attention-based pruning (CAP) algorithm for large-scale BONNs. The calculated results demonstrate that the CAP algorithm can prune 60% and 80% of the parameters for MNIST and Fashion-MNIST datasets, respectively, while only degrades the performance by 3.289% and 4.693%. Considering the influence of dynamic noise in phase shifters, our proposed CAP algorithm (performance degradation of 22.327% for MNIST dataset and 24.019% for Fashion-MNIST dataset utilizing a poor fabricated chip and electrical control with a standard deviation of 0.5) exhibits strongest robustness compared with both our previously reported block adjoint training algorithm (43.963% and 41.074%) and the covariance matrix adaptation evolution strategy (25.757% and 32.871%), respectively. Moreover, when 60% of the parameters are pruned, the CAP algorithm realizes 88.5% accuracy in experiment for the simplified MNIST dataset, which is similar to the simulation result without noise (92.1%). Additionally, we simulationally and experimentally demonstrate that using MZIs with only internal phase shifters to construct BONNs is an efficient way to reduce both the system area and the required trainable parameters. Notably, our proposed CAP algorithm show excellent potential for larger-scale network models and more complex tasks.</li>
</ul>

<h3>Title: Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wang, Yu Jin, Lan Chen, Bo Jiang, Lin Zhu, Yonghong Tian, Jin Tang, Bin Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12908">https://arxiv.org/abs/2505.12908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12908">https://arxiv.org/pdf/2505.12908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12908]] Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection(https://arxiv.org/abs/2505.12908)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Event-based Vision Sensors (EVS) have demonstrated significant advantages over traditional RGB frame-based cameras in low-light conditions, high-speed motion capture, and low latency. Consequently, object detection based on EVS has attracted increasing attention from researchers. Current event stream object detection algorithms are typically built upon Convolutional Neural Networks (CNNs) or Transformers, which either capture limited local features using convolutional filters or incur high computational costs due to the utilization of self-attention. Recently proposed vision heat conduction backbone networks have shown a good balance between efficiency and accuracy; however, these models are not specifically designed for event stream data. They exhibit weak capability in modeling object contour information and fail to exploit the benefits of multi-scale features. To address these issues, this paper proposes a novel dynamic graph induced contour-aware heat conduction network for event stream based object detection, termed CvHeat-DET. The proposed model effectively leverages the clear contour information inherent in event streams to predict the thermal diffusivity coefficients within the heat conduction model, and integrates hierarchical structural graph features to enhance feature learning across multiple scales. Extensive experiments on three benchmark datasets for event stream-based object detection fully validated the effectiveness of the proposed model. The source code of this paper will be released on this https URL.</li>
</ul>

<h3>Title: Sinusoidal Initialization, Time for a New Start</h3>
<ul>
<li><strong>Authors: </strong>Alberto Fernndez-Hernndez, Jose I. Mestre, Manuel F. Dolz, Jose Duato, Enrique S. Quintana-Ort</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12909">https://arxiv.org/abs/2505.12909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12909">https://arxiv.org/pdf/2505.12909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12909]] Sinusoidal Initialization, Time for a New Start(https://arxiv.org/abs/2505.12909)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Initialization plays a critical role in Deep Neural Network training, directly influencing convergence, stability, and generalization. Common approaches such as Glorot and He initializations rely on randomness, which can produce uneven weight distributions across layer connections. In this paper, we introduce the Sinusoidal initialization, a novel deterministic method that employs sinusoidal functions to construct structured weight matrices expressly to improve the spread and balance of weights throughout the network while simultaneously fostering a more uniform, well-conditioned distribution of neuron activation states from the very first forward pass. Because Sinusoidal initialization begins with weights and activations that are already evenly and efficiently utilized, it delivers consistently faster convergence, greater training stability, and higher final accuracy across a wide range of models, including convolutional neural networks, vision transformers, and large language models. On average, our experiments show an increase of 4.8 % in final validation accuracy and 20.9 % in convergence speed. By replacing randomness with structure, this initialization provides a stronger and more reliable foundation for Deep Learning systems.</li>
</ul>

<h3>Title: Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption</h3>
<ul>
<li><strong>Authors: </strong>Kazuki Adachi, Shin'ya Yamaguchi, Tomoki Hamagami</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12912">https://arxiv.org/abs/2505.12912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12912">https://arxiv.org/pdf/2505.12912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12912]] Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption(https://arxiv.org/abs/2505.12912)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pre-trained vision-language models such as contrastive language-image pre-training (CLIP) have demonstrated a remarkable generalizability, which has enabled a wide range of applications represented by zero-shot classification. However, vision-language models still suffer when they face datasets with large gaps from training ones, i.e., distribution shifts. We found that CLIP is especially vulnerable to sensor degradation, a type of realistic distribution shift caused by sensor conditions such as weather, light, or noise. Collecting a new dataset from a test distribution for fine-tuning highly costs since sensor degradation occurs unexpectedly and has a range of variety. Thus, we investigate test-time adaptation (TTA) of zero-shot classification, which enables on-the-fly adaptation to the test distribution with unlabeled test data. Existing TTA methods for CLIP mainly focus on modifying image and text embeddings or predictions to address distribution shifts. Although these methods can adapt to domain shifts, such as fine-grained labels spaces or different renditions in input images, they fail to adapt to distribution shifts caused by sensor degradation. We found that this is because image embeddings are "corrupted" in terms of uniformity, a measure related to the amount of information. To make models robust to sensor degradation, we propose a novel method called uniformity-aware information-balanced TTA (UnInfo). To address the corruption of image embeddings, we introduce uniformity-aware confidence maximization, information-aware loss balancing, and knowledge distillation from the exponential moving average (EMA) teacher. Through experiments, we demonstrate that our UnInfo improves accuracy under sensor degradation by retaining information in terms of uniformity.</li>
</ul>

<h3>Title: Active Learning on Synthons for Molecular Design</h3>
<ul>
<li><strong>Authors: </strong>Tom George Grigg, Mason Burlage, Oliver Brook Scott, Adam Taouil, Dominique Sydow, Liam Wilbraham</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12913">https://arxiv.org/abs/2505.12913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12913">https://arxiv.org/pdf/2505.12913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12913]] Active Learning on Synthons for Molecular Design(https://arxiv.org/abs/2505.12913)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Exhaustive virtual screening is highly informative but often intractable against the expensive objective functions involved in modern drug discovery. This problem is exacerbated in combinatorial contexts such as multi-vector expansion, where molecular spaces can quickly become ultra-large. Here, we introduce Scalable Active Learning via Synthon Acquisition (SALSA): a simple algorithm applicable to multi-vector expansion which extends pool-based active learning to non-enumerable spaces by factoring modeling and acquisition over synthon or fragment choices. Through experiments on ligand- and structure-based objectives, we highlight SALSA's sample efficiency, and its ability to scale to spaces of trillions of compounds. Further, we demonstrate application toward multi-parameter objective design tasks on three protein targets - finding SALSA-generated molecules have comparable chemical property profiles to known bioactives, and exhibit greater diversity and higher scores over an industry-leading generative approach.</li>
</ul>

<h3>Title: Temporal Query Network for Efficient Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Shengsheng Lin, Haojun Chen, Haijie Wu, Chunyun Qiu, Weiwei Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12917">https://arxiv.org/abs/2505.12917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12917">https://arxiv.org/pdf/2505.12917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12917]] Temporal Query Network for Efficient Multivariate Time Series Forecasting(https://arxiv.org/abs/2505.12917)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sufficiently modeling the correlations among variables (aka channels) is crucial for achieving accurate multivariate time series forecasting (MTSF). In this paper, we propose a novel technique called Temporal Query (TQ) to more effectively capture multivariate correlations, thereby improving model performance in MTSF tasks. Technically, the TQ technique employs periodically shifted learnable vectors as queries in the attention mechanism to capture global inter-variable patterns, while the keys and values are derived from the raw input data to encode local, sample-level correlations. Building upon the TQ technique, we develop a simple yet efficient model named Temporal Query Network (TQNet), which employs only a single-layer attention mechanism and a lightweight multi-layer perceptron (MLP). Extensive experiments demonstrate that TQNet learns more robust multivariate correlations, achieving state-of-the-art forecasting accuracy across 12 challenging real-world datasets. Furthermore, TQNet achieves high efficiency comparable to linear-based methods even on high-dimensional datasets, balancing performance and computational cost. The code is available at: this https URL.</li>
</ul>

<h3>Title: RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees</h3>
<ul>
<li><strong>Authors: </strong>Eilon Vaknin Laufer, Boaz Nadler</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12919">https://arxiv.org/abs/2505.12919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12919">https://arxiv.org/pdf/2505.12919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12919]] RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees(https://arxiv.org/abs/2505.12919)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recovering a low rank matrix from a subset of its entries, some of which may be corrupted, is known as the robust matrix completion (RMC) problem. Existing RMC methods have several limitations: they require a relatively large number of observed entries; they may fail under overparametrization, when their assumed rank is higher than the correct one; and many of them fail to recover even mildly ill-conditioned matrices. In this paper we propose a novel RMC method, denoted $\texttt{RGNMR}$, which overcomes these limitations. $\texttt{RGNMR}$ is a simple factorization-based iterative algorithm, which combines a Gauss-Newton linearization with removal of entries suspected to be outliers. On the theoretical front, we prove that under suitable assumptions, $\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank matrix. Our theoretical results improve upon the best currently known for factorization-based methods. On the empirical front, we show via several simulations the advantages of $\texttt{RGNMR}$ over existing RMC methods, and in particular its ability to handle a small number of observed entries, overparameterization of the rank and ill-conditioned matrices.</li>
</ul>

<h3>Title: Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhihe Yang, Xufang Luo, Zilong Wang, Dongqi Han, Zhiyuan He, Dongsheng Li, Yunjian Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12929">https://arxiv.org/abs/2505.12929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12929">https://arxiv.org/pdf/2505.12929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12929]] Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs(https://arxiv.org/abs/2505.12929)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has become a cornerstone for enhancing the reasoning capabilities of large language models (LLMs), with recent innovations such as Group Relative Policy Optimization (GRPO) demonstrating exceptional effectiveness. In this study, we identify a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes. This dominance hinders the effective learning of high-probability tokens, whose gradients are essential for LLMs' performance but are substantially suppressed. To mitigate this interference, we propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti), both of which effectively attenuate gradients from low-probability tokens while emphasizing parameter updates driven by high-probability tokens. Our approaches promote balanced updates across tokens with varying probabilities, thereby enhancing the efficiency of RL training. Experimental results demonstrate that they substantially improve the performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K Logic Puzzle reasoning tasks. Our implementation is available at this https URL.</li>
</ul>

<h3>Title: LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Di You, Daniel Siromani, Pier Luigi Dragotti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12935">https://arxiv.org/abs/2505.12935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12935">https://arxiv.org/pdf/2505.12935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12935]] LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration(https://arxiv.org/abs/2505.12935)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>There is a growing interest in the use of latent diffusion models (LDMs) for image restoration (IR) tasks due to their ability to model effectively the distribution of natural images. While significant progress has been made, there are still key challenges that need to be addressed. First, many approaches depend on a predefined degradation operator, making them ill-suited for complex or unknown degradations that deviate from standard analytical models. Second, many methods struggle to provide a stable guidance in the latent space and finally most methods convert latent representations back to the pixel domain for guidance at every sampling iteration, which significantly increases computational and memory overhead. To overcome these limitations, we introduce a wavelet-inspired invertible neural network (INN) that simulates degradations through a forward transform and reconstructs lost details via the inverse transform. We further integrate this design into a latent diffusion pipeline through two proposed approaches: LatentINDIGO-PixelINN, which operates in the pixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space to reduce complexity. Both approaches alternate between updating intermediate latent variables under the guidance of our INN and refining the INN forward model to handle unknown degradations. In addition, a regularization step preserves the proximity of latent variables to the natural image manifold. Experiments demonstrate that our algorithm achieves state-of-the-art performance on synthetic and real-world low-quality images, and can be readily adapted to arbitrary output sizes.</li>
</ul>

<h3>Title: Leveraging LLM Inconsistency to Boost Pass@k Performance</h3>
<ul>
<li><strong>Authors: </strong>Uri Dalal, Meirav Segal, Zvika Ben-Haim, Dan Lahav, Omer Nevo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12938">https://arxiv.org/abs/2505.12938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12938">https://arxiv.org/pdf/2505.12938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12938]] Leveraging LLM Inconsistency to Boost Pass@k Performance(https://arxiv.org/abs/2505.12938)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve impressive abilities in numerous domains, but exhibit inconsistent performance in response to minor input changes. Rather than view this as a drawback, in this paper we introduce a novel method for leveraging models' inconsistency to boost Pass@k performance. Specifically, we present a "Variator" agent that generates k variants of a given task and submits one candidate solution for each one. Our variant generation approach is applicable to a wide range of domains as it is task agnostic and compatible with free-form inputs. We demonstrate the efficacy of our agent theoretically using a probabilistic model of the inconsistency effect, and show empirically that it outperforms the baseline on the APPS dataset. Furthermore, we establish that inconsistency persists even in frontier reasoning models across coding and cybersecurity domains, suggesting our method is likely to remain relevant for future model generations.</li>
</ul>

<h3>Title: A3 : an Analytical Low-Rank Approximation Framework for Attention</h3>
<ul>
<li><strong>Authors: </strong>Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12942">https://arxiv.org/abs/2505.12942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12942">https://arxiv.org/pdf/2505.12942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12942]] A3 : an Analytical Low-Rank Approximation Framework for Attention(https://arxiv.org/abs/2505.12942)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.</li>
</ul>

<h3>Title: CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs</h3>
<ul>
<li><strong>Authors: </strong>Jan Hagnberger, Daniel Musekamp, Mathias Niepert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12944">https://arxiv.org/abs/2505.12944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12944">https://arxiv.org/pdf/2505.12944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12944]] CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs(https://arxiv.org/abs/2505.12944)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized spatial domain is a fundamental problem in various scientific and engineering disciplines, including modeling climate phenomena and fluid dynamics. However, performing these computations directly in the physical space often incurs significant computational costs. To address this issue, several neural surrogate models have been developed that operate in a compressed latent space to solve the PDE. While these approaches reduce computational complexity, they often use Transformer-based attention mechanisms to handle irregularly sampled domains, resulting in increased memory consumption. In contrast, convolutional neural networks allow memory-efficient encoding and decoding but are limited to regular discretizations. Motivated by these considerations, we propose CALM-PDE, a model class that efficiently solves arbitrarily discretized PDEs in a compressed latent space. We introduce a novel continuous convolution-based encoder-decoder architecture that uses an epsilon-neighborhood-constrained kernel and learns to apply the convolution operator to adaptive and optimized query points. We demonstrate the effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and irregularly sampled spatial domains. CALM-PDE is competitive with or outperforms existing baseline methods while offering significant improvements in memory and inference time efficiency compared to Transformer-based methods.</li>
</ul>

<h3>Title: GuRE:Generative Query REwriter for Legal Passage Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Daehee Kim, Deokhyung Kang, Jonghwi Kim, Sangwon Ryu, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12950">https://arxiv.org/abs/2505.12950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12950">https://arxiv.org/pdf/2505.12950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12950]] GuRE:Generative Query REwriter for Legal Passage Retrieval(https://arxiv.org/abs/2505.12950)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Legal Passage Retrieval (LPR) systems are crucial as they help practitioners save time when drafting legal arguments. However, it remains an underexplored avenue. One primary reason is the significant vocabulary mismatch between the query and the target passage. To address this, we propose a simple yet effective method, the Generative query REwriter (GuRE). We leverage the generative capabilities of Large Language Models (LLMs) by training the LLM for query rewriting. "Rewritten queries" help retrievers to retrieve target passages by mitigating vocabulary mismatch. Experimental results show that GuRE significantly improves performance in a retriever-agnostic manner, outperforming all baseline methods. Further analysis reveals that different training objectives lead to distinct retrieval behaviors, making GuRE more suitable than direct retriever fine-tuning for real-world applications. Codes are avaiable at this http URL.</li>
</ul>

<h3>Title: DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management</h3>
<ul>
<li><strong>Authors: </strong>Xuerui Su, Liya Guo, Yue Wang, Yi Zhu, Zhiming Ma, Zun Wang, Yuting Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12951">https://arxiv.org/abs/2505.12951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12951">https://arxiv.org/pdf/2505.12951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12951]] DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management(https://arxiv.org/abs/2505.12951)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Inference scaling further accelerates Large Language Models (LLMs) toward Artificial General Intelligence (AGI), with large-scale Reinforcement Learning (RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning approaches usually rely on handcrafted rule-based reward functions. However, the tarde-offs of exploration and exploitation in RL algorithms involves multiple complex considerations, and the theoretical and empirical impacts of manually designed reward functions remain insufficiently explored. In this paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional regularization coefficient into two independent hyperparameters: one scales the policy gradient term, and the other regulates the distance from the sampling policy. This decoupling not only enables precise control over balancing exploration and exploitation, but also can be seamlessly extended to Online Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward Optimization. On the other hand, we observe that reward variance significantly affects both convergence speed and final model performance. We conduct both theoretical analysis and extensive empirical validation to assess DGRO, including a detailed ablation study that investigates its performance and optimization dynamics. Experimental results show that DGRO achieves state-of-the-art performance on the Logic dataset with an average accuracy of 96.9\%, and demonstrates strong generalization across mathematical benchmarks.</li>
</ul>

<h3>Title: MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shanshan Liu, Noriki Nishida, Rumana Ferdous Munne, Narumi Tokunaga, Yuki Yamagata, Kouji Kozaki, Yuji Matsumoto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12964">https://arxiv.org/abs/2505.12964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12964">https://arxiv.org/pdf/2505.12964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12964]] MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition(https://arxiv.org/abs/2505.12964)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recognizing biomedical concepts in the text is vital for ontology refinement, knowledge graph construction, and concept relationship discovery. However, traditional concept recognition methods, relying on explicit mention identification, often fail to capture complex concepts not explicitly stated in the text. To overcome this limitation, we introduce MA-COIR, a framework that reformulates concept recognition as an indexing-recognition task. By assigning semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in ontology entries and enhances recognition efficiency. Using a pretrained BART-based model fine-tuned on small datasets, our approach reduces computational requirements to facilitate adoption by domain experts. Furthermore, we incorporate large language models (LLMs)-generated queries and synthetic data to improve recognition in low-resource settings. Experimental results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of MA-COIR in recognizing both explicit and implicit concepts without the need for mention-level annotations during inference, advancing ontology-driven concept recognition in biomedical domain applications. Our code and constructed data are available at this https URL.</li>
</ul>

<h3>Title: Lara: Lightweight Anonymous Authentication with Asynchronous Revocation Auditability</h3>
<ul>
<li><strong>Authors: </strong>Claudio Correia, Guilherme Santos, Luis Rodrigues</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12968">https://arxiv.org/abs/2505.12968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12968">https://arxiv.org/pdf/2505.12968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12968]] Lara: Lightweight Anonymous Authentication with Asynchronous Revocation Auditability(https://arxiv.org/abs/2505.12968)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Anonymous authentication is a technique that allows to combine access control with privacy preservation. Typically, clients use different pseudonyms for each access, hindering providers from correlating their activities. To perform the revocation of pseudonyms in a privacy preserving manner is notoriously challenging. When multiple pseudonyms are revoked together, an adversary may infer that these pseudonyms belong to the same client and perform privacy breaking correlations, in particular if these pseudonyms have already been used. Backward unlinkability and revocation auditability are two properties that address this problem. Most systems that offer these properties rely on some sort of time slots, which assume a common reference of time that must be shared among clients and providers; for instance, the client must be aware that it should not use a pseudonym after a certain time or should be able to assess the freshness of a revocation list prior to perform authentication. In this paper we propose Lara, a Lightweight Anonymous Authentication with Asynchronous Revocation Auditability that does not require parties to agree on the current time slot and it is not affected by the clock skew. Prior to disclosing a pseudonym, clients are provided with a revocation list (RL) and can check that the pseudonym has not been revoked. Then, they provide a proof on non-revocation that cannot be used against any other (past or future) RL, avoiding any dependency of timing assumptions. Lara can be implemented using efficient public-key primitives and space-efficient data structures. We have implemented a prototype of Lara and have assessed experimentally its efficiency.</li>
</ul>

<h3>Title: A Structured Literature Review on Traditional Approaches in Current Natural Language Processing</h3>
<ul>
<li><strong>Authors: </strong>Robin Jegan, Andreas Henrich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12970">https://arxiv.org/abs/2505.12970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12970">https://arxiv.org/pdf/2505.12970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12970]] A Structured Literature Review on Traditional Approaches in Current Natural Language Processing(https://arxiv.org/abs/2505.12970)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The continued rise of neural networks and large language models in the more recent past has altered the natural language processing landscape, enabling new approaches towards typical language tasks and achieving mainstream success. Despite the huge success of large language models, many disadvantages still remain and through this work we assess the state of the art in five application scenarios with a particular focus on the future perspectives and sensible application scenarios of traditional and older approaches and techniques. In this paper we survey recent publications in the application scenarios classification, information and relation extraction, text simplification as well as text summarization. After defining our terminology, i.e., which features are characteristic for traditional techniques in our interpretation for the five scenarios, we survey if such traditional approaches are still being used, and if so, in what way they are used. It turns out that all five application scenarios still exhibit traditional models in one way or another, as part of a processing pipeline, as a comparison/baseline to the core model of the respective paper, or as the main model(s) of the paper. For the complete statistics, see this https URL</li>
</ul>

<h3>Title: From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Liangxuan Wu, Chao Wang, Tianming Liu, Yanjie Zhao, Haoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12981">https://arxiv.org/abs/2505.12981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12981">https://arxiv.org/pdf/2505.12981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12981]] From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents(https://arxiv.org/abs/2505.12981)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, large language model</a></li>
<li><strong>Abstract: </strong>The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing--LLM-powered mobile AI agents--capable of decomposing and automating complex tasks directly on smartphones. However, the security implications of these agents remain largely unexplored. In this paper, we present the first comprehensive security analysis of mobile LLM agents, encompassing three representative categories: System-level AI Agents developed by original equipment manufacturers (e.g., YOYO Assistant), Third-party Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g., Alibaba Mobile Agent). We begin by analyzing the general workflow of mobile agents and identifying security threats across three core capability dimensions: language-based reasoning, GUI-based interaction, and system-level execution. Our analysis reveals 11 distinct attack surfaces, all rooted in the unique capabilities and interaction patterns of mobile LLM agents, and spanning their entire operational lifecycle. To investigate these threats in practice, we introduce AgentScan, a semi-automated security analysis framework that systematically evaluates mobile LLM agents across all 11 attack scenarios. Applying AgentScan to nine widely deployed agents, we uncover a concerning trend: every agent is vulnerable to targeted attacks. In the most severe cases, agents exhibit vulnerabilities across eight distinct attack vectors. These attacks can cause behavioral deviations, privacy leakage, or even full execution hijacking. Based on these findings, we propose a set of defensive design principles and practical recommendations for building secure mobile LLM agents. Our disclosures have received positive feedback from two major device vendors. Overall, this work highlights the urgent need for standardized security practices in the fast-evolving landscape of LLM-driven mobile automation.</li>
</ul>

<h3>Title: An Empirical Study of Many-to-Many Summarization with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaan Wang, Fandong Meng, Zengkui Sun, Yunlong Liang, Yuxuan Cao, Jiarong Xu, Haoxiang Shi, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12983">https://arxiv.org/abs/2505.12983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12983">https://arxiv.org/pdf/2505.12983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12983]] An Empirical Study of Many-to-Many Summarization with Large Language Models(https://arxiv.org/abs/2505.12983)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs' M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate that this task-specific improvement does not sacrifice the LLMs' general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worth noting in future research.</li>
</ul>

<h3>Title: Fractured Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12992">https://arxiv.org/abs/2505.12992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12992">https://arxiv.org/pdf/2505.12992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12992]] Fractured Chain-of-Thought Reasoning(https://arxiv.org/abs/2505.12992)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.</li>
</ul>

<h3>Title: ACE: Confidential Computing for Embedded RISC-V Systems</h3>
<ul>
<li><strong>Authors: </strong>Wojciech Ozga, Guerney D.H. Hunt, Michael V. Le, Lennard Gher, Avraham Shinnar, Elaine R. Palmer, Hani Jamjoom, Silvio Dragone</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.12995">https://arxiv.org/abs/2505.12995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.12995">https://arxiv.org/pdf/2505.12995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.12995]] ACE: Confidential Computing for Embedded RISC-V Systems(https://arxiv.org/abs/2505.12995)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Confidential computing plays an important role in isolating sensitive applications from the vast amount of untrusted code commonly found in the modern cloud. We argue that it can also be leveraged to build safer and more secure mission-critical embedded systems. In this paper, we introduce the Assured Confidential Execution (ACE), an open-source and royalty-free confidential computing technology targeted for embedded RISC-V systems. We present a set of principles and a methodology that we used to build \ACE and that might be applied for developing other embedded systems that require formal verification. An evaluation of our prototype on the first available RISC-V hardware supporting virtualization indicates that ACE is a viable candidate for our target systems.</li>
</ul>

<h3>Title: Generative Modeling of Random Fields from Limited Data via Constrained Latent Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>James E. Warner, Tristan A. Shah, Patrick E. Leser, Geoffrey F. Bomarito, Joshua D. Pribe, Michael C. Stanley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13007">https://arxiv.org/abs/2505.13007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13007">https://arxiv.org/pdf/2505.13007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13007]] Generative Modeling of Random Fields from Limited Data via Constrained Latent Flow Matching(https://arxiv.org/abs/2505.13007)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models are promising tools for science and engineering, but their reliance on abundant, high-quality data limits applicability. We present a novel framework for generative modeling of random fields (probability distributions over continuous functions) that incorporates domain knowledge to supplement limited, sparse, and indirect data. The foundation of the approach is latent flow matching, where generative modeling occurs on compressed function representations in the latent space of a pre-trained variational autoencoder (VAE). Innovations include the adoption of a function decoder within the VAE and integration of physical/statistical constraints into the VAE training process. In this way, a latent function representation is learned that yields continuous random field samples satisfying domain-specific constraints when decoded, even in data-limited regimes. Efficacy is demonstrated on two challenging applications: wind velocity field reconstruction from sparse sensors and material property inference from a limited number of indirect measurements. Results show that the proposed framework achieves significant improvements in reconstruction accuracy compared to unconstrained methods and enables effective inference with relatively small training datasets that is intractable without constraints.</li>
</ul>

<h3>Title: To Bias or Not to Bias: Detecting bias in News with bias-detector</h3>
<ul>
<li><strong>Authors: </strong>Himel Ghosh, Ahmed Mosharafa, Georg Groh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13010">https://arxiv.org/abs/2505.13010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13010">https://arxiv.org/pdf/2505.13010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13010]] To Bias or Not to Bias: Detecting bias in News with bias-detector(https://arxiv.org/abs/2505.13010)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, interpretability</a></li>
<li><strong>Abstract: </strong>Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.</li>
</ul>

<h3>Title: Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions</h3>
<ul>
<li><strong>Authors: </strong>Yimao Guo, Zuomin Qu, Wei Lu, Xiangyang Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13023">https://arxiv.org/abs/2505.13023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13023">https://arxiv.org/pdf/2505.13023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13023]] Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions(https://arxiv.org/abs/2505.13023)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, robust, diffusion</a></li>
<li><strong>Abstract: </strong>As diffusion-based malicious image manipulation becomes increasingly prevalent, multiple proactive defense methods are developed to safeguard images against unauthorized tampering. However, most proactive defense methods only can safeguard images against manipulation under known conditions, and fail to protect images from manipulations guided by tampering conditions crafted by malicious users. To tackle this issue, we propose Anti-Inpainting, a proactive defense method that achieves adequate protection under unknown conditions through a triple mechanism to address this challenge. Specifically, a multi-level deep feature extractor is presented to obtain intricate features during the diffusion denoising process to improve protective effectiveness. We design multi-scale semantic-preserving data augmentation to enhance the transferability of adversarial perturbations across unknown conditions by multi-scale transformations while preserving semantic integrity. In addition, we propose a selection-based distribution deviation optimization strategy to improve the protection of adversarial perturbation against manipulation under diverse random seeds. Extensive experiments indicate the proactive defensive performance of Anti-Inpainting against diffusion-based inpainters guided by unknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we also demonstrate the proposed approach's robustness under various image purification methods and its transferability across different versions of diffusion models.</li>
</ul>

<h3>Title: Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, Xiao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13026">https://arxiv.org/abs/2505.13026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13026">https://arxiv.org/pdf/2505.13026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13026]] Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs(https://arxiv.org/abs/2505.13026)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at mathematical reasoning and logical problem-solving. The current popular training paradigms primarily use supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the models' reasoning abilities. However, when using SFT or RL alone, there are respective challenges: SFT may suffer from overfitting, while RL is prone to mode collapse. The state-of-the-art methods have proposed hybrid training schemes. However, static switching faces challenges such as poor generalization across different tasks and high dependence on data quality. In response to these challenges, inspired by the curriculum learning-quiz mechanism in human reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training framework that theoretically unifies SFT and RL and dynamically balances the two throughout optimization. SASR uses SFT for initial warm-up to establish basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm based on gradient norm and divergence relative to the original distribution to seamlessly integrate SFT with the online RL method GRPO. By monitoring the training status of LLMs and adjusting the training process in sequence, SASR ensures a smooth transition between training schemes, maintaining core reasoning abilities while exploring different paths. Experimental results demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.</li>
</ul>

<h3>Title: Unpacking Positional Encoding in Transformers: A Spectral Analysis of Content-Position Coupling</h3>
<ul>
<li><strong>Authors: </strong>Zihan Gu, Han Zhang, Ruoyu Chen, Yue Hu, Hua Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13027">https://arxiv.org/abs/2505.13027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13027">https://arxiv.org/pdf/2505.13027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13027]] Unpacking Positional Encoding in Transformers: A Spectral Analysis of Content-Position Coupling(https://arxiv.org/abs/2505.13027)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Positional encoding (PE) is essential for enabling Transformers to model sequential structure. However, the mechanisms by which different PE schemes couple token content and positional information-and how these mechanisms influence model dynamics-remain theoretically underexplored. In this work, we present a unified framework that analyzes PE through the spectral properties of Toeplitz and related matrices derived from attention logits. We show that multiplicative content-position coupling-exemplified by Rotary Positional Encoding (RoPE) via a Hadamard product with a Toeplitz matrix-induces spectral contraction, which theoretically improves optimization stability and efficiency. Guided by this theory, we construct synthetic tasks that contrast content-position dependent and content-position independent settings, and evaluate a range of PE methods. Our experiments reveal strong alignment with theory: RoPE consistently outperforms other methods on position-sensitive tasks and induces "single-head deposit" patterns in early layers, indicating localized positional processing. Further analyses show that modifying the method and timing of PE coupling, such as MLA in Deepseek-V3, can effectively mitigate this concentration. These results establish explicit content-relative mixing with relative-position Toeplitz signals as a key principle for effective PE design and provide new insight into how positional structure is integrated in Transformer architectures.</li>
</ul>

<h3>Title: Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset</h3>
<ul>
<li><strong>Authors: </strong>Sayon Palit, Daniel Woods</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13028">https://arxiv.org/abs/2505.13028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13028">https://arxiv.org/pdf/2505.13028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13028]] Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset(https://arxiv.org/abs/2505.13028)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated into critical systems in industries like healthcare and finance. Users can often submit queries to LLM-enabled chatbots, some of which can enrich responses with information retrieved from internal databases storing sensitive data. This gives rise to a range of attacks in which a user submits a malicious query and the LLM-system outputs a response that creates harm to the owner, such as leaking internal data or creating legal liability by harming a third-party. While security tools are being developed to counter these threats, there is little formal evaluation of their effectiveness and usability. This study addresses this gap by conducting a thorough comparative analysis of LLM security tools. We identified 13 solutions (9 closed-source, 4 open-source), but only 7 were evaluated due to a lack of participation by proprietary model this http URL evaluate, we built a benchmark dataset of malicious prompts, and evaluate these tools performance against a baseline LLM model (ChatGPT-3.5-Turbo). Our results show that the baseline model has too many false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard emerged as the best overall tools showcasing the tradeoff between usability and performance. The study concluded with recommendations for greater transparency among closed source providers, improved context-aware detections, enhanced open-source engagement, increased user awareness, and the adoption of more representative performance metrics.</li>
</ul>

<h3>Title: TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis</h3>
<ul>
<li><strong>Authors: </strong>Vijay Ekambaram, Subodh Kumar, Arindam Jati, Sumanta Mukherjee, Tomoya Sakai, Pankaj Dayama, Wesley M. Gifford, Jayant Kalagnanam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13033">https://arxiv.org/abs/2505.13033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13033">https://arxiv.org/pdf/2505.13033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13033]] TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis(https://arxiv.org/abs/2505.13033)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rise of time-series pre-trained models has advanced temporal representation learning, but current state-of-the-art models are often large-scale, requiring substantial compute. We introduce TSPulse, ultra-compact time-series pre-trained models with only 1M parameters, specialized to perform strongly across classification, anomaly detection, imputation, and retrieval tasks. TSPulse introduces innovations at both the architecture and task levels. At the architecture level, it employs a dual-space masked reconstruction, learning from both time and frequency domains to capture complementary signals. This is further enhanced by a dual-embedding disentanglement, generating both detailed embeddings for fine-grained analysis and high-level semantic embeddings for broader task understanding. Notably, TSPulse's semantic embeddings are robust to shifts in time, magnitude, and noise, which is important for robust retrieval. At the task level, TSPulse incorporates TSLens, a fine-tuning component enabling task-specific feature attention. It also introduces a multi-head triangulation technique that correlates deviations from multiple prediction heads, enhancing anomaly detection by fusing complementary model outputs. Additionally, a hybrid mask pretraining is proposed to improves zero-shot imputation by reducing pre-training bias. These architecture and task innovations collectively contribute to TSPulse's significant performance gains: 5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly detection leaderboard, +50% in zero-shot imputation, and +25% in time-series retrieval. Remarkably, these results are achieved with just 1M parameters, making TSPulse 10-100X smaller than existing pre-trained models. Its efficiency enables GPU-free inference and rapid pre-training, setting a new standard for efficient time-series pre-trained models. Models will be open-sourced soon.</li>
</ul>

<h3>Title: KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025</h3>
<ul>
<li><strong>Authors: </strong>Sai Koneru, Maike Zfle, Thai-Binh Nguyen, Seymanur Akti, Jan Niehues, Alexander Waibel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13036">https://arxiv.org/abs/2505.13036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13036">https://arxiv.org/pdf/2505.13036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13036]] KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025(https://arxiv.org/abs/2505.13036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The scope of the International Workshop on Spoken Language Translation (IWSLT) has recently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answering and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of Technology's submissions for the Offline ST and Instruction Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recognition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to improve translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level refinement stage to further enhance output quality by using contextual information.</li>
</ul>

<h3>Title: Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wu, Xiaoqing Zhang, Zunjie Xiao, Lingxi Hu, Risa Higashita, Jiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13039">https://arxiv.org/abs/2505.13039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13039">https://arxiv.org/pdf/2505.13039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13039]] Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification(https://arxiv.org/abs/2505.13039)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Efficient convolutional neural network (CNN) architecture designs have attracted growing research interests. However, they usually apply single receptive field (RF), small asymmetric RFs, or pyramid RFs to learn different feature representations, still encountering two significant challenges in medical image classification tasks: 1) They have limitations in capturing diverse lesion characteristics efficiently, e.g., tiny, coordination, small and salient, which have unique roles on results, especially imbalanced medical image classification. 2) The predictions generated by those CNNs are often unfair/biased, bringing a high risk by employing them to real-world medical diagnosis conditions. To tackle these issues, we develop a new concept, Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields (ERoHPRF), to simultaneously boost medical image classification performance and fairness. This concept aims to mimic the multi-expert consultation mode by applying the well-designed heterogeneous pyramid RF bags to capture different lesion characteristics effectively via convolution operations with multiple heterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like structural reparameterization technique to merge its parameters with the two-stage strategy, ensuring competitive computation cost and inference speed through comparisons to a single RF. To manifest the effectiveness and generalization ability of ERoHPRF, we incorporate it into mainstream efficient CNN architectures. The extensive experiments show that our method maintains a better trade-off than state-of-the-art methods in terms of medical image classification, fairness, and computation overhead. The codes of this paper will be released soon.</li>
</ul>

<h3>Title: PPTNet: A Hybrid Periodic Pattern-Transformer Architecture for Traffic Flow Prediction and Congestion Identification</h3>
<ul>
<li><strong>Authors: </strong>Hongrui Kou, Jingkai Li, Ziyu Wang, Zhouhang Lv, Yuxin Zhang, Cheng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13047">https://arxiv.org/abs/2505.13047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13047">https://arxiv.org/pdf/2505.13047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13047]] PPTNet: A Hybrid Periodic Pattern-Transformer Architecture for Traffic Flow Prediction and Congestion Identification(https://arxiv.org/abs/2505.13047)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Accurate prediction of traffic flow parameters and real time identification of congestion states are essential for the efficient operation of intelligent transportation systems. This paper proposes a Periodic Pattern Transformer Network (PPTNet) for traffic flow prediction, integrating periodic pattern extraction with the Transformer architecture, coupled with a fuzzy inference method for real-time congestion identification. Firstly, a high-precision traffic flow dataset (Traffic Flow Dataset for China's Congested Highways and Expressways, TF4CHE) suitable for congested highway scenarios in China is constructed based on drone aerial imagery data. Subsequently, the proposed PPTNet employs Fast Fourier Transform to capture multi-scale periodic patterns and utilizes two-dimensional Inception convolutions to efficiently extract intra and inter periodic features. A Transformer decoder dynamically models temporal dependencies, enabling accurate predictions of traffic density and speed. Finally, congestion probabilities are calculated in real-time using the predicted outcomes via a Mamdani fuzzy inference-based congestion identification module. Experimental results demonstrate that the proposed PPTNet significantly outperforms mainstream traffic prediction methods in prediction accuracy, and the congestion identification module effectively identifies real-time road congestion states, verifying the superiority and practicality of the proposed method in real-world traffic scenarios. Project page: this https URL.</li>
</ul>

<h3>Title: RGB-to-Polarization Estimation: A New Task and Benchmark Study</h3>
<ul>
<li><strong>Authors: </strong>Beibei Lin, Zifeng Yuan, Tingting Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13050">https://arxiv.org/abs/2505.13050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13050">https://arxiv.org/pdf/2505.13050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13050]] RGB-to-Polarization Estimation: A New Task and Benchmark Study(https://arxiv.org/abs/2505.13050)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Polarization images provide rich physical information that is fundamentally absent from standard RGB images, benefiting a wide range of computer vision applications such as reflection separation and material classification. However, the acquisition of polarization images typically requires additional optical components, which increases both the cost and the complexity of the applications. To bridge this gap, we introduce a new task: RGB-to-polarization image estimation, which aims to infer polarization information directly from RGB images. In this work, we establish the first comprehensive benchmark for this task by leveraging existing polarization datasets and evaluating a diverse set of state-of-the-art deep learning models, including both restoration-oriented and generative architectures. Through extensive quantitative and qualitative analysis, our benchmark not only establishes the current performance ceiling of RGB-to-polarization estimation, but also systematically reveals the respective strengths and limitations of different model families -- such as direct reconstruction versus generative synthesis, and task-specific training versus large-scale pre-training. In addition, we provide some potential directions for future research on polarization estimation. This benchmark is intended to serve as a foundational resource to facilitate the design and evaluation of future methods for polarization estimation from standard RGB inputs.</li>
</ul>

<h3>Title: Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs</h3>
<ul>
<li><strong>Authors: </strong>Shmulik Markovich-Golan, Daniel Ohayon, Itay Niv, Yair Hanani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13060">https://arxiv.org/abs/2505.13060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13060">https://arxiv.org/pdf/2505.13060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13060]] Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs(https://arxiv.org/abs/2505.13060)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Quantization is essential for Neural Network (NN) compression, reducing model size and computational demands by using lower bit-width data types, though aggressive reduction often hampers accuracy. Mixed Precision (MP) mitigates this tradeoff by varying the numerical precision across network layers. This study focuses on automatically selecting an optimal MP configuration within Post-Training Quantization (PTQ) for inference. The first key contribution is a novel sensitivity metric derived from a first-order Taylor series expansion of the loss function as a function of quantization errors in weights and activations. This metric, based on the Mean Square Error (MSE) of the loss, is efficiently calculated per layer using high-precision forward and backward passes over a small calibration dataset. The metric is additive across layers, with low calibration memory overhead as weight optimization is unnecessary. The second contribution is an accurate hardware-aware method for predicting MP time gain by modeling it as additive for sequential sub-graphs. An algorithm partitions the model graph into sequential subgraphs, measuring time gain for each configuration using a few samples. After calibrating per-layer sensitivity and time gain, an Integer Programming (IP) problem is formulated to maximize time gain while keeping loss MSE below a set threshold. Memory gain and theoretical time gain based on Multiply and Accumulate (MAC) operations are also considered. Rigorous experiments on the Intel Gaudi 2 accelerator validate the approach on several Large Language Models (LLMs).</li>
</ul>

<h3>Title: 3D Visual Illusion Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>CHengtang Yao, Zhidan Liu, Jiaxi Zeng, Lidong Yu, Yuwei Wu, Yunde Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13061">https://arxiv.org/abs/2505.13061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13061">https://arxiv.org/pdf/2505.13061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13061]] 3D Visual Illusion Depth Estimation(https://arxiv.org/abs/2505.13061)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D visual illusion is a perceptual phenomenon where a two-dimensional plane is manipulated to simulate three-dimensional spatial relationships, making a flat artwork or object look three-dimensional in the human visual system. In this paper, we reveal that the machine visual system is also seriously fooled by 3D visual illusions, including monocular and binocular depth estimation. In order to explore and analyze the impact of 3D visual illusion on depth estimation, we collect a large dataset containing almost 3k scenes and 200k images to train and evaluate SOTA monocular and binocular depth estimation methods. We also propose a robust depth estimation framework that uses common sense from a vision-language model to adaptively select reliable depth from binocular disparity and monocular depth. Experiments show that SOTA monocular, binocular, and multi-view depth estimation approaches are all fooled by various 3D visual illusions, while our method achieves SOTA performance.</li>
</ul>

<h3>Title: OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jie Yan, Xin Liu, Zhong-Yuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13071">https://arxiv.org/abs/2505.13071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13071">https://arxiv.org/pdf/2505.13071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13071]] OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction(https://arxiv.org/abs/2505.13071)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated clustering (FC) aims to discover global cluster structures across decentralized clients without sharing raw data, making privacy preservation a fundamental requirement. There are two critical challenges: (1) privacy leakage during collaboration, and (2) robustness degradation due to aggregation of proxy information from non-independent and identically distributed (Non-IID) local data, leading to inaccurate or inconsistent global clustering. Existing solutions typically rely on model-specific local proxies, which are sensitive to data heterogeneity and inherit inductive biases from their centralized counterparts, thus limiting robustness and generality. We propose Omni Federated Clustering (OmniFC), a unified and model-agnostic framework. Leveraging Lagrange coded computing, our method enables clients to share only encoded data, allowing exact reconstruction of the global distance matrix--a fundamental representation of sample relationships--without leaking private information, even under client collusion. This construction is naturally resilient to Non-IID data distributions. This approach decouples FC from model-specific proxies, providing a unified extension mechanism applicable to diverse centralized clustering methods. Theoretical analysis confirms both reconstruction fidelity and privacy guarantees, while comprehensive experiments demonstrate OmniFC's superior robustness, effectiveness, and generality across various benchmarks compared to state-of-the-art methods. Code will be released.</li>
</ul>

<h3>Title: Orthogonal Survival Learners for Estimating Heterogeneous Treatment Effects from Time-to-Event Data</h3>
<ul>
<li><strong>Authors: </strong>Dennis Frauen, Maresa Schrder, Konstantin Hess, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13072">https://arxiv.org/abs/2505.13072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13072">https://arxiv.org/pdf/2505.13072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13072]] Orthogonal Survival Learners for Estimating Heterogeneous Treatment Effects from Time-to-Event Data(https://arxiv.org/abs/2505.13072)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating heterogeneous treatment effects (HTEs) is crucial for personalized decision-making. However, this task is challenging in survival analysis, which includes time-to-event data with censored outcomes (e.g., due to study dropout). In this paper, we propose a toolbox of novel orthogonal survival learners to estimate HTEs from time-to-event data under censoring. Our learners have three main advantages: (i) we show that learners from our toolbox are guaranteed to be orthogonal and thus come with favorable theoretical properties; (ii) our toolbox allows for incorporating a custom weighting function, which can lead to robustness against different types of low overlap, and (iii) our learners are model-agnostic (i.e., they can be combined with arbitrary machine learning models). We instantiate the learners from our toolbox using several weighting functions and, as a result, propose various neural orthogonal survival learners. Some of these coincide with existing survival learners (including survival versions of the DR- and R-learner), while others are novel and further robust w.r.t. low overlap regimes specific to the survival setting (i.e., survival overlap and censoring overlap). We then empirically verify the effectiveness of our learners for HTE estimation in different low-overlap regimes through numerical experiments. In sum, we provide practitioners with a large toolbox of learners that can be used for randomized and observational studies with censored time-to-event data.</li>
</ul>

<h3>Title: The Hidden Dangers of Browsing AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Mykyta Mudryi, Markiyan Chaklosh, Grzegorz Wjcik</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13076">https://arxiv.org/abs/2505.13076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13076">https://arxiv.org/pdf/2505.13076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13076]] The Hidden Dangers of Browsing AI Agents(https://arxiv.org/abs/2505.13076)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Autonomous browsing agents powered by large language models (LLMs) are increasingly used to automate web-based tasks. However, their reliance on dynamic content, tool execution, and user-provided data exposes them to a broad attack surface. This paper presents a comprehensive security evaluation of such agents, focusing on systemic vulnerabilities across multiple architectural layers. Our work outlines the first end-to-end threat model for browsing agents and provides actionable guidance for securing their deployment in real-world environments. To address discovered threats, we propose a defense in depth strategy incorporating input sanitization, planner executor isolation, formal analyzers, and session safeguards. These measures protect against both initial access and post exploitation attack vectors. Through a white box analysis of a popular open source project, Browser Use, we demonstrate how untrusted web content can hijack agent behavior and lead to critical security breaches. Our findings include prompt injection, domain validation bypass, and credential exfiltration, evidenced by a disclosed CVE and a working proof of concept exploit.</li>
</ul>

<h3>Title: Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Yang, Jie Lu, En Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13081">https://arxiv.org/abs/2505.13081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13081">https://arxiv.org/pdf/2505.13081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13081]] Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning(https://arxiv.org/abs/2505.13081)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper uncovers a critical yet overlooked phenomenon in multi-modal large language models (MLLMs): detrimental concept drift within chain-of-thought (CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where reasoning token distributions evolve unpredictably, thereby introducing significant biases in final predictions. To address this, we are pioneers in establishing the theoretical bridge between concept drift theory and RFT processes by formalizing CoT's autoregressive token streams as non-stationary distributions undergoing arbitrary temporal shifts. Leveraging this framework, we propose a novel counterfact-aware RFT that systematically decouples beneficial distribution adaptation from harmful concept drift through concept graph-empowered LLM experts generating counterfactual reasoning trajectories. Our solution, Counterfactual Preference Optimization (CPO), enables stable RFT in non-stationary environments, particularly within the medical domain, through custom-tuning of counterfactual-aware preference alignment. Extensive experiments demonstrate our superior performance of robustness, generalization and coordination within RFT. Besides, we also contributed a large-scale dataset CXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual reasoning trajectories derived from MIMIC-CXR. Our code and data are public.</li>
</ul>

<h3>Title: Cross-modal feature fusion for robust point cloud registration with ambiguous geometry</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyi Wang, Shengyu Huang, Jemil Avers Butt, Yuanzhou Cai, Matej Varga, Andreas Wieser</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13088">https://arxiv.org/abs/2505.13088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13088">https://arxiv.org/pdf/2505.13088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13088]] Cross-modal feature fusion for robust point cloud registration with ambiguous geometry(https://arxiv.org/abs/2505.13088)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Point cloud registration has seen significant advancements with the application of deep learning techniques. However, existing approaches often overlook the potential of integrating radiometric information from RGB images. This limitation reduces their effectiveness in aligning point clouds pairs, especially in regions where geometric data alone is insufficient. When used effectively, radiometric information can enhance the registration process by providing context that is missing from purely geometric data. In this paper, we propose CoFF, a novel Cross-modal Feature Fusion method that utilizes both point cloud geometry and RGB images for pairwise point cloud registration. Assuming that the co-registration between point clouds and RGB images is available, CoFF explicitly addresses the challenges where geometric information alone is unclear, such as in regions with symmetric similarity or planar structures, through a two-stage fusion of 3D point cloud features and 2D image features. It incorporates a cross-modal feature fusion module that assigns pixel-wise image features to 3D input point clouds to enhance learned 3D point features, and integrates patch-wise image features with superpoint features to improve the quality of coarse matching. This is followed by a coarse-to-fine matching module that accurately establishes correspondences using the fused features. We extensively evaluate CoFF on four common datasets: 3DMatch, 3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In addition, we assess CoFF on specific subset datasets containing geometrically ambiguous cases. Our experimental results demonstrate that CoFF achieves state-of-the-art registration performance across all benchmarks, including remarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch and 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)</li>
</ul>

<h3>Title: Systematic Generalization in Language Models Scales with Information Entropy</h3>
<ul>
<li><strong>Authors: </strong>Sondre Wold, Lucas Georges Gabriel Charpentier, tienne Simon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13089">https://arxiv.org/abs/2505.13089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13089">https://arxiv.org/pdf/2505.13089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13089]] Systematic Generalization in Language Models Scales with Information Entropy(https://arxiv.org/abs/2505.13089)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Systematic generalization remains challenging for current language models, which are known to be both sensitive to semantically similar permutations of the input and to struggle with known concepts presented in novel contexts. Although benchmarks exist for assessing compositional behavior, it is unclear how to measure the difficulty of a systematic generalization problem. In this work, we show how one aspect of systematic generalization can be described by the entropy of the distribution of component parts in the training data. We formalize a framework for measuring entropy in a sequence-to-sequence task and find that the performance of popular model architectures scales with the entropy. Our work connects systematic generalization to information efficiency, and our results indicate that success at high entropy can be achieved even without built-in priors, and that success at low entropy can serve as a target for assessing progress towards robust systematic generalization.</li>
</ul>

<h3>Title: The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation</h3>
<ul>
<li><strong>Authors: </strong>David Stap, Christof Monz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13090">https://arxiv.org/abs/2505.13090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13090">https://arxiv.org/pdf/2505.13090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13090]] The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation(https://arxiv.org/abs/2505.13090)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prior research diverges on language diversity in LLM fine-tuning: Some studies report benefits while others find no advantages. Through controlled fine-tuning experiments across 132 translation directions, we systematically resolve these disparities. We find that expanding language diversity during fine-tuning improves translation quality for both unsupervised and -- surprisingly -- supervised pairs, despite less diverse models being fine-tuned exclusively on these supervised pairs. However, benefits plateau or decrease beyond a certain diversity threshold. We show that increased language diversity creates more language-agnostic representations. These representational adaptations help explain the improved performance in models fine-tuned with greater diversity.</li>
</ul>

<h3>Title: Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yuanbo Wang, Zhaoxuan Zhang, Jiajin Qiu, Dilong Sun, Zhengyu Meng, Xiaopeng Wei, Xin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13091">https://arxiv.org/abs/2505.13091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13091">https://arxiv.org/pdf/2505.13091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13091]] Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction(https://arxiv.org/abs/2505.13091)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have made breakthroughs in 3D generation tasks. Current 3D diffusion models focus on reconstructing target shape from images or a set of partial observations. While excelling in global context understanding, they struggle to capture the local details of complex shapes and limited to the occlusion and lighting conditions. To overcome these limitations, we utilize tactile images to capture the local 3D information and propose a Touch2Shape model, which leverages a touch-conditioned diffusion model to explore and reconstruct the target shape from touch. For shape reconstruction, we have developed a touch embedding module to condition the diffusion model in creating a compact representation and a touch shape fusion module to refine the reconstructed shape. For shape exploration, we combine the diffusion model with reinforcement learning to train a policy. This involves using the generated latent vector from the diffusion model to guide the touch exploration policy training through a novel reward design. Experiments validate the reconstruction quality thorough both qualitatively and quantitative analysis, and our touch exploration policy further boosts reconstruction performance.</li>
</ul>

<h3>Title: Industry-focused Synthetic Segmentation Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Shinichi Mae, Ryosuke Yamada, Hirokatsu Kataoka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13099">https://arxiv.org/abs/2505.13099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13099">https://arxiv.org/pdf/2505.13099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13099]] Industry-focused Synthetic Segmentation Pre-training(https://arxiv.org/abs/2505.13099)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Pre-training on real-image datasets has been widely proven effective for improving instance segmentation. However, industrial applications face two key challenges: (1) legal and ethical restrictions, such as ImageNet's prohibition of commercial use, and (2) limited transferability due to the domain gap between web images and industrial imagery. Even recent vision foundation models, including the segment anything model (SAM), show notable performance degradation in industrial settings. These challenges raise critical questions: Can we build a vision foundation model for industrial applications without relying on real images or manual annotations? And can such models outperform even fine-tuned SAM on industrial datasets? To address these questions, we propose the Instance Core Segmentation Dataset (InsCore), a synthetic pre-training dataset based on formula-driven supervised learning (FDSL). InsCore generates fully annotated instance segmentation images that reflect key characteristics of industrial data, including complex occlusions, dense hierarchical masks, and diverse non-rigid shapes, distinct from typical web imagery. Unlike previous methods, InsCore requires neither real images nor human annotations. Experiments on five industrial datasets show that models pre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as well as fine-tuned SAM, achieving an average improvement of 6.2 points in instance segmentation performance. This result is achieved using only 100k synthetic images, more than 100 times fewer than the 11 million images in SAM's SA-1B dataset, demonstrating the data efficiency of our approach. These findings position InsCore as a practical and license-free vision foundation model for industrial applications.</li>
</ul>

<h3>Title: Time series saliency maps: explaining models across multiple domains</h3>
<ul>
<li><strong>Authors: </strong>Christodoulos Kechris, Jonathan Dan, David Atienza</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13100">https://arxiv.org/abs/2505.13100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13100">https://arxiv.org/pdf/2505.13100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13100]] Time series saliency maps: explaining models across multiple domains(https://arxiv.org/abs/2505.13100)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, explainability</a></li>
<li><strong>Abstract: </strong>Traditional saliency map methods, popularized in computer vision, highlight individual points (pixels) of the input that contribute the most to the model's output. However, in time-series they offer limited insights as semantically meaningful features are often found in other domains. We introduce Cross-domain Integrated Gradients, a generalization of Integrated Gradients. Our method enables feature attributions on any domain that can be formulated as an invertible, differentiable transformation of the time domain. Crucially, our derivation extends the original Integrated Gradients into the complex domain, enabling frequency-based attributions. We provide the necessary theoretical guarantees, namely, path independence and completeness. Our approach reveals interpretable, problem-specific attributions that time-domain methods cannot capture, on three real-world tasks: wearable sensor heart rate extraction, electroencephalography-based seizure detection, and zero-shot time-series forecasting. We release an open-source Tensorflow/PyTorch library to enable plug-and-play cross-domain explainability for time-series models. These results demonstrate the ability of cross-domain integrated gradients to provide semantically meaningful insights in time-series models that are impossible with traditional time-domain saliency.</li>
</ul>

<h3>Title: ARIW-Framework: Adaptive Robust Iterative Watermarking Framework</h3>
<ul>
<li><strong>Authors: </strong>Shaowu Wu, Liting Zeng, Wei Lu, Xiangyang Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13101">https://arxiv.org/abs/2505.13101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13101">https://arxiv.org/pdf/2505.13101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13101]] ARIW-Framework: Adaptive Robust Iterative Watermarking Framework(https://arxiv.org/abs/2505.13101)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>With the rapid rise of large models, copyright protection for generated image content has become a critical security challenge. Although deep learning watermarking techniques offer an effective solution for digital image copyright protection, they still face limitations in terms of visual quality, robustness and generalization. To address these issues, this paper proposes an adaptive robust iterative watermarking framework (ARIW-Framework) that achieves high-quality watermarked images while maintaining exceptional robustness and generalization performance. Specifically, we introduce an iterative approach to optimize the encoder for generating robust residuals. The encoder incorporates noise layers and a decoder to compute robustness weights for residuals under various noise attacks. By employing a parallel optimization strategy, the framework enhances robustness against multiple types of noise attacks. Furthermore, we leverage image gradients to determine the embedding strength at each pixel location, significantly improving the visual quality of the watermarked images. Extensive experiments demonstrate that the proposed method achieves superior visual quality while exhibiting remarkable robustness and generalization against noise attacks.</li>
</ul>

<h3>Title: Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast</h3>
<ul>
<li><strong>Authors: </strong>Ji Qi, Tam Thuc Do, Mingxiao Liu, Zhuoshi Pan, Yuzhe Li, Gene Cheung, H. Vicky Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13102">https://arxiv.org/abs/2505.13102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13102">https://arxiv.org/pdf/2505.13102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13102]] Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast(https://arxiv.org/abs/2505.13102)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>To forecast traffic with both spatial and temporal dimensions, we unroll a mixed-graph-based optimization algorithm into a lightweight and interpretable transformer-like neural net. Specifically, we construct two graphs: an undirected graph $\mathcal{G}^u$ capturing spatial correlations across geography, and a directed graph $\mathcal{G}^d$ capturing sequential relationships over time. We formulate a prediction problem for the future samples of signal $\mathbf{x}$, assuming it is "smooth" with respect to both $\mathcal{G}^u$ and $\mathcal{G}^d$, where we design new $\ell_2$ and $\ell_1$-norm variational terms to quantify and promote signal smoothness (low-frequency reconstruction) on a directed graph. We construct an iterative algorithm based on alternating direction method of multipliers (ADMM), and unroll it into a feed-forward network for data-driven parameter learning. We insert graph learning modules for $\mathcal{G}^u$ and $\mathcal{G}^d$, which are akin to the self-attention mechanism in classical transformers. Experiments show that our unrolled networks achieve competitive traffic forecast performance as state-of-the-art prediction schemes, while reducing parameter counts drastically. Our code is available in this https URL.</li>
</ul>

<h3>Title: FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Guangda Liu, Chengwei Li, Zhenyu Ning, Jing Lin, Yiwu Yao, Danning Ke, Minyi Guo, Jieru Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13109">https://arxiv.org/abs/2505.13109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13109">https://arxiv.org/pdf/2505.13109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13109]] FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference(https://arxiv.org/abs/2505.13109)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.</li>
</ul>

<h3>Title: Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation</h3>
<ul>
<li><strong>Authors: </strong>Sungmin Cha, Kyunghyun Cho</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13111">https://arxiv.org/abs/2505.13111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13111">https://arxiv.org/pdf/2505.13111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13111]] Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation(https://arxiv.org/abs/2505.13111)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) is a core component in the training and deployment of modern generative models, particularly large language models (LLMs). While its empirical benefits are well documented--enabling smaller student models to emulate the performance of much larger teachers--the underlying mechanisms by which KD improves generative quality remain poorly understood. In this work, we present a minimal working explanation of KD in generative modeling. Using a controlled simulation with mixtures of Gaussians, we demonstrate that distillation induces a trade-off between precision and recall in the student model. As the teacher distribution becomes more selective, the student concentrates more probability mass on high-likelihood regions at the expense of coverage--a behavior modulated by a single entropy-controlling parameter. We then validate this effect in a large-scale language modeling setup using the SmolLM2 family of models. Empirical results reveal the same precision-recall dynamics observed in simulation, where precision corresponds to sample quality and recall to distributional coverage. This precision-recall trade-off proves especially beneficial in scenarios where sample quality outweighs diversity, such as instruction tuning or downstream generation. Our analysis provides a simple and general explanation for the effectiveness of KD in generative modeling.</li>
</ul>

<h3>Title: Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13115">https://arxiv.org/abs/2505.13115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13115">https://arxiv.org/pdf/2505.13115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13115]] Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning(https://arxiv.org/abs/2505.13115)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA). We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.</li>
</ul>

<h3>Title: Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data</h3>
<ul>
<li><strong>Authors: </strong>Kathrin Lammers, Valerie Vaquet, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13116">https://arxiv.org/abs/2505.13116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13116">https://arxiv.org/pdf/2505.13116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13116]] Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data(https://arxiv.org/abs/2505.13116)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>As machine learning is increasingly applied in an online fashion to deal with evolving data streams, the fairness of these algorithms is a matter of growing ethical and legal concern. In many use cases, class imbalance in the data also needs to be dealt with to ensure predictive performance. Current fairness-aware stream learners typically attempt to solve these issues through in- or post-processing by focusing on optimizing one specific discrimination metric, addressing class imbalance in a separate processing step. While C-SMOTE is a highly effective model-agnostic pre-processing approach to mitigate class imbalance, as a side effect of this method, algorithmic bias is often introduced. Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant - as a pre-processing approach to simultaneously address the class imbalance and fairness concerns by employing situation testing and balancing fairness-relevant groups during oversampling. Unlike other fairness-aware stream learners, CFSMOTE is not optimizing for only one specific fairness metric, therefore avoiding potentially problematic trade-offs. Our experiments show significant improvement on several common group fairness metrics in comparison to vanilla C-SMOTE while maintaining competitive performance, also in comparison to other fairness-aware algorithms.</li>
</ul>

<h3>Title: Just Dance with $$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Snehashis Majhi, Giacomo D'Amicantonio, Antitza Dantcheva, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Egor Bondarev, Francois Bremond</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13123">https://arxiv.org/abs/2505.13123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13123">https://arxiv.org/pdf/2505.13123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13123]] Just Dance with $$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection(https://arxiv.org/abs/2505.13123)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Weakly-supervised methods for video anomaly detection (VAD) are conventionally based merely on RGB spatio-temporal features, which continues to limit their reliability in real-world scenarios. This is due to the fact that RGB-features are not sufficiently distinctive in setting apart categories such as shoplifting from visually similar events. Therefore, towards robust complex real-world VAD, it is essential to augment RGB spatio-temporal features by additional modalities. Motivated by this, we introduce the Poly-modal Induced framework for VAD: "PI-VAD", a novel approach that augments RGB representations by five additional modalities. Specifically, the modalities include sensitivity to fine-grained motion (Pose), three dimensional scene and entity representation (Depth), surrounding objects (Panoptic masks), global motion (optical flow), as well as language cues (VLM). Each modality represents an axis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two plug-in modules, namely Pseudo-modality Generation module and Cross Modal Induction module, which generate modality-specific prototypical representation and, thereby, induce multi-modal information into RGB cues. These modules operate by performing anomaly-aware auxiliary tasks and necessitate five modality backbones -- only during training. Notably, PI-VAD achieves state-of-the-art accuracy on three prominent VAD datasets encompassing real-world scenarios, without requiring the computational overhead of five modality backbones at inference.</li>
</ul>

<h3>Title: $$PC: Scaling Predictive Coding to 100+ Layer Networks</h3>
<ul>
<li><strong>Authors: </strong>Francesco Innocenti, El Mehdi Achour, Christopher L. Buckley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13124">https://arxiv.org/abs/2505.13124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13124">https://arxiv.org/pdf/2505.13124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13124]] $$PC: Scaling Predictive Coding to 100+ Layer Networks(https://arxiv.org/abs/2505.13124)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The biological implausibility of backpropagation (BP) has motivated many alternative, brain-inspired algorithms that attempt to rely only on local information, such as predictive coding (PC) and equilibrium propagation. However, these algorithms have notoriously struggled to train very deep networks, preventing them from competing with BP in large-scale settings. Indeed, scaling PC networks (PCNs) has recently been posed as a challenge for the community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can be trained reliably using a Depth-$\mu$P parameterisation (Yang et al., 2023; Bordelon et al., 2023) which we call "$\mu$PC". Through an extensive analysis of the scaling behaviour of PCNs, we reveal several pathologies that make standard PCNs difficult to train at large depths. We then show that, despite addressing only some of these instabilities, $\mu$PC allows stable training of very deep (up to 128-layer) residual networks on simple classification tasks with competitive performance and little tuning compared to current benchmarks. Moreover, $\mu$PC enables zero-shot transfer of both weight and activity learning rates across widths and depths. Our results have implications for other local algorithms and could be extended to convolutional and transformer architectures. Code for $\mu$PC is made available as part of a JAX library for PCNs at this https URL (Innocenti et al., 2024).</li>
</ul>

<h3>Title: Adaptive Image Restoration for Video Surveillance: A Real-Time Approach</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Awais Amin, Adama Ilboudo, Abdul Samad bin Shahid, Amjad Ali, Waqas Haider Khan Bangyal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13130">https://arxiv.org/abs/2505.13130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13130">https://arxiv.org/pdf/2505.13130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13130]] Adaptive Image Restoration for Video Surveillance: A Real-Time Approach(https://arxiv.org/abs/2505.13130)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>One of the major challenges in the field of computer vision especially for detection, segmentation, recognition, monitoring, and automated solutions, is the quality of images. Image degradation, often caused by factors such as rain, fog, lighting, etc., has a negative impact on automated this http URL, several image restoration solutions exist, including restoration models for single degradation and restoration models for multiple degradations. However, these solutions are not suitable for real-time processing. In this study, the aim was to develop a real-time image restoration solution for video surveillance. To achieve this, using transfer learning with ResNet_50, we developed a model for automatically identifying the types of degradation present in an image to reference the necessary treatment(s) for image restoration. Our solution has the advantage of being flexible and scalable.</li>
</ul>

<h3>Title: Learning to Adapt to Position Bias in Vision Transformer Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Robert-Jan Bruintjes, Jan van Gemert</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13137">https://arxiv.org/abs/2505.13137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13137">https://arxiv.org/pdf/2505.13137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13137]] Learning to Adapt to Position Bias in Vision Transformer Classifiers(https://arxiv.org/abs/2505.13137)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>How discriminative position information is for image classification depends on the data. On the one hand, the camera position is arbitrary and objects can appear anywhere in the image, arguing for translation invariance. At the same time, position information is key for exploiting capture/center bias, and scene layout, e.g.: the sky is up. We show that position bias, the level to which a dataset is more easily solved when positional information on input features is used, plays a crucial role in the performance of Vision Transformers image classifiers. To investigate, we propose Position-SHAP, a direct measure of position bias by extending SHAP to work with position embeddings. We show various levels of position bias in different datasets, and find that the optimal choice of position embedding depends on the position bias apparent in the dataset. We therefore propose Auto-PE, a single-parameter position embedding extension, which allows the position embedding to modulate its norm, enabling the unlearning of position information. Auto-PE combines with existing PEs to match or improve accuracy on classification datasets.</li>
</ul>

<h3>Title: Neurosymbolic Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Emile van Krieken, Pasquale Minervini, Edoardo Ponti, Antonio Vergari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13138">https://arxiv.org/abs/2505.13138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13138">https://arxiv.org/pdf/2505.13138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13138]] Neurosymbolic Diffusion Models(https://arxiv.org/abs/2505.13138)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Neurosymbolic (NeSy) predictors combine neural perception with symbolic reasoning to solve tasks like visual reasoning. However, standard NeSy predictors assume conditional independence between the symbols they extract, thus limiting their ability to model interactions and uncertainty - often leading to overconfident predictions and poor out-of-distribution generalisation. To overcome the limitations of the independence assumption, we introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. Our approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. Across both synthetic and real-world benchmarks - including high-dimensional visual path planning and rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among NeSy predictors and demonstrate strong calibration.</li>
</ul>

<h3>Title: CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow</h3>
<ul>
<li><strong>Authors: </strong>Takahiro Maeda, Jinkun Cao, Norimichi Ukita, Kris Kitani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13140">https://arxiv.org/abs/2505.13140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13140">https://arxiv.org/pdf/2505.13140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13140]] CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow(https://arxiv.org/abs/2505.13140)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Many density estimation techniques for 3D human motion prediction require a significant amount of inference time, often exceeding the duration of the predicted time horizon. To address the need for faster density estimation for 3D human motion prediction, we introduce a novel flow-based method for human motion prediction called CacheFlow. Unlike previous conditional generative models that suffer from time efficiency, CacheFlow takes advantage of an unconditional flow-based generative model that transforms a Gaussian mixture into the density of future motions. The results of the computation of the flow-based generative model can be precomputed and cached. Then, for conditional prediction, we seek a mapping from historical trajectories to samples in the Gaussian mixture. This mapping can be done by a much more lightweight model, thus saving significant computation overhead compared to a typical conditional flow model. In such a two-stage fashion and by caching results from the slow flow model computation, we build our CacheFlow without loss of prediction accuracy and model expressiveness. This inference process is completed in approximately one millisecond, making it 4 times faster than previous VAE methods and 30 times faster than previous diffusion-based methods on standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our method demonstrates improved density estimation accuracy and comparable prediction accuracy to a SOTA method on Human3.6M. Our code and models will be publicly available.</li>
</ul>

<h3>Title: Understanding Cross-Lingual Inconsistency in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Wei Lim, Alham Fikri Aji, Trevor Cohn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13141">https://arxiv.org/abs/2505.13141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13141">https://arxiv.org/pdf/2505.13141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13141]] Understanding Cross-Lingual Inconsistency in Large Language Models(https://arxiv.org/abs/2505.13141)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are demonstrably capable of cross-lingual transfer, but can produce inconsistent output when prompted with the same queries written in different languages. To understand how language models are able to generalize knowledge from one language to the others, we apply the logit lens to interpret the implicit steps taken by LLMs to solve multilingual multi-choice reasoning questions. We find LLMs predict inconsistently and are less accurate because they rely on subspaces of individual languages, rather than working in a shared semantic space. While larger models are more multilingual, we show their hidden states are more likely to dissociate from the shared representation compared to smaller models, but are nevertheless more capable of retrieving knowledge embedded across different languages. Finally, we demonstrate that knowledge sharing can be modulated by steering the models' latent processing towards the shared semantic space. We find reinforcing utilization of the shared space improves the models' multilingual reasoning performance, as a result of more knowledge transfer from, and better output consistency with English.</li>
</ul>

<h3>Title: Parallel Layer Normalization for Universal Approximation</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Ni, Yuhe Liu, Wenxin Sun, Yitong Tang, Yuxin Guo, Peilin Feng, Wenjun Wu, Lei Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13142">https://arxiv.org/abs/2505.13142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13142">https://arxiv.org/pdf/2505.13142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13142]] Parallel Layer Normalization for Universal Approximation(https://arxiv.org/abs/2505.13142)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Universal approximation theorem (UAT) is a fundamental theory for deep neural networks (DNNs), demonstrating their powerful representation capacity to represent and approximate any function. The analyses and proofs of UAT are based on traditional network with only linear and nonlinear activation functions, but omitting normalization layers, which are commonly employed to enhance the training of modern networks. This paper conducts research on UAT of DNNs with normalization layers for the first time. We theoretically prove that an infinitely wide network -- composed solely of parallel layer normalization (PLN) and linear layers -- has universal approximation capacity. Additionally, we investigate the minimum number of neurons required to approximate $L$-Lipchitz continuous functions, with a single hidden-layer network. We compare the approximation capacity of PLN with traditional activation functions in theory. Different from the traditional activation functions, we identify that PLN can act as both activation function and normalization in deep neural networks at the same time. We also find that PLN can improve the performance when replacing LN in transformer architectures, which reveals the potential of PLN used in neural architectures.</li>
</ul>

<h3>Title: Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongsu Lee, Minhae Kwon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13144">https://arxiv.org/abs/2505.13144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13144">https://arxiv.org/pdf/2505.13144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13144]] Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning(https://arxiv.org/abs/2505.13144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The goal of offline reinforcement learning (RL) is to extract a high-performance policy from the fixed datasets, minimizing performance degradation due to out-of-distribution (OOD) samples. Offline model-based RL (MBRL) is a promising approach that ameliorates OOD issues by enriching state-action transitions with augmentations synthesized via a learned dynamics model. Unfortunately, seminal offline MBRL methods often struggle in sparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL framework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA), that generates augmented transitions in a temporally structured latent space rather than in raw state space. To model long-horizon behavior, TempDATA learns a latent abstraction that captures a temporal distance from both trajectory and transition levels of state space. Our experiments confirm that TempDATA outperforms previous offline MBRL methods and achieves matching or surpassing the performance of diffusion-based trajectory augmentation and goal-conditioned RL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.</li>
</ul>

<h3>Title: What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text</h3>
<ul>
<li><strong>Authors: </strong>Aswathy Velutharambath, Roman Klinger, Kai Sassenberg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13147">https://arxiv.org/abs/2505.13147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13147">https://arxiv.org/pdf/2505.13147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13147]] What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text(https://arxiv.org/abs/2505.13147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Can deception be detected solely from written text? Cues of deceptive communication are inherently subtle, even more so in text-only communication. Yet, prior studies have reported considerable success in automatic deception detection. We hypothesize that such findings are largely driven by artifacts introduced during data collection and do not generalize beyond specific datasets. We revisit this assumption by introducing a belief-based deception framework, which defines deception as a misalignment between an author's claims and true beliefs, irrespective of factual accuracy, allowing deception cues to be studied in isolation. Based on this framework, we construct three corpora, collectively referred to as DeFaBel, including a German-language corpus of deceptive and non-deceptive arguments and a multilingual version in German and English, each collected under varying conditions to account for belief change and enable cross-linguistic analysis. Using these corpora, we evaluate commonly reported linguistic cues of deception. Across all three DeFaBel variants, these cues show negligible, statistically insignificant correlations with deception labels, contrary to prior work that treats such cues as reliable indicators. We further benchmark against other English deception datasets following similar data collection protocols. While some show statistically significant correlations, effect sizes remain low and, critically, the set of predictive cues is inconsistent across datasets. We also evaluate deception detection using feature-based models, pretrained language models, and instruction-tuned large language models. While some models perform well on established deception datasets, they consistently perform near chance on DeFaBel. Our findings challenge the assumption that deception can be reliably inferred from linguistic cues and call for rethinking how deception is studied and modeled in NLP.</li>
</ul>

<h3>Title: Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Maksim Bobrin, Ilya Zisman, Alexander Nikulin, Vladislav Kurenkov, Dmitry Dylov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13150">https://arxiv.org/abs/2505.13150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13150">https://arxiv.org/pdf/2505.13150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13150]] Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics(https://arxiv.org/abs/2505.13150)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Behavioral Foundation Models (BFMs) proved successful in producing policies for arbitrary tasks in a zero-shot manner, requiring no test-time training or task-specific fine-tuning. Among the most promising BFMs are the ones that estimate the successor measure learned in an unsupervised way from task-agnostic offline data. However, these methods fail to react to changes in the dynamics, making them inefficient under partial observability or when the transition function changes. This hinders the applicability of BFMs in a real-world setting, e.g., in robotics, where the dynamics can unexpectedly change at test time. In this work, we demonstrate that Forward-Backward (FB) representation, one of the methods from the BFM family, cannot distinguish between distinct dynamics, leading to an interference among the latent directions, which parametrize different policies. To address this, we propose a FB model with a transformer-based belief estimator, which greatly facilitates zero-shot adaptation. We also show that partitioning the policy encoding space into dynamics-specific clusters, aligned with the context-embedding directions, yields additional gain in performance. These traits allow our method to respond to the dynamics observed during training and to generalize to unseen ones. Empirically, in the changing dynamics setting, our approach achieves up to a 2x higher zero-shot returns compared to the baselines for both discrete and continuous tasks.</li>
</ul>

<h3>Title: Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice</h3>
<ul>
<li><strong>Authors: </strong>Zhi Liu, Tao Yang, Jing Wang, Yexin Chen, Zhan Gao, Jiaxi Yang, Kui Chen, Bingji Lu, Xiaochen Li, Changyong Luo, Yan Li, Xiaohong Gu, Peng Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13156">https://arxiv.org/abs/2505.13156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13156">https://arxiv.org/pdf/2505.13156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13156]] Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice(https://arxiv.org/abs/2505.13156)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural medicines, particularly Traditional Chinese Medicine (TCM), are gaining global recognition for their therapeutic potential in addressing human symptoms and diseases. TCM, with its systematic theories and extensive practical experience, provides abundant resources for healthcare. However, the effective application of TCM requires precise syndrome diagnosis, determination of treatment principles, and prescription formulation, which demand decades of clinical expertise. Despite advancements in TCM-based decision systems, machine learning, and deep learning research, limitations in data and single-objective constraints hinder their practical application. In recent years, large language models (LLMs) have demonstrated potential in complex tasks, but lack specialization in TCM and face significant challenges, such as too big model scale to deploy and issues with hallucination. To address these challenges, we introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and specifically designed for TCM, pre-trained and fine-tuned on diverse TCM corpora, including classical texts, expert treatises, clinical records, and knowledge graphs. Tianyi is designed to assimilate interconnected and systematic TCM knowledge through a progressive learning manner. Additionally, we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in TCM examinations, clinical tasks, domain-specific question-answering, and real-world trials. The extensive evaluations demonstrate the significant potential of Tianyi as an AI assistant in TCM clinical practice and research, bridging the gap between TCM knowledge and practical application.</li>
</ul>

<h3>Title: Role-Playing Evaluation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yassine El Boudouri, Walter Nuninger, Julian Alvarez, Yvan Peter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13157">https://arxiv.org/abs/2505.13157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13157">https://arxiv.org/pdf/2505.13157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13157]] Role-Playing Evaluation for Large Language Models(https://arxiv.org/abs/2505.13157)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at this https URL</li>
</ul>

<h3>Title: Network-wide Quantum Key Distribution with Onion Routing Relay (Conference Version)</h3>
<ul>
<li><strong>Authors: </strong>Pedro Otero-Garca, David Prez-Castro, Manuel Fernndez-Veiga, Ana Fernndez-Vilas</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13158">https://arxiv.org/abs/2505.13158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13158">https://arxiv.org/pdf/2505.13158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13158]] Network-wide Quantum Key Distribution with Onion Routing Relay (Conference Version)(https://arxiv.org/abs/2505.13158)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The advancement of quantum computing threatens classical cryptographic methods, necessitating the development of secure quantum key distribution (QKD) solutions for QKD Networks (QKDN). In this paper, a novel key distribution protocol, Onion Routing Relay (ORR), that integrates onion routing (OR) with post-quantum cryptography (PQC) in a key-relay (KR) model is evaluated for QKDNs. This approach increases the security by enhancing confidentiality, integrity, authenticity (CIA principles), and anonymity in quantum-secure communications. By employing PQC-based encapsulation, ORR aims to avoid the security risks posed by intermediate malicious nodes and ensures end-to-end security. Our results show a competitive performance of the basic ORR model, against current KR and trusted-node (TN) approaches, demonstrating its feasibility and applicability in high-security environments maintaining a consistent Quality of Service (QoS). The results also show that while basic ORR incurs higher encryption overhead, it provides substantial security improvements without significantly impacting the overall key distribution time. Nevertheless, the introduction of an end-to-end authentication extension (ORR-Ext) has a significant impact on the Quality of Service (QoS), thereby limiting its suitability to applications with stringent security requirements.</li>
</ul>

<h3>Title: RIFLES: Resource-effIcient Federated LEarning via Scheduling</h3>
<ul>
<li><strong>Authors: </strong>Sara Alosaime (University of Warwick), Arshad Jhumka (University of Leeds)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13169">https://arxiv.org/abs/2505.13169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13169">https://arxiv.org/pdf/2505.13169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13169]] RIFLES: Resource-effIcient Federated LEarning via Scheduling(https://arxiv.org/abs/2505.13169)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a privacy-preserving machine learning technique that allows decentralized collaborative model training across a set of distributed clients, by avoiding raw data exchange. A fundamental component of FL is the selection of a subset of clients in each round for model training by a central server. Current selection strategies are myopic in nature in that they are based on past or current interactions, often leading to inefficiency issues such as straggling clients. In this paper, we address this serious shortcoming by proposing the RIFLES approach that builds a novel availability forecasting layer to support the client selection process. We make the following contributions: (i) we formalise the sequential selection problem and reduce it to a scheduling problem and show that the problem is NP-complete, (ii) leveraging heartbeat messages from clients, RIFLES build an availability prediction layer to support (long term) selection decisions, (iii) we propose a novel adaptive selection strategy to support efficient learning and resource usage. To circumvent the inherent exponential complexity, we present RIFLES, a heuristic that leverages clients' historical availability data by using a CNN-LSTM time series forecasting model, allowing the server to predict the optimal participation times of clients, thereby enabling informed selection decisions. By comparing against other FL techniques, we show that RIFLES provide significant improvement by between 10%-50% on a variety of metrics such as accuracy and test loss. To the best of our knowledge, it is the first work to investigate FL as a scheduling problem.</li>
</ul>

<h3>Title: Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Xu, Antoine Bosselut, Imanol Schlag</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13171">https://arxiv.org/abs/2505.13171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13171">https://arxiv.org/pdf/2505.13171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13171]] Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks(https://arxiv.org/abs/2505.13171)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are known to memorize parts of their training data, posing risk of copyright violations. To systematically examine this risk, we pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing web-scale data with public domain books used to simulate copyrighted content at controlled frequencies at lengths at least ten times longer than prior work. We thereby identified the offset effect, a phenomenon characterized by two key findings: (1) verbatim memorization is most strongly triggered by short prefixes drawn from the beginning of the context window, with memorization decreasing counterintuitively as prefix length increases; and (2) a sharp decline in verbatim recall when prefix begins offset from the initial tokens of the context window. We attribute this to positional fragility: models rely disproportionately on the earliest tokens in their context window as retrieval anchors, making them sensitive to even slight shifts. We further observe that when the model fails to retrieve memorized content, it often produces degenerated text. Leveraging these findings, we show that shifting sensitive data deeper into the context window suppresses both extractable memorization and degeneration. Our results suggest that positional offset is a critical and previously overlooked axis for evaluating memorization risks, since prior work implicitly assumed uniformity by probing only from the beginning of training sequences.</li>
</ul>

<h3>Title: A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs</h3>
<ul>
<li><strong>Authors: </strong>V.S.D.S.Mahesh Akavarapu, Hrishikesh Terdalkar, Pramit Bhattacharyya, Shubhangi Agarwal, Vishakha Deulgaonkar, Pralay Manna, Chaitali Dangarikar, Arnab Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13173">https://arxiv.org/abs/2505.13173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13173">https://arxiv.org/pdf/2505.13173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13173]] A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs(https://arxiv.org/abs/2505.13173)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages -- Sanskrit, Ancient Greek and Latin -- to investigate the factors affecting cross-lingual zero-shot generalization. First, we explore named entity recognition and machine translation into English. While LLMs perform equal to or better than fine-tuned baselines on out-of-domain data, smaller models often struggle, especially with niche or abstract entity types. In addition, we concentrate on Sanskrit by presenting a factoid question-answering (QA) dataset and show that incorporating context via retrieval-augmented generation approach significantly boosts performance. In contrast, we observe pronounced performance drops for smaller LLMs across these QA tasks. These results suggest model scale as an important factor influencing cross-lingual generalization. Assuming that models used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical languages, our findings provide insights into how LLMs may generalize on these languages and their consequent utility in classical studies.</li>
</ul>

<h3>Title: FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching</h3>
<ul>
<li><strong>Authors: </strong>Alp Eren Sari, Paolo Favaro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13174">https://arxiv.org/abs/2505.13174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13174">https://arxiv.org/pdf/2505.13174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13174]] FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching(https://arxiv.org/abs/2505.13174)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We propose FlowCut, a simple and capable method for unsupervised video instance segmentation consisting of a three-stage framework to construct a high-quality video dataset with pseudo labels. To our knowledge, our work is the first attempt to curate a video dataset with pseudo-labels for unsupervised video instance segmentation. In the first stage, we generate pseudo-instance masks by exploiting the affinities of features from both images and optical flows. In the second stage, we construct short video segments containing high-quality, consistent pseudo-instance masks by temporally matching them across the frames. In the third stage, we use the YouTubeVIS-2021 video dataset to extract our training instance segmentation set, and then train a video segmentation model. FlowCut achieves state-of-the-art performance on the YouTubeVIS-2019, YouTubeVIS-2021, DAVIS-2017, and DAVIS-2017 Motion benchmarks.</li>
</ul>

<h3>Title: ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihao Cheng, Hongru Wang, Zeming Liu, Yuhang Guo, Yuanfang Guo, Yunhong Wang, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13176">https://arxiv.org/abs/2505.13176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13176">https://arxiv.org/pdf/2505.13176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13176]] ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models(https://arxiv.org/abs/2505.13176)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions, overlooking the context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs' capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool utilization. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool utilization significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code are available at this https URL.</li>
</ul>

<h3>Title: Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Pan, Yonekura Shogo, Yasuo Kuniyoshi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13191">https://arxiv.org/abs/2505.13191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13191">https://arxiv.org/pdf/2505.13191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13191]] Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision(https://arxiv.org/abs/2505.13191)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Inspired by foveal vision, hard attention models promise interpretability and parameter economy. However, existing models like the Recurrent Model of Visual Attention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the hierarchy of human vision system, that compromise on the visual exploration dynamics. As a result, they tend to produce attention that are either overly fixational or excessively saccadic, diverging from human eye movement behavior. In this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a novel hard attention framework that explicitly models the neural hierarchy of human visual processing. By decoupling the function of glimpse location generation and task execution in two recurrent layers, MRAM emergent a balanced behavior between fixation and saccadic movement. Our results show that MRAM not only achieves more human-like attention dynamics, but also consistently outperforms CNN, RAM and DRAM baselines on standard image classification benchmarks.</li>
</ul>

<h3>Title: True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics</h3>
<ul>
<li><strong>Authors: </strong>Christoph Jrgen Hemmer, Daniel Durstewitz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DS, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13192">https://arxiv.org/abs/2505.13192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13192">https://arxiv.org/pdf/2505.13192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13192]] True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics(https://arxiv.org/abs/2505.13192)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Complex, temporally evolving phenomena, from climate to brain activity, are governed by dynamical systems (DS). DS reconstruction (DSR) seeks to infer generative surrogate models of these from observed data, reproducing their long-term behavior. Existing DSR approaches require purpose-training for any new system observed, lacking the zero-shot and in-context inference capabilities known from LLMs. Here we introduce DynaMix, a novel multivariate ALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR model able to generalize zero-shot to out-of-domain DS. Just from a provided context signal, without any re-training, DynaMix faithfully forecasts the long-term evolution of novel DS where existing time series (TS) foundation models, like Chronos, fail -- at a fraction of the number of parameters and orders of magnitude faster inference times. DynaMix outperforms TS foundation models in terms of long-term statistics, and often also short-term forecasts, even on real-world time series, like traffic or weather data, typically used for training and evaluating TS models, but not at all part of DynaMix' training corpus. We illustrate some of the failure modes of TS models for DSR problems, and conclude that models built on DS principles may bear a huge potential also for advancing the TS prediction field.</li>
</ul>

<h3>Title: A Physics-Inspired Optimizer: Velocity Regularized Adam</h3>
<ul>
<li><strong>Authors: </strong>Pranav Vaidhyanathan, Lucas Schorling, Natalia Ares, Michael A. Osborne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13196">https://arxiv.org/abs/2505.13196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13196">https://arxiv.org/pdf/2505.13196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13196]] A Physics-Inspired Optimizer: Velocity Regularized Adam(https://arxiv.org/abs/2505.13196)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer for training deep neural networks that draws on ideas from quartic terms for kinetic energy with its stabilizing effects on various system dynamics. Previous algorithms, including the ubiquitous Adam, operate at the so called adaptive edge of stability regime during training leading to rapid oscillations and slowed convergence of loss. However, VRAdam adds a higher order penalty on the learning rate based on the velocity such that the algorithm automatically slows down whenever weight updates become large. In practice, we observe that the effective dynamic learning rate shrinks in high-velocity regimes, damping oscillations and allowing for a more aggressive base step size when necessary without divergence. By combining this velocity-based regularizer for global damping with per-parameter scaling of Adam to create a hybrid optimizer, we demonstrate that VRAdam consistently exceeds the performance against standard optimizers including AdamW. We benchmark various tasks such as image classification, language modeling, image generation and generative modeling using diverse architectures and training methodologies including Convolutional Neural Networks (CNNs), Transformers, and GFlowNets.</li>
</ul>

<h3>Title: Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification</h3>
<ul>
<li><strong>Authors: </strong>Jikai Wang, Zhenxu Tian, Juntao Li, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13204">https://arxiv.org/abs/2505.13204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13204">https://arxiv.org/pdf/2505.13204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13204]] Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification(https://arxiv.org/abs/2505.13204)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent works have revealed the great potential of speculative decoding in accelerating the autoregressive generation process of large language models. The success of these methods relies on the alignment between draft candidates and the sampled outputs of the target model. Existing methods mainly achieve draft-target alignment with training-based methods, e.g., EAGLE, Medusa, involving considerable training costs. In this paper, we present a training-free alignment-augmented speculative decoding algorithm. We propose alignment sampling, which leverages output distribution obtained in the prefilling phase to provide more aligned draft candidates. To further benefit from high-quality but non-aligned draft candidates, we also introduce a simple yet effective flexible verification strategy. Through an adaptive probability threshold, our approach can improve generation accuracy while further improving inference efficiency. Experiments on 8 datasets (including question answering, summarization and code completion tasks) show that our approach increases the average generation score by 3.3 points for the LLaMA3 model. Our method achieves a mean acceptance length up to 2.39 and speed up generation by 2.23.</li>
</ul>

<h3>Title: MAGI-1: Autoregressive Video Generation at Scale</h3>
<ul>
<li><strong>Authors: </strong>Sand.ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W.Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, Yuqiao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13211">https://arxiv.org/abs/2505.13211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13211">https://arxiv.org/pdf/2505.13211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13211]] MAGI-1: Autoregressive Video Generation at Scale(https://arxiv.org/abs/2505.13211)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present MAGI-1, a world model that generates videos by autoregressively predicting a sequence of video chunks, defined as fixed-length segments of consecutive frames. Trained to denoise per-chunk noise that increases monotonically over time, MAGI-1 enables causal temporal modeling and naturally supports streaming generation. It achieves strong performance on image-to-video (I2V) tasks conditioned on text instructions, providing high temporal consistency and scalability, which are made possible by several algorithmic innovations and a dedicated infrastructure stack. MAGI-1 facilitates controllable generation via chunk-wise prompting and supports real-time, memory-efficient deployment by maintaining constant peak inference cost, regardless of video length. The largest variant of MAGI-1 comprises 24 billion parameters and supports context lengths of up to 4 million tokens, demonstrating the scalability and robustness of our approach. The code and models are available at this https URL and this https URL. The product can be accessed at this https URL.</li>
</ul>

<h3>Title: Swin DiT: Diffusion Transformer using Pseudo Shifted Windows</h3>
<ul>
<li><strong>Authors: </strong>Jiafu Wu, Yabiao Wang, Jian Li, Jinlong Peng, Yun Cao, Chengjie Wang, Jiangning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13219">https://arxiv.org/abs/2505.13219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13219">https://arxiv.org/pdf/2505.13219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13219]] Swin DiT: Diffusion Transformer using Pseudo Shifted Windows(https://arxiv.org/abs/2505.13219)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) achieve remarkable performance within the domain of image generation through the incorporation of the transformer architecture. Conventionally, DiTs are constructed by stacking serial isotropic global information modeling transformers, which face significant computational cost when processing high-resolution images. We empirically analyze that latent space image generation does not exhibit a strong dependence on global information as traditionally assumed. Most of the layers in the model demonstrate redundancy in global computation. In addition, conventional attention mechanisms exhibit low-frequency inertia issues. To address these issues, we propose \textbf{P}seudo \textbf{S}hifted \textbf{W}indow \textbf{A}ttention (PSWA), which fundamentally mitigates global model redundancy. PSWA achieves intermediate global-local information interaction through window attention, while employing a high-frequency bridging branch to simulate shifted window operations, supplementing appropriate global and high-frequency information. Furthermore, we propose the Progressive Coverage Channel Allocation(PCCA) strategy that captures high-order attention similarity without additional computational cost. Building upon all of them, we propose a series of Pseudo \textbf{S}hifted \textbf{Win}dow DiTs (\textbf{Swin DiT}), accompanied by extensive experiments demonstrating their superior performance. For example, our proposed Swin-DiT-L achieves a 54%$\uparrow$ FID improvement over DiT-XL/2 while requiring less computational. this https URL</li>
</ul>

<h3>Title: SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science</h3>
<ul>
<li><strong>Authors: </strong>Jie Ying, Zihong Chen, Zhefan Wang, Wanli Jiang, Chenyang Wang, Zhonghang Yuan, Haoyang Su, Huanjun Kong, Fan Yang, Nanqing Dong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13220">https://arxiv.org/abs/2505.13220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13220">https://arxiv.org/pdf/2505.13220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13220]] SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science(https://arxiv.org/abs/2505.13220)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Seed science is essential for modern agriculture, directly influencing crop yields and global food security. However, challenges such as interdisciplinary complexity and high costs with limited returns hinder progress, leading to a shortage of experts and insufficient technological support. While large language models (LLMs) have shown promise across various fields, their application in seed science remains limited due to the scarcity of digital resources, complex gene-trait relationships, and the lack of standardized benchmarks. To address this gap, we introduce SeedBench -- the first multi-task benchmark specifically designed for seed science. Developed in collaboration with domain experts, SeedBench focuses on seed breeding and simulates key aspects of modern breeding processes. We conduct a comprehensive evaluation of 26 leading LLMs, encompassing proprietary, open-source, and domain-specific fine-tuned models. Our findings not only highlight the substantial gaps between the power of LLMs and the real-world seed science problems, but also make a foundational step for research on LLMs for seed design.</li>
</ul>

<h3>Title: Implicit bias produces neural scaling laws in learning curves, from perceptrons to deep networks</h3>
<ul>
<li><strong>Authors: </strong>Francesco D'Amico, Dario Bocchi, Matteo Negri</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13230">https://arxiv.org/abs/2505.13230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13230">https://arxiv.org/pdf/2505.13230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13230]] Implicit bias produces neural scaling laws in learning curves, from perceptrons to deep networks(https://arxiv.org/abs/2505.13230)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Scaling laws in deep learning - empirical power-law relationships linking model performance to resource growth - have emerged as simple yet striking regularities across architectures, datasets, and tasks. These laws are particularly impactful in guiding the design of state-of-the-art models, since they quantify the benefits of increasing data or model size, and hint at the foundations of interpretability in machine learning. However, most studies focus on asymptotic behavior at the end of training or on the optimal training time given the model size. In this work, we uncover a richer picture by analyzing the entire training dynamics through the lens of spectral complexity norms. We identify two novel dynamical scaling laws that govern how performance evolves during training. These laws together recover the well-known test error scaling at convergence, offering a mechanistic explanation of generalization emergence. Our findings are consistent across CNNs, ResNets, and Vision Transformers trained on MNIST, CIFAR-10 and CIFAR-100. Furthermore, we provide analytical support using a solvable model: a single-layer perceptron trained with binary cross-entropy. In this setting, we show that the growth of spectral complexity driven by the implicit bias mirrors the generalization behavior observed at fixed norm, allowing us to connect the performance dynamics to classical learning rules in the perceptron.</li>
</ul>

<h3>Title: From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection</h3>
<ul>
<li><strong>Authors: </strong>Lincan Cai, Jingxuan Kang, Shuang Li, Wenxuan Ma, Binhui Xie, Zhida Qin, Jian Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13233">https://arxiv.org/abs/2505.13233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13233">https://arxiv.org/pdf/2505.13233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13233]] From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection(https://arxiv.org/abs/2505.13233)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive zero-shot capabilities on downstream tasks. Prior research highlights the crucial role of visual augmentation techniques, like random cropping, in alignment with fine-grained class descriptions generated by large language models (LLMs), significantly enhancing zero-shot performance by incorporating multi-view information. However, the inherent randomness of these augmentations can inevitably introduce background artifacts and cause models to overly focus on local details, compromising global semantic understanding. To address these issues, we propose an \textbf{A}ttention-\textbf{B}ased \textbf{S}election (\textbf{ABS}) method from local details to global context, which applies attention-guided cropping in both raw images and feature space, supplement global semantic information through strategic feature selection. Additionally, we introduce a soft matching technique to effectively filter LLM descriptions for better alignment. \textbf{ABS} achieves state-of-the-art performance on out-of-distribution generalization and zero-shot classification tasks. Notably, \textbf{ABS} is training-free and even rivals few-shot and test-time adaptation methods. Our code is available at \href{this https URL}{\textcolor{darkgreen}{this https URL}}.</li>
</ul>

<h3>Title: WriteViT: Handwritten Text Generation with Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Dang Hoai Nam, Huynh Tong Dang Khoa, Vo Nguyen Le Duy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13235">https://arxiv.org/abs/2505.13235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13235">https://arxiv.org/pdf/2505.13235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13235]] WriteViT: Handwritten Text Generation with Vision Transformer(https://arxiv.org/abs/2505.13235)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Humans can quickly generalize handwriting styles from a single example by intuitively separating content from style. Machines, however, struggle with this task, especially in low-data settings, often missing subtle spatial and stylistic cues. Motivated by this gap, we introduce WriteViT, a one-shot handwritten text synthesis framework that incorporates Vision Transformers (ViT), a family of models that have shown strong performance across various computer vision tasks. WriteViT integrates a ViT-based Writer Identifier for extracting style embeddings, a multi-scale generator built with Transformer encoder-decoder blocks enhanced by conditional positional encoding (CPE), and a lightweight ViT-based recognizer. While previous methods typically rely on CNNs or CRNNs, our design leverages transformers in key components to better capture both fine-grained stroke details and higher-level style information. Although handwritten text synthesis has been widely explored, its application to Vietnamese -- a language rich in diacritics and complex typography -- remains limited. Experiments on Vietnamese and English datasets demonstrate that WriteViT produces high-quality, style-consistent handwriting while maintaining strong recognition performance in low-resource scenarios. These results highlight the promise of transformer-based designs for multilingual handwriting generation and efficient style adaptation.</li>
</ul>

<h3>Title: A Geometry-Grounded Data Perimeter in Azure</h3>
<ul>
<li><strong>Authors: </strong>Christophe Parisel</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13238">https://arxiv.org/abs/2505.13238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13238">https://arxiv.org/pdf/2505.13238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13238]] A Geometry-Grounded Data Perimeter in Azure(https://arxiv.org/abs/2505.13238)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>While data perimeter is ubiquitous in cybersecurity speak, it rarely defines how boundary points are arranged. In this paper we show how Azure s blast radius ultrametric provides the distance, and how solving the Traveling Salesman Problem in this ultrametric space provides the ordering, yielding a true geometric contour: an actionable perimeter measure for SPN prioritization.</li>
</ul>

<h3>Title: Network-wide Quantum Key Distribution with Onion Routing Relay</h3>
<ul>
<li><strong>Authors: </strong>Pedro Otero-Garca, David Prez-Castro, Manuel Fernndez-Veiga, Ana Fernndez-Vilas</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13239">https://arxiv.org/abs/2505.13239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13239">https://arxiv.org/pdf/2505.13239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13239]] Network-wide Quantum Key Distribution with Onion Routing Relay(https://arxiv.org/abs/2505.13239)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The advancement of quantum computing threatens classical cryptographic methods, necessitating the development of secure quantum key distribution (QKD) solutions for QKD Networks (QKDN). In this paper, a novel key distribution protocol, Onion Routing Relay (ORR), that integrates onion routing (OR) with post-quantum cryptography (PQC) in a key-relay (KR) model is evaluated for QKDNs. This approach increases the security by enhancing confidentiality, integrity, authenticity, and anonymity in quantum-secure communications. By employing PQC-based encapsulation, ORR pretends to avoid the security risks posed by intermediate malicious nodes and ensures end-to-end security. Results show that the performance of the ORR model, against current key-relay (KR) and trusted-node (TN) approaches, demonstrating its feasibility and applicability in high-security environments maintaining a consistent Quality of Service (QoS). The results show that while ORR incurs higher encryption overhead, it provides substantial security improvements without significantly impacting the overall key distribution time.</li>
</ul>

<h3>Title: JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jieying Xue, Phuong Minh Nguyen, Minh Le Nguyen, Xin Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13244">https://arxiv.org/abs/2505.13244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13244">https://arxiv.org/pdf/2505.13244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13244]] JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models(https://arxiv.org/abs/2505.13244)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of global digitalization, users from different countries increasingly rely on social media for information exchange. In this context, multilingual multi-label emotion detection has emerged as a critical research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task: (1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity. To tackle multilingual challenges, we leverage pre-trained multilingual models and focus on two architectures: (1) a fine-tuned BERT-based classification model and (2) an instruction-tuned generative LLM. Additionally, we propose two methods for handling multi-label classification: the base method, which maps an input directly to all its corresponding emotion labels, and the pairwise method, which models the relationship between the input text and each emotion category individually. Experimental results demonstrate the strong generalization ability of our approach in multilingual emotion recognition. In Track A, our method achieved Top 4 performance across 10 languages, ranking 1st in Hindi. In Track B, our approach also secured Top 5 performance in 7 languages, highlighting its simplicity and effectiveness\footnote{Our code is available at this https URL.</li>
</ul>

<h3>Title: RN-F: A Novel Approach for Mitigating Contaminated Data in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Le Vu Anh, Dinh Duc Nha Nguyen, Phi Long Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13249">https://arxiv.org/abs/2505.13249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13249">https://arxiv.org/pdf/2505.13249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13249]] RN-F: A Novel Approach for Mitigating Contaminated Data in Large Language Models(https://arxiv.org/abs/2505.13249)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become foundational in modern artificial intelligence, powering a wide range of applications from code generation and virtual assistants to scientific research and enterprise automation. However, concerns about data contamination--where test data overlaps with training data--have raised serious questions about the reliability of these applications. Despite awareness of this issue, existing methods fall short in effectively identifying or mitigating contamination. In this paper, we propose Residual-Noise Fingerprinting (RN-F), a novel framework for detecting contaminated data in LLMs. RN-F is a single-pass, gradient-free detection method that leverages residual signal patterns without introducing additional floating-point operations. Our approach is lightweight, model-agnostic, and efficient. We evaluate RN-F on multiple LLMs across various contaminated datasets and show that it consistently outperforms existing state-of-the-art methods, achieving performance improvements of up to 10.5% in contamination detection metrics.</li>
</ul>

<h3>Title: Natural Language Planning via Coding and Inference Scaling</h3>
<ul>
<li><strong>Authors: </strong>Rikhil Amonkar, Ronan Le Bras, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13252">https://arxiv.org/abs/2505.13252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13252">https://arxiv.org/pdf/2505.13252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13252]] Natural Language Planning via Coding and Inference Scaling(https://arxiv.org/abs/2505.13252)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-life textual planning tasks such as meeting scheduling have posed much challenge to LLMs especially when the complexity is high. While previous work primarily studied auto-regressive generation of plans with closed-source models, we systematically evaluate both closed- and open-source models, including those that scales output length with complexity during inference, in generating programs, which are executed to output the plan. We consider not only standard Python code, but also the code to a constraint satisfaction problem solver. Despite the algorithmic nature of the task, we show that programming often but not always outperforms planning. Our detailed error analysis also indicates a lack of robustness and efficiency in the generated code that hinders generalization.</li>
</ul>

<h3>Title: HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Siran Liu, Yang Ye, Qianchao Zhu, Zheng Cao, Yongchao He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13254">https://arxiv.org/abs/2505.13254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13254">https://arxiv.org/pdf/2505.13254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13254]] HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding(https://arxiv.org/abs/2505.13254)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive decoding, the standard approach for Large Language Model (LLM) inference, remains a significant bottleneck due to its sequential nature. While speculative decoding algorithms mitigate this inefficiency through parallel verification, they fail to exploit the inherent heterogeneity in linguistic complexity, a key factor leading to suboptimal resource allocation. We address this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding framework that dynamically optimizes computational resource allocation based on linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying predictable contexts. (2) A dynamic resource allocation strategy based on data-driven entropy partitioning, enabling adaptive speculative expansion and pruning tailored to local context difficulty. Evaluated on five public benchmarks and four models, HeteroSpec achieves an average speedup of 4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across speedup rates, average acceptance length, and verification cost. Notably, HeteroSpec requires no draft model retraining, incurs minimal overhead, and is orthogonal to other acceleration techniques. It demonstrates enhanced acceleration with stronger draft models, establishing a new paradigm for context-aware LLM inference acceleration.</li>
</ul>

<h3>Title: Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Ren, Yekun Xu, Xiaolong Wang, Weitao Li, Weizhi Ma, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13258">https://arxiv.org/abs/2505.13258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13258">https://arxiv.org/pdf/2505.13258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13258]] Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability(https://arxiv.org/abs/2505.13258)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive domains. However, although RAG achieved successes across distinct domains, there are still some unsolved challenges: 1) Effectiveness. Existing research mainly focuses on developing more powerful RAG retrievers, but how to enhance the generator's (LLM's) ability to utilize the retrieved information for reasoning and generation? 2) Transparency. Most RAG methods ignore which retrieved content actually contributes to the reasoning process, resulting in a lack of interpretability and visibility. To address this, we propose ARENA (Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator framework trained via reinforcement learning (RL) with our proposed rewards. Based on the structured generation and adaptive reward calculation, our RL-based training enables the model to identify key evidence, perform structured reasoning, and generate answers with interpretable decision traces. Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments with various RAG baselines demonstrate that our model achieves 10-30% improvements on all multi-hop QA datasets, which is comparable with the SOTA Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses show that ARENA has strong flexibility to be adopted on new datasets without extra training. Our models and codes are publicly released.</li>
</ul>

<h3>Title: From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery</h3>
<ul>
<li><strong>Authors: </strong>Tianshi Zheng, Zheye Deng, Hong Ting Tsang, Weiqi Wang, Jiaxin Bai, Zihao Wang, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13259">https://arxiv.org/abs/2505.13259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13259">https://arxiv.org/pdf/2505.13259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13259]] From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery(https://arxiv.org/abs/2505.13259)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement. Github Repository: this https URL.</li>
</ul>

<h3>Title: DB3D-L: Depth-aware BEV Feature Transformation for Accurate 3D Lane Detection</h3>
<ul>
<li><strong>Authors: </strong>Yehao Liu, Xiaosu Xu, Zijian Wang, Yiqing Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13266">https://arxiv.org/abs/2505.13266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13266">https://arxiv.org/pdf/2505.13266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13266]] DB3D-L: Depth-aware BEV Feature Transformation for Accurate 3D Lane Detection(https://arxiv.org/abs/2505.13266)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>3D Lane detection plays an important role in autonomous driving. Recent advances primarily build Birds-Eye-View (BEV) feature from front-view (FV) images to perceive 3D information of Lane more effectively. However, constructing accurate BEV information from FV image is limited due to the lacking of depth information, causing previous works often rely heavily on the assumption of a flat ground plane. Leveraging monocular depth estimation to assist in constructing BEV features is less constrained, but existing methods struggle to effectively integrate the two tasks. To address the above issue, in this paper, an accurate 3D lane detection method based on depth-aware BEV feature transtormation is proposed. In detail, an effective feature extraction module is designed, in which a Depth Net is integrated to obtain the vital depth information for 3D perception, thereby simplifying the complexity of view transformation. Subquently a feature reduce module is proposed to reduce height dimension of FV features and depth features, thereby enables effective fusion of crucial FV features and depth features. Then a fusion module is designed to build BEV feature from prime FV feature and depth information. The proposed method performs comparably with state-of-the-art methods on both synthetic Apollo, realistic OpenLane datasets.</li>
</ul>

<h3>Title: CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Lei Sheng, Shuai-Shuai Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13271">https://arxiv.org/abs/2505.13271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13271">https://arxiv.org/pdf/2505.13271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13271]] CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning(https://arxiv.org/abs/2505.13271)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular, test-time scaling techniques such as Self-Consistency and Self-Correction can enhance SQL generation accuracy by increasing computational effort during inference. However, these methods have notable limitations: Self-Consistency may select suboptimal outputs despite majority votes, while Self-Correction typically addresses only syntactic errors. To leverage the strengths of both approaches, we propose CSC-SQL, a novel method that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two most frequently occurring outputs from parallel sampling and feeds them into a merge revision model for correction. Additionally, we employ the Group Relative Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and revision models via reinforcement learning, significantly enhancing output quality. Experimental results confirm the effectiveness and generalizability of CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution accuracy, while the 7B model achieves 69.19%. The code will be open sourced at this https URL.</li>
</ul>

<h3>Title: FlowPure: Continuous Normalizing Flows for Adversarial Purification</h3>
<ul>
<li><strong>Authors: </strong>Elias Collaert, Abel Rodrguez, Sander Joos, Lieven Desmet, Vera Rimmer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13280">https://arxiv.org/abs/2505.13280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13280">https://arxiv.org/pdf/2505.13280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13280]] FlowPure: Continuous Normalizing Flows for Adversarial Purification(https://arxiv.org/abs/2505.13280)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in the area, adversarial robustness remains a critical challenge in systems employing machine learning models. The removal of adversarial perturbations at inference time, known as adversarial purification, has emerged as a promising defense strategy. To achieve this, state-of-the-art methods leverage diffusion models that inject Gaussian noise during a forward process to dilute adversarial perturbations, followed by a denoising step to restore clean samples before classification. In this work, we propose FlowPure, a novel purification method based on Continuous Normalizing Flows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings from adversarial examples to their clean counterparts. Unlike prior diffusion-based approaches that rely on fixed noise processes, FlowPure can leverage specific attack knowledge to improve robustness under known threats, while also supporting a more general stochastic variant trained on Gaussian perturbations for settings where such knowledge is unavailable. Experiments on CIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art purification-based defenses in preprocessor-blind and white-box scenarios, and can do so while fully preserving benign accuracy in the former. Moreover, our results show that not only is FlowPure a highly effective purifier but it also holds a strong potential for adversarial detection, identifying preprocessor-blind PGD samples with near-perfect accuracy.</li>
</ul>

<h3>Title: Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts</h3>
<ul>
<li><strong>Authors: </strong>Zekun Wang, Sashank Varma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13281">https://arxiv.org/abs/2505.13281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13281">https://arxiv.org/pdf/2505.13281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13281]] Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts(https://arxiv.org/abs/2505.13281)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>With the rapid improvement of machine learning (ML) models, cognitive scientists are increasingly asking about their alignment with how humans think. Here, we ask this question for computer vision models and human sensitivity to geometric and topological (GT) concepts. Under the core knowledge account, these concepts are innate and supported by dedicated neural circuitry. In this work, we investigate an alternative explanation, that GT concepts are learned ``for free'' through everyday interaction with the environment. We do so using computer visions models, which are trained on large image datasets. We build on prior studies to investigate the overall performance and human alignment of three classes of models -- convolutional neural networks (CNNs), transformer-based models, and vision-language models -- on an odd-one-out task testing 43 GT concepts spanning seven classes. Transformer-based models achieve the highest overall accuracy, surpassing that of young children. They also show strong alignment with children's performance, finding the same classes of concepts easy vs. difficult. By contrast, vision-language models underperform their vision-only counterparts and deviate further from human profiles, indicating that nave multimodality might compromise abstract geometric sensitivity. These findings support the use of computer vision models to evaluate the sufficiency of the learning account for explaining human sensitivity to GT concepts, while also suggesting that integrating linguistic and visual representations might have unpredicted deleterious consequences.</li>
</ul>

<h3>Title: $\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion</h3>
<ul>
<li><strong>Authors: </strong>Sahil Mishra, Kumar Arjun, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13282">https://arxiv.org/abs/2505.13282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13282">https://arxiv.org/pdf/2505.13282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13282]] $\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion(https://arxiv.org/abs/2505.13282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Taxonomies are hierarchical knowledge graphs crucial for recommendation systems, and web applications. As data grows, expanding taxonomies is essential, but existing methods face key challenges: (1) discriminative models struggle with representation limits and generalization, while (2) generative methods either process all candidates at once, introducing noise and exceeding context limits, or discard relevant entities by selecting noisy candidates. We propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines discriminative ranking and generative reasoning for efficient taxonomy expansion. Unlike prior methods, LORex ranks and chunks candidate terms into batches, filtering noise and iteratively refining selections by reasoning candidates' hierarchy to ensure contextual efficiency. Extensive experiments across four benchmarks and twelve baselines show that LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.</li>
</ul>

<h3>Title: RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization</h3>
<ul>
<li><strong>Authors: </strong>Alonso Urbano, David W. Romero, Max Zimmer, Sebastian Pokutta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13289">https://arxiv.org/abs/2505.13289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13289">https://arxiv.org/pdf/2505.13289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13289]] RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization(https://arxiv.org/abs/2505.13289)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-world data often exhibits unknown or approximate symmetries, yet existing equivariant networks must commit to a fixed transformation group prior to training, e.g., continuous $SO(2)$ rotations. This mismatch degrades performance when the actual data symmetries differ from those in the transformation group. We introduce RECON, a framework to discover each input's intrinsic symmetry distribution from unlabeled data. RECON leverages class-pose decompositions and applies a data-driven normalization to align arbitrary reference frames into a common natural pose, yielding directly comparable and interpretable symmetry descriptors. We demonstrate effective symmetry discovery on 2D image benchmarks and -- for the first time -- extend it to 3D transformation groups, paving the way towards more flexible equivariant modeling.</li>
</ul>

<h3>Title: Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Huaiying Luo, Cheng Ji</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13292">https://arxiv.org/abs/2505.13292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13292">https://arxiv.org/pdf/2505.13292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13292]] Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs(https://arxiv.org/abs/2505.13292)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>In the age of cloud computing, data privacy protection has become a major challenge, especially when sharing sensitive data across cloud environments. However, how to optimize collaboration across cloud environments remains an unresolved problem. In this paper, we combine federated learning with large-scale language models to optimize the collaborative mechanism of AI systems. Based on the existing federated learning framework, we introduce a cross-cloud architecture in which federated learning works by aggregating model updates from decentralized nodes without exposing the original data. At the same time, combined with large-scale language models, its powerful context and semantic understanding capabilities are used to improve model training efficiency and decision-making ability. We've further innovated by introducing a secure communication layer to ensure the privacy and integrity of model updates and training data. The model enables continuous model adaptation and fine-tuning across different cloud environments while protecting sensitive data. Experimental results show that the proposed method is significantly better than the traditional federated learning model in terms of accuracy, convergence speed and data privacy protection.</li>
</ul>

<h3>Title: DD-Ranking: Rethinking the Evaluation of Dataset Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zekai Li, Xinhao Zhong, Samir Khaki, Zhiyuan Liang, Yuhao Zhou, Mingjia Shi, Ziqiao Wang, Xuanlei Zhao, Wangbo Zhao, Ziheng Qin, Mengxuan Wu, Pengfei Zhou, Haonan Wang, David Junhao Zhang, Jia-Wei Liu, Shaobo Wang, Dai Liu, Linfeng Zhang, Guang Li, Kun Wang, Zheng Zhu, Zhiheng Ma, Joey Tianyi Zhou, Jiancheng Lv, Yaochu Jin, Peihao Wang, Kaipeng Zhang, Lingjuan Lyu, Yiran Huang, Zeynep Akata, Zhiwei Deng, Xindi Wu, George Cazenavette, Yuzhang Shang, Justin Cui, Jindong Gu, Qian Zheng, Hao Ye, Shuo Wang, Xiaobo Wang, Yan Yan, Angela Yao, Mike Zheng Shou, Tianlong Chen, Hakan Bilen, Baharan Mirzasoleiman, Manolis Kellis, Konstantinos N. Plataniotis, Zhangyang Wang, Bo Zhao, Yang You, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13300">https://arxiv.org/abs/2505.13300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13300">https://arxiv.org/pdf/2505.13300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13300]] DD-Ranking: Rethinking the Evaluation of Dataset Distillation(https://arxiv.org/abs/2505.13300)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In recent years, dataset distillation has provided a reliable solution for data compression, where models trained on the resulting smaller synthetic datasets achieve performance comparable to those trained on the original datasets. To further improve the performance of synthetic datasets, various training pipelines and optimization objectives have been proposed, greatly advancing the field of dataset distillation. Recent decoupled dataset distillation methods introduce soft labels and stronger data augmentation during the post-evaluation phase and scale dataset distillation up to larger datasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy still a reliable metric to fairly evaluate dataset distillation methods? Our empirical findings suggest that the performance improvements of these methods often stem from additional techniques rather than the inherent quality of the images themselves, with even randomly sampled images achieving superior results. Such misaligned evaluation settings severely hinder the development of DD. Therefore, we propose DD-Ranking, a unified evaluation framework, along with new general evaluation metrics to uncover the true performance improvements achieved by different methods. By refocusing on the actual information enhancement of distilled datasets, DD-Ranking provides a more comprehensive and fair evaluation standard for future research advancements.</li>
</ul>

<h3>Title: I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alice Plebe, Timothy Douglas, Diana Riazi, R. Maria del Rio-Chanona</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13302">https://arxiv.org/abs/2505.13302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13302">https://arxiv.org/pdf/2505.13302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13302]] I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models(https://arxiv.org/abs/2505.13302)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly integrated into news recommendation systems, raising concerns about their role in spreading misinformation. In humans, visual content is known to boost credibility and shareability of information, yet its effect on vision-language models (VLMs) remains unclear. We present the first study examining how images influence VLMs' propensity to reshare news content, whether this effect varies across model families, and how persona conditioning and content attributes modulate this behavior. To support this analysis, we introduce two methodological contributions: a jailbreaking-inspired prompting strategy that elicits resharing decisions from VLMs while simulating users with antisocial traits and political alignments; and a multimodal dataset of fact-checked political news from PolitiFact, paired with corresponding images and ground-truth veracity labels. Experiments across model families reveal that image presence increases resharing rates by 4.8% for true news and 15.0% for false news. Persona conditioning further modulates this effect: Dark Triad traits amplify resharing of false news, whereas Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the tested models, only Claude-3-Haiku demonstrates robustness to visual misinformation. These findings highlight emerging risks in multimodal model behavior and motivate the development of tailored evaluation frameworks and mitigation strategies for personalized AI systems. Code and dataset are available at: this https URL</li>
</ul>

<h3>Title: GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Chengsong Sun, Weiping Li, Xiang Li, Yuankun Liu, Lianlei Shan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13306">https://arxiv.org/abs/2505.13306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13306">https://arxiv.org/pdf/2505.13306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13306]] GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval(https://arxiv.org/abs/2505.13306)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Few-shot cross-modal retrieval focuses on learning cross-modal representations with limited training samples, enabling the model to handle unseen classes during inference. Unlike traditional cross-modal retrieval tasks, which assume that both training and testing data share the same class distribution, few-shot retrieval involves data with sparse representations across modalities. Existing methods often fail to adequately model the multi-peak distribution of few-shot cross-modal data, resulting in two main biases in the latent semantic space: intra-modal bias, where sparse samples fail to capture intra-class diversity, and inter-modal bias, where misalignments between image and text distributions exacerbate the semantic gap. These biases hinder retrieval accuracy. To address these issues, we propose a novel method, GCRDP, for few-shot cross-modal retrieval. This approach effectively captures the complex multi-peak distribution of data using a Gaussian Mixture Model (GMM) and incorporates a multi-positive sample contrastive learning mechanism for comprehensive feature modeling. Additionally, we introduce a new strategy for cross-modal semantic alignment, which constrains the relative distances between image and text feature distributions, thereby improving the accuracy of cross-modal representations. We validate our approach through extensive experiments on four benchmark datasets, demonstrating superior performance over six state-of-the-art methods.</li>
</ul>

<h3>Title: RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Qiguang Chen, Libo Qin, Jinhao Liu, Yue Liao, Jiaqi Wang, Jingxuan Zhou, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13307">https://arxiv.org/abs/2505.13307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13307">https://arxiv.org/pdf/2505.13307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13307]] RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning(https://arxiv.org/abs/2505.13307)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models (LLMs) on complex tasks, spurring research into its underlying mechanisms. However, two primary challenges remain for real-world applications: (1) the lack of quantitative metrics and actionable guidelines for evaluating and optimizing measurable boundaries of CoT capability, and (2) the absence of methods to assess boundaries of unmeasurable CoT capability, such as multimodal perception. To address these gaps, we introduce the Reasoning Boundary Framework++ (RBF++). To tackle the first challenge, we define the reasoning boundary (RB) as the maximum limit of CoT performance. We also propose a combination law for RBs, enabling quantitative analysis and offering actionable guidance across various CoT tasks. For the second challenge, particularly in multimodal scenarios, we introduce a constant assumption, which replaces unmeasurable RBs with scenario-specific constants. Additionally, we propose the reasoning boundary division mechanism, which divides unmeasurable RBs into two sub-boundaries, facilitating the quantification and optimization of both unmeasurable domain knowledge and multimodal perception capabilities. Extensive experiments involving 38 models across 13 tasks validate the feasibility of our framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies, offer insights into optimization and decay from two complementary perspectives, and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope this work advances the understanding of RBs and optimization strategies in LLMs. Code and data are available at this https URL.</li>
</ul>

<h3>Title: Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, Zilong Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13308">https://arxiv.org/abs/2505.13308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13308">https://arxiv.org/pdf/2505.13308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13308]] Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space(https://arxiv.org/abs/2505.13308)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.</li>
</ul>

<h3>Title: GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhijie Deng, Chris Yuhao Liu, Zirui Pang, Xinlei He, Lei Feng, Qi Xuan, Zhaowei Zhu, Jiaheng Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13312">https://arxiv.org/abs/2505.13312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13312">https://arxiv.org/pdf/2505.13312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13312]] GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection(https://arxiv.org/abs/2505.13312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong capabilities in memorizing vast amounts of knowledge across diverse domains. However, the ability to selectively forget specific knowledge is critical for ensuring the safety and compliance of deployed models. Existing unlearning efforts typically fine-tune the model with resources such as forget data, retain data, and a calibration model. These additional gradient steps blur the decision boundary between forget and retain knowledge, making unlearning often at the expense of overall performance. To avoid the negative impact of fine-tuning, it would be better to unlearn solely at inference time by safely guarding the model against generating responses related to the forget target, without destroying the fluency of text generation. In this work, we propose Generation-time Unlearning via Adaptive Restriction and Detection (GUARD), a framework that enables dynamic unlearning during LLM generation. Specifically, we first employ a prompt classifier to detect unlearning targets and extract the corresponding forbidden token. We then dynamically penalize and filter candidate tokens during generation using a combination of token matching and semantic matching, effectively preventing the model from leaking the forgotten content. Experimental results on copyright content unlearning tasks over the Harry Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on the TOFU dataset, demonstrate that GUARD achieves strong forget quality across various tasks while causing almost no degradation to the LLM's general capabilities, striking an excellent trade-off between forgetting and utility.</li>
</ul>

<h3>Title: KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation</h3>
<ul>
<li><strong>Authors: </strong>Reza T. Batley, Sourav Saha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13315">https://arxiv.org/abs/2505.13315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13315">https://arxiv.org/pdf/2505.13315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13315]] KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation(https://arxiv.org/abs/2505.13315)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Contemporary models of high dimensional physical systems are constrained by the curse of dimensionality and a reliance on dense data. We introduce KHRONOS (Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an AI framework for model based, model free and model inversion tasks. KHRONOS constructs continuously differentiable target fields with a hierarchical composition of per-dimension kernel expansions, which are tensorized into modes and then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation benchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square errors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov Arnold Networks (which itself reports a 100 times improvement on MLPs/PINNs with 100 times fewer parameters) when controlling for the number of parameters. This also represents a 1e4 times improvement in L2 square error compared to standard linear FEM at comparable DoFs. Inference complexity is dominated by inner products, yielding sub-millisecond full-field predictions that scale to an arbitrary resolution. For inverse problems, KHRONOS facilitates rapid, iterative level set recovery in only a few forward evaluations, with sub-microsecond per sample latency. KHRONOS scalability, expressivity, and interpretability open new avenues in constrained edge computing, online control, computer vision, and beyond.</li>
</ul>

<h3>Title: Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates</h3>
<ul>
<li><strong>Authors: </strong>Gabriele Spadaro, Alberto Presta, Jhony H. Giraldo, Marco Grangetto, Wei Hu, Giuseppe Valenzise, Attilio Fiandrotti, Enzo Tartaglione</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13316">https://arxiv.org/abs/2505.13316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13316">https://arxiv.org/pdf/2505.13316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13316]] Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates(https://arxiv.org/abs/2505.13316)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Efficient compression of low-bit-rate point clouds is critical for bandwidth-constrained applications. However, existing techniques mainly focus on high-fidelity reconstruction, requiring many bits for compression. This paper proposes a "Denoising Diffusion Probabilistic Model" (DDPM) architecture for point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder produces the condition vector for the generation, which is then quantized via a learnable vector quantizer. This configuration allows to achieve a low bitrates while preserving quality. Experiments on ShapeNet and ModelNet40 show improved rate-distortion at low rates compared to standardized and state-of-the-art approaches. We publicly released the code at this https URL.</li>
</ul>

<h3>Title: Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Song-Lin Li, Rui Zhu, Yu-Feng Li, Lan-Zhe Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13317">https://arxiv.org/abs/2505.13317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13317">https://arxiv.org/pdf/2505.13317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13317]] Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning(https://arxiv.org/abs/2505.13317)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning (SSL) alleviates the cost of data labeling process by exploiting unlabeled data, and has achieved promising results on various tasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm has garnered significant attention in recent years, and exploiting pre-trained models could also reduce the requirement of labeled data in downstream tasks. Therefore, a question naturally occurs: \emph{When the labeled data is scarce in the target tasks, should we exploit unlabeled data or pre-trained models?} To answer this question, we select pre-trained Vision-Language Models (VLMs) as representative pretrain-finetuning instances and propose \textit{Few-shot SSL} -- a framework that enables fair comparison between these two paradigms by controlling the amount of labeled data used. Extensive experiments across various settings demonstrate that pre-trained VLMs generally outperform SSL methods in nearly all cases, except when the data has low resolution or lacks clear semantic structure. Therefore, we encourage future SSL research to compare with pre-trained models and explore deeper integration, such as using pre-trained knowledge to enhance pseudo-labeling. To support future research, we release our unified reproduction and evaluation framework. Codes are available at this https URL</li>
</ul>

<h3>Title: VesselGPT: Autoregressive Modeling of Vascular Geometry</h3>
<ul>
<li><strong>Authors: </strong>Paula Feldman, Martin Sinnona, Viviana Siless, Claudio Delrieux, Emmanuel Iarussi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13318">https://arxiv.org/abs/2505.13318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13318">https://arxiv.org/pdf/2505.13318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13318]] VesselGPT: Autoregressive Modeling of Vascular Geometry(https://arxiv.org/abs/2505.13318)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Anatomical trees are critical for clinical diagnosis and treatment planning, yet their complex and diverse geometry make accurate representation a significant challenge. Motivated by the latest advances in large language models, we introduce an autoregressive method for synthesizing anatomical trees. Our approach first embeds vessel structures into a learned discrete vocabulary using a VQ-VAE architecture, then models their generation autoregressively with a GPT-2 model. This method effectively captures intricate geometries and branching patterns, enabling realistic vascular tree synthesis. Comprehensive qualitative and quantitative evaluations reveal that our technique achieves high-fidelity tree reconstruction with compact discrete representations. Moreover, our B-spline representation of vessel cross-sections preserves critical morphological details that are often overlooked in previous' methods parameterizations. To the best of our knowledge, this work is the first to generate blood vessels in an autoregressive manner. Code, data, and trained models will be made available.</li>
</ul>

<h3>Title: SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated Distillation</h3>
<ul>
<li><strong>Authors: </strong>Tian Wen, Sheng Sun, Yuwei Wang, Peiyan Chen, Zhiyuan Wu, Min Liu, Bo Gao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13319">https://arxiv.org/abs/2505.13319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13319">https://arxiv.org/pdf/2505.13319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13319]] SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated Distillation(https://arxiv.org/abs/2505.13319)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Secure Aggregation (SA) is an indispensable component of Federated Learning (FL) that concentrates on privacy preservation while allowing for robust aggregation. However, most SA designs rely heavily on the unrealistic assumption of homogeneous model architectures. Federated Distillation (FD), which aggregates locally computed logits instead of model parameters, introduces a promising alternative for cooperative training in heterogeneous model settings. Nevertheless, we recognize two major challenges in implementing SA for FD. (i) Prior SA designs encourage a dominant server, who is solely responsible for collecting, aggregating and distributing. Such central authority facilitates server to forge aggregation proofs or collude to bypass the claimed security guarantees; (ii) Existing SA, tailored for FL models, overlook the intrinsic properties of logits, making them unsuitable for FD. To address these challenges, we propose SVAFD, the first SA protocol that is specifically designed for FD. At a high level, SVAFD incorporates two innovations: (i) a multilateral co-aggregation method tha redefines the responsibilities of clients and server. Clients autonomously evaluate and aggregate logits shares locally with a lightweight coding scheme, while the server handles ciphertext decoding and performs the task of generating verification proofs; (ii) a quality-aware knowledge filtration method that facilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD is resilient to stragglers and colluding clients, making it well-suited for dynamic networks in real-world applications. We have implemented the SVAFD prototype over four emerging FD architectures and evaluated it against poisoning and inference attacks. Results demonstrate that SVAFD improves model accuracy, making it a significant step forward in secure and verifiable aggregation for heterogeneous FL systems.</li>
</ul>

<h3>Title: Thinking Short and Right Over Thinking Long: Serving LLM Reasoning Efficiently and Accurately</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Wang, Youhe Jiang, Bin Cui, Fangcheng Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13326">https://arxiv.org/abs/2505.13326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13326">https://arxiv.org/pdf/2505.13326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13326]] Thinking Short and Right Over Thinking Long: Serving LLM Reasoning Efficiently and Accurately(https://arxiv.org/abs/2505.13326)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in test-time scaling suggest that Large Language Models (LLMs) can gain better capabilities by generating Chain-of-Thought reasoning (analogous to human thinking) to respond a given request, and meanwhile exploring more reasoning branches (i.e., generating multiple responses and ensembling them) can improve the final output quality. However, when incorporating the two scaling dimensions, we find that the system efficiency is dampened significantly for two reasons. Firstly, the time cost to generate the final output increases substantially as many reasoning branches would be trapped in the over-thinking dilemma, producing excessively long responses. Secondly, generating multiple reasoning branches for each request increases memory consumption, which is unsuitable for LLM serving since we can only batch a limited number of requests to process simultaneously. To address this, we present SART, a serving framework for efficient and accurate LLM reasoning. The essential idea is to manage the thinking to be short and right, rather than long. For one thing, we devise a redundant sampling with early stopping approach based on empirical observations and theoretic analysis, which increases the likelihood of obtaining short-thinking responses when sampling reasoning branches. For another, we propose to dynamically prune low-quality branches so that only right-thinking branches are maintained, reducing the memory consumption and allowing us to batch more requests. Experimental results demonstrate that SART not only improves the accuracy of LLM reasoning but also enhances the serving efficiency, outperforming existing methods by up to 28.2 times and on average 15.7 times in terms of efficiency when achieving the same level of accuracy.</li>
</ul>

<h3>Title: Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ajian Liu, Haocheng Yuan, Xiao Guo, Hui Ma, Wanyi Zhuang, Changtao Miao, Yan Hong, Chuanbiao Song, Jun Lan, Qi Chu, Tao Gong, Yanyan Liang, Weiqiang Wang, Jun Wan, Xiaoming Liu, Zhen Lei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13327">https://arxiv.org/abs/2505.13327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13327">https://arxiv.org/pdf/2505.13327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13327]] Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning(https://arxiv.org/abs/2505.13327)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Presentation Attack Detection and Face Forgery Detection are designed to protect face data from physical media-based Presentation Attacks and digital editing-based DeepFakes respectively. But separate training of these two models makes them vulnerable to unknown attacks and burdens deployment environments. The lack of a Unified Face Attack Detection model to handle both types of attacks is mainly due to two factors. First, there's a lack of adequate benchmarks for models to explore. Existing UAD datasets have limited attack types and samples, restricting the model's ability to address advanced threats. To address this, we propose UniAttackDataPlus (UniAttackData+), the most extensive and sophisticated collection of forgery techniques to date. It includes 2,875 identities and their 54 kinds of falsified samples, totaling 697,347 videos. Second, there's a lack of a reliable classification criterion. Current methods try to find an arbitrary criterion within the same semantic space, which fails when encountering diverse attacks. So, we present a novel Visual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that adaptively explores multiple classification criteria from different semantic spaces. We build a Visual Prompt Tree to explore various classification rules hierarchically. Then, by adaptively pruning the prompts, the model can select the most suitable prompts to guide the encoder to extract discriminative features at different levels in a coarse-to-fine way. Finally, to help the model understand the classification criteria in visual space, we propose a Dynamically Prompt Integration module to project the visual prompts to the text encoder for more accurate semantics. Experiments on 12 datasets have shown the potential to inspire further innovations in the UAD field.</li>
</ul>

<h3>Title: Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Hongru Wang, Wenyu Huang, Yufei Wang, Yuanhao Xi, Jianqiao Lu, Huan Zhang, Nan Hu, Zeming Liu, Jeff Z. Pan, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13328">https://arxiv.org/abs/2505.13328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13328">https://arxiv.org/pdf/2505.13328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13328]] Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges(https://arxiv.org/abs/2505.13328)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing benchmarks that assess Language Models (LMs) as Language Agents (LAs) for tool use primarily focus on stateless, single-turn interactions or partial evaluations, such as tool selection in a single turn, overlooking the inherent stateful nature of interactions in multi-turn applications. To fulfill this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with stateful tool interactions considering the whole life cycle of tool use, across six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool utilization}: tool awareness, tool selection, tool execution; and 3) \textit{role-consistent response}: response generation and role play. Furthermore, we build \texttt{VirtualMobile} -- an embodied virtual mobile evaluation environment to simulate API calls and assess the robustness of the created APIs\footnote{We will use tools and APIs alternatively, there are no significant differences between them in this paper.}. Taking advantage of these artifacts, we conduct comprehensive evaluation on 13 distinct open- and closed-source LLMs and provide detailed analysis at each stage, revealing that the existing state-of-the-art LLMs still cannot perform well to use tools over long horizons.</li>
</ul>

<h3>Title: Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation</h3>
<ul>
<li><strong>Authors: </strong>Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13338">https://arxiv.org/abs/2505.13338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13338">https://arxiv.org/pdf/2505.13338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13338]] Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation(https://arxiv.org/abs/2505.13338)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.</li>
</ul>

<h3>Title: Detect and Correct: A Selective Noise Correction Method for Learning with Noisy Labels</h3>
<ul>
<li><strong>Authors: </strong>Yuval Grinberg, Nimrod Harel, Jacob Goldberger, Ofir Lindenbaum</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13342">https://arxiv.org/abs/2505.13342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13342">https://arxiv.org/pdf/2505.13342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13342]] Detect and Correct: A Selective Noise Correction Method for Learning with Noisy Labels(https://arxiv.org/abs/2505.13342)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Falsely annotated samples, also known as noisy labels, can significantly harm the performance of deep learning models. Two main approaches for learning with noisy labels are global noise estimation and data filtering. Global noise estimation approximates the noise across the entire dataset using a noise transition matrix, but it can unnecessarily adjust correct labels, leaving room for local improvements. Data filtering, on the other hand, discards potentially noisy samples but risks losing valuable data. Our method identifies potentially noisy samples based on their loss distribution. We then apply a selection process to separate noisy and clean samples and learn a noise transition matrix to correct the loss for noisy samples while leaving the clean data unaffected, thereby improving the training process. Our approach ensures robust learning and enhanced model performance by preserving valuable information from noisy samples and refining the correction process. We applied our method to standard image datasets (MNIST, CIFAR-10, and CIFAR-100) and a biological scRNA-seq cell-type annotation dataset. We observed a significant improvement in model accuracy and robustness compared to traditional methods.</li>
</ul>

<h3>Title: RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Ahmet Berke Gokmen, Yigit Ekin, Bahri Batuhan Bilecen, Aysegul Dundar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13344">https://arxiv.org/abs/2505.13344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13344">https://arxiv.org/pdf/2505.13344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13344]] RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers(https://arxiv.org/abs/2505.13344)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.</li>
</ul>

<h3>Title: J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13346">https://arxiv.org/abs/2505.13346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13346">https://arxiv.org/pdf/2505.13346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13346]] J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization(https://arxiv.org/abs/2505.13346)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.</li>
</ul>

<h3>Title: Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>Narek Maloyan, Bislan Ashinov, Dmitry Namiot</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13348">https://arxiv.org/abs/2505.13348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13348">https://arxiv.org/pdf/2505.13348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13348]] Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks(https://arxiv.org/abs/2505.13348)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly employed as evaluators (LLM-as-a-Judge) for assessing the quality of machine-generated text. This paradigm offers scalability and cost-effectiveness compared to human annotation. However, the reliability and security of such systems, particularly their robustness against adversarial manipulations, remain critical concerns. This paper investigates the vulnerability of LLM-as-a-Judge architectures to prompt-injection attacks, where malicious inputs are designed to compromise the judge's decision-making process. We formalize two primary attack strategies: Comparative Undermining Attack (CUA), which directly targets the final decision output, and Justification Manipulation Attack (JMA), which aims to alter the model's generated reasoning. Using the Greedy Coordinate Gradient (GCG) optimization method, we craft adversarial suffixes appended to one of the responses being compared. Experiments conducted on the MT-Bench Human Judgments dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable effectiveness. These findings highlight substantial vulnerabilities in current LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and further research into adversarial evaluation and trustworthiness in LLM-based assessment frameworks.</li>
</ul>

<h3>Title: Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Adam torek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13353">https://arxiv.org/abs/2505.13353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13353">https://arxiv.org/pdf/2505.13353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13353]] Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning(https://arxiv.org/abs/2505.13353)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although modern Large Language Models (LLMs) support extremely large contexts, their effectiveness in utilizing long context for code reasoning remains unclear. This paper investigates LLM reasoning ability over code snippets within large repositories and how it relates to their recall ability. Specifically, we differentiate between lexical code recall (verbatim retrieval) and semantic code recall (remembering what the code does). To measure semantic recall, we propose SemTrace, a code reasoning technique where the impact of specific statements on output is attributable and unpredictable. We also present a method to quantify semantic recall sensitivity in existing benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop in code reasoning accuracy as a code snippet approaches the middle of the input context, particularly with techniques requiring high semantic recall like SemTrace. Moreover, we find that lexical recall varies by granularity, with models excelling at function retrieval but struggling with line-by-line recall. Notably, a disconnect exists between lexical and semantic recall, suggesting different underlying mechanisms. Finally, our findings indicate that current code reasoning benchmarks may exhibit low semantic recall sensitivity, potentially underestimating LLM challenges in leveraging in-context information.</li>
</ul>

<h3>Title: One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling</h3>
<ul>
<li><strong>Authors: </strong>Nimrod Berman, Ilan Naiman, Moshe Eliasof, Hedi Zisling, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13358">https://arxiv.org/abs/2505.13358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13358">https://arxiv.org/pdf/2505.13358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13358]] One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling(https://arxiv.org/abs/2505.13358)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative models have demonstrated exceptional performance, yet their iterative sampling procedures remain computationally expensive. A prominent strategy to mitigate this cost is distillation, with offline distillation offering particular advantages in terms of efficiency, modularity, and flexibility. In this work, we identify two key observations that motivate a principled distillation framework: (1) while diffusion models have been viewed through the lens of dynamical systems theory, powerful and underexplored tools can be further leveraged; and (2) diffusion models inherently impose structured, semantically coherent trajectories in latent space. Building on these observations, we introduce the Koopman Distillation Model KDM, a novel offline distillation approach grounded in Koopman theory-a classical framework for representing nonlinear dynamics linearly in a transformed space. KDM encodes noisy inputs into an embedded space where a learned linear operator propagates them forward, followed by a decoder that reconstructs clean samples. This enables single-step generation while preserving semantic fidelity. We provide theoretical justification for our approach: (1) under mild assumptions, the learned diffusion dynamics admit a finite-dimensional Koopman representation; and (2) proximity in the Koopman latent space correlates with semantic similarity in the generated outputs, allowing for effective trajectory alignment. Empirically, KDM achieves state-of-the-art performance across standard offline distillation benchmarks, improving FID scores by up to 40% in a single generation step. All implementation details and code for the experimental setups are provided in our GitHub - this https URL, or in our project page - this https URL.</li>
</ul>

<h3>Title: What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Yang, Yike Shi, Qianou Ma, Michael Xieyang Liu, Christian Kstner, Tongshuang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13360">https://arxiv.org/abs/2505.13360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13360">https://arxiv.org/pdf/2505.13360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13360]] What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts(https://arxiv.org/abs/2505.13360)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Building LLM-powered software requires developers to communicate their requirements through natural language, but developer prompts are frequently underspecified, failing to fully capture many user-important requirements. In this paper, we present an in-depth analysis of prompt underspecification, showing that while LLMs can often (41.1%) guess unspecified requirements by default, such behavior is less robust: Underspecified prompts are 2x more likely to regress over model or prompt changes, sometimes with accuracy drops by more than 20%. We then demonstrate that simply adding more requirements to a prompt does not reliably improve performance, due to LLMs' limited instruction-following capabilities and competing constraints, and standard prompt optimizers do not offer much help. To address this, we introduce novel requirements-aware prompt optimization mechanisms that can improve performance by 4.8% on average over baselines that naively specify everything in the prompt. Beyond prompt optimization, we envision that effectively managing prompt underspecification requires a broader process, including proactive requirements discovery, evaluation, and monitoring.</li>
</ul>

<h3>Title: DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against Membership Inference Attacks</h3>
<ul>
<li><strong>Authors: </strong>Javad Forough, Hamed Haddadi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13362">https://arxiv.org/abs/2505.13362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13362">https://arxiv.org/pdf/2505.13362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13362]] DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against Membership Inference Attacks(https://arxiv.org/abs/2505.13362)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Membership Inference Attacks (MIAs) pose a significant risk to the privacy of training datasets by exploiting subtle differences in model outputs to determine whether a particular data sample was used during training. These attacks can compromise sensitive information, especially in domains such as healthcare and finance, where data privacy is paramount. Traditional mitigation techniques, such as static differential privacy, rely on injecting a fixed amount of noise during training or inference. However, this approach often leads to a detrimental trade-off: the noise may be insufficient to counter sophisticated attacks or, when increased, may substantially degrade model performance. In this paper, we present DynaNoise, an adaptive approach that dynamically modulates noise injection based on query sensitivity. Our approach performs sensitivity analysis using measures such as Shannon entropy to evaluate the risk associated with each query and adjusts the noise variance accordingly. A probabilistic smoothing step is then applied to renormalize the perturbed outputs, ensuring that the model maintains high accuracy while effectively obfuscating membership signals. We further propose an empirical metric, the Membership Inference Defense Privacy-Utility Tradeoff (MIDPUT), which quantifies the balance between reducing attack success rates and preserving the target model's accuracy. Our extensive evaluation on several benchmark datasets demonstrates that DynaNoise not only significantly reduces MIA success rates but also achieves up to a fourfold improvement in the MIDPUT metric compared to the state-of-the-art. Moreover, DynaNoise maintains competitive model accuracy while imposing only marginal inference overhead, highlighting its potential as an effective and efficient privacy defense against MIAs.</li>
</ul>

<h3>Title: Restoration Score Distillation: From Corrupted Diffusion Pretraining to One-Step High-Quality Generation</h3>
<ul>
<li><strong>Authors: </strong>Yasi Zhang, Tianyu Chen, Zhendong Wang, Ying Nian Wu, Mingyuan Zhou, Oscar Leong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13377">https://arxiv.org/abs/2505.13377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13377">https://arxiv.org/pdf/2505.13377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13377]] Restoration Score Distillation: From Corrupted Diffusion Pretraining to One-Step High-Quality Generation(https://arxiv.org/abs/2505.13377)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning generative models from corrupted data is a fundamental yet persistently challenging task across scientific disciplines, particularly when access to clean data is limited or expensive. Denoising Score Distillation (DSD) \cite{chen2025denoising} recently introduced a novel and surprisingly effective strategy that leverages score distillation to train high-fidelity generative models directly from noisy observations. Building upon this foundation, we propose \textit{Restoration Score Distillation} (RSD), a principled generalization of DSD that accommodates a broader range of corruption types, such as blurred, incomplete, or low-resolution images. RSD operates by first pretraining a teacher diffusion model solely on corrupted data and subsequently distilling it into a single-step generator that produces high-quality reconstructions. Empirically, RSD consistently surpasses its teacher model across diverse restoration tasks on both natural and scientific datasets. Moreover, beyond standard diffusion objectives, the RSD framework is compatible with several corruption-aware training techniques such as Ambient Tweedie, Ambient Diffusion, and its Fourier-space variant, enabling flexible integration with recent advances in diffusion modeling. Theoretically, we demonstrate that in a linear regime, RSD recovers the eigenspace of the clean data covariance matrix from linear measurements, thereby serving as an implicit regularizer. This interpretation recasts score distillation not only as a sampling acceleration technique but as a principled approach to enhancing generative performance in severely degraded data regimes.</li>
</ul>

<h3>Title: R3: Robust Rubric-Agnostic Reward Models</h3>
<ul>
<li><strong>Authors: </strong>David Anugraha, Zilu Tang, Lester James V. Miranda, Hanyang Zhao, Mohammad Rifqi Farhansyah, Garry Kuwanto, Derry Wijaya, Genta Indra Winata</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13388">https://arxiv.org/abs/2505.13388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13388">https://arxiv.org/pdf/2505.13388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13388]] R3: Robust Rubric-Agnostic Reward Models(https://arxiv.org/abs/2505.13388)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at this https URL</li>
</ul>

<h3>Title: Faster Video Diffusion with Trainable Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Peiyuan Zhang, Haofeng Huang, Yongqi Chen, Will Lin, Zhengzhong Liu, Ion Stoica, Eric P. Xing, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13389">https://arxiv.org/abs/2505.13389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13389">https://arxiv.org/pdf/2505.13389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13389]] Faster Video Diffusion with Trainable Sparse Attention(https://arxiv.org/abs/2505.13389)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at \emph{both} training and inference. In VSA, a lightweight coarse stage pools tokens into tiles and identifies high-weight \emph{critical tokens}; a fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to a single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of FlashAttention3 MFU. We perform a large sweep of ablation studies and scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with comparable quality. These results establish trainable sparse attention as a practical alternative to full attention and a key enabler for further scaling of video diffusion models.</li>
</ul>

<h3>Title: MR. Judge: Multimodal Reasoner as a Judge</h3>
<ul>
<li><strong>Authors: </strong>Renjie Pi, Felix Bai, Qibin Chen, Simon Wang, Jiulong Shan, Kieran Liu, Meng Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13403">https://arxiv.org/abs/2505.13403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13403">https://arxiv.org/pdf/2505.13403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13403]] MR. Judge: Multimodal Reasoner as a Judge(https://arxiv.org/abs/2505.13403)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The paradigm of using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) as evaluative judges has emerged as an effective approach in RLHF and inference-time scaling. In this work, we propose Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering general-purpose MLLMs judges with strong reasoning capabilities. Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem. Specifically, the judge model first conducts deliberate reasoning covering different aspects of the responses and eventually selects the best response from them. This reasoning process not only improves the interpretibility of the judgement, but also greatly enhances the performance of MLLM judges. To cope with the lack of questions with scored responses, we propose the following strategy to achieve automatic annotation: 1) Reverse Response Candidates Synthesis: starting from a supervised fine-tuning (SFT) dataset, we treat the original response as the best candidate and prompt the MLLM to generate plausible but flawed negative candidates. 2) Text-based reasoning extraction: we carefully design a data synthesis pipeline for distilling the reasoning capability from a text-based reasoning model, which is adopted to enable the MLLM judges to regain complex reasoning ability via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge is effective across a wide range of tasks. Specifically, our MR. Judge-7B surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet during inference-time scaling by up to 7.7%.</li>
</ul>

<h3>Title: Granary: Speech Recognition and Translation Dataset in 25 European Languages</h3>
<ul>
<li><strong>Authors: </strong>Nithin Rao Koluguri, Monica Sekoyan, George Zelenfroynd, Sasha Meister, Shuoyang Ding, Sofia Kostandian, He Huang, Nikolay Karpov, Jagadeesh Balam, Vitaly Lavrukhin, Yifan Peng, Sara Papi, Marco Gaido, Alessio Brutti, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13404">https://arxiv.org/abs/2505.13404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13404">https://arxiv.org/pdf/2505.13404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13404]] Granary: Speech Recognition and Translation Dataset in 25 European Languages(https://arxiv.org/abs/2505.13404)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multi-task and multilingual approaches benefit large models, yet speech processing for low-resource languages remains underexplored due to data scarcity. To address this, we present Granary, a large-scale collection of speech datasets for recognition and translation across 25 European languages. This is the first open-source effort at this scale for both transcription and translation. We enhance data quality using a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. We further generate translation pairs from pseudo-labeled transcriptions using EuroLLM, followed by a data filtration pipeline. Designed for efficiency, our pipeline processes vast amount of data within hours. We assess models trained on processed data by comparing their performance on previously curated datasets for both high- and low-resource languages. Our findings show that these models achieve similar performance using approx. 50% less data. Dataset will be made available at this https URL</li>
</ul>

<h3>Title: A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Malikal, Ismail Alkhouri, Alvaro Velasquez, Adam M Alessio, Saiprasad Ravishankar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13405">https://arxiv.org/abs/2505.13405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13405">https://arxiv.org/pdf/2505.13405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13405]] A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut(https://arxiv.org/abs/2505.13405)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal solution is NP-hard in the worst case. As a result, heuristic-based algorithms are commonly used, though their design often requires significant domain expertise. More recently, learning-based methods trained on large (un)labeled datasets have been proposed; however, these approaches often struggle with generalizability and scalability. A well-known approximation algorithm for MaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic Unconstrained Binary Optimization (QUBO) formulation into a semidefinite program (SDP). The GW algorithm then applies hyperplane rounding by uniformly sampling a random hyperplane to convert the SDP solution into binary node assignments. In this paper, we propose a training-data-free approach based on a non-episodic reinforcement learning formulation, in which an agent learns to select improved rounding hyperplanes that yield better cuts than those produced by the GW algorithm. By optimizing over a Markov Decision Process (MDP), our method consistently achieves better cuts across large-scale graphs with varying densities and degree distributions.</li>
</ul>

<h3>Title: FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhuozhao Hu, Kaishen Yuan, Xin Liu, Zitong Yu, Yuan Zong, Jingang Shi, Huanjing Yue, Jingyu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13419">https://arxiv.org/abs/2505.13419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13419">https://arxiv.org/pdf/2505.13419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13419]] FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning(https://arxiv.org/abs/2505.13419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Facial Emotion Analysis (FEA) plays a crucial role in visual affective computing, aiming to infer a person's emotional state based on facial data. Scientifically, facial expressions (FEs) result from the coordinated movement of facial muscles, which can be decomposed into specific action units (AUs) that provide detailed emotional insights. However, traditional methods often struggle with limited interpretability, constrained generalization and reasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have shown exceptional performance in various visual tasks, while they still face significant challenges in FEA due to the lack of specialized datasets and their inability to capture the intricate relationships between FEs and AUs. To address these issues, we introduce a novel FEA Instruction Dataset that provides accurate and aligned FE and AU descriptions and establishes causal reasoning relationships between them, followed by constructing a new benchmark, FEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to capture more detailed facial information, enhancing its capability in FEA tasks. Our model demonstrates strong performance on FEABench and impressive generalization capability through zero-shot evaluation on various datasets, including RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and effectiveness in FEA tasks. The dataset and code will be available at this https URL.</li>
</ul>

<h3>Title: Make Still Further Progress: Chain of Thoughts for Tabular Data Leaderboard</h3>
<ul>
<li><strong>Authors: </strong>Si-Yang Liu, Qile Zhou, Han-Jia Ye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13421">https://arxiv.org/abs/2505.13421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13421">https://arxiv.org/pdf/2505.13421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13421]] Make Still Further Progress: Chain of Thoughts for Tabular Data Leaderboard(https://arxiv.org/abs/2505.13421)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tabular data, a fundamental data format in machine learning, is predominantly utilized in competitions and real-world applications. The performance of tabular models--such as gradient boosted decision trees and neural networks--can vary significantly across datasets due to differences in feature distributions and task characteristics. Achieving top performance on each dataset often requires specialized expert knowledge. To address this variability, practitioners often aggregate the predictions of multiple models. However, conventional aggregation strategies typically rely on static combination rules and lack instance-level adaptability. In this work, we propose an in-context ensemble framework for tabular prediction that leverages large language models (LLMs) to perform dynamic, instance-specific integration of external model predictions. Without access to raw tabular features or semantic information, our method constructs a context around each test instance using its nearest neighbors and the predictions from a pool of external models. Within this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$), a prompting strategy that guides LLMs through multi-step, interpretable reasoning, making still further progress toward expert-level decision-making. Experimental results show that our method outperforms well-tuned baselines and standard ensemble techniques across a wide range of tabular datasets.</li>
</ul>

<h3>Title: Learnware of Language Models: Specialized Small Language Models Can Do Big</h3>
<ul>
<li><strong>Authors: </strong>Zhi-Hao Tan, Zi-Chen Zhao, Hao-Yu Shi, Xin-Yu Zhang, Peng Tan, Yang Yu, Zhi-Hua Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13425">https://arxiv.org/abs/2505.13425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13425">https://arxiv.org/pdf/2505.13425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13425]] Learnware of Language Models: Specialized Small Language Models Can Do Big(https://arxiv.org/abs/2505.13425)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The learnware paradigm offers a novel approach to machine learning by enabling users to reuse a set of well-trained models for tasks beyond the models' original purposes. It eliminates the need to build models from scratch, instead relying on specifications (representations of a model's capabilities) to identify and leverage the most suitable models for new tasks. While learnware has proven effective in many scenarios, its application to language models has remained largely unexplored. At the same time, large language models (LLMs) have demonstrated remarkable universal question-answering abilities, yet they face challenges in specialized scenarios due to data scarcity, privacy concerns, and high computational costs, thus more and more specialized small language models (SLMs) are being trained for specific domains. To address these limitations systematically, the learnware paradigm provides a promising solution by enabling maximum utilization of specialized SLMs, and allowing users to identify and reuse them in a collaborative and privacy-preserving manner. This paper presents a preliminary attempt to apply the learnware paradigm to language models. We simulated a learnware system comprising approximately 100 learnwares of specialized SLMs with 8B parameters, fine-tuned across finance, healthcare, and mathematics domains. Each learnware contains an SLM and a specification, which enables users to identify the most relevant models without exposing their own data. Experimental results demonstrate promising performance: by selecting one suitable learnware for each task-specific inference, the system outperforms the base SLMs on all benchmarks. Compared to LLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and Llama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses Flan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical domain tasks.</li>
</ul>

<h3>Title: Fine-tuning Quantized Neural Networks with Zeroth-order Optimization</h3>
<ul>
<li><strong>Authors: </strong>Sifeng Shang, Jiayi Zhou, Chenyu Lin, Minxian Li, Kaiyang Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13430">https://arxiv.org/abs/2505.13430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13430">https://arxiv.org/pdf/2505.13430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13430]] Fine-tuning Quantized Neural Networks with Zeroth-order Optimization(https://arxiv.org/abs/2505.13430)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within a single 24GB GPU.</li>
</ul>

<h3>Title: Synthetic-Powered Predictive Inference</h3>
<ul>
<li><strong>Authors: </strong>Meshi Bashari, Roy Maor Lotan, Yonghoon Lee, Edgar Dobriban, Yaniv Romano</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13432">https://arxiv.org/abs/2505.13432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13432">https://arxiv.org/pdf/2505.13432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13432]] Synthetic-Powered Predictive Inference(https://arxiv.org/abs/2505.13432)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conformal prediction is a framework for predictive inference with a distribution-free, finite-sample guarantee. However, it tends to provide uninformative prediction sets when calibration data are scarce. This paper introduces Synthetic-powered predictive inference (SPPI), a novel framework that incorporates synthetic data -- e.g., from a generative model -- to improve sample efficiency. At the core of our method is a score transporter: an empirical quantile mapping that aligns nonconformity scores from trusted, real data with those from synthetic data. By carefully integrating the score transporter into the calibration process, SPPI provably achieves finite-sample coverage guarantees without making any assumptions about the real and synthetic data distributions. When the score distributions are well aligned, SPPI yields substantially tighter and more informative prediction sets than standard conformal prediction. Experiments on image classification and tabular regression demonstrate notable improvements in predictive efficiency in data-scarce settings.</li>
</ul>

<h3>Title: SMOTExT: SMOTE meets Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Bystroski, Mikoaj Hoysz, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13434">https://arxiv.org/abs/2505.13434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13434">https://arxiv.org/pdf/2505.13434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13434]] SMOTExT: SMOTE meets Large Language Models(https://arxiv.org/abs/2505.13434)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>Data scarcity and class imbalance are persistent challenges in training robust NLP models, especially in specialized domains or low-resource settings. We propose a novel technique, SMOTExT, that adapts the idea of Synthetic Minority Over-sampling (SMOTE) to textual data. Our method generates new synthetic examples by interpolating between BERT-based embeddings of two existing examples and then decoding the resulting latent point into text with xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation framework, we can effectively turn interpolated vectors into coherent text. While this is preliminary work supported by qualitative outputs only, the method shows strong potential for knowledge distillation and data augmentation in few-shot settings. Notably, our approach also shows promise for privacy-preserving machine learning: in early experiments, training models solely on generated data achieved comparable performance to models trained on the original dataset. This suggests a viable path toward safe and effective learning under data protection constraints.</li>
</ul>

<h3>Title: FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</h3>
<ul>
<li><strong>Authors: </strong>Dian Shao, Mingfei Shi, Shengda Xu, Haodong Chen, Yongle Huang, Binglu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13437">https://arxiv.org/abs/2505.13437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13437">https://arxiv.org/pdf/2505.13437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13437]] FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance(https://arxiv.org/abs/2505.13437)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as "switch leap with 0.5 turn" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys's ability to generate more natural and plausible fine-grained human actions.</li>
</ul>

<h3>Title: Optimizing Anytime Reasoning via Budget Relative Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13438">https://arxiv.org/abs/2505.13438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13438">https://arxiv.org/pdf/2505.13438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13438]] Optimizing Anytime Reasoning via Budget Relative Policy Optimization(https://arxiv.org/abs/2505.13438)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.</li>
</ul>

<h3>Title: ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13444">https://arxiv.org/abs/2505.13444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13444">https://arxiv.org/pdf/2505.13444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13444]] ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models(https://arxiv.org/abs/2505.13444)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.</li>
</ul>

<h3>Title: Mean Flows for One-step Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.13447">https://arxiv.org/abs/2505.13447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.13447">https://arxiv.org/pdf/2505.13447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.13447]] Mean Flows for One-step Generative Modeling(https://arxiv.org/abs/2505.13447)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the MeanFlow model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256x256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
