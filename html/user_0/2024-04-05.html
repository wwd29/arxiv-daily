<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-05</h1>
<h3>Title: An Unsupervised Adversarial Autoencoder for Cyber Attack Detection in  Power Distribution Grids</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Jabbari Zideh, Mohammad Reza Khalghani, Sarika Khushalani Solanki</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02923">https://arxiv.org/abs/2404.02923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02923">https://arxiv.org/pdf/2404.02923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02923]] An Unsupervised Adversarial Autoencoder for Cyber Attack Detection in  Power Distribution Grids(https://arxiv.org/abs/2404.02923)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, generative</a></li>
<li><strong>Abstract: </strong>Detection of cyber attacks in smart power distribution grids with unbalanced configurations poses challenges due to the inherent nonlinear nature of these uncertain and stochastic systems. It originates from the intermittent characteristics of the distributed energy resources (DERs) generation and load variations. Moreover, the unknown behavior of cyber attacks, especially false data injection attacks (FDIAs) in the distribution grids with complex temporal correlations and the limited amount of labeled data increases the vulnerability of the grids and imposes a high risk in the secure and reliable operation of the grids. To address these challenges, this paper proposes an unsupervised adversarial autoencoder (AAE) model to detect FDIAs in unbalanced power distribution grids integrated with DERs, i.e., PV systems and wind generation. The proposed method utilizes long short-term memory (LSTM) in the structure of the autoencoder to capture the temporal dependencies in the time-series measurements and leverages the power of generative adversarial networks (GANs) for better reconstruction of the input data. The advantage of the proposed data-driven model is that it can detect anomalous points for the system operation without reliance on abstract models or mathematical representations. To evaluate the efficacy of the approach, it is tested on IEEE 13-bus and 123-bus systems with historical meteorological data (wind speed, ambient temperature, and solar irradiance) as well as historical real-world load data under three types of data falsification functions. The comparison of the detection results of the proposed model with other unsupervised learning methods verifies its superior performance in detecting cyber attacks in unbalanced power distribution grids.</li>
</ul>

<h3>Title: Jailbreaking Prompt Attack: A Controllable Adversarial Attack against  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Ma, Anda Cao, Zhiqing Xiao, Jie Zhang, Chao Ye, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02928">https://arxiv.org/abs/2404.02928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02928">https://arxiv.org/pdf/2404.02928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02928]] Jailbreaking Prompt Attack: A Controllable Adversarial Attack against  Diffusion Models(https://arxiv.org/abs/2404.02928)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, diffusion</a></li>
<li><strong>Abstract: </strong>The fast advance of the image generation community has attracted attention worldwide. The safety issue needs to be further scrutinized and studied. There have been a few works around this area mostly achieving a post-processing design, model-specific, or yielding suboptimal image quality generation. Despite that, in this article, we discover a black-box attack method that enjoys three merits. It enables (i)-attacks both directed and semantic-driven that theoretically and practically pose a hazard to this vast user community, (ii)-surprisingly surpasses the white-box attack in a black-box manner and (iii)-without requiring any post-processing effort. Core to our approach is inspired by the concept guidance intriguing property of Classifier-Free guidance (CFG) in T2I models, and we discover that conducting frustratingly simple guidance in the CLIP embedding space, coupled with the semantic loss and an additionally sensitive word list works very well. Moreover, our results expose and highlight the vulnerabilities in existing defense mechanisms.</li>
</ul>

<h3>Title: Using Large Language Models to Understand Telecom Standards</h3>
<ul>
<li><strong>Authors: </strong>Athanasios Karapantelakis, Mukesh Shakur, Alexandros Nikou, Farnaz Moradi, Christian Orlog, Fitsum Gaim, Henrik Holm, Doumitrou Daniil Nimara, Vincent Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02929">https://arxiv.org/abs/2404.02929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02929">https://arxiv.org/pdf/2404.02929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02929]] Using Large Language Models to Understand Telecom Standards(https://arxiv.org/abs/2404.02929)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The Third Generation Partnership Project (3GPP) has successfully introduced standards for global mobility. However, the volume and complexity of these standards has increased over time, thus complicating access to relevant information for vendors and service providers. Use of Generative Artificial Intelligence (AI) and in particular Large Language Models (LLMs), may provide faster access to relevant information. In this paper, we evaluate the capability of state-of-art LLMs to be used as Question Answering (QA) assistants for 3GPP document reference. Our contribution is threefold. First, we provide a benchmark and measuring methods for evaluating performance of LLMs. Second, we do data preprocessing and fine-tuning for one of these LLMs and provide guidelines to increase accuracy of the responses that apply to all LLMs. Third, we provide a model of our own, TeleRoBERTa, that performs on-par with foundation LLMs but with an order of magnitude less number of parameters. Results show that LLMs can be used as a credible reference tool on telecom technical documents, and thus have potential for a number of different applications from troubleshooting and maintenance, to network operations and software product development.</li>
</ul>

<h3>Title: READ: Improving Relation Extraction from an ADversarial Perspective</h3>
<ul>
<li><strong>Authors: </strong>Dawei Li, William Hogan, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02931">https://arxiv.org/abs/2404.02931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02931">https://arxiv.org/pdf/2404.02931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02931]] READ: Improving Relation Extraction from an ADversarial Perspective(https://arxiv.org/abs/2404.02931)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction</a></li>
<li><strong>Abstract: </strong>Recent works in relation extraction (RE) have achieved promising benchmark accuracy; however, our adversarial attack experiments show that these works excessively rely on entities, making their generalization capability questionable. To address this issue, we propose an adversarial training method specifically designed for RE. Our approach introduces both sequence- and token-level perturbations to the sample and uses a separate perturbation vocabulary to improve the search for entity and context perturbations. Furthermore, we introduce a probabilistic strategy for leaving clean tokens in the context during adversarial training. This strategy enables a larger attack budget for entities and coaxes the model to leverage relational patterns embedded in the context. Extensive experiments show that compared to various adversarial training methods, our method significantly improves both the accuracy and robustness of the model. Additionally, experiments on different data availability settings highlight the effectiveness of our method in low-resource scenarios. We also perform in-depth analyses of our proposed method and provide further hints. We will release our code at https://github.com/David-Li0406/READ.</li>
</ul>

<h3>Title: GreedLlama: Performance of Financial Value-Aligned Large Language Models  in Moral Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jeffy Yu, Maximilian Huber, Kevin Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02934">https://arxiv.org/abs/2404.02934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02934">https://arxiv.org/pdf/2404.02934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02934]] GreedLlama: Performance of Financial Value-Aligned Large Language Models  in Moral Reasoning(https://arxiv.org/abs/2404.02934)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the ethical implications of aligning Large Language Models (LLMs) with financial optimization, through the case study of GreedLlama, a model fine-tuned to prioritize economically beneficial outcomes. By comparing GreedLlama's performance in moral reasoning tasks to a base Llama2 model, our results highlight a concerning trend: GreedLlama demonstrates a marked preference for profit over ethical considerations, making morally appropriate decisions at significantly lower rates than the base model in scenarios of both low and high moral ambiguity. In low ambiguity situations, GreedLlama's ethical decisions decreased to 54.4%, compared to the base model's 86.9%, while in high ambiguity contexts, the rate was 47.4% against the base model's 65.1%. These findings emphasize the risks of single-dimensional value alignment in LLMs, underscoring the need for integrating broader ethical values into AI development to ensure decisions are not solely driven by financial incentives. The study calls for a balanced approach to LLM deployment, advocating for the incorporation of ethical considerations in models intended for business applications, particularly in light of the absence of regulatory oversight.</li>
</ul>

<h3>Title: KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual  Checking</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Zhang, Chejian Xu, Yu Gai, Freddy Lecue, Dawn Song, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02935">https://arxiv.org/abs/2404.02935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02935">https://arxiv.org/pdf/2404.02935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02935]] KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual  Checking(https://arxiv.org/abs/2404.02935)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces KnowHalu, a novel approach for detecting hallucinations in text generated by large language models (LLMs), utilizing step-wise reasoning, multi-formulation query, multi-form knowledge for factual checking, and fusion-based detection mechanism. As LLMs are increasingly applied across various domains, ensuring that their outputs are not hallucinated is critical. Recognizing the limitations of existing approaches that either rely on the self-consistency check of LLMs or perform post-hoc fact-checking without considering the complexity of queries or the form of knowledge, KnowHalu proposes a two-phase process for hallucination detection. In the first phase, it identifies non-fabrication hallucinations--responses that, while factually correct, are irrelevant or non-specific to the query. The second phase, multi-form based factual checking, contains five key steps: reasoning and query decomposition, knowledge retrieval, knowledge optimization, judgment generation, and judgment aggregation. Our extensive evaluations demonstrate that KnowHalu significantly outperforms SOTA baselines in detecting hallucinations across diverse tasks, e.g., improving by 15.65% in QA tasks and 5.50% in summarization tasks, highlighting its effectiveness and versatility in detecting hallucinations in LLM-generated content.</li>
</ul>

<h3>Title: Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Yang, Hai Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02936">https://arxiv.org/abs/2404.02936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02936">https://arxiv.org/pdf/2404.02936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02936]] Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large  Language Models(https://arxiv.org/abs/2404.02936)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. The current state-of-the-art approach, Min-K%, measures the raw token probability which we argue may not be the most informative signal. Instead, we propose Min-K%++ to normalize the token probability with statistics of the categorical distribution over the whole vocabulary, which accurately reflects the relative likelihood of the target token compared with other candidate tokens in the vocabulary. Theoretically, we back up our method by showing that the statistic it estimates is explicitly optimized during LLM training, thus serving as a reliable indicator for detecting training data. Empirically, on the WikiMIA benchmark, Min-K%++ outperforms the SOTA Min-K% by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, Min-K%++ consistently improves upon Min-K% and performs on par with reference-based method, despite not requiring an extra reference model.</li>
</ul>

<h3>Title: Explainable Traffic Flow Prediction with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhua, Hao (Frank)Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02937">https://arxiv.org/abs/2404.02937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02937">https://arxiv.org/pdf/2404.02937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02937]] Explainable Traffic Flow Prediction with Large Language Models(https://arxiv.org/abs/2404.02937)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Traffic flow prediction provides essential future views in the intelligent transportation system. Explainable predictions offer valuable insights into the factors influencing traffic patterns, which help urban planners, traffic engineers, and policymakers make informed decisions about infrastructure development, traffic management strategies, and public transportation planning. Despite their widespread popularity and commendable accuracy, prediction methods grounded in deep learning frequently disappoint in terms of transparency and interpretability. Recently, the availability of large-scale spatio-temporal data and the development of large language models (LLMs) have opened up new opportunities for urban traffic prediction. With the popularity of LLMs, people witnessed the potential reasoning and generating ability of foundation models in various tasks. Considering text as input and output, LLMs have advantages in generating more intuitive and interpretable predictions. Hence, this work introduces TP-LLM, an explainable foundation-model-based method for traffic prediction, aiming at more direct and reasonable forecasting. TP-LLM presents a framework to unify multi-modality factors as language-based inputs, TP-LLM avoids complex spatial-temporal data programming and outperforms state-of-art baselines merely under fine-tuning foundation models. Also, TP-LLM can generate input-dependency explanations for more confident prediction and can be easily generalized to different city dynamics for zero-shot prediction with a similar framework. These findings demonstrate the potential of LLMs for explainable traffic prediction.</li>
</ul>

<h3>Title: Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Arrighi, Luca Pennella, Gabriel Marques Tavares, Sylvio Barbon Junior</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02942">https://arxiv.org/abs/2404.02942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02942">https://arxiv.org/pdf/2404.02942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02942]] Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles(https://arxiv.org/abs/2404.02942)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Understanding the decisions of tree-based ensembles and their relationships is pivotal for machine learning model interpretation. Recent attempts to mitigate the human-in-the-loop interpretation challenge have explored the extraction of the decision structure underlying the model taking advantage of graph simplification and path emphasis. However, while these efforts enhance the visualisation experience, they may either result in a visually complex representation or compromise the interpretability of the original ensemble model. In addressing this challenge, especially in complex scenarios, we introduce the Decision Predicate Graph (DPG) as a model-agnostic tool to provide a global interpretation of the model. DPG is a graph structure that captures the tree-based ensemble model and learned dataset details, preserving the relations among features, logical decisions, and predictions towards emphasising insightful points. Leveraging well-known graph theory concepts, such as the notions of centrality and community, DPG offers additional quantitative insights into the model, complementing visualisation techniques, expanding the problem space descriptions, and offering diverse possibilities for extensions. Empirical experiments demonstrate the potential of DPG in addressing traditional benchmarks and complex classification scenarios.</li>
</ul>

<h3>Title: Foundation Models for Structural Health Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Luca Benfenati, Daniele Jahier Pagliari, Luca Zanatta, Yhorman Alexander Bedoya Velez, Andrea Acquaviva, Massimo Poncino, Enrico Macii, Luca Benini, Alessio Burrello</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02944">https://arxiv.org/abs/2404.02944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02944">https://arxiv.org/pdf/2404.02944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02944]] Foundation Models for Structural Health Monitoring(https://arxiv.org/abs/2404.02944)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Structural Health Monitoring (SHM) is a critical task for ensuring the safety and reliability of civil infrastructures, typically realized on bridges and viaducts by means of vibration monitoring. In this paper, we propose for the first time the use of Transformer neural networks, with a Masked Auto-Encoder architecture, as Foundation Models for SHM. We demonstrate the ability of these models to learn generalizable representations from multiple large datasets through self-supervised pre-training, which, coupled with task-specific fine-tuning, allows them to outperform state-of-the-art traditional methods on diverse tasks, including Anomaly Detection (AD) and Traffic Load Estimation (TLE). We then extensively explore model size versus accuracy trade-offs and experiment with Knowledge Distillation (KD) to improve the performance of smaller Transformers, enabling their embedding directly into the SHM edge nodes. We showcase the effectiveness of our foundation models using data from three operational viaducts. For AD, we achieve a near-perfect 99.9% accuracy with a monitoring time span of just 15 windows. In contrast, a state-of-the-art method based on Principal Component Analysis (PCA) obtains its first good result (95.03% accuracy) only considering 120 windows. On two different TLE tasks, our models obtain state-of-the-art performance on multiple evaluation metrics (R$^2$ score, MAE% and MSE%). On the first benchmark, we achieve an R$^2$ score of 0.97 and 0.85 for light and heavy vehicle traffic, respectively, while the best previous approach stops at 0.91 and 0.84. On the second one, we achieve an R$^2$ score of 0.54 versus the 0.10 of the best existing method.</li>
</ul>

<h3>Title: Optimizing the Deployment of Tiny Transformers on Low-Power MCUs</h3>
<ul>
<li><strong>Authors: </strong>Victor J.B. Jung, Alessio Burrello, Moritz Scherer, Francesco Conti, Luca Benini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02945">https://arxiv.org/abs/2404.02945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02945">https://arxiv.org/pdf/2404.02945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02945]] Optimizing the Deployment of Tiny Transformers on Low-Power MCUs(https://arxiv.org/abs/2404.02945)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer networks are rapidly becoming SotA in many fields, such as NLP and CV. Similarly to CNN, there is a strong push for deploying Transformer models at the extreme edge, ultimately fitting the tiny power budget and memory footprint of MCUs. However, the early approaches in this direction are mostly ad-hoc, platform, and model-specific. This work aims to enable and optimize the flexible, multi-platform deployment of encoder Tiny Transformers on commercial MCUs. We propose a complete framework to perform end-to-end deployment of Transformer models onto single and multi-core MCUs. Our framework provides an optimized library of kernels to maximize data reuse and avoid unnecessary data marshaling operations into the crucial attention block. A novel MHSA inference schedule, named Fused-Weight Self-Attention, is introduced, fusing the linear projection weights offline to further reduce the number of operations and parameters. Furthermore, to mitigate the memory peak reached by the computation of the attention map, we present a Depth-First Tiling scheme for MHSA. We evaluate our framework on three different MCU classes exploiting ARM and RISC-V ISA, namely the STM32H7, the STM32L4, and GAP9 (RV32IMC-XpulpV2). We reach an average of 4.79x and 2.0x lower latency compared to SotA libraries CMSIS-NN (ARM) and PULP-NN (RISC-V), respectively. Moreover, we show that our MHSA depth-first tiling scheme reduces the memory peak by up to 6.19x, while the fused-weight attention can reduce the runtime by 1.53x, and number of parameters by 25%. We report significant improvements across several Tiny Transformers: for instance, when executing a transformer block for the task of radar-based hand-gesture recognition on GAP9, we achieve a latency of 0.14ms and energy consumption of 4.92 micro-joules, 2.32x lower than the SotA PULP-NN library on the same platform.</li>
</ul>

<h3>Title: DNN Memory Footprint Reduction via Post-Training Intra-Layer  Multi-Precision Quantization</h3>
<ul>
<li><strong>Authors: </strong>Behnam Ghavami, Amin Kamjoo, Lesley Shannon, Steve Wilton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02947">https://arxiv.org/abs/2404.02947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02947">https://arxiv.org/pdf/2404.02947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02947]] DNN Memory Footprint Reduction via Post-Training Intra-Layer  Multi-Precision Quantization(https://arxiv.org/abs/2404.02947)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The imperative to deploy Deep Neural Network (DNN) models on resource-constrained edge devices, spurred by privacy concerns, has become increasingly apparent. To facilitate the transition from cloud to edge computing, this paper introduces a technique that effectively reduces the memory footprint of DNNs, accommodating the limitations of resource-constrained edge devices while preserving model accuracy. Our proposed technique, named Post-Training Intra-Layer Multi-Precision Quantization (PTILMPQ), employs a post-training quantization approach, eliminating the need for extensive training data. By estimating the importance of layers and channels within the network, the proposed method enables precise bit allocation throughout the quantization process. Experimental results demonstrate that PTILMPQ offers a promising solution for deploying DNNs on edge devices with restricted memory resources. For instance, in the case of ResNet50, it achieves an accuracy of 74.57\% with a memory footprint of 9.5 MB, representing a 25.49\% reduction compared to previous similar methods, with only a minor 1.08\% decrease in accuracy.</li>
</ul>

<h3>Title: PiSSA: Principal Singular Values and Singular Vectors Adaptation of  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fanxu Meng, Zhaohui Wang, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02948">https://arxiv.org/abs/2404.02948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02948">https://arxiv.org/pdf/2404.02948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02948]] PiSSA: Principal Singular Values and Singular Vectors Adaptation of  Large Language Models(https://arxiv.org/abs/2404.02948)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the parameters of LLMs expand, the computational cost of fine-tuning the entire model becomes prohibitive. To address this challenge, we introduce a PEFT method, Principal Singular values and Singular vectors Adaptation (PiSSA), which optimizes a significantly reduced parameter space while achieving or surpassing the performance of full-parameter fine-tuning. PiSSA is inspired by Intrinsic SAID, which suggests that pre-trained, over-parametrized models inhabit a space of low intrinsic dimension. Consequently, PiSSA represents a matrix W within the model by the product of two trainable matrices A and B, plus a residual matrix $W^{res}$ for error correction. SVD is employed to factorize W, and the principal singular values and vectors of W are utilized to initialize A and B. The residual singular values and vectors initialize the residual matrix $W^{res}$, which keeps frozen during fine-tuning. Notably, PiSSA shares the same architecture with LoRA. However, LoRA approximates Delta W through the product of two matrices, A, initialized with Gaussian noise, and B, initialized with zeros, while PiSSA initializes A and B with principal singular values and vectors of the original matrix W. PiSSA can better approximate the outcomes of full-parameter fine-tuning at the beginning by changing the essential parts while freezing the "noisy" parts. In comparison, LoRA freezes the original matrix and updates the "noise". This distinction enables PiSSA to convergence much faster than LoRA and also achieve better performance in the end. Due to the same architecture, PiSSA inherits many of LoRA's advantages, such as parameter efficiency and compatibility with quantization. Leveraging a fast SVD method, the initialization of PiSSA takes only a few seconds, inducing negligible cost of switching LoRA to PiSSA.</li>
</ul>

<h3>Title: The SaTML '24 CNN Interpretability Competition: New Innovations for  Concept-Level Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Stephen Casper, Jieun Yun, Joonhyuk Baek, Yeseong Jung, Minhwan Kim, Kiwan Kwon, Saerom Park, Hayden Moore, David Shriver, Marissa Connor, Keltin Grimes, Angus Nicolson, Arush Tagade, Jessica Rumbelow, Hieu Minh Nguyen, Dylan Hadfield-Menell</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02949">https://arxiv.org/abs/2404.02949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02949">https://arxiv.org/pdf/2404.02949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02949]] The SaTML '24 CNN Interpretability Competition: New Innovations for  Concept-Level Interpretability(https://arxiv.org/abs/2404.02949)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Interpretability techniques are valuable for helping humans understand and oversee AI systems. The SaTML 2024 CNN Interpretability Competition solicited novel methods for studying convolutional neural networks (CNNs) at the ImageNet scale. The objective of the competition was to help human crowd-workers identify trojans in CNNs. This report showcases the methods and results of four featured competition entries. It remains challenging to help humans reliably diagnose trojans via interpretability tools. However, the competition's entries have contributed new techniques and set a new record on the benchmark from Casper et al., 2023.</li>
</ul>

<h3>Title: Deep Generative Models through the Lens of the Manifold Hypothesis: A  Survey and New Connections</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Loaiza-Ganem, Brendan Leigh Ross, Rasa Hosseinzadeh, Anthony L. Caterini, Jesse C. Cresswell</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02954">https://arxiv.org/abs/2404.02954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02954">https://arxiv.org/pdf/2404.02954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02954]] Deep Generative Models through the Lens of the Manifold Hypothesis: A  Survey and New Connections(https://arxiv.org/abs/2404.02954)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years there has been increased interest in understanding the interplay between deep generative models (DGMs) and the manifold hypothesis. Research in this area focuses on understanding the reasons why commonly-used DGMs succeed or fail at learning distributions supported on unknown low-dimensional manifolds, as well as developing new models explicitly designed to account for manifold-supported data. This manifold lens provides both clarity as to why some DGMs (e.g. diffusion models and some generative adversarial networks) empirically surpass others (e.g. likelihood-based models such as variational autoencoders, normalizing flows, or energy-based models) at sample generation, and guidance for devising more performant DGMs. We carry out the first survey of DGMs viewed through this lens, making two novel contributions along the way. First, we formally establish that numerical instability of high-dimensional likelihoods is unavoidable when modelling low-dimensional data. We then show that DGMs on learned representations of autoencoders can be interpreted as approximately minimizing Wasserstein distance: this result, which applies to latent diffusion models, helps justify their outstanding empirical results. The manifold lens provides a rich perspective from which to understand DGMs, which we aim to make more accessible and widespread.</li>
</ul>

<h3>Title: Towards a Fully Interpretable and More Scalable RSA Model for Metaphor  Understanding</h3>
<ul>
<li><strong>Authors: </strong>Gaia Carenini, Luca Bischetti, Walter Schaeken, Valentina Bambini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02983">https://arxiv.org/abs/2404.02983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02983">https://arxiv.org/pdf/2404.02983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02983]] Towards a Fully Interpretable and More Scalable RSA Model for Metaphor  Understanding(https://arxiv.org/abs/2404.02983)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The Rational Speech Act (RSA) model provides a flexible framework to model pragmatic reasoning in computational terms. However, state-of-the-art RSA models are still fairly distant from modern machine learning techniques and present a number of limitations related to their interpretability and scalability. Here, we introduce a new RSA framework for metaphor understanding that addresses these limitations by providing an explicit formula - based on the mutually shared information between the speaker and the listener - for the estimation of the communicative goal and by learning the rationality parameter using gradient-based methods. The model was tested against 24 metaphors, not limited to the conventional $\textit{John-is-a-shark}$ type. Results suggest an overall strong positive correlation between the distributions generated by the model and the interpretations obtained from the human behavioral data, which increased when the intended meaning capitalized on properties that were inherent to the vehicle concept. Overall, findings suggest that metaphor processing is well captured by a typicality-based Bayesian model, even when more scalable and interpretable, opening up possible applications to other pragmatic phenomena and novel uses for increasing Large Language Models interpretability. Yet, results highlight that the more creative nuances of metaphorical meaning, not strictly encoded in the lexical concepts, are a challenging aspect for machines.</li>
</ul>

<h3>Title: Universal Functional Regression with Neural Operator Flows</h3>
<ul>
<li><strong>Authors: </strong>Yaozhong Shi, Angela F. Gao, Zachary E. Ross, Kamyar Azizzadenesheli</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02986">https://arxiv.org/abs/2404.02986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02986">https://arxiv.org/pdf/2404.02986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02986]] Universal Functional Regression with Neural Operator Flows(https://arxiv.org/abs/2404.02986)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Regression on function spaces is typically limited to models with Gaussian process priors. We introduce the notion of universal functional regression, in which we aim to learn a prior distribution over non-Gaussian function spaces that remains mathematically tractable for functional regression. To do this, we develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of normalizing flows. OpFlow is an invertible operator that maps the (potentially unknown) data function space into a Gaussian process, allowing for exact likelihood estimation of functional point evaluations. OpFlow enables robust and accurate uncertainty quantification via drawing posterior samples of the Gaussian process and subsequently mapping them into the data function space. We empirically study the performance of OpFlow on regression and generation tasks with data generated from Gaussian processes with known posterior forms and non-Gaussian processes, as well as real-world earthquake seismograms with an unknown closed-form distribution.</li>
</ul>

<h3>Title: ASAP: Interpretable Analysis and Summarization of AI-generated Image  Patterns at Scale</h3>
<ul>
<li><strong>Authors: </strong>Jinbin Huang, Chen Chen, Aditi Mishra, Bum Chul Kwon, Zhicheng Liu, Chris Bryan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.02990">https://arxiv.org/abs/2404.02990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.02990">https://arxiv.org/pdf/2404.02990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.02990]] ASAP: Interpretable Analysis and Summarization of AI-generated Image  Patterns at Scale(https://arxiv.org/abs/2404.02990)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Generative image models have emerged as a promising technology to produce realistic images. Despite potential benefits, concerns grow about its misuse, particularly in generating deceptive images that could raise significant ethical, legal, and societal issues. Consequently, there is growing demand to empower users to effectively discern and comprehend patterns of AI-generated images. To this end, we developed ASAP, an interactive visualization system that automatically extracts distinct patterns of AI-generated images and allows users to interactively explore them via various views. To uncover fake patterns, ASAP introduces a novel image encoder, adapted from CLIP, which transforms images into compact "distilled" representations, enriched with information for differentiating authentic and fake images. These representations generate gradients that propagate back to the attention maps of CLIP's transformer block. This process quantifies the relative importance of each pixel to image authenticity or fakeness, exposing key deceptive patterns. ASAP enables the at scale interactive analysis of these patterns through multiple, coordinated visualizations. This includes a representation overview with innovative cell glyphs to aid in the exploration and qualitative evaluation of fake patterns across a vast array of images, as well as a pattern view that displays authenticity-indicating patterns in images and quantifies their impact. ASAP supports the analysis of cutting-edge generative models with the latest architectures, including GAN-based models like proGAN and diffusion models like the latent diffusion model. We demonstrate ASAP's usefulness through two usage scenarios using multiple fake image detection benchmark datasets, revealing its ability to identify and understand hidden patterns in AI-generated images, especially in detecting fake human faces produced by diffusion-based techniques.</li>
</ul>

<h3>Title: Spectral Clustering in Convex and Constrained Settings</h3>
<ul>
<li><strong>Authors: </strong>Swarup Ranjan Behera, Vijaya V. Saradhi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03012">https://arxiv.org/abs/2404.03012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03012">https://arxiv.org/pdf/2404.03012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03012]] Spectral Clustering in Convex and Constrained Settings(https://arxiv.org/abs/2404.03012)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spectral clustering methods have gained widespread recognition for their effectiveness in clustering high-dimensional data. Among these techniques, constrained spectral clustering has emerged as a prominent approach, demonstrating enhanced performance by integrating pairwise constraints. However, the application of such constraints to semidefinite spectral clustering, a variant that leverages semidefinite programming to optimize clustering objectives, remains largely unexplored. In this paper, we introduce a novel framework for seamlessly integrating pairwise constraints into semidefinite spectral clustering. Our methodology systematically extends the capabilities of semidefinite spectral clustering to capture complex data structures, thereby addressing real-world clustering challenges more effectively. Additionally, we extend this framework to encompass both active and self-taught learning scenarios, further enhancing its versatility and applicability. Empirical studies conducted on well-known datasets demonstrate the superiority of our proposed framework over existing spectral clustering methods, showcasing its robustness and scalability across diverse datasets and learning settings. By bridging the gap between constrained learning and semidefinite spectral clustering, our work contributes to the advancement of spectral clustering techniques, offering researchers and practitioners a versatile tool for addressing complex clustering challenges in various real-world applications. Access to the data, code, and experimental results is provided for further exploration (https://github.com/swarupbehera/SCCCS).</li>
</ul>

<h3>Title: DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object  Detection</h3>
<ul>
<li><strong>Authors: </strong>Felix Fent, Andras Palffy, Holger Caesar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03015">https://arxiv.org/abs/2404.03015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03015">https://arxiv.org/pdf/2404.03015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03015]] DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object  Detection(https://arxiv.org/abs/2404.03015)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The perception of autonomous vehicles has to be efficient, robust, and cost-effective. However, cameras are not robust against severe weather conditions, lidar sensors are expensive, and the performance of radar-based perception is still inferior to the others. Camera-radar fusion methods have been proposed to address this issue, but these are constrained by the typical sparsity of radar point clouds and often designed for radars without elevation information. We propose a novel camera-radar fusion approach called Dual Perspective Fusion Transformer (DPFT), designed to overcome these limitations. Our method leverages lower-level radar data (the radar cube) instead of the processed point clouds to preserve as much information as possible and employs projections in both the camera and ground planes to effectively use radars with elevation information and simplify the fusion with camera data. As a result, DPFT has demonstrated state-of-the-art performance on the K-Radar dataset while showing remarkable robustness against adverse weather conditions and maintaining a low inference time. The code is made available as open-source software under https://github.com/TUMFTM/DPFT.</li>
</ul>

<h3>Title: Blessing or curse? A survey on the Impact of Generative AI on Fake News</h3>
<ul>
<li><strong>Authors: </strong>Alexander Loth, Martin Kappes, Marc-Oliver Pahl</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03021">https://arxiv.org/abs/2404.03021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03021">https://arxiv.org/pdf/2404.03021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03021]] Blessing or curse? A survey on the Impact of Generative AI on Fake News(https://arxiv.org/abs/2404.03021)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fake news significantly influence our society. They impact consumers, voters, and many other societal groups. While Fake News exist for a centuries, Generative AI brings fake news on a new level. It is now possible to automate the creation of masses of high-quality individually targeted Fake News. On the other end, Generative AI can also help detecting Fake News. Both fields are young but developing fast. This survey provides a comprehensive examination of the research and practical use of Generative AI for Fake News detection and creation in 2024. Following the Structured Literature Survey approach, the paper synthesizes current results in the following topic clusters 1) enabling technologies, 2) creation of Fake News, 3) case study social media as most relevant distribution channel, 4) detection of Fake News, and 5) deepfakes as upcoming technology. The article also identifies current challenges and open issues.</li>
</ul>

<h3>Title: JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal  Large Language Models against Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03027">https://arxiv.org/abs/2404.03027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03027">https://arxiv.org/pdf/2404.03027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03027]] JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal  Large Language Models against Jailbreak Attacks(https://arxiv.org/abs/2404.03027)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancements in Multimodal Large Language Models (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking MLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering benchmark designed to assess the transferability of LLM jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak prompts using advanced jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 open-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks transferred from LLMs, highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs.</li>
</ul>

<h3>Title: An Incomplete Loop: Deductive, Inductive, and Abductive Learning in  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Emmy Liu, Graham Neubig, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03028">https://arxiv.org/abs/2404.03028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03028">https://arxiv.org/pdf/2404.03028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03028]] An Incomplete Loop: Deductive, Inductive, and Abductive Learning in  Large Language Models(https://arxiv.org/abs/2404.03028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern language models (LMs) can learn to perform new tasks in different ways: in instruction following, the target task is described explicitly in natural language; in few-shot prompting, the task is specified implicitly with a small number of examples; in instruction inference, LMs are presented with in-context examples and are then prompted to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of reasoning: instruction following involves deductive reasoning, few-shot prompting involves inductive reasoning, and instruction inference involves abductive reasoning. How do these different capabilities relate? Across four LMs (from the gpt and llama families) and two learning problems (involving arithmetic functions and machine translation) we find a strong dissociation between the different types of reasoning: LMs can sometimes learn effectively from few-shot prompts even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of reasoning even in some of today's largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar prompting procedures.</li>
</ul>

<h3>Title: MuLan: A Study of Fact Mutability in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Constanza Fierro, Nicolas Garneau, Emanuele Bugliarello, Yova Kementchedjhieva, Anders Søgaard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03036">https://arxiv.org/abs/2404.03036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03036">https://arxiv.org/pdf/2404.03036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03036]] MuLan: A Study of Fact Mutability in Language Models(https://arxiv.org/abs/2404.03036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Facts are subject to contingencies and can be true or false in different circumstances. One such contingency is time, wherein some facts mutate over a given period, e.g., the president of a country or the winner of a championship. Trustworthy language models ideally identify mutable facts as such and process them accordingly. We create MuLan, a benchmark for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. We hypothesize that mutable facts are encoded differently than immutable ones, hence being easier to update. In a detailed evaluation of six popular large language models, we consistently find differences in the LLMs' confidence, representations, and update behavior, depending on the mutability of a fact. Our findings should inform future work on the injection of and induction of time-contingent knowledge to/from LLMs.</li>
</ul>

<h3>Title: The Artificial Intelligence Ontology: LLM-assisted construction of AI  concept hierarchies</h3>
<ul>
<li><strong>Authors: </strong>Marcin P. Joachimiak, Mark A. Miller, J. Harry Caufield, Ryan Ly, Nomi L. Harris, Andrew Tritt, Christopher J. Mungall, Kristofer E. Bouchard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03044">https://arxiv.org/abs/2404.03044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03044">https://arxiv.org/pdf/2404.03044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03044]] The Artificial Intelligence Ontology: LLM-assisted construction of AI  concept hierarchies(https://arxiv.org/abs/2404.03044)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Artificial Intelligence Ontology (AIO) is a systematization of artificial intelligence (AI) concepts, methodologies, and their interrelations. Developed via manual curation, with the additional assistance of large language models (LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a comprehensive framework that encompasses both technical and ethical aspects of AI technologies. The primary audience for AIO includes AI researchers, developers, and educators seeking standardized terminology and concepts within the AI domain. The ontology is structured around six top-level branches: Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to support the modular composition of AI methods and facilitate a deeper understanding of deep learning architectures and ethical considerations in AI. AIO's development utilized the Ontology Development Kit (ODK) for its creation and maintenance, with its content being dynamically updated through AI-driven curation support. This approach not only ensures the ontology's relevance amidst the fast-paced advancements in AI but also significantly enhances its utility for researchers, developers, and educators by simplifying the integration of new AI concepts and methodologies. The ontology's utility is demonstrated through the annotation of AI methods data in a catalog of AI research publications and the integration into the BioPortal ontology resource, highlighting its potential for cross-disciplinary research. The AIO ontology is open source and is available on GitHub (https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal (https://bioportal.bioontology.org/ontologies/AIO).</li>
</ul>

<h3>Title: ANOVA-boosting for Random Fourier Features</h3>
<ul>
<li><strong>Authors: </strong>Daniel Potts, Laura Weidensager</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03050">https://arxiv.org/abs/2404.03050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03050">https://arxiv.org/pdf/2404.03050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03050]] ANOVA-boosting for Random Fourier Features(https://arxiv.org/abs/2404.03050)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We propose two algorithms for boosting random Fourier feature models for approximating high-dimensional functions. These methods utilize the classical and generalized analysis of variance (ANOVA) decomposition to learn low-order functions, where there are few interactions between the variables. Our algorithms are able to find an index set of important input variables and variable interactions reliably. Furthermore, we generalize already existing random Fourier feature models to an ANOVA setting, where terms of different order can be used. Our algorithms have the advantage of interpretability, meaning that the influence of every input variable is known in the learned model, even for dependent input variables. We give theoretical as well as numerical results that our algorithms perform well for sensitivity analysis. The ANOVA-boosting step reduces the approximation error of existing methods significantly.</li>
</ul>

<h3>Title: Automatic Extraction of Linguistic Description from Fuzzy Rule Base</h3>
<ul>
<li><strong>Authors: </strong>Krzysztof Siminski, Konrad Wnuk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03058">https://arxiv.org/abs/2404.03058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03058">https://arxiv.org/pdf/2404.03058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03058]] Automatic Extraction of Linguistic Description from Fuzzy Rule Base(https://arxiv.org/abs/2404.03058)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Neuro-fuzzy systems are a technique of explainable artificial intelligence (XAI). They elaborate knowledge models as a set of fuzzy rules. Fuzzy sets are crucial components of fuzzy rules. They are used to model linguistic terms. In this paper, we present an automatic extraction of fuzzy rules in the natural English language. Full implementation is available free from a public repository.</li>
</ul>

<h3>Title: Construction of Functional Materials Knowledge Graph in  Multidisciplinary Materials Science via Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Tong Xie, Wenjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03080">https://arxiv.org/abs/2404.03080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03080">https://arxiv.org/pdf/2404.03080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03080]] Construction of Functional Materials Knowledge Graph in  Multidisciplinary Materials Science via Large Language Model(https://arxiv.org/abs/2404.03080)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The convergence of materials science and artificial intelligence has unlocked new opportunities for gathering, analyzing, and generating novel materials sourced from extensive scientific literature. Despite the potential benefits, persistent challenges such as manual annotation, precise extraction, and traceability issues remain. Large language models have emerged as promising solutions to address these obstacles. This paper introduces Functional Materials Knowledge Graph (FMKG), a multidisciplinary materials science knowledge graph. Through the utilization of advanced natural language processing techniques, extracting millions of entities to form triples from a corpus comprising all high-quality research papers published in the last decade. It organizes unstructured information into nine distinct labels, covering Name, Formula, Acronym, Structure/Phase, Properties, Descriptor, Synthesis, Characterization Method, Application, and Domain, seamlessly integrating papers' Digital Object Identifiers. As the latest structured database for functional materials, FMKG acts as a powerful catalyst for expediting the development of functional materials and a fundation for building a more comprehensive material knowledge graph using full paper text. Furthermore, our research lays the groundwork for practical text-mining-based knowledge management systems, not only in intricate materials systems but also applicable to other specialized domains.</li>
</ul>

<h3>Title: Robust Federated Learning for Wireless Networks: A Demonstration with  Channel Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zexin Fang, Bin Han, Hans D. Schotten</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03088">https://arxiv.org/abs/2404.03088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03088">https://arxiv.org/pdf/2404.03088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03088]] Robust Federated Learning for Wireless Networks: A Demonstration with  Channel Estimation(https://arxiv.org/abs/2404.03088)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) offers a privacy-preserving collaborative approach for training models in wireless networks, with channel estimation emerging as a promising application. Despite extensive studies on FL-empowered channel estimation, the security concerns associated with FL require meticulous attention. In a scenario where small base stations (SBSs) serve as local models trained on cached data, and a macro base station (MBS) functions as the global model setting, an attacker can exploit the vulnerability of FL, launching attacks with various adversarial attacks or deployment tactics. In this paper, we analyze such vulnerabilities, corresponding solutions were brought forth, and validated through simulation.</li>
</ul>

<h3>Title: SalFoM: Dynamic Saliency Prediction with Video Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Morteza Moradi, Mohammad Moradi, Francesco Rundo, Concetto Spampinato, Ali Borji, Simone Palazzo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03097">https://arxiv.org/abs/2404.03097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03097">https://arxiv.org/pdf/2404.03097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03097]] SalFoM: Dynamic Saliency Prediction with Video Foundation Models(https://arxiv.org/abs/2404.03097)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in video saliency prediction (VSP) have shown promising performance compared to the human visual system, whose emulation is the primary goal of VSP. However, current state-of-the-art models employ spatio-temporal transformers trained on limited amounts of data, hindering generalizability adaptation to downstream tasks. The benefits of vision foundation models present a potential solution to improve the VSP process. However, adapting image foundation models to the video domain presents significant challenges in modeling scene dynamics and capturing temporal information. To address these challenges, and as the first initiative to design a VSP model based on video foundation models, we introduce SalFoM, a novel encoder-decoder video transformer architecture. Our model employs UnMasked Teacher (UMT) as feature extractor and presents a heterogeneous decoder which features a locality-aware spatio-temporal transformer and integrates local and global spatio-temporal information from various perspectives to produce the final saliency map. Our qualitative and quantitative experiments on the challenging VSP benchmark datasets of DHF1K, Hollywood-2 and UCF-Sports demonstrate the superiority of our proposed model in comparison with the state-of-the-art methods.</li>
</ul>

<h3>Title: Exploring the Trade-off Between Model Performance and Explanation  Plausibility of Text Classifiers Using Human Rationales</h3>
<ul>
<li><strong>Authors: </strong>Lucas E. Resck, Marcos M. Raimundo, Jorge Poco</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03098">https://arxiv.org/abs/2404.03098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03098">https://arxiv.org/pdf/2404.03098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03098]] Exploring the Trade-off Between Model Performance and Explanation  Plausibility of Text Classifiers Using Human Rationales(https://arxiv.org/abs/2404.03098)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Saliency post-hoc explainability methods are important tools for understanding increasingly complex NLP models. While these methods can reflect the model's reasoning, they may not align with human intuition, making the explanations not plausible. In this work, we present a methodology for incorporating rationales, which are text annotations explaining human decisions, into text classification models. This incorporation enhances the plausibility of post-hoc explanations while preserving their faithfulness. Our approach is agnostic to model architectures and explainability methods. We introduce the rationales during model training by augmenting the standard cross-entropy loss with a novel loss function inspired by contrastive learning. By leveraging a multi-objective optimization algorithm, we explore the trade-off between the two loss functions and generate a Pareto-optimal frontier of models that balance performance and plausibility. Through extensive experiments involving diverse models, datasets, and explainability methods, we demonstrate that our approach significantly enhances the quality of model explanations without causing substantial (sometimes negligible) degradation in the original model's performance.</li>
</ul>

<h3>Title: Methodology for Interpretable Reinforcement Learning for Optimizing  Mechanical Ventilation</h3>
<ul>
<li><strong>Authors: </strong>Joo Seung Lee, Malini Mahendra, Anil Aswani</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03105">https://arxiv.org/abs/2404.03105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03105">https://arxiv.org/pdf/2404.03105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03105]] Methodology for Interpretable Reinforcement Learning for Optimizing  Mechanical Ventilation(https://arxiv.org/abs/2404.03105)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Mechanical ventilation is a critical life-support intervention that uses a machine to deliver controlled air and oxygen to a patient's lungs, assisting or replacing spontaneous breathing. While several data-driven approaches have been proposed to optimize ventilator control strategies, they often lack interpretability and agreement with general domain knowledge. This paper proposes a methodology for interpretable reinforcement learning (RL) using decision trees for mechanical ventilation control. Using a causal, nonparametric model-based off-policy evaluation, we evaluate the policies in their ability to gain increases in SpO2 while avoiding aggressive ventilator settings which are known to cause ventilator induced lung injuries and other complications. Numerical experiments using MIMIC-III data on the stays of real patients' intensive care unit stays demonstrate that the decision tree policy outperforms the behavior cloning policy and is comparable to state-of-the-art RL policy. Future work concerns better aligning the cost function with medical objectives to generate deeper clinical insights.</li>
</ul>

<h3>Title: Many-to-many Image Generation with Auto-regressive Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ying Shen, Yizhe Zhang, Shuangfei Zhai, Lifu Huang, Joshua M. Susskind, Jiatao Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03109">https://arxiv.org/abs/2404.03109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03109">https://arxiv.org/pdf/2404.03109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03109]] Many-to-many Image Generation with Auto-regressive Diffusion Models(https://arxiv.org/abs/2404.03109)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in image generation have made significant progress, yet existing models present limitations in perceiving and generating an arbitrary number of interrelated images within a broad context. This limitation becomes increasingly critical as the demand for multi-image scenarios, such as multi-view images and visual narratives, grows with the expansion of multimedia platforms. This paper introduces a domain-general framework for many-to-many image generation, capable of producing interrelated image series from a given set of images, offering a scalable solution that obviates the need for task-specific solutions across different multi-image scenarios. To facilitate this, we present MIS, a novel large-scale multi-image dataset, containing 12M synthetic multi-image samples, each with 25 interconnected images. Utilizing Stable Diffusion with varied latent noises, our method produces a set of interconnected images from a single caption. Leveraging MIS, we learn M2M, an autoregressive model for many-to-many generation, where each image is modeled within a diffusion framework. Throughout training on the synthetic MIS, the model excels in capturing style and content from preceding images - synthetic or real - and generates novel images following the captured patterns. Furthermore, through task-specific fine-tuning, our model demonstrates its adaptability to various multi-image generation tasks, including Novel View Synthesis and Visual Procedure Generation.</li>
</ul>

<h3>Title: Ego-Motion Aware Target Prediction Module for Robust Multi-Object  Tracking</h3>
<ul>
<li><strong>Authors: </strong>Navid Mahdian, Mohammad Jani, Amir M. Soufi Enayati, Homayoun Najjaran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03110">https://arxiv.org/abs/2404.03110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03110">https://arxiv.org/pdf/2404.03110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03110]] Ego-Motion Aware Target Prediction Module for Robust Multi-Object  Tracking(https://arxiv.org/abs/2404.03110)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories. Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target. Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model. These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections. Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories. In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models. Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter. This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model. We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT. In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively. At the same time, it elevates other performance metrics such as HOTA by more than 5%. Our source code is available at https://github.com/noyzzz/EMAP.</li>
</ul>

<h3>Title: LVLM-Intrepret: An Interpretability Tool for Large Vision-Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Gabriela Ben Melech Stan, Raanan Yehezkel Rohekar, Yaniv Gurwicz, Matthew Lyle Olson, Anahita Bhiwandiwalla, Estelle Aflalo, Chenfei Wu, Nan Duan, Shao-Yen Tseng, Vasudev Lal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03118">https://arxiv.org/abs/2404.03118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03118">https://arxiv.org/pdf/2404.03118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03118]] LVLM-Intrepret: An Interpretability Tool for Large Vision-Language  Models(https://arxiv.org/abs/2404.03118)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of artificial intelligence, multi-modal large language models are emerging as a significant area of interest. These models, which combine various forms of data input, are becoming increasingly popular. However, understanding their internal mechanisms remains a complex task. Numerous advancements have been made in the field of explainability tools and mechanisms, yet there is still much to explore. In this work, we present a novel interactive application aimed towards understanding the internal mechanisms of large vision-language models. Our interface is designed to enhance the interpretability of the image patches, which are instrumental in generating an answer, and assess the efficacy of the language model in grounding its output in the image. With our application, a user can systematically investigate the model and uncover system limitations, paving the way for enhancements in system capabilities. Finally, we present a case study of how our application can aid in understanding failure mechanisms in a popular large multi-modal model: LLaVA.</li>
</ul>

<h3>Title: Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning,  Repeating, or Just Biased?</h3>
<ul>
<li><strong>Authors: </strong>Vagrant Gautam, Eileen Bingert, Dawei Zhu, Anne Lauscher, Dietrich Klakow</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03134">https://arxiv.org/abs/2404.03134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03134">https://arxiv.org/pdf/2404.03134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03134]] Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning,  Repeating, or Just Biased?(https://arxiv.org/abs/2404.03134)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these components at a time. To measure progress towards the combined goal, we introduce the task of pronoun use fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later, independent of potential distractors. We present a carefully-designed dataset of over 5 million instances to evaluate pronoun use fidelity in English, and we use it to evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters). We find that while models can mostly faithfully reuse previously-specified pronouns in the presence of no distractors, they are significantly worse at processing she/her/her, singular they and neopronouns. Additionally, models are not robustly faithful to pronouns, as they are easily distracted. With even one additional sentence containing a distractor pronoun, accuracy drops on average by 34%. With 5 distractor sentences, accuracy drops by 52% for decoder-only models and 13% for encoder-only models. We show that widely-used large language models are still brittle, with large gaps in reasoning and in processing different pronouns in a setting that is very simple for humans, and we encourage researchers in bias and reasoning to bridge them.</li>
</ul>

<h3>Title: Diverse and Tailored Image Generation for Zero-shot Multi-label  Classification</h3>
<ul>
<li><strong>Authors: </strong>Kaixin Zhang, Zhixiang Yuan, Tao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03144">https://arxiv.org/abs/2404.03144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03144">https://arxiv.org/pdf/2404.03144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03144]] Diverse and Tailored Image Generation for Zero-shot Multi-label  Classification(https://arxiv.org/abs/2404.03144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recently, zero-shot multi-label classification has garnered considerable attention for its capacity to operate predictions on unseen labels without human annotations. Nevertheless, prevailing approaches often use seen classes as imperfect proxies for unseen ones, resulting in suboptimal performance. Drawing inspiration from the success of text-to-image generation models in producing realistic images, we propose an innovative solution: generating synthetic data to construct a training set explicitly tailored for proxyless training on unseen labels. Our approach introduces a novel image generation framework that produces multi-label synthetic images of unseen classes for classifier training. To enhance diversity in the generated images, we leverage a pre-trained large language model to generate diverse prompts. Employing a pre-trained multi-modal CLIP model as a discriminator, we assess whether the generated images accurately represent the target classes. This enables automatic filtering of inaccurately generated images, preserving classifier accuracy. To refine text prompts for more precise and effective multi-label object generation, we introduce a CLIP score-based discriminative loss to fine-tune the text encoder in the diffusion model. Additionally, to enhance visual features on the target task while maintaining the generalization of original features and mitigating catastrophic forgetting resulting from fine-tuning the entire visual encoder, we propose a feature fusion module inspired by transformer attention mechanisms. This module aids in capturing global dependencies between multiple objects more effectively. Extensive experimental results validate the effectiveness of our approach, demonstrating significant improvements over state-of-the-art methods.</li>
</ul>

<h3>Title: DreamWalk: Style Space Exploration using Diffusion Guidance</h3>
<ul>
<li><strong>Authors: </strong>Michelle Shu, Charles Herrmann, Richard Strong Bowen, Forrester Cole, Ramin Zabih</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03145">https://arxiv.org/abs/2404.03145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03145">https://arxiv.org/pdf/2404.03145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03145]] DreamWalk: Style Space Exploration using Diffusion Guidance(https://arxiv.org/abs/2404.03145)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform "prompt engineering," constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model's neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: https://mshu1.github.io/dreamwalk.github.io/</li>
</ul>

<h3>Title: Eigenpruning</h3>
<ul>
<li><strong>Authors: </strong>Tomás Vergara-Browne, Álvaro Soto, Akiko Aizawa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03147">https://arxiv.org/abs/2404.03147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03147">https://arxiv.org/pdf/2404.03147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03147]] Eigenpruning(https://arxiv.org/abs/2404.03147)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce eigenpruning, a method that removes singular values from weight matrices in an LLM to improve its performance in a particular task. This method is inspired by interpretability methods designed to automatically find subnetworks of a model which solve a specific task. In our tests, the pruned model outperforms the original model by a large margin, while only requiring minimal computation to prune the weight matrices. In the case of a small synthetic task in integer multiplication, the Phi-2 model can improve its accuracy in the test set from 13.75% to 97.50%. Interestingly, these results seem to indicate the existence of a computation path that can solve the task very effectively, but it was not being used by the original model. Finally, we plan to open-source our implementation in the camera-ready version of our work.</li>
</ul>

<h3>Title: HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</h3>
<ul>
<li><strong>Authors: </strong>Wencan Cheng, Hao Tang, Luc Van Gool, Jong Hwan Ko</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03159">https://arxiv.org/abs/2404.03159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03159">https://arxiv.org/pdf/2404.03159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03159]] HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud(https://arxiv.org/abs/2404.03159)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDiff.</li>
</ul>

<h3>Title: LTRDetector: Exploring Long-Term Relationship for Advanced Persistent  Threats Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxiao Liu, Fan Xu, Nan Wang, Qinxin Zhao, Dalin Zhang, Xibin Zhao, Jiqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03162">https://arxiv.org/abs/2404.03162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03162">https://arxiv.org/pdf/2404.03162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03162]] LTRDetector: Exploring Long-Term Relationship for Advanced Persistent  Threats Detection(https://arxiv.org/abs/2404.03162)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Advanced Persistent Threat (APT) is challenging to detect due to prolonged duration, infrequent occurrence, and adept concealment techniques. Existing approaches primarily concentrate on the observable traits of attack behaviors, neglecting the intricate relationships formed throughout the persistent attack lifecycle. Thus, we present an innovative APT detection framework named LTRDetector, implementing an end-to-end holistic operation. LTRDetector employs an innovative graph embedding technique to retain comprehensive contextual information, then derives long-term features from these embedded provenance graphs. During the process, we compress the data of the system provenance graph for effective feature learning. Furthermore, in order to detect attacks conducted by using zero-day exploits, we captured the system's regular behavior and detects abnormal activities without relying on predefined attack signatures. We also conducted extensive evaluations using five prominent datasets, the efficacy evaluation of which underscores the superiority of LTRDetector compared to existing state-of-the-art techniques.</li>
</ul>

<h3>Title: Uncertainty in Language Models: Assessment through Rank-Calibration</h3>
<ul>
<li><strong>Authors: </strong>Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup Lee, Osbert Bastani, Edgar Dobriban</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03163">https://arxiv.org/abs/2404.03163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03163">https://arxiv.org/pdf/2404.03163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03163]] Uncertainty in Language Models: Assessment through Rank-Calibration(https://arxiv.org/abs/2404.03163)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.</li>
</ul>

<h3>Title: Goldfish: An Efficient Federated Unlearning Framework</h3>
<ul>
<li><strong>Authors: </strong>Houzhe Wang, Xiaojie Zhu, Chi Chen, Paulo Esteves-Veríssimo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03180">https://arxiv.org/abs/2404.03180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03180">https://arxiv.org/pdf/2404.03180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03180]] Goldfish: An Efficient Federated Unlearning Framework(https://arxiv.org/abs/2404.03180)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>With recent legislation on the right to be forgotten, machine unlearning has emerged as a crucial research area. It facilitates the removal of a user's data from federated trained machine learning models without the necessity for retraining from scratch. However, current machine unlearning algorithms are confronted with challenges of efficiency and validity.To address the above issues, we propose a new framework, named Goldfish. It comprises four modules: basic model, loss function, optimization, and extension. To address the challenge of low validity in existing machine unlearning algorithms, we propose a novel loss function. It takes into account the loss arising from the discrepancy between predictions and actual labels in the remaining dataset. Simultaneously, it takes into consideration the bias of predicted results on the removed dataset. Moreover, it accounts for the confidence level of predicted results. Additionally, to enhance efficiency, we adopt knowledge distillation technique in basic model and introduce an optimization module that encompasses the early termination mechanism guided by empirical risk and the data partition mechanism. Furthermore, to bolster the robustness of the aggregated model, we propose an extension module that incorporates a mechanism using adaptive distillation temperature to address the heterogeneity of user local data and a mechanism using adaptive weight to handle the variety in the quality of uploaded models. Finally, we conduct comprehensive experiments to illustrate the effectiveness of proposed approach.</li>
</ul>

<h3>Title: AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying  Scales</h3>
<ul>
<li><strong>Authors: </strong>Tianrui Guan, Ruiqi Xian, Xijun Wang, Xiyang Wu, Mohamed Elnoor, Daeun Song, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03187">https://arxiv.org/abs/2404.03187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03187">https://arxiv.org/pdf/2404.03187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03187]] AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying  Scales(https://arxiv.org/abs/2404.03187)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present AGL-NET, a novel learning-based method for global localization using LiDAR point clouds and satellite maps. AGL-NET tackles two critical challenges: bridging the representation gap between image and points modalities for robust feature matching, and handling inherent scale discrepancies between global view and local view. To address these challenges, AGL-NET leverages a unified network architecture with a novel two-stage matching design. The first stage extracts informative neural features directly from raw sensor data and performs initial feature matching. The second stage refines this matching process by extracting informative skeleton features and incorporating a novel scale alignment step to rectify scale variations between LiDAR and map data. Furthermore, a novel scale and skeleton loss function guides the network toward learning scale-invariant feature representations, eliminating the need for pre-processing satellite maps. This significantly improves real-world applicability in scenarios with unknown map scales. To facilitate rigorous performance evaluation, we introduce a meticulously designed dataset within the CARLA simulator specifically tailored for metric localization training and assessment. The code and dataset will be made publicly available.</li>
</ul>

<h3>Title: The Probabilities Also Matter: A More Faithful Metric for Faithfulness  of Free-Text Explanations in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Noah Y. Siegel, Oana-Maria Camburu, Nicolas Heess, Maria Perez-Ortiz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03189">https://arxiv.org/abs/2404.03189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03189">https://arxiv.org/pdf/2404.03189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03189]] The Probabilities Also Matter: A More Faithful Metric for Faithfulness  of Free-Text Explanations in Large Language Models(https://arxiv.org/abs/2404.03189)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.</li>
</ul>

<h3>Title: CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception  Tasks</h3>
<ul>
<li><strong>Authors: </strong>Beibei Wang, Lu Zhang, Shuang Meng, Chenjie Wang, Jingjing Huang, Yao Li, Haojie Ren, Yuxuan Xiao, Yuru Peng, Jianmin Ji, Yu Zhang, Yanyong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03191">https://arxiv.org/abs/2404.03191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03191">https://arxiv.org/pdf/2404.03191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03191]] CORP: A Multi-Modal Dataset for Campus-Oriented Roadside Perception  Tasks(https://arxiv.org/abs/2404.03191)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Numerous roadside perception datasets have been introduced to propel advancements in autonomous driving and intelligent transportation systems research and development. However, it has been observed that the majority of their concentrates is on urban arterial roads, inadvertently overlooking residential areas such as parks and campuses that exhibit entirely distinct characteristics. In light of this gap, we propose CORP, which stands as the first public benchmark dataset tailored for multi-modal roadside perception tasks under campus scenarios. Collected in a university campus, CORP consists of over 205k images plus 102k point clouds captured from 18 cameras and 9 LiDAR sensors. These sensors with different configurations are mounted on roadside utility poles to provide diverse viewpoints within the campus region. The annotations of CORP encompass multi-dimensional information beyond 2D and 3D bounding boxes, providing extra support for 3D seamless tracking and instance segmentation with unique IDs and pixel masks for identifying targets, to enhance the understanding of objects and their behaviors distributed across the campus premises. Unlike other roadside datasets about urban traffic, CORP extends the spectrum to highlight the challenges for multi-modal perception in campuses and other residential areas.</li>
</ul>

<h3>Title: Future-Proofing Class Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03200">https://arxiv.org/abs/2404.03200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03200">https://arxiv.org/pdf/2404.03200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03200]] Future-Proofing Class Incremental Learning(https://arxiv.org/abs/2404.03200)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning.</li>
</ul>

<h3>Title: LeGrad: An Explainability Method for Vision Transformers via Feature  Formation Sensitivity</h3>
<ul>
<li><strong>Authors: </strong>Walid Bousselham, Angie Boggust, Sofian Chaybouti, Hendrik Strobelt, Hilde Kuehne</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03214">https://arxiv.org/abs/2404.03214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03214">https://arxiv.org/pdf/2404.03214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03214]] LeGrad: An Explainability Method for Vision Transformers via Feature  Formation Sensitivity(https://arxiv.org/abs/2404.03214)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs), with their ability to model long-range dependencies through self-attention mechanisms, have become a standard architecture in computer vision. However, the interpretability of these models remains a challenge. To address this, we propose LeGrad, an explainability method specifically designed for ViTs. LeGrad computes the gradient with respect to the attention maps of ViT layers, considering the gradient itself as the explainability signal. We aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map. This makes LeGrad a conceptually simple and an easy-to-implement tool for enhancing the transparency of ViTs. We evaluate LeGrad in challenging segmentation, perturbation, and open-vocabulary settings, showcasing its versatility compared to other SotA explainability methods demonstrating its superior spatial fidelity and robustness to perturbations. A demo and the code is available at https://github.com/WalBouss/LeGrad.</li>
</ul>

<h3>Title: Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators  for Fast Private Inference in Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Jianming Tong, Jingtian Dang, Anupam Golder, Callie Hao, Arijit Raychowdhury, Tushar Krishna</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03216">https://arxiv.org/abs/2404.03216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03216">https://arxiv.org/pdf/2404.03216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03216]] Accurate Low-Degree Polynomial Approximation of Non-polynomial Operators  for Fast Private Inference in Homomorphic Encryption(https://arxiv.org/abs/2404.03216)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>As machine learning (ML) permeates fields like healthcare, facial recognition, and blockchain, the need to protect sensitive data intensifies. Fully Homomorphic Encryption (FHE) allows inference on encrypted data, preserving the privacy of both data and the ML model. However, it slows down non-secure inference by up to five magnitudes, with a root cause of replacing non-polynomial operators (ReLU and MaxPooling) with high-degree Polynomial Approximated Function (PAF). We propose SmartPAF, a framework to replace non-polynomial operators with low-degree PAF and then recover the accuracy of PAF-approximated model through four techniques: (1) Coefficient Tuning (CT) -- adjust PAF coefficients based on the input distributions before training, (2) Progressive Approximation (PA) -- progressively replace one non-polynomial operator at a time followed by a fine-tuning, (3) Alternate Training (AT) -- alternate the training between PAFs and other linear operators in the decoupled manner, and (4) Dynamic Scale (DS) / Static Scale (SS) -- dynamically scale PAF input value within (-1, 1) in training, and fix the scale as the running max value in FHE deployment. The synergistic effect of CT, PA, AT, and DS/SS enables SmartPAF to enhance the accuracy of the various models approximated by PAFs with various low degrees under multiple datasets. For ResNet-18 under ImageNet-1k, the Pareto-frontier spotted by SmartPAF in latency-accuracy tradeoff space achieves 1.42x ~ 13.64x accuracy improvement and 6.79x ~ 14.9x speedup than prior works. Further, SmartPAF enables a 14-degree PAF (f1^2 g_1^2) to achieve 7.81x speedup compared to the 27-degree PAF obtained by minimax approximation with the same 69.4% post-replacement accuracy. Our code is available at https://github.com/TorchFHE/SmartPAF.</li>
</ul>

<h3>Title: iSeg: Interactive 3D Segmentation via Interactive Attention</h3>
<ul>
<li><strong>Authors: </strong>Itai Lang, Fei Xu, Dale Decatur, Sudarshan Babu, Rana Hanocka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03219">https://arxiv.org/abs/2404.03219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03219">https://arxiv.org/pdf/2404.03219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03219]] iSeg: Interactive 3D Segmentation via Interactive Attention(https://arxiv.org/abs/2404.03219)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present iSeg, a new interactive technique for segmenting 3D shapes. Previous works have focused mainly on leveraging pre-trained 2D foundation models for 3D segmentation based on text. However, text may be insufficient for accurately describing fine-grained spatial segmentations. Moreover, achieving a consistent 3D segmentation using a 2D model is challenging since occluded areas of the same semantic region may not be visible together from any 2D view. Thus, we design a segmentation method conditioned on fine user clicks, which operates entirely in 3D. Our system accepts user clicks directly on the shape's surface, indicating the inclusion or exclusion of regions from the desired shape partition. To accommodate various click settings, we propose a novel interactive attention module capable of processing different numbers and types of clicks, enabling the training of a single unified interactive segmentation model. We apply iSeg to a myriad of shapes from different domains, demonstrating its versatility and faithfulness to the user's specifications. Our project page is at https://threedle.github.io/iSeg/.</li>
</ul>

<h3>Title: FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR  Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Xu Wang, Tian Ye, Rajgopal Kannan, Viktor Prasanna</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03225">https://arxiv.org/abs/2404.03225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03225">https://arxiv.org/pdf/2404.03225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03225]] FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR  Image Classification(https://arxiv.org/abs/2404.03225)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Deep Learning (DL) Models for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR), while delivering improved performance, have been shown to be quite vulnerable to adversarial attacks. Existing works improve robustness by training models on adversarial samples. However, by focusing mostly on attacks that manipulate images randomly, they neglect the real-world feasibility of such attacks. In this paper, we propose FACTUAL, a novel Contrastive Learning framework for Adversarial Training and robust SAR classification. FACTUAL consists of two components: (1) Differing from existing works, a novel perturbation scheme that incorporates realistic physical adversarial attacks (such as OTSA) to build a supervised adversarial pre-training network. This network utilizes class labels for clustering clean and perturbed images together into a more informative feature space. (2) A linear classifier cascaded after the encoder to use the computed representations to predict the target labels. By pre-training and fine-tuning our model on both clean and adversarial samples, we show that our model achieves high prediction accuracy on both cases. Our model achieves 99.7% accuracy on clean samples, and 89.6% on perturbed samples, both outperforming previous state-of-the-art methods.</li>
</ul>

<h3>Title: Learn What You Want to Unlearn: Unlearning Inversion Attacks against  Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Hongsheng Hu, Shuo Wang, Tian Dong, Minhui Xue</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03233">https://arxiv.org/abs/2404.03233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03233">https://arxiv.org/pdf/2404.03233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03233]] Learn What You Want to Unlearn: Unlearning Inversion Attacks against  Machine Unlearning(https://arxiv.org/abs/2404.03233)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>Machine unlearning has become a promising solution for fulfilling the "right to be forgotten", under which individuals can request the deletion of their data from machine learning models. However, existing studies of machine unlearning mainly focus on the efficacy and efficiency of unlearning methods, while neglecting the investigation of the privacy vulnerability during the unlearning process. With two versions of a model available to an adversary, that is, the original model and the unlearned model, machine unlearning opens up a new attack surface. In this paper, we conduct the first investigation to understand the extent to which machine unlearning can leak the confidential content of the unlearned data. Specifically, under the Machine Learning as a Service setting, we propose unlearning inversion attacks that can reveal the feature and label information of an unlearned sample by only accessing the original and unlearned model. The effectiveness of the proposed unlearning inversion attacks is evaluated through extensive experiments on benchmark datasets across various model architectures and on both exact and approximate representative unlearning approaches. The experimental results indicate that the proposed attack can reveal the sensitive information of the unlearned data. As such, we identify three possible defenses that help to mitigate the proposed attacks, while at the cost of reducing the utility of the unlearned model. The study in this paper uncovers an underexplored gap between machine unlearning and the privacy of unlearned data, highlighting the need for the careful design of mechanisms for implementing unlearning without leaking the information of the unlearned data.</li>
</ul>

<h3>Title: Would Deep Generative Models Amplify Bias in Future Models?</h3>
<ul>
<li><strong>Authors: </strong>Tianwei Chen, Yusuke Hirota, Mayu Otani, Noa Garcia, Yuta Nakashima</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03242">https://arxiv.org/abs/2404.03242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03242">https://arxiv.org/pdf/2404.03242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03242]] Would Deep Generative Models Amplify Bias in Future Models?(https://arxiv.org/abs/2404.03242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We investigate the impact of deep generative models on potential social biases in upcoming computer vision models. As the internet witnesses an increasing influx of AI-generated images, concerns arise regarding inherent biases that may accompany them, potentially leading to the dissemination of harmful content. This paper explores whether a detrimental feedback loop, resulting in bias amplification, would occur if generated images were used as the training data for future models. We conduct simulations by progressively substituting original images in COCO and CC3M datasets with images generated through Stable Diffusion. The modified datasets are used to train OpenCLIP and image captioning models, which we evaluate in terms of quality and bias. Contrary to expectations, our findings indicate that introducing generated images during training does not uniformly amplify bias. Instead, instances of bias mitigation across specific tasks are observed. We further explore the factors that may influence these phenomena, such as artifacts in image generation (e.g., blurry faces) or pre-existing biases in the original datasets.</li>
</ul>

<h3>Title: Real-time Noise Source Estimation of a Camera System from an Image and  Metadata</h3>
<ul>
<li><strong>Authors: </strong>Maik Wischow, Patrick Irmisch, Anko Boerner, Guillermo Gallego</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03251">https://arxiv.org/abs/2404.03251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03251">https://arxiv.org/pdf/2404.03251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03251]] Real-time Noise Source Estimation of a Camera System from an Image and  Metadata(https://arxiv.org/abs/2404.03251)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous machines must self-maintain proper functionality to ensure the safety of humans and themselves. This pertains particularly to its cameras as predominant sensors to perceive the environment and support actions. A fundamental camera problem addressed in this study is noise. Solutions often focus on denoising images a posteriori, that is, fighting symptoms rather than root causes. However, tackling root causes requires identifying the noise sources, considering the limitations of mobile platforms. This work investigates a real-time, memory-efficient and reliable noise source estimator that combines data- and physically-based models. To this end, a DNN that examines an image with camera metadata for major camera noise sources is built and trained. In addition, it quantifies unexpected factors that impact image noise or metadata. This study investigates seven different estimators on six datasets that include synthetic noise, real-world noise from two camera systems, and real field campaigns. For these, only the model with most metadata is capable to accurately and robustly quantify all individual noise contributions. This method outperforms total image noise estimators and can be plug-and-play deployed. It also serves as a basis to include more advanced noise sources, or as part of an automatic countermeasure feedback-loop to approach fully reliable machines.</li>
</ul>

<h3>Title: Enhancing the Performance of Aspect-Based Sentiment Analysis Systems</h3>
<ul>
<li><strong>Authors: </strong>Chen Li, Jinli Zhang, Huidong Tang, Peng Ju, Debo Cheng, Yasuhiko Morimoto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03259">https://arxiv.org/abs/2404.03259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03259">https://arxiv.org/pdf/2404.03259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03259]] Enhancing the Performance of Aspect-Based Sentiment Analysis Systems(https://arxiv.org/abs/2404.03259)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Aspect-based sentiment analysis aims to predict sentiment polarity with fine granularity. While Graph Convolutional Networks (GCNs) are widely utilized for sentimental feature extraction, their naive application for syntactic feature extraction can compromise information preservation. This study introduces an innovative edge-enhanced GCN, named SentiSys, to navigate the syntactic graph while preserving intact feature information, leading to enhanced performance. Specifically,we first integrate a bidirectional long short-term memory (Bi-LSTM) network and a self-attention-based transformer. This combination facilitates effective text encoding, preventing the loss of information and predicting long dependency text. A bidirectional GCN (Bi-GCN) with message passing is then employed to encode relationships between entities. Additionally, unnecessary information is filtered out using an aspect-specific masking technique. To validate the effectiveness of our proposed model, we conduct extensive evaluation experiments and ablation studies on four benchmark datasets. The results consistently demonstrate improved performance in aspect-based sentiment analysis when employing SentiSys. This approach successfully addresses the challenges associated with syntactic feature extraction, highlighting its potential for advancing sentiment analysis methodologies.</li>
</ul>

<h3>Title: On the Surprising Efficacy of Distillation as an Alternative to  Pre-Training Small Models</h3>
<ul>
<li><strong>Authors: </strong>Sean Farhat, Deming Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03263">https://arxiv.org/abs/2404.03263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03263">https://arxiv.org/pdf/2404.03263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03263]] On the Surprising Efficacy of Distillation as an Alternative to  Pre-Training Small Models(https://arxiv.org/abs/2404.03263)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose that small models may not need to absorb the cost of pre-training to reap its benefits. Instead, they can capitalize on the astonishing results achieved by modern, enormous models to a surprising degree. We observe that, when distilled on a task from a pre-trained teacher model, a small model can achieve or surpass the performance it would achieve if it was pre-trained then finetuned on that task. To allow this phenomenon to be easily leveraged, we establish a connection reducing knowledge distillation to modern contrastive learning, opening two doors: (1) vastly different model architecture pairings can work for the distillation, and (2) most contrastive learning algorithms rooted in the theory of Noise Contrastive Estimation can be easily applied and used. We demonstrate this paradigm using pre-trained teacher models from open-source model hubs, Transformer and convolution based model combinations, and a novel distillation algorithm that massages the Alignment/Uniformity perspective of contrastive learning by Wang & Isola (2020) into a distillation objective. We choose this flavor of contrastive learning due to its low computational cost, an overarching theme of this work. We also observe that this phenomenon tends not to occur if the task is data-limited. However, this can be alleviated by leveraging yet another scale-inspired development: large, pre-trained generative models for dataset augmentation. Again, we use an open-source model, and our rudimentary prompts are sufficient to boost the small model`s performance. Thus, we highlight a training method for small models that is up to 94% faster than the standard pre-training paradigm without sacrificing performance. For practitioners discouraged from fully utilizing modern foundation datasets for their small models due to the prohibitive scale, we believe our work keeps that door open.</li>
</ul>

<h3>Title: Gaussian-Smoothed Sliced Probability Divergences</h3>
<ul>
<li><strong>Authors: </strong>Mokhtar Z. Alaya (LMAC), Alain Rakotomamonjy (LITIS), Maxime Berar (LITIS), Gilles Gasso (LITIS)</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03273">https://arxiv.org/abs/2404.03273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03273">https://arxiv.org/pdf/2404.03273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03273]] Gaussian-Smoothed Sliced Probability Divergences(https://arxiv.org/abs/2404.03273)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Gaussian smoothed sliced Wasserstein distance has been recently introduced for comparing probability distributions, while preserving privacy on the data. It has been shown that it provides performances similar to its non-smoothed (non-private) counterpart. However, the computationaland statistical properties of such a metric have not yet been well-established. This work investigates the theoretical properties of this distance as well as those of generalized versions denoted as Gaussian-smoothed sliced divergences. We first show that smoothing and slicing preserve the metric property and the weak topology. To study the sample complexity of such divergences, we then introduce $\hat{\hat\mu}_{n}$ the double empirical distribution for the smoothed-projected $\mu$. The distribution $\hat{\hat\mu}_{n}$ is a result of a double sampling process: one from sampling according to the origin distribution $\mu$ and the second according to the convolution of the projection of $\mu$ on the unit sphere and the Gaussian smoothing. We particularly focus on the Gaussian smoothed sliced Wasserstein distance and prove that it converges with a rate $O(n^{-1/2})$. We also derive other properties, including continuity, of different divergences with respect to the smoothing parameter. We support our theoretical findings with empirical studies in the context of privacy-preserving domain adaptation.</li>
</ul>

<h3>Title: A Deep Reinforcement Learning Approach for Security-Aware Service  Acquisition in IoT</h3>
<ul>
<li><strong>Authors: </strong>Marco Arazzi, Serena Nicolazzo, Antonino Nocera</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03276">https://arxiv.org/abs/2404.03276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03276">https://arxiv.org/pdf/2404.03276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03276]] A Deep Reinforcement Learning Approach for Security-Aware Service  Acquisition in IoT(https://arxiv.org/abs/2404.03276)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The novel Internet of Things (IoT) paradigm is composed of a growing number of heterogeneous smart objects and services that are transforming architectures and applications, increasing systems' complexity, and the need for reliability and autonomy. In this context, both smart objects and services are often provided by third parties which do not give full transparency regarding the security and privacy of the features offered. Although machine-based Service Level Agreements (SLA) have been recently leveraged to establish and share policies in Cloud-based scenarios, and also in the IoT context, the issue of making end users aware of the overall system security levels and the fulfillment of their privacy requirements through the provision of the requested service remains a challenging task. To tackle this problem, we propose a complete framework that defines suitable levels of privacy and security requirements in the acquisition of services in IoT, according to the user needs. Through the use of a Reinforcement Learning based solution, a user agent, inside the environment, is trained to choose the best smart objects granting access to the target services. Moreover, the solution is designed to guarantee deadline requirements and user security and privacy needs. Finally, to evaluate the correctness and the performance of the proposed approach we illustrate an extensive experimental analysis.</li>
</ul>

<h3>Title: SiloFuse: Cross-silo Synthetic Data Generation with Latent Tabular  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Aditya Shankar, Hans Brouwer, Rihan Hai, Lydia Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DB, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03299">https://arxiv.org/abs/2404.03299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03299">https://arxiv.org/pdf/2404.03299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03299]] SiloFuse: Cross-silo Synthetic Data Generation with Latent Tabular  Diffusion Models(https://arxiv.org/abs/2404.03299)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data is crucial for sharing and augmenting data across silos, especially for enterprises with proprietary data. However, existing synthesizers are designed for centrally stored data. Hence, they struggle with real-world scenarios where features are distributed across multiple silos, necessitating on-premise data storage. We introduce SiloFuse, a novel generative framework for high-quality synthesis from cross-silo tabular data. To ensure privacy, SiloFuse utilizes a distributed latent tabular diffusion architecture. Through autoencoders, latent representations are learned for each client's features, masking their actual values. We employ stacked distributed training to improve communication efficiency, reducing the number of rounds to a single step. Under SiloFuse, we prove the impossibility of data reconstruction for vertically partitioned synthesis and quantify privacy risks through three attacks using our benchmark framework. Experimental results on nine datasets showcase SiloFuse's competence against centralized diffusion-based synthesizers. Notably, SiloFuse achieves 43.8 and 29.8 higher percentage points over GANs in resemblance and utility. Experiments on communication show stacked training's fixed cost compared to the growing costs of end-to-end training as the number of training iterations increases. Additionally, SiloFuse proves robust to feature permutations and varying numbers of clients.</li>
</ul>

<h3>Title: Probing Large Language Models for Scalar Adjective Lexical Semantics and  Scalar Diversity Pragmatics</h3>
<ul>
<li><strong>Authors: </strong>Fangru Lin, Daniel Altshuler, Janet B. Pierrehumbert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03301">https://arxiv.org/abs/2404.03301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03301">https://arxiv.org/pdf/2404.03301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03301]] Probing Large Language Models for Scalar Adjective Lexical Semantics and  Scalar Diversity Pragmatics(https://arxiv.org/abs/2404.03301)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scalar adjectives pertain to various domain scales and vary in intensity within each scale (e.g. certain is more intense than likely on the likelihood scale). Scalar implicatures arise from the consideration of alternative statements which could have been made. They can be triggered by scalar adjectives and require listeners to reason pragmatically about them. Some scalar adjectives are more likely to trigger scalar implicatures than others. This phenomenon is referred to as scalar diversity. In this study, we probe different families of Large Language Models such as GPT-4 for their knowledge of the lexical semantics of scalar adjectives and one specific aspect of their pragmatics, namely scalar diversity. We find that they encode rich lexical-semantic information about scalar adjectives. However, the rich lexical-semantic knowledge does not entail a good understanding of scalar diversity. We also compare current models of different sizes and complexities and find that larger models are not always better. Finally, we explain our probing results by leveraging linguistic intuitions and model training objectives.</li>
</ul>

<h3>Title: How Easily do Irrelevant Inputs Skew the Responses of Large Language  Models?</h3>
<ul>
<li><strong>Authors: </strong>Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03302">https://arxiv.org/abs/2404.03302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03302">https://arxiv.org/pdf/2404.03302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03302]] How Easily do Irrelevant Inputs Skew the Responses of Large Language  Models?(https://arxiv.org/abs/2404.03302)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>By leveraging the retrieval of information from external knowledge databases, Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing many knowledge-intensive tasks. However, due to the inherent flaws of current retrieval systems, there might exist irrelevant information within those retrieving top-ranked passages. In this work, we present a comprehensive investigation into the robustness of LLMs to different types of irrelevant information under various conditions. We initially introduce a framework to construct high-quality irrelevant information that ranges from semantically unrelated, partially related, and related to questions. Furthermore, our analysis demonstrates that the constructed irrelevant information not only scores highly on similarity metrics, being highly retrieved by existing systems, but also bears semantic connections to the context. Our investigation reveals that current LLMs still face challenges in discriminating highly semantically related information and can be easily distracted by these irrelevant yet misleading contents. Besides, we also find that current solutions for handling irrelevant information have limitations in improving the robustness of LLMs to such distractions. Resources are available at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.</li>
</ul>

<h3>Title: Optimistic Online Non-stochastic Control via FTRL</h3>
<ul>
<li><strong>Authors: </strong>Naram Mhaisen, George Iosifidis</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03309">https://arxiv.org/abs/2404.03309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03309">https://arxiv.org/pdf/2404.03309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03309]] Optimistic Online Non-stochastic Control via FTRL(https://arxiv.org/abs/2404.03309)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper brings the concept of "optimism" to the new and promising framework of online Non-stochastic Control (NSC). Namely, we study how can NSC benefit from a prediction oracle of unknown quality responsible for forecasting future costs. The posed problem is first reduced to an optimistic learning with delayed feedback problem, which is handled through the Optimistic Follow the Regularized Leader (OFTRL) algorithmic family. This reduction enables the design of OptFTRL-C, the first Disturbance Action Controller (DAC) with optimistic policy regret bounds. These new bounds are commensurate with the oracle's accuracy, ranging from $\mathcal{O}(1)$ for perfect predictions to the order-optimal $\mathcal{O}(\sqrt{T})$ even when all predictions fail. By addressing the challenge of incorporating untrusted predictions into control systems, our work contributes to the advancement of the NSC framework and paves the way towards effective and robust learning-based controllers.</li>
</ul>

<h3>Title: Exploring Lightweight Federated Learning for Distributed Load  Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Duttagupta, Jin Zhao, Shanker Shreejith</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03320">https://arxiv.org/abs/2404.03320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03320">https://arxiv.org/pdf/2404.03320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03320]] Exploring Lightweight Federated Learning for Distributed Load  Forecasting(https://arxiv.org/abs/2404.03320)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed learning scheme that enables deep learning to be applied to sensitive data streams and applications in a privacy-preserving manner. This paper focuses on the use of FL for analyzing smart energy meter data with the aim to achieve comparable accuracy to state-of-the-art methods for load forecasting while ensuring the privacy of individual meter data. We show that with a lightweight fully connected deep neural network, we are able to achieve forecasting accuracy comparable to existing schemes, both at each meter source and at the aggregator, by utilising the FL framework. The use of lightweight models further reduces the energy and resource consumption caused by complex deep-learning models, making this approach ideally suited for deployment across resource-constrained smart meter systems. With our proposed lightweight model, we are able to achieve an overall average load forecasting RMSE of 0.17, with the model having a negligible energy overhead of 50 mWh when performing training and inference on an Arduino Uno platform.</li>
</ul>

<h3>Title: A Comparative Analysis of Word-Level Metric Differential Privacy:  Benchmarking The Privacy-Utility Trade-off</h3>
<ul>
<li><strong>Authors: </strong>Stephen Meisenbacher, Nihildev Nandakumar, Alexandra Klymenko, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03324">https://arxiv.org/abs/2404.03324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03324">https://arxiv.org/pdf/2404.03324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03324]] A Comparative Analysis of Word-Level Metric Differential Privacy:  Benchmarking The Privacy-Utility Trade-off(https://arxiv.org/abs/2404.03324)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The application of Differential Privacy to Natural Language Processing techniques has emerged in relevance in recent years, with an increasing number of studies published in established NLP outlets. In particular, the adaptation of Differential Privacy for use in NLP tasks has first focused on the $\textit{word-level}$, where calibrated noise is added to word embedding vectors to achieve "noisy" representations. To this end, several implementations have appeared in the literature, each presenting an alternative method of achieving word-level Differential Privacy. Although each of these includes its own evaluation, no comparative analysis has been performed to investigate the performance of such methods relative to each other. In this work, we conduct such an analysis, comparing seven different algorithms on two NLP tasks with varying hyperparameters, including the $\textit{epsilon ($\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an in-depth analysis of the results with a focus on the privacy-utility trade-off, as well as open-source our implementation code for further reproduction. As a result of our analysis, we give insight into the benefits and challenges of word-level Differential Privacy, and accordingly, we suggest concrete steps forward for the research field.</li>
</ul>

<h3>Title: Meta Invariance Defense Towards Generalizable Robustness to Unknown  Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Lei Zhang, Yuhang Zhou, Yi Yang, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03340">https://arxiv.org/abs/2404.03340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03340">https://arxiv.org/pdf/2404.03340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03340]] Meta Invariance Defense Towards Generalizable Robustness to Unknown  Adversarial Attacks(https://arxiv.org/abs/2404.03340)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Despite providing high-performance solutions for computer vision tasks, the deep neural network (DNN) model has been proved to be extremely vulnerable to adversarial attacks. Current defense mainly focuses on the known attacks, but the adversarial robustness to the unknown attacks is seriously overlooked. Besides, commonly used adaptive learning and fine-tuning technique is unsuitable for adversarial defense since it is essentially a zero-shot problem when deployed. Thus, to tackle this challenge, we propose an attack-agnostic defense method named Meta Invariance Defense (MID). Specifically, various combinations of adversarial attacks are randomly sampled from a manually constructed Attacker Pool to constitute different defense tasks against unknown attacks, in which a student encoder is supervised by multi-consistency distillation to learn the attack-invariant features via a meta principle. The proposed MID has two merits: 1) Full distillation from pixel-, feature- and prediction-level between benign and adversarial samples facilitates the discovery of attack-invariance. 2) The model simultaneously achieves robustness to the imperceptible adversarial perturbations in high-level image classification and attack-suppression in low-level robust image regeneration. Theoretical and empirical studies on numerous benchmarks such as ImageNet verify the generalizable robustness and superiority of MID under various attacks.</li>
</ul>

<h3>Title: Knowledge Distillation-Based Model Extraction Attack using Private  Counterfactual Explanations</h3>
<ul>
<li><strong>Authors: </strong>Fatima Ezzeddine, Omran Ayoub, Silvia Giordano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03348">https://arxiv.org/abs/2404.03348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03348">https://arxiv.org/pdf/2404.03348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03348]] Knowledge Distillation-Based Model Extraction Attack using Private  Counterfactual Explanations(https://arxiv.org/abs/2404.03348)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, extraction, generative</a></li>
<li><strong>Abstract: </strong>In recent years, there has been a notable increase in the deployment of machine learning (ML) models as services (MLaaS) across diverse production software applications. In parallel, explainable AI (XAI) continues to evolve, addressing the necessity for transparency and trustworthiness in ML models. XAI techniques aim to enhance the transparency of ML models by providing insights, in terms of the model's explanations, into their decision-making process. Simultaneously, some MLaaS platforms now offer explanations alongside the ML prediction outputs. This setup has elevated concerns regarding vulnerabilities in MLaaS, particularly in relation to privacy leakage attacks such as model extraction attacks (MEA). This is due to the fact that explanations can unveil insights about the inner workings of the model which could be exploited by malicious users. In this work, we focus on investigating how model explanations, particularly Generative adversarial networks (GANs)-based counterfactual explanations (CFs), can be exploited for performing MEA within the MLaaS platform. We also delve into assessing the effectiveness of incorporating differential privacy (DP) as a mitigation strategy. To this end, we first propose a novel MEA methodology based on Knowledge Distillation (KD) to enhance the efficiency of extracting a substitute model of a target model exploiting CFs. Then, we advise an approach for training CF generators incorporating DP to generate private CFs. We conduct thorough experimental evaluations on real-world datasets and demonstrate that our proposed KD-based MEA can yield a high-fidelity substitute model with reduced queries with respect to baseline approaches. Furthermore, our findings reveal that the inclusion of a privacy layer impacts the performance of the explainer, the quality of CFs, and results in a reduction in the MEA performance.</li>
</ul>

<h3>Title: Towards Pareto Optimal Throughput in Small Language Model Serving</h3>
<ul>
<li><strong>Authors: </strong>Pol G.Recasens, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03353">https://arxiv.org/abs/2404.03353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03353">https://arxiv.org/pdf/2404.03353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03353]] Towards Pareto Optimal Throughput in Small Language Model Serving(https://arxiv.org/abs/2404.03353)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.</li>
</ul>

<h3>Title: REACT: Revealing Evolutionary Action Consequence Trajectories for  Interpretable Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Philipp Altmann, Céline Davignon, Maximilian Zorn, Fabian Ritz, Claudia Linnhoff-Popien, Thomas Gabor</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03359">https://arxiv.org/abs/2404.03359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03359">https://arxiv.org/pdf/2404.03359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03359]] REACT: Revealing Evolutionary Action Consequence Trajectories for  Interpretable Reinforcement Learning(https://arxiv.org/abs/2404.03359)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>To enhance the interpretability of Reinforcement Learning (RL), we propose Revealing Evolutionary Action Consequence Trajectories (REACT). In contrast to the prevalent practice of validating RL models based on their optimal behavior learned during training, we posit that considering a range of edge-case trajectories provides a more comprehensive understanding of their inherent behavior. To induce such scenarios, we introduce a disturbance to the initial state, optimizing it through an evolutionary algorithm to generate a diverse population of demonstrations. To evaluate the fitness of trajectories, REACT incorporates a joint fitness function that encourages both local and global diversity in the encountered states and chosen actions. Through assessments with policies trained for varying durations in discrete and continuous environments, we demonstrate the descriptive power of REACT. Our results highlight its effectiveness in revealing nuanced aspects of RL models' behavior beyond optimal performance, thereby contributing to improved interpretability.</li>
</ul>

<h3>Title: nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion  Cause in Conversations with Chain-of-Thought on Emotion States</h3>
<ul>
<li><strong>Authors: </strong>Nicolay Rusnachenko, Huizhi Liang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03361">https://arxiv.org/abs/2404.03361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03361">https://arxiv.org/pdf/2404.03361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03361]] nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion  Cause in Conversations with Chain-of-Thought on Emotion States(https://arxiv.org/abs/2404.03361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emotion expression is one of the essential traits of conversations. It may be self-related or caused by another speaker. The variety of reasons may serve as a source of the further emotion causes: conversation history, speaker's emotional state, etc. Inspired by the most recent advances in Chain-of-Thought, in this work, we exploit the existing three-hop reasoning approach (THOR) to perform large language model instruction-tuning for answering: emotion states (THOR-state), and emotion caused by one speaker to the other (THOR-cause). We equip THOR-cause with the reasoning revision (rr) for devising a reasoning path in fine-tuning. In particular, we rely on the annotated speaker emotion states to revise reasoning path. Our final submission, based on Flan-T5-base (250M) and the rule-based span correction technique, preliminary tuned with THOR-state and fine-tuned with THOR-cause-rr on competition training data, results in 3rd and 4th places (F1-proportional) and 5th place (F1-strict) among 15 participating teams. Our THOR implementation fork is publicly available: https://github.com/nicolay-r/THOR-ECAC</li>
</ul>

<h3>Title: On the Theoretical Expressive Power and the Design Space of Higher-Order  Graph Transformers</h3>
<ul>
<li><strong>Authors: </strong>Cai Zhou, Rose Yu, Yusu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CG, math.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03380">https://arxiv.org/abs/2404.03380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03380">https://arxiv.org/pdf/2404.03380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03380]] On the Theoretical Expressive Power and the Design Space of Higher-Order  Graph Transformers(https://arxiv.org/abs/2404.03380)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph transformers have recently received significant attention in graph learning, partly due to their ability to capture more global interaction via self-attention. Nevertheless, while higher-order graph neural networks have been reasonably well studied, the exploration of extending graph transformers to higher-order variants is just starting. Both theoretical understanding and empirical results are limited. In this paper, we provide a systematic study of the theoretical expressive power of order-$k$ graph transformers and sparse variants. We first show that, an order-$k$ graph transformer without additional structural information is less expressive than the $k$-Weisfeiler Lehman ($k$-WL) test despite its high computational cost. We then explore strategies to both sparsify and enhance the higher-order graph transformers, aiming to improve both their efficiency and expressiveness. Indeed, sparsification based on neighborhood information can enhance the expressive power, as it provides additional information about input graph structures. In particular, we show that a natural neighborhood-based sparse order-$k$ transformer model is not only computationally efficient, but also expressive -- as expressive as $k$-WL test. We further study several other sparse graph attention models that are computationally efficient and provide their expressiveness analysis. Finally, we provide experimental results to show the effectiveness of the different sparsification strategies.</li>
</ul>

<h3>Title: DIDA: Denoised Imitation Learning based on Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Kaichen Huang, Hai-Hang Sun, Shenghua Wan, Minghao Shao, Shuai Feng, Le Gan, De-Chuan Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03382">https://arxiv.org/abs/2404.03382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03382">https://arxiv.org/pdf/2404.03382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03382]] DIDA: Denoised Imitation Learning based on Domain Adaptation(https://arxiv.org/abs/2404.03382)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Imitating skills from low-quality datasets, such as sub-optimal demonstrations and observations with distractors, is common in real-world applications. In this work, we focus on the problem of Learning from Noisy Demonstrations (LND), where the imitator is required to learn from data with noise that often occurs during the processes of data collection or transmission. Previous IL methods improve the robustness of learned policies by injecting an adversarially learned Gaussian noise into pure expert data or utilizing additional ranking information, but they may fail in the LND setting. To alleviate the above problems, we propose Denoised Imitation learning based on Domain Adaptation (DIDA), which designs two discriminators to distinguish the noise level and expertise level of data, facilitating a feature encoder to learn task-related but domain-agnostic representations. Experiment results on MuJoCo demonstrate that DIDA can successfully handle challenging imitation tasks from demonstrations with various types of noise, outperforming most baseline methods.</li>
</ul>

<h3>Title: LongVLM: Efficient Long Video Understanding via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, Bohan Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03384">https://arxiv.org/abs/2404.03384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03384">https://arxiv.org/pdf/2404.03384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03384]] LongVLM: Efficient Long Video Understanding via Large Language Models(https://arxiv.org/abs/2404.03384)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Empowered by Large Language Models (LLMs), recent advancements in VideoLLMs have driven progress in various video understanding tasks. These models encode video representations through pooling or query aggregation over a vast number of visual tokens, making computational and memory costs affordable. Despite successfully providing an overall comprehension of video content, existing VideoLLMs still face challenges in achieving detailed understanding in videos due to overlooking local information in long-term videos. To tackle this challenge, we introduce LongVLM, a straightforward yet powerful VideoLLM for long video understanding, building upon the observation that long videos often consist of sequential key events, complex actions, and camera movements. Our approach proposes to decompose long videos into multiple short-term segments and encode local features for each local segment via a hierarchical token merging module. These features are concatenated in temporal order to maintain the storyline across sequential short-term segments. Additionally, we propose to integrate global semantics into each local feature to enhance context understanding. In this way, we encode video representations that incorporate both local and global information, enabling the LLM to generate comprehensive responses for long-term videos. Experimental results on the VideoChatGPT benchmark and zero-shot video question-answering datasets demonstrate the superior capabilities of our model over the previous state-of-the-art methods. Qualitative examples demonstrate that our model produces more precise responses for long videos understanding. Code is available at \url{https://github.com/ziplab/LongVLM}.</li>
</ul>

<h3>Title: Heckler: Breaking Confidential VMs with Malicious Interrupts</h3>
<ul>
<li><strong>Authors: </strong>Benedict Schlüter, Supraja Sridhara, Mark Kuhne, Andrin Bertschi, Shweta Shinde</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03387">https://arxiv.org/abs/2404.03387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03387">https://arxiv.org/pdf/2404.03387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03387]] Heckler: Breaking Confidential VMs with Malicious Interrupts(https://arxiv.org/abs/2404.03387)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Hardware-based Trusted execution environments (TEEs) offer an isolation granularity of virtual machine abstraction. They provide confidential VMs (CVMs) that host security-sensitive code and data. AMD SEV-SNP and Intel TDX enable CVMs and are now available on popular cloud platforms. The untrusted hypervisor in these settings is in control of several resource management and configuration tasks, including interrupts. We present Heckler, a new attack wherein the hypervisor injects malicious non-timer interrupts to break the confidentiality and integrity of CVMs. Our insight is to use the interrupt handlers that have global effects, such that we can manipulate a CVM's register states to change the data and control flow. With AMD SEV-SNP and Intel TDX, we demonstrate Heckler on OpenSSH and sudo to bypass authentication. On AMD SEV-SNP we break execution integrity of C, Java, and Julia applications that perform statistical and text analysis. We explain the gaps in current defenses and outline guidelines for future defenses.</li>
</ul>

<h3>Title: Two Tricks to Improve Unsupervised Segmentation Learning</h3>
<ul>
<li><strong>Authors: </strong>Alp Eren Sari, Francesco Locatello, Paolo Favar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03392">https://arxiv.org/abs/2404.03392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03392">https://arxiv.org/pdf/2404.03392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03392]] Two Tricks to Improve Unsupervised Segmentation Learning(https://arxiv.org/abs/2404.03392)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present two practical improvement techniques for unsupervised segmentation learning. These techniques address limitations in the resolution and accuracy of predicted segmentation maps of recent state-of-the-art methods. Firstly, we leverage image post-processing techniques such as guided filtering to refine the output masks, improving accuracy while avoiding substantial computational costs. Secondly, we introduce a multi-scale consistency criterion, based on a teacher-student training scheme. This criterion matches segmentation masks predicted from regions of the input image extracted at different resolutions to each other. Experimental results on several benchmarks used in unsupervised segmentation learning demonstrate the effectiveness of our proposed techniques.</li>
</ul>

<h3>Title: Background Noise Reduction of Attention Map for Weakly Supervised  Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Izumi Fujimori, Masaki Oono, Masami Shishibori</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03394">https://arxiv.org/abs/2404.03394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03394">https://arxiv.org/pdf/2404.03394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03394]] Background Noise Reduction of Attention Map for Weakly Supervised  Semantic Segmentation(https://arxiv.org/abs/2404.03394)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In weakly-supervised semantic segmentation (WSSS) using only image-level class labels, a problem with CNN-based Class Activation Maps (CAM) is that they tend to activate the most discriminative local regions of objects. On the other hand, methods based on Transformers learn global features but suffer from the issue of background noise contamination. This paper focuses on addressing the issue of background noise in attention weights within the existing WSSS method based on Conformer, known as TransCAM. The proposed method successfully reduces background noise, leading to improved accuracy of pseudo labels. Experimental results demonstrate that our model achieves segmentation performance of 70.5% on the PASCAL VOC 2012 validation data, 71.1% on the test data, and 45.9% on MS COCO 2014 data, outperforming TransCAM in terms of segmentation performance.</li>
</ul>

<h3>Title: Scaling Up Video Summarization Pretraining with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dawit Mureja Argaw, Seunghyun Yoon, Fabian Caba Heilbron, Hanieh Deilamsalehy, Trung Bui, Zhaowen Wang, Franck Dernoncourt, Joon Son Chung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03398">https://arxiv.org/abs/2404.03398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03398">https://arxiv.org/pdf/2404.03398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03398]] Scaling Up Video Summarization Pretraining with Large Language Models(https://arxiv.org/abs/2404.03398)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-form video content constitutes a significant portion of internet traffic, making automated video summarization an essential research problem. However, existing video summarization datasets are notably limited in their size, constraining the effectiveness of state-of-the-art methods for generalization. Our work aims to overcome this limitation by capitalizing on the abundance of long-form videos with dense speech-to-video alignment and the remarkable capabilities of recent large language models (LLMs) in summarizing long text. We introduce an automated and scalable pipeline for generating a large-scale video summarization dataset using LLMs as Oracle summarizers. By leveraging the generated dataset, we analyze the limitations of existing approaches and propose a new video summarization model that effectively addresses them. To facilitate further research in the field, our work also presents a new benchmark dataset that contains 1200 long videos each with high-quality summaries annotated by professionals. Extensive experiments clearly indicate that our proposed approach sets a new state-of-the-art in video summarization across several benchmarks.</li>
</ul>

<h3>Title: AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Chunyi Li, Tengchuan Kou, Yixuan Gao, Yuqin Cao, Wei Sun, Zicheng Zhang, Yingjie Zhou, Zhichao Zhang, Weixia Zhang, Haoning Wu, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03407">https://arxiv.org/abs/2404.03407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03407">https://arxiv.org/pdf/2404.03407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03407]] AIGIQA-20K: A Large Database for AI-Generated Image Quality Assessment(https://arxiv.org/abs/2404.03407)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancements in AI-Generated Content (AIGC), AI-Generated Images (AIGIs) have been widely applied in entertainment, education, and social media. However, due to the significant variance in quality among different AIGIs, there is an urgent need for models that consistently match human subjective ratings. To address this issue, we organized a challenge towards AIGC quality assessment on NTIRE 2024 that extensively considers 15 popular generative models, utilizing dynamic hyper-parameters (including classifier-free guidance, iteration epochs, and output image resolution), and gather subjective scores that consider perceptual quality and text-to-image alignment altogether comprehensively involving 21 subjects. This approach culminates in the creation of the largest fine-grained AIGI subjective quality database to date with 20,000 AIGIs and 420,000 subjective ratings, known as AIGIQA-20K. Furthermore, we conduct benchmark experiments on this database to assess the correspondence between 16 mainstream AIGI quality models and human perception. We anticipate that this large-scale quality database will inspire robust quality indicators for AIGIs and propel the evolution of AIGC for vision. The database is released on https://www.modelscope.cn/datasets/lcysyzxdxc/AIGCQA-30K-Image.</li>
</ul>

<h3>Title: Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak  Attacks?</h3>
<ul>
<li><strong>Authors: </strong>Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03411">https://arxiv.org/abs/2404.03411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03411">https://arxiv.org/pdf/2404.03411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03411]] Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak  Attacks?(https://arxiv.org/abs/2404.03411)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found here https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md .</li>
</ul>

<h3>Title: MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with  Interleaved Visual-Textual Tokens</h3>
<ul>
<li><strong>Authors: </strong>Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03413">https://arxiv.org/abs/2404.03413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03413">https://arxiv.org/pdf/2404.03413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03413]] MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with  Interleaved Visual-Textual Tokens(https://arxiv.org/abs/2404.03413)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/</li>
</ul>

<h3>Title: Can Small Language Models Help Large Language Models Reason Better?:  LM-Guided Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, Kai-Wei Chang, Chengwei Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03414">https://arxiv.org/abs/2404.03414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03414">https://arxiv.org/pdf/2404.03414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03414]] Can Small Language Models Help Large Language Models Reason Better?:  LM-Guided Chain-of-Thought(https://arxiv.org/abs/2404.03414)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a novel framework, LM-Guided CoT, that leverages a lightweight (i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM in reasoning tasks. Specifically, the lightweight LM first generates a rationale for each input instance. The Frozen large LM is then prompted to predict a task output based on the rationale generated by the lightweight LM. Our approach is resource-efficient in the sense that it only requires training the lightweight LM. We optimize the model through 1) knowledge distillation and 2) reinforcement learning from rationale-oriented and task-oriented reward signals. We assess our method with multi-hop extractive question answering (QA) benchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our approach outperforms all baselines regarding answer prediction accuracy. We also find that reinforcement learning helps the model to produce higher-quality rationales with improved QA performance.</li>
</ul>

<h3>Title: Edisum: Summarizing and Explaining Wikipedia Edits at Scale</h3>
<ul>
<li><strong>Authors: </strong>Marija Šakota, Isaac Johnson, Guosheng Feng, Robert West</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03428">https://arxiv.org/abs/2404.03428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03428">https://arxiv.org/pdf/2404.03428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03428]] Edisum: Summarizing and Explaining Wikipedia Edits at Scale(https://arxiv.org/abs/2404.03428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>An edit summary is a succinct comment written by a Wikipedia editor explaining the nature of, and reasons for, an edit to a Wikipedia page. Edit summaries are crucial for maintaining the encyclopedia: they are the first thing seen by content moderators and help them decide whether to accept or reject an edit. Additionally, edit summaries constitute a valuable data source for researchers. Unfortunately, as we show, for many edits, summaries are either missing or incomplete. To overcome this problem and help editors write useful edit summaries, we propose a model for recommending edit summaries generated by a language model trained to produce good edit summaries given the representation of an edit diff. This is a challenging task for multiple reasons, including mixed-quality training data, the need to understand not only what was changed in the article but also why it was changed, and efficiency requirements imposed by the scale of Wikipedia. We address these challenges by curating a mix of human and synthetically generated training data and fine-tuning a generative language model sufficiently small to be used on Wikipedia at scale. Our model performs on par with human editors. Commercial large language models are able to solve this task better than human editors, but would be too expensive to run on Wikipedia at scale. More broadly, this paper showcases how language modeling technology can be used to support humans in maintaining one of the largest and most visible projects on the Web.</li>
</ul>

<h3>Title: Scaffolding Language Learning via Multi-modal Tutoring Systems with  Pedagogical Instructions</h3>
<ul>
<li><strong>Authors: </strong>Zhengyuan Liu, Stella Xin Yin, Carolyn Lee, Nancy F. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03429">https://arxiv.org/abs/2404.03429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03429">https://arxiv.org/pdf/2404.03429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03429]] Scaffolding Language Learning via Multi-modal Tutoring Systems with  Pedagogical Instructions(https://arxiv.org/abs/2404.03429)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Intelligent tutoring systems (ITSs) that imitate human tutors and aim to provide immediate and customized instructions or feedback to learners have shown their effectiveness in education. With the emergence of generative artificial intelligence, large language models (LLMs) further entitle the systems to complex and coherent conversational interactions. These systems would be of great help in language education as it involves developing skills in communication, which, however, drew relatively less attention. Additionally, due to the complicated cognitive development at younger ages, more endeavors are needed for practical uses. Scaffolding refers to a teaching technique where teachers provide support and guidance to students for learning and developing new concepts or skills. It is an effective way to support diverse learning needs, goals, processes, and outcomes. In this work, we investigate how pedagogical instructions facilitate the scaffolding in ITSs, by conducting a case study on guiding children to describe images for language learning. We construct different types of scaffolding tutoring systems grounded in four fundamental learning theories: knowledge construction, inquiry-based learning, dialogic teaching, and zone of proximal development. For qualitative and quantitative analyses, we build and refine a seven-dimension rubric to evaluate the scaffolding process. In our experiment on GPT-4V, we observe that LLMs demonstrate strong potential to follow pedagogical instructions and achieve self-paced learning in different student groups. Moreover, we extend our evaluation framework from a manual to an automated approach, paving the way to benchmark various conversational tutoring systems.</li>
</ul>

<h3>Title: Privacy Engineering From Principles to Practice: A Roadmap</h3>
<ul>
<li><strong>Authors: </strong>Frank Pallas, Katharina Koerner, Isabel Barberá, Jaap-Henk Hoepman, Meiko Jensen, Nandita Rao Narla, Nikita Samarin, Max-R. Ulbricht, Isabel Wagner, Kim Wuyts, Christian Zimmermann</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03442">https://arxiv.org/abs/2404.03442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03442">https://arxiv.org/pdf/2404.03442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03442]] Privacy Engineering From Principles to Practice: A Roadmap(https://arxiv.org/abs/2404.03442)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Privacy engineering is gaining momentum in industry and academia alike. So far, manifold low-level primitives and higher-level methods and strategies have successfully been established. Still, fostering adoption in real-world information systems calls for additional aspects to be consciously considered in research and practice.</li>
</ul>

<h3>Title: How Much Data are Enough? Investigating Dataset Requirements for  Patch-Based Brain MRI Segmentation Tasks</h3>
<ul>
<li><strong>Authors: </strong>Dongang Wang, Peilin Liu, Hengrui Wang, Heidi Beadnall, Kain Kyle, Linda Ly, Mariano Cabezas, Geng Zhan, Ryan Sullivan, Weidong Cai, Wanli Ouyang, Fernando Calamante, Michael Barnett, Chenyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03451">https://arxiv.org/abs/2404.03451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03451">https://arxiv.org/pdf/2404.03451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03451]] How Much Data are Enough? Investigating Dataset Requirements for  Patch-Based Brain MRI Segmentation Tasks(https://arxiv.org/abs/2404.03451)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Training deep neural networks reliably requires access to large-scale datasets. However, obtaining such datasets can be challenging, especially in the context of neuroimaging analysis tasks, where the cost associated with image acquisition and annotation can be prohibitive. To mitigate both the time and financial costs associated with model development, a clear understanding of the amount of data required to train a satisfactory model is crucial. This paper focuses on an early stage phase of deep learning research, prior to model development, and proposes a strategic framework for estimating the amount of annotated data required to train patch-based segmentation networks. This framework includes the establishment of performance expectations using a novel Minor Boundary Adjustment for Threshold (MinBAT) method, and standardizing patch selection through the ROI-based Expanded Patch Selection (REPS) method. Our experiments demonstrate that tasks involving regions of interest (ROIs) with different sizes or shapes may yield variably acceptable Dice Similarity Coefficient (DSC) scores. By setting an acceptable DSC as the target, the required amount of training data can be estimated and even predicted as data accumulates. This approach could assist researchers and engineers in estimating the cost associated with data collection and annotation when defining a new segmentation task based on deep neural networks, ultimately contributing to their efficient translation to real-world applications.</li>
</ul>

<h3>Title: Reevaluating Bias Detection in Language Models: The Role of Implicit  Norm</h3>
<ul>
<li><strong>Authors: </strong>Farnaz Kohankhaki, Jacob-Junqi Tian, David Emerson, Laleh Seyyed-Kalantari, Faiza Khan Khattak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03471">https://arxiv.org/abs/2404.03471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03471">https://arxiv.org/pdf/2404.03471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03471]] Reevaluating Bias Detection in Language Models: The Role of Implicit  Norm(https://arxiv.org/abs/2404.03471)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), trained on vast datasets, can carry biases that manifest in various forms, from overt discrimination to implicit stereotypes. One facet of bias is performance disparities in LLMs, often harming underprivileged groups, such as racial minorities. A common approach to quantifying bias is to use template-based bias probes, which explicitly state group membership (e.g. White) and evaluate if the outcome of a task, sentiment analysis for instance, is invariant to the change of group membership (e.g. change White race to Black). This approach is widely used in bias quantification. However, in this work, we find evidence of an unexpectedly overlooked consequence of using template-based probes for LLM bias quantification. We find that in doing so, text examples associated with White ethnicities appear to be classified as exhibiting negative sentiment at elevated rates. We hypothesize that the scenario arises artificially through a mismatch between the pre-training text of LLMs and the templates used to measure bias through reporting bias, unstated norms that imply group membership without explicit statement. Our finding highlights the potential misleading impact of varying group membership through explicit mention in bias quantification</li>
</ul>

<h3>Title: Performance of computer vision algorithms for fine-grained  classification using crowdsourced insect images</h3>
<ul>
<li><strong>Authors: </strong>Rita Pucci, Vincent J. Kalkman, Dan Stowell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03474">https://arxiv.org/abs/2404.03474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03474">https://arxiv.org/pdf/2404.03474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03474]] Performance of computer vision algorithms for fine-grained  classification using crowdsourced insect images(https://arxiv.org/abs/2404.03474)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>With fine-grained classification, we identify unique characteristics to distinguish among classes of the same super-class. We are focusing on species recognition in Insecta, as they are critical for biodiversity monitoring and at the base of many ecosystems. With citizen science campaigns, billions of images are collected in the wild. Once these are labelled, experts can use them to create distribution maps. However, the labelling process is time-consuming, which is where computer vision comes in. The field of computer vision offers a wide range of algorithms, each with its strengths and weaknesses; how do we identify the algorithm that is in line with our application? To answer this question, we provide a full and detailed evaluation of nine algorithms among deep convolutional networks (CNN), vision transformers (ViT), and locality-based vision transformers (LBVT) on 4 different aspects: classification performance, embedding quality, computational cost, and gradient activity. We offer insights that we haven't yet had in this domain proving to which extent these algorithms solve the fine-grained tasks in Insecta. We found that the ViT performs the best on inference speed and computational cost while the LBVT outperforms the others on performance and embedding quality; the CNN provide a trade-off among the metrics.</li>
</ul>

<h3>Title: Towards Automated Movie Trailer Generation</h3>
<ul>
<li><strong>Authors: </strong>Dawit Mureja Argaw, Mattia Soldan, Alejandro Pardo, Chen Zhao, Fabian Caba Heilbron, Joon Son Chung, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03477">https://arxiv.org/abs/2404.03477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03477">https://arxiv.org/pdf/2404.03477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03477]] Towards Automated Movie Trailer Generation(https://arxiv.org/abs/2404.03477)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Movie trailers are an essential tool for promoting films and attracting audiences. However, the process of creating trailers can be time-consuming and expensive. To streamline this process, we propose an automatic trailer generation framework that generates plausible trailers from a full movie by automating shot selection and composition. Our approach draws inspiration from machine translation techniques and models the movies and trailers as sequences of shots, thus formulating the trailer generation problem as a sequence-to-sequence task. We introduce Trailer Generation Transformer (TGT), a deep-learning framework utilizing an encoder-decoder architecture. TGT movie encoder is tasked with contextualizing each movie shot representation via self-attention, while the autoregressive trailer decoder predicts the feature representation of the next trailer shot, accounting for the relevance of shots' temporal order in trailers. Our TGT significantly outperforms previous methods on a comprehensive suite of metrics.</li>
</ul>

<h3>Title: Generative AI and Teachers -- For Us or Against Us? A Case Study</h3>
<ul>
<li><strong>Authors: </strong>Jenny Pettersson, Elias Hult, Tim Eriksson, Tosin Adewumi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03486">https://arxiv.org/abs/2404.03486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03486">https://arxiv.org/pdf/2404.03486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03486]] Generative AI and Teachers -- For Us or Against Us? A Case Study(https://arxiv.org/abs/2404.03486)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We present insightful results of a survey on the adoption of generative artificial intelligence (GenAI) by university teachers in their teaching activities. The transformation of education by GenAI, particularly large language models (LLMs), has been presenting both opportunities and challenges, including cheating by students. We prepared the online survey according to best practices and the questions were created by the authors, who have pedagogy experience. The survey contained 12 questions and a pilot study was first conducted. The survey was then sent to all teachers in multiple departments across different campuses of the university of interest in Sweden: Lule{\aa} University of Technology. The survey was available in both Swedish and English. The results show that 35 teachers (more than half) use GenAI out of 67 respondents. Preparation is the teaching activity with the most frequency that GenAI is used for and ChatGPT is the most commonly used GenAI. 59% say it has impacted their teaching, however, 55% say there should be legislation around the use of GenAI, especially as inaccuracies and cheating are the biggest concerns.</li>
</ul>

<h3>Title: A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded  Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Zijun Yao, Jing Zhang, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03491">https://arxiv.org/abs/2404.03491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03491">https://arxiv.org/pdf/2404.03491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03491]] A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded  Dialogue Generation(https://arxiv.org/abs/2404.03491)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.</li>
</ul>

<h3>Title: Privacy-Enhancing Technologies for Artificial Intelligence-Enabled  Systems</h3>
<ul>
<li><strong>Authors: </strong>Liv d'Aliberti, Evan Gronberg, Joseph Kovba</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03509">https://arxiv.org/abs/2404.03509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03509">https://arxiv.org/pdf/2404.03509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03509]] Privacy-Enhancing Technologies for Artificial Intelligence-Enabled  Systems(https://arxiv.org/abs/2404.03509)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) models introduce privacy vulnerabilities to systems. These vulnerabilities may impact model owners or system users; they exist during model development, deployment, and inference phases, and threats can be internal or external to the system. In this paper, we investigate potential threats and propose the use of several privacy-enhancing technologies (PETs) to defend AI-enabled systems. We then provide a framework for PETs evaluation for a AI-enabled systems and discuss the impact PETs may have on system-level variables.</li>
</ul>

<h3>Title: Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive  Model-Aware Approach</h3>
<ul>
<li><strong>Authors: </strong>Chengkai Huang, Rui Wang, Kaige Xie, Tong Yu, Lina Yao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03514">https://arxiv.org/abs/2404.03514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03514">https://arxiv.org/pdf/2404.03514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03514]] Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive  Model-Aware Approach(https://arxiv.org/abs/2404.03514)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks. Despite their great success, the knowledge provided by the retrieval process is not always useful for improving the model prediction, since in some samples LLMs may already be quite knowledgeable and thus be able to answer the question correctly without retrieval. Aiming to save the cost of retrieval, previous work has proposed to determine when to do/skip the retrieval in a data-aware manner by analyzing the LLMs' pretraining data. However, these data-aware methods pose privacy risks and memory limitations, especially when requiring access to sensitive or extensive pretraining data. Moreover, these methods offer limited adaptability under fine-tuning or continual learning settings. We hypothesize that token embeddings are able to capture the model's intrinsic knowledge, which offers a safer and more straightforward way to judge the need for retrieval without the privacy risks associated with accessing pre-training data. Moreover, it alleviates the need to retain all the data utilized during model pre-training, necessitating only the upkeep of the token embeddings. Extensive experiments and in-depth analyses demonstrate the superiority of our model-aware approach.</li>
</ul>

<h3>Title: SDPose: Tokenized Pose Estimation via Circulation-Guide  Self-Distillation</h3>
<ul>
<li><strong>Authors: </strong>Sichen Chen, Yingyi Zhang, Siming Huang, Ran Yi, Ke Fan, Ruixin Zhang, Peixian Chen, Jun Wang, Shouhong Ding, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03518">https://arxiv.org/abs/2404.03518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03518">https://arxiv.org/pdf/2404.03518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03518]] SDPose: Tokenized Pose Estimation via Circulation-Guide  Self-Distillation(https://arxiv.org/abs/2404.03518)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE). Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms. Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models. To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters. Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods. Our code is available at https://github.com/MartyrPenink/SDPose.</li>
</ul>

<h3>Title: Approximate Gradient Coding for Privacy-Flexible Federated Learning with  Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Okko Makkonen, Sampo Niemelä, Camilla Hollanti, Serge Kas Hanna</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03524">https://arxiv.org/abs/2404.03524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03524">https://arxiv.org/pdf/2404.03524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03524]] Approximate Gradient Coding for Privacy-Flexible Federated Learning with  Non-IID Data(https://arxiv.org/abs/2404.03524)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>This work focuses on the challenges of non-IID data and stragglers/dropouts in federated learning. We introduce and explore a privacy-flexible paradigm that models parts of the clients' local data as non-private, offering a more versatile and business-oriented perspective on privacy. Within this framework, we propose a data-driven strategy for mitigating the effects of label heterogeneity and client straggling on federated learning. Our solution combines both offline data sharing and approximate gradient coding techniques. Through numerical simulations using the MNIST dataset, we demonstrate that our approach enables achieving a deliberate trade-off between privacy and utility, leading to improved model convergence and accuracy while using an adaptable portion of non-private data.</li>
</ul>

<h3>Title: WeSee: Using Malicious #VC Interrupts to Break AMD SEV-SNP</h3>
<ul>
<li><strong>Authors: </strong>Benedict Schlüter, Supraja Sridhara, Andrin Bertschi, Shweta Shinde</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03526">https://arxiv.org/abs/2404.03526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03526">https://arxiv.org/pdf/2404.03526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03526]] WeSee: Using Malicious #VC Interrupts to Break AMD SEV-SNP(https://arxiv.org/abs/2404.03526)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>AMD SEV-SNP offers VM-level trusted execution environments (TEEs) to protect the confidentiality and integrity for sensitive cloud workloads from untrusted hypervisor controlled by the cloud provider. AMD introduced a new exception, #VC, to facilitate the communication between the VM and the untrusted hypervisor. We present WeSee attack, where the hypervisor injects malicious #VC into a victim VM's CPU to compromise the security guarantees of AMD SEV-SNP. Specifically, WeSee injects interrupt number 29, which delivers a #VC exception to the VM who then executes the corresponding handler that performs data and register copies between the VM and the hypervisor. WeSee shows that using well-crafted #VC injections, the attacker can induce arbitrary behavior in the VM. Our case-studies demonstrate that WeSee can leak sensitive VM information (kTLS keys for NGINX), corrupt kernel data (firewall rules), and inject arbitrary code (launch a root shell from the kernel space).</li>
</ul>

<h3>Title: HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid,  Asymmetric, and Progressive Heterogeneous Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jiahang Li, Peng Yun, Qijun Chen, Rui Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03527">https://arxiv.org/abs/2404.03527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03527">https://arxiv.org/pdf/2404.03527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03527]] HAPNet: Toward Superior RGB-Thermal Scene Parsing via Hybrid,  Asymmetric, and Progressive Heterogeneous Feature Fusion(https://arxiv.org/abs/2404.03527)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Data-fusion networks have shown significant promise for RGB-thermal scene parsing. However, the majority of existing studies have relied on symmetric duplex encoders for heterogeneous feature extraction and fusion, paying inadequate attention to the inherent differences between RGB and thermal modalities. Recent progress in vision foundation models (VFMs) trained through self-supervision on vast amounts of unlabeled data has proven their ability to extract informative, general-purpose features. However, this potential has yet to be fully leveraged in the domain. In this study, we take one step toward this new research area by exploring a feasible strategy to fully exploit VFM features for RGB-thermal scene parsing. Specifically, we delve deeper into the unique characteristics of RGB and thermal modalities, thereby designing a hybrid, asymmetric encoder that incorporates both a VFM and a convolutional neural network. This design allows for more effective extraction of complementary heterogeneous features, which are subsequently fused in a dual-path, progressive manner. Moreover, we introduce an auxiliary task to further enrich the local semantics of the fused features, thereby improving the overall performance of RGB-thermal scene parsing. Our proposed HAPNet, equipped with all these components, demonstrates superior performance compared to all other state-of-the-art RGB-thermal scene parsing networks, achieving top ranks across three widely used public RGB-thermal scene parsing datasets. We believe this new paradigm has opened up new opportunities for future developments in data-fusion scene parsing approaches.</li>
</ul>

<h3>Title: Evaluating Generative Language Models in Information Extraction as  Subjective Question Correction</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Fan, Yantao Liu, Zijun Yao, Jifan Yu, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03532">https://arxiv.org/abs/2404.03532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03532">https://arxiv.org/pdf/2404.03532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03532]] Evaluating Generative Language Models in Information Extraction as  Subjective Question Correction(https://arxiv.org/abs/2404.03532)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Modern Large Language Models (LLMs) have showcased remarkable prowess in various tasks necessitating sophisticated cognitive behaviors. Nevertheless, a paradoxical performance discrepancy is observed, where these models underperform in seemingly elementary tasks like relation extraction and event extraction due to two issues in conventional evaluation. (1) The imprecision of existing evaluation metrics that struggle to effectively gauge semantic consistency between model outputs and ground truth, and (2) The inherent incompleteness of evaluation benchmarks, primarily due to restrictive human annotation schemas, resulting in underestimated LLM performances. Inspired by the principles in subjective question correction, we propose a new evaluation method, SQC-Score. This method innovatively utilizes LLMs, fine-tuned through subjective question correction data, to refine matching between model outputs and golden labels. Additionally, by incorporating a Natural Language Inference (NLI) model, SQC-Score enriches golden labels, addressing benchmark incompleteness by acknowledging correct yet previously omitted answers. Results on three information extraction tasks show that SQC-Score is more preferred by human annotators than the baseline metrics. Utilizing SQC-Score, we conduct a comprehensive evaluation of the state-of-the-art LLMs and provide insights for future research for information extraction. Dataset and associated codes can be accessed at https://github.com/THU-KEG/SQC-Score.</li>
</ul>

<h3>Title: If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face  Recognition through Synthetic Faces</h3>
<ul>
<li><strong>Authors: </strong>Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, Mirko Marras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03537">https://arxiv.org/abs/2404.03537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03537">https://arxiv.org/pdf/2404.03537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03537]] If It's Not Enough, Make It So: Reducing Authentic Data Demand in Face  Recognition through Synthetic Faces(https://arxiv.org/abs/2404.03537)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Recent advances in deep face recognition have spurred a growing demand for large, diverse, and manually annotated face datasets. Acquiring authentic, high-quality data for face recognition has proven to be a challenge, primarily due to privacy concerns. Large face datasets are primarily sourced from web-based images, lacking explicit user consent. In this paper, we examine whether and how synthetic face data can be used to train effective face recognition models with reduced reliance on authentic images, thereby mitigating data collection concerns. First, we explored the performance gap among recent state-of-the-art face recognition models, trained with synthetic data only and authentic (scarce) data only. Then, we deepened our analysis by training a state-of-the-art backbone with various combinations of synthetic and authentic data, gaining insights into optimizing the limited use of the latter for verification accuracy. Finally, we assessed the effectiveness of data augmentation approaches on synthetic and authentic data, with the same goal in mind. Our results highlighted the effectiveness of FR trained on combined datasets, particularly when combined with appropriate augmentation techniques.</li>
</ul>

<h3>Title: How does Multi-Task Training Affect Transformer In-Context Capabilities?  Investigations with Function Classes</h3>
<ul>
<li><strong>Authors: </strong>Harmon Bhasin, Timothy Ossowski, Yiqiao Zhong, Junjie Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03558">https://arxiv.org/abs/2404.03558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03558">https://arxiv.org/pdf/2404.03558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03558]] How does Multi-Task Training Affect Transformer In-Context Capabilities?  Investigations with Function Classes(https://arxiv.org/abs/2404.03558)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) have recently shown the extraordinary ability to perform unseen tasks based on few-shot examples provided as text, also known as in-context learning (ICL). While recent works have attempted to understand the mechanisms driving ICL, few have explored training strategies that incentivize these models to generalize to multiple tasks. Multi-task learning (MTL) for generalist models is a promising direction that offers transfer learning potential, enabling large parameterized models to be trained from simpler, related tasks. In this work, we investigate the combination of MTL with ICL to build models that efficiently learn tasks while being robust to out-of-distribution examples. We propose several effective curriculum learning strategies that allow ICL models to achieve higher data efficiency and more stable convergence. Our experiments reveal that ICL models can effectively learn difficult tasks by training on progressively harder tasks while mixing in prior tasks, denoted as mixed curriculum in this work. Our code and models are available at https://github.com/harmonbhasin/curriculum_learning_icl .</li>
</ul>

<h3>Title: Personalized LLM Response Generation with Parameterized Memory Injection</h3>
<ul>
<li><strong>Authors: </strong>Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03565">https://arxiv.org/abs/2404.03565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03565">https://arxiv.org/pdf/2404.03565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03565]] Personalized LLM Response Generation with Parameterized Memory Injection(https://arxiv.org/abs/2404.03565)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \textbf{L}LM \textbf{P}ersonalization(\textbf{MiLP}).</li>
</ul>

<h3>Title: PointInfinity: Resolution-Invariant Point Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Huang, Justin Johnson, Shoubhik Debnath, James M. Rehg, Chao-Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03566">https://arxiv.org/abs/2404.03566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03566">https://arxiv.org/pdf/2404.03566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03566]] PointInfinity: Resolution-Invariant Point Diffusion Models(https://arxiv.org/abs/2404.03566)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We present PointInfinity, an efficient family of point cloud diffusion models. Our core idea is to use a transformer-based architecture with a fixed-size, resolution-invariant latent representation. This enables efficient training with low-resolution point clouds, while allowing high-resolution point clouds to be generated during inference. More importantly, we show that scaling the test-time resolution beyond the training resolution improves the fidelity of generated point clouds and surfaces. We analyze this phenomenon and draw a link to classifier-free guidance commonly used in diffusion models, demonstrating that both allow trading off fidelity and variability during inference. Experiments on CO3D show that PointInfinity can efficiently generate high-resolution point clouds (up to 131k points, 31 times more than Point-E) with state-of-the-art quality.</li>
</ul>

<h3>Title: Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning  Skills in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03577">https://arxiv.org/abs/2404.03577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03577">https://arxiv.org/pdf/2404.03577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03577]] Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning  Skills in Large Language Models(https://arxiv.org/abs/2404.03577)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Providing knowledge documents for large language models (LLMs) has emerged as a promising solution to update the static knowledge inherent in their parameters. However, knowledge in the document may conflict with the memory of LLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads to the necessity of examining the capability of LLMs to assimilate supplemental external knowledge that conflicts with their memory. While previous studies have explained to what extent LLMs extract conflicting knowledge from the provided text, they neglect the necessity to reason with conflicting knowledge. Furthermore, there lack a detailed analysis on strategies to enable LLMs to resolve conflicting knowledge via prompting, decoding strategy, and supervised fine-tuning. To address these limitations, we construct a new dataset, dubbed KNOT, for knowledge conflict resolution examination in the form of question answering. KNOT facilitates in-depth analysis by dividing reasoning with conflicting knowledge into three levels: (1) Direct Extraction, which directly extracts conflicting knowledge to answer questions. (2) Explicit Reasoning, which reasons with conflicting knowledge when the reasoning path is explicitly provided in the question. (3) Implicit Reasoning, where reasoning with conflicting knowledge requires LLMs to infer the reasoning path independently to answer questions. We also conduct extensive experiments on KNOT to establish empirical guidelines for LLMs to utilize conflicting knowledge in complex circumstances. Dataset and associated codes can be accessed at https://github.com/THU-KEG/KNOT .</li>
</ul>

<h3>Title: Distributionally Robust Reinforcement Learning with Interactive Data  Collection: Fundamental Hardness and Near-Optimal Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Miao Lu, Han Zhong, Tong Zhang, Jose Blanchet</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03578">https://arxiv.org/abs/2404.03578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03578">https://arxiv.org/pdf/2404.03578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03578]] Distributionally Robust Reinforcement Learning with Interactive Data  Collection: Fundamental Hardness and Near-Optimal Algorithm(https://arxiv.org/abs/2404.03578)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The sim-to-real gap, which represents the disparity between training and testing environments, poses a significant challenge in reinforcement learning (RL). A promising approach to addressing this challenge is distributionally robust RL, often framed as a robust Markov decision process (RMDP). In this framework, the objective is to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment. Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error. In this robust RL paradigm, two main challenges emerge: managing distributional robustness while striking a balance between exploration and exploitation during data collection. Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between the training and testing environments. To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation (TV) distance robust set, postulating that the minimal value of the optimal robust value function is zero. We prove that such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and present an algorithm with a provable sample complexity guarantee. Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for designing a sample-efficient algorithm accompanied by sharp sample complexity analysis.</li>
</ul>

<h3>Title: Leveraging Interpolation Models and Error Bounds for Verifiable  Scientific Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Tyler Chang, Andrew Gillette, Romit Maulik</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03586">https://arxiv.org/abs/2404.03586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03586">https://arxiv.org/pdf/2404.03586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03586]] Leveraging Interpolation Models and Error Bounds for Verifiable  Scientific Machine Learning(https://arxiv.org/abs/2404.03586)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Effective verification and validation techniques for modern scientific machine learning workflows are challenging to devise. Statistical methods are abundant and easily deployed, but often rely on speculative assumptions about the data and methods involved. Error bounds for classical interpolation techniques can provide mathematically rigorous estimates of accuracy, but often are difficult or impractical to determine computationally. In this work, we present a best-of-both-worlds approach to verifiable scientific machine learning by demonstrating that (1) multiple standard interpolation techniques have informative error bounds that can be computed or estimated efficiently; (2) comparative performance among distinct interpolants can aid in validation goals; (3) deploying interpolation methods on latent spaces generated by deep learning techniques enables some interpretability for black-box models. We present a detailed case study of our approach for predicting lift-drag ratios from airfoil images. Code developed for this work is available in a public Github repository.</li>
</ul>

<h3>Title: SemGrasp: Semantic Grasp Generation via Language Aligned Discretization</h3>
<ul>
<li><strong>Authors: </strong>Kailin Li, Jingbo Wang, Lixin Yang, Cewu Lu, Bo Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03590">https://arxiv.org/abs/2404.03590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03590">https://arxiv.org/pdf/2404.03590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03590]] SemGrasp: Semantic Grasp Generation via Language Aligned Discretization(https://arxiv.org/abs/2404.03590)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating natural human grasps necessitates consideration of not just object geometry but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semantic-based grasp generation method, termed SemGrasp, which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space. To facilitate the training of SemGrasp, we have compiled a large-scale, grasp-text-aligned dataset named CapGrasp, featuring about 260k detailed captions and 50k diverse grasps. Experimental findings demonstrate that SemGrasp efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset are available publicly at: https://kailinli.github.io/SemGrasp.</li>
</ul>

<h3>Title: ReFT: Representation Finetuning for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning, Christopher Potts</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03592">https://arxiv.org/abs/2404.03592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03592">https://arxiv.org/pdf/2404.03592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03592]] ReFT: Representation Finetuning for Language Models(https://arxiv.org/abs/2404.03592)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. Here, we pursue this hypothesis by developing a family of $\textbf{Representation Finetuning (ReFT)}$ methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a drop-in replacement for existing PEFTs and learns interventions that are 10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best balance of efficiency and performance, and almost always outperforms state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft.</li>
</ul>

<h3>Title: Intent Detection and Entity Extraction from BioMedical Literature</h3>
<ul>
<li><strong>Authors: </strong>Ankan Mullick, Mukur Gupta, Pawan Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03598">https://arxiv.org/abs/2404.03598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03598">https://arxiv.org/pdf/2404.03598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03598]] Intent Detection and Entity Extraction from BioMedical Literature(https://arxiv.org/abs/2404.03598)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Biomedical queries have become increasingly prevalent in web searches, reflecting the growing interest in accessing biomedical literature. Despite recent research on large-language models (LLMs) motivated by endeavours to attain generalized intelligence, their efficacy in replacing task and domain-specific natural language understanding approaches remains questionable. In this paper, we address this question by conducting a comprehensive empirical evaluation of intent detection and named entity recognition (NER) tasks from biomedical text. We show that Supervised Fine Tuned approaches are still relevant and more effective than general-purpose LLMs. Biomedical transformer models such as PubMedBERT can surpass ChatGPT on NER task with only 5 supervised examples.</li>
</ul>

<h3>Title: Evaluating LLMs at Detecting Errors in LLM Responses</h3>
<ul>
<li><strong>Authors: </strong>Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03602">https://arxiv.org/abs/2404.03602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03602">https://arxiv.org/pdf/2404.03602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03602]] Evaluating LLMs at Detecting Errors in LLM Responses(https://arxiv.org/abs/2404.03602)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of LLM responses. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs. ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans. 2) Explanations by LLM-based error detectors lack reliability. 3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve. 4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance. Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake.</li>
</ul>

<h3>Title: Sailor: Open Language Models for South-East Asia</h3>
<ul>
<li><strong>Authors: </strong>Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03608">https://arxiv.org/abs/2404.03608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03608">https://arxiv.org/pdf/2404.03608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03608]] Sailor: Open Language Models for South-East Asia(https://arxiv.org/abs/2404.03608)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present Sailor, a family of open language models ranging from 0.5B to 7B parameters, tailored for South-East Asian (SEA) languages. These models are continually pre-trained from Qwen1.5, a great language model for multilingual use cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily covering the languages of English, Chinese, Vietnamese, Thai, Indonesian, Malay, and Lao. The training leverages several techniques, including BPE dropout for improving the model robustness, aggressive data cleaning and deduplication, and small proxy models to optimize data mixture. Experimental results on four typical tasks indicate that Sailor models demonstrate strong performance across different benchmarks, including commonsense reasoning, question answering, reading comprehension and examination. Embracing the open-source spirit, we share our insights through this report to spark a wider interest in developing large language models for multilingual use cases.</li>
</ul>

<h3>Title: InsectMamba: Insect Pest Classification with State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Qianning Wang, Chenglin Wang, Zhixin Lai, Yucheng Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03611">https://arxiv.org/abs/2404.03611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03611">https://arxiv.org/pdf/2404.03611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03611]] InsectMamba: Insect Pest Classification with State Space Model(https://arxiv.org/abs/2404.03611)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>The classification of insect pests is a critical task in agricultural technology, vital for ensuring food security and environmental sustainability. However, the complexity of pest identification, due to factors like high camouflage and species diversity, poses significant obstacles. Existing methods struggle with the fine-grained feature extraction needed to distinguish between closely related pest species. Although recent advancements have utilized modified network structures and combined deep learning approaches to improve accuracy, challenges persist due to the similarity between pests and their surroundings. To address this problem, we introduce InsectMamba, a novel approach that integrates State Space Models (SSMs), Convolutional Neural Networks (CNNs), Multi-Head Self-Attention mechanism (MSA), and Multilayer Perceptrons (MLPs) within Mix-SSM blocks. This integration facilitates the extraction of comprehensive visual features by leveraging the strengths of each encoding strategy. A selective module is also proposed to adaptively aggregate these features, enhancing the model's ability to discern pest characteristics. InsectMamba was evaluated against strong competitors across five insect pest classification datasets. The results demonstrate its superior performance and verify the significance of each model component by an ablation study.</li>
</ul>

<h3>Title: DeViDe: Faceted medical knowledge for improved medical vision-language  pre-training</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Luo, Ziyu Zhou, Corentin Royer, Anjany Sekuboyina, Bjoern Menze</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03618">https://arxiv.org/abs/2404.03618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03618">https://arxiv.org/pdf/2404.03618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03618]] DeViDe: Faceted medical knowledge for improved medical vision-language  pre-training(https://arxiv.org/abs/2404.03618)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision-language pre-training for chest X-rays has made significant strides, primarily by utilizing paired radiographs and radiology reports. However, existing approaches often face challenges in encoding medical knowledge effectively. While radiology reports provide insights into the current disease manifestation, medical definitions (as used by contemporary methods) tend to be overly abstract, creating a gap in knowledge. To address this, we propose DeViDe, a novel transformer-based method that leverages radiographic descriptions from the open web. These descriptions outline general visual characteristics of diseases in radiographs, and when combined with abstract definitions and radiology reports, provide a holistic snapshot of knowledge. DeViDe incorporates three key features for knowledge-augmented vision language alignment: First, a large-language model-based augmentation is employed to homogenise medical knowledge from diverse sources. Second, this knowledge is aligned with image information at various levels of granularity. Third, a novel projection layer is proposed to handle the complexity of aligning each image with multiple descriptions arising in a multi-label setting. In zero-shot settings, DeViDe performs comparably to fully supervised models on external datasets and achieves state-of-the-art results on three large-scale datasets. Additionally, fine-tuning DeViDe on four downstream tasks and six segmentation tasks showcases its superior performance across data from diverse distributions.</li>
</ul>

<h3>Title: LCM-Lookahead for Encoder-based Text-to-Image Personalization</h3>
<ul>
<li><strong>Authors: </strong>Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03620">https://arxiv.org/abs/2404.03620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03620">https://arxiv.org/pdf/2404.03620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03620]] LCM-Lookahead for Encoder-based Text-to-Image Personalization(https://arxiv.org/abs/2404.03620)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have introduced fast sampling methods that can effectively produce high-quality images in just one or a few denoising steps. Interestingly, when these are distilled from existing diffusion models, they often maintain alignment with the original model, retaining similar outputs for similar prompts and seeds. These properties present opportunities to leverage fast sampling methods as a shortcut-mechanism, using them to create a preview of denoised outputs through which we can backpropagate image-space losses. In this work, we explore the potential of using such shortcut-mechanisms to guide the personalization of text-to-image models to specific facial identities. We focus on encoder-based personalization approaches, and demonstrate that by tuning them with a lookahead identity loss, we can achieve higher identity fidelity, without sacrificing layout diversity or prompt alignment. We further explore the use of attention sharing mechanisms and consistent data generation for the task of personalization, and find that encoder training can benefit from both.</li>
</ul>

<h3>Title: Visualization-of-Thought Elicits Spatial Reasoning in Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03622">https://arxiv.org/abs/2404.03622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03622">https://arxiv.org/pdf/2404.03622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03622]] Visualization-of-Thought Elicits Spatial Reasoning in Large Language  Models(https://arxiv.org/abs/2404.03622)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as \textbf{the Mind's Eye}, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (\textbf{VoT}) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate \textit{mental images} to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.</li>
</ul>

<h3>Title: Unveiling LLMs: The Evolution of Latent Representations in a Temporal  Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03623">https://arxiv.org/abs/2404.03623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03623">https://arxiv.org/pdf/2404.03623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03623]] Unveiling LLMs: The Evolution of Latent Representations in a Temporal  Knowledge Graph(https://arxiv.org/abs/2404.03623)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of common factual knowledge information. However, unravelling the underlying reasoning of LLMs and explaining their internal mechanisms of exploiting this factual knowledge remain active areas of investigation. Our work analyzes the factual knowledge encoded in the latent representation of LLMs when prompted to assess the truthfulness of factual claims. We propose an end-to-end framework that jointly decodes the factual knowledge embedded in the latent space of LLMs from a vector space to a set of ground predicates and represents its evolution across the layers using a temporal knowledge graph. Our framework relies on the technique of activation patching which intervenes in the inference computation of a model by dynamically altering its latent representations. Consequently, we neither rely on external models nor training processes. We showcase our framework with local and global interpretability analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The local interpretability analysis exposes different latent errors from representation to multi-hop reasoning errors. On the other hand, the global analysis uncovered patterns in the underlying evolution of the model's factual knowledge (e.g., store-and-seek factual information). By enabling graph-based analyses of the latent representations, this work represents a step towards the mechanistic interpretability of LLMs.</li>
</ul>

<h3>Title: Training LLMs over Neurally Compressed Text</h3>
<ul>
<li><strong>Authors: </strong>Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03626">https://arxiv.org/abs/2404.03626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03626">https://arxiv.org/pdf/2404.03626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03626]] Training LLMs over Neurally Compressed Text(https://arxiv.org/abs/2404.03626)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.</li>
</ul>

<h3>Title: Robust Concept Erasure Using Task Vectors</h3>
<ul>
<li><strong>Authors: </strong>Minh Pham, Kelly O. Marshall, Chinmay Hegde, Niv Cohen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03631">https://arxiv.org/abs/2404.03631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03631">https://arxiv.org/pdf/2404.03631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03631]] Robust Concept Erasure Using Task Vectors(https://arxiv.org/abs/2404.03631)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>With the rapid growth of text-to-image models, a variety of techniques have been suggested to prevent undesirable image generations. Yet, these methods often only protect against specific user prompts and have been shown to allow unsafe generations with other inputs. Here we focus on unconditionally erasing a concept from a text-to-image model rather than conditioning the erasure on the user's prompt. We first show that compared to input-dependent erasure methods, concept erasure that uses Task Vectors (TV) is more robust to unexpected user inputs, not seen during training. However, TV-based erasure can also affect the core performance of the edited model, particularly when the required edit strength is unknown. To this end, we propose a method called Diverse Inversion, which we use to estimate the required strength of the TV edit. Diverse Inversion finds within the model input space a large set of word embeddings, each of which induces the generation of the target concept. We find that encouraging diversity in the set makes our estimation more robust to unexpected prompts. Finally, we show that Diverse Inversion enables us to apply a TV edit only to a subset of the model weights, enhancing the erasure capabilities while better maintaining the core functionality of the model.</li>
</ul>

<h3>Title: Reference-Based 3D-Aware Image Editing with Triplane</h3>
<ul>
<li><strong>Authors: </strong>Bahri Batuhan Bilecen, Yigit Yalin, Ning Yu, Aysegul Dundar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03632">https://arxiv.org/abs/2404.03632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03632">https://arxiv.org/pdf/2404.03632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03632]] Reference-Based 3D-Aware Image Editing with Triplane(https://arxiv.org/abs/2404.03632)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) have emerged as powerful tools not only for high-quality image generation but also for real image editing through manipulation of their interpretable latent spaces. Recent advancements in GANs include the development of 3D-aware models such as EG3D, characterized by efficient triplane-based architectures enabling the reconstruction of 3D geometry from single images. However, scant attention has been devoted to providing an integrated framework for high-quality reference-based 3D-aware image editing within this domain. This study addresses this gap by exploring and demonstrating the effectiveness of EG3D's triplane space for achieving advanced reference-based edits, presenting a unique perspective on 3D-aware image editing through our novel pipeline. Our approach integrates the encoding of triplane features, spatial disentanglement and automatic localization of features in the triplane domain, and fusion learning for desired image editing. Moreover, our framework demonstrates versatility across domains, extending its effectiveness to animal face edits and partial stylization of cartoon portraits. The method shows significant improvements over relevant 3D-aware latent editing and 2D reference-based editing methods, both qualitatively and quantitatively. Project page: https://three-bee.github.io/triplane_edit</li>
</ul>

<h3>Title: DiffBody: Human Body Restoration by Imagining with Generative Diffusion  Prior</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhang, Zhe Wang, Xinjie Li, Yunchen Yuan, Chengsong Zhang, Xiao Sun, Zhihang Zhong, Jian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03642">https://arxiv.org/abs/2404.03642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03642">https://arxiv.org/pdf/2404.03642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03642]] DiffBody: Human Body Restoration by Imagining with Generative Diffusion  Prior(https://arxiv.org/abs/2404.03642)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the diffusion model's focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods.</li>
</ul>

<h3>Title: Decoupling Static and Hierarchical Motion Perception for Referring Video  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shuting He, Henghui Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03645">https://arxiv.org/abs/2404.03645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03645">https://arxiv.org/pdf/2404.03645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03645]] Decoupling Static and Hierarchical Motion Perception for Referring Video  Segmentation(https://arxiv.org/abs/2404.03645)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Referring video segmentation relies on natural language expressions to identify and segment objects, often emphasizing motion clues. Previous works treat a sentence as a whole and directly perform identification at the video-level, mixing up static image-level cues with temporal motion cues. However, image-level features cannot well comprehend motion cues in sentences, and static cues are not crucial for temporal perception. In fact, static cues can sometimes interfere with temporal perception by overshadowing motion cues. In this work, we propose to decouple video-level referring expression understanding into static and motion perception, with a specific emphasis on enhancing temporal comprehension. Firstly, we introduce an expression-decoupling module to make static cues and motion cues perform their distinct role, alleviating the issue of sentence embeddings overlooking motion cues. Secondly, we propose a hierarchical motion perception module to capture temporal information effectively across varying timescales. Furthermore, we employ contrastive learning to distinguish the motions of visually similar objects. These contributions yield state-of-the-art performance across five datasets, including a remarkable $\textbf{9.2%}$ $\mathcal{J\&F}$ improvement on the challenging $\textbf{MeViS}$ dataset. Code is available at https://github.com/heshuting555/DsHmp.</li>
</ul>

<h3>Title: Locating and Editing Factual Associations in Mamba</h3>
<ul>
<li><strong>Authors: </strong>Arnab Sen Sharma, David Atkinson, David Bau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03646">https://arxiv.org/abs/2404.03646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03646">https://arxiv.org/pdf/2404.03646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03646]] Locating and Editing Factual Associations in Mamba(https://arxiv.org/abs/2404.03646)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We investigate the mechanisms of factual recall in the Mamba state space model. Our work is inspired by previous findings in autoregressive transformer language models suggesting that their knowledge recall is localized to particular modules at specific token locations; we therefore ask whether factual recall in Mamba can be similarly localized. To investigate this, we conduct four lines of experiments on Mamba. First, we apply causal tracing or interchange interventions to localize key components inside Mamba that are responsible for recalling facts, revealing that specific components within middle layers show strong causal effects at the last token of the subject, while the causal effect of intervening on later layers is most pronounced at the last token of the prompt, matching previous findings on autoregressive transformers. Second, we show that rank-one model editing methods can successfully insert facts at specific locations, again resembling findings on transformer models. Third, we examine the linearity of Mamba's representations of factual relations. Finally we adapt attention-knockout techniques to Mamba to dissect information flow during factual recall. We compare Mamba directly to a similar-sized transformer and conclude that despite significant differences in architectural approach, when it comes to factual recall, the two architectures share many similarities.</li>
</ul>

<h3>Title: AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web  Navigating Agent</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03648">https://arxiv.org/abs/2404.03648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03648">https://arxiv.org/pdf/2404.03648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03648]] AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web  Navigating Agent(https://arxiv.org/abs/2404.03648)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have fueled many intelligent agent tasks, such as web navigation -- but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web. In light of the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly. We employ a hybrid human-AI method to build web browsing data for curriculum training. Then, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For testing, we establish a bilingual benchmark -- AutoWebBench -- for real-world web browsing tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, revealing its improvements but also underlying challenges to tackle real environments. Related code, model, and data will be released at \url{https://github.com/THUDM/AutoWebGLM}.</li>
</ul>

<h3>Title: OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features  and Rendered Novel Views</h3>
<ul>
<li><strong>Authors: </strong>Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Marc Pollefeys, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03650">https://arxiv.org/abs/2404.03650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03650">https://arxiv.org/pdf/2404.03650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03650]] OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features  and Rendered Novel Views(https://arxiv.org/abs/2404.03650)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-set segmentation in 3D scenes have appeared in the literature. These methods are heavily influenced by closed-set 3D convolutional approaches that process point clouds or polygon meshes. However, these 3D scene representations do not align well with the image-based nature of the visual-language models. Indeed, point cloud and 3D meshes typically have a lower resolution than images and the reconstructed 3D scene geometry might not project well to the underlying 2D image sequences used to compute pixel-aligned CLIP features. To address these challenges, we propose OpenNeRF which naturally operates on posed images and directly encodes the VLM features within the NeRF. This is similar in spirit to LERF, however our work shows that using pixel-wise VLM features (instead of global CLIP features) results in an overall less complex architecture without the need for additional DINO regularization. Our OpenNeRF further leverages NeRF's ability to render novel views and extract open-set VLM features from areas that are not well observed in the initial posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF outperforms recent open-vocabulary methods such as LERF and OpenScene by at least +4.9 mIoU.</li>
</ul>

<h3>Title: The More You See in 2D, the More You Perceive in 3D</h3>
<ul>
<li><strong>Authors: </strong>Xinyang Han, Zelin Gao, Angjoo Kanazawa, Shubham Goel, Yossi Gandelsman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03652">https://arxiv.org/abs/2404.03652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03652">https://arxiv.org/pdf/2404.03652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03652]] The More You See in 2D, the More You Perceive in 3D(https://arxiv.org/abs/2404.03652)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Humans can infer 3D structure from 2D images of an object based on past experience and improve their 3D understanding as they see more images. Inspired by this behavior, we introduce SAP3D, a system for 3D reconstruction and novel view synthesis from an arbitrary number of unposed images. Given a few unposed images of an object, we adapt a pre-trained view-conditioned diffusion model together with the camera poses of the images via test-time fine-tuning. The adapted diffusion model and the obtained camera poses are then utilized as instance-specific priors for 3D reconstruction and novel view synthesis. We show that as the number of input images increases, the performance of our approach improves, bridging the gap between optimization-based prior-less 3D reconstruction methods and single-image-to-3D diffusion-based methods. We demonstrate our system on real images as well as standard synthetic benchmarks. Our ablation studies confirm that this adaption behavior is key for more accurate 3D understanding.</li>
</ul>

<h3>Title: CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept  Matching</h3>
<ul>
<li><strong>Authors: </strong>Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03653">https://arxiv.org/abs/2404.03653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03653">https://arxiv.org/pdf/2404.03653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03653]] CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept  Matching(https://arxiv.org/abs/2404.03653)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the diffusion model's insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism. We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance.</li>
</ul>

<h3>Title: RaFE: Generative Radiance Fields Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03654">https://arxiv.org/abs/2404.03654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03654">https://arxiv.org/pdf/2404.03654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03654]] RaFE: Generative Radiance Fields Restoration(https://arxiv.org/abs/2404.03654)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website https://zkaiwu.github.io/RaFE-Project/.</li>
</ul>

<h3>Title: MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</h3>
<ul>
<li><strong>Authors: </strong>Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03656">https://arxiv.org/abs/2404.03656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03656">https://arxiv.org/pdf/2404.03656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03656]] MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation(https://arxiv.org/abs/2404.03656)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods. We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches.</li>
</ul>

<h3>Title: OW-VISCap: Open-World Video Instance Segmentation and Captioning</h3>
<ul>
<li><strong>Authors: </strong>Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.03657">https://arxiv.org/abs/2404.03657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.03657">https://arxiv.org/pdf/2404.03657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.03657]] OW-VISCap: Open-World Video Instance Segmentation and Captioning(https://arxiv.org/abs/2404.03657)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Open-world video instance segmentation is an important video understanding task. Yet most methods either operate in a closed-world setting, require an additional user-input, or use classic region-based proposals to identify never before seen objects. Further, these methods only assign a one-word label to detected objects, and don't generate rich object-centric descriptions. They also often suffer from highly overlapping predictions. To address these issues, we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap), an approach to jointly segment, track, and caption previously seen or unseen objects in a video. For this, we introduce open-world object queries to discover never before seen objects without additional user-input. We generate rich and descriptive object-centric captions for each detected object via a masked attention augmented LLM input. We introduce an inter-query contrastive loss to ensure that the object queries differ from one another. Our generalized approach matches or surpasses state-of-the-art on three tasks: open-world video instance segmentation on the BURST dataset, dense video object captioning on the VidSTG dataset, and closed-world video instance segmentation on the OVIS dataset.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
