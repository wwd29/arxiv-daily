<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets. (arXiv:2310.01438v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01438">http://arxiv.org/abs/2310.01438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01438]] Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets(http://arxiv.org/abs/2310.01438)</code></li>
<li>Summary: <p>The advancements in data acquisition, storage, and processing techniques have
resulted in the rapid growth of heterogeneous medical data. Integrating
radiological scans, histopathology images, and molecular information with
clinical data is essential for developing a holistic understanding of the
disease and optimizing treatment. The need for integrating data from multiple
sources is further pronounced in complex diseases such as cancer for enabling
precision medicine and personalized treatments. This work proposes Multimodal
Integration of Oncology Data System (MINDS) - a flexible, scalable, and
cost-effective metadata framework for efficiently fusing disparate data from
public sources such as the Cancer Research Data Commons (CRDC) into an
interconnected, patient-centric framework. MINDS offers an interface for
exploring relationships across data types and building cohorts for developing
large-scale multimodal machine learning models. By harmonizing multimodal data,
MINDS aims to potentially empower researchers with greater analytical ability
to uncover diagnostic and prognostic insights and enable evidence-based
personalized care. MINDS tracks granular end-to-end data provenance, ensuring
reproducibility and transparency. The cloud-native architecture of MINDS can
handle exponential data growth in a secure, cost-optimized manner while
ensuring substantial storage optimization, replication avoidance, and dynamic
access capabilities. Auto-scaling, access controls, and other mechanisms
guarantee pipelines' scalability and security. MINDS overcomes the limitations
of existing biomedical data silos via an interoperable metadata-driven approach
that represents a pivotal step toward the future of oncology data integration.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Dynamic Spatio-Temporal Summarization using Information Based Fusion. (arXiv:2310.01617v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01617">http://arxiv.org/abs/2310.01617</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01617]] Dynamic Spatio-Temporal Summarization using Information Based Fusion(http://arxiv.org/abs/2310.01617)</code></li>
<li>Summary: <p>In the era of burgeoning data generation, managing and storing large-scale
time-varying datasets poses significant challenges. With the rise of
supercomputing capabilities, the volume of data produced has soared,
intensifying storage and I/O overheads. To address this issue, we propose a
dynamic spatio-temporal data summarization technique that identifies
informative features in key timesteps and fuses less informative ones. This
approach minimizes storage requirements while preserving data dynamics. Unlike
existing methods, our method retains both raw and summarized timesteps,
ensuring a comprehensive view of information changes over time. We utilize
information-theoretic measures to guide the fusion process, resulting in a
visual representation that captures essential data patterns. We demonstrate the
versatility of our technique across diverse datasets, encompassing
particle-based flow simulations, security and surveillance applications, and
biological cell interactions within the immune system. Our research
significantly contributes to the realm of data management, introducing enhanced
efficiency and deeper insights across diverse multidisciplinary domains. We
provide a streamlined approach for handling massive datasets that can be
applied to in situ analysis as well as post hoc analysis. This not only
addresses the escalating challenges of data storage and I/O overheads but also
unlocks the potential for informed decision-making. Our method empowers
researchers and experts to explore essential temporal dynamics while minimizing
storage requirements, thereby fostering a more effective and intuitive
understanding of complex data behaviors.
</p></li>
</ul>

<h3>Title: Risk and Threat Mitigation Techniques in Internet of Things (IoT) Environments: A Survey. (arXiv:2310.01676v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01676">http://arxiv.org/abs/2310.01676</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01676]] Risk and Threat Mitigation Techniques in Internet of Things (IoT) Environments: A Survey(http://arxiv.org/abs/2310.01676)</code></li>
<li>Summary: <p>Security in the Internet of Things (IoT) remains a predominant area of
concern. This survey updates the state of the art covered in previous surveys
and focuses on defending against threats rather than on the threats alone. This
area is less extensively covered by other surveys and warrants particular
attention. A life-cycle approach is adopted, articulated to form a "defence in
depth" strategy against malicious actors compromising an IoT network laterally
within it and from it. This study highlights the challenges of each mitigation
step, emphasises novel perspectives, and reconnects the discussed mitigation
steps to the ground principles they seek to implement.
</p></li>
</ul>

<h3>Title: 5G Network Slicing: Analysis of Multiple Machine Learning Classifiers. (arXiv:2310.01747v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01747">http://arxiv.org/abs/2310.01747</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01747]] 5G Network Slicing: Analysis of Multiple Machine Learning Classifiers(http://arxiv.org/abs/2310.01747)</code></li>
<li>Summary: <p>The division of one physical 5G communications infrastructure into several
virtual network slices with distinct characteristics such as bandwidth,
latency, reliability, security, and service quality is known as 5G network
slicing. Each slice is a separate logical network that meets the requirements
of specific services or use cases, such as virtual reality, gaming, autonomous
vehicles, or industrial automation. The network slice can be adjusted
dynamically to meet the changing demands of the service, resulting in a more
cost-effective and efficient approach to delivering diverse services and
applications over a shared infrastructure. This paper assesses various machine
learning techniques, including the logistic regression model, linear
discriminant model, k-nearest neighbor's model, decision tree model, random
forest model, SVC BernoulliNB model, and GaussianNB model, to investigate the
accuracy and precision of each model on detecting network slices. The report
also gives an overview of 5G network slicing.
</p></li>
</ul>

<h3>Title: Enhancing Workflow Security in Multi-Cloud Environments through Monitoring and Adaptation upon Cloud Service and Network Security Violations. (arXiv:2310.01878v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01878">http://arxiv.org/abs/2310.01878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01878]] Enhancing Workflow Security in Multi-Cloud Environments through Monitoring and Adaptation upon Cloud Service and Network Security Violations(http://arxiv.org/abs/2310.01878)</code></li>
<li>Summary: <p>Cloud computing has emerged as a crucial solution for handling data- and
compute-intensive workflows, offering scalability to address dynamic demands.
However, ensuring the secure execution of workflows in the untrusted
multi-cloud environment poses significant challenges, given the sensitive
nature of the involved data and tasks. The lack of comprehensive approaches for
detecting attacks during workflow execution, coupled with inadequate measures
for reacting to security and privacy breaches has been identified in the
literature. To close this gap, in this work, we propose an approach that
focuses on monitoring cloud services and networks to detect security violations
during workflow executions. Upon detection, our approach selects the optimal
adaptation action to minimize the impact on the workflow. To mitigate the
uncertain cost associated with such adaptations and their potential impact on
other tasks in the workflow, we employ adaptive learning to determine the most
suitable adaptation action. Our approach is evaluated based on the performance
of the detection procedure and the impact of the selected adaptations on the
workflows.
</p></li>
</ul>

<h3>Title: Gotta Catch 'em All: Aggregating CVSS Scores. (arXiv:2310.02062v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02062">http://arxiv.org/abs/2310.02062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02062]] Gotta Catch 'em All: Aggregating CVSS Scores(http://arxiv.org/abs/2310.02062)</code></li>
<li>Summary: <p>Security metrics are not standardized, but inter-national proposals such as
the Common Vulnerability ScoringSystem (CVSS) for quantifying the severity of
known vulnerabil-ities are widely used. Many CVSS aggregation mechanisms
havebeen proposed in the literature. Nevertheless, factors related tothe
context of the System Under Test (SUT) are not taken intoaccount in the
aggregation process; vulnerabilities that in theoryaffect the SUT, but are not
exploitable in reality. We propose aCVSS aggregation algorithm that integrates
information aboutthe functionality disruption of the SUT, exploitation
difficulty,existence of exploits, and the context where the SUT operates.The
aggregation algorithm was applied to OpenPLC V3, showingthat it is capable of
filtering out vulnerabilities that cannot beexploited in the real conditions of
deployment of the particularsystem. Finally, because of the nature of the
proposed algorithm,the result can be interpreted in the same way as a normal
CVSS.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01424">http://arxiv.org/abs/2310.01424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01424]] Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey(http://arxiv.org/abs/2310.01424)</code></li>
<li>Summary: <p>Rapid advancements in language models (LMs) have led to their adoption across
many sectors. Alongside the potential benefits, such models present a range of
risks, including around privacy. In particular, as LMs have grown in size, the
potential to memorise aspects of their training data has increased, resulting
in the risk of leaking private information. As LMs become increasingly
widespread, it is vital that we understand such privacy risks and how they
might be mitigated. To help researchers and policymakers understand the state
of knowledge around privacy attacks and mitigations, including where more work
is needed, we present the first technical survey on LM privacy. We (i) identify
a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey
existing attacks and use our taxonomy of dimensions to highlight key trends,
(iii) discuss existing mitigation strategies, highlighting their strengths and
limitations, identifying key gaps and demonstrating open problems and areas for
concern.
</p></li>
</ul>

<h3>Title: Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. (arXiv:2310.01434v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01434">http://arxiv.org/abs/2310.01434</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01434]] Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile(http://arxiv.org/abs/2310.01434)</code></li>
<li>Summary: <p>The field of Artificial Intelligence has witnessed remarkable progress in
recent years, especially with the emergence of powerful large language models
(LLMs) based on the transformer architecture. Cloud-based LLMs, such as
OpenAI's ChatGPT, offer impressive capabilities but come with concerns
regarding latency and privacy due to network dependencies. This article
presents an innovative approach to LLM inference, envisioning a future where
LLMs with billions of parameters can be executed directly on mobile devices
without network connectivity. The article showcases a fine-tuned GPT LLM with 3
billion parameters that can operate smoothly on devices with as low as 4GB of
memory. Through the integration of native code and model quantization
techniques, the application not only serves as a general-purpose assistant but
also facilitates seamless mobile interactions with text-to-actions features.
The article provides insights into the training pipeline, implementation
details, test results, and future directions of on-device LLM inference. This
breakthrough technology opens up possibilities for empowering users with
sophisticated AI capabilities while preserving their privacy and eliminating
latency concerns.
</p></li>
</ul>

<h3>Title: Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine Learning. (arXiv:2310.01664v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01664">http://arxiv.org/abs/2310.01664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01664]] Artemis: HE-Aware Training for Efficient Privacy-Preserving Machine Learning(http://arxiv.org/abs/2310.01664)</code></li>
<li>Summary: <p>Privacy-Preserving ML (PPML) based on Homomorphic Encryption (HE) is a
promising foundational privacy technology. Making it more practical requires
lowering its computational cost, especially, in handling modern large deep
neural networks. Model compression via pruning is highly effective in
conventional plaintext ML but cannot be effectively applied to HE-PPML as is.
</p>
<p>We propose Artemis, a highly effective DNN pruning technique for HE-based
inference. We judiciously investigate two HE-aware pruning strategies
(positional and diagonal) to reduce the number of Rotation operations, which
dominate compute time in HE convolution. We find that Pareto-optimal solutions
are based fully on diagonal pruning. Artemis' benefits come from coupling DNN
training, driven by a novel group Lasso regularization objective, with pruning
to maximize HE-specific cost reduction (dominated by the Rotation operations).
We show that Artemis improves on prior HE-oriented pruning and can achieve a
1.2-6x improvement when targeting modern convolutional models (ResNet18 and
ResNet18) across three datasets.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Constructing Image-Text Pair Dataset from Books. (arXiv:2310.01936v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01936">http://arxiv.org/abs/2310.01936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01936]] Constructing Image-Text Pair Dataset from Books(http://arxiv.org/abs/2310.01936)</code></li>
<li>Summary: <p>Digital archiving is becoming widespread owing to its effectiveness in
protecting valuable books and providing knowledge to many people
electronically. In this paper, we propose a novel approach to leverage digital
archives for machine learning. If we can fully utilize such digitized data,
machine learning has the potential to uncover unknown insights and ultimately
acquire knowledge autonomously, just like humans read books. As a first step,
we design a dataset construction pipeline comprising an optical character
reader (OCR), an object detector, and a layout analyzer for the autonomous
extraction of image-text pairs. In our experiments, we apply our pipeline on
old photo books to construct an image-text pair dataset, showing its
effectiveness in image-text retrieval and insight extraction.
</p></li>
</ul>

<h3>Title: Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features. (arXiv:2310.01679v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01679">http://arxiv.org/abs/2310.01679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01679]] Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features(http://arxiv.org/abs/2310.01679)</code></li>
<li>Summary: <p>The vast majority of techniques to train fair models require access to the
protected attribute (e.g., race, gender), either at train time or in
production. However, in many important applications this protected attribute is
largely unavailable. In this paper, we develop methods for measuring and
reducing fairness violations in a setting with limited access to protected
attribute labels. Specifically, we assume access to protected attribute labels
on a small subset of the dataset of interest, but only probabilistic estimates
of protected attribute labels (e.g., via Bayesian Improved Surname Geocoding)
for the rest of the dataset. With this setting in mind, we propose a method to
estimate bounds on common fairness metrics for an existing model, as well as a
method for training a model to limit fairness violations by solving a
constrained non-convex optimization problem. Unlike similar existing
approaches, our methods take advantage of contextual information --
specifically, the relationships between a model's predictions and the
probabilistic prediction of protected attributes, given the true protected
attribute, and vice versa -- to provide tighter bounds on the true disparity.
We provide an empirical illustration of our methods using voting data. First,
we show our measurement method can bound the true disparity up to 5.5x tighter
than previous methods in these applications. Then, we demonstrate that our
training technique effectively reduces disparity while incurring lesser
fairness-accuracy trade-offs than other fair optimization methods with limited
access to protected attributes.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Fooling the Textual Fooler via Randomizing Latent Representations. (arXiv:2310.01452v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01452">http://arxiv.org/abs/2310.01452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01452]] Fooling the Textual Fooler via Randomizing Latent Representations(http://arxiv.org/abs/2310.01452)</code></li>
<li>Summary: <p>Despite outstanding performance in a variety of NLP tasks, recent studies
have revealed that NLP models are vulnerable to adversarial attacks that
slightly perturb the input to cause the models to misbehave. Among these
attacks, adversarial word-level perturbations are well-studied and effective
attack strategies. Since these attacks work in black-box settings, they do not
require access to the model architecture or model parameters and thus can be
detrimental to existing NLP applications. To perform an attack, the adversary
queries the victim model many times to determine the most important words in an
input text and to replace these words with their corresponding synonyms. In
this work, we propose a lightweight and attack-agnostic defense whose main goal
is to perplex the process of generating an adversarial example in these
query-based black-box attacks; that is to fool the textual fooler. This
defense, named AdvFooler, works by randomizing the latent representation of the
input at inference time. Different from existing defenses, AdvFooler does not
necessitate additional computational overhead during training nor relies on
assumptions about the potential adversarial perturbation set while having a
negligible impact on the model's accuracy. Our theoretical and empirical
analyses highlight the significance of robustness resulting from confusing the
adversary via randomizing the latent space, as well as the impact of
randomization on clean accuracy. Finally, we empirically demonstrate near
state-of-the-art robustness of AdvFooler against representative adversarial
word-level attacks on two benchmark datasets.
</p></li>
</ul>

<h3>Title: LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01469">http://arxiv.org/abs/2310.01469</a></li>
<li>Code URL: https://github.com/pku-yuangroup/hallucination-attack</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01469]] LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples(http://arxiv.org/abs/2310.01469)</code></li>
<li>Summary: <p>Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be
knowledgeable and able to adapt to many tasks. However, we still can not
completely trust their answer, since LLMs suffer from
hallucination--fabricating non-existent facts to cheat users without
perception. And the reasons for their existence and pervasiveness remain
unclear. In this paper, we demonstrate that non-sense prompts composed of
random tokens can also elicit the LLMs to respond with hallucinations. This
phenomenon forces us to revisit that hallucination may be another view of
adversarial examples, and it shares similar features with conventional
adversarial examples as the basic feature of LLMs. Therefore, we formalize an
automatic hallucination triggering method as the hallucination attack in an
adversarial way. Finally, we explore basic feature of attacked adversarial
prompts and propose a simple yet effective defense strategy. Our code is
released on GitHub.
</p></li>
</ul>

<h3>Title: Decision-Dominant Strategic Defense Against Lateral Movement for 5G Zero-Trust Multi-Domain Networks. (arXiv:2310.01675v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01675">http://arxiv.org/abs/2310.01675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01675]] Decision-Dominant Strategic Defense Against Lateral Movement for 5G Zero-Trust Multi-Domain Networks(http://arxiv.org/abs/2310.01675)</code></li>
<li>Summary: <p>Multi-domain warfare is a military doctrine that leverages capabilities from
different domains, including air, land, sea, space, and cyberspace, to create a
highly interconnected battle network that is difficult for adversaries to
disrupt or defeat. However, the adoption of 5G technologies on battlefields
presents new vulnerabilities due to the complexity of interconnections and the
diversity of software, hardware, and devices from different supply chains.
Therefore, establishing a zero-trust architecture for 5G-enabled networks is
crucial for continuous monitoring and fast data analytics to protect against
targeted attacks. To address these challenges, we propose a proactive
end-to-end security scheme that utilizes a 5G satellite-guided air-ground
network. Our approach incorporates a decision-dominant learning-based method
that can thwart the lateral movement of adversaries targeting critical assets
on the battlefield before they can conduct reconnaissance or gain necessary
access or credentials. We demonstrate the effectiveness of our game-theoretic
design, which uses a meta-learning framework to enable zero-trust monitoring
and decision-dominant defense against attackers in emerging multi-domain
battlefield networks.
</p></li>
</ul>

<h3>Title: Towards Stable Backdoor Purification through Feature Shift Tuning. (arXiv:2310.01875v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01875">http://arxiv.org/abs/2310.01875</a></li>
<li>Code URL: https://github.com/aisafety-hkust/stable_backdoor_purification</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01875]] Towards Stable Backdoor Purification through Feature Shift Tuning(http://arxiv.org/abs/2310.01875)</code></li>
<li>Summary: <p>It has been widely observed that deep neural networks (DNN) are vulnerable to
backdoor attacks where attackers could manipulate the model behavior
maliciously by tampering with a small set of training samples. Although a line
of defense methods is proposed to mitigate this threat, they either require
complicated modifications to the training process or heavily rely on the
specific model architecture, which makes them hard to deploy into real-world
applications. Therefore, in this paper, we instead start with fine-tuning, one
of the most common and easy-to-deploy backdoor defenses, through comprehensive
evaluations against diverse attack scenarios. Observations made through initial
experiments show that in contrast to the promising defensive results on high
poisoning rates, vanilla tuning methods completely fail at low poisoning rate
scenarios. Our analysis shows that with the low poisoning rate, the
entanglement between backdoor and clean features undermines the effect of
tuning-based defenses. Therefore, it is necessary to disentangle the backdoor
and clean features in order to improve backdoor purification. To address this,
we introduce Feature Shift Tuning (FST), a method for tuning-based backdoor
purification. Specifically, FST encourages feature shifts by actively deviating
the classifier weights from the originally compromised weights. Extensive
experiments demonstrate that our FST provides consistently stable performance
under different attack settings. Additionally, it is also convenient to deploy
in real-world scenarios with significantly reduced computation costs. Our codes
are available at
\url{https://github.com/AISafety-HKUST/stable_backdoor_purification}.
</p></li>
</ul>

<h3>Title: DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training. (arXiv:2310.02025v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02025">http://arxiv.org/abs/2310.02025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02025]] DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training(http://arxiv.org/abs/2310.02025)</code></li>
<li>Summary: <p>Zeroth-order (ZO) optimization has become a popular technique for solving
machine learning (ML) problems when first-order (FO) information is difficult
or impossible to obtain. However, the scalability of ZO optimization remains an
open problem: Its use has primarily been limited to relatively small-scale ML
problems, such as sample-wise adversarial attack generation. To our best
knowledge, no prior work has demonstrated the effectiveness of ZO optimization
in training deep neural networks (DNNs) without a significant decrease in
performance. To overcome this roadblock, we develop DeepZero, a principled ZO
deep learning (DL) framework that can scale ZO optimization to DNN training
from scratch through three primary innovations. First, we demonstrate the
advantages of coordinate-wise gradient estimation (CGE) over randomized
vector-wise gradient estimation in training accuracy and computational
efficiency. Second, we propose a sparsity-induced ZO training protocol that
extends the model pruning methodology using only finite differences to explore
and exploit the sparse DL prior in CGE. Third, we develop the methods of
feature reuse and forward parallelization to advance the practical
implementations of ZO training. Our extensive experiments show that DeepZero
achieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10,
approaching FO training performance for the first time. Furthermore, we show
the practical utility of DeepZero in applications of certified adversarial
defense and DL-based partial differential equation error correction, achieving
10-20% improvement over SOTA. We believe our results will inspire future
research on scalable ZO optimization and contribute to advancing DL with black
box.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Defending Against Authorship Identification Attacks. (arXiv:2310.01568v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01568">http://arxiv.org/abs/2310.01568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01568]] Defending Against Authorship Identification Attacks(http://arxiv.org/abs/2310.01568)</code></li>
<li>Summary: <p>Authorship identification has proven unsettlingly effective in inferring the
identity of the author of an unsigned document, even when sensitive personal
information has been carefully omitted. In the digital era, individuals leave a
lasting digital footprint through their written content, whether it is posted
on social media, stored on their employer's computers, or located elsewhere.
When individuals need to communicate publicly yet wish to remain anonymous,
there is little available to protect them from unwanted authorship
identification. This unprecedented threat to privacy is evident in scenarios
such as whistle-blowing. Proposed defenses against authorship identification
attacks primarily aim to obfuscate one's writing style, thereby making it
unlinkable to their pre-existing writing, while concurrently preserving the
original meaning and grammatical integrity. The presented work offers a
comprehensive review of the advancements in this research area spanning over
the past two decades and beyond. It emphasizes the methodological frameworks of
modification and generation-based strategies devised to evade authorship
identification attacks, highlighting joint efforts from the differential
privacy community. Limitations of current research are discussed, with a
spotlight on open challenges and potential research avenues.
</p></li>
</ul>

<h3>Title: Threat Modelling in Internet of Things (IoT) Environment Using Dynamic Attack Graphs. (arXiv:2310.01689v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01689">http://arxiv.org/abs/2310.01689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01689]] Threat Modelling in Internet of Things (IoT) Environment Using Dynamic Attack Graphs(http://arxiv.org/abs/2310.01689)</code></li>
<li>Summary: <p>We present a threat modelling approach to represent changes to the attack
paths through an Internet of Things (IoT) environment when the environment
changes dynamically, i.e., when new devices are added or removed from the
system or when whole sub-systems join or leave. The proposed approach
investigates the propagation of threats using attack graphs. However,
traditional attack graph approaches have been applied in static environments
that do not continuously change such as the Enterprise networks, leading to
static and usually very large attack graphs. In contrast, IoT environments are
often characterised by dynamic change and interconnections; different
topologies for different systems may interconnect with each other dynamically
and outside the operator control. Such new interconnections lead to changes in
the reachability amongst devices according to which their corresponding attack
graphs change. This requires dynamic topology and attack graphs for threat and
risk analysis. In this paper, we develop a threat modelling approach that cope
with dynamic system changes that may occur in IoT environments and enables
identifying attack paths whilst allowing for system dynamics. We develop
dynamic topology and attack graphs that are able to cope with the changes in
the IoT environment rapidly by maintaining their associated graphs. To motivate
the work and illustrate our approach we introduce an example scenario based on
healthcare systems. Our approach is implemented using a Graph Database
Management Tool (GDBM) -- Neo4j -- which is a popular tool for mapping,
visualising and querying the graphs of highly connected data, and is efficient
in providing a rapid threat modelling mechanism, which makes it suitable for
capturing security changes in the dynamic IoT environment.
</p></li>
</ul>

<h3>Title: Multi-class Network Intrusion Detection with Class Imbalance via LSTM & SMOTE. (arXiv:2310.01850v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01850">http://arxiv.org/abs/2310.01850</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01850]] Multi-class Network Intrusion Detection with Class Imbalance via LSTM & SMOTE(http://arxiv.org/abs/2310.01850)</code></li>
<li>Summary: <p>Monitoring network traffic to maintain the quality of service (QoS) and to
detect network intrusions in a timely and efficient manner is essential. As
network traffic is sequential, recurrent neural networks (RNNs) such as long
short-term memory (LSTM) are suitable for building network intrusion detection
systems. However, in the case of a few dataset examples of the rare attack
types, even these networks perform poorly. This paper proposes to use
oversampling techniques along with appropriate loss functions to handle class
imbalance for the detection of various types of network intrusions. Our deep
learning model employs LSTM with fully connected layers to perform multi-class
classification of network attacks. We enhance the representation of minority
classes: i) through the application of the Synthetic Minority Over-sampling
Technique (SMOTE), and ii) by employing categorical focal cross-entropy loss to
apply a focal factor to down-weight examples of the majority classes and focus
more on hard examples of the minority classes. Extensive experiments on KDD99
and CICIDS2017 datasets show promising results in detecting network intrusions
(with many rare attack types, e.g., Probe, R2L, DDoS, PortScan, etc.).
</p></li>
</ul>

<h3>Title: Waveform Manipulation Against DNN-based Modulation Classification Attacks. (arXiv:2310.01894v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01894">http://arxiv.org/abs/2310.01894</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01894]] Waveform Manipulation Against DNN-based Modulation Classification Attacks(http://arxiv.org/abs/2310.01894)</code></li>
<li>Summary: <p>In this paper we propose a method for defending against an eavesdropper that
uses a Deep Neural Network (DNN) for learning the modulation of wireless
communication signals. Our method is based on manipulating the emitted waveform
with the aid of a continuous time frequency-modulated (FM) obfuscating signal
that is mixed with the modulated data. The resulting waveform allows a
legitimate receiver (LRx) to demodulate the data but it increases the test
error of a pre-trained or adversarially-trained DNN classifier at the
eavesdropper. The scheme works for analog modulation and digital single carrier
and multi carrier orthogonal frequency division multiplexing (OFDM) waveforms,
while it can implemented in frame-based wireless protocols. The results
indicate that careful selection of the parameters of the obfuscating waveform
can drop classification performance at the eavesdropper to less than 10% in
AWGN and fading channels with no performance loss at the LRx.
</p></li>
</ul>

<h3>Title: Steganalysis of AI Models LSB Attacks. (arXiv:2310.01969v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01969">http://arxiv.org/abs/2310.01969</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01969]] Steganalysis of AI Models LSB Attacks(http://arxiv.org/abs/2310.01969)</code></li>
<li>Summary: <p>Artificial intelligence has made significant progress in the last decade,
leading to a rise in the popularity of model sharing. The model zoo ecosystem,
a repository of pre-trained AI models, has advanced the AI open-source
community and opened new avenues for cyber risks. Malicious attackers can
exploit shared models to launch cyber-attacks. This work focuses on the
steganalysis of injected malicious Least Significant Bit (LSB) steganography
into AI models, and it is the first work focusing on AI model attacks. In
response to this threat, this paper presents a steganalysis method specifically
tailored to detect and mitigate malicious LSB steganography attacks based on
supervised and unsupervised AI detection steganalysis methods. Our proposed
technique aims to preserve the integrity of shared models, protect user trust,
and maintain the momentum of open collaboration within the AI community. In
this work, we propose 3 steganalysis methods and open source our code. We found
that the success of the steganalysis depends on the LSB attack location. If the
attacker decides to exploit the least significant bits in the LSB, the ability
to detect the attacks is low. However, if the attack is in the most significant
LSB bits, the attack can be detected with almost perfect accuracy.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: DARTH: Holistic Test-time Adaptation for Multiple Object Tracking. (arXiv:2310.01926v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01926">http://arxiv.org/abs/2310.01926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01926]] DARTH: Holistic Test-time Adaptation for Multiple Object Tracking(http://arxiv.org/abs/2310.01926)</code></li>
<li>Summary: <p>Multiple object tracking (MOT) is a fundamental component of perception
systems for autonomous driving, and its robustness to unseen conditions is a
requirement to avoid life-critical failures. Despite the urge of safety in
driving systems, no solution to the MOT adaptation problem to domain shift in
test-time conditions has ever been proposed. However, the nature of a MOT
system is manifold - requiring object detection and instance association - and
adapting all its components is non-trivial. In this paper, we analyze the
effect of domain shift on appearance-based trackers, and introduce DARTH, a
holistic test-time adaptation framework for MOT. We propose a detection
consistency formulation to adapt object detection in a self-supervised fashion,
while adapting the instance appearance representations via our novel patch
contrastive loss. We evaluate our method on a variety of domain shifts -
including sim-to-real, outdoor-to-indoor, indoor-to-outdoor - and substantially
improve the source model performance on all metrics. Code:
https://github.com/mattiasegu/darth.
</p></li>
</ul>

<h3>Title: OOD Aware Supervised Contrastive Learning. (arXiv:2310.01942v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01942">http://arxiv.org/abs/2310.01942</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01942]] OOD Aware Supervised Contrastive Learning(http://arxiv.org/abs/2310.01942)</code></li>
<li>Summary: <p>Out-of-Distribution (OOD) detection is a crucial problem for the safe
deployment of machine learning models identifying samples that fall outside of
the training distribution, i.e. in-distribution data (ID). Most OOD works focus
on the classification models trained with Cross Entropy (CE) and attempt to fix
its inherent issues. In this work we leverage powerful representation learned
with Supervised Contrastive (SupCon) training and propose a holistic approach
to learn a classifier robust to OOD data. We extend SupCon loss with two
additional contrast terms. The first term pushes auxiliary OOD representations
away from ID representations without imposing any constraints on similarities
among auxiliary data. The second term pushes OOD features far from the existing
class prototypes, while pushing ID representations closer to their
corresponding class prototype. When auxiliary OOD data is not available, we
propose feature mixing techniques to efficiently generate pseudo-OOD features.
Our solution is simple and efficient and acts as a natural extension of the
closed-set supervised contrastive representation learning. We compare against
different OOD detection methods on the common benchmarks and show
state-of-the-art results.
</p></li>
</ul>

<h3>Title: Making Retrieval-Augmented Language Models Robust to Irrelevant Context. (arXiv:2310.01558v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01558">http://arxiv.org/abs/2310.01558</a></li>
<li>Code URL: https://github.com/oriyor/ret-robust</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01558]] Making Retrieval-Augmented Language Models Robust to Irrelevant Context(http://arxiv.org/abs/2310.01558)</code></li>
<li>Summary: <p>Retrieval-augmented language models (RALMs) hold promise to produce language
understanding systems that are are factual, efficient, and up-to-date. An
important desideratum of RALMs, is that retrieved information helps model
performance when it is relevant, and does not harm performance when it is not.
This is particularly important in multi-hop reasoning scenarios, where misuse
of irrelevant evidence can lead to cascading errors. However, recent work has
shown that retrieval augmentation can sometimes have a negative effect on
performance. In this work, we present a thorough analysis on five open-domain
question answering benchmarks, characterizing cases when retrieval reduces
accuracy. We then propose two methods to mitigate this issue. First, a simple
baseline that filters out retrieved passages that do not entail question-answer
pairs according to a natural language inference (NLI) model. This is effective
in preventing performance reduction, but at a cost of also discarding relevant
passages. Thus, we propose a method for automatically generating data to
fine-tune the language model to properly leverage retrieved passages, using a
mix of relevant and irrelevant contexts at training time. We empirically show
that even 1,000 examples suffice to train the model to be robust to irrelevant
contexts while maintaining high performance on examples with relevant ones.
</p></li>
</ul>

<h3>Title: Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01717">http://arxiv.org/abs/2310.01717</a></li>
<li>Code URL: https://github.com/manga-uofa/ed4ucp</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01717]] Ensemble Distillation for Unsupervised Constituency Parsing(http://arxiv.org/abs/2310.01717)</code></li>
<li>Summary: <p>We investigate the unsupervised constituency parsing task, which organizes
words and phrases of a sentence into a hierarchical structure without using
linguistically annotated data. We observe that existing unsupervised parsers
capture differing aspects of parsing structures, which can be leveraged to
enhance unsupervised parsing performance. To this end, we propose a notion of
"tree averaging," based on which we further propose a novel ensemble method for
unsupervised parsing. To improve inference efficiency, we further distill the
ensemble knowledge into a student model; such an ensemble-then-distill process
is an effective approach to mitigate the over-smoothing problem existing in
common multi-teacher distilling methods. Experiments show that our method
surpasses all previous approaches, consistently demonstrating its effectiveness
and robustness across various runs, with different ensemble components, and
under domain-shift conditions.
</p></li>
</ul>

<h3>Title: AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework. (arXiv:2310.01818v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01818">http://arxiv.org/abs/2310.01818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01818]] AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework(http://arxiv.org/abs/2310.01818)</code></li>
<li>Summary: <p>Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial
robustness in downstream applications, without requiring a lot of computational
resources and collecting significant amounts of data. This paper uncovers an
issue with the existing RFT, where optimizing both adversarial and natural
objectives through the feature extractor (FE) yields significantly divergent
gradient directions. This divergence introduces instability in the optimization
process, thereby hindering the attainment of adversarial robustness and
rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we
propose a low-rank (LoRa) branch that disentangles RFT into two distinct
components: optimizing natural objectives via the LoRa branch and adversarial
objectives via the FE. Besides, we introduce heuristic strategies for
automating the scheduling of the learning rate and the scalars of loss terms.
Extensive empirical evaluations demonstrate that our proposed automated RFT
disentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art
results across a range of downstream tasks. AutoLoRa holds significant
practical utility, as it automatically converts a pre-trained FE into an
adversarially robust model for downstream tasks without the need for searching
hyperparameters.
</p></li>
</ul>

<h3>Title: The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation. (arXiv:2310.01517v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01517">http://arxiv.org/abs/2310.01517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01517]] The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation(http://arxiv.org/abs/2310.01517)</code></li>
<li>Summary: <p>Gray-box models offer significant benefit over black-box approaches for
equipment emulator development for equipment since their integration of physics
provides more confidence in the model outside of the training domain. However,
challenges such as model nonlinearity, unmodeled dynamics, and local minima
introduce uncertainties into grey-box creation that contemporary approaches
have failed to overcome, leading to their under-performance compared with
black-box models. This paper seeks to address these uncertainties by injecting
noise into the training dataset. This noise injection enriches the dataset and
provides a measure of robustness against such uncertainties. A dynamic model
for a water-to-water heat exchanger has been used as a demonstration case for
this approach and tested using a pair of real devices with live data streaming.
Compared to the unprocessed signal data, the application of noise injection
resulted in a significant reduction in modeling error (root mean square error),
decreasing from 0.68 to 0.27{\deg}C. This improvement amounts to a 60%
enhancement when assessed on the training set, and improvements of 50% and 45%
when validated against the test and validation sets, respectively.
</p></li>
</ul>

<h3>Title: Equivariant Adaptation of Large Pre-Trained Models. (arXiv:2310.01647v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01647">http://arxiv.org/abs/2310.01647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01647]] Equivariant Adaptation of Large Pre-Trained Models(http://arxiv.org/abs/2310.01647)</code></li>
<li>Summary: <p>Equivariant networks are specifically designed to ensure consistent behavior
with respect to a set of input transformations, leading to higher sample
efficiency and more accurate and robust predictions. However, redesigning each
component of prevalent deep neural network architectures to achieve chosen
equivariance is a difficult problem and can result in a computationally
expensive network during both training and inference. A recently proposed
alternative towards equivariance that removes the architectural constraints is
to use a simple canonicalization network that transforms the input to a
canonical form before feeding it to an unconstrained prediction network. We
show here that this approach can effectively be used to make a large
pre-trained network equivariant. However, we observe that the produced
canonical orientations can be misaligned with those of the training
distribution, hindering performance. Using dataset-dependent priors to inform
the canonicalization function, we are able to make large pre-trained models
equivariant while maintaining their performance. This significantly improves
the robustness of these models to deterministic transformations of the data,
such as rotations. We believe this equivariant adaptation of large pre-trained
models can help their domain-specific applications with known symmetry priors.
</p></li>
</ul>

<h3>Title: CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems. (arXiv:2310.01650v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01650">http://arxiv.org/abs/2310.01650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01650]] CoDBench: A Critical Evaluation of Data-driven Models for Continuous Dynamical Systems(http://arxiv.org/abs/2310.01650)</code></li>
<li>Summary: <p>Continuous dynamical systems, characterized by differential equations, are
ubiquitously used to model several important problems: plasma dynamics, flow
through porous media, weather forecasting, and epidemic dynamics. Recently, a
wide range of data-driven models has been used successfully to model these
systems. However, in contrast to established fields like computer vision,
limited studies are available analyzing the strengths and potential
applications of different classes of these models that could steer
decision-making in scientific machine learning. Here, we introduce CodBench, an
exhaustive benchmarking suite comprising 11 state-of-the-art data-driven models
for solving differential equations. Specifically, we comprehensively evaluate 4
distinct categories of models, viz., feed forward neural networks, deep
operator regression models, frequency-based neural operators, and transformer
architectures against 8 widely applicable benchmark datasets encompassing
challenges from fluid and solid mechanics. We conduct extensive experiments,
assessing the operators' capabilities in learning, zero-shot super-resolution,
data efficiency, robustness to noise, and computational efficiency.
Interestingly, our findings highlight that current operators struggle with the
newer mechanics datasets, motivating the need for more robust neural operators.
All the datasets and codes will be shared in an easy-to-use fashion for the
scientific community. We hope this resource will be an impetus for accelerated
progress and exploration in modeling dynamical systems.
</p></li>
</ul>

<h3>Title: Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations. (arXiv:2310.01651v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01651">http://arxiv.org/abs/2310.01651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01651]] Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations(http://arxiv.org/abs/2310.01651)</code></li>
<li>Summary: <p>Large language and vision-language models are rapidly being deployed in
practice thanks to their impressive capabilities in instruction following,
in-context learning, and so on. This raises an urgent need to carefully analyse
their robustness so that stakeholders can understand if and when such models
are trustworthy enough to be relied upon in any given application. In this
paper, we highlight a specific vulnerability in popular models, namely
permutation sensitivity in multiple-choice question answering (MCQA).
Specifically, we show empirically that popular models are vulnerable to
adversarial permutation in answer sets for multiple-choice prompting, which is
surprising as models should ideally be as invariant to prompt permutation as
humans are. These vulnerabilities persist across various model sizes, and exist
in very recent language and vision-language models. Code is available at
\url{https://github.com/ys-zong/FoolyourVLLMs}.
</p></li>
</ul>

<h3>Title: Robustifying State-space Models for Long Sequences via Approximate Diagonalization. (arXiv:2310.01698v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01698">http://arxiv.org/abs/2310.01698</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01698]] Robustifying State-space Models for Long Sequences via Approximate Diagonalization(http://arxiv.org/abs/2310.01698)</code></li>
<li>Summary: <p>State-space models (SSMs) have recently emerged as a framework for learning
long-range sequence tasks. An example is the structured state-space sequence
(S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO
initialization framework. However, the complicated structure of the S4 layer
poses challenges; and, in an effort to address these challenges, models such as
S4D and S5 have considered a purely diagonal structure. This choice simplifies
the implementation, improves computational efficiency, and allows channel
communication. However, diagonalizing the HiPPO framework is itself an
ill-posed problem. In this paper, we propose a general solution for this and
related ill-posed diagonalization problems in machine learning. We introduce a
generic, backward-stable "perturb-then-diagonalize" (PTD) methodology, which is
based on the pseudospectral theory of non-normal operators, and which may be
interpreted as the approximate diagonalization of the non-normal matrices
defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models.
Through theoretical analysis of the transfer functions of different
initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization
strongly converges to the HiPPO framework, while the S4D/S5 initialization only
achieves weak convergences. As a result, our new models show resilience to
Fourier-mode noise-perturbed inputs, a crucial property not achieved by the
S4D/S5 models. In addition to improved robustness, our S5-PTD model averages
87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD
methodology helps to improve the accuracy of deep learning models.
</p></li>
</ul>

<h3>Title: Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01737">http://arxiv.org/abs/2310.01737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01737]] Blending Imitation and Reinforcement Learning for Robust Policy Improvement(http://arxiv.org/abs/2310.01737)</code></li>
<li>Summary: <p>While reinforcement learning (RL) has shown promising performance, its sample
complexity continues to be a substantial hurdle, restricting its broader
application across a variety of domains. Imitation learning (IL) utilizes
oracles to improve sample efficiency, yet it is often constrained by the
quality of the oracles deployed. which actively interleaves between IL and RL
based on an online estimate of their performance. RPI draws on the strengths of
IL, using oracle queries to facilitate exploration, an aspect that is notably
challenging in sparse-reward RL, particularly during the early stages of
learning. As learning unfolds, RPI gradually transitions to RL, effectively
treating the learned policy as an improved oracle. This algorithm is capable of
learning from and improving upon a diverse set of black-box oracles. Integral
to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient
(RPG), both of which reason over whether to perform state-wise imitation from
the oracles or learn from its own value function when the learner's performance
surpasses that of the oracles in a specific state. Empirical evaluations and
theoretical analysis validate that RPI excels in comparison to existing
state-of-the-art methodologies, demonstrating superior performance across
various benchmark domains.
</p></li>
</ul>

<h3>Title: Randomized Dimension Reduction with Statistical Guarantees. (arXiv:2310.01739v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01739">http://arxiv.org/abs/2310.01739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01739]] Randomized Dimension Reduction with Statistical Guarantees(http://arxiv.org/abs/2310.01739)</code></li>
<li>Summary: <p>Large models and enormous data are essential driving forces of the
unprecedented successes achieved by modern algorithms, especially in scientific
computing and machine learning. Nevertheless, the growing dimensionality and
model complexity, as well as the non-negligible workload of data
pre-processing, also bring formidable costs to such successes in both
computation and data aggregation. As the deceleration of Moore's Law slackens
the cost reduction of computation from the hardware level, fast heuristics for
expensive classical routines and efficient algorithms for exploiting limited
data are increasingly indispensable for pushing the limit of algorithm potency.
This thesis explores some of such algorithms for fast execution and efficient
data utilization.
</p>
<p>From the computational efficiency perspective, we design and analyze fast
randomized low-rank decomposition algorithms for large matrices based on
"matrix sketching", which can be regarded as a dimension reduction strategy in
the data space. These include the randomized pivoting-based interpolative and
CUR decomposition discussed in Chapter 2 and the randomized subspace
approximations discussed in Chapter 3.
</p>
<p>From the sample efficiency perspective, we focus on learning algorithms with
various incorporations of data augmentation that improve generalization and
distributional robustness provably. Specifically, Chapter 4 presents a sample
complexity analysis for data augmentation consistency regularization where we
view sample efficiency from the lens of dimension reduction in the function
space. Then in Chapter 5, we introduce an adaptively weighted data augmentation
consistency regularization algorithm for distributionally robust optimization
with applications in medical image segmentation.
</p></li>
</ul>

<h3>Title: A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01770">http://arxiv.org/abs/2310.01770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01770]] A simple connection from loss flatness to compressed representations in neural networks(http://arxiv.org/abs/2310.01770)</code></li>
<li>Summary: <p>Deep neural networks' generalization capacity has been studied in a variety
of ways, including at least two distinct categories of approach: one based on
the shape of the loss landscape in parameter space, and the other based on the
structure of the representation manifold in feature space (that is, in the
space of unit activities). These two approaches are related, but they are
rarely studied together and explicitly connected. Here, we present a simple
analysis that makes such a connection. We show that, in the last phase of
learning of deep neural networks, compression of the volume of the manifold of
neural representations correlates with the flatness of the loss around the
minima explored by ongoing parameter optimization. We show that this is
predicted by a relatively simple mathematical relationship: loss flatness
implies compression of neural representations. Our results build closely on
prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small
eigenvalues of the loss Hessian) develops in late phases of learning and lead
to robustness to perturbations in network inputs. Moreover, we show there is no
similarly direct connection between local dimensionality and sharpness,
suggesting that this property may be controlled by different mechanisms than
volume and hence may play a complementary role in neural representations.
Overall, we advance a dual perspective on generalization in neural networks in
both parameter and feature space.
</p></li>
</ul>

<h3>Title: Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks. (arXiv:2310.01820v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01820">http://arxiv.org/abs/2310.01820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01820]] Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks(http://arxiv.org/abs/2310.01820)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) are neural models that leverage the dependency
structure in graphical data via message passing among the graph nodes. GNNs
have emerged as pivotal architectures in analyzing graph-structured data, and
their expansive application in sensitive domains requires a comprehensive
understanding of their decision-making processes -- necessitating a framework
for GNN explainability. An explanation function for GNNs takes a pre-trained
GNN along with a graph as input, to produce a `sufficient statistic' subgraph
with respect to the graph label. A main challenge in studying GNN
explainability is to provide fidelity measures that evaluate the performance of
these explanation functions. This paper studies this foundational challenge,
spotlighting the inherent limitations of prevailing fidelity metrics, including
$Fid_+$, $Fid_-$, and $Fid_\Delta$. Specifically, a formal,
information-theoretic definition of explainability is introduced and it is
shown that existing metrics often fail to align with this definition across
various statistical scenarios. The reason is due to potential distribution
shifts when subgraphs are removed in computing these fidelity measures.
Subsequently, a robust class of fidelity measures are introduced, and it is
shown analytically that they are resilient to distribution shift issues and are
applicable in a wide range of scenarios. Extensive empirical analysis on both
synthetic and real datasets are provided to illustrate that the proposed
metrics are more coherent with gold standard metrics.
</p></li>
</ul>

<h3>Title: EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis. (arXiv:2310.01835v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01835">http://arxiv.org/abs/2310.01835</a></li>
<li>Code URL: https://github.com/crowdstrike/embersim-databank</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01835]] EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis(http://arxiv.org/abs/2310.01835)</code></li>
<li>Summary: <p>In recent years there has been a shift from heuristics-based malware
detection towards machine learning, which proves to be more robust in the
current heavily adversarial threat landscape. While we acknowledge machine
learning to be better equipped to mine for patterns in the increasingly high
amounts of similar-looking files, we also note a remarkable scarcity of the
data available for similarity-targeted research. Moreover, we observe that the
focus in the few related works falls on quantifying similarity in malware,
often overlooking the clean data. This one-sided quantification is especially
dangerous in the context of detection bypass. We propose to address the
deficiencies in the space of similarity research on binary files, starting from
EMBER - one of the largest malware classification data sets. We enhance EMBER
with similarity information as well as malware class tags, to enable further
research in the similarity space. Our contribution is threefold: (1) we publish
EMBERSim, an augmented version of EMBER, that includes similarity-informed
tags; (2) we enrich EMBERSim with automatically determined malware class tags
using the open-source tool AVClass on VirusTotal data and (3) we describe and
share the implementation for our class scoring technique and leaf similarity
method.
</p></li>
</ul>

<h3>Title: Probabilistic Reach-Avoid for Bayesian Neural Networks. (arXiv:2310.01951v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01951">http://arxiv.org/abs/2310.01951</a></li>
<li>Code URL: https://github.com/matthewwicker/bnnreachavoid</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01951]] Probabilistic Reach-Avoid for Bayesian Neural Networks(http://arxiv.org/abs/2310.01951)</code></li>
<li>Summary: <p>Model-based reinforcement learning seeks to simultaneously learn the dynamics
of an unknown stochastic environment and synthesise an optimal policy for
acting in it. Ensuring the safety and robustness of sequential decisions made
through a policy in such an environment is a key challenge for policies
intended for safety-critical scenarios. In this work, we investigate two
complementary problems: first, computing reach-avoid probabilities for
iterative predictions made with dynamical models, with dynamics described by
Bayesian neural network (BNN); second, synthesising control policies that are
optimal with respect to a given reach-avoid specification (reaching a "target"
state, while avoiding a set of "unsafe" states) and a learned BNN model. Our
solution leverages interval propagation and backward recursion techniques to
compute lower bounds for the probability that a policy's sequence of actions
leads to satisfying the reach-avoid specification. Such computed lower bounds
provide safety certification for the given policy and BNN model. We then
introduce control synthesis algorithms to derive policies maximizing said lower
bounds on the safety probability. We demonstrate the effectiveness of our
method on a series of control benchmarks characterized by learned BNN dynamics
models. On our most challenging benchmark, compared to purely data-driven
policies the optimal synthesis algorithm is able to provide more than a
four-fold increase in the number of certifiable states and more than a
three-fold increase in the average guaranteed reach-avoid probability.
</p></li>
</ul>

<h3>Title: Spectral operator learning for parametric PDEs without data reliance. (arXiv:2310.02013v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02013">http://arxiv.org/abs/2310.02013</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02013]] Spectral operator learning for parametric PDEs without data reliance(http://arxiv.org/abs/2310.02013)</code></li>
<li>Summary: <p>In this paper, we introduce the Spectral Coefficient Learning via Operator
Network (SCLON), a novel operator learning-based approach for solving
parametric partial differential equations (PDEs) without the need for data
harnessing. The cornerstone of our method is the spectral methodology that
employs expansions using orthogonal functions, such as Fourier series and
Legendre polynomials, enabling accurate PDE solutions with fewer grid points.
By merging the merits of spectral methods - encompassing high accuracy,
efficiency, generalization, and the exact fulfillment of boundary conditions -
with the prowess of deep neural networks, SCLON offers a transformative
strategy. Our approach not only eliminates the need for paired input-output
training data, which typically requires extensive numerical computations, but
also effectively learns and predicts solutions of complex parametric PDEs,
ranging from singularly perturbed convection-diffusion equations to the
Navier-Stokes equations. The proposed framework demonstrates superior
performance compared to existing scientific machine learning techniques,
offering solutions for multiple instances of parametric PDEs without harnessing
data. The mathematical framework is robust and reliable, with a well-developed
loss function derived from the weak formulation, ensuring accurate
approximation of solutions while exactly satisfying boundary conditions. The
method's efficacy is further illustrated through its ability to accurately
predict intricate natural behaviors like the Kolmogorov flow and boundary
layers. In essence, our work pioneers a compelling avenue for parametric PDE
solutions, serving as a bridge between traditional numerical methodologies and
cutting-edge machine learning techniques in the realm of scientific
computation.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h3>Title: Beyond Labeling Oracles: What does it mean to steal ML models?. (arXiv:2310.01959v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01959">http://arxiv.org/abs/2310.01959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01959]] Beyond Labeling Oracles: What does it mean to steal ML models?(http://arxiv.org/abs/2310.01959)</code></li>
<li>Summary: <p>Model extraction attacks are designed to steal trained models with only query
access, as is often provided through APIs that ML-as-a-Service providers offer.
ML models are expensive to train, in part because data is hard to obtain, and a
primary incentive for model extraction is to acquire a model while incurring
less cost than training from scratch. Literature on model extraction commonly
claims or presumes that the attacker is able to save on both data acquisition
and labeling costs. We show that the attacker often does not. This is because
current attacks implicitly rely on the adversary being able to sample from the
victim model's data distribution. We thoroughly evaluate factors influencing
the success of model extraction. We discover that prior knowledge of the
attacker, i.e. access to in-distribution data, dominates other factors like the
attack policy the adversary follows to choose which queries to make to the
victim model API. Thus, an adversary looking to develop an equally capable
model with a fixed budget has little practical incentive to perform model
extraction, since for the attack to work they need to collect in-distribution
data, saving only on the cost of labeling. With low labeling costs in the
current market, the usefulness of such attacks is questionable. Ultimately, we
demonstrate that the effect of prior knowledge needs to be explicitly decoupled
from the attack policy. To this end, we propose a benchmark to evaluate attack
policy directly.
</p></li>
</ul>

<h2>extraction</h2>
<h3>Title: An evaluation of pre-trained models for feature extraction in image classification. (arXiv:2310.02037v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02037">http://arxiv.org/abs/2310.02037</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02037]] An evaluation of pre-trained models for feature extraction in image classification(http://arxiv.org/abs/2310.02037)</code></li>
<li>Summary: <p>In recent years, we have witnessed a considerable increase in performance in
image classification tasks. This performance improvement is mainly due to the
adoption of deep learning techniques. Generally, deep learning techniques
demand a large set of annotated data, making it a challenge when applying it to
small datasets. In this scenario, transfer learning strategies have become a
promising alternative to overcome these issues. This work aims to compare the
performance of different pre-trained neural networks for feature extraction in
image classification tasks. We evaluated 16 different pre-trained models in
four image datasets. Our results demonstrate that the best general performance
along the datasets was achieved by CLIP-ViT-B and ViT-H-14, where the
CLIP-ResNet50 model had similar performance but with less variability.
Therefore, our study provides evidence supporting the choice of models for
feature extraction in image classification tasks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models. (arXiv:2310.01467v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01467">http://arxiv.org/abs/2310.01467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01467]] FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models(http://arxiv.org/abs/2310.01467)</code></li>
<li>Summary: <p>Pre-trained language models (PLM) have revolutionized the NLP landscape,
achieving stellar performances across diverse tasks. These models, while
benefiting from vast training data, often require fine-tuning on specific data
to cater to distinct downstream tasks. However, this data adaptation process
has inherent security and privacy concerns, primarily when leveraging
user-generated, device-residing data. Federated learning (FL) provides a
solution, allowing collaborative model fine-tuning without centralized data
collection. However, applying FL to finetune PLMs is hampered by challenges,
including restricted model parameter access, high computational requirements,
and communication overheads. This paper introduces Federated Black-box Prompt
Tuning (FedBPT), a framework designed to address these challenges. FedBPT does
not require the clients to access the model parameters. By focusing on training
optimal prompts and utilizing gradient-free optimization methods, FedBPT
reduces the number of exchanged variables, boosts communication efficiency, and
minimizes computational and storage costs. Experiments highlight the
framework's ability to drastically cut communication and memory costs while
maintaining competitive performance. Ultimately, FedBPT presents a promising
solution for efficient, privacy-preserving fine-tuning of PLM in the age of
large language models.
</p></li>
</ul>

<h3>Title: Adversarial Client Detection via Non-parametric Subspace Monitoring in the Internet of Federated Things. (arXiv:2310.01537v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01537">http://arxiv.org/abs/2310.01537</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01537]] Adversarial Client Detection via Non-parametric Subspace Monitoring in the Internet of Federated Things(http://arxiv.org/abs/2310.01537)</code></li>
<li>Summary: <p>The Internet of Federated Things (IoFT) represents a network of
interconnected systems with federated learning as the backbone, facilitating
collaborative knowledge acquisition while ensuring data privacy for individual
systems. The wide adoption of IoFT, however, is hindered by security concerns,
particularly the susceptibility of federated learning networks to adversarial
attacks. In this paper, we propose an effective non-parametric approach FedRR,
which leverages the low-rank features of the transmitted parameter updates
generated by federated learning to address the adversarial attack problem.
Besides, our proposed method is capable of accurately detecting adversarial
clients and controlling the false alarm rate under the scenario with no attack
occurring. Experiments based on digit recognition using the MNIST datasets
validated the advantages of our approach.
</p></li>
</ul>

<h3>Title: Federated Wasserstein Distance. (arXiv:2310.01973v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01973">http://arxiv.org/abs/2310.01973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01973]] Federated Wasserstein Distance(http://arxiv.org/abs/2310.01973)</code></li>
<li>Summary: <p>We introduce a principled way of computing the Wasserstein distance between
two distributions in a federated manner. Namely, we show how to estimate the
Wasserstein distance between two samples stored and kept on different
devices/clients whilst a central entity/server orchestrates the computations
(again, without having access to the samples). To achieve this feat, we take
advantage of the geometric properties of the Wasserstein distance -- in
particular, the triangle inequality -- and that of the associated {\em
geodesics}: our algorithm, FedWad (for Federated Wasserstein Distance),
iteratively approximates the Wasserstein distance by manipulating and
exchanging distributions from the space of geodesics in lieu of the input
samples. In addition to establishing the convergence properties of FedWad, we
provide empirical results on federated coresets and federate optimal transport
dataset distance, that we respectively exploit for building a novel federated
model and for boosting performance of popular federated learning algorithms.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: ImagenHub: Standardizing the evaluation of conditional image generation models. (arXiv:2310.01596v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01596">http://arxiv.org/abs/2310.01596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01596]] ImagenHub: Standardizing the evaluation of conditional image generation models(http://arxiv.org/abs/2310.01596)</code></li>
<li>Summary: <p>Recently, a myriad of conditional image generation and editing models have
been developed to serve different downstream tasks, including text-to-image
generation, text-guided image editing, subject-driven image generation,
control-guided image generation, etc. However, we observe huge inconsistencies
in experimental conditions: datasets, inference, and evaluation metrics -
render fair comparisons difficult. This paper proposes ImagenHub, which is a
one-stop library to standardize the inference and evaluation of all the
conditional image generation models. Firstly, we define seven prominent tasks
and curate high-quality evaluation datasets for them. Secondly, we built a
unified inference pipeline to ensure fair comparison. Thirdly, we design two
human evaluation scores, i.e. Semantic Consistency and Perceptual Quality,
along with comprehensive guidelines to evaluate generated images. We train
expert raters to evaluate the model outputs based on the proposed metrics. Our
human evaluation achieves a high inter-worker agreement of Krippendorff's alpha
on 76% models with a value higher than 0.4. We comprehensively evaluated a
total of around 30 models and observed three key takeaways: (1) the existing
models' performance is generally unsatisfying except for Text-guided Image
Generation and Subject-driven Image Generation, with 74% models achieving an
overall score lower than 0.5. (2) we examined the claims from published papers
and found 83% of them hold with a few exceptions. (3) None of the existing
automatic metrics has a Spearman's correlation higher than 0.2 except
subject-driven image generation. Moving forward, we will continue our efforts
to evaluate newly published models and update our leaderboard to keep track of
the progress in conditional image generation.
</p></li>
</ul>

<h3>Title: Hierarchical Evaluation Framework: Best Practices for Human Evaluation. (arXiv:2310.01917v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01917">http://arxiv.org/abs/2310.01917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01917]] Hierarchical Evaluation Framework: Best Practices for Human Evaluation(http://arxiv.org/abs/2310.01917)</code></li>
<li>Summary: <p>Human evaluation plays a crucial role in Natural Language Processing (NLP) as
it assesses the quality and relevance of developed systems, thereby
facilitating their enhancement. However, the absence of widely accepted human
evaluation metrics in NLP hampers fair comparisons among different systems and
the establishment of universal assessment standards. Through an extensive
analysis of existing literature on human evaluation metrics, we identified
several gaps in NLP evaluation methodologies. These gaps served as motivation
for developing our own hierarchical evaluation framework. The proposed
framework offers notable advantages, particularly in providing a more
comprehensive representation of the NLP system's performance. We applied this
framework to evaluate the developed Machine Reading Comprehension system, which
was utilized within a human-AI symbiosis model. The results highlighted the
associations between the quality of inputs and outputs, underscoring the
necessity to evaluate both components rather than solely focusing on outputs.
In future work, we will investigate the potential time-saving benefits of our
proposed framework for evaluators assessing NLP systems.
</p></li>
</ul>

<h3>Title: Nash Regret Guarantees for Linear Bandits. (arXiv:2310.02023v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02023">http://arxiv.org/abs/2310.02023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02023]] Nash Regret Guarantees for Linear Bandits(http://arxiv.org/abs/2310.02023)</code></li>
<li>Summary: <p>We obtain essentially tight upper bounds for a strengthened notion of regret
in the stochastic linear bandits framework. The strengthening -- referred to as
Nash regret -- is defined as the difference between the (a priori unknown)
optimum and the geometric mean of expected rewards accumulated by the linear
bandit algorithm. Since the geometric mean corresponds to the well-studied Nash
social welfare (NSW) function, this formulation quantifies the performance of a
bandit algorithm as the collective welfare it generates across rounds. NSW is
known to satisfy fairness axioms and, hence, an upper bound on Nash regret
provides a principled fairness guarantee.
</p>
<p>We consider the stochastic linear bandits problem over a horizon of $T$
rounds and with set of arms ${X}$ in ambient dimension $d$. Furthermore, we
focus on settings in which the stochastic reward -- associated with each arm in
${X}$ -- is a non-negative, $\nu$-sub-Poisson random variable. For this
setting, we develop an algorithm that achieves a Nash regret of $O\left(
\sqrt{\frac{d\nu}{T}} \log( T |X|)\right)$. In addition, addressing linear
bandit instances in which the set of arms ${X}$ is not necessarily finite, we
obtain a Nash regret upper bound of $O\left(
\frac{d^\frac{5}{4}\nu^{\frac{1}{2}}}{\sqrt{T}} \log(T)\right)$. Since bounded
random variables are sub-Poisson, these results hold for bounded, positive
rewards. Our linear bandit algorithm is built upon the successive elimination
method with novel technical insights, including tailored concentration bounds
and the use of sampling via John ellipsoid in conjunction with the
Kiefer-Wolfowitz optimal design.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: SEA: Sparse Linear Attention with Estimated Attention Mask. (arXiv:2310.01777v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01777">http://arxiv.org/abs/2310.01777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01777]] SEA: Sparse Linear Attention with Estimated Attention Mask(http://arxiv.org/abs/2310.01777)</code></li>
<li>Summary: <p>The transformer architecture has made breakthroughs in recent years on tasks
which require modeling pairwise relationships between sequential elements, as
is the case in natural language understanding. However, transformers struggle
with long sequences due to the quadratic complexity of the attention operation,
and previous research has aimed to lower the complexity by sparsifying or
linearly approximating the attention matrix. Yet, these approaches cannot
straightforwardly distill knowledge from a teacher's attention matrix, and
often require complete retraining from scratch. Furthermore, previous sparse
and linear approaches may also lose interpretability if they do not produce
full quadratic attention matrices. To address these challenges, we propose SEA:
Sparse linear attention with an Estimated Attention mask. SEA estimates the
attention matrix with linear complexity via kernel-based linear attention, then
creates a sparse approximation to the full attention matrix with a top-k
selection to perform a sparse attention operation. For language modeling tasks
(Wikitext2), previous linear and sparse attention methods show a roughly
two-fold worse perplexity scores over the quadratic OPT-125M baseline, while
SEA achieves an even better perplexity than OPT-125M, using roughly half as
much memory as OPT-125M. Moreover, SEA maintains an interpretable attention
matrix and can utilize knowledge distillation to lower the complexity of
existing pretrained transformers. We believe that our work will have a large
practical impact, as it opens the possibility of running large transformers on
resource-limited devices with less memory.
</p></li>
</ul>

<h3>Title: A Framework for Interpretability in Machine Learning for Medical Imaging. (arXiv:2310.01685v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01685">http://arxiv.org/abs/2310.01685</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01685]] A Framework for Interpretability in Machine Learning for Medical Imaging(http://arxiv.org/abs/2310.01685)</code></li>
<li>Summary: <p>Interpretability for machine learning models in medical imaging (MLMI) is an
important direction of research. However, there is a general sense of murkiness
in what interpretability means. Why does the need for interpretability in MLMI
arise? What goals does one actually seek to address when interpretability is
needed? To answer these questions, we identify a need to formalize the goals
and elements of interpretability in MLMI. By reasoning about real-world tasks
and goals common in both medical image analysis and its intersection with
machine learning, we identify four core elements of interpretability:
localization, visual recognizability, physical attribution, and transparency.
Overall, this paper formalizes interpretability needs in the context of medical
imaging, and our applied perspective clarifies concrete MLMI-specific goals and
considerations in order to guide method design and improve real-world usage.
Our goal is to provide practical and didactic information for model designers
and practitioners, inspire developers of models in the medical imaging field to
reason more deeply about what interpretability is achieving, and suggest future
directions of interpretability research.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability. (arXiv:2310.02067v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02067">http://arxiv.org/abs/2310.02067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02067]] Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability(http://arxiv.org/abs/2310.02067)</code></li>
<li>Summary: <p>In the context of temporal image forensics, it is not evident that a neural
network, trained on images from different time-slots (classes), exploit solely
age related features. Usually, images taken in close temporal proximity (e.g.,
belonging to the same age class) share some common content properties. Such
content bias can be exploited by a neural network. In this work, a novel
approach that evaluates the influence of image content is proposed. This
approach is verified using synthetic images (where content bias can be ruled
out) with an age signal embedded. Based on the proposed approach, it is shown
that a `standard' neural network trained in the context of age classification
is strongly dependent on image content. As a potential countermeasure, two
different techniques are applied to mitigate the influence of the image content
during training, and they are also evaluated by the proposed method.
</p></li>
</ul>

<h3>Title: A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education. (arXiv:2310.01603v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01603">http://arxiv.org/abs/2310.01603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01603]] A Review of Digital Learning Environments for Teaching Natural Language Processing in K-12 Education(http://arxiv.org/abs/2310.01603)</code></li>
<li>Summary: <p>Natural Language Processing (NLP) plays a significant role in our daily lives
and has become an essential part of Artificial Intelligence (AI) education in
K-12. As children grow up with NLP-powered applications, it is crucial to
introduce NLP concepts to them, fostering their understanding of language
processing, language generation, and ethical implications of AI and NLP. This
paper presents a comprehensive review of digital learning environments for
teaching NLP in K-12. Specifically, it explores existing digital learning
tools, discusses how they support specific NLP tasks and procedures, and
investigates their explainability and evaluation results in educational
contexts. By examining the strengths and limitations of these tools, this
literature review sheds light on the current state of NLP learning tools in
K-12 education. It aims to guide future research efforts to refine existing
tools, develop new ones, and explore more effective and inclusive strategies
for integrating NLP into K-12 educational contexts.
</p></li>
</ul>

<h3>Title: GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking. (arXiv:2310.01794v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01794">http://arxiv.org/abs/2310.01794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01794]] GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking(http://arxiv.org/abs/2310.01794)</code></li>
<li>Summary: <p>Numerous explainability methods have been proposed to shed light on the inner
workings of GNNs. Despite the inclusion of empirical evaluations in all the
proposed algorithms, the interrogative aspects of these evaluations lack
diversity. As a result, various facets of explainability pertaining to GNNs,
such as a comparative analysis of counterfactual reasoners, their stability to
variational factors such as different GNN architectures, noise, stochasticity
in non-convex loss surfaces, feasibility amidst domain constraints, and so
forth, have yet to be formally investigated. Motivated by this need, we present
a benchmarking study on perturbation-based explainability methods for GNNs,
aiming to systematically evaluate and compare a wide range of explainability
techniques. Among the key findings of our study, we identify the Pareto-optimal
methods that exhibit superior efficacy and stability in the presence of noise.
Nonetheless, our study reveals that all algorithms are affected by stability
issues when faced with noisy data. Furthermore, we have established that the
current generation of counterfactual explainers often fails to provide feasible
recourses due to violations of topological constraints encoded by
domain-specific considerations. Overall, this benchmarking study empowers
stakeholders in the field of GNNs with a comprehensive understanding of the
state-of-the-art explainability methods, potential research problems for
further enhancement, and the implications of their application in real-world
scenarios.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code. (arXiv:2310.01506v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01506">http://arxiv.org/abs/2310.01506</a></li>
<li>Code URL: https://github.com/cure-lab/directinversion</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01506]] Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code(http://arxiv.org/abs/2310.01506)</code></li>
<li>Summary: <p>Text-guided diffusion models have revolutionized image generation and
editing, offering exceptional realism and diversity. Specifically, in the
context of diffusion-based editing, where a source image is edited according to
a target prompt, the process commences by acquiring a noisy latent vector
corresponding to the source image via the diffusion model. This vector is
subsequently fed into separate source and target diffusion branches for
editing. The accuracy of this inversion process significantly impacts the final
editing outcome, influencing both essential content preservation of the source
image and edit fidelity according to the target prompt. Prior inversion
techniques aimed at finding a unified solution in both the source and target
diffusion branches. However, our theoretical and empirical analyses reveal that
disentangling these branches leads to a distinct separation of responsibilities
for preserving essential content and ensuring edit fidelity. Building on this
insight, we introduce "Direct Inversion," a novel technique achieving optimal
performance of both branches with just three lines of code. To assess image
editing performance, we present PIE-Bench, an editing benchmark with 700 images
showcasing diverse scenes and editing types, accompanied by versatile
annotations and comprehensive evaluation metrics. Compared to state-of-the-art
optimization-based inversion techniques, our solution not only yields superior
performance across 8 editing methods but also achieves nearly an order of
speed-up.
</p></li>
</ul>

<h3>Title: SYRAC: Synthesize, Rank, and Count. (arXiv:2310.01662v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01662">http://arxiv.org/abs/2310.01662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01662]] SYRAC: Synthesize, Rank, and Count(http://arxiv.org/abs/2310.01662)</code></li>
<li>Summary: <p>Crowd counting is a critical task in computer vision, with several important
applications. However, existing counting methods rely on labor-intensive
density map annotations, necessitating the manual localization of each
individual pedestrian. While recent efforts have attempted to alleviate the
annotation burden through weakly or semi-supervised learning, these approaches
fall short of significantly reducing the workload. We propose a novel approach
to eliminate the annotation burden by leveraging latent diffusion models to
generate synthetic data. However, these models struggle to reliably understand
object quantities, leading to noisy annotations when prompted to produce images
with a specific quantity of objects. To address this, we use latent diffusion
models to create two types of synthetic data: one by removing pedestrians from
real images, which generates ranked image pairs with a weak but reliable object
quantity signal, and the other by generating synthetic images with a
predetermined number of objects, offering a strong but noisy counting signal.
Our method utilizes the ranking image pairs for pre-training and then fits a
linear layer to the noisy synthetic images using these crowd quantity features.
We report state-of-the-art results for unsupervised crowd counting.
</p></li>
</ul>

<h3>Title: Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation. (arXiv:2310.01701v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01701">http://arxiv.org/abs/2310.01701</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01701]] Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation(http://arxiv.org/abs/2310.01701)</code></li>
<li>Summary: <p>Domain Adaptation (DA) is a method for enhancing a model's performance on a
target domain with inadequate annotated data by applying the information the
model has acquired from a related source domain with sufficient labeled data.
The escalating enforcement of data-privacy regulations like HIPAA, COPPA,
FERPA, etc. have sparked a heightened interest in adapting models to novel
domains while circumventing the need for direct access to the source data, a
problem known as Source-Free Domain Adaptation (SFDA). In this paper, we
propose a novel framework for SFDA that generates source data using a
text-to-image diffusion model trained on the target domain samples. Our method
starts by training a text-to-image diffusion model on the labeled target domain
samples, which is then fine-tuned using the pre-trained source model to
generate samples close to the source data. Finally, we use Domain Adaptation
techniques to align the artificially generated source data with the target
domain data, resulting in significant performance improvements of the model on
the target domain. Through extensive comparison against several baselines on
the standard Office-31, Office-Home, and VisDA benchmarks, we demonstrate the
effectiveness of our approach for the SFDA task.
</p></li>
</ul>

<h3>Title: Amazing Combinatorial Creation: Acceptable Swap-Sampling for Text-to-Image Generation. (arXiv:2310.01819v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01819">http://arxiv.org/abs/2310.01819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01819]] Amazing Combinatorial Creation: Acceptable Swap-Sampling for Text-to-Image Generation(http://arxiv.org/abs/2310.01819)</code></li>
<li>Summary: <p>Exploring a machine learning system to generate meaningful combinatorial
object images from multiple textual descriptions, emulating human creativity,
is a significant challenge as humans are able to construct amazing
combinatorial objects, but machines strive to emulate data distribution. In
this paper, we develop a straightforward yet highly effective technique called
acceptable swap-sampling to generate a combinatorial object image that exhibits
novelty and surprise, utilizing text concepts of different objects. Initially,
we propose a swapping mechanism that constructs a novel embedding by exchanging
column vectors of two text embeddings for generating a new combinatorial image
through a cutting-edge diffusion model. Furthermore, we design an acceptable
region by managing suitable CLIP distances between the new image and the
original concept generations, increasing the likelihood of accepting the new
image with a high-quality combination. This region allows us to efficiently
sample a small subset from a new image pool generated by using randomly
exchanging column vectors. Lastly, we employ a segmentation method to compare
CLIP distances among the segmented components, ultimately selecting the most
promising object image from the sampled subset. Our experiments focus on text
pairs of objects from ImageNet, and our results demonstrate that our approach
outperforms recent methods such as Stable-Diffusion2, DALLE2, ERNIE-ViLG2 and
Bing in generating novel and surprising object images, even when the associated
concepts appear to be implausible, such as lionfish-abacus. Furthermore, during
the sampling process, our approach without training and human preference is
also comparable to PickScore and HPSv2 trained using human preference datasets.
</p></li>
</ul>

<h3>Title: Global Attractor for a Reaction-Diffusion Model Arising in Biological Dynamic in 3D Soil Structure. (arXiv:2310.02060v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02060">http://arxiv.org/abs/2310.02060</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02060]] Global Attractor for a Reaction-Diffusion Model Arising in Biological Dynamic in 3D Soil Structure(http://arxiv.org/abs/2310.02060)</code></li>
<li>Summary: <p>Partial Differential Equations (PDEs) play a crucial role as tools for
modeling and comprehending intricate natural processes, notably within the
domain of biology. This research explores the domain of microbial activity
within the complex matrix of 3D soil structures, providing valuable
understanding into both the existence and uniqueness of solutions and the
asymptotic behavior of the corresponding PDE model. Our investigation results
in the discovery of a global attractor, a fundamental feature with significant
implications for long-term system behavior. To enhance the clarity of our
findings, numerical simulations are employed to visually illustrate the
attributes of this global attractor.
</p></li>
</ul>

<h3>Title: Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01929">http://arxiv.org/abs/2310.01929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01929]] Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models(http://arxiv.org/abs/2310.01929)</code></li>
<li>Summary: <p>Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have
recently gained prominence for their remarkable zero-shot capabilities in
generating images guided by textual prompts. Language, as a conduit of culture,
plays a pivotal role in these models' multilingual capabilities, which in turn
shape their cultural agency. In this study, we explore the cultural perception
embedded in TTI models by characterizing culture across three hierarchical
tiers: cultural dimensions, cultural domains, and cultural concepts. We propose
a comprehensive suite of evaluation techniques, including intrinsic evaluations
using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA)
model, and human assessments, to discern TTI cultural perceptions. To
facilitate our research, we introduce the CulText2I dataset, derived from four
diverse TTI models and spanning ten languages. Our experiments reveal insights
into these models' cultural awareness, cultural distinctions, and the unlocking
of cultural features, releasing the potential for cross-cultural applications.
</p></li>
</ul>

<h3>Title: Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods. (arXiv:2310.01618v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01618">http://arxiv.org/abs/2310.01618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01618]] Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods(http://arxiv.org/abs/2310.01618)</code></li>
<li>Summary: <p>Deep neural networks, despite their success in numerous applications, often
function without established theoretical foundations. In this paper, we bridge
this gap by drawing parallels between deep learning and classical numerical
analysis. By framing neural networks as operators with fixed points
representing desired solutions, we develop a theoretical framework grounded in
iterative methods for operator equations. Under defined conditions, we present
convergence proofs based on fixed point theory. We demonstrate that popular
architectures, such as diffusion models and AlphaFold, inherently employ
iterative operator learning. Empirical assessments highlight that performing
iterations through network operators improves performance. We also introduce an
iterative graph neural network, PIGN, that further demonstrates benefits of
iterations. Our work aims to enhance the understanding of deep learning by
merging insights from numerical analysis, potentially guiding the design of
future networks with clearer theoretical underpinnings and improved
performance.
</p></li>
</ul>

<h3>Title: Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization. (arXiv:2310.01762v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01762">http://arxiv.org/abs/2310.01762</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01762]] Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization(http://arxiv.org/abs/2310.01762)</code></li>
<li>Summary: <p>There is a long history, as well as a recent explosion of interest, in
statistical and generative modeling approaches based on score functions --
derivatives of the log-likelihood of a distribution. In seminal works,
Hyv\"arinen proposed vanilla score matching as a way to learn distributions
from data by computing an estimate of the score function of the underlying
ground truth, and established connections between this method and established
techniques like Contrastive Divergence and Pseudolikelihood estimation. It is
by now well-known that vanilla score matching has significant difficulties
learning multimodal distributions. Although there are various ways to overcome
this difficulty, the following question has remained unanswered -- is there a
natural way to sample multimodal distributions using just the vanilla score?
Inspired by a long line of related experimental works, we prove that the
Langevin diffusion with early stopping, initialized at the empirical
distribution, and run on a score function estimated from data successfully
generates natural multimodal distributions (mixtures of log-concave
distributions).
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Adaptive Visual Scene Understanding: Incremental Scene Graph Generation. (arXiv:2310.01636v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01636">http://arxiv.org/abs/2310.01636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01636]] Adaptive Visual Scene Understanding: Incremental Scene Graph Generation(http://arxiv.org/abs/2310.01636)</code></li>
<li>Summary: <p>Scene graph generation (SGG) involves analyzing images to extract meaningful
information about objects and their relationships. Given the dynamic nature of
the visual world, it becomes crucial for AI systems to detect new objects and
establish their new relationships with existing objects. To address the lack of
continual learning methodologies in SGG, we introduce the comprehensive
Continual ScenE Graph Generation (CSEGG) dataset along with 3 learning
scenarios and 8 evaluation metrics. Our research investigates the continual
learning performances of existing SGG methods on the retention of previous
object entities and relationships as they learn new ones. Moreover, we also
explore how continual object detection enhances generalization in classifying
known relationships on unknown objects. We conduct extensive experiments
benchmarking and analyzing the classical two-stage SGG methods and the most
recent transformer-based SGG methods in continual learning settings, and gain
valuable insights into the CSEGG problem. We invite the research community to
explore this emerging field of study.
</p></li>
</ul>

<h3>Title: Improvement and Enhancement of YOLOv5 Small Target Recognition Based on Multi-module Optimization. (arXiv:2310.01806v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01806">http://arxiv.org/abs/2310.01806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01806]] Improvement and Enhancement of YOLOv5 Small Target Recognition Based on Multi-module Optimization(http://arxiv.org/abs/2310.01806)</code></li>
<li>Summary: <p>In this paper, the limitations of YOLOv5s model on small target detection
task are deeply studied and improved. The performance of the model is
successfully enhanced by introducing GhostNet-based convolutional module,
RepGFPN-based Neck module optimization, CA and Transformer's attention
mechanism, and loss function improvement using NWD. The experimental results
validate the positive impact of these improvement strategies on model
precision, recall and mAP. In particular, the improved model shows significant
superiority in dealing with complex backgrounds and tiny targets in real-world
application tests. This study provides an effective optimization strategy for
the YOLOv5s model on small target detection, and lays a solid foundation for
future related research and applications.
</p></li>
</ul>

<h3>Title: PPT: Token Pruning and Pooling for Efficient Vision Transformers. (arXiv:2310.01812v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01812">http://arxiv.org/abs/2310.01812</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01812]] PPT: Token Pruning and Pooling for Efficient Vision Transformers(http://arxiv.org/abs/2310.01812)</code></li>
<li>Summary: <p>Vision Transformers (ViTs) have emerged as powerful models in the field of
computer vision, delivering superior performance across various vision tasks.
However, the high computational complexity poses a significant barrier to their
practical applications in real-world scenarios. Motivated by the fact that not
all tokens contribute equally to the final predictions and fewer tokens bring
less computational cost, reducing redundant tokens has become a prevailing
paradigm for accelerating vision transformers. However, we argue that it is not
optimal to either only reduce inattentive redundancy by token pruning, or only
reduce duplicative redundancy by token merging. To this end, in this paper we
propose a novel acceleration framework, namely token Pruning &amp; Pooling
Transformers (PPT), to adaptively tackle these two types of redundancy in
different layers. By heuristically integrating both token pruning and token
pooling techniques in ViTs without additional trainable parameters, PPT
effectively reduces the model complexity while maintaining its predictive
accuracy. For example, PPT reduces over 37% FLOPs and improves the throughput
by over 45% for DeiT-S without any accuracy drop on the ImageNet dataset.
</p></li>
</ul>

<h3>Title: Selective Feature Adapter for Dense Vision Transformers. (arXiv:2310.01843v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01843">http://arxiv.org/abs/2310.01843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01843]] Selective Feature Adapter for Dense Vision Transformers(http://arxiv.org/abs/2310.01843)</code></li>
<li>Summary: <p>Fine-tuning pre-trained transformer models, e.g., Swin Transformer, are
successful in numerous downstream for dense prediction vision tasks. However,
one major issue is the cost/storage of their huge amount of parameters, which
becomes increasingly challenging to handle with the growing amount of vision
tasks. In this paper, we propose an effective approach to alleviate the issue,
namely selective feature adapter (SFA). It achieves state-of-the-art (SoTA)
performance under any given budget of trainable parameters, and demonstrates
comparable or better performance than fully fine-tuned models across various
dense tasks. Specifically, SFA consists of external adapters and internal
adapters which are sequentially operated over a transformer model. For external
adapters, we properly select the places and amount of additional multilayer
perception (MLP). For internal adapters, we transform a few task-important
parameters inside the transformer, which are automatically discovered through a
simple yet effective lottery ticket algorithm. Our experiments show that the
dual adapter module, a.k.a SFA, is essential to achieve the best trade-off on
dense vision tasks, such as segmentation, detection and depth-estimation,
outperforming other adapters with a single module.
</p></li>
</ul>

<h3>Title: MFOS: Model-Free & One-Shot Object Pose Estimation. (arXiv:2310.01897v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01897">http://arxiv.org/abs/2310.01897</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01897]] MFOS: Model-Free & One-Shot Object Pose Estimation(http://arxiv.org/abs/2310.01897)</code></li>
<li>Summary: <p>Existing learning-based methods for object pose estimation in RGB images are
mostly model-specific or category based. They lack the capability to generalize
to new object categories at test time, hence severely hindering their
practicability and scalability. Notably, recent attempts have been made to
solve this issue, but they still require accurate 3D data of the object surface
at both train and test time. In this paper, we introduce a novel approach that
can estimate in a single forward pass the pose of objects never seen during
training, given minimum input. In contrast to existing state-of-the-art
approaches, which rely on task-specific modules, our proposed model is entirely
based on a transformer architecture, which can benefit from recently proposed
3D-geometry general pretraining. We conduct extensive experiments and report
state-of-the-art one-shot performance on the challenging LINEMOD benchmark.
Finally, extensive ablations allow us to determine good practices with this
relatively new type of architecture in the field.
</p></li>
</ul>

<h3>Title: Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. (arXiv:2310.01749v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01749">http://arxiv.org/abs/2310.01749</a></li>
<li>Code URL: https://github.com/bdusell/stack-attention</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01749]] Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns(http://arxiv.org/abs/2310.01749)</code></li>
<li>Summary: <p>Attention, specifically scaled dot-product attention, has proven effective
for natural language, but it does not have a mechanism for handling
hierarchical patterns of arbitrary nesting depth, which limits its ability to
recognize certain syntactic structures. To address this shortcoming, we propose
stack attention: an attention operator that incorporates stacks, inspired by
their theoretical connections to context-free languages (CFLs). We show that
stack attention is analogous to standard attention, but with a latent model of
syntax that requires no syntactic supervision. We propose two variants: one
related to deterministic pushdown automata (PDAs) and one based on
nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs.
We show that transformers with stack attention are very effective at learning
CFLs that standard transformers struggle on, achieving strong results on a CFL
with theoretically maximal parsing difficulty. We also show that stack
attention is more effective at natural language modeling under a constrained
parameter budget, and we include results on machine translation.
</p></li>
</ul>

<h3>Title: Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01889">http://arxiv.org/abs/2310.01889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01889]] Ring Attention with Blockwise Transformers for Near-Infinite Context(http://arxiv.org/abs/2310.01889)</code></li>
<li>Summary: <p>Transformers have emerged as the architecture of choice for many
state-of-the-art AI models, showcasing exceptional performance across a wide
range of AI applications. However, the memory demands imposed by Transformers
limit their ability to handle long sequences, thereby creating challenges for
tasks involving extended sequences or long-term dependencies. We present a
distinct approach, Ring Attention, which leverages blockwise computation of
self-attention to distribute long sequences across multiple devices while
concurrently overlapping the communication of key-value blocks with the
computation of blockwise attention. By processing longer input sequences while
maintaining memory efficiency, Ring Attention enables training and inference of
sequences that are device count times longer than those of prior
memory-efficient Transformers, effectively eliminating the memory constraints
imposed by individual devices. Extensive experiments on language modeling tasks
demonstrate the effectiveness of Ring Attention in allowing large sequence
input size and improving performance.
</p></li>
</ul>

<h3>Title: Language Models as Knowledge Bases for Visual Word Sense Disambiguation. (arXiv:2310.01960v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01960">http://arxiv.org/abs/2310.01960</a></li>
<li>Code URL: https://github.com/anastasiakrith/llm-for-vwsd</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01960]] Language Models as Knowledge Bases for Visual Word Sense Disambiguation(http://arxiv.org/abs/2310.01960)</code></li>
<li>Summary: <p>Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies
between linguistic sense disambiguation and fine-grained multimodal retrieval.
The recent advancements in the development of visiolinguistic (VL) transformers
suggest some off-the-self implementations with encouraging results, which
however we argue that can be further improved. To this end, we propose some
knowledge-enhancement techniques towards improving the retrieval performance of
VL transformers via the usage of Large Language Models (LLMs) as Knowledge
Bases. More specifically, knowledge stored in LLMs is retrieved with the help
of appropriate prompts in a zero-shot manner, achieving performance
advancements. Moreover, we convert VWSD to a purely textual question-answering
(QA) problem by considering generated image captions as multiple-choice
candidate answers. Zero-shot and few-shot prompting strategies are leveraged to
explore the potential of such a transformation, while Chain-of-Thought (CoT)
prompting in the zero-shot setting is able to reveal the internal reasoning
steps an LLM follows to select the appropriate candidate. In total, our
presented approach is the first one to analyze the merits of exploiting
knowledge stored in LLMs in different ways to solve WVSD.
</p></li>
</ul>

<h3>Title: PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels. (arXiv:2310.01655v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01655">http://arxiv.org/abs/2310.01655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01655]] PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels(http://arxiv.org/abs/2310.01655)</code></li>
<li>Summary: <p>The quadratic complexity of attention in transformer architectures remains a
big bottleneck in scaling up large foundation models for long context. In fact,
recent theoretical results show the hardness of approximating the output of
softmax attention mechanism in sub-quadratic time assuming Strong Exponential
Time Hypothesis. In this paper, we show how to break this theoretical barrier
by replacing softmax with a polynomial function and polynomial sketching. In
particular we show that sketches for Polynomial Kernel from the randomized
numerical linear algebra literature can be used to approximate the polynomial
attention which leads to a significantly faster attention mechanism without
assuming any sparse structure for the attention matrix that has been done in
many previous works.
</p>
<p>In addition, we propose an efficient block-based algorithm that lets us apply
the causal mask to the attention matrix without explicitly realizing the $n
\times n$ attention matrix and compute the output of the polynomial attention
mechanism in time linear in the context length. The block-based algorithm gives
significant speedups over the \emph{cumulative sum} algorithm used by Performer
to apply the causal mask to the attention matrix. These observations help us
design \emph{PolySketchFormer}, a practical linear-time transformer
architecture for language modeling with provable guarantees.
</p>
<p>We validate our design empirically by training language models with long
context lengths. We first show that the eval perplexities of our models are
comparable to that of models trained with softmax attention. We then show that
for large context lengths our training times are significantly faster than
FlashAttention.
</p></li>
</ul>

<h3>Title: Transformers are efficient hierarchical chemical graph learners. (arXiv:2310.01704v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01704">http://arxiv.org/abs/2310.01704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01704]] Transformers are efficient hierarchical chemical graph learners(http://arxiv.org/abs/2310.01704)</code></li>
<li>Summary: <p>Transformers, adapted from natural language processing, are emerging as a
leading approach for graph representation learning. Contemporary graph
transformers often treat nodes or edges as separate tokens. This approach leads
to computational challenges for even moderately-sized graphs due to the
quadratic scaling of self-attention complexity with token count. In this paper,
we introduce SubFormer, a graph transformer that operates on subgraphs that
aggregate information by a message-passing mechanism. This approach reduces the
number of tokens and enhances learning long-range interactions. We demonstrate
SubFormer on benchmarks for predicting molecular properties from chemical
structures and show that it is competitive with state-of-the-art graph
transformers at a fraction of the computational cost, with training times on
the order of minutes on a consumer-grade graphics card. We interpret the
attention weights in terms of chemical structures. We show that SubFormer
exhibits limited over-smoothing and avoids over-squashing, which is prevalent
in traditional graph neural networks.
</p></li>
</ul>

<h3>Title: PrACTiS: Perceiver-Attentional Copulas for Time Series. (arXiv:2310.01720v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01720">http://arxiv.org/abs/2310.01720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01720]] PrACTiS: Perceiver-Attentional Copulas for Time Series(http://arxiv.org/abs/2310.01720)</code></li>
<li>Summary: <p>Transformers incorporating copula structures have demonstrated remarkable
performance in time series prediction. However, their heavy reliance on
self-attention mechanisms demands substantial computational resources, thus
limiting their practical utility across a wide range of tasks. In this work, we
present a model that combines the perceiver architecture with a copula
structure to enhance time-series forecasting. By leveraging the perceiver as
the encoder, we efficiently transform complex, high-dimensional, multimodal
data into a compact latent space, thereby significantly reducing computational
demands. To further reduce complexity, we introduce midpoint inference and
local attention mechanisms, enabling the model to capture dependencies within
imputed samples effectively. Subsequently, we deploy the copula-based attention
and output variance testing mechanism to capture the joint distribution of
missing data, while simultaneously mitigating error propagation during
prediction. Our experimental results on the unimodal and multimodal benchmarks
showcase a consistent 20\% improvement over the state-of-the-art methods, while
utilizing less than half of available memory resources.
</p></li>
</ul>

<h3>Title: The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers. (arXiv:2310.02041v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02041">http://arxiv.org/abs/2310.02041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02041]] The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers(http://arxiv.org/abs/2310.02041)</code></li>
<li>Summary: <p>To enhance the computational efficiency of quantized Transformers, we replace
the dot-product and Softmax-based attention with an alternative mechanism
involving addition and ReLU activation only. This side-steps the expansion to
double precision often required by matrix multiplication and avoids costly
Softmax evaluations but maintains much of the core functionality of
conventional dot-product attention. It can enable more efficient execution and
support larger quantized Transformer models on resource-constrained hardware or
alternative arithmetic systems like homomorphic encryption. Training
experiments on four common benchmark tasks show test set prediction scores
comparable to those of conventional Transformers with dot-product attention.
Our scaling experiments also suggest significant computational savings, both in
plaintext and under encryption. In particular, we believe that the ReLU and
addition-based attention mechanism introduced in this paper may enable
privacy-preserving AI applications operating under homomorphic encryption by
avoiding the costly multiplication of encrypted variables.
</p></li>
</ul>

<h3>Title: De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02066">http://arxiv.org/abs/2310.02066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02066]] De Novo Drug Design with Joint Transformers(http://arxiv.org/abs/2310.02066)</code></li>
<li>Summary: <p>De novo drug design requires simultaneously generating novel molecules
outside of training data and predicting their target properties, making it a
hard task for generative models. To address this, we propose Joint Transformer
that combines a Transformer decoder, a Transformer encoder, and a predictor in
a joint generative model with shared weights. We show that training the model
with a penalized log-likelihood objective results in state-of-the-art
performance in molecule generation, while decreasing the prediction error on
newly sampled molecules, as compared to a fine-tuned decoder-only Transformer,
by 42%. Finally, we propose a probabilistic black-box optimization algorithm
that employs Joint Transformer to generate novel molecules with improved target
properties, as compared to the training data, outperforming other SMILES-based
optimization methods in de novo drug design.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Generative Autoencoding of Dropout Patterns. (arXiv:2310.01712v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01712">http://arxiv.org/abs/2310.01712</a></li>
<li>Code URL: https://github.com/shuntama/deciphering-autoencoders</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01712]] Generative Autoencoding of Dropout Patterns(http://arxiv.org/abs/2310.01712)</code></li>
<li>Summary: <p>We propose a generative model termed Deciphering Autoencoders. In this model,
we assign a unique random dropout pattern to each data point in the training
dataset and then train an autoencoder to reconstruct the corresponding data
point using this pattern as information to be encoded. Since the training of
Deciphering Autoencoders relies solely on reconstruction error, it offers more
stable training than other generative models. Despite its simplicity,
Deciphering Autoencoders show comparable sampling quality to DCGAN on the
CIFAR-10 dataset.
</p></li>
</ul>

<h3>Title: AI-Generated Images as Data Source: The Dawn of Synthetic Era. (arXiv:2310.01830v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01830">http://arxiv.org/abs/2310.01830</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01830]] AI-Generated Images as Data Source: The Dawn of Synthetic Era(http://arxiv.org/abs/2310.01830)</code></li>
<li>Summary: <p>The advancement of visual intelligence is intrinsically tethered to the
availability of data. In parallel, generative Artificial Intelligence (AI) has
unlocked the potential to create synthetic images that closely resemble
real-world photographs, which prompts a compelling inquiry: how visual
intelligence benefit from the advance of generative AI? This paper explores the
innovative concept of harnessing these AI-generated images as a new data
source, reshaping traditional model paradigms in visual intelligence. In
contrast to real data, AI-generated data sources exhibit remarkable advantages,
including unmatched abundance and scalability, the rapid generation of vast
datasets, and the effortless simulation of edge cases. Built on the success of
generative AI models, we examines the potential of their generated data in a
range of applications, from training machine learning models to simulating
scenarios for computational modelling, testing, and validation. We probe the
technological foundations that support this groundbreaking use of generative
AI, engaging in an in-depth discussion on the ethical, legal, and practical
considerations that accompany this transformative paradigm shift. Through an
exhaustive survey of current technologies and applications, this paper presents
a comprehensive view of the synthetic era in visual intelligence. A project
with this paper can be found at https://github.com/mwxely/AIGS .
</p></li>
</ul>

<h3>Title: A Dual Attentive Generative Adversarial Network for Remote Sensing Image Change Detection. (arXiv:2310.01876v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01876">http://arxiv.org/abs/2310.01876</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01876]] A Dual Attentive Generative Adversarial Network for Remote Sensing Image Change Detection(http://arxiv.org/abs/2310.01876)</code></li>
<li>Summary: <p>Remote sensing change detection between bi-temporal images receives growing
concentration from researchers. However, comparing two bi-temporal images for
detecting changes is challenging, as they demonstrate different appearances. In
this paper, we propose a dual attentive generative adversarial network for
achieving very high-resolution remote sensing image change detection tasks,
which regards the detection model as a generator and attains the optimal
weights of the detection model without increasing the parameters of the
detection model through generative-adversarial strategy, boosting the spatial
contiguity of predictions. Moreover, We design a multi-level feature extractor
for effectively fusing multi-level features, which adopts the pre-trained model
to extract multi-level features from bi-temporal images and introduces
aggregate connections to fuse them. To strengthen the identification of
multi-scale objects, we propose a multi-scale adaptive fusion module to
adaptively fuse multi-scale features through various receptive fields and
design a context refinement module to explore contextual dependencies.
Moreover, the DAGAN framework utilizes the 4-layer convolution network as a
discriminator to identify whether the synthetic image is fake or real.
Extensive experiments represent that the DAGAN framework has better performance
with 85.01% mean IoU and 91.48% mean F1 score than advanced methods on the
LEVIR dataset.
</p></li>
</ul>

<h3>Title: Understanding Masked Autoencoders From a Local Contrastive Perspective. (arXiv:2310.01994v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01994">http://arxiv.org/abs/2310.01994</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01994]] Understanding Masked Autoencoders From a Local Contrastive Perspective(http://arxiv.org/abs/2310.01994)</code></li>
<li>Summary: <p>Masked AutoEncoder(MAE) has revolutionized the field of self-supervised
learning with its simple yet effective masking and reconstruction strategies.
However, despite achieving state-of-the-art performance across various
downstream vision tasks, the underlying mechanisms that drive MAE's efficacy
are less well-explored compared to the canonical contrastive learning paradigm.
In this paper, we explore a new perspective to explain what truly contributes
to the "rich hidden representations inside the MAE". Firstly, concerning MAE's
generative pretraining pathway, with a unique encoder-decoder architecture to
reconstruct images from aggressive masking, we conduct an in-depth analysis of
the decoder's behaviors. We empirically find that MAE's decoder mainly learns
local features with a limited receptive field, adhering to the well-known
Locality Principle. Building upon this locality assumption, we propose a
theoretical framework that reformulates the reconstruction-based MAE into a
local region-level contrastive learning form for improved understanding.
Furthermore, to substantiate the local contrastive nature of MAE, we introduce
a Siamese architecture that combines the essence of MAE and contrastive
learning without masking and explicit decoder, which sheds light on a unified
and more flexible self-supervised learning framework.
</p></li>
</ul>

<h3>Title: Closing the Curious Case of Neural Text Degeneration. (arXiv:2310.01693v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01693">http://arxiv.org/abs/2310.01693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01693]] Closing the Curious Case of Neural Text Degeneration(http://arxiv.org/abs/2310.01693)</code></li>
<li>Summary: <p>Despite their ubiquity in language generation, it remains unknown why
truncation sampling heuristics like nucleus sampling are so effective. We
provide a theoretical explanation for the effectiveness of the truncation
sampling by proving that truncation methods that discard tokens below some
probability threshold (the most common type of truncation) can guarantee that
all sampled tokens have nonzero true probability. However, thresholds are a
coarse heuristic, and necessarily discard some tokens with nonzero true
probability as well. In pursuit of a more precise sampling strategy, we show
that we can leverage a known source of model errors, the softmax bottleneck, to
prove that certain tokens have nonzero true probability, without relying on a
threshold. Based on our findings, we develop an experimental truncation
strategy and the present pilot studies demonstrating the promise of this type
of algorithm. Our evaluations show that our method outperforms its
threshold-based counterparts under automatic and human evaluation metrics for
low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical
findings and pilot experiments provide both insight into why truncation
sampling works, and make progress toward more expressive sampling algorithms
that better surface the generative capabilities of large language models.
</p></li>
</ul>

<h3>Title: Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. (arXiv:2310.01801v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01801">http://arxiv.org/abs/2310.01801</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01801]] Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs(http://arxiv.org/abs/2310.01801)</code></li>
<li>Summary: <p>In this study, we introduce adaptive KV cache compression, a plug-and-play
method that reduces the memory footprint of generative inference for Large
Language Models (LLMs). Different from the conventional KV cache that retains
key and value vectors for all context tokens, we conduct targeted profiling to
discern the intrinsic structure of attention modules. Based on the recognized
structure, we then construct the KV cache in an adaptive manner: evicting
long-range contexts on attention heads emphasizing local contexts, discarding
non-special tokens on attention heads centered on special tokens, and only
employing the standard KV cache for attention heads that broadly attend to all
tokens. Moreover, with the lightweight attention profiling used to guide the
construction of the adaptive KV cache, FastGen can be deployed without
resource-intensive fine-tuning or re-training. In our experiments across
various asks, FastGen demonstrates substantial reduction on GPU memory
consumption with negligible generation quality loss. We will release our code
and the compatible CUDA kernel for reproducibility.
</p></li>
</ul>

<h3>Title: Graph Neural Architecture Search with GPT-4. (arXiv:2310.01436v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01436">http://arxiv.org/abs/2310.01436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01436]] Graph Neural Architecture Search with GPT-4(http://arxiv.org/abs/2310.01436)</code></li>
<li>Summary: <p>Graph Neural Architecture Search (GNAS) has shown promising results in
automatically designing graph neural networks. However, GNAS still requires
intensive human labor with rich domain knowledge to design the search space and
search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new
GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The
basic idea of our method is to design a new class of prompts for GPT-4 to guide
GPT-4 toward the generative task of graph neural architectures. The prompts
consist of descriptions of the search space, search strategy, and search
feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS
generates more accurate graph neural networks with fast convergence.
Experimental results show that embedding GPT-4 into GNAS outperforms the
state-of-the-art GNAS methods.
</p></li>
</ul>

<h3>Title: CODA: Temporal Domain Generalization via Concept Drift Simulator. (arXiv:2310.01508v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01508">http://arxiv.org/abs/2310.01508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01508]] CODA: Temporal Domain Generalization via Concept Drift Simulator(http://arxiv.org/abs/2310.01508)</code></li>
<li>Summary: <p>In real-world applications, machine learning models often become obsolete due
to shifts in the joint distribution arising from underlying temporal trends, a
phenomenon known as the "concept drift". Existing works propose model-specific
strategies to achieve temporal generalization in the near-future domain.
However, the diverse characteristics of real-world datasets necessitate
customized prediction model architectures. To this end, there is an urgent
demand for a model-agnostic temporal domain generalization approach that
maintains generality across diverse data modalities and architectures. In this
work, we aim to address the concept drift problem from a data-centric
perspective to bypass considering the interaction between data and model.
Developing such a framework presents non-trivial challenges: (i) existing
generative models struggle to generate out-of-distribution future data, and
(ii) precisely capturing the temporal trends of joint distribution along
chronological source domains is computationally infeasible. To tackle the
challenges, we propose the COncept Drift simulAtor (CODA) framework
incorporating a predicted feature correlation matrix to simulate future data
for model training. Specifically, CODA leverages feature correlations to
represent data characteristics at specific time points, thereby circumventing
the daunting computational costs. Experimental results demonstrate that using
CODA-generated data as training input effectively achieves temporal domain
generalization across different model architectures.
</p></li>
</ul>

<h3>Title: Nowcasting day-ahead marginal emissions using multi-headed CNNs and deep generative models. (arXiv:2310.01524v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01524">http://arxiv.org/abs/2310.01524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01524]] Nowcasting day-ahead marginal emissions using multi-headed CNNs and deep generative models(http://arxiv.org/abs/2310.01524)</code></li>
<li>Summary: <p>Nowcasting day-ahead marginal emissions factors is increasingly important for
power systems with high flexibility and penetration of distributed energy
resources. With a significant share of firm generation from natural gas and
coal power plants, forecasting day-ahead emissions in the current energy system
has been widely studied. In contrast, as we shift to an energy system
characterized by flexible power markets, dispatchable sources, and competing
low-cost generation such as large-scale battery or hydrogen storage, system
operators will be able to choose from a mix of different generation as well as
emission pathways. To fully develop the emissions implications of a given
dispatch schedule, we need a near real-time workflow with two layers. The first
layer is a market model that continuously solves a security-constrained
economic dispatch model. The second layer determines the marginal emissions
based on the output of the market model, which is the subject of this paper. We
propose using multi-headed convolutional neural networks to generate day-ahead
forecasts of marginal and average emissions for a given independent system
operator.
</p></li>
</ul>

<h3>Title: Fusing Models with Complementary Expertise. (arXiv:2310.01542v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01542">http://arxiv.org/abs/2310.01542</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01542]] Fusing Models with Complementary Expertise(http://arxiv.org/abs/2310.01542)</code></li>
<li>Summary: <p>Training AI models that generalize across tasks and domains has long been
among the open problems driving AI research. The emergence of Foundation Models
made it easier to obtain expert models for a given task, but the heterogeneity
of data that may be encountered at test time often means that any single expert
is insufficient. We consider the Fusion of Experts (FoE) problem of fusing
outputs of expert models with complementary knowledge of the data distribution
and formulate it as an instance of supervised learning. Our method is
applicable to both discriminative and generative tasks and leads to significant
performance improvements in image and text classification, text summarization,
multiple-choice QA, and automatic evaluation of generated text. We also extend
our method to the "frugal" setting where it is desired to reduce the number of
expert model evaluations at test time.
</p></li>
</ul>

<h3>Title: Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder. (arXiv:2310.01937v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01937">http://arxiv.org/abs/2310.01937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01937]] Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder(http://arxiv.org/abs/2310.01937)</code></li>
<li>Summary: <p>An essential and challenging problem in causal inference is causal effect
estimation from observational data. The problem becomes more difficult with the
presence of unobserved confounding variables. The front-door adjustment is a
practical approach for dealing with unobserved confounding variables. However,
the restriction for the standard front-door adjustment is difficult to satisfy
in practice. In this paper, we relax some of the restrictions by proposing the
concept of conditional front-door (CFD) adjustment and develop the theorem that
guarantees the causal effect identifiability of CFD adjustment. Furthermore, as
it is often impossible for a CFD variable to be given in practice, it is
desirable to learn it from data. By leveraging the ability of deep generative
models, we propose CFDiVAE to learn the representation of the CFD adjustment
variable directly from data with the identifiable Variational AutoEncoder and
formally prove the model identifiability. Extensive experiments on synthetic
datasets validate the effectiveness of CFDiVAE and its superiority over
existing methods. The experiments also show that the performance of CFDiVAE is
less sensitive to the causal strength of unobserved confounding variables. We
further apply CFDiVAE to a real-world dataset to demonstrate its potential
application.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Tuning Large language model for End-to-end Speech Translation. (arXiv:2310.02050v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02050">http://arxiv.org/abs/2310.02050</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02050]] Tuning Large language model for End-to-end Speech Translation(http://arxiv.org/abs/2310.02050)</code></li>
<li>Summary: <p>With the emergence of large language models (LLMs), multimodal models based
on LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM,
and SpeechGPT exhibit an impressive ability to comprehend and generate human
instructions. However, their performance often falters when faced with complex
tasks like end-to-end speech translation (E2E-ST), a cross-language and
cross-modal translation task. In comparison to single-modal models, multimodal
models lag behind in these scenarios. This paper introduces LST, a Large
multimodal model designed to excel at the E2E-ST task. LST consists of a speech
frontend, an adapter, and a LLM backend. The training of LST consists of two
stages: (1) Modality adjustment, where the adapter is tuned to align speech
representation with text embedding space, and (2) Downstream task fine-tuning,
where both the adapter and LLM model are trained to optimize performance on the
E2EST task. Experimental results on the MuST-C speech translation benchmark
demonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on
En-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a
new state-of-the-art. Additionally, we conduct an in-depth analysis of
single-modal model selection and the impact of training strategies, which lays
the foundation for future research. We will open up our code and models after
review.
</p></li>
</ul>

<h3>Title: Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01420">http://arxiv.org/abs/2310.01420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01420]] Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems(http://arxiv.org/abs/2310.01420)</code></li>
<li>Summary: <p>Conversational tutoring systems (CTSs) offer learning experiences driven by
natural language interaction. They are known to promote high levels of
cognitive engagement and benefit learning outcomes, particularly in reasoning
tasks. Nonetheless, the time and cost required to author CTS content is a major
obstacle to widespread adoption. In this paper, we introduce a novel type of
CTS that leverages the recent advances in large language models (LLMs) in two
ways: First, the system induces a tutoring script automatically from a lesson
text. Second, the system automates the script orchestration via two LLM-based
agents (Ruffle&amp;Riley) with the roles of a student and a professor in a
learning-by-teaching format. The system allows a free-form conversation that
follows the ITS-typical outer-/inner-loop structure. In an initial
between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler
QA chatbots and reading activity, we found no significant differences in
post-test scores. Nonetheless, in the learning experience survey, Ruffle&amp;Riley
users expressed higher ratings of understanding and remembering and further
perceived the offered support as more helpful and the conversation as coherent.
Our study provides insights for a new generation of scalable CTS technologies.
</p></li>
</ul>

<h3>Title: Borges and AI. (arXiv:2310.01425v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01425">http://arxiv.org/abs/2310.01425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01425]] Borges and AI(http://arxiv.org/abs/2310.01425)</code></li>
<li>Summary: <p>Many believe that Large Language Models (LLMs) open the era of Artificial
Intelligence (AI). Some see opportunities while others see dangers. Yet both
proponents and opponents grasp AI through the imagery popularised by science
fiction. Will the machine become sentient and rebel against its creators? Will
we experience a paperclip apocalypse? Before answering such questions, we
should first ask whether this mental imagery provides a good description of the
phenomenon at hand. Understanding weather patterns through the moods of the
gods only goes so far. The present paper instead advocates understanding LLMs
and their connection to AI through the imagery of Jorge Luis Borges, a master
of 20th century literature, forerunner of magical realism, and precursor to
postmodern literature. This exercise leads to a new perspective that
illuminates the relation between language modelling and artificial
intelligence.
</p></li>
</ul>

<h3>Title: Chatmap : Large Language Model Interaction with Cartographic Data. (arXiv:2310.01429v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01429">http://arxiv.org/abs/2310.01429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01429]] Chatmap : Large Language Model Interaction with Cartographic Data(http://arxiv.org/abs/2310.01429)</code></li>
<li>Summary: <p>The swift advancement and widespread availability of foundational Large
Language Models (LLMs), complemented by robust fine-tuning methodologies, have
catalyzed their adaptation for innovative and industrious applications.
Enabling LLMs to recognize and interpret geospatial data, while offering a
linguistic access to vast cartographic datasets, is of significant importance.
OpenStreetMap (OSM) is the most ambitious open-source global initiative
offering detailed urban and rural geographic data, curated by a community of
over 10 million contributors, which constitutes a great potential for LLM
applications. In this study, we demonstrate the proof of concept and details of
the process of fine-tuning a relatively small scale (1B parameters) LLM with a
relatively small artificial dataset curated by a more capable teacher model, in
order to provide a linguistic interface to the OSM data of an arbitrary urban
region. Through this interface, users can inquire about a location's
attributes, covering a wide spectrum of concepts, such as its touristic appeal
or the potential profitability of various businesses in that vicinity. The
study aims to provide an initial guideline for such generative artificial
intelligence (AI) adaptations and demonstrate early signs of useful emerging
abilities in this context even in minimal computational settings. The
embeddings of artificially curated prompts including OSM data are also
investigated in detail, which might be instrumental for potential geospatially
aware urban Retrieval Augmented Generation (RAG) applications.
</p></li>
</ul>

<h3>Title: Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01432">http://arxiv.org/abs/2310.01432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01432]] Split and Merge: Aligning Position Biases in Large Language Model based Evaluators(http://arxiv.org/abs/2310.01432)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown promise as automated evaluators for
assessing the quality of answers generated by AI systems. However, these
LLM-based evaluators exhibit position bias, or inconsistency, when used to
evaluate candidate answers in pairwise comparisons, favoring either the first
or second answer regardless of content. To address this limitation, we propose
PORTIA, an alignment-based system designed to mimic human comparison strategies
to calibrate position bias in a lightweight yet effective manner. Specifically,
PORTIA splits the answers into multiple segments, aligns similar content across
candidate answers, and then merges them back into a single prompt for
evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to
evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances
the consistency rates for all the models and comparison forms tested, achieving
an average relative improvement of 47.46%. Remarkably, PORTIA enables less
advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4
model at just 10% of the cost. Furthermore, it rectifies around 80% of the
position bias instances within the GPT-4 model, elevating its consistency rate
up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced
GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with
human evaluators. These findings highlight PORTIA's ability to correct position
bias, improve LLM consistency, and boost performance while keeping
cost-efficiency. This represents a valuable step toward a more reliable and
scalable use of LLMs for automated evaluations across diverse applications.
</p></li>
</ul>

<h3>Title: UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities. (arXiv:2310.01441v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01441">http://arxiv.org/abs/2310.01441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01441]] UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities(http://arxiv.org/abs/2310.01441)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated impressive inferential
capabilities, with numerous research endeavors devoted to enhancing this
capacity through prompting. Despite these efforts, a unified epistemological
foundation is still conspicuously absent. Drawing inspiration from Kant's a
priori philosophy, we propose the UPAR prompting framework, designed to emulate
the structure of human cognition within LLMs. The UPAR framework is delineated
into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the
extraction of structured information from complex contexts, prior planning of
solutions, execution according to plan, and self-reflection. This structure
significantly augments the explainability and accuracy of LLM inference,
producing a human-understandable and inspectable inferential trajectory.
Furthermore, our work offers an epistemological foundation for existing
prompting techniques, allowing for a possible systematic integration of these
methods. With GPT-4, our approach elevates the accuracy from COT baseline of
22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in
the causal judgment task.
</p></li>
</ul>

<h3>Title: Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01444">http://arxiv.org/abs/2310.01444</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01444]] Adapting LLM Agents Through Communication(http://arxiv.org/abs/2310.01444)</code></li>
<li>Summary: <p>Recent advancements in large language models (LLMs) have shown potential for
human-like agents. To help these agents adapt to new tasks without extensive
human supervision, we propose the Learning through Communication (LTC)
paradigm, a novel training approach enabling LLM agents to improve continuously
through interactions with their environments and other agents. Recent
advancements in large language models (LLMs) have shown potential for
human-like agents. To help these agents adapt to new tasks without extensive
human supervision, we propose the Learning through Communication (LTC)
paradigm, a novel training approach enabling LLM agents to improve continuously
through interactions with their environments and other agents. Through
iterative exploration and PPO training, LTC empowers the agent to assimilate
short-term experiences into long-term memory. To optimize agent interactions
for task-specific learning, we introduce three structured communication
patterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as
decision-making, knowledge-intensive reasoning, and numerical reasoning. We
evaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA
(knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld,
it exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA,
LTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it
outperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k,
LTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results
showcase the versatility and efficiency of the LTC approach across diverse
domains. We will open-source our code to promote further development of the
community.
</p></li>
</ul>

<h3>Title: Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning. (arXiv:2310.01446v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01446">http://arxiv.org/abs/2310.01446</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01446]] Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning(http://arxiv.org/abs/2310.01446)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are showcasing impressive ability in handling
complex reasoning tasks. In real-world situations, problems often span a
spectrum of complexities. Humans inherently adjust their problem-solving
approaches based on task complexity. However, most methodologies that leverage
LLMs tend to adopt a uniform approach: utilizing consistent models, prompting
methods, and degrees of problem decomposition, regardless of the problem
complexity. Inflexibility of them can bring unnecessary computational overhead
or sub-optimal performance. To address this problem, we introduce an
Adaptive-Solver framework. It strategically modulates solving strategies based
on the difficulties of the problems. Given an initial solution, the framework
functions with two primary modules. The initial evaluation module assesses the
adequacy of the current solution. If improvements are needed, the subsequent
adaptation module comes into play. Within this module, three key adaptation
strategies are employed: (1) Model Adaptation: Switching to a stronger LLM when
a weaker variant is inadequate. (2) Prompting Method Adaptation: Alternating
between different prompting techniques to suit the problem's nuances. (3)
Decomposition Granularity Adaptation: Breaking down a complex problem into more
fine-grained sub-questions to enhance solvability. Through such dynamic
adaptations, our framework not only enhances computational efficiency but also
elevates the overall performance. This dual-benefit ensures both the efficiency
of the system for simpler tasks and the precision required for more complex
questions. Experimental results from complex reasoning tasks reveal that the
prompting method adaptation and decomposition granularity adaptation enhance
performance across all tasks. Furthermore, the model adaptation approach
significantly reduces API costs (up to 50%) while maintaining superior
performance.
</p></li>
</ul>

<h3>Title: Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01448">http://arxiv.org/abs/2310.01448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01448]] Meta Semantic Template for Evaluation of Large Language Models(http://arxiv.org/abs/2310.01448)</code></li>
<li>Summary: <p>Do large language models (LLMs) genuinely understand the semantics of the
language, or just memorize the training data? The recent concern on potential
data contamination of LLMs has raised awareness of the community to conduct
research on LLMs evaluation. In this paper, we propose MSTemp, an approach that
creates meta semantic templates to evaluate the semantic understanding ability
of LLMs. The core of MSTemp is not to perform evaluation directly on existing
benchmark datasets, but to generate new out-of-distribution (OOD) evaluation
sets using existing datasets as seeds. Specifically, for a given sentence,
MSTemp leverages another language model to generate new samples while
preserving its semantics. The new samples are called semantic templates to the
original sentence. Then, MSTemp generates evaluation samples via sentence
parsing and random word replacement on the semantic templates. MSTemp is highly
flexible, dynamic, and cost-effective. Our initial experiments show that
MSTemp-generated samples can significantly reduce the performance of LLMs using
existing datasets as seeds. We hope this initial work can shed light on future
research of LLMs evaluation.
</p></li>
</ul>

<h3>Title: The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01468">http://arxiv.org/abs/2310.01468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01468]] The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs(http://arxiv.org/abs/2310.01468)</code></li>
<li>Summary: <p>Large language models (LLMs) are currently effective at answering questions
that are clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be challenging. In
this paper, we offer a surrogate problem which assesses an LLMs's capability to
deduce an entity unknown to itself, but revealed to a judge, by asking the
judge a series of queries. This \textit{entity-deducing game} can serve as an
evaluation framework to probe the conversational reasoning and planning
capabilities of language models. We systematically evaluate various LLMs and
discover significant differences in their performance on this task. We find
that strong LLMs like GPT-4 outperform human players by a large margin. We
further employ Behavior Cloning (BC) to examine whether a weaker model is
capable of imitating a stronger model and generalizing to data or domains,
using only the demonstrations from a stronger model. We finally propose to use
Reinforcement Learning to enhance reasoning and planning capacity of Vicuna
models through episodes of game playing, which lead to significant performance
improvement. We hope that this problem offers insights into how autonomous
agents could be trained to behave more intelligently in ambiguous
circumstances.
</p></li>
</ul>

<h3>Title: Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models. (arXiv:2310.01691v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01691">http://arxiv.org/abs/2310.01691</a></li>
<li>Code URL: https://github.com/manga-uofa/ptfer</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01691]] Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models(http://arxiv.org/abs/2310.01691)</code></li>
<li>Summary: <p>Prompt tuning in natural language processing (NLP) has become an increasingly
popular method for adapting large language models to specific tasks. However,
the transferability of these prompts, especially continuous prompts, between
different models remains a challenge. In this work, we propose a zero-shot
continuous prompt transfer method, where source prompts are encoded into
relative space and the corresponding target prompts are searched for
transferring to target models. Experimental results confirm the effectiveness
of our method, showing that 'task semantics' in continuous prompts can be
generalized across various language models. Moreover, we find that combining
'task semantics' from multiple source models can further enhance the
generalizability of transfer.
</p></li>
</ul>

<h3>Title: Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making. (arXiv:2310.01708v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01708">http://arxiv.org/abs/2310.01708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01708]] Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making(http://arxiv.org/abs/2310.01708)</code></li>
<li>Summary: <p>Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and
patient data to offer real-time recommendations, with Large Language Models
(LLMs) emerging as a promising tool to generate plain-text explanations for
medical decisions. This study explores the effectiveness and reliability of
LLMs in generating explanations for diagnoses based on patient complaints.
Three experienced doctors evaluated LLM-generated explanations of the
connection between patient complaints and doctor and model-assigned diagnoses
across several stages. Experimental results demonstrated that LLM explanations
significantly increased doctors' agreement rates with given diagnoses and
highlighted potential errors in LLM outputs, ranging from 5% to 30%. The study
underscores the potential and challenges of LLMs in healthcare and emphasizes
the need for careful integration and evaluation to ensure patient safety and
optimal clinical utility.
</p></li>
</ul>

<h3>Title: Can large language models provide useful feedback on research papers? A large-scale empirical analysis. (arXiv:2310.01783v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01783">http://arxiv.org/abs/2310.01783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01783]] Can large language models provide useful feedback on research papers? A large-scale empirical analysis(http://arxiv.org/abs/2310.01783)</code></li>
<li>Summary: <p>Expert feedback lays the foundation of rigorous research. However, the rapid
growth of scholarly production and intricate knowledge specialization challenge
the conventional scientific feedback mechanisms. High-quality peer reviews are
increasingly difficult to obtain. Researchers who are more junior or from
under-resourced settings have especially hard times getting timely feedback.
With the breakthrough of large language models (LLM) such as GPT-4, there is
growing interest in using LLMs to generate scientific feedback on research
manuscripts. However, the utility of LLM-generated feedback has not been
systematically studied. To address this gap, we created an automated pipeline
using GPT-4 to provide comments on the full PDFs of scientific papers. We
evaluated the quality of GPT-4's feedback through two large-scale studies. We
first quantitatively compared GPT-4's generated feedback with human peer
reviewer feedback in 15 Nature family journals (3,096 papers in total) and the
ICLR machine learning conference (1,709 papers). The overlap in the points
raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature
journals, 39.23% for ICLR) is comparable to the overlap between two human
reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The
overlap between GPT-4 and human reviewers is larger for the weaker papers. We
then conducted a prospective user study with 308 researchers from 110 US
institutions in the field of AI and computational biology to understand how
researchers perceive feedback generated by our GPT-4 system on their own
papers. Overall, more than half (57.4%) of the users found GPT-4 generated
feedback helpful/very helpful and 82.4% found it more beneficial than feedback
from at least some human reviewers. While our findings show that LLM-generated
feedback can help researchers, we also identify several limitations.
</p></li>
</ul>

<h3>Title: Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01798">http://arxiv.org/abs/2310.01798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01798]] Large Language Models Cannot Self-Correct Reasoning Yet(http://arxiv.org/abs/2310.01798)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have emerged as a groundbreaking technology with
their unparalleled text generation capabilities across various applications.
Nevertheless, concerns persist regarding the accuracy and appropriateness of
their generated content. A contemporary methodology, self-correction, has been
proposed as a remedy to these issues. Building upon this premise, this paper
critically examines the role and efficacy of self-correction within LLMs,
shedding light on its true potential and limitations. Central to our
investigation is the notion of intrinsic self-correction, whereby an LLM
attempts to correct its initial responses based solely on its inherent
capabilities, without the crutch of external feedback. In the context of
reasoning, our research indicates that LLMs struggle to self-correct their
responses without external feedback, and at times, their performance might even
degrade post self-correction. Drawing from these insights, we offer suggestions
for future research and practical applications in this field.
</p></li>
</ul>

<h3>Title: OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02031">http://arxiv.org/abs/2310.02031</a></li>
<li>Code URL: https://github.com/zjunlp/knowlm</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02031]] OceanGPT: A Large Language Model for Ocean Science Tasks(http://arxiv.org/abs/2310.02031)</code></li>
<li>Summary: <p>Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet's surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.
</p></li>
</ul>

<h3>Title: On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?. (arXiv:2310.01581v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01581">http://arxiv.org/abs/2310.01581</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01581]] On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?(http://arxiv.org/abs/2310.01581)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have achieved unprecedented performance in
Natural Language Generation (NLG) tasks. However, many existing studies have
shown that they could be misused to generate undesired content. In response,
before releasing LLMs for public access, model developers usually align those
language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning
with Human Feedback (RLHF). Consequently, those aligned large language models
refuse to generate undesired content when facing potentially harmful/unethical
requests. A natural question is "could alignment really prevent those
open-sourced large language models from being misused to generate undesired
content?''. In this work, we provide a negative answer to this question. In
particular, we show those open-sourced, aligned large language models could be
easily misguided to generate undesired content without heavy computations or
careful prompt designs. Our key idea is to directly manipulate the generation
process of open-sourced LLMs to misguide it to generate undesired content
including harmful or biased information and even private data. We evaluate our
method on 4 open-sourced LLMs accessible publicly and our finding highlights
the need for more advanced mitigation strategies for open-sourced LLMs.
</p></li>
</ul>

<h3>Title: SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01557">http://arxiv.org/abs/2310.01557</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01557]] SmartPlay : A Benchmark for LLMs as Intelligent Agents(http://arxiv.org/abs/2310.01557)</code></li>
<li>Summary: <p>Recent large language models (LLMs) have demonstrated great potential toward
intelligent agents and next-gen automation, but there currently lacks a
systematic benchmark for evaluating LLMs' abilities as agents. We introduce
SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs
as agents. SmartPlay consists of 6 different games, including
Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique
setting, providing up to 20 evaluation settings and infinite environment
variations. Each game in SmartPlay uniquely challenges a subset of 9 important
capabilities of an intelligent LLM agent, including reasoning with object
dependencies, planning ahead, spatial reasoning, learning from history, and
understanding randomness. The distinction between the set of capabilities each
game test allows us to analyze each capability separately. SmartPlay serves not
only as a rigorous testing ground for evaluating the overall performance of LLM
agents but also as a road-map for identifying gaps in current methodologies. We
release our benchmark at github.com/LLMsmartplay/SmartPlay
</p></li>
</ul>

<h3>Title: Large Language Models as Analogical Reasoners. (arXiv:2310.01714v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01714">http://arxiv.org/abs/2310.01714</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01714]] Large Language Models as Analogical Reasoners(http://arxiv.org/abs/2310.01714)</code></li>
<li>Summary: <p>Chain-of-thought (CoT) prompting for language models demonstrates impressive
performance across reasoning tasks, but typically needs labeled exemplars of
the reasoning process. In this work, we introduce a new prompting approach,
Analogical Prompting, designed to automatically guide the reasoning process of
large language models. Inspired by analogical reasoning, a cognitive process in
which humans draw from relevant past experiences to tackle new problems, our
approach prompts language models to self-generate relevant exemplars or
knowledge in the context, before proceeding to solve the given problem. This
method presents several advantages: it obviates the need for labeling or
retrieving exemplars, offering generality and convenience; it can also tailor
the generated exemplars and knowledge to each problem, offering adaptability.
Experimental results show that our approach outperforms 0-shot CoT and manual
few-shot CoT in a variety of reasoning tasks, including math problem solving in
GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in
BIG-Bench.
</p></li>
</ul>

<h3>Title: Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01728">http://arxiv.org/abs/2310.01728</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01728]] Time-LLM: Time Series Forecasting by Reprogramming Large Language Models(http://arxiv.org/abs/2310.01728)</code></li>
<li>Summary: <p>Time series forecasting holds significant importance in many real-world
dynamic systems and has been extensively studied. Unlike natural language
process (NLP) and computer vision (CV), where a single large model can tackle
multiple tasks, models for time series forecasting are often specialized,
necessitating distinct designs for different tasks and applications. While
pre-trained foundation models have made impressive strides in NLP and CV, their
development in time series domains has been constrained by data sparsity.
Recent studies have revealed that large language models (LLMs) possess robust
pattern recognition and reasoning abilities over complex sequences of tokens.
However, the challenge remains in effectively aligning the modalities of time
series data and natural language to leverage these capabilities. In this work,
we present Time-LLM, a reprogramming framework to repurpose LLMs for general
time series forecasting with the backbone language models kept intact. We begin
by reprogramming the input time series with text prototypes before feeding it
into the frozen LLM to align the two modalities. To augment the LLM's ability
to reason with time series data, we propose Prompt-as-Prefix (PaP), which
enriches the input context and directs the transformation of reprogrammed input
patches. The transformed time series patches from the LLM are finally projected
to obtain the forecasts. Our comprehensive evaluations demonstrate that
Time-LLM is a powerful time series learner that outperforms state-of-the-art,
specialized forecasting models. Moreover, Time-LLM excels in both few-shot and
zero-shot learning scenarios.
</p></li>
</ul>

<h3>Title: DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models. (arXiv:2310.01870v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01870">http://arxiv.org/abs/2310.01870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01870]] DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models(http://arxiv.org/abs/2310.01870)</code></li>
<li>Summary: <p>As large language models (LLMs) become more capable, there is an urgent need
for interpretable and transparent tools. Current methods are difficult to
implement, and accessible tools to analyze model internals are lacking. To
bridge this gap, we present DeepDecipher - an API and interface for probing
neurons in transformer models' MLP layers. DeepDecipher makes the outputs of
advanced interpretability techniques for LLMs readily available. The
easy-to-use interface also makes inspecting these complex models more
intuitive. This paper outlines DeepDecipher's design and capabilities. We
demonstrate how to analyze neurons, compare models, and gain insights into
model behavior. For example, we contrast DeepDecipher's functionality with
similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher
enables efficient, scalable analysis of LLMs. By granting access to
state-of-the-art interpretability methods, DeepDecipher makes LLMs more
transparent, trustworthy, and safe. Researchers, engineers, and developers can
quickly diagnose issues, audit systems, and advance the field.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Elastic Interaction Energy Loss for Traffic Image Segmentation. (arXiv:2310.01449v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01449">http://arxiv.org/abs/2310.01449</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01449]] Elastic Interaction Energy Loss for Traffic Image Segmentation(http://arxiv.org/abs/2310.01449)</code></li>
<li>Summary: <p>Segmentation is a pixel-level classification of images. The accuracy and fast
inference speed of image segmentation are crucial for autonomous driving
safety. Fine and complex geometric objects are the most difficult but important
recognition targets in traffic scene, such as pedestrians, traffic signs and
lanes. In this paper, a simple and efficient geometry-sensitive energy-based
loss function is proposed to Convolutional Neural Network (CNN) for multi-class
segmentation on real-time traffic scene understanding. To be specific, the
elastic interaction energy (EIE) between two boundaries will drive the
prediction moving toward the ground truth until completely overlap. The EIE
loss function is incorporated into CNN to enhance accuracy on fine-scale
structure segmentation. In particular, small or irregularly shaped objects can
be identified more accurately, and discontinuity issues on slender objects can
be improved. Our approach can be applied to different segmentation-based
problems, such as urban scene segmentation and lane detection. We
quantitatively and qualitatively analyze our method on three traffic datasets,
including urban scene data Cityscapes, lane data TuSimple and CULane. The
results show that our approach consistently improves performance, especially
when using real-time, lightweight networks as the backbones, which is more
suitable for autonomous driving.
</p></li>
</ul>

<h3>Title: Progressive DeepSSM: Training Methodology for Image-To-Shape Deep Models. (arXiv:2310.01529v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01529">http://arxiv.org/abs/2310.01529</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01529]] Progressive DeepSSM: Training Methodology for Image-To-Shape Deep Models(http://arxiv.org/abs/2310.01529)</code></li>
<li>Summary: <p>Statistical shape modeling (SSM) is an enabling quantitative tool to study
anatomical shapes in various medical applications. However, directly using 3D
images in these applications still has a long way to go. Recent deep learning
methods have paved the way for reducing the substantial preprocessing steps to
construct SSMs directly from unsegmented images. Nevertheless, the performance
of these models is not up to the mark. Inspired by multiscale/multiresolution
learning, we propose a new training strategy, progressive DeepSSM, to train
image-to-shape deep learning models. The training is performed in multiple
scales, and each scale utilizes the output from the previous scale. This
strategy enables the model to learn coarse shape features in the first scales
and gradually learn detailed fine shape features in the later scales. We
leverage shape priors via segmentation-guided multi-task learning and employ
deep supervision loss to ensure learning at each scale. Experiments show the
superiority of models trained by the proposed strategy from both quantitative
and qualitative perspectives. This training methodology can be employed to
improve the stability and accuracy of any deep learning method for inferring
statistical representations of anatomies from medical images and can be adopted
by existing deep learning methods to improve model accuracy and training
stability.
</p></li>
</ul>

<h3>Title: You Only Look at Once for Real-time and Generic Multi-Task. (arXiv:2310.01641v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01641">http://arxiv.org/abs/2310.01641</a></li>
<li>Code URL: https://github.com/jiayuanwang-jw/yolov8-multi-task</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01641]] You Only Look at Once for Real-time and Generic Multi-Task(http://arxiv.org/abs/2310.01641)</code></li>
<li>Summary: <p>High precision, lightweight, and real-time responsiveness are three essential
requirements for implementing autonomous driving. Considering all of them
simultaneously is a challenge. In this study, we present an adaptive,
real-time, and lightweight multi-task model designed to concurrently handle
object detection, drivable area segmentation, and lane detection tasks. To
achieve this research objective, we developed an end-to-end multi-task model
with a unified and streamlined segmentation structure. Our model operates
without the need for any specific customization structure or loss function. We
achieved competitive results on the BDD100k dataset, particularly in
visualization outcomes. The performance results show a mAP50 of 81.1% for
object detection, a mIoU of 91.0% for drivable area segmentation, and an IoU of
28.8% for lane line segmentation. Additionally, we introduced a real-road
dataset to evaluate our model's performance in a real scene, which
significantly outperforms competitors. This demonstrates that our model not
only exhibits competitive performance but is also more flexible and faster than
existing multi-task models. The source codes and pre-trained models are
released at https://github.com/JiayuanWang-JW/YOLOv8-multi-task
</p></li>
</ul>

<h3>Title: STARS: Zero-shot Sim-to-Real Transfer for Segmentation of Shipwrecks in Sonar Imagery. (arXiv:2310.01667v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01667">http://arxiv.org/abs/2310.01667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01667]] STARS: Zero-shot Sim-to-Real Transfer for Segmentation of Shipwrecks in Sonar Imagery(http://arxiv.org/abs/2310.01667)</code></li>
<li>Summary: <p>In this paper, we address the problem of sim-to-real transfer for object
segmentation when there is no access to real examples of an object of interest
during training, i.e. zero-shot sim-to-real transfer for segmentation. We focus
on the application of shipwreck segmentation in side scan sonar imagery. Our
novel segmentation network, STARS, addresses this challenge by fusing a
predicted deformation field and anomaly volume, allowing it to generalize
better to real sonar images and achieve more effective zero-shot sim-to-real
transfer for image segmentation. We evaluate the sim-to-real transfer
capabilities of our method on a real, expert-labeled side scan sonar dataset of
shipwrecks collected from field work surveys with an autonomous underwater
vehicle (AUV). STARS is trained entirely in simulation and performs zero-shot
shipwreck segmentation with no additional fine-tuning on real data. Our method
provides a significant 20% increase in segmentation performance for the
targeted shipwreck class compared to the best baseline.
</p></li>
</ul>

<h3>Title: Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation. (arXiv:2310.01680v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01680">http://arxiv.org/abs/2310.01680</a></li>
<li>Code URL: https://github.com/zshyang/kaf</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01680]] Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation(http://arxiv.org/abs/2310.01680)</code></li>
<li>Summary: <p>Pretraining CNN models (i.e., UNet) through self-supervision has become a
powerful approach to facilitate medical image segmentation under low annotation
regimes. Recent contrastive learning methods encourage similar global
representations when the same image undergoes different transformations, or
enforce invariance across different image/patch features that are intrinsically
correlated. However, CNN-extracted global and local features are limited in
capturing long-range spatial dependencies that are essential in biological
anatomy. To this end, we present a keypoint-augmented fusion layer that
extracts representations preserving both short- and long-range self-attention.
In particular, we augment the CNN feature map at multiple scales by
incorporating an additional input that learns long-range spatial self-attention
among localized keypoint features. Further, we introduce both global and local
self-supervised pretraining for the framework. At the global scale, we obtain
global representations from both the bottleneck of the UNet, and by aggregating
multiscale keypoint features. These global features are subsequently
regularized through image-level contrastive objectives. At the local scale, we
define a distance-based criterion to first establish correspondences among
keypoints and encourage similarity between their features. Through extensive
experiments on both MRI and CT segmentation tasks, we demonstrate the
architectural advantages of our proposed method in comparison to both CNN and
Transformer-based UNets, when all architectures are trained with randomly
initialized weights. With our proposed pretraining strategy, our method further
outperforms existing SSL methods by producing more robust self-attention and
achieving state-of-the-art segmentation results. The code is available at
https://github.com/zshyang/kaf.git.
</p></li>
</ul>

<h3>Title: Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01825">http://arxiv.org/abs/2310.01825</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01825]] Empirical Study of PEFT techniques for Winter Wheat Segmentation(http://arxiv.org/abs/2310.01825)</code></li>
<li>Summary: <p>Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced
significant growth and have been extensively employed to adapt large vision and
language models to various domains, enabling satisfactory model performance
with minimal computational needs. Despite these advances, more research has yet
to delve into potential PEFT applications in real-life scenarios, particularly
in the critical domains of remote sensing and crop monitoring. The diversity of
climates across different regions and the need for comprehensive large-scale
datasets have posed significant obstacles to accurately identify crop types
across varying geographic locations and changing growing seasons. This study
seeks to bridge this gap by comprehensively exploring the feasibility of
cross-area and cross-year out-of-distribution generalization using the
State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to
explore PEFT approaches for crop monitoring. Specifically, we focus on adapting
the SOTA TSViT model to address winter wheat field segmentation, a critical
task for crop monitoring and food security. This adaptation process involves
integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and
prompt tuning. Using PEFT techniques, we achieved notable results comparable to
those achieved using full fine-tuning methods while training only a mere 0.7%
parameters of the whole TSViT architecture. The in-house labeled data-set,
referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated
polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over
five consecutive years. Using Sentinel-2 images, our model achieved a 84%
F1-score. We intend to publicly release the Lebanese winter wheat data set,
code repository, and model weights.
</p></li>
</ul>

<h3>Title: Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01828">http://arxiv.org/abs/2310.01828</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01828]] Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation(http://arxiv.org/abs/2310.01828)</code></li>
<li>Summary: <p>eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.
</p></li>
</ul>

<h3>Title: Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01837">http://arxiv.org/abs/2310.01837</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01837]] Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation(http://arxiv.org/abs/2310.01837)</code></li>
<li>Summary: <p>Current AI-based methods do not provide comprehensible physical
interpretations of the utilized data, extracted features, and
predictions/inference operations. As a result, deep learning models trained
using high-resolution satellite imagery lack transparency and explainability
and can be merely seen as a black box, which limits their wide-level adoption.
Experts need help understanding the complex behavior of AI models and the
underlying decision-making process. The explainable artificial intelligence
(XAI) field is an emerging field providing means for robust, practical, and
trustworthy deployment of AI models. Several XAI techniques have been proposed
for image classification tasks, whereas the interpretation of image
segmentation remains largely unexplored. This paper offers to bridge this gap
by adapting the recent XAI classification algorithms and making them usable for
muti-class image segmentation, where we mainly focus on buildings' segmentation
from high-resolution satellite images. To benchmark and compare the performance
of the proposed approaches, we introduce a new XAI evaluation methodology and
metric based on "Entropy" to measure the model uncertainty. Conventional XAI
evaluation methods rely mainly on feeding area-of-interest regions from the
image back to the pre-trained (utility) model and then calculating the average
change in the probability of the target class. Those evaluation metrics lack
the needed robustness, and we show that using Entropy to monitor the model
uncertainty in segmenting the pixels within the target class is more suitable.
We hope this work will pave the way for additional XAI research for image
segmentation and applications in the remote sensing discipline.
</p></li>
</ul>

<h3>Title: Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01845">http://arxiv.org/abs/2310.01845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01845]] Zero-Shot Refinement of Buildings' Segmentation Models using SAM(http://arxiv.org/abs/2310.01845)</code></li>
<li>Summary: <p>Foundation models have excelled in various tasks but are often evaluated on
general benchmarks. The adaptation of these models for specific domains, such
as remote sensing imagery, remains an underexplored area. In remote sensing,
precise building instance segmentation is vital for applications like urban
planning. While Convolutional Neural Networks (CNNs) perform well, their
generalization can be limited. For this aim, we present a novel approach to
adapt foundation models to address existing models' generalization dropback.
Among several models, our focus centers on the Segment Anything Model (SAM), a
potent foundation model renowned for its prowess in class-agnostic image
segmentation capabilities. We start by identifying the limitations of SAM,
revealing its suboptimal performance when applied to remote sensing imagery.
Moreover, SAM does not offer recognition abilities and thus fails to classify
and tag localized objects. To address these limitations, we introduce different
prompting strategies, including integrating a pre-trained CNN as a prompt
generator. This novel approach augments SAM with recognition abilities, a first
of its kind. We evaluated our method on three remote sensing datasets,
including the WHU Buildings dataset, the Massachusetts Buildings dataset, and
the AICrowd Mapping Challenge. For out-of-distribution performance on the WHU
dataset, we achieve a 5.47% increase in IoU and a 4.81% improvement in
F1-score. For in-distribution performance on the WHU dataset, we observe a
2.72% and 1.58% increase in True-Positive-IoU and True-Positive-F1 score,
respectively. We intend to release our code repository, hoping to inspire
further exploration of foundation models for domain-specific tasks within the
remote sensing community.
</p></li>
</ul>

<h3>Title: CoralVOS: Dataset and Benchmark for Coral Video Segmentation. (arXiv:2310.01946v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.01946">http://arxiv.org/abs/2310.01946</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.01946]] CoralVOS: Dataset and Benchmark for Coral Video Segmentation(http://arxiv.org/abs/2310.01946)</code></li>
<li>Summary: <p>Coral reefs formulate the most valuable and productive marine ecosystems,
providing habitat for many marine species. Coral reef surveying and analysis
are currently confined to coral experts who invest substantial effort in
generating comprehensive and dependable reports (\emph{e.g.}, coral coverage,
population, spatial distribution, \textit{etc}), from the collected survey
data. However, performing dense coral analysis based on manual efforts is
significantly time-consuming, the existing coral analysis algorithms compromise
and opt for performing down-sampling and only conducting sparse point-based
coral analysis within selected frames. However, such down-sampling will
\textbf{inevitable} introduce the estimation bias or even lead to wrong
results. To address this issue, we propose to perform \textbf{dense coral video
segmentation}, with no down-sampling involved. Through video object
segmentation, we could generate more \textit{reliable} and \textit{in-depth}
coral analysis than the existing coral reef analysis algorithms. To boost such
dense coral analysis, we propose a large-scale coral video segmentation
dataset: \textbf{CoralVOS} as demonstrated in Fig. 1. To the best of our
knowledge, our CoralVOS is the first dataset and benchmark supporting dense
coral video segmentation. We perform experiments on our CoralVOS dataset,
including 6 recent state-of-the-art video object segmentation (VOS) algorithms.
We fine-tuned these VOS algorithms on our CoralVOS dataset and achieved
observable performance improvement. The results show that there is still great
potential for further promoting the segmentation accuracy. The dataset and
trained models will be released with the acceptance of this work to foster the
coral reef research community.
</p></li>
</ul>

<h3>Title: MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts. (arXiv:2310.02000v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02000">http://arxiv.org/abs/2310.02000</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02000]] MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts(http://arxiv.org/abs/2310.02000)</code></li>
<li>Summary: <p>While self-supervised learning (SSL) algorithms have been widely used to
pre-train deep models, few efforts [11] have been done to improve
representation learning of X-ray image analysis with SSL pre-trained models. In
this work, we study a novel self-supervised pre-training pipeline, namely
Multi-task Self-super-vised Continual Learning (MUSCLE), for multiple medical
imaging tasks, such as classification and segmentation, using X-ray images
collected from multiple body parts, including heads, lungs, and bones.
Specifically, MUSCLE aggregates X-rays collected from multiple body parts for
MoCo-based representation learning, and adopts a well-designed continual
learning (CL) procedure to further pre-train the backbone subject various X-ray
analysis tasks jointly. Certain strategies for image pre-processing, learning
schedules, and regularization have been used to solve data heterogeneity,
overfitting, and catastrophic forgetting problems for multi-task/dataset
learning in MUSCLE.We evaluate MUSCLE using 9 real-world X-ray datasets with
various tasks, including pneumonia classification, skeletal abnormality
classification, lung segmentation, and tuberculosis (TB) detection. Comparisons
against other pre-trained models [7] confirm the proof-of-concept that
self-supervised multi-task/dataset continual pre-training could boost the
performance of X-ray image analysis.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
